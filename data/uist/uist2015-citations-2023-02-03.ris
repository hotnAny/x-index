
TY  - CHAP
AU  - Tanner, Kesler; Johnson, Naomi; Landay, James A.
TI  - Poirot: A Web Inspector for Designers
PY  - 2019
AB  - NA
SP  - 229
EP  - 251
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-28960-7_14
ER  - 

TY  - JOUR
AU  - Valente, Luis; Feijó, Bruno; Ribeiro, Alexandre; Clua, Esteban
TI  - Pervasive virtuality in digital entertainment applications and its quality requirements
PY  - 2018
AB  - NA
SP  - 139
EP  - 152
JF  - Entertainment Computing
VL  - 26
IS  - NA
PB  - 
DO  - 10.1016/j.entcom.2018.02.006
ER  - 

TY  - JOUR
AU  - Ozacar, Kasim; Hincapié-Ramos, Juan David; Takashima, Kazuki; Kitamura, Yoshifumi
TI  - 3D Selection Techniques for Mobile Augmented Reality Head-Mounted Displays
PY  - 2016
AB  - NA
SP  - 579
EP  - 591
JF  - Interacting with Computers
VL  - 29
IS  - 4
PB  - 
DO  - 10.1093/iwc/iww035
ER  - 

TY  - NA
AU  - Li, Zhi; Zhao, Maozheng; Das, Dibyendu; ZHAO, HANG; Ma, Yan; Liu, Wanyu; Beaudouin-Lafon, Michel; Wang, Fusheng; Ramakrishnan, IV; Bi, Xiaojun
TI  - Select or Suggest? Reinforcement Learning-based Method for High-Accuracy Target Selection on Touchscreens
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517472
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles; Haliyo, Sinan; Frey, Pascal
TI  - UIST - CoVR: A Large-Scale Force-Feedback Robotic Interface for Non-Deterministic Scenarios in VR
PY  - 2020
AB  - We present CoVR, a novel robotic interface providing strong kinesthetic feedback (100 N) in a room-scale VR arena. It consists of a physical column mounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of (1) resisting to body-scaled users' actions such as pushing or leaning; (2) acting on the users by pulling or transporting them as well as (3) carrying multiple potentially heavy objects (up to 80kg) that users can freely manipulate or make interact with each other. We describe its implementation and define a trajectory generation algorithm based on a novel user intention model to support non-deterministic scenarios, where the users are free to interact with any virtual object of interest with no regards to the scenarios' progress. A technical evaluation and a user study demonstrate the feasibility and usability of CoVR, as well as the relevance of whole-body interactions involving strong forces, such as being pulled through or transported.
SP  - 209
EP  - 222
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415891
ER  - 

TY  - NA
AU  - Pohl, Henning; Muresan, Andreea; Hornbæk, Kasper
TI  - CHI - Charting Subtle Interaction in the HCI Literature
PY  - 2019
AB  - Human-computer interaction is replete with ways of talking about qualities of interaction or interfaces, including if they are expressive, rich, fluid, or playful. An example of such a quality is subtle. While this word is frequently used in the literature, we lack a coherent account of what it means to be subtle, how to achieve subtleness in an interface, and what theoretical backing subtleness has. To create such an account, we analyze a sample of 55 publications that use the word subtle. We describe the variants of subtle interaction in the literature, including claimed benefits, empirical approaches, and ethical considerations. Not only does this create a basis for thinking about subtleness as a quality of interaction, it also works to show how to solidify varieties of quality in HCI. We conclude by outlining some open empirical and conceptual questions about subtleness.
SP  - 418
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300648
ER  - 

TY  - BOOK
AU  - Matulic, Fabrice; Vogel, Daniel; Dachselt, Raimund
TI  - ISS - Hand Contact Shape Recognition for Posture-Based Tabletop Widgets and Interaction
PY  - 2017
AB  - Tabletop interaction can be enriched by considering whole hands as input instead of only fingertips. We describe a generalised, reproducible computer vision algorithm to recognise hand contact shapes, with support for arm rejection, as well as dynamic properties like finger movement and hover. A controlled experiment shows the algorithm can detect seven different contact shapes with roughly 91% average accuracy. The effect of long sleeves and non-user specific templates is also explored. The algorithm is used to trigger, parameterise, and dynamically control menu and tool widgets, and the usability of a subset of these are qualitatively evaluated in a realistic application. Based on our findings, we formulate a number of design recommendations for hand shape-based interaction.
SP  - 3
EP  - 11
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134126
ER  - 

TY  - JOUR
AU  - Pérez-Medina, Jorge Luis; Villarreal, Santiago; Vanderdonckt, Jean
TI  - A Gesture Elicitation Study of Nose-Based Gestures.
PY  - 2020
AB  - Presently, miniaturized sensors can be embedded in any small-size wearable to recognize movements on some parts of the human body. For example, an electrooculography-based sensor in smart glasses recognizes finger movements on the nose. To explore the interaction capabilities, this paper conducts a gesture elicitation study as a between-subjects experiment involving one group of 12 females and one group of 12 males, expressing their preferred nose-based gestures on 19 Internet-of-Things tasks. Based on classification criteria, the 912 elicited gestures are clustered into 53 unique gestures resulting in 23 categories, to form a taxonomy and a consensus set of 38 final gestures, providing researchers and practitioners with a larger base with six design guidelines. To test whether the measurement method impacts these results, the agreement scores and rates, computed for determining the most agreed gestures upon participants, are compared with the Condorcet and the de Borda count methods to observe that the results remain consistent, sometimes with a slightly different order. To test whether the results are sensitive to gender, inferential statistics suggest that no significant difference exists between males and females for agreement scores and rates.
SP  - 7118
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 24
PB  - 
DO  - 10.3390/s20247118
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Wu, Cheng-Yuan; Huang, Lee-Ting; Hung, Yi-Ping
TI  - MobileHCI Adjunct - ThumbRing: private interactions using one-handed thumb motion input on finger segments
PY  - 2016
AB  - We propose an input device, ThumbRing, for items selection on head-mounted displays (HMDs) or smart glasses. ThumbRing is a ring with an inertial measurement unit (IMU) worn on the thumb to track the motion. By arranging an item to a finger segment, users touch and slide finger segments to select the items. To resist shake in mobile conditions such as walking, another IMU is attached to the back of the hand to compute relative angles between the hand and the thumb. Sliding and touching the segments with the thumb in the hand provide privacy, subtlety, natural haptic feedback and similar input area to smartphones. A pilot study is performed to obtain users' preference finger segments. We evaluate the performance of ThumbRing in different conditions and commitment approaches in a user study. The results show that accuracy are 92.3% and 89.7% in the sitting and walking conditions, respectively.
SP  - 791
EP  - 798
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2957265.2961859
ER  - 

TY  - NA
AU  - Dalsgaard, Tor-Salve; Knibbe, Jarrod; Bergström, Joanna
TI  - Modeling Pointing for 3D Target Selection in VR
PY  - 2021
AB  - Virtual reality (VR) allows users to interact similarly to how they do in the physical world, such as touching, moving, and pointing at objects. To select objects at a distance, most VR techniques rely on casting a ray through one or two points located on the user’s body (e.g., on the head and a finger), and placing a cursor on that ray. However, previous studies show that such rays do not help users achieve optimal pointing accuracy nor correspond to how they would naturally point. We seek to find features, which would best describe natural pointing at distant targets. We collect motion data from seven locations on the hand, arm, and body, while participants point at 27 targets across a virtual room. We evaluate the features of pointing and analyse sets of those for predicting pointing targets. Our analysis shows an 87% classification accuracy between the 27 targets for the best feature set and a mean distance of 23.56 cm in predicting pointing targets across the room. The feature sets can inform the design of more natural and effective VR pointing techniques for distant object selection.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489853
ER  - 

TY  - NA
AU  - Fraser, C. Ailie; Grossman, Tovi; Fitzmaurice, George
TI  - CHI - WeBuild: Automatically Distributing Assembly Tasks Among Collocated Workers to Improve Coordination
PY  - 2017
AB  - Physical construction and assembly tasks are often carried out by groups of collocated workers, and they can be difficult to coordinate. Group members must spend time deciding how to split up the task, how to assign subtasks to each other, and in what order subtasks should be completed. Informed by an observational study examining group coordination challenges, we built a task distribution system called WeBuild. Our custom algorithm dynamically assigns subtasks to workers in a group, taking into account factors such as the dependencies between subtasks and the skills of each group member. Each worker views personalized step-by-step instructions on a mobile phone, while a dashboard visualizes the entire process. An initial study found that WeBuild reduced the start-up time needed to coordinate and begin a task, and provides direction for future research to build on toward improving group efficiency and coordination for complex tasks.
SP  - 1817
EP  - 1830
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3026036
ER  - 

TY  - NA
AU  - Han, Ping-Hsuan; Chen, Yang-Sheng; Lee, Kong-Chang; Wang, Hao-Cheng; Hsieh, Chiao-En; Hsiao, Jui-Chun; Chou, Chien-Hsing; Hung, Yi-Ping
TI  - VRST - Haptic around: multiple tactile sensations for immersive environment and interaction in virtual reality
PY  - 2018
AB  - In this paper, we present Haptic Around, a hybrid-haptic feedback system, which utilizes fan, hot air blower, mist creator and heat light to recreate multiple tactile sensations in virtual reality for enhancing the immersive environment and interaction. This system consists of a steerable haptic device rigged on the top of the user head and a handheld device also with haptics feedbacks to simultaneously provide tactile sensations to the users in a 2m x 2m space. The steerable haptic device can enhance the immersive environment for providing full body experience, such as heat in the desert or cold in the snow mountain. Additionally, the handheld device can enhance the immersive interaction for providing partial body experience, such as heating the iron or quenching the hot iron. With our system, the users can perceive visual, auditory and haptic when they are moving around in virtual space and interacting with virtual object. In our study, the result has shown the potential of the hybrid-haptic feedback system, which the participants rated the enjoyment, realism, quality, immersion higher than the other.
SP  - 35
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281507
ER  - 

TY  - JOUR
AU  - Jeong, Kisung; Kim, Jinmo; Kim, Mingyu; Lee, Jiwon; Kim, Chanhun
TI  - Asymmetric Interface: User Interface of Asymmetric Virtual Reality for New Presence and Experience
PY  - 2019
AB  - This study proposes an asymmetric interface that can provide head-mounted display (HMD) and non-HMD users with improved presence and an experience consistent with the user’s environment in an asymmetric virtual reality. For the proposed asymmetric interface, a controller-based hand interface is designed for portability, easy and convenient use, and high immersion. Subsequently, a three-step decision-making structure that supports accurate and efficient decision-making is defined based on the asymmetric experience structure of each user (HMD and non-HMD). Based on this process, an optimal interface that distinguishes between HMD (direct interaction) and non-HMD users (multi-viewpoint interaction) is implemented. With the objective of surveying and analyzing each user’s experience along with the presence provided by the proposed interface, an asymmetric virtual reality application is developed directly, and an experiment is conducted with the participants. Finally, it is statistically analyzed and verified that the use of the proposed asymmetric interface can provide optimal presence and user-optimized experience to both HMD and non-HMD users.
SP  - 53
EP  - NA
JF  - Symmetry
VL  - 12
IS  - 1
PB  - 
DO  - 10.3390/sym12010053
ER  - 

TY  - NA
AU  - McGrath, William; Drew, Daniel S.; Warner, Jeremy; Kazemitabaar, Majeed; Karchemsky, Mitchell; Mellis, David A.; Hartmann, Björn
TI  - UIST - Bifröst: Visualizing and Checking Behavior of Embedded Systems across Hardware and Software
PY  - 2017
AB  - The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifrost automatically instruments and captures the progress of the user's code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware configuration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifrost influences debugging workflows.
SP  - 299
EP  - 310
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126658
ER  - 

TY  - CHAP
AU  - Carlson, Gordon; Caporusso, Nicholas
TI  - A Physically Immersive Platform for Training Emergency Responders and Law Enforcement Officers
PY  - 2018
AB  - Training law enforcement officers and emergency responders requires significant investment in terms of time, financial resources, logistics, organization, and personnel reallocation.
SP  - 108
EP  - 116
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-93882-0_11
ER  - 

TY  - JOUR
AU  - Lewis, Blaine; Vogel, Daniel
TI  - Longer Delays in Rehearsal-based Interfaces Increase Expert Use
PY  - 2020
AB  - Rehearsal-based interfaces are designed to encourage a transition from novice to expert, but many users fail to make this transition. Most of these interfaces activate novice mode after a short delay, between 150 and 500 ms. We investigate the impact of delay time on expert usage and learning in three crowdsourced experiments. The first experiment examines an 8-item marking menu with delay times from 200 ms to 2 s. Results show longer delays increase successful expert selections. The second and third experiments generalise this result to a different rehearsal-based menu, a desktop clone of FastTap with 8 items and 15 items. Together, our results show that expert use correlates positively with increased delay time, but can increase errors since users are less risk averse. We also find imperceptible delays of 200 ms can harm long-term retention of menu items. Designers should consider longer delays in rehearsal-based interfaces to encourage a transition to expert usage.
SP  - 1
EP  - 41
JF  - ACM Transactions on Computer-Human Interaction
VL  - 27
IS  - 6
PB  - 
DO  - 10.1145/3418196
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Wu, K. D.; Chen, Jacqueline; Lopes, Pedro
TI  - Prolonging VR Haptic Experiences by Harvesting Kinetic Energy from the User
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545635
ER  - 

TY  - NA
AU  - Yamanaka, Shota; Stuerzlinger, Wolfgang; Miyashita, Homei
TI  - CHI - Steering through Successive Objects
PY  - 2018
AB  - We investigate stroking motions through successive objects with styli. There are several promising models for stroking motions, such as crossing tasks, which require endpoint accuracy of a stroke, or steering tasks, which require continuous accuracy throughout the trajectory. However, a task requiring users to repeatedly steer through constrained path segments has never been studied, although such operations are needed in GUIs, e.g., for selecting icons or objects on illustration software through lassoing. We empirically confirmed that the interval, trajectory width, and obstacle size significantly affect the movement speed. Existing models can not accurately predict user performance in such tasks. We found several unexpected results such as that steering through denser objects sometimes required less times than expected. Speed profile analysis showed the reasons behind such behaviors, such as participants' anticipation strategies. We also discuss the applicability of exiting performance models and revisions.
SP  - 603
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174177
ER  - 

TY  - NA
AU  - Sra, Misha; Garrido-Jurado, Sergio; Schmandt, Chris; Maes, Pattie
TI  - VRST - Procedurally generated virtual reality from 3D reconstructed physical space
PY  - 2016
AB  - We present a novel system for automatically generating immersive and interactive virtual reality (VR) environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas (WA) to enable real-walking in the generated virtual environment (VE). Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our approach is new, in that it allows a casual user to easily create virtual reality worlds in any indoor space of arbitrary size and shape without requiring specialized equipment or training. We demonstrate our approach through a fully working system implemented on the Google Project Tango tablet device.
SP  - 191
EP  - 200
JF  - Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2993369.2993372
ER  - 

TY  - NA
AU  - Hu, Yonghui; Yan, Yong; Efstratiou, Christos; Vela-Orte, David
TI  - I2MTC - Quantitative Shape Measurement of An Inflatable Rubber Dam Using Inertial Sensors
PY  - 2020
AB  - Shape measurement is of great importance for the effective control and safe operation of inflatable rubber dams. This paper presents for the first time a method to measure the cross-sectional shape of a rubber dam by placing an array of inertial measurement units (IMUs) on the peripheral of the rubber dam. The IMU array measures tangent angles of the dam peripheral by fusing accelerometer and gyroscope measurements. A continuous tangent angle function is derived by interpolating the tangent angles at discrete locations using a cubic spline. Finally, the shape is reconstructed by integrating the tangent angle function along the peripheral of the rubber dam. The performance of the measurement system is validated against a camera on a purpose-built test rig. Experimental results show that the measured and reference shapes are very similar, with a maximum similarity index of 8.5% under typical conditions. In addition, it is demonstrated that the system is robust against node failure by excluding readings of faulty nodes from shape reconstruction.
SP  - 1
EP  - 5
JF  - 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/i2mtc43012.2020.9128397
ER  - 

TY  - JOUR
AU  - Lapusan, Ciprian; Hancu, Olimpiu; Rad, Ciprian
TI  - Shape Sensing of Hyper-Redundant Robots Using an AHRS IMU Sensor Network.
PY  - 2022
AB  - The paper proposes a novel approach for shape sensing of hyper-redundant robots based on an AHRS IMU sensor network embedded into the structure of the robot. The proposed approach uses the data from the sensor network to directly calculate the kinematic parameters of the robot in modules operational space reducing thus the computational time and facilitating implementation of advanced real-time feedback system for shape sensing. In the paper the method is applied for shape sensing and pose estimation of an articulated joint-based hyper-redundant robot with identical 2-DoF modules serially connected. Using a testing method based on HIL techniques the authors validate the computed kinematic model and the computed shape of the robot prototype. A second testing method is used to validate the end effector pose using an external sensory system. The experimental results obtained demonstrate the feasibility of using this type of sensor network and the effectiveness of the proposed shape sensing approach for hyper-redundant robots.
SP  - 373
EP  - 373
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 1
PB  - 
DO  - 10.3390/s22010373
ER  - 

TY  - NA
AU  - Arora, Jatin; Saini, Aryan; Mehra, Nirmita; Jain, Varnit; Shrey, Shwetank; Parnami, Aman
TI  - CHI - VirtualBricks: Exploring a Scalable, Modular Toolkit for Enabling Physical Manipulation in VR
PY  - 2019
AB  - Often Virtual Reality (VR) experiences are limited by the design of standard controllers. This work aims to liberate a VR developer from these limitations in the physical realm to provide an expressive match to the limitless possibilities in the virtual realm. VirtualBricks is a LEGO based toolkit that enables construction of a variety of physical-manipulation enabled controllers for VR, by offering a set of feature bricks that emulate as well as extend the capabilities of default controllers. Based on the LEGO platform, the toolkit provides a modular, scalable solution for enabling passive haptics in VR. We demonstrate the versatility of our designs through a rich set of applications including re-implementations of artifacts from recent research. We share a VR Integration package for integration with Unity VR IDE, the CAD models for the feature bricks, for easy deployment of VirtualBricks within the community.
SP  - 56
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300286
ER  - 

TY  - NA
AU  - Gugenheimer, Jan; Stemasov, Evgeny; Frommel, Julian; Rukzio, Enrico
TI  - CHI - ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users
PY  - 2017
AB  - Virtual reality (VR) head-mounted displays (HMD) allow for a highly immersive experience and are currently becoming part of the living room entertainment. Current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user), resulting in all the bystanders (Non-HMD users) being excluded from the experience. We propose ShareVR, a proof-of-concept prototype using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. We designed and implemented ShareVR based on the insights of an initial online survey (n=48) with early adopters of VR HMDs. We ran a user study (n=16) comparing ShareVRto a baseline condition showing how the interaction using ShareVR led to an increase of enjoyment, presence and social interaction. In a last step we implemented several experiences for ShareVR, exploring its design space and giving insights for designers of co-located asymmetric VR experiences.
SP  - 4021
EP  - 4033
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025683
ER  - 

TY  - NA
AU  - Grubert, Jens
TI  - Mixed Reality Interaction Techniques.
PY  - 2021
AB  - This chapter gives an overview of interaction techniques for mixed reality including augmented and virtual reality (AR/VR). Various modalities for input and output are discussed. Specifically, techniques for tangible and surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and mouse-based, as well as haptic interaction are discussed. Furthermore, the combination of multiple modalities in multisensory and multimodal interaction, as well as interaction using multiple physical or virtual displays, are presented. Finally, interaction with intelligent virtual agents is considered.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Lee, Juyoung; Kim, Hyung-il; Gupta, Aakar; Bianchi, Andrea; Vogel, Daniel; Koike, Hideki; Woo, Woontack; Quigley, Aaron
TI  - MobileHCI - WRIST: Watch-Ring Interaction and Sensing Technique for Wrist Gestures and Macro-Micro Pointing
PY  - 2019
AB  - To better explore the incorporation of pointing and gesturing into ubiquitous computing, we introduce WRIST, an interaction and sensing technique that leverages the dexterity of human wrist motion. WRIST employs a sensor fusion approach which combines inertial measurement unit (IMU) data from a smartwatch and a smart ring. The relative orientation difference of the two devices is measured as the wrist rotation that is independent from arm rotation, which is also position and orientation invariant. Employing our test hardware, we demonstrate that WRIST affords and enables a number of novel yet simplistic interaction techniques, such as (i) macro-micro pointing without explicit mode switching and (ii) wrist gesture recognition when the hand is held in different orientations (e.g., raised or lowered). We report on two studies to evaluate the proposed techniques and we present a set of applications that demonstrate the benefits of WRIST. We conclude with a discussion of the limitations and highlight possible future pathways for research in pointing and gesturing with wearable devices.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340130
ER  - 

TY  - NA
AU  - Schmitz, Martin; Riemann, Jan; Müller, Florian; Kreis, Steffen; Mühlhäuser, Max
TI  - CHI - Oh, Snap! A Fabrication Pipeline to Magnetically Connect Conventional and 3D-Printed Electronics
PY  - 2021
AB  - 3D printing has revolutionized rapid prototyping by speeding up the creation of custom-shaped objects. With the rise of multi-material 3D printers, these custom-shaped objects can now be made interactive in a single pass through passive conductive structures. However, connecting conventional electronics to these conductive structures often still requires time-consuming manual assembly involving many wires, soldering or gluing. To alleviate these shortcomings, we propose : a fabrication pipeline and interfacing concept to magnetically connect a 3D-printed object equipped with passive sensing structures to conventional sensing electronics. To this end, utilizes ferromagnetic and conductive 3D-printed structures, printable in a single pass on standard printers. We further present a proof-of-concept capacitive sensing board that enables easy and robust magnetic assembly to quickly create interactive 3D-printed objects. We evaluate by assessing the robustness and quality of the connection and demonstrate its broad applicability by a series of example applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445641
ER  - 

TY  - NA
AU  - Henderson, Jay; Malacria, Sylvain; Nancel, Mathieu; Lank, Edward
TI  - CHI - Investigating the Necessity of Delay in Marking Menu Invocation
PY  - 2020
AB  - Delayed display of menu items is a core design component of marking menus, arguably to prevent visual distraction and foster the use of mark mode. We investigate these assumptions, by contrasting the original marking menu design with immediately-displayed marking menus. In three controlled experiments, we fail to reveal obvious and systematic performance or usability advantages to using delay and mark mode. Only in very constrained settings  after significant training and only two items to learn  did traditional marking menus show a time improvement of about 260~ms. Otherwise, we found an overall decrease in performance with delay, whether participants exhibited practiced or unpracticed behaviour. Our final study failed to demonstrate that an immediately-displayed menu interface is more visually disrupting than a delayed menu. These findings inform the costs and benefits of incorporating delay in marking menus, and motivate guidelines for situations in which its use is desirable.
SP  - 13
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376296
ER  - 

TY  - NA
AU  - Freeman, Euan; Vo, Dong-Bach; Brewster, Stephen
TI  - WHC - HaptiGlow: Helping Users Position their Hands for Better Mid-Air Gestures and Ultrasound Haptic Feedback
PY  - 2019
AB  - We present HaptiGlow, a technique that combines ultrasound haptics with peripheral visual feedback to help users find where to place their hand for improved mid-air interaction. Hand position is important. If a user’s hand is poorly placed, input sensors may have difficulty recognising their gestures. Mid-air haptic feedback is also hard to perceive when the hand is in a poor position. Our novel feedback addresses this important usability problem. Our results show the combination of ultrasound haptics and peripheral visuals is effective, with the strengths of each leading to accurate (23mm) and fast (4.6s) guidance in a 3D targeting task. Our technique improves midair interaction by easily helping users find a good hand position.
SP  - 289
EP  - 294
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816092
ER  - 

TY  - NA
AU  - Shi, Jiamin; Buschek, Daniel; Alt, Florian
TI  - CHI Extended Abstracts - Investigating the Impact of Feedback on Gaming Performance on Motivation to Interact with Public Displays
PY  - 2016
AB  - This paper investigates the influence of feedback about users' performance on their motivation as they interact with games on displays in public space. Our research is motivated by the fact that games are popular among both researchers and practitioners, due to their ability to attract many users. However, it is widely unclear, which factors impact on how much people play and whether they leave personal information on the display. We investigate different forms of feedback (highscore, real-time score and real-time rank during gameplay) and report on how they influence the behavior of users. Our results are based on data from the deployment of an interactive game in a public space.
SP  - 1344
EP  - 1351
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892465
ER  - 

TY  - CHAP
AU  - Pollock, Jordan; Teather, Robert J.
TI  - HCI (42) - CountMarks: Multi-Finger Marking Menus for Mobile Interaction with Head-Mounted Displays
PY  - 2020
AB  - We designed, implemented, and evaluated CountMarks, a novel menu that extends marking menus with multi-touch input on a secondary touchscreen, for use with smart glasses and head-mounted displays. We experimentally compared CountMarks to marking menus. Results indicate that CountMarks offers faster selection and better search accuracy, but slightly worse selection accuracy. A second experiment compared standing vs. walking while using CountMarks, and handheld vs. on-leg interaction. Results indicate that CountMarks can be used effectively with a handheld device while both standing and walking. We present an example application employing CountMarks in a mock Netflix UI, to demonstrate how the technique can be applied in existing applications.
SP  - 241
EP  - 260
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-60117-1_18
ER  - 

TY  - NA
AU  - Groenewald, Celeste; Anslow, Craig; Islam, Junayed; Rooney, Chris; Passmore, Peter J.; Wong, William
TI  - BCS HCI - Understanding 3D mid-air hand gestures with interactive surfaces and displays: a systematic literature review
PY  - 2016
AB  - 3D gesture based systems are becoming ubiquitous and there are many mid-air hand gestures that exist for interacting with digital surfaces and displays. There is no well defined gesture set for 3D mid-air hand gestures which makes it difficult to develop applications that have consistent gestures. To understand what gestures exist we conducted the first comprehensive systematic literature review on mid-air hand gestures following existing research methods. The results of the review identified 65 papers where the mid-air hand gestures supported tasks for selection, navigation, and manipulation. We also classified the gestures according to a gesture classification scheme and identified how these gestures have been empirically evaluated. The results of the review provide a richer understanding of what mid-air hand gestures have been designed, implemented, and evaluated in the literature which can help developers design better user experiences for digital interactive surfaces and displays.
SP  - 43
EP  - NA
JF  - Electronic Workshops in Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.14236/ewic/hci2016.43
ER  - 

TY  - NA
AU  - Fang, Cathy Mengying; Gu, Jianzhe; Yao, Lining; Harrison, Chris
TI  - ElectriPop: Low-Cost, Shape-Changing Displays Using Electrostatically Inflated Mylar Sheets
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501837
ER  - 

TY  - CHAP
AU  - Khalaf, Ahmed S.; Alharthi, A.; Alshehri, Ali; Dolgov, Igor; Toups, Zachary O.
TI  - HCI (2) - A Comparative Study of Hand-Gesture Recognition Devices for Games
PY  - 2020
AB  - Gesture recognition devices provide a new means for natural human-computer interaction. However, when selecting these devices to be used in games, designers might find it challenging to decide which gesture recognition device will work best. In the present research, we compare three vision-based, hand-gesture devices: Leap Motion, Microsoft’s Kinect, and Intel’s RealSense. The comparison provides game designers with an understanding of the main factors to consider when selecting these devices and how to design games that use them. We developed a simple hand-gesture-based game to evaluate performance, cognitive demand, comfort, and player experience of using these gesture devices. We found that participants preferred and performed much better using Leap Motion and Kinect compared to using RealSense. Leap Motion also outperformed or was equivalent to Kinect. These findings were supported by players’ accounts of their experiences using these gesture devices. Based on these findings, we discuss how such devices can be used by game designers and provide them with a set of design cautions that provide insights into the design of gesture-based games.
SP  - 57
EP  - 76
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_4
ER  - 

TY  - NA
AU  - Wang, Guanyun; Yao, Lining; Wang, Wen; Ou, Jifei; Cheng, Chin-Yi; Ishii, Hiroshi
TI  - CHI - xPrint: A Modularized Liquid Printer for Smart Materials Deposition
PY  - 2016
AB  - To meet the increasing requirements of HCI researchers who are looking into using liquid-based materials (e.g., hydrogels) to create novel interfaces, we present a design strategy for HCI researchers to build and customize a liquid-based smart material printing platform with off-the-shelf or easy-to-machine parts. For the hardware, we suggest a magnetic assembly-based modular design. These modularized parts can be easily and precisely reconfigured with off-the-shelf or easy-to-machine parts that can meet different processing requirements such as mechanical mixing, chemical reaction, light activation, and solution vaporization. In addition, xPrint supports an open-source, highly customizable software design and simulation platform, which is applicable for simulating and facilitating smart material constructions. Furthermore, compared to inkjet or pneumatic syringe-based printing systems, xPrint has a large range of printable materials from synthesized polymers to natural micro-organism-living cells with a printing resolution from 10μm up to 5mm (droplet size). In this paper, we will introduce the system design in detail and three use cases to demonstrate the material variability and the customizability for users with different demands (e.g., designers, scientific researchers, or artists).
SP  - 5743
EP  - 5752
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858281
ER  - 

TY  - NA
AU  - Rietzler, Michael; Haas, Gabriel; Dreja, Thomas; Geiselhart, Florian; Rukzio, Enrico
TI  - UIST - Virtual Muscle Force: Communicating Kinesthetic Forces Through Pseudo-Haptic Feedback and Muscle Input
PY  - 2019
AB  - Natural haptic feedback in virtual reality (VR) is complex andchallenging, due to the intricacy of necessary stimuli and re-spective hardware. Pseudo-haptic feedback aims at providinghaptic feedback without providing actual haptic stimuli butby using other sensory channels (e.g. visual cues) for feed-back. We combine such an approach with the additional inputmodality of muscle activity that is mapped to a virtual force toinfluence the interaction flow. In comparison to existing approaches as well as to no kines-thetic feedback at all the presented solution significantly in-creased immersion, enjoyment as well as the perceived qualityof kinesthetic feedback.
SP  - 913
EP  - 922
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347871
ER  - 

TY  - NA
AU  - Brasier, Eugenie; Chapuis, Olivier; Férey, Nicolas; Vezien, Jeanne; Appert, Caroline
TI  - ISMAR - ARPads: Mid-air Indirect Input for Augmented Reality
PY  - 2020
AB  - Interacting efficiently and comfortably with Augmented Reality (AR) headsets remains a major issue. We investigate the concept of mid-air pads as an alternative to gaze or direct hand input to control a cursor in windows anchored in the environment. ARPads allow users to control the cursor displayed in the headset screen through movements on a mid-air plane, which is not spatially aligned with the headset screen. We investigate a design space for ARPads, which takes into account the position of the pad relative to the user’s body, and the orientation of the pad relative to that of the headset screen. Our study suggests that 1) indirect input can achieve the same performance as direct input while causing less fatigue than hand raycast, 2) an ARPad should be attached to the wrist or waist rather than to the thigh, and 3) the ARPad and the screen should have the same orientation.
SP  - 332
EP  - 343
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00060
ER  - 

TY  - JOUR
AU  - Leiva, Luis A.; Vatavu, Radu-Daniel; Martín-Albo, Daniel; Plamondon, Réjean
TI  - Omnis Prædictio: Estimating the full spectrum of human performance with stroke gestures
PY  - 2020
AB  - Abstract Designing effective, usable, and widely adoptable stroke gesture commands for graphical user interfaces is a challenging task that traditionally involves multiple iterative rounds of prototyping, implementation, and follow-up user studies and controlled experiments for evaluation, verification, and validation. An alternative approach is to employ theoretical models of human performance, which can deliver practitioners with insightful information right from the earliest stages of user interface design. However, very few aspects of the large spectrum of human performance with stroke gesture input have been investigated and modeled so far, leaving researchers and practitioners of gesture-based user interface design with a very narrow range of predictable measures of human performance, mostly focused on estimating production time, of which extremely few cases delivered accompanying software tools to assist modeling. We address this problem by introducing “Omnis Praedictio” ( Omnis for short), a generic technique and companion web tool that provides accurate user-independent estimations of any numerical stroke gesture feature, including custom features specified in code. Our experimental results on three public datasets show that our model estimations correlate on average r s > 0.9 with groundtruth data. Omnis also enables researchers and practitioners to understand human performance with stroke gestures on many levels and, consequently, raises the bar for human performance models and estimation techniques for stroke gesture input.
SP  - 102466
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 142
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2020.102466
ER  - 

TY  - NA
AU  - Kao, Hsin-Liu Cindy; Bamforth, Miren; Kim, David; Schmandt, Chris
TI  - Skinmorph: texture-tunable on-skin interface through thin, programmable gel
PY  - 2018
AB  - SkinMorph is an on-skin interface which can selectively transition between soft and rigid states to serve as a texture-tunable wearable skin output. This texture change is made possible through the material design of smart hydrophillic gels. These gels are soft in resting state, yet when activate by heat (>36°C), they generate a micro-level structural change which results in observable stiffening. These gels are encapsulated in thin silicone patterned with resistive wires through a sew-and-transfer fabrication approach. We demonstrate application examples using the texture-tunable skin overlay as wearable, interactive protection for scenarios including: a carpal tunnel splint for rehabilitation, a protective layer for joints when engaging in high impact activities, and foot pads when wearing uncomfortable shoes. Our evaluation shows that the gel is 10 times stiffer when activated, and that users find the device skin-conformable.
SP  - 196
EP  - 203
JF  - Proceedings of the 2018 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267242.3267262
ER  - 

TY  - BOOK
AU  - Brauer, Christoph; Ariza, Oscar; Steinicke, Frank
TI  - Mensch &amp; Computer - An Active Tangible Device for Multitouch-Display Interaction
PY  - 2019
AB  - We introduce an interactive tangible input device (TID) for touchscreens. Our approach complements a passive TID design by active micro-controller-driven features focusing on user-experience aspects. The TID provides battery-powered wireless operation, autonomous position sensing capabilities, visual and tactile feedback as well as multiple touch inputs and momentary buttons. The device can be accurately tracked on capacitive touchscreens, enabling novel interaction techniques for content selection and manipulation in 2D or stereoscopic tabletop environments. Mid-air interaction is supported by the use of an inertia measurement unit (IMU) and short-to-mid-range distance sensors. Overall, the presented multi-purpose device can be built using off-the-shelf components, featuring a seamless firmware integration, a 3D-printable body enclosure and Unity3D integration.
SP  - 439
EP  - 444
JF  - Proceedings of Mensch und Computer 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340764.3344436
ER  - 

TY  - NA
AU  - Lin, Chuan-en; Cheng, Ta Ying; Ma, Xiaojuan
TI  - CHI - ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop
PY  - 2020
AB  - Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376614
ER  - 

TY  - NA
AU  - Vonach, Emanuel; Gatterer, Clemens; Kaufmann, Hannes
TI  - VR - VRRobot: Robot actuated props in an infinite virtual environment
PY  - 2017
AB  - We present the design and development of a fully immersive virtual reality (VR) system that can provide prop-based haptic feedback in an infinite virtual environment. It is conceived as a research tool for studying topics related to haptics in VR and based on off-the-shelf components. A robotic arm moves physical props, dynamically matching pose and location of an object in the virtual world. When the user reaches for the virtual object, his or her hands also encounter it in the real physical space. The interaction is not limited to specific body parts and does not rely on an external structure like an exoskeleton. In combination with a locomotion platform for close-to-natural walking, this allows unrestricted haptic interaction in a natural way in virtual environments of unlimited size. We describe the concept, the hardware and software architecture in detail. We establish safety design guidelines for human-robot interaction in VR. Our technical evaluation shows good response times and accuracy. We report on a user study conducted with 34 participants indicating promising results, and discuss the potential of our system.
SP  - 74
EP  - 83
JF  - 2017 IEEE Virtual Reality (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2017.7892233
ER  - 

TY  - JOUR
AU  - Sra, Misha; Garrido-Jurado, Sergio; Maes, Pattie
TI  - Oasis: Procedurally Generated Social Virtual Spaces from 3D Scanned Real Spaces
PY  - 2017
AB  - We present Oasis, a novel system for automatically generating immersive and interactive virtual reality environments for single and multiuser experiences. Oasis enables real-walking in the generated virtual environment by capturing indoor scenes in 3D and mapping walkable areas. It makes use of available depth information for recognizing objects in the real environment which are paired with virtual counterparts to leverage the physicality of the real world, for a more immersive virtual experience. Oasis allows co-located and remotely located users to interact seamlessly and walk naturally in a shared virtual environment. Experiencing virtual reality with currently available devices can be cumbersome due to presence of objects and furniture which need to be removed every time the user wishes to use VR. Our approach is new, in that it allows casual users to easily create virtual reality environments in any indoor space without rearranging furniture or requiring specialized equipment, skill or training. We demonstrate our approach to overlay a virtual environment over an existing physical space through fully working single and multiuser systems implemented on a Tango tablet device.
SP  - 3174
EP  - 3187
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2017.2762691
ER  - 

TY  - NA
AU  - Hoppe, Matthias; Knierim, Pascal; Kosch, Thomas; Funk, Markus; Futami, Lauren; Schneegass, Stefan; Henze, Niels; Schmidt, Albrecht; Machulla, Tonja
TI  - MUM - VRHapticDrones: Providing Haptics in Virtual Reality through Quadcopters
PY  - 2018
AB  - We present VRHapticDrones, a system utilizing quadcopters as levitating haptic feedback proxy. A touchable surface is attached to the side of the quadcopters to provide unintrusive, flexible, and programmable haptic feedback in virtual reality. Since the users' sense of presence in virtual reality is a crucial factor for the overall user experience, our system simulates haptic feedback of virtual objects. Quadcopters are dynamically positioned to provide haptic feedback relative to the physical interaction space of the user. In a first user study, we demonstrate that haptic feedback provided by VRHapticDrones significantly increases users' sense of presence compared to vibrotactile controllers and interactions without additional haptic feedback. In a second user study, we explored the quality of induced feedback regarding the expected feeling of different objects. Results show that VRHapticDrones is best suited to simulate objects that are expected to feel either light-weight or have yielding surfaces. With VRHapticDrones we contribute a solution to provide unintrusive and flexible feedback as well as insights for future VR haptic feedback systems.
SP  - 7
EP  - 18
JF  - Proceedings of the 17th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3282894.3282898
ER  - 

TY  - JOUR
AU  - Richter, Alina; Kessing, David; Fischer, Fabian; Pelzer, Lukas; Dilger, Klaus
TI  - Print-On Strategies to Bond Injection Molded Parts with Structures Produced by Fused-Deposition-Modeling
PY  - 2019
AB  - The biggest advantage of Additive Manufacturing is the individualization of products. Mass Customization is well known as a promising future application. The use of Additive Manufacturing for assembly groups is mostly not reasonable, however combining it with conventional manufacturing processes can lead to new opportunities. This paper works out concepts to join, by using similar material combinations, an injection molded part with an additive deposited geometry by the Fused-Deposition-Modeling (FDM) process. Therefore, two of the main industrially used polymers, acrylonitrile butadiene styrene (ABS) and polypropylene (PP), are selected for further study. In particular, this investigation focuses on the procedural potentials and surface preparation of the injection molded part. By the variation of adhesive bonding, the fusion of similar materials can be identified and tested in several series of testing. First in general a direct joining function by the FLM process will be tested. After proving this hypothesis, the results will be summarised in a recommendation of joining similar materials, which are manufactured in different ways.
SP  - 819
EP  - 828
JF  - Proceedings of the Design Society: International Conference on Engineering Design
VL  - 1
IS  - 1
PB  - 
DO  - 10.1017/dsi.2019.86
ER  - 

TY  - NA
AU  - Babic, Teo; Perteneder, Florian; Reiterer, Harald; Haller, Michael J.
TI  - CHI Extended Abstracts - Simo: Interactions with Distant Displays by Smartphones with Simultaneous Face and World Tracking
PY  - 2020
AB  - The interaction with distant displays often demands complex, multi-modal inputs which need to be achieved with a very simple hardware solution so that users can perform rich inputs wherever they encounter a distant display. We present Simo, a novel approach, that transforms a regular smartphone into a highly-expressive user motion tracking device and controller for distant displays. Both the front and back cameras of the smartphone are used simultaneously to track the user's hand as well as the head, and body movements in real-world space and scale. In this work, we first define the possibilities for simultaneous face- and world-tracking using current off-the-shelf smartphones. Next, we present the implementation of a smartphone app enabling hand, head, and body motion tracking. Finally, we present a technical analysis outlining the possible tracking range.
SP  - 1
EP  - 12
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382962
ER  - 

TY  - NA
AU  - Villarreal-Narvaez, Santiago; Vanderdonckt, Jean; Vatavu, Radu-Daniel; Wobbrock, Jacob O.
TI  - Conference on Designing Interactive Systems - A Systematic Review of Gesture Elicitation Studies: What Can We Learn from 216 Studies?
PY  - 2020
AB  - Gesture elicitation studies represent a popular and resourceful method in HCI to inform the design of intuitive gesture commands, reflective of end-users' behavior, for controlling all kinds of interactive devices, applications, and systems. In the last ten years, an impressive body of work has been published on this topic, disseminating useful design knowledge regarding users' preferences for finger, hand, wrist, arm, head, leg, foot, and whole-body gestures. In this paper, we deliver a systematic literature review of this large body of work by summarizing the characteristics and findings ofN=216gesture elicitation studies subsuming 5,458 participants, 3,625 referents, and 148,340 elicited gestures. We highlight the descriptive, comparative, and generative virtues of our examination to provide practitioners with an effective method to (i) understand how new gesture elicitation studies position in the literature; (ii) compare studies from different authors; and (iii) identify opportunities for new research. We make our large corpus of papers accessible online as a Zotero group library at https://www.zotero.org/groups/2132650/gesture_elicitation_studies.
SP  - 855
EP  - 872
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395511
ER  - 

TY  - NA
AU  - Harley, Daniel; Verni, Alexander; Willis, Mackenzie; Ng, Ashley; Bozzo, Lucas; Mazalek, Ali
TI  - Tangible and Embedded Interaction - Sensory VR: Smelling, Touching, and Eating Virtual Reality
PY  - 2018
AB  - We present two proof of concept sensory experiences designed for virtual reality (VR). Our experiences bring together smell, sound, taste, touch, and sight, focusing on low-cost, non-digital materials and on passive interactions. We also contribute a design rationale and a review of sensory interactions, particularly those designed for VR. We argue that current sensory experiences designed for VR often lack a broader consideration of the senses, especially in their neglect of the non-digital. We discuss some implications of non-digital design for sensory VR, suggesting that there may be opportunities to expand conceptions of what sensory design in VR can be.
SP  - 386
EP  - 397
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173241
ER  - 

TY  - NA
AU  - Yu, Difeng; Lu, Xueshi; Shi, Rongkai; Liang, Hai-Ning; Dingler, Tilman; Velloso, Eduardo; Goncalves, Jorge
TI  - CHI - Gaze-Supported 3D Object Manipulation in Virtual Reality
PY  - 2021
AB  - This paper investigates integration, coordination, and transition strategies of gaze and hand input for 3D object manipulation in VR. Specifically, this work aims to understand whether incorporating gaze input can benefit VR object manipulation tasks, and how it should be combined with hand input for improved usability and efficiency. We designed four gaze-supported techniques that leverage different combination strategies for object manipulation and evaluated them in two user studies. Overall, we show that gaze did not offer significant performance benefits for transforming objects in the primary working space, where all objects were located in front of the user and within the arm-reach distance, but can be useful for a larger environment with distant targets. We further offer insights regarding combination strategies of gaze and hand input, and derive implications that can help guide the design of future VR systems that incorporate gaze input for 3D object manipulation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445343
ER  - 

TY  - NA
AU  - Alt, Florian; Torma, Sarah; Buschek, Daniel
TI  - PerDis - Don't disturb me: understanding secondary tasks on public displays
PY  - 2016
AB  - A growing number of displays provide information and applications in public spaces. Most applications today are considered to pose one task to the user, such as navigating a map. In contrast to such primary tasks, secondary tasks have yet received little attention in research, despite practical relevance. For example, a secondary task might occur by displaying special ticket offers to a tourist browsing a city map for attractions. This paper investigates secondary tasks with two key-contributions: First, we describe a design space for secondary tasks on public displays, identifying dimensions of interest to application designers. Second, we present a user study with text entry and mental arithmetic tasks to assess how secondary tasks influence performance in the primary task depending on two main dimensions -- difficulty and temporal integration. We report performance (completion times, error rates) and subjective user ratings, such as distraction and frustration. Analysis of gaze data suggests three main strategies of how users switch between primary and secondary tasks. Based on our findings, we conclude with recommendations for designing apps with secondary tasks on public displays.
SP  - 1
EP  - 12
JF  - Proceedings of the 5th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2914920.2915023
ER  - 

TY  - BOOK
AU  - Mariani, Ilaria; Mattiassi, Alan D. A.
TI  - GHItaly@AVI - Things from another world. vr, ui and ux through run of mydan
PY  - 2018
AB  - NA
SP  - 1
EP  - 6
JF  - NA
VL  - 2246
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Han, Ping-Hsuan; Chen, Yang-Sheng; Yang, Kai-Ti; Chuan, Wei-Shu; Chang, Yu-Tong; Yang, Tin-Ming; Lin, Jia-Yan; Lee, Kong-Chang; Hsieh, Chiao-En; Lee, Lai-Chung; Chou, Chien-Hsing; Hung, Yi-Ping
TI  - BoEs: attachable haptics bits on gaming controller for designing interactive gameplay
PY  - 2017
AB  - Gaming controller is a general way for playing in the immersive virtual reality (VR) game. With haptics technology and head-mounted display (HMD), user can have an enhanced immersive experience when they manipulate in the virtual environment. Currently, the VR controller only provide vibration feedback when player interact with the virtual object. For enhancing the touch experience, adding multiple tactile sensations is one of the quick and useful methods for designing interactive gameplay with haptics. Additionally, the designer need a rapidly prototyped and demonstration to validate their gameplay, which also is a key for designing haptic experience. In this paper, we present Bits of Elements (BoEs), an attachable module system on gaming controller, which allow the game designer to rapidly prototype their VR controller with tactile sensations, such as wind, wet, heat and motion feedback. Therefore, the game designer can explore the interactive gameplay with haptics in VR by replacement and combination of several modules.
SP  - 3
EP  - NA
JF  - SIGGRAPH Asia 2017 VR Showcase
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3139468.3139474
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Baier, Anita; Henze, Niels; Alt, Florian; Bulling, Andreas
TI  - CHI - Understanding Face and Eye Visibility in Front-Facing Cameras of Smartphones used in the Wild
PY  - 2018
AB  - Commodity mobile devices are now equipped with high-resolution front-facing cameras, allowing applications in biometrics (e.g., FaceID in the iPhone X), facial expression analysis, or gaze interaction. However, it is unknown how often users hold devices in a way that allows capturing their face or eyes, and how this impacts detection accuracy. We collected 25,726 in-the-wild photos, taken from the front-facing camera of smartphones as well as associated application usage logs. We found that the full face is visible about 29% of the time, and that in most cases the face is only partially visible. Furthermore, we identified an influence of users' current activity; for example, when watching videos, the eyes but not the entire face are visible 75% of the time in our dataset. We found that a state-of-the-art face detection algorithm performs poorly against photos taken from front-facing cameras. We discuss how these findings impact mobile applications that leverage face and eye detection, and derive practical implications to address state-of-the art's limitations.
SP  - 280
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173854
ER  - 

TY  - NA
AU  - Weigel, Martin; Schön, Oliver; Janssen, Herbert
TI  - UbiComp/ISWC Adjunct - Evaluation of body-worn FPCBs with bluetooth low energy, capacitive touch, and resistive flex sensing
PY  - 2020
AB  - Commercially available flexible printed circuit boards (FPCBs) have the potential to embed electronics, connectivity, and interactivity into the same surface. This makes them an ideal platform for untethered and interactive wearable devices. However, we lack an understanding how well FPCB-based antennas and sensors perform when worn directly on the body. This work contributes an understanding by studying body-worn FPCBs in three technical evaluations: First, we study the integration of Bluetooth Low Energy and compare the signal strength of our body-worn FPCB with a rigid BLE developer board. Second, we study the accuracy of capacitive touch sensing with two electrode sizes. Finally, we develop a resistive flex sensor based on commercially available FPCB materials and compare its accuracy with a state-of-the-art flex sensor. Taken together, our results demonstrate a high usability of FPCB-based wearable devices.
SP  - 147
EP  - 150
JF  - Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410530.3414380
ER  - 

TY  - JOUR
AU  - Ikematsu, Kaori; Yamanaka, Shota
TI  - ScraTouch: Extending Interaction Technique Using Fingernail on Unmodified Capacitive Touch Surfaces
PY  - 2020
AB  - We present ScraTouch, an interaction technique using fingernails, as a new input modality by leveraging capacitive touch sensing. Differentiating between fingertip and fingernail touches requires only tens of milliseconds worth of shunt current data from unmodified capacitive touch surfaces, thus requires no hardware modification. ScraTouch is simple but practical technique for command invocation and mode switching. An evaluation using a point-and-select task on a touchpad showed that although the switching between the finger and nail in ScraTouch required a little more time compared with the baseline (finger touching without mode switching), in overall the operations, ScraTouch was just as fast as the baseline, and on average, 29 % faster than a long press with 500-ms threshold. We also confirmed that setting a simple threshold on the measured shunt current for recognition works robustly across users (97 % accuracy).
SP  - 81
EP  - 19
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1145/3411831
ER  - 

TY  - NA
AU  - Okamoto, Masahiro; Murao, Kazuya
TI  - ISWC - Estimating Upper Arm sEMG from Wrist PPG
PY  - 2021
AB  - The surface electromyogram (sEMG) involves the acquisition of muscle-action potentials transmitted by volume conduction from the skin. Surface electrodes require disposable conductive gel or adhesive tape to be attached to the skin, which is costly, and the tape may damage the skin when it is removed. This paper proposes a method for recognizing the muscle-activity state of the arm and a method for estimating sEMG using pulse-wave data (photoplethysmography). From an evaluation experiment with five participants, three types of muscle activity were recognized with 75+% accuracy and sEMG was estimated with approximately 20% error rate.
SP  - 147
EP  - 149
JF  - 2021 International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460421.3480430
ER  - 

TY  - CHAP
AU  - Pianpak, Poom; Son, Tran Cao; Toups, Zachary O.
TI  - PRIMA - A Multi-agent Simulator Environment Based on the Robot Operating System for Human-Robot Interaction Applications.
PY  - 2018
AB  - This paper describes a simulator environment for humans to direct a team of independent drones by allowing humans to issue high-level goals to the teams or drones. Given a goal, the environment will generate plans for the drones and monitor their execution while attending to humans requests (e.g., aborting a goal, introducing a new goal). For this reason, the environment includes two specific modules, a planning module and an execution and monitoring module, besides the modules for simulation and control of drones. The environment is implemented on the Robot Operating System (ROS), a well-known framework for the development of robotic applications, that facilitates the communication between its components. Experiments are included to highlight the applicability of the environment.
SP  - 612
EP  - 620
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-03098-8_48
ER  - 

TY  - JOUR
AU  - Kang, Jiheong; Son, Dong Hee; Vardoulis, Orestis; Mun, Jaewan; Matsuhisa, Naoji; Kim, Yeongin; Kim, Jaemin; Tok, Jeffrey B.-H.; Bao, Zhenan
TI  - Modular and Reconfigurable Stretchable Electronic Systems
PY  - 2018
AB  - NA
SP  - 1800417
EP  - NA
JF  - Advanced Materials Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1002/admt.201800417
ER  - 

TY  - NA
AU  - Hibschman, Joshua; Zhang, Haoqi
TI  - UIST - Telescope: Fine-Tuned Discovery of Interactive Web UI Feature Implementation
PY  - 2016
AB  - Professional websites contain rich interactive features that developers can learn from, yet understanding their implementation remains a challenge due to the nature of unfamiliar code. Existing tools provide affordances to analyze source code, but feature-rich websites reveal tens of thousands of lines of code and can easily overwhelm the user. We thus present Telescope, a platform for discovering how JavaScript and HTML support a website interaction. Telescope helps users understand unfamiliar website code through a composite view they control by adjusting JavaScript detail, scoping the runtime timeline, and triggering relational links between JS, HTML, and website components. To support these affordances on the open web, Telescope instruments the JavaScript in a website without request intercepts using a novel sleight-of-hand technique, then watches for traces emitted from the website. In a case study across seven popular websites, Telescope helped identify less than 150 lines of front-end code out of tens of thousands that accurately describe the desired interaction in six of the sites. In an exploratory user study, we observed users identifying difficult programming concepts by developing strategies to analyze relatively small amounts of unfamiliar website source code with Telescope.
SP  - 233
EP  - 245
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984570
ER  - 

TY  - JOUR
AU  - Mehta, Kanishk; Peeketi, Akhil Reddy; Liu, Ling; Broer, Dirk J.; Onck, Patrick; Annabattula, Ratna Kumar
TI  - Design and applications of light responsive liquid crystal polymer thin films
PY  - 2020
AB  - Responding to external stimuli is a trait observed in all living organisms. Modern artificial materials have adopted this profound characteristic, thereby commencing the field of stimuli–responsive systems. Liquid crystal polymers are attractive members of this family of systems owing to the available control on their anisotropic properties capable of generating complex morphologies under external stimuli. Liquid crystal polymer systems have been designed to respond to various stimuli such as heat, light, pH, humidity, and electric and magnetic fields. The attainable shapes and topographies open exciting possibilities for novel applications in a wide range of different fields such as microfluidics, artificial muscles, haptics, and optical functions. The microstructural design of liquid crystal polymers leading to diverse applications is the focus of this review. We conclude by presenting the future prospects and developments in these promising material systems.
SP  - 041306
EP  - NA
JF  - Applied Physics Reviews
VL  - 7
IS  - 4
PB  - 
DO  - 10.1063/5.0014619
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Hedayati, Hooman; Zheng, Clement; Bohn, James L.; Szafir, Daniel; Yi-Luen, Ellen; Gross, Mark D.; Leithinger, Daniel
TI  - CHI - RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots
PY  - 2020
AB  - RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376523
ER  - 

TY  - CHAP
AU  - Tseng, Juin-Ling; Chu, Chia-Wei
TI  - Interaction Design in Virtual Reality Game Using Arduino Sensors
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Simulation and Gaming
VL  - NA
IS  - NA
PB  - 
DO  - 10.5772/intechopen.71016
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Hsiu, Min-Chieh; Hsiao, Jui-Chun; Huang, Lee-Ting; Chen, Mike Y.; Hung, Yi-Ping
TI  - MobileHCI Adjunct - TouchRing: subtle and always-available input using a multi-touch ring
PY  - 2016
AB  - We propose a finger-worn touch device TouchRing to provide subtle and multi-touch input. TouchRing leverages printed electrodes and the capacitive sensing technique to detect touch input. It allows users to perform multi-touch gestures in one hand to increase input modality. TouchRing worn on the index finger allows multi-touch using the thumb and middle finger. Ten multi-touch gestures are designed in this paper. We also propose touch detection and gesture recognition approaches in TouchRing. Gesture Recognition accuracy is evaluated in the user study. Applications for TouchRing are also proposed to make controlling smart glasses more convenient.
SP  - 891
EP  - 898
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2957265.2961860
ER  - 

TY  - NA
AU  - Henderson, Jay; Ceha, Jessy; Lank, Edward
TI  - MobileHCI - STAT: Subtle Typing Around the Thigh for Head-Mounted Displays
PY  - 2020
AB  - In head-mounted display (HMD) interaction, text entry is frequently supported via some form of virtual touch, controller, or ray casting keyboard. While these options effectively support text entry, they often incur costs of additional external hardware, awkward movements, and hand encumbrance. We propose STAT, a low-cost, mobile, touch typing technique that leverages a smartphone screen located at the thigh, to support both tap and word gesture text input for HMDs. Through a controlled laboratory study, we explore the efficacy of our technique – including a comparison of typing in and out of an enclosed pocket – and present design recommendations for the opportunistic use of a personal touchscreen device positioned at a user’s thigh for HMD text entry.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403549
ER  - 

TY  - NA
AU  - Meng, Wang; Lei, Kehua; Li, Zhichun; Mi, Haipeng; Xu, Ying-Qing
TI  - Tangible and Embedded Interaction - TwistBlocks: Pluggable and Twistable Modular TUI for Armature Interaction in 3D Design
PY  - 2018
AB  - The use of armatures is a convenient way of deforming and animating 3D digital models. However, interact with an armature is usually time-consuming, and often requires professional skills. Tangible interfaces, such as building blocks, while having improved the accessibility of digital construction, are still lacking in flexibility and present difficulties in dealing with curved armatures. This paper introduces TwistBlocks, a pluggable and twistable modular TUI that improves the accessibility of 3D modeling and animating by physical armature interaction. TwistBlocks is capable of creating complex armatures with dense branches, and supports a high DOF (Degree of Freedom) in physical manipulation. In addition, a set of software tools are provided for novice users to easily create, rig, and animate models. The global-posture sensing network sensing scheme can also measure the rotation and movement of the physical armature, and enables interaction between multiple models.
SP  - 19
EP  - 26
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173231
ER  - 

TY  - JOUR
AU  - Coppers, Sven; Luyten, Kris; Vanacken, Davy; Navarre, David; Palanque, Philippe; Gris, Christine
TI  - Fortunettes: Feedforward about the Future State of GUI Widgets
PY  - 2019
AB  - Feedback is commonly used to explain what happened in an interface. What if questions, on the other hand, remain mostly unanswered. In this paper, we present the concept of enhanced widgets capable of visualizing their future state, which helps users to understand what will happen without committing to an action. We describe two approaches to extend GUI toolkits to support widget-level feedforward, and illustrate the usefulness of widget-level feedforward in a standardized interface to control the weather radar in commercial aircraft. In our evaluation, we found that users require less clicks to achieve tasks and are more confident about their actions when feedforward information was available. These findings suggest that widget-level feedforward is highly suitable in applications the user is unfamiliar with, or when high confidence is desirable.
SP  - 20
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - EICS
PB  - 
DO  - 10.1145/3331162
ER  - 

TY  - CHAP
AU  - Majaranta, Päivi; Räihä, Kari-Jouko; Hyrskykari, Aulikki; Špakov, Oleg
TI  - Eye Movements and Human-Computer Interaction
PY  - 2019
AB  - Gaze provides an attractive input channel for human-computer interaction because of its capability to convey the focus of interest. Gaze input allows people with severe disabilities to communicate with eyes alone. The advances in eye tracking technology and its reduced cost make it an increasingly interesting option to be added to the conventional modalities in every day applications. For example, gaze-aware games can enhance the gaming experience by providing timely effects at the right location, knowing exactly where the player is focusing at each moment. However, using gaze both as a viewing organ as well as a control method poses some challenges. In this chapter, we will give an introduction to using gaze as an input method. We will show how to use gaze as an explicit control method and how to exploit it subtly in the background as an additional information channel. We will summarize research on the application of different types of eye movements in interaction and present research-based design guidelines for coping with typical challenges. We will also discuss the role of gaze in multimodal, pervasive and mobile interfaces and contemplate with ideas for future developments.
SP  - 971
EP  - 1015
JF  - Eye Movement Research
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-20085-5_23
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Schuster, Nora; George, Ceenu; Pfeiffer, Max
TI  - VRST - ElectroCutscenes: Realistic Haptic Feedback in Cutscenes of Virtual Reality Games Using Electric Muscle Stimulation
PY  - 2019
AB  - Cutscenes in Virtual Reality (VR) games enhance story telling by delivering output in the form of visual, auditory, or haptic feedback (e.g., using vibrating handheld controllers). Since they lack interaction in the form of user input, cutscenes would significantly benefit from improved feedback. We introduce the concept and implementation of ElectroCutscenes, where Electric Muscle Stimulation (EMS) is leveraged to elicit physical user movements to different body parts to correspond to those of personal avatars in cutscenes of VR games while the user stays passive. Through a user study (N=22) in which users passively received kinesthetic feedback resulting in involuntarily movements, we show that ElectroCutscenes significantly increases perceived presence and realism compared to controller-based vibrotactile and no haptic feedback. Furthermore, we found preliminary evidence that combining visual and EMS feedback can evoke movements that are not actuated by either of them alone. We discuss how to enhance realism and presence of cutscenes in VR games even when EMS can partially rather than completely actuate the desired body movements.
SP  - NA
EP  - NA
JF  - 25th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359996.3364250
ER  - 

TY  - NA
AU  - Lafreniere, Benjamin; Grossman, Tovi; Anderson, Fraser; Matejka, Justin; Kerrick, Heather; Nagy, Danil; Vasey, Lauren; Atherton, Evan; Beirne, Nicholas; Coelho, Marcelo; Cote, Nicholas; Li, Steven; Nogueira, Andy; Nguyen, Long; Schwinn, Tobias; Stoddart, James; Thomasson, David; Wang, Ray; White, Thomas; Benjamin, David; Conti, Maurice Ugo; Menges, Achim; Fitzmaurice, George
TI  - Crowdsourced Fabrication
PY  - 2016
AB  - In recent years, extensive research in the HCI literature has explored interactive techniques for digital fabrication. However, little attention in this body of work has examined how to involve and guide human workers in fabricating larger-scale structures. We propose a novel model of crowdsourced fabrication, in which a large number of workers and volunteers are guided through the process of building a pre-designed structure. The process is facilitated by an intelligent construction space capable of guiding individual workers and coordinating the overall build process. More specifically, we explore the use of smartwatches, indoor location sensing, and instrumented construction materials to provide real-time guidance to workers, coordinated by a foreman engine that manages the overall build process. We report on a three day deployment of our system to construct a 12-tall bamboo pavilion with assistance from more than one hundred volunteer workers, and reflect on observations and feedback collected during the exhibit.
SP  - 15
EP  - 28
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984553
ER  - 

TY  - NA
AU  - Cheng, Lung-Pan; Ofek, Eyal; Holz, Christian; Benko, Hrvoje; Wilson, Andrew D.
TI  - CHI - Sparse Haptic Proxy: Touch Feedback in Virtual Environments Using a General Passive Prop
PY  - 2017
AB  - We propose a class of passive haptics that we call Sparse Haptic Proxy: a set of geometric primitives that simulate touch feedback in elaborate virtual reality scenes. Unlike previous passive haptics that replicate the virtual environment in physical space, a Sparse Haptic Proxy simulates a scene's detailed geometry by redirecting the user's hand to a matching primitive of the proxy. To bridge the divergence of the scene from the proxy, we augment an existing Haptic Retargeting technique with an on-the-fly target remapping: We predict users' intentions during interaction in the virtual space by analyzing their gaze and hand motions, and consequently redirect their hand to a matching part of the proxy. We conducted three user studies on haptic retargeting technique and implemented a system from three main results: 1) The maximum angle participants found acceptable for retargeting their hand is 40°, with an average rating of 4.6 out of 5. 2) Tracking participants' eye gaze reliably predicts their touch intentions (97.5%), even while simultaneously manipulating the user's hand-eye coordination for retargeting. 3) Participants preferred minimized retargeting distances over better-matching surfaces of our Sparse Haptic Proxy when receiving haptic feedback for single-finger touch input. We demonstrate our system with two virtual scenes: a flight cockpit and a room quest game. While their scene geometries differ substantially, both use the same sparse haptic proxy to provide haptic feedback to the user during task completion.
SP  - 3718
EP  - 3728
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025753
ER  - 

TY  - NA
AU  - Rietzler, Michael; Geiselhart, Florian; Frommel, Julian; Rukzio, Enrico
TI  - CHI - Conveying the Perception of Kinesthetic Feedback in Virtual Reality using State-of-the-Art Hardware
PY  - 2018
AB  - Including haptic feedback in current consumer VR applications is frequently challenging, since technical possibilities to create haptic feedback in consumer-grade VR are limited. While most systems include and make use of the possibility to create tactile feedback through vibration, kinesthetic feedback systems almost exclusively rely on external mechanical hardware to induce actual sensations so far. In this paper, we describe an approach to create a feeling of such sensations by using unmodified off-the-shelf hardware and a software solution for a multi-modal pseudo-haptics approach. We first explore this design space by applying user-elicited methods, and afterwards evaluate our refined solution in a user study. The results show that it is indeed possible to communicate kinesthetic feedback by visual and tactile cues only and even induce its perception. While visual clipping was generally unappreciated, our approach led to significant increases of enjoyment and presence.
SP  - 460
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174034
ER  - 

TY  - BOOK
AU  - Zhao, Yiwei; Kim, Lawrence H.; Wang, Ye; Le Goc, Mathieu; Follmer, Sean
TI  - ISS - Robotic Assembly of Haptic Proxy Objects for Tangible Interaction and Virtual Reality
PY  - 2017
AB  - Passive haptic proxy objects allow for rich tangible interaction, and this is especially true in VR applications. However, this requires users to have many physical objects at hand. Our paper proposes robotic assembly at run-time of low-resolution haptic proxies for tangible interaction and virtual reality. These assembled physical proxy objects are composed of magnetically attached blocks which are assembled by a small multi robot system, specifically Zooids. We explore the design of the basic building blocks and illustrate two approaches to assembling physical proxies: using multirobot systems to (1) self-assemble into structures and (2) assemble 2.5D structure with passive blocks of various heights. The success rate and completion time are evaluated for both approaches. Finally, we demonstrate the potential of assembled proxy objects for tangible interaction and virtual reality through a set of demonstrations.
SP  - 82
EP  - 91
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134143
ER  - 

TY  - NA
AU  - Dementyev, Artem; Qil, Jie; Ou, Jifei; Paradiso, Joseph A.
TI  - IROS - Mass Manufacturing of Self-Actuating Robots: Integrating Sensors and Actuators Using Flexible Electronics
PY  - 2018
AB  - Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.
SP  - 6099
EP  - 6104
JF  - 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iros.2018.8593631
ER  - 

TY  - NA
AU  - Tu, Huawei; Huang, Susu; Yuan, Jiabin; Ren, Xiangshi; Tian, Feng
TI  - CHI - Crossing-Based Selection with Virtual Reality Head-Mounted Displays
PY  - 2019
AB  - This paper presents the first investigation into using the goal-crossing paradigm for object selection with virtual reality (VR) head-mounted displays. Two experiments were carried out to evaluate ray-casting crossing tasks with target discs in 3D space and goal lines on 2D plane respectively in comparison to ray-casting pointing tasks. Five factors, i.e. task difficulty, the direction of movement constraint (collinear vs. orthogonal), the nature of the task (discrete vs. continuous), field of view of VR devices and target depth, were considered in both experiments. Our findings are: (1) crossing generally had shorter or no longer time, and higher or similar accuracy than pointing, indicating crossing can complement or substitute pointing; (2) crossing tasks can be well modelled with Fitts' Law; (3) crossing performance depended on target depth; (4) crossing target discs in 3D space differed from crossing goal lines on 2D plane in many aspects such as time and error performance, the effects of target depth and the parameters of Fitts' models. Based on these findings, we formulate a number of design recommendations for crossing-based interaction in VR.
SP  - 618
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300848
ER  - 

TY  - NA
AU  - Oney, Steve; Krosnick, Rebecca; Brandt, Joel; Myers, Brad A.
TI  - CHI - Implementing Multi-Touch Gestures with Touch Groups and Cross Events
PY  - 2019
AB  - Multi-touch gestures can be very difficult to program correctly because they require that developers build high-level abstractions from low-level touch events. In this paper, we introduce programming primitives that enable programmers to implement multi-touch gestures in a more understandable way by helping them build these abstractions. Our design of these primitives was guided by a formative study, in which we observed developers' natural implementations of custom gestures. Touch groups provide summaries of multiple fingers rather than requiring that programmers track them manually. Cross events allow programmers to summarize the movement of one or a group of fingers. We implemented these two primitives in two environments: a declarative programming system and in a standard imperative programming language. We found that these primitives are capable of defining nuanced multi-touch gestures, which we illustrate through a series of examples. Further, in two user evaluations of these programming primitives, we found that multi-touch behaviors implemented in these programming primitives are more understandable than those implemented with standard touch events.
SP  - 355
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300585
ER  - 

TY  - NA
AU  - Hechuan, Zhang; Zhiyong, Chen; Guo, Shihui; Lin, Juncong; Yating, Shi; Xiangyang, Liu; Ma, Yong
TI  - CHI - Sensock: 3D Foot Reconstruction with Flexible Sensors
PY  - 2020
AB  - Capturing 3D foot models is important for applications such as manufacturing customized shoes and creating clubfoot orthotics. In this paper, we propose a novel prototype, Sensock, to offer a fully wearable solution for the task of 3D foot reconstruction. The prototype consists of four soft stretchable sensors, made from silk fibroin yarn. We identify four characteristic foot girths based on the existing knowledge of foot anatomy, and measure their lengths with the resistance value of the stretchable sensors. A learning-based model is trained offline and maps the foot girths to the corresponding 3D foot shapes. We compare our method with existing solutions using red-green-blue (RGB) or RGBD (RGB-depth) cameras, and show the advantages of our method in terms of both efficiency and accuracy. In the user experiment, we find that the relative error of Sensock is lower than 0.55%. It performs consistently across different trials and is considered comfortable and suitable for long-term wearing.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376387
ER  - 

TY  - NA
AU  - He, Zhenyi; Zhu, Fengyuan; Perlin, Ken; Ma, Xiaojuan
TI  - Manifest the Invisible: Design for Situational Awareness of Physical Environments in Virtual Reality.
PY  - 2018
AB  - Virtual Reality (VR) provides immersive experiences in the virtual world, but it may reduce users' awareness of physical surroundings and cause safety concerns and psychological discomfort. Hence, there is a need of an ambient information design to increase users' situational awareness (SA) of physical elements when they are immersed in VR environment. This is challenging, since there is a tradeoff between the awareness in reality and the interference with users' experience in virtuality. In this paper, we design five representations (indexical, symbolic, and iconic with three emotions) based on two dimensions (vividness and emotion) to address the problem. We conduct an empirical study to evaluate participants' SA, perceived breaks in presence (BIPs), and perceived engagement through VR tasks that require movement in space. Results show that designs with higher vividness evoke more SA, designs that are more consistent with the virtual environment can mitigate the BIP issue, and emotion-evoking designs are more engaging.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Sun, Zhida; Wang, Sitong; Liu, Chengzhong; Ma, Xiaojuan
TI  - Metaphoraction: Support Gesture-based Interaction Design with Metaphorical Meanings
PY  - 2022
AB  - <jats:p> Previous user experience research emphasizes meaning in interaction design beyond conventional interactive gestures. However, existing exemplars that successfully reify abstract meanings through interactions are usually case-specific, and it is currently unclear how to systematically create or extend meanings for general gesture-based interactions. We present Metaphoraction, a creativity support tool that formulates design ideas for gesture-based interactions to show metaphorical meanings with four interconnected components: <jats:italic>gesture</jats:italic> , <jats:italic>action</jats:italic> , <jats:italic>object</jats:italic> , and <jats:italic>meaning</jats:italic> . To represent the interaction design ideas with these four components, Metaphoraction links interactive gestures to actions based on the similarity of appearances, movements, and experiences; relates actions to objects by applying the immediate association; bridges objects and meanings by leveraging the metaphor TARGET-SOURCE mappings. We build a dataset containing 588,770 unique design idea candidates through surveying related research and conducting two crowdsourced studies to support meaningful gesture-based interaction design ideation. Five design experts validate that Metaphoraction can effectively support creativity and productivity during the ideation process. The paper concludes by presenting insights into meaningful gesture-based interaction design and discussing potential future uses of the tool. </jats:p>
SP  - 1
EP  - 33
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 5
PB  - 
DO  - 10.1145/3511892
ER  - 

TY  - NA
AU  - Chang, Steve
TI  - InfraNotes: Inconspicuous Handwritten Trajectory Tracking for Lecture Note Recording with Infrared Sensors.
PY  - 2016
AB  - Lecture notes are important for students to review and understand the key points in the class. Unfortunately, the students often miss or lose part of the lecture notes. In this paper, we design and implement an infrared sensor based system, InfraNotes, to automatically record the notes on the board by sensing and analyzing hand gestures of the lecturer. Compared with existing techniques, our system does not require special accessories with lecturers such as sensor-facilitated pens, writing surfaces or the video-taping infrastructure. Instead, it only has an infrared-sensor module on the eraser holder of black/white board to capture handwritten trajectories. With a lightweight framework for handwritten trajectory processing, clear lecture notes can be generated automatically. We evaluate the quality of lecture notes by three standard character recognition techniques. The results indicate that InfraNotes is a promising solution to create clear and complete lectures to promote the education.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Surale, Hemant Bhaskar; Matulic, Fabrice; Vogel, Daniel
TI  - CHI - Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality
PY  - 2019
AB  - We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard "subtraction method" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.
SP  - 196
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300426
ER  - 

TY  - NA
AU  - Serim, Baris; Klouche, Khalil; Jacucci, Giulio
TI  - Conference on Designing Interactive Systems - Gaze-Adaptive Above and On-Surface Interaction
PY  - 2017
AB  - We explore the combination of above-surface sensing with eye tracking to facilitate concurrent interaction with multiple regions on touch screens. Conventional touch input relies on positional accuracy, thereby requiring tight visual monitoring of one's own motor action. In contrast, above-surface sensing and eye tracking provides information about how user's hands and gaze are distributed across the interface. In these situations we facilitate interaction by 1) showing the visual feedback of the hand hover near user's gaze point and 2) decrease the requisite of positional accuracy by employing gestural information. We contribute input and visual feedback techniques that combine these modalities and demonstrate their use in example applications. A controlled study showed the effectiveness of our techniques for manipulation tasks against conventional touch, while the effectiveness in acquisition tasks depended on the amount of mid-air motion, leading to our conclusion that the techniques can benefit interacting with multiple interface regions.
SP  - 115
EP  - 127
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064744
ER  - 

TY  - JOUR
AU  - Cortes, Carlos A. Tirado; Chen, Hsiang-Ting; Sturnieks, Daina L.; Garcia, Jaime A.; Lord, Stephen R.; Lin, Chin-Teng
TI  - Evaluating Balance Recovery Techniques for Users Wearing Head-Mounted Display in VR
PY  - 2020
AB  - Room-scale 3D position tracking enables users to explore a virtual environment by physically walking, which improves comfort and the level of immersion. However, when users walk with their eyesight blocked by a head-mounted display, they may unexpectedly lose their balance and fall if they bump into real-world obstacles or unintentionally shift their center of mass outside the margin of stability. This paper evaluates balance recovery methods and intervention timing during the use of VR with the assumption that the onset of a fall is given. Our experiment followed the tether-release protocol during clinical research and induced a fall while a subject was engaged in a secondary 3D object selection task. The experiment employed a two-by-two design that evaluated two assistive techniques, i.e., video-see-through and auditory warning at two different timings, i.e., at fall onset and 500ms prior to fall onset. The data from 17 subjects showed that video-see-through triggered 500 ms before the onset of fall can effectively help users recover from falls. Surprisingly, video-see-through at fall onset has a significant negative impact on balance recovery and produces similar results to those of the baseline condition (no intervention).
SP  - 204
EP  - 215
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2019.2927477
ER  - 

TY  - JOUR
AU  - Hsieh, Yi-Ta; Jylhä, Antti; Orso, Valeria; Andolina, Salvatore; Hoggan, Eve; Gamberini, Luciano; Jacucci, Giulio
TI  - Developing hand-worn input and haptic support for real-world target finding
PY  - 2018
AB  - Locating places in cities is typically facilitated by handheld mobile devices, which draw the visual attention of the user on the screen of the device instead of the surroundings. In this research, we aim at strengthening the connection between people and their surroundings through enabling mid-air gestural interaction with real-world landmarks and delivering information through audio to retain users’ visual attention on the scene. Recent research on gesture-based and haptic techniques for such purposes has mainly considered handheld devices that eventually direct users’ attention back to the devices. We contribute a hand-worn, mid-air gestural interaction design with directional vibrotactile guidance for finding points of interest (POIs). Through three design iterations, we address aspects of (1) sensing technologies and the placement of actuators considering users’ instinctive postures, (2) the feasibility of finding and fetching information regarding landmarks without visual feedback, and (3) the benefits of such interaction in a tourist application. In a final evaluation, participants located POIs and fetched information by pointing and following directional guidance, thus realising a vision in which they found and experienced real-world landmarks while keeping their visual attention on the scene. The results show that the interaction technique has comparable performance to a visual baseline, enables high mobility, and facilitates keeping visual attention on the surroundings.
SP  - 117
EP  - 132
JF  - Personal and Ubiquitous Computing
VL  - 23
IS  - 1
PB  - 
DO  - 10.1007/s00779-018-1180-z
ER  - 

TY  - NA
AU  - Matulic, Fabrice; Vogel, Daniel
TI  - CHI - Multiray: Multi-Finger Raycasting for Large Displays
PY  - 2018
AB  - We explore and evaluate a multi-finger raycasting design space that we call "multiray". Each finger projects a ray on to the display, so the user is interacting from a distance using a form of direct input. Specifically, we propose techniques, where patterns of ray intersections created by hand postures form 2D geometric shapes to trigger actions and perform direct manipulations that go beyond single-point selections. Two formative studies examine characteristics of multi-finger raycasting for different projection methods, shapes, and tasks. Based on the results of those investigations, we demonstrate a number of dynamic UI controls and operations that utilise multiray points and shapes.
SP  - 245
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173819
ER  - 

TY  - NA
AU  - Yang, Keng-Ta; Wang, Chiu-Hsuan; Chan, Liwei
TI  - UIST - ShareSpace: Facilitating Shared Use of the Physical Space by both VR Head-Mounted Display and External Users
PY  - 2018
AB  - Currently, "walkable" virtual reality (VR) is achieved by dedicating a room-sized space for VR activities, which is not shared with non-HMD users engaged in their own activities. To achieve the goal of allowing shared use of space for all users while overcoming the obvious difficulty of integrating use with those immersed in a VR experience, we present ShareSpace, a system that allows external users to communicate their needs for physical space to those wearing an HMD and immersed in their VR experience. ShareSpace works by allowing external users to place "shields" in the virtual environment by using a set of physical shield tools. A pad visualizer helps this process by allowing external users to examine the arrangement of virtual shields. We also discuss interaction techniques that minimize the interference between the respective activities of the HMD wearers and the other users of the same physical space. To evaluate our design, a user study was conducted to collect user feedback from participants in four trial scenarios. The results indicate that our ShareSpace system allows users to perform their respective activities with improved engagement and safety. In addition, this study shows that while the HMD users did perceive a considerable degree of interference due to the internal visual indications from the ShareSpace system, they were still more engaged in their VR experience than when interrupted by direct external physical interference initiated by external users.
SP  - 499
EP  - 509
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242630
ER  - 

TY  - BOOK
AU  - Xue, Yaqiong; Weng, Dongdong; Jiang, Haiyan; Gao, Qing
TI  - IGTA - MMRPet: Modular Mixed Reality Pet System Based on Passive Props
PY  - 2019
AB  - We present MMRPet, a modular mixed reality pet system based on passive props. In addition to superimposing virtual pets onto pet entities to take advantages of physical interactions provided by pet entities and personalized appearance and rich expressional capabilities provided by virtual pets, the key idea behind MMRPet is the modular design of pet entities. The user can reconfigure limited modules to construct pet entities of various forms and structures. These modular pet entities can provide flexible haptic feedback and support the system to render virtual pets of personalized form and structure. By integrating tracking information from the head and hands of the user, as well as each module of pet entities, MMRPet can infer rich interaction intents and support rich human-pet interactions when the user touches, moves, rotates or gazes each module. We explore the design space for the construction of modular pet entities and the design space of the human-pet interaction enabled by MMRPet. Furthermore, a series of prototypes demonstrate the advantages of using modular entities in a mixed reality pet system.
SP  - 645
EP  - 658
JF  - Image and Graphics Technologies and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-13-9917-6_61
ER  - 

TY  - NA
AU  - Chan, Justin; Wang, Anran; Iyer, Vikram; Gollakota, Shyamnath
TI  - MobiCom - Surface MIMO: Using Conductive Surfaces For MIMO Between Small Devices
PY  - 2018
AB  - As connected devices continue to decrease in size, we explore the idea of leveraging everyday surfaces such as tabletops and walls to augment the wireless capabilities of devices. Specifically, we introduce Surface MIMO, a technique that enables MIMO communication between small devices via surfaces coated with conductive paint or covered with conductive cloth. These surfaces act as an additional spatial path that enables MIMO capabilities without increasing the physical size of the devices themselves. We provide an extensive characterization of these surfaces that reveal their effect on the propagation of EM waves. Our evaluation shows that we can enable additional spatial streams using the conductive surface and achieve average throughput gains of 2.6-3x for small devices. Finally, we also leverage the wideband characteristics of these conductive surfaces to demonstrate the first Gbps surface communication system that can directly transfer bits through the surface at up to 1.3Gbps.
SP  - 3
EP  - 18
JF  - Proceedings of the 24th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3241539.3241562
ER  - 

TY  - BOOK
AU  - Tokuda, Yutaka; Moya, Jose Luis Berna; Memoli, Gianluca; Neate, Timothy; Sahoo, Deepak Ranjan; Robinson, Simon; Pearson, Jennifer; Jones, Matt; Subramanian, Sriram
TI  - ISS - Programmable Liquid Matter: 2D Shape Deformation of Highly Conductive Liquid Metals in a Dynamic Electric Field
PY  - 2017
AB  - In this paper, we demonstrate a method for the dynamic 2D transformation of liquid matter and present unique organic animations based on spatio-temporally controlled electric fields. In particular, we deploy a droplet of liquid metal (Gallium indium eutectic alloy) in a 7x7 electrode array prototype system, featuring an integrated image tracking system and a simple GUI. Exploiting the strong dependance of EGaIn's surface tension on external electric voltages, we control multiple electrodes dynamically to manipulate the liquid metal into a fine-grained desired shape. Taking advantage of the high conductivity of liquid metals, we introduce a shape changing, reconfigurable smart circuit as an example of unique applications. We discuss system constraints and the overarching challenge of controlling liquid metals in the presence of phenomena such as splitting, self-electrode interference and finger instabilities. Finally, we reflect on the broader vision of this project and discuss our work in the context of the wider scope of programmable materials.
SP  - 142
EP  - 150
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134132
ER  - 

TY  - NA
AU  - Min, Dae-Hong; Lee, Dong-Yong; Cho, Yong-Hun; Lee, In-Kwon
TI  - VR - Shaking Hands in Virtual Space: Recovery in Redirected Walking for Direct Interaction between Two Users
PY  - 2020
AB  - Various studies have been conducted to realize realistic direct interaction in the virtual environment. In this study, we focus on a situation wherein two users using the same physical space explore the same virtual environment using redirected walking (RDW) technology. For two users to meet each other in a virtual environment to realize realistic direct interaction, they must simultaneously meet each other in physical space. However, if the RDW algorithm is applied to each user independently, the relative positions and orientations of the two users can be significantly different in the virtual and physical spaces. We present a recovery algorithm that adjusts the relative position and orientation such that they become the same in the two spaces. Our recovery algorithm uses either modified subtle RDW techniques or overt recovery techniques in three cases depending on the relative position and orientation of the two users. Once the recovered state is reached, the two users can go forward to meet each other and directly interact in the virtual and physical spaces simultaneously. Based on the experiment results, we can confirm that the application of our recovery technology to the system increases the user’s satisfaction in usability and the presence of coexistence in the virtual environment with other users.
SP  - 164
EP  - 173
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581308731108
ER  - 

TY  - CHAP
AU  - Dobosz, Krzysztof; Trzcionkowski, Mirosław
TI  - ICCHP (2) - Text Input with Foot Gestures Using the Myo Armband
PY  - 2020
AB  - This article describes the study in the area of alternative text entry. The aim of the project was to use the Myo band to detect foot gestures using EMG or acceleration signals for the special needs of people with physical disabilities. The Myo controller in the form of a band attached to the leg below the calf is responsible for detecting signals of a change in muscle tension. It can also be applied directly to the foot and measure the change in acceleration caused by the movement of the foot. Myo sends data to the application that uses it to control the virtual keyboard. The research confirmed that foot gestures allowed the user to enter text. The results obtained are comparable to other methods of linear alphabet scanning.
SP  - 335
EP  - 342
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58805-2_39
ER  - 

TY  - NA
AU  - Zhu, Suwen; Kim, Yoonsang; Zheng, Jingjie; Luo, Jennifer Yi; Qin, Ryan; Wang, Liuping; Fan, Xiangmin; Tian, Feng; Bi, Xiaojun
TI  - CHI - Using Bayes' Theorem for Command Input: Principle, Models, and Applications
PY  - 2020
AB  - Entering commands on touchscreens can be noisy, but existing interfaces commonly adopt deterministic principles for deciding targets and often result in errors. Building on prior research of using Bayes' theorem to handle uncertainty in input, this paper formalized Bayes' theorem as a generic guiding principle for deciding targets in command input (referred to as "BayesianCommand"), developed three models for estimating prior and likelihood probabilities, and carried out experiments to demonstrate the effectiveness of this formalization. More specifically, we applied BayesianCommand to improve the input accuracy of (1) point-and-click and (2) word-gesture command input. Our evaluation showed that applying BayesianCommand reduced errors compared to using deterministic principles (by over 26.9% for point-and-click and by 39.9% for word-gesture command input) or applying the principle partially (by over 28.0% and 24.5%).
SP  - 1
EP  - 15
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376771
ER  - 

TY  - NA
AU  - Esteves, Augusto; Quintal, Filipe; Caires, Fabio; Baptista, Vitor; Mendes, Pedro
TI  - exp.at - Wattom: Ambient Eco-feedback with Mid-air Input
PY  - 2019
AB  - This paper presents Wattom, a highly interactive ambient eco-feedback smart plug that aims to promote a more sustainable use of electricity in the home. This paper describes our latest implementation of the Wattom plug, and three system applications. The first enables Wattom to power a connected device, and provide real-time feedback on the amount of electricity in the grid from renewable sources. The second enables users to schedule power events from their smart watches. Finally, the third application uses non-intrusive load monitoring (NILM) to provide users with personal consumption information on multiple devices connected to a single Wattom plug. The paper concludes by presenting insights into the development and use of various iterations of the Wattom plug.
SP  - 12
EP  - 15
JF  - 2019 5th Experiment International Conference (exp.at'19)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/expat.2019.8876565
ER  - 

TY  - NA
AU  - Friedel, Marcus
TI  - HapticLever: Kinematic Force Feedback using a 3D Pantograph
PY  - 2022
AB  - HapticLever is a new kinematic approach for VR haptics which uses a 3D pantograph to stiffly render large-scale surfaces using small-scale proxies. The HapticLever approach does not consume power to render forces, but rather puts a mechanical constraint on the end effector using a small-scale proxy surface. The HapticLever approach provides stiff force feedback when the user interacts with a static virtual surface, but allows the user to move their arm freely when moving through free virtual space. We present the problem space, the related work, and the HapticLever design approach.
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558736
ER  - 

TY  - JOUR
AU  - Reibert, Joshua; Riehmann, Patrick; Froehlich, Bernd
TI  - Multitouch Interaction with Parallel Coordinates on Large Vertical Displays
PY  - 2020
AB  - This paper presents a multitouch vocabulary for interacting with parallel coordinates plots on wall-sized displays. The gesture set relies on principles such as two-finger range definition, a functional distinction of background and foreground for applying the Hold-and-Move concept to wall-sized displays as well as fling-based interaction for triggering and controlling long-range movements. Our implementation demonstrates that out-of-reach problems and limitations regarding multitouch technology and display size can be tackled by the coherent integration of our multitouch gestures. Expert reviews indicate that our gesture vocabulary helps to solve typical analysis tasks that require interaction beyond arms' reach, and it also shows how often certain gestures were used.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427327
ER  - 

TY  - BOOK
AU  - Flad, Nina; Fomina, Tatiana; Buelthoff, Heinrich H.; Chuang, Lewis L.
TI  - ETVIS - Unsupervised Clustering of EOG as a Viable Substitute for Optical Eye Tracking
PY  - 2017
AB  - Eye-movements are typically measured with video cameras and image recognition algorithms. Unfortunately, these systems are susceptible to changes in illumination during measurements. Electrooculography (EOG) is another approach for measuring eye-movements that does not suffer from the same weakness. Here, we introduce and compare two methods that allow us to extract the dwells of our participants from EOG signals under presentation conditions that are too difficult for optical eye tracking. The first method is unsupervised and utilizes density-based clustering. The second method combines the optical eye-tracker’s methods to determine fixations and saccades with unsupervised clustering. Our results show that EOG can serve as a sufficiently precise and robust substitute for optical eye tracking, especially in studies with changing lighting conditions. Moreover, EOG can be recorded alongside electroencephalography (EEG) without additional effort.
SP  - 151
EP  - 167
JF  - Eye Tracking and Visualization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-47024-5_9
ER  - 

TY  - NA
AU  - Honnet, Cedric; Perner-Wilson, Hannah; Teyssier, Marc; Fruchard, Bruno; Steimle, Jürgen; Baptista, Ana Catarina; Strohmeier, Paul
TI  - CHI - PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization
PY  - 2020
AB  - We present a method for enabling arbitrary textiles to sense pressure and deformation: In-situ polymerization supports integration of piezoresistive properties at the material level, preserving a textile's haptic and mechanical characteristics. We demonstrate how to enhance a wide set of fabrics and yarns using only readily available tools. To further support customisation by the designer, we present methods for patterning, as needed to create circuits and sensors, and demonstrate how to combine areas of different conductance in one material. Technical evaluation results demonstrate the performance of sensors created using our method is comparable to off-the-shelf piezoresistive textiles. As application examples, we demonstrate rapid manufacturing of on-body interfaces, tie-dyed motion-capture clothing, and zippers that act as potentiometers.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376841
ER  - 

TY  - JOUR
AU  - Lu, Yiqin; Huang, Bingjian; Yu, Chun; Liu, Guahong; Shi, Yuanchun
TI  - Designing and Evaluating Hand-to-Hand Gestures with Dual Commodity Wrist-Worn Devices
PY  - 2020
AB  - Hand gestures provide a natural and easy-to-use way to input commands. However, few works have studied the design space of bimanual hand gestures or attempted to infer gestures that involve devices on both hands. We explore the design space of hand-to-hand gestures, a group of gestures that are performed by touching one hand with the other hand. Hand-to-hand gestures are easy to perform and provide haptic feedback on both hands. Moreover, hand-to-hand gestures generate simultaneous vibration on two hands that can be sensed by dual off-the-shelf wrist-worn devices. In this work, we derive a hand-to-hand gesture vocabulary with subjective ratings from users and select gesture sets for real-life scenarios. We also take advantage of devices on both wrists to demonstrate their gesture-sensing capability. Our results show that the recognition accuracy for fourteen gestures is 94.6% when the user is stationary, and the accuracy for five gestures is 98.4% or 96.3% when the user is walking or running, respectively. This is significantly more accurate than a single device worn on either wrist. Our further evaluation also validates that users can easily remember hand-to-hand gestures and use our technique to invoke commands in real-life contexts.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3380984
ER  - 

TY  - NA
AU  - Wessely, Michael; Tsandilas, Theophanis; Mackay, Wendy E.
TI  - UIST - Stretchis: Fabricating Highly Stretchable User Interfaces
PY  - 2016
AB  - Recent advances in materials science research allow production of highly stretchable sensors and displays. Such technologies, however, are still not accessible to non-expert makers. We present a novel and inexpensive fabrication method for creating Stretchis, highly stretchable user interfaces that combine sensing capabilities and visual output. We use Polydimethylsiloxan (PDMS) as the base material for a Stretchi and show how to embed stretchable touch and proximity sensors and stretchable electroluminescent displays. Stretchis can be ultra-thin (≈ 200μm), flexible, and fully customizable, enabling non-expert makers to add interaction to elastic physical objects, shape-changing surfaces, fabrics, and the human body. We demonstrate the usefulness of our approach with three application examples that range from ubiquitous computing to wearables and on-skin interaction.
SP  - 697
EP  - 704
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984521
ER  - 

TY  - NA
AU  - Leiva, Luis A.; Martín-Albo, Daniel; Vatavu, Radu-Daniel
TI  - MobileHCI - GATO: predicting human performance with multistroke and multitouch gesture input
PY  - 2018
AB  - We introduce GATO, a human performance analysis technique grounded in the Kinematic Theory that delivers accurate predictions for the expected user production time of stroke gestures of all kinds: unistrokes, multistrokes, multitouch, or combinations thereof. Our experimental results obtained on several public datasets (82 distinct gesture types, 123 participants, a36k gesture samples) show that GATO predicts user-independent gesture production times that correlate rs > .9 with groundtruth, while delivering an average relative error of less than 10% with respect to actual measured times. With its accurate estimations of users' a priori time performance with stroke gesture input, GATO will help researchers to understand better users' gesture articulation patterns on touchscreen devices of all kinds. GATO will also benefit practitioners to inform highly effective gesture set designs.
SP  - 32
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229478
ER  - 

TY  - NA
AU  - Alt, Florian; Ziegler, Lukas
TI  - ACM Multimedia - PD-Survey: Supporting Audience-Centric Research through Surveys on Pervasive Display Networks
PY  - 2017
AB  - We present PD-Survey, a platform to conduct surveys across a network of interactive screens. Our research is motivated by the fact that obtaining and analyzing data about users of public displays requires signi cant effort; e.g., running long-term observations or post-hoc analyses of video/interaction logs. As a result, research is often constrained to a single installation within a particular context, neither accounting for a diverse audience (children, shoppers, commuters) nor for different situations (waiting vs. passing by) or times of the day. As displays become networked, one way to address this challenge is through surveys on displays, where audience feedback is collected insitu. Since current tools do not appropriately address the requirements of a display network, we implemented a tool for use on public displays and report on its design and development. Our research is complemented by two in-the-wild deployments that (a) investigate di erent channels for feedback collection, (b) showcase how the work of researchers is supported, and (c) testify that the platform can easily be extended with novel features.
SP  - 360
EP  - 368
JF  - Proceedings of the 25th ACM international conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123266.3123293
ER  - 

TY  - JOUR
AU  - Van Duong, Quang; Nguyen, Vinh Phu; Santos, Fabrice Domingues Dos; Choi, Seung Tae
TI  - Localized Fretting-Vibrotactile Sensations for Large-Area Displays.
PY  - 2019
AB  - Tactile perception in large-area displays is currently attracting substantial research attention since, in conjunction with visible and auditory sensations, it provides more immersive and realistic interactions with displayed contents. Here, a new vibrotactile display based on the fretting phenomenon is developed for the first time to provide localized tactile feedback on a large-area display. Normal pressure by a human fingertip activates a locally concentrated electric field in a relaxor ferroelectric polymer (RFP) film under the contact area, which produces a localized electrostrictive strain. The synergistic interplay among the localized electric field, electrostrictive deformation of the RFP film, and contact area dramatically amplifies acoustic vibrations near the contact edge of a human fingertip. A blend of poly(vinylidene fluoride-trifluoroethylene-chlorofluoroethylene) terpolymer and poly(vinylidene fluoride-trifluoroethylene) (55:45) copolymer is proposed for the RFP to provide an enhanced actuation performance even at elevated temperatures. The fretting-vibrotactile mechanism has several interesting properties, such as tactile feedback on a stationary fingertip, pressure-responsive simple on-off mechanism, multitouch interaction, excellent transparency, and easy integration with capacitive or resistive touch sensors and friction-based haptic-feedback mechanisms. An array of RFP film vibrators can provide addressable content-related multiple tactile feedback on large-area displays by modulating the frequency, amplitude, and profile of the driving voltage signals.
SP  - 33292
EP  - 33301
JF  - ACS applied materials & interfaces
VL  - 11
IS  - 36
PB  - 
DO  - 10.1021/acsami.9b09691
ER  - 

TY  - JOUR
AU  - Cho, Yun-Sik; Kang, Jiewon; Jeon, Jaekyung; Park, Jongchan; Kim, Mingyu; Kim, Jinmo
TI  - X‐person asymmetric interaction in virtual and augmented realities
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - Computer Animation and Virtual Worlds
VL  - 32
IS  - 5
PB  - 
DO  - 10.1002/cav.1985
ER  - 

TY  - NA
AU  - Henley, Austin Z.; Fleming, Scott D.
TI  - VL/HCC - Yestercode: Improving code-change support in visual dataflow programming environments
PY  - 2016
AB  - In this paper, we present the Yestercode tool for supporting code changes in visual dataflow programming environments. In a formative investigation of LabVIEW programmers, we found that making code changes posed a significant challenge. To address this issue, we designed Yestercode to enable the efficient recording, retrieval, and juxtaposition of visual dataflow code while making code changes. To evaluate Yestercode, we implemented our design as a prototype extension to the LabVIEW programming environment, and ran a user study involving 14 professional LabVIEW programmers that compared Yestercode-extended LabVIEW to the standard LabVIEW IDE. Our results showed that Yestercode users introduced fewer bugs during tasks, completed tasks in about the same time, and experienced lower cognitive loads on tasks. Moreover, participants generally reported that Yestercode was easy to use and that it helped in making change tasks easier.
SP  - 106
EP  - 114
JF  - 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2016.7739672
ER  - 

TY  - JOUR
AU  - Bajammal, Mohammad; Stocco, Andrea; Mazinanian, Davood; Mesbah, Ali
TI  - A Survey on the Use of Computer Vision to Improve Software Engineering Tasks
PY  - 2022
AB  - NA
SP  - 1722
EP  - 1742
JF  - IEEE Transactions on Software Engineering
VL  - 48
IS  - 5
PB  - 
DO  - 10.1109/tse.2020.3032986
ER  - 

TY  - BOOK
AU  - Lohse, Andreas L.; Kjar, Christoffer K.; Hamulic, Ervin; Lima, Ingrid G. A.; Jensen, Tilde H.; Bruni, Luis Emilio; Nilsson, Niels Christian
TI  - WEVR@VR - Leveraging Change Blindness for Haptic Remapping in Virtual Environments
PY  - 2019
AB  - Passive haptic feedback provides an inexpensive and convenient approach to virtual touch. However, this approach requires that all virtual objects represented by physical props. In this paper we present change blindness haptic remapping—a novel approach that leverages change blindness to map two or more virtual objects onto a single physical prop. We describe a preliminary evaluation comparing the proposed approach to a control condition where all virtual objects where mapped to physical props. The study revealed no notable differences in terms of the participants’ experience and less than one fourth of the participants noticed the manipulation. However, the participants did perform interaction errors when exposed to haptic remapping. Based on the findings, we discuss improvements to the proposed approach and potential directions for future work.
SP  - 8809587
EP  - NA
JF  - 2019 IEEE 5th Workshop on Everyday Virtual Reality (WEVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wevr.2019.8809587
ER  - 

TY  - NA
AU  - He, Zhenyi; Zhu, Fengyuan; Perlin, Ken
TI  - PhyShare: Sharing Physical Interaction in Virtual Reality
PY  - 2017
AB  - We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chen, Yan; Lee, Sang Won; Oney, Steve
TI  - CHI - CoCapture: Effectively Communicating UI Behaviors on Existing Websites by Demonstrating and Remixing
PY  - 2021
AB  - User Interface (UI) mockups are commonly used as shared context during interface development collaboration. In practice, UI designers often use screenshots and sketches to create mockups of desired UI behaviors for communication. However, in the later stages of UI development, interfaces can be arbitrarily complex, making it labor-intensive to sketch, and static screenshots are limited in the types of interactive and dynamic behaviors they can express. We introduce CoCapture, a system that allows designers to easily create UI behavior mockups on existing web interfaces by demonstrating and remixing, and to accurately describe their requests to helpers by referencing the resulting mockups using hypertext. We showed that participants could more accurately describe UI behaviors with CoCapture than with existing sketch and communication tools and that the resulting descriptions were clear and easy to follow. Our approach can help teams develop UIs efficiently by bridging communication gaps with more accurate visual context.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445573
ER  - 

TY  - NA
AU  - Ens, Barrett; Quigley, Aaron; Yeo, Hui-Shyong; Irani, Pourang; Piumsomboon, Thammathip; Billinghurst, Mark
TI  - CHI Extended Abstracts - Counterpoint: Exploring Mixed-Scale Gesture Interaction for AR Applications
PY  - 2018
AB  - This paper presents ongoing work on a design exploration for mixed-scale gestures, which interleave microgestures with larger gestures for computer interaction. We describe three prototype applications that show various facets of this multi-dimensional design space. These applications portray various tasks on a Hololens Augmented Reality display, using different combinations of wearable sensors. Future work toward expanding the design space and exploration is discussed, along with plans toward evaluation of mixed-scale gesture design.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188513
ER  - 

TY  - NA
AU  - Kang, Seokbin; Norooz, Leyla; Bonsignore, Elizabeth; Byrne, Virginia L.; Clegg, Tamara; Froehlich, Jon E.
TI  - IDC - PrototypAR: Prototyping and Simulating Complex Systems with Paper Craft and Augmented Reality
PY  - 2019
AB  - We introduce PrototypAR, an Augmented Reality (AR) system that allows children to rapidly build complex systems using paper crafts and to test their designs in a digital environment. PrototypAR combines lo-fidelity prototyping to facilitate iterative design, real-time AR feedback to scaffold learning, and a virtual simulation environment to support personalized experiments. Informed by three participatory design sessions, we developed three PrototypAR applications: build-a-bike, build-a-camera, and build-an-aquarium---each highlights different aspects of our system. To evaluate PrototypAR, we conducted four single-session qualitative evaluations with 21 children working in teams. Our findings show how children build and explore complex systems models, how they use AR scaffolds, and the challenges they face when conducting experiments with their own prototypes.
SP  - 253
EP  - 266
JF  - Proceedings of the 18th ACM International Conference on Interaction Design and Children
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311927.3323135
ER  - 

TY  - JOUR
AU  - Imtiaz, Javaria; Iqbal, Muhammad Zohaib; Khan, Muhammad Uzair
TI  - An automated model-based approach to repair test suites of evolving web applications
PY  - 2021
AB  - NA
SP  - 110841
EP  - NA
JF  - Journal of Systems and Software
VL  - 171
IS  - NA
PB  - 
DO  - 10.1016/j.jss.2020.110841
ER  - 

TY  - NA
AU  - Berntsen, Kristina Rakneberg; Palacios, Ricardo Colomo; Herranz, Eduardo
TI  - TEEM - Virtual reality and its uses: a systematic literature review
PY  - 2016
AB  - Virtual reality (VR) has challenged the way we perceive the world and user experience is being explored to achieve an immersive and effective experience. However, the commercial effects and impact of this technology lacks sufficient research. It is hence uncertain which role virtual reality has in information systems nowadays. This systematic literature review (SLR) focuses on the commercial impact of virtual reality and which field of study this technology is most used. To answer this question, a reference manual was used from literature review protocol standards and carried out. Results show that VR systems have a wide specter of applications and a significant potential for revolutionizing our everyday life in the digital world.
SP  - 435
EP  - 439
JF  - Proceedings of the Fourth International Conference on Technological Ecosystems for Enhancing Multiculturality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3012430.3012553
ER  - 

TY  - NA
AU  - Sra, Misha
TI  - UIST (Adjunct Volume) - Asymmetric Design Approach and Collision Avoidance Techniques For Room-scale Multiplayer Virtual Reality
PY  - 2016
AB  - Recent advances in consumer virtual reality (VR) technology have made it easy to accurately capture users' motions over room-sized areas allowing natural locomotion for navigation in VR. While this helps create a stronger match between proprioceptive information from human body movements for enhancing immersion and reducing motion sickness, it introduces a few challenges. Walking is only possible within virtual environments (VEs) that fit inside the boundaries of the tracked physical space which for most users is quite small. Within this space the potential for colliding with physical objects around the play area is high. Additionally, only limited haptic feedback is available. In this paper, I focus on the problem of variations in the size and shape of each user's tracked physical space for multiplayer interactions. As part of the constrained physical space problem, I also present an automated system for steering the user away from play area boundaries using Galvanic Vestibular Stimulation (GVS). In my thesis, I will build techniques to enable the system to intelligently apply redirection and GVS-based steering as users explore virtual environments of arbitrary sizes.
SP  - 29
EP  - 32
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2984788
ER  - 

TY  - CHAP
AU  - Bailly, Gilles; Malacria, Sylvain
TI  - Command Selection
PY  - 2022
AB  - NA
SP  - 1
EP  - 35
JF  - Handbook of Human Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-27648-9_19-1
ER  - 

TY  - NA
AU  - Tanner, Kesler; Johnson, Naomi; Landay, James A.
TI  - CHI - Poirot: A Web Inspector for Designers
PY  - 2019
AB  - To better understand the issues designers face as they interact with developers and use developer tools to create websites, we conducted a formative investigation consisting of interviews, a survey, and an analysis of professional design documents. Based on insights gained from these efforts, we developed Poirot, a web inspection tool for designers that enables them to make style edits to websites using a familiar graphical interface. We compared Poirot to Chrome DevTools in a lab study with 16 design professionals. We observed common problems designers experience when using Chrome DevTools and found that when using Poirot, designers were more successful in accomplishing typical design tasks (97% to 63%). In addition, we found that Poirot had a significantly lower perceived cognitive load and was overwhelmingly preferred by the designers in our study.
SP  - 528
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300758
ER  - 

TY  - NA
AU  - Satriadi, Kadek Ananta; Ens, Barrett; Cordeil, Maxime; Jenny, Bernhard; Czauderna, Tobias; Willett, Wesley
TI  - VR - Augmented Reality Map Navigation with Freehand Gestures
PY  - 2019
AB  - Freehand gesture interaction has long been proposed as a ‘natural’ input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.
SP  - 593
EP  - 603
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798340
ER  - 

TY  - NA
AU  - Gasques, Danilo; Johnson, Janet G.; Sharkey, Tommy; Feng, Yuanyuan; Wang, Ru; Xu, Zhuoqun Robin; Zavala, Enrique; Zhang, Yifei; Xie, Wanze; Zhang, Xinming; Davis, Konrad; Yip, Michael C.; Weibel, Nadir
TI  - CHI - ARTEMIS: A Collaborative Mixed-Reality System for Immersive Surgical Telementoring
PY  - 2021
AB  - Traumatic injuries require timely intervention, but medical expertise is not always available at the patient’s location. Despite recent advances in telecommunications, surgeons still have limited tools to remotely help inexperienced surgeons. Mixed Reality hints at a future where remote collaborators work side-by-side as if co-located; however, we still do not know how current technology can improve remote surgical collaboration. Through role-playing and iterative-prototyping, we identify collaboration practices used by expert surgeons to aid novice surgeons as well as technical requirements to facilitate these practices. We then introduce ARTEMIS, an AR-VR collaboration system that supports these key practices. Through an observational study with two expert surgeons and five novice surgeons operating on cadavers, we find that ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips. Most participants found that ARTEMIS facilitates collaboration despite existing technology limitations explored in this paper.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445576
ER  - 

TY  - NA
AU  - Dao, Emily; Muresan, Andreea; Hornbæk, Kasper; Knibbe, Jarrod
TI  - CHI - Bad Breakdowns, Useful Seams, and Face Slapping: Analysis of VR Fails on YouTube
PY  - 2021
AB  - Virtual reality (VR) is increasingly used in complex social and physical settings outside of the lab. However, not much is known about how these settings influence use, nor how to design for them. We analyse 233 YouTube videos of VR Fails to: (1) understand when breakdowns occur, and (2) reveal how the seams between VR use and the social and physical setting emerge. The videos show a variety of fails, including users flailing, colliding with surroundings, and hitting spectators. They also suggest causes of the fails, including fear, sensorimotor mismatches, and spectator participation. We use the videos as inspiration to generate design ideas. For example, we discuss more flexible boundaries between the real and virtual world, ways of involving spectators, and interaction designs to help overcome fear. Based on the findings, we further discuss the ‘moment of breakdown’ as an opportunity for designing engaging and enhanced VR experiences.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445435
ER  - 

TY  - NA
AU  - Rossmy, Beat; Unger, Sebastian; Wiethoff, Alexander
TI  - TouchGrid – Combining Touch Interaction with Musical Grid Interfaces
PY  - NA
AB  - Musical grid interfaces such as the monome grid have developed into standard interfaces for musical equipment over the last 15 years. However, the types of possible interactions more or less remained the same, only expanding grid capabilities by external IO elements. Therefore, we propose to transfer capacitive touch technology to grid devices to expand their input capabilities by combining tangible and capacitive-touch based interaction paradigms. This enables to keep the generic nature of grid interfaces which is a key feature for many users. In this paper we present the TouchGrid concept and share our proof-of-concept implementation as well as an expert evaluation regarding the general concept of touch interaction used on grid devices. TouchGrid provides swipe and bezel interaction derived from smart phone interfaces to allow navigation between applications and access to menu systems in a familiar way.
SP  - NA
EP  - NA
JF  - NIME 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.21428/92fbeb44.303223db
ER  - 

TY  - JOUR
AU  - Chadalavada, Perumal Varun; Palaniappan, Goutham; Chandran, Vimal Kumar; Truong, Khai N.; Wigdor, Daniel
TI  - ID'em: Inductive Sensing for Embedding and Extracting Information in Robust Materials
PY  - 2018
AB  - We present ID'em, a novel tagging and localization method that employs an array of Inductive Sensors to 'image' patterns of electrically conductive dots that are embedded underneath the surfaces of materials that cover the environments that we inhabit. ID'em addresses drawbacks found with existing tagging/localization technologies, while drawing on some of their attributes and strengths, thus creating a cost-effective, scalable system that is robust enough to be deployed pervasively. We present a detailed description of the system, applications that leverage ID'em's unique affordances, and address ID'em's strengths and limitations. With ID'em, we envision a future where the materials that we use to cover and build our everyday environments come imbued with information that can provide valuable context for rich, diverse interactions and capabilities.
SP  - 97
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 3
PB  - 
DO  - 10.1145/3264907
ER  - 

TY  - NA
AU  - Auda, Jonas; Busse, Leon; Pfeuffer, Ken; Gruenefeld, Uwe; Rivu, Radiah; Alt, Florian; Schneegass, Stefan
TI  - SUI - I’m in Control! Transferring Object Ownership Between Remote Users with Haptic Props in Virtual Reality
PY  - 2021
AB  - Virtual Reality (VR) remote collaboration is becoming more and more relevant in a wide range of scenarios, such as remote assistance or group work. A way to enhance the user experience is using haptic props that make virtual objects graspable. But physical objects are only present in one location and cannot be manipulated directly by remote users. We explore different strategies to handle ownership of virtual objects enhanced by haptic props. In particular, two strategies of handling object ownership – SingleOwnership and SharedOwnership. SingleOwnership restricts virtual objects to local haptic props, while SharedOwnership allows collaborators to take over ownership of virtual objects using local haptic props. We study both strategies for a collaborative puzzle task regarding their influence on performance and user behavior. Our findings show that SingleOwnership increases communication and enhanced with virtual instructions, results in higher task completion times. SharedOwnership is less reliant on verbal communication and faster, but there is less social interaction between the collaborators.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485287
ER  - 

TY  - JOUR
AU  - Dickson, Terence; Wehbe, Rina R.; Matulic, Fabrice; Vogel, Daniel
TI  - HybridPointing for Touch: Switching Between Absolute and Relative Pointing on Large Touch Screens
PY  - 2021
AB  - We propose CursorTap, an extension of Forlines et al.'s mixed, absolute and relative "HybridPointing" to large wall-sized multitouch displays. Our technique uses a relative pointing quasimode activated with one hand, while the other hand controls a distant cursor similar to a large touchpad. A controlled experiment compares the technique to standard absolute touch input as a baseline and a whole-display "Drag" technique representing a common alternate approach. Results show CursorTap is fastest for the common usage scenario of reaching distant targets and then returning to nearby targets. Overall, median selection times across distances are similar with CursorTap, but linearly increase with the other techniques. As further validation, a second study explore show people use CursorTap in a two-person game. The results found just over half of the participants choose to use CursorTap for half of the primary interactions where "enemies" are eliminated using a tap, drag, or lasso "tool".
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3488540
ER  - 

TY  - NA
AU  - Oh, Hyunjoo; Hsi, Sherry; Eisenberg, Michael; Gross, Mark D.
TI  - IDC - Paper mechatronics: present and future
PY  - 2018
AB  - Creative iterative development over the past several years has generated an extensive set of computational tools, learning resources, and materials in the realm of paper mechatronics - an educational craft and design approach that weaves computational and mechanical elements into established traditions of children's construction with paper. Here, we both reflect upon our past and recent work of paper mechatronics, then look to the near- to medium-term future to speculate upon both the emerging trends in technology design and expanding learning potential of this medium for children along material, spatial, and temporal dimensions. We summarize lessons learned through various children's workshops with our materials; and we use these lessons as a foundation upon which to create a wide variety of novel tools and activities in educational papercrafting. We speculate upon the frontiers of this work based on current convergences and shifts in tangible creative computational media.
SP  - 389
EP  - 395
JF  - Proceedings of the 17th ACM Conference on Interaction Design and Children
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3202185.3202761
ER  - 

TY  - JOUR
AU  - Coe, Patrick; Evreinov, Grigori; Sinivaara, Hasse; Hippula, Arto; Raisamo, Roope
TI  - Haptic Actuation Plate for Multi-Layered In-Vehicle Control Panel
PY  - 2021
AB  - High-fidelity localized feedback has the potential of providing new and unique levels of interaction with a given device. Achieving this in a cost-effective reproducible manner has been a challenge in modern technology. Past experiments have shown that by using the principles of constructive wave interference introduced by time offsets it is possible to achieve a position of increased vibration displacement at any given location. As new interface form factors increasingly incorporate curved surfaces, we now show that these same techniques can successfully be applied and mechanically coupled with a universal actuation plate.
SP  - 25
EP  - NA
JF  - Multimodal Technologies and Interaction
VL  - 5
IS  - 5
PB  - 
DO  - 10.3390/mti5050025
ER  - 

TY  - JOUR
AU  - De Paolis, Lucio Tommaso; De Luca, Valerio
TI  - The effects of touchless interaction on usability and sense of presence in a virtual environment
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>For software applications with a significant level of user involvement, the traditional concept of usability has evolved into the more complex idea of user experience, which also covers emotional, cognitive or physical responses. In virtual reality, user experience also depends on the user perception related to some peculiarities of immersive environments, where also the devices employed for user interaction play a determinant role. This has led to the design of the Presence Questionnaire (PQ) for the evaluation of the effectiveness of virtual environments. This work analyzes the effects of two different interaction modalities on usability and sense of presence: in particular, the Myo armband, a gesture-based device for touchless interaction, is compared with the Vive handheld controller bundled with the HTC Vive headset. A total of 84 subjects were recruited to test the virtual environment and asked them to fill in a questionnaire obtained by combining the Usability Metric for User eXperience (UMUX) questionnaire, the System Usability Scale (SUS) and the presence questionnaire (PQ), which was specifically designed for virtual environments. A comparison between the scores obtained for the two interaction modalities revealed which questionnaire items are significantly influenced by the input interface and deduce some insights about the consequences on human factors.</jats:p>
SP  - 1551
EP  - 1571
JF  - Virtual Reality
VL  - 26
IS  - 4
PB  - 
DO  - 10.1007/s10055-022-00647-1
ER  - 

TY  - NA
AU  - Gugenheimer, Jan; Stemasov, Evgeny; Sareen, Harpreet; Rukzio, Enrico
TI  - CHI - FaceDisplay: Towards Asymmetric Multi-User Interaction for Nomadic Virtual Reality
PY  - 2018
AB  - Mobile VR HMDs enable scenarios where they are being used in public, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. We present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. People in the surrounding can perceive the virtual world through the displays and interact with the HMD user via touch or gestures. To further explore the design space of FaceDisplay, we implemented three applications (FruitSlicer, SpaceFace and Conductor) each presenting different sets of aspects of the asymmetric co-located interaction (e.g. gestures vs touch). We conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications and showing a high level of enjoyment and social interaction with and without an HMD. Based on the findings we derive design considerations for asymmetric co-located VR applications and argue that VR HMDs are currently designed having only the HMD user in mind but should also include Non-HMD Users.
SP  - 54
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173628
ER  - 

TY  - NA
AU  - Markvicka, Eric; Wang, Guanyun; Lee, Yi-Chin; Laput, Gierad; Majidi, Carmel; Yao, Lining
TI  - CHI - ElectroDermis: Fully Untethered, Stretchable, and Highly-Customizable Electronic Bandages
PY  - 2019
AB  - Wearables have emerged as an increasingly promising interactive platform, imbuing the human body with always-available computational capabilities. This unlocks a wide range of applications, including discreet information access, health monitoring, fitness, and fashion. However, unlike previous platforms, wearable electronics require structural conformity, must be comfortable for the wearer, and should be soft, elastic, and aesthetically appealing. We envision a future where electronics can be temporarily attached to the body (like bandages or party masks), but in functional and aesthetically pleasing ways. Towards this vision, we introduce ElectroDermis, a fabrication approach that simplifies the creation of highly-functional and stretchable wearable electronics that are conformal and fully untethered by discretizing rigid circuit boards into individual components. These individual components are wired together using stretchable electrical wiring and assembled on a spandex blend fabric, to provide high functionality in a robust form-factor that is reusable. We describe our system in detail- including our fabrication parameters and its operational limits-which we hope researchers and practitioners can leverage. We describe a series of example applications that illustrate the feasibility and utility of our system. Overall, we believe ElectroDermis offers a complementary approach to wearable electronics-one that places value on the notion of impermanence (i.e., unlike tattoos and implants), better conforming to the dynamic nature of the human body.
SP  - 632
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300862
ER  - 

TY  - NA
AU  - Surale, Hemant Bhaskar; Matulic, Fabrice; Vogel, Daniel
TI  - CHI - Experimental Analysis of Mode Switching Techniques in Touch-based User Interfaces
PY  - 2017
AB  - This paper presents the results of a 36 participant empirical comparison of touch mode-switching. Six techniques are evaluated, spanning current and future techniques: long press, non-dominant hand, two-fingers, hard press, knuckle, and thumb-on-finger. Two poses are controlled for, seated with the tablet on a desk and standing with the tablet held on the forearm. Findings indicate pose has no effect on mode switching time and little effect on error rate; using two-fingers is fastest while long press is much slower; non-preferred hand and thumb-on-finger also rate highly in subjective scores. The experiment protocol is based on Li et al.'s pen mode-switching study, enabling a comparison of touch and pen mode switching. Among the common techniques, the non-dominant hand is faster than pressure with touch, whereas no significant difference had been found for pen. Our work addresses the lack of empirical evidence comparing touch mode-switching techniques and provides guidance to practitioners when choosing techniques and to researchers when designing new mode-switching methods.
SP  - 3267
EP  - 3280
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025865
ER  - 

TY  - JOUR
AU  - Takahashi, Ryo; Sasatani, Takuya; Okuya, Fuminori; Narusue, Yoshiaki; Kawahara, Yoshihiro
TI  - A Cuttable Wireless Power Transfer Sheet
PY  - 2018
AB  - We propose a cuttable wireless power transfer sheet which allows users to modify its size and shape. This intuitive manipulation allows users to easily add wireless power transmission capabilities to everyday objects. The properties of the sheet such as thinness, flexibility, and lightness make our sheet highly compatible with various configurations. We contribute a set of technical principles for the design of circuitry, which integrates H-tree wiring and time division power supply techniques. H-tree wiring allows the sheet to remain functional even when cut from the outside of the sheet, whereas time division power supply avoids the reduction in power transfer efficiency caused by the magnetic interference between adjacent transmitter coils. Through the evaluations, we found that our time division power supply scheme mitigates the degradation of power transfer efficiency and successfully improves the average efficiency. Furthermore, we present four applications which integrates our sheet into daily objects: wireless charging furniture, bag, jacket, and craft; these applications confirmed the feasibility of our prototype.
SP  - 190
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287068
ER  - 

TY  - JOUR
AU  - Wang, April Yi; Chen, Yan; Chung, John Joon Young; Brooks, Christopher; Oney, Steve
TI  - PuzzleMe: Leveraging Peer Assessment for In-Class Programming Exercises
PY  - 2021
AB  - Peer assessment, as a form of collaborative learning, can engage students in active learning and improve their learning gains. However, current teaching platforms and programming environments provide little support to integrate peer assessment for in-class programming exercises. We identified challenges in conducting such exercises and adopting peer assessment through formative interviews with instructors of introductory programming courses. To address these challenges, we introduce PuzzleMe, a tool to help Computer Science instructors to conduct engaging in-class programming exercises. PuzzleMe leverages peer assessment to support a collaboration model where students provide timely feedback on their peers' work. We propose two assessment techniques tailored to in-class programming exercises: live peer testing and live peer code review. Live peer testing can improve students' code robustness by allowing them to create and share lightweight tests with peers. Live peer code review can improve code understanding by intelligently grouping students to maximize meaningful code reviews. A two-week deployment study revealed that PuzzleMe encourages students to write useful test cases, identify code problems, correct misunderstandings, and learn a diverse set of problem-solving approaches from peers.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW2
PB  - 
DO  - 10.1145/3479559
ER  - 

TY  - NA
AU  - Kim, Jeeeun; Zhou, Qingnan; Ghassaei, Amanda; Chen, Xiang 'Anthony'
TI  - TEI - OmniSoft: A Design Tool for Soft Objects by Example
PY  - 2021
AB  - Softness is one of the most important factors in human tactile perception. With recent advances in 3Dprinting, there has been significant progress in fabricating compliant objects. However, existing methods typically remain inaccessible to end-users, mainly due to the separation between designing shapes and setting printing parameters to achieve desired softness, resulting in the exclusion of its customization in early design processes. In this work, we contribute an end-to-end design tool that takes a design-by-example approach: given a 3D model, a user can specify the region of interest and a level of softness, by shopping everyday objects as a reference. The tool then generates both geometry and 3D printing parameters to reproduce the desired softness, which can be fabricated using low-cost FDM 3D printing and materials for it. We also provide a data-driven pipeline to enable other compliance modeling methods to be generalized within our design tool. In two user studies, we demonstrated that users could easily locate existing reference objects’ softness to a 3D printed object. In a design session, end-users successfully used OmniSoft to design augmented functions.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440634
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Bulling, Andreas; Alt, Florian
TI  - UbiComp/ISWC Adjunct - Tackling challenges of interactive public displays using gaze
PY  - 2015
AB  - Falling hardware prices led to a widespread use of public displays. Common interaction techniques for such displays currently include touch, mid-air, or smartphone-based interaction. While these techniques are well understood from a technical perspective, several remaining challenges hinder the uptake of interactive displays among passersby. In this paper we propose addressing major public display challenges through gaze as a novel interaction modality. We discuss why gaze-based interaction can tackle these challenges effectively and discuss how solutions can be technically realized. Furthermore, we summarize state-of-the-art eye tracking techniques that show particular promise in the area of public displays.
SP  - 763
EP  - 766
JF  - Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers - UbiComp '15
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2800835.2807951
ER  - 

TY  - JOUR
AU  - Song, Seungtaek; Kim, Namhyun; Lee, Sungkil; Whang, Joyce Jiyoung; Lee, Jinkyu
TI  - Design and Feasibility Study: Customized Virtual Buttons for Electronic Mobile Devices
PY  - 2019
AB  - NA
SP  - 668
EP  - 671
JF  - IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences
VL  - 102
IS  - 4
PB  - 
DO  - 10.1587/transfun.e102.a.668
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Soares, Gustavo; Head, Andrew; Glassman, Elena L.; Reis, Ruan; Mongiovi, Melina; D'Antoni, Loris; Hartmann, Björn
TI  - VL/HCC - TraceDiff: Debugging unexpected code behavior using trace divergences
PY  - 2017
AB  - Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.
SP  - 107
EP  - 115
JF  - 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2017.8103457
ER  - 

TY  - NA
AU  - Harley, Daniel; Tarun, Aneesh P.; Elsharawy, Sara; Verni, Alexander; Tibu, Tudor; Bilic, Marko; Bakogeorge, Alexander; Mazalek, Ali
TI  - Conference on Designing Interactive Systems - Mobile Realities: Designing for the Medium of Smartphone-VR
PY  - 2019
AB  - We present two proof of concept experiences for a virtual reality (VR) game that draws on several medium-specific qualities of mobile, location-based, and tangible storytelling. In contemporary smartphone-VR, experiences are limited by short playtimes, limited interactions, and limited movement within a physical space. To address these limitations, we suggest a reconceptualization of smartphone-VR. Rather than design that deems the smartphone the least capable VR platform, we propose design that adds VR to an already rich mobile storytelling platform. We argue that by drawing on otherwise separate storytelling media, designers can circumvent limitations related to smartphone-VR while also extending the range of smartphone-based storytelling. We conclude by reflecting on possible implications of this extended design space.
SP  - 1131
EP  - 1144
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322341
ER  - 

TY  - NA
AU  - Freeman, Euan; Griffiths, Gareth; Brewster, Stephen
TI  - ICMI - Rhythmic micro-gestures: discreet interaction on-the-go
PY  - 2017
AB  - We present rhythmic micro-gestures, micro-movements of the hand that are repeated in time with a rhythm. We present a user study that investigated how well users can perform rhythmic micro-gestures and if they can use them eyes-free with non-visual feedback. We found that users could successfully use our interaction technique (97% success rate across all gestures) with short interaction times, rating them as low difficulty as well. Simple audio cues that only convey the rhythm outperformed animations showing the hand movements, supporting rhythmic micro-gestures as an eyes-free input technique.
SP  - 115
EP  - 119
JF  - Proceedings of the 19th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3136755.3136815
ER  - 

TY  - NA
AU  - Gupta, Shrey; McGuffin, Michael J.
TI  - AVI - Multitouch Radial Menu Integrating Command Selection and Control of Arguments with up to 4 Degrees of Freedom
PY  - 2016
AB  - We design and evaluate a multitouch radial menu for large screens with two desirable properties. First, it allows a single gesture to select a command and then continuously control arguments for that command with unbroken kinesthetic tension. Second, arguments are controlled with 1 or 2 fingers for up to 4 degrees of freedom (DoF). For example, the user may select one command for 4 DoF direct manipulation (translation + scaling + rotation), or another command for 3 DoF camera operations (pan + zoom), using the same two-finger pinch gesture, but with different initial orientations of the gesture to disambiguate. We present a taxonomy to classify previous menuing techniques sharing the first property, and discuss how very few techniques have both of these properties. Our work also extends previous work by Banovic et al. in the following ways: our menu supports submenus and a fast default command, and we experimentally evaluate the effect of varying the number of rings in the menu, the symmetry of the menu, and the use of one hand vs. two hands vs. a stylus and hand.
SP  - 256
EP  - 263
JF  - Proceedings of the International Working Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909132.2909266
ER  - 

TY  - NA
AU  - Chen, Yang-Sheng; Han, Ping-Hsuan; Hsiao, Jui-Chun; Lee, Kong-Chang; Hsieh, Chiao-En; Lu, Kuan-Yin; Chou, Chien-Hsing; Hung, Yi-Ping
TI  - UIST (Adjunct Volume) - SoEs: Attachable Augmented Haptic on Gaming Controller for Immersive Interaction
PY  - 2016
AB  - We present SoEs (Sword of Elements), an attachable augmented haptic device for enhancing gaming controller in the immersive first-person game. Generally, Player can easily receive visual and auditory feedback through head-mounted displays (HMD) and headphones from first-person perspective in virtual world. However, the tactile feedback is less than those feedbacks in immersive environment. Although gaming controller, i.e. VIVE or Oculus controller, can provide tactile feedback by some vibration sensors, the haptic feedback is more complicated and various, it includes kinesthesia and cutaneous feedback. Our key idea is to provide a low-cost approach to simulate the haptic feedback of player manipulation in the immersive environment such as striking while the iron is hot which the player could feel the heat and reaction force. Eventually, the game makers could utilize the attachable device into their games for providing haptic feedback.
SP  - 71
EP  - 72
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985707
ER  - 

TY  - JOUR
AU  - Huang, Susu; Qi, Daqing; Yuan, Jiabin; Tu, Huawei
TI  - Review of studies on target acquisition in virtual reality based on the crossing paradigm
PY  - 2019
AB  - Abstract Crossing is a fundamental paradigm for target selection in human-computer interaction systems. This paradigm was first introduced to virtual reality (VR) interactions by Tu et al., who investigated its performance in comparison to pointing, and concluded that crossing is generally no less effective than pointing and has unique advantages. However, owing to the characteristics of VR interactions, there are still many factors to consider when applying crossing to a VR environment. Thus, this review summarizes the main techniques for object selection in VR and crossing-related studies. Then, factors that may affect crossing interactions are analyzed from the perspectives of the input space and visual space. The aim of this study is to provide a reference for future studies on target selection based on the crossing paradigm in virtual reality.
SP  - 251
EP  - 264
JF  - Virtual Reality & Intelligent Hardware
VL  - 1
IS  - 3
PB  - 
DO  - 10.3724/sp.j.2096-5796.2019.0006
ER  - 

TY  - JOUR
AU  - Joshi, Yashas; Poullis, Charalambos
TI  - Inattentional Blindness for Redirected Walking Using Dynamic Foveated Rendering
PY  - 2020
AB  - Redirected walking is a Virtual Reality(VR) locomotion technique which enables users to navigate virtual environments (VEs) that are spatially larger than the available physical tracked space. In this work we present a novel technique for redirected walking in VR based on the psychological phenomenon of inattentional blindness . Based on the user’s visual fixation points we divide the user’s view into zones. Spatially-varying rotations are applied according to the zone’s importance and are rendered using foveated rendering . Our technique is real-time and applicable to small and large physical spaces. Furthermore, the proposed technique does not require the use of stimulated saccades but rather takes advantage of naturally occurring saccades and blinks for a complete refresh of the framebuffer. We performed extensive testing and present the analysis of the results of three user studies conducted for the evaluation.
SP  - 39013
EP  - 39024
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.2975032
ER  - 

TY  - JOUR
AU  - Park, Wonjun; Heo, Hayoung; Park, Seongjun; Kim, Jinmo
TI  - A Study on the Presence of Immersive User Interface in Collaborative Virtual Environments Application
PY  - 2019
AB  - This study proposes a collaboration-based interaction as a new method for providing an improved presence and a satisfying experience to various head-mounted display (HMD) users utilized in immersive virtual reality (IVR), and analyzes the experiences (improved presence, satisfying enjoyment, and social interaction) of applying collaboration to user interfaces. The key objective of the proposed interaction is to provide an environment where HMD users are able to collaborate with each other, based on their differentiated roles and behaviors. To this end, a collaboration-based interaction structured in three parts was designed, including a synchronization procedure and a communication interface that enable users to swiftly execute common goals with precision, based on immersive interactions that allow users to directly exchange information and provide feedback with their hands and feet. Moreover, experimental VR applications were built to systematically analyze the improved presence, enjoyment, and social interaction experienced by users through collaboration. Finally, by conducting a survey on the participants of the experiment, this study confirmed that the proposed interface indeed provided users with an improved presence and a satisfying experience, based on collaboration.
SP  - 476
EP  - NA
JF  - Symmetry
VL  - 11
IS  - 4
PB  - 
DO  - 10.3390/sym11040476
ER  - 

TY  - NA
AU  - Champagne, Alex; Pritchard, Bob; Dietz, Paul; Fels, Sidney
TI  - Investigation of a Novel Shape Sensor for Musical Expression
PY  - NA
AB  - A novel, high-fidelity, shape-sensing technology, BendShape , is investigated as an expressive music controller for sound effects, direct sound manipulation, and voice synthesis. Various approaches are considered for developing mapping strategies that create transparent metaphors to facilitate expression for both the performer and the audience. We explore strategies in the input, intermediate, and output mapping layers using a two-step approach guided by Perry’s Principles . First, we use trial-and-error to establish simple mappings between single input parameter control and effects to identify promising directions for further study. Then, we compose a specific piece that supports different uses of the BendShape mappings in a performance context: this allows us to study a performer trying different types of expressive techniques, enabling us to analyse the role each mapping has in facilitating musical expression. We also investigate the effects these mapping strategies have on performer bandwidth. Our main finding is that the high fidelity of the novel BendShape sensor facilitates creating interpretable input representations to control sound representations, and thereby match interpretations that provide better expressive mappings, such as with vocal shape to vocal sound and bumpiness control; however, direct mappings of individual, independent sensor mappings to effects does not provide obvious advantages over simpler controls. Furthermore, while the BendShape sensor enables rich explorations for sound, the ability to find expressive interpretable shape-to-sound representations while respecting the performer’s bandwidth limitations (caused by having many coupled input degrees of freedom) remains a challenge and an opportunity.
SP  - NA
EP  - NA
JF  - NIME 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.21428/92fbeb44.a72b68dd
ER  - 

TY  - NA
AU  - Yixian, Yan; Takashima, Kazuki; Tang, Anthony; Tanno, Takayuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - UIST - ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World
PY  - 2020
AB  - We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.
SP  - 223
EP  - 235
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415859
ER  - 

TY  - BOOK
AU  - Zielasko, Daniel; Kruger, Marcel; Weyers, Benjamin; Kuhlen, Torsten
TI  - WEVR@VR - Passive Haptic Menus for Desk-Based and HMD-Projected Virtual Reality
PY  - 2019
AB  - In this work we evaluate the impact of passive haptic feedback on touch-based menus, given the constraints and possibilities of a seated, desk-based scenario in VR. Therefore, we compare a menu that once is placed on the surface of a desk and once mid-air on a surface in front of the user. The study design is completed by two conditions without passive haptic feedback. In the conducted user study (n = 33) we found effects of passive haptics (present vs- non-present) and menu alignment (desk vs. mid-air) on the task performance and subjective look & feel, however the race between the conditions was close. An overall winner was the mid-air menu with passive haptic feedback, which however raises hardware requirements.
SP  - 1
EP  - 6
JF  - 2019 IEEE 5th Workshop on Everyday Virtual Reality (WEVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wevr.2019.8809589
ER  - 

TY  - NA
AU  - Chang, Zekun; Ta, Tung D.; Narumi, Koya; Kim, Heeju; Okuya, Fuminori; Li, Dongchi; Kato, Kunihiro; Qi, Jie; Miyamoto, Yoshinobu; Saito, Kazuya; Kawahara, Yoshihiro
TI  - CHI Extended Abstracts - Demonstrating Kirigami Haptic Swatches for Cut-and-Fold Haptic Feedback Mechanisms
PY  - 2020
AB  - Kirigami Haptic Swatches demonstrates how kirigami and origami based structures enable sophisticated haptic feedback through simple cut-and-fold fabrication techniques. We leverage four types of geometric patterns: rotational erection system (RES), split-fold waterbomb (SFWB), the overlaid structure of SFWB and RES (SFWB+RES), and cylindrical origami, to render different sets of haptic feedback (i.e., linear, bistable, snap-through, and angular rotational force behaviors, respectively). In each structure, form factor and force feedback properties can be tuned through geometric parameters. Based on the experimental results and analysis, we implemented software to automatically generate 2D patterns for desired haptic properties. We also showcased five example applications: assistive input interfaces, rotational switch, multi-sensory toy, task checklist, and phone accessories. We believe the Kirigami Haptic Swatches helps tinkerers, designers, and even researchers to create interactions that enrich our haptic experience.
SP  - 3383162
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383162
ER  - 

TY  - NA
AU  - Gupta, Aakar; Ji, Cheng; Yeo, Hui-Shyong; Quigley, Aaron; Vogel, Daniel
TI  - CHI - RotoSwype: Word-Gesture Typing using a Ring
PY  - 2019
AB  - We propose RotoSwype, a technique for word-gesture typing using the orientation of a ring worn on the index finger. RotoSwype enables one-handed text-input without encumbering the hand with a device, a desirable quality in many scenarios, including virtual or augmented reality. The method is evaluated using two arm positions: with the hand raised up with the palm parallel to the ground; and with the hand resting at the side with the palm facing the body. A five-day study finds both hand positions achieved speeds of at least 14 words-per-minute (WPM) with uncorrected error rates near 1%, outperforming previous comparable techniques.
SP  - 14
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300244
ER  - 

TY  - NA
AU  - Alt, Florian; Bulling, Andreas; Mecke, Lukas; Buschek, Daniel
TI  - Conference on Designing Interactive Systems - Attention, please!: Comparing Features for Measuring Audience Attention Towards Pervasive Displays
PY  - 2016
AB  - Measuring audience attention towards pervasive displays is important but accurate measurement in real time remains a significant sensing challenge. Consequently, researchers and practitioners typically use other features, such as face presence, as a proxy. We provide a principled comparison of the performance of six features and their combinations for measuring attention: face presence, movement trajectory, walking speed, shoulder orientation, head pose, and gaze direction. We implemented a prototype that is capable of capturing this rich set of features from video and depth camera data. Using a controlled lab experiment (N=18) we show that as a single feature, face presence is indeed among the most accurate. We further show that accuracy can be increased through a combination of features (+10.3%), knowledge about the audience (+63.8%), as well as user identities (+69.0%). Our findings are valuable for display providers who want to collect data on display effectiveness or build interactive, responsive apps.
SP  - 823
EP  - 828
JF  - Proceedings of the 2016 ACM Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2901790.2901897
ER  - 

TY  - JOUR
AU  - Floris, Ignazio; Adam, Jose M.; Calderón, Pedro A.; Sales, Salvador
TI  - Fiber Optic Shape Sensors: A comprehensive review
PY  - 2021
AB  - NA
SP  - 106508
EP  - NA
JF  - Optics and Lasers in Engineering
VL  - 139
IS  - NA
PB  - 
DO  - 10.1016/j.optlaseng.2020.106508
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Dementyev, Artem; Follmer, Sean; Paradiso, Joseph A.; Ishii, Hiroshi
TI  - UIST - ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces
PY  - 2016
AB  - This paper presents ChainFORM: a linear, modular, actuated hardware system as a novel type of shape changing interface. Using rich sensing and actuation capability, this modular hardware system allows users to construct and customize a wide range of interactive applications. Inspired by modular and serpentine robotics, our prototype comprises identical modules that connect in a chain. Modules are equipped with rich input and output capability: touch detection on multiple surfaces, angular detection, visual output, and motor actuation. Each module includes a servo motor wrapped with a flexible circuit board with an embedded microcontroller.Leveraging the modular functionality, we introduce novel interaction capability with shape changing interfaces, such as rearranging the shape/configuration and attaching to passive objects and bodies. To demonstrate the capability and interaction design space of ChainFORM, we implemented a variety of applications for both computer interfaces and hands-on prototyping tools.
SP  - 87
EP  - 96
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984587
ER  - 

TY  - NA
AU  - Kubo, Yuki; Koguchi, Yuto; Shizuki, Buntarou; Takahashi, Shin; Hilliges, Otmar
TI  - MobileHCI - AudioTouch: Minimally Invasive Sensing of Micro-Gestures via Active Bio-Acoustic Sensing
PY  - 2019
AB  - We present AudioTouch, a minimally invasive approach for sensing micro-gestures using active bio-acoustic sensing. It only requires attaching two piezo-electric elements, acting as a surface mounted speaker and microphone, on the back of the hand. It does not require any instrumentation on the palm or fingers; therefore, it does not encumber interactions with physical objects. The signal is rich enough to detect small differences in micro-gestures with standard machine-learning classifiers. This approach also allows for the discrimination of different levels of touch-force, further expanding the interaction vocabulary. We conducted four experiments to evaluate the performances of AudioTouch: a user study for measuring the gesture recognition accuracy, a follow-up study investigating the ability to discriminate different levels of touch-force, an experiment assessing the cross-session robustness, and, a systematic evaluation assessing the effect of sensor placement on the back of the hand.
SP  - 36
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340147
ER  - 

TY  - NA
AU  - Klefeker, Josephine; Devendorf, Laura
TI  - CHI Extended Abstracts - String Figuring: A Story of Reflection, Material Inquiry, and a Novel Sensor
PY  - 2018
AB  - We describe a process of materials inquiry that gave rise to a new kind of sensor: a string figure sensor that correlates resistance changes with the topology of a closed loop of string. We describe the critical and reflective process from which our string figure sensor emerged, how the sensor works, and the future applications we envision our sensor supporting.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188570
ER  - 

TY  - JOUR
AU  - Chen, Xiang 'Anthony'; Li, Yang
TI  - Improv: An Input Framework for Improvising Cross-Device Interaction by Demonstration
PY  - 2017
AB  - As computing devices become increasingly ubiquitous, it is now possible to combine the unique capabilities of different devices or Internet of Things to accomplish a task. However, there is currently a high technical barrier for creating cross-device interaction. This is especially challenging for end users who have limited technical expertise—end users would greatly benefit from custom cross-device interaction that best suits their needs. In this article, we present Improv, a cross-device input framework that allows a user to easily leverage the capability of additional devices to create new input methods for an existing, unmodified application, e.g., creating custom gestures on a smartphone to control a desktop presentation application. Instead of requiring developers to anticipate and program these cross-device behaviors in advance, Improv enables end users to improvise them on the fly by simple demonstration, for their particular needs and devices at hand. We showcase a range of scenarios where Improv is used to create a diverse set of useful cross-device input. Our study with 14 participants indicated that on average it took a participant 10 seconds to create a cross-device input technique. In addition, Improv achieved 93.7% accuracy in interpreting user demonstration of a target UI behavior by looking at the raw input events from a single example.
SP  - 15
EP  - 21
JF  - ACM Transactions on Computer-Human Interaction
VL  - 24
IS  - 2
PB  - 
DO  - 10.1145/3057862
ER  - 

TY  - JOUR
AU  - Kobylarz, Jhonatan; Bird, Jordan J.; Faria, Diego R.; Ribeiro, Eduardo Parente; Ekárt, Anikó
TI  - Thumbs up, thumbs down: non-verbal human-robot interaction through real-time EMG classification via inductive and supervised transductive transfer learning
PY  - 2020
AB  - In this study, we present a transfer learning method for gesture classification via an inductive and supervised transductive approach with an electromyographic dataset gathered via the Myo armband. A ternary gesture classification problem is presented by states of ’thumbs up’, ’thumbs down’, and ’relax’ in order to communicate in the affirmative or negative in a non-verbal fashion to a machine. Of the nine statistical learning paradigms benchmarked over 10-fold cross validation (with three methods of feature selection), an ensemble of Random Forest and Support Vector Machine through voting achieves the best score of 91.74% with a rule-based feature selection method. When new subjects are considered, this machine learning approach fails to generalise new data, and thus the processes of Inductive and Supervised Transductive Transfer Learning are introduced with a short calibration exercise (15 s). Failure of generalisation shows that 5 s of data per-class is the strongest for classification (versus one through seven seconds) with only an accuracy of 55%, but when a short 5 s per class calibration task is introduced via the suggested transfer method, a Random Forest can then classify unseen data from the calibrated subject at an accuracy of around 97%, outperforming the 83% accuracy boasted by the proprietary Myo system. Finally, a preliminary application is presented through social interaction with a humanoid Pepper robot, where the use of our approach and a most-common-class metaclassifier achieves 100% accuracy for all trials of a ‘20 Questions’ game.
SP  - 6021
EP  - 6031
JF  - Journal of Ambient Intelligence and Humanized Computing
VL  - 11
IS  - 12
PB  - 
DO  - 10.1007/s12652-020-01852-z
ER  - 

TY  - NA
AU  - Groeger, Daniel; Feick, Martin; Withana, Anusha; Steimle, Jürgen
TI  - UIST - Tactlets: Adding Tactile Feedback to 3D Objects Using Custom Printed Controls
PY  - 2019
AB  - Rapid prototyping of haptic output on 3D objects promises to enable a more widespread use of the tactile channel for ubiquitous, tangible, and wearable computing. Existing prototyping approaches, however, have limited tactile output capabilities, require advanced skills for design and fabrication, or are incompatible with curved object geometries. In this paper, we present a novel digital fabrication approach for printing custom, high-resolution controls for electro-tactile output with integrated touch sensing on interactive objects. It supports curved geometries of everyday objects. We contribute a design tool for modeling, testing, and refining tactile input and output at a high level of abstraction, based on parameterized electro-tactile controls. We further contribute an inventory of 10 parametric Tactlet controls that integrate sensing of user input with real-time electro-tactile feedback. We present two approaches for printing Tactlets on 3D objects, using conductive inkjet printing or FDM 3D printing. Empirical results from a psychophysical study and findings from two practical application cases confirm the functionality and practical feasibility of the Tactlets approach.
SP  - 923
EP  - 936
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347937
ER  - 

TY  - NA
AU  - Zhu, Kening; Chen, Taizhou; Han, Feng; Wu, Yi-Shiun
TI  - CHI - HapTwist: Creating Interactive Haptic Proxies in Virtual Reality Using Low-cost Twistable Artefacts
PY  - 2019
AB  - In this paper, we present a series of studies on using Rubik's Twist, a type of low-cost twistable artefact, to create haptic proxies for various hand-graspable VR objects. Our pilot studies validated the feasibility and effectiveness of Rubik's-Twist-based haptic proxies. The pilot results also revealed user challenges in the physical shape creation, motivating the development of the HapTwist toolkit. The toolkit consists of the shape-generation algorithm, the software interface for shape-construction guidance and interaction authoring, and the hardware modules for constructing interactive haptic proxies. The user studies showed that HapTwist was easy to learn and use, and it significantly improved user performance in creating interactive haptic proxies with Rubik's Twist. Furthermore, HapTwist-generated haptic proxies achieved similar VR experience as the real objects.
SP  - 693
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300923
ER  - 

TY  - JOUR
AU  - Anggoro, Pius Dian Widi
TI  - Sistem pelacakan posisi pengguna menggunakan marker-based AR dalam menjelajahi galeri museum VR
PY  - 2019
AB  - This study examines the user position tracking system using marker-based AR on smartphones camera. The tracking system uses a homographic algorithm integrated into the Galeri Museum VR application. In the test, the user performed exploration interactions by 6 degrees of freedom in ten different positions in the museum gallery. The physical space used in this study was 4 x 4 m2 and a marker attached to the wall in front of the user. This system results in errors in XYZ field (0.102 m, 0.047 m, 0.044 m). If the camera's orientation is not directing to the marker and the user is moving, jitter appears because of the untracked marker. The use of marker-based AR successfully applied to track the position of users who perform natural locomotion interactions in the VR environment.
SP  - 134
EP  - 140
JF  - Jurnal Teknologi dan Sistem Komputer
VL  - 7
IS  - 4
PB  - 
DO  - 10.14710/jtsiskom.7.4.2019.134-140
ER  - 

TY  - NA
AU  - Boldt, Mette; Bonfert, Michael; Lehne, Inga; Cahnbley, Melina; Korschinq, Kim; Bikas, Loannis; Finke, Stefan; Hanci, Martin; Kraft, Valentin; Liu, Boxuan; Nguyen, Tram; Panova, Alina; Singh, Ramneek; Steenbergen, Alexander; Malaka, Rainer; Smeddinck, Jan Jan
TI  - VR - You Shall Not Pass: Non-Intrusive Feedback for Virtual Walls in VR Environments with Room-Scale Mapping
PY  - 2018
AB  - Room-scale mapping facilitates natural locomotion in virtual reality (VR), but it creates a problem when encountering virtual walls. In traditional video games, player avatars can simply be prevented from moving through walls. This is not possible in VR with room-scale mapping due to the lack of physical boundaries. Game design is either limited by avoiding walls, or the players might ignore them, which endangers the immersion and the overall game experience. To prevent players from walking through walls, we propose a combination of auditory, visual, and vibrotactile feedback for wall collisions. This solution can be implemented with standard game engine features, does not require any additional hardware or sensors, and is independent of game concept and narrative. A between-group study with 46 participants showed that a large majority of players without the feedback did pass through virtual walls, while 87% of the participants with the feedback refrained from walking through walls. The study found no notable differences in game experience.
SP  - 143
EP  - 150
JF  - 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2018.8446177
ER  - 

TY  - NA
AU  - Zhou, Zhuoming; Segura, Elena Márquez; Duval, Jared; John, Michael; Isbister, Katherine
TI  - CHI PLAY - Astaire: A Collaborative Mixed Reality Dance Game for Collocated Players
PY  - 2019
AB  - Despite the growth of Virtual Reality (VR), the design space of collocated social play in VR remains narrow. Here we present Astaire, a collaborative hybrid VR dance game for two players sharing an HTC Vive VR system. The game resulted from a design research process using embodied design methods, and drawing upon concepts in HCI and Play Design, including social affordances, and asymmetric and interdependent play. Here we present insights from a study playtesting Astaire alongside two VR games that inspired ours: Keep Talking and Nobody Explodes (KTNE), and Audioshield. We examined players' and spectators' enjoyment, and interpersonal relationships, which were self-reported higher for Astaire. Using data from semi-structured interviews, we foreground design elements that impacted our participants' play experience, grouped under the themes of balance of players' roles, the physicality afforded by the game, and the social experience enabled. Our work contributes to opening the design space of hybrid collocated VR--through our game, we surface inspirational design concepts in HCI, and share design knowledge gained during our design process.
SP  - 5
EP  - 18
JF  - Proceedings of the Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311350.3347152
ER  - 

TY  - JOUR
AU  - Wilhelm, Mathias; Krakowczyk, Daniel; Albayrak, Sahin
TI  - PeriSense: Ring-Based Multi-Finger Gesture Interaction Utilizing Capacitive Proximity Sensing.
PY  - 2020
AB  - Rings are widely accepted wearables for gesture interaction. However, most rings can sense only the motion of one finger or the whole hand. We present PeriSense, a ring-shaped interaction device enabling multi-finger gesture interaction. Gestures of the finger wearing ring and its adjacent fingers are sensed by measuring capacitive proximity between electrodes and human skin. Our main contribution is the determination of PeriSense's interaction space involving the evaluation of capabilities and limitations. We introduce a prototype named PeriSense, analyze the sensor resolution at different distances, and evaluate finger gestures and unistroke gestures based on gesture sets allowing the determination of the strengths and limitations. We show that PeriSense is able to sense the change of conductive objects reliably up to 2.5 cm. Furthermore, we show that this capability enables different interaction techniques such as multi-finger gesture recognition or two-handed unistroke input.
SP  - 3990
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 14
PB  - 
DO  - 10.3390/s20143990
ER  - 

TY  - JOUR
AU  - Annett, Michelle
TI  - Understanding the Homepreneurship Opportunities Afforded by Social Networking and Personal Fabrication Technologies
PY  - 2020
AB  - The decreased cost and increased usability of personal fabrication technologies has enabled a new generation of crafters to integrate digital designs and computationally created artifacts into physically-based practices. With the simultaneous ubiquity of e-commerce and social networking channels, these technologies have enabled many crafters to transform their hobbies into home-based businesses. To understand the opportunities and challenges that fusing social networking platforms, personal fabrication equipment, and e-commerce have afforded these homepreneurs, an online survey and follow-up interviews were conducted with crafters who use hobbyist cutting plotters to personalize and sell goods online. The findings uncovered an emerging group of homepreneurs, i.e., mompreneurs, who use these technologies for supplemental income for their families and highlighted the emotional and opportunistic benefits that such personalized, at-home manufacturing affords. They also highlighted the workflows and lifestyle implications of using these technologies to run home-based businesses, the multi-faceted usage and dependence these homepreneurs have on online social platforms such as Facebook, the complex software toolchains that are used, and the commonplace practice of appropriating designs from others that occurs in this community.
SP  - 1
EP  - 48
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW2
PB  - 
DO  - 10.1145/3415170
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Lin, Cheng-Lung; Chiang, Chi-huan; Kuo, Tzu-Sheng; Chan, Liwei; Huang, Da-Yuan; Chen, Bing-Yu
TI  - UIST - TilePoP: Tile-type Pop-up Prop for Virtual Reality
PY  - 2019
AB  - We present TilePoP, a new type of pneumatically-actuated interface deployed as floor tiles which dynamically pop up by inflating into large shapes constructing proxy objects for whole-body interactions in Virtual Reality. TilePoP consists of a 2D array of stacked cube-shaped airbags designed with specific folding structures, enabling each airbag to be inflated into a physical proxy and then deflated down back to its original tile shape when not in use. TilePoP is capable of providing haptic feedback for the whole body and can even support human body weight. Thus, it allows new interaction possibilities in VR. Herein, the design and implementation of TilePoP are described in detail along with demonstrations of its applications and the results of a preliminary user evaluation conducted to understand the users' experience with TilePoP.
SP  - 639
EP  - 649
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347958
ER  - 

TY  - NA
AU  - Rossmy, Beat; Wiethoff, Alexander
TI  - Musical Grid Interfaces: Past, Present, and Future Directions
PY  - NA
AB  - This paper examines grid interfaces which are currently used in many musical devices and instruments. This type of interface concept has been rooted in the NIME community since the early 2000s. We provide an overview of research projects and commercial products and conducted an expert interview as well as an online survey. In summary this work shares: (1) an overview on grid controller research, (2) a set of three usability issues deduced by a multi method approach, and (3) an evaluation of user perceptions regarding persistent usability issues and common reasons for the use of grid interfaces.
SP  - NA
EP  - NA
JF  - NIME 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.21428/92fbeb44.6a2451e6
ER  - 

TY  - JOUR
AU  - Sturdee, Miriam; Alexander, Jason
TI  - Analysis and Classification of Shape-Changing Interfaces for Design and Application-based Research
PY  - 2018
AB  - Shape-changing interfaces are physically tangible, interactive devices, surfaces, or spaces that allow for rich, organic, and novel experiences with computational devices. Over the last 15 years, research has produced functional prototypes over many use applications; reviews have identified themes and possible future directions but have not yet looked at possible design or application-based research. Here, we gather this information together to provide a reference for designers and researchers wishing to build upon existing prototyping work, using synthesis and discussion of existing shape-changing interface reviews and comprehensive analysis and classification of 84 shape-changing interfaces. Eight categories of prototype are identified alongside recommendations for the field.
SP  - 2
EP  - 32
JF  - ACM Computing Surveys
VL  - 51
IS  - 1
PB  - 
DO  - 10.1145/3143559
ER  - 

TY  - JOUR
AU  - Yan, Yukang; Shi, Yingtian; Yu, Chun; Shi, Yuanchun
TI  - HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays
PY  - 2020
AB  - We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3380983
ER  - 

TY  - JOUR
AU  - Xiaoling, Li; Han, Feng; Xiuwen, Sun; Liu, Yang; Li, Yin; Chen, Yao
TI  - Bracelet: arms-down selection for Kinect mid-air gesture
PY  - 2018
AB  - ABSTRACTGesture-based interaction has become more affordable and ubiquitous as an interaction style in recent years. Since gesture-based interactions lead to fatigue and cause heaviness in upper limbs, a problem commonly known as ‘Gorilla-Arm Syndrome’ occurs. Then Bracelet is proposed, an arms-down selection method based on Kinect. Its purpose is reducing fatigue in a long mid-air gesture interaction session. An evaluation of 16 participants compared with previous methods such as mid-air gestures and other arms-down interactions showed the effectiveness of the Bracelet in reducing fatigue. As the Bracelet is helpful to alleviate fatigue in some situations where selection is intensive and has no time limit, it can be used as a ‘plug-in’ for other methods and applied for display in many public places such as airports, stations, shopping malls and waiting rooms.
SP  - 401
EP  - 409
JF  - Behaviour & Information Technology
VL  - 38
IS  - 4
PB  - 
DO  - 10.1080/0144929x.2018.1536166
ER  - 

TY  - NA
AU  - Cockburn, Andy; Woolley, Dion; Thai, Kien Tran Pham; Clucas, Don; Hoermann, Simon; Gutwin, Carl
TI  - AutomotiveUI - Reducing the Attentional Demands of In-Vehicle Touchscreens with Stencil Overlays
PY  - 2018
AB  - Vehicle manufacturers are increasingly using touchscreens to support driver access to controls. However, input mechanisms displayed on touchscreens lack the tactile sensations of physical controls, creating risks of greater demand for visual attention. These risks can potentially be mitigated by restoring some degree of tactile feedback to touchscreen interaction. This paper describes a study that examines whether touchscreen target selection during simulated driving is improved by overlaying the touchscreen with a see-through 3D printed stencil that allows underlying touchscreen controls to be located or guided by feel. Results showed that touchscreen targets were selected more quickly and with shorter periods of visual attention towards the touchscreen when the stencil was present than when it was absent. Subjective preferences also favoured the stencil condition. The work demonstrates the value of adding tactile feedback to touchscreen interaction, and shows that stencils are a simple and effective way to reduce attentional demands.
SP  - 33
EP  - 42
JF  - Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3239060.3239061
ER  - 

TY  - BOOK
AU  - Auda, Jonas; Gruenefeld, Uwe; Mayer, Sven
TI  - XR@ISS - It Takes Two To Tango: Conflicts Between Users on the Reality-Virtuality Continuum and Their Bystanders.
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yu, Difeng; Zhou, Qiushi; Dingler, Tilman; Velloso, Eduardo; Goncalves, Jorge
TI  - Blending On-Body and Mid-Air Interaction in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00081
ER  - 

TY  - NA
AU  - Cheng, Lung-Pan; Marwecki, Sebastian; Baudisch, Patrick
TI  - UIST - Mutual Human Actuation
PY  - 2017
AB  - Human actuation is the idea of using people to provide large-scale force feedback to users. The Haptic Turk system, for example, used four human actuators to lift and push a virtual reality user; TurkDeck used ten human actuators to place and animate props for a single user. While the experience of human actuators was decent, it was still inferior to the experience these people could have had, had they participated as a user. In this paper, we address this issue by making everyone a user. We introduce mutual human actuation, a version of human actuation that works without dedicated human actuators. The key idea is to run pairs of users at the same time and have them provide human actuation to each other. Our system, Mutual Turk, achieves this by (1) offering shared props through which users can exchange forces while obscuring the fact that there is a human on the other side, and (2) synchronizing the two users' timelines such that their way of manipulating the shared props is consistent across both virtual worlds. We demonstrate mutual human actuation with an example experience in which users pilot kites though storms, tug fish out of ponds, are pummeled by hail, battle monsters, hop across chasms, push loaded carts, and ride in moving vehicles.
SP  - 797
EP  - 805
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126667
ER  - 

TY  - NA
AU  - Ens, Barrett; Quigley, Aaron; Yeo, Hui-Shyong; Irani, Pourang; Piumsomboon, Thammathip; Billinghurst, Mark
TI  - SIGGRAPH ASIA (Posters) - Exploring mixed-scale gesture interaction
PY  - 2017
AB  - This paper presents ongoing work toward a design exploration for combining microgestures with other types of gestures within the greater lexicon of gestures for computer interaction. We describe three prototype applications that show various facets of this multi-dimensional design space. These applications portray various tasks on a Hololens Augmented Reality display, using different combinations of wearable sensors.
SP  - 27
EP  - NA
JF  - SIGGRAPH Asia 2017 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3145690.3145740
ER  - 

TY  - NA
AU  - Miyata, Natsuki; Honoki, Takehiro; Maeda, Yusuke; Endo, Yui; Tada, Mitsunori; Sugiura, Yuta
TI  - UIST (Adjunct Volume) - Wrap & Sense: Grasp Capture by a Band Sensor
PY  - 2016
AB  - This paper proposes a bare hand grasp observation system named Wrap & Sense. We built a band type sensing equipment composed of infrared distance sensors placed in an array. The sensor band is attached to a target object with all sensors directed along the object surface and detects the hand side edge with respect to the object. Assuming type of grasp as 'power grasp', the whole hand posture can be determined according to the 3D shape of the object. Three types of application are shown as proof-of-concept.
SP  - 87
EP  - 89
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985713
ER  - 

TY  - CHAP
AU  - Roy, Quentin; Zakaria, Camelia; Perrault, Simon T.; Nancel, Mathieu; Kim, Wonjung; Misra, Archan; Cockburn, Andy
TI  - INTERACT (3) - A Comparative Study of Pointing Techniques for Eyewear Using a Simulated Pedestrian Environment
PY  - 2019
AB  - Eyewear displays allow users to interact with virtual content displayed over real-world vision, in active situations like standing and walking. Pointing techniques for eyewear displays have been proposed, but their social acceptability, efficiency, and situation awareness remain to be assessed. Using a novel street-walking simulator, we conducted an empirical study of target acquisition while standing and walking under different levels of street crowdedness. We evaluated three phone-based eyewear pointing techniques: indirect touch on a touchscreen, and two in-air techniques using relative device rotations around forward and a downward axes. Direct touch on a phone, without eyewear, was used as a control condition. Results showed that indirect touch was the most efficient and socially acceptable technique, and that in-air pointing was inefficient when walking. Interestingly, the eyewear displays did not improve situation awareness compared to the control condition. We discuss implications for eyewear interaction design.
SP  - 625
EP  - 646
JF  - Human-Computer Interaction – INTERACT 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29387-1_36
ER  - 

TY  - JOUR
AU  - Chheang, Vuthea; Jeong, Sangkwon; Lee, Gookhwan; Ha, Jong-Sung; Yoo, Kwan-Hee
TI  - Natural embedding of live actors and entities into 360° virtual reality scenes
PY  - 2018
AB  - This paper is concerned with techniques for directly embedding moving objects in the real world into 360° virtual reality scenes captured by 360° camera, DSLR camera, smartphone, and others. For more natural embedding, we present a cylindrical mapping methodology based on a proposed international standard model for mixed and augmented reality, in which the living physical objects are called live actors and entities. Our experiments illustrate the realistic movements and interactions of live actors and entities that are embedded into 360° virtual reality scenes.
SP  - 5655
EP  - 5677
JF  - The Journal of Supercomputing
VL  - 76
IS  - 7
PB  - 
DO  - 10.1007/s11227-018-2615-z
ER  - 

TY  - JOUR
AU  - Hu, Yonghui; Yan, Yong; Efstratiou, Christos; Vela-Orte, David
TI  - Quantitative Shape Measurement of an Inflatable Rubber Dam Using an Array of Inertial Measurement Units
PY  - 2021
AB  - Shape measurement plays an important role in the condition monitoring and operation control of inflatable rubber dams. This article presents a method to measure the cross-sectional shape of a rubber dam using an array of inertial measurement units (IMUs) placed on the circumference of the dam. Accelerometer and gyroscope measurements are combined using an adaptive complementary filter to determine the tangent angles of the dam circumference. The adaptive complementary filter adjusts the weights of the accelerometer and gyroscope measurements dynamically in order to reduce the uncertainty in orientation estimation due to external acceleration under dynamic conditions. A natural cubic spline that interpolates the measured tangent angles at discrete locations is used to represent the tangent angles along the dam circumference as a continuous function of the arc length. Finally, the cross-sectional shape is reconstructed by integrating the continuous tangent angle function along the circumference of the dam. Experimental assessment of the measurement system was performed on a purpose-built test rig using a digital camera as a reference measuring device. Results under a typical static condition show that the measured and reference shapes agree well with each other, with a similarity index being 3.74%, a mismatch distance of the last IMU node being 12.3 mm, and a relative error of height measurement being −2.44%. Under dynamic conditions, the measurement results deteriorate due to external acceleration, but considerable improvement is achieved in comparison with an accelerometer-only approach. In addition, the elimination of faulty nodes from shape reconstruction has negligible influence on the results, suggesting that the measurement system enjoys a high degree of fault tolerance.
SP  - 1
EP  - 10
JF  - IEEE Transactions on Instrumentation and Measurement
VL  - 70
IS  - NA
PB  - 
DO  - 10.1109/tim.2021.3061244
ER  - 

TY  - NA
AU  - Antoine, Axel; Malacria, Sylvain; Casiez, Géry
TI  - CHI - ForceEdge: Controlling Autoscroll on Both Desktop and Mobile Computers Using the Force
PY  - 2017
AB  - Operating systems support autoscroll to allow users to scroll a view while in dragging mode: the user moves the pointer near the window's edge to trigger an "automatic" scrolling whose rate is typically proportional to the distance between the pointer and the window's edge. This approach suffers from several problems, especially when the window is maximized, resulting in a very limited space around it. Another problem is that for some operations, such as object drag-and-drop, the source and destination might be located in different windows, making it complicated for the computer to understand user's intention. In this paper, we present ForceEdge, a novel autoscroll technique relying on touch surfaces with force-sensing capabilities to alleviate the problems related to autoscroll. We report on the results of three controlled experiments showing that it improves over macOS and iOS systems baselines for top-to-bottom select and move tasks.
SP  - 3281
EP  - 3292
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025605
ER  - 

TY  - NA
AU  - Loureiro, João; Rangarajan, Raghuraman; Nikolic, Borislav; Indrusiak, Leandro Soares; Tovar, Eduardo
TI  - RTCSA - Real-time dense wired sensor network based on traffic shaping
PY  - 2017
AB  - XDense is a novel wired 2D-mesh grid sensor network system for application scenarios that benefit from densely deployed sensing (e.g. thousands of sensors per square meter). It was conceived for closed-loop cyber-physical systems (CPS) that require real-time actuation, like active flow control (AFC) on aircraft wing surfaces. XDense communication and distributed processing capabilities are designed such that they enable to extract complex features within bounded time and in a responsive manner. In this paper we tackle the issue of deterministic behavior of XDense. We present a methodology that uses traffic shaping heuristics to guarantee bounded communication delays and the fulfillment of memory requirements. We evaluate the model for varied network configurations and workload, and demonstrate the effectiveness of running real-time applications supported on XDense.
SP  - 1
EP  - 10
JF  - 2017 IEEE 23rd International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/rtcsa.2017.8046307
ER  - 

TY  - NA
AU  - Samudio, David I.; LaToza, Thomas D.
TI  - Barriers in Front-End Web Development
PY  - 2022
AB  - Developers building web applications constantly face challenges, particularly in working with complex APIs. In response, developers often turn to Stack Overflow, offering a window into the programming barriers developers face. We examined 301 posts on Stack Overflow related to front-end web development and systematically characterized the challenges present in these posts. We found that most challenges reflected not a request for new code or an explanation of an error message but a request about how a specific code snippet might be edited to make its behavior as desired. Many challenges also reflected an underlying need to gather information about how specific code idioms are implemented within a framework or library. We identified 28 barriers developers face in front-end web development. Our findings suggest opportunities for facilitating more effective interactions with complex APIs through new types of programming content and tools that better address barriers in working with code idioms.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833127
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles
TI  - "Let's Meet and Work it Out": Understanding and Mitigating Encountered-Type of Haptic Devices Failure Modes in VR
PY  - 2022
AB  - Encountered-type of Haptic devices (ETHD) are robotic interfaces physically overlaying virtual counterparts prior to a user interaction in Virtual Reality. They theoretically reliably provide haptics in Virtual environments, yet they raise several intrinsic design challenges to properly display rich haptic feedback and interactions in VR applications. In this paper, we use a Failure Mode and Effects Analysis (FMEA) approach to identify, organise and analyse the failure modes and their causes in the different stages of an ETHD scenario and highlight appropriate solutions from the literature to mitigate them. We help justify these interfaces&#x2019; lack of deployment, to ultimately identify guidelines for future ETHD designers.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00055
ER  - 

TY  - JOUR
AU  - Jernigan, William; Horvath, Amber; Lee, Michael; Burnett, Margaret; Cuilty, Taylor; Kuttal, Sandeep Kaur; Peters, Anicia; Kwan, Irwin; Bahmani, Faezeh; Ko, Andrew L.; Mendez, Christopher; Oleson, Alannah
TI  - General principles for a Generalized Idea Garden
PY  - 2017
AB  - NA
SP  - 51
EP  - 65
JF  - Journal of Visual Languages & Computing
VL  - 39
IS  - NA
PB  - 
DO  - 10.1016/j.jvlc.2017.04.005
ER  - 

TY  - JOUR
AU  - Huang, Jian-Lin; Zhakypov, Zhenishbek; Sonar, Harshal Arun; Paik, Jamie
TI  - A reconfigurable interactive interface for controlling robotic origami in virtual environments
PY  - 2018
AB  - Origami shape transformation is dictated by predefined folding patterns and their folding sequence. The working principle of robotic origami is based on the same principle: we design quasi-two-dime...
SP  - 629
EP  - 647
JF  - The International Journal of Robotics Research
VL  - 37
IS  - 6
PB  - 
DO  - 10.1177/0278364918769157
ER  - 

TY  - NA
AU  - Ogawa, Nami; Narumi, Takuji; Kuzuoka, Hideaki; Hirose, Michitaka
TI  - CHI - Do You Feel Like Passing Through Walls?: Effect of Self-Avatar Appearance on Facilitating Realistic Behavior in Virtual Environments
PY  - 2020
AB  - Preventing users from walking through virtual boundaries (e.g., walls) is an important issue to be addressed in room-scale virtual environments (VEs), considering the safety and design limitations. Sensory feedback from wall collisions has been shown to be effective; however, it can disrupt the immersion. We assumed that a greater sense of presence would discourage users from walking through walls and conducted a two-factor between-subjects experiment (N = 92) that controls the anthropomorphism (realistic or abstract) and visibility (full-body or hand-only) of self-avatars. We analyzed the participants' behaviors and the moment they first penetrated the wall in game-like VEs that gradually instigated participants to penetrate the walls. The results showed that the realistic full-body self-avatar was the most effective for discouraging the participants from penetrating the walls. Furthermore, the participants with lower presence tended to walk through the walls sooner. This study can contribute to applications that require realistic user responses in VEs.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376562
ER  - 

TY  - JOUR
AU  - Gentile, Vito; Khamis, Mohamed; Milazzo, Fabrizio; Sorce, Salvatore; Malizia, Alessio; Alt, Florian
TI  - Predicting mid-air gestural interaction with public displays based on audience behaviour
PY  - 2020
AB  - Abstract Knowledge about the expected interaction duration and expected distance from which users will interact with public displays can be useful in many ways. For example, knowing upfront that a certain setup will lead to shorter interactions can nudge space owners to alter the setup. If a system can predict that incoming users will interact at a long distance for a short amount of time, it can accordingly show shorter versions of content (e.g., videos/advertisements) and employ at-a-distance interaction modalities (e.g., mid-air gestures). In this work, we propose a method to build models for predicting users’ interaction duration and distance in public display environments, focusing on mid-air gestural interactive displays. First, we report our findings from a field study showing that multiple variables, such as audience size and behaviour, significantly influence interaction duration and distance. We then train predictor models using contextual data, based on the same variables. By applying our method to a mid-air gestural interactive public display deployment, we build a model that predicts interaction duration with an average error of about 8 s, and interaction distance with an average error of about 35 cm. We discuss how researchers and practitioners can use our work to build their own predictor models, and how they can use them to optimise their deployment.
SP  - 102497
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 144
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2020.102497
ER  - 

TY  - NA
AU  - Valkov, Dimitar; Flagge, Steffen
TI  - SUI - Smooth immersion: the benefits of making the transition to virtual environments a continuous process
PY  - 2017
AB  - In this paper we discuss the benefits and the limitations, as well as different implementation options for smooth immersion into a HMD-based IVE. We evaluated our concept in a preliminary user study, in which we have tested users' awareness, reality judgment and experience in the IVE, when using different transition techniques to enter it. Our results show that a smooth transition to the IVE improves the awareness of the user and may increase the perceived interactivity of the system.
SP  - 12
EP  - 19
JF  - Proceedings of the 5th Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131277.3132183
ER  - 

TY  - NA
AU  - Yang, Qiang; Zou, Yongpan; Meng, Zhao; Jiawei, Lin; Wu, Kaishun
TI  - MobiQuitous - ArmIn: Explore the Feasibility of Designing a Text-entry Application Using EMG Signals
PY  - 2018
AB  - EMG is becoming an emerging interface for human-computer interface and has been applied to gesture recognition in previous work. However, those existing EMG-based interfaces can only recognize gestures at a coarse-grained level such as hand and arm gestures, which constraints their usage in applications involving fine-grained activities such as text entry via keystrokes. As a result, in this paper, we attempt to push the limit of existing EMG-based interfaces and propose the first wearable text-entry system, named ArmIn, with EMG signals. ArmIn is designed to recognize keystroke gestures with the help of a finger on printed and physical keyboards. We implement ArmIn using commodity EMG sensors and custom hardware board, and conduct experiments to evaluate its performance. By carefully designing the data processing scheme, ArmIn can recognize keystrokes on both kinds of keyboard, with 89.5% and 87.5% accuracy respectively, when it is worn on a user's left arm.
SP  - 117
EP  - 126
JF  - Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3286978.3287030
ER  - 

TY  - NA
AU  - Han, Ping-Hsuan; Chen, Yang-Sheng; Hsieh, Chiao-En; Wang, Hao-Cheng; Hung, Yi-Ping
TI  - WHC - Hapmosphere: Simulating the Weathers for Walking Around in Immersive Environment with Haptics Feedback
PY  - 2019
AB  - With the advance of locomotion techniques and virtual reality head-mount display (VR-HMD), the users can explore the virtual world by moving around in the virtual environment. Although VR-HMD can provide immersive visual and auditory feedback, without haptic technologies, the users cannot perceive the multiple tactile sensations from the virtual environment. In this paper, we present Hapmosphere, a multiple tactile display for simulating weather in the immersive environment, which can provide thermal, wind, and humidity feedback simultaneously in a room-scale space. This system consists of a steerable structure and haptic modules rigged on the ceiling, so the users can walk around in the physical area. Furthermore, to evaluate the ability of the tactile display, we propose seven kinds of weathers via design consideration and conduct a user study to investigate the feasibility. In our study, the result has shown the potential of utilizing this haptic technique to enhance the immersive experience. Finally, we highlight the limitations and challenges of designing an immersive environment with haptic feedback.
SP  - 247
EP  - 252
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816140
ER  - 

TY  - NA
AU  - Wang, Chiu-Hsuan; Tsai, Chia-En; Yong, Seraphina; Chan, Liwei
TI  - UIST - Slice of Light: Transparent and Integrative Transition Among Realities in a Multi-HMD-User Environment
PY  - 2020
AB  - This work presents Slice of Light, a visualization design created to enhance transparency and integrative transition between realities of Head-Mounted Display (HMD) users sharing the same physical environment. Targeted at reality-guests, Slice of Light's design enables guests to view other HMD users' interactions contextualized in their own virtual environments while allowing the guests to navigate among these virtual environments. In this paper, we detail our visualization design and the implementation. We demonstrate Slice of Light with a block-world construction scenario that involves a multi-HMD-user environment. VR developer and HCI expert participants were recruited to evaluate the scenario, and responded positively to Slice of Light. We discuss their feedback, our design insights, and the limitations of this work.
SP  - 805
EP  - 817
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415868
ER  - 

TY  - NA
AU  - Alimadadi, Saba
TI  - SIGSOFT FSE - Understanding behavioural patterns in JavaScript
PY  - 2016
AB  - JavaScript is one of the most popular programming languages. How- ever, understanding the dynamic behaviour of JavaScript apps is challenging in practice. There are many factors that hinder JavaScript comprehension, such as its dynamic, asynchronous, and event- driven nature, the dynamic interplay between JavaScript and the Document Object Model, and the asynchronous communication between client and server. In this research work, we have already proposed methods for understanding event-based and asynchronous JavaScript behaviour. To enhance the scalability of our methods, we propose a new technique that adopts bio-informatics algorithms to extract sequences of actions from execution traces that form higher-level patterns.
SP  - 1076
EP  - 1078
JF  - Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2950290.2983947
ER  - 

TY  - JOUR
AU  - Hibschman, Joshua; Gergle, Darren; O'Rourke, Eleanor; Zhang, Haoqi
TI  - Isopleth: Supporting Sensemaking of Professional Web Applications to Create Readily Available Learning Experiences
PY  - 2019
AB  - Online resources can help novice developers learn basic programming skills, but few resources support progressing from writing working code to learning professional web development practices. We address this gap by advancing Readily Available Learning Experiences, a conceptual approach for transforming all professional web applications into opportunities for authentic learning. This article presents Isopleth, a web-based platform that helps learners make sense of complex code constructs and hidden asynchronous relationships in professional web code. Isopleth embeds sensemaking scaffolds informed by the learning sciences to (1) expose hidden functional and event-driven relationships, (2) surface functionally related slices of code, and (3) support learners manipulating the provided code representations. To expose event-driven relationships, Isopleth implements a novel technique called Serialized Deanonymization to determine and visualize asynchronous functional relationships. To evaluate Isopleth, we conducted a case study across 12 professional websites and a user study with 14 junior and senior developers. Results show that Isopleth’s sensemaking scaffolds helped to surface implementation approaches in event binding, web application design, and complex interactive features across a range of complex professional web applications. Moreover, Isopleth helped junior developers improve the accuracy of their conceptual models of how features are implemented by 31% on average.
SP  - 16
EP  - 42
JF  - ACM Transactions on Computer-Human Interaction
VL  - 26
IS  - 3
PB  - 
DO  - 10.1145/3310274
ER  - 

TY  - NA
AU  - Bajammal, Mohammad; Mesbah, Ali
TI  - ICSE - Semantic Web Accessibility Testing via Hierarchical Visual Analysis
PY  - 2021
AB  - Web accessibility, the design of web apps to be usable by users with disabilities, impacts millions of people around the globe. Although accessibility has traditionally been a marginal afterthought that is often ignored in many software products, it is increasingly becoming a legal requirement that must be satisfied. While some web accessibility testing tools exist, most only perform rudimentary syntactical checks that do not assess the more important high-level semantic aspects that users with disabilities rely on. Accordingly, assessing web accessibility has largely remained a laborious manual process requiring human input. In this paper, we propose an approach, called AxeRay, that infers semantic groupings of various regions of a web page and their semantic roles. We evaluate our approach on 30 real-world websites and assess the accuracy of semantic inference as well as the ability to detect accessibility failures. The results show that AxeRay achieves, on average, an F-measure of 87% for inferring semantic groupings, and is able to detect accessibility failures with 85% accuracy.
SP  - 1610
EP  - 1621
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse43902.2021.00143
ER  - 

TY  - JOUR
AU  - Loureiro, João; Rangarajan, Raghuraman; Nikolic, Borislav; Indrusiak, Leandro Soares; Tovar, Eduardo
TI  - Extensive Analysis of a Real-Time Dense Wired Sensor Network Based on Traffic Shaping
PY  - 2019
AB  - XDense is a novel wired 2D mesh grid sensor network system for application scenarios that benefit from densely deployed sensing (e.g., thousands of sensors per square meter). It was conceived for cyber-physical systems that require real-time sensing and actuation, like active flow control on aircraft wing surfaces. XDense communication and distributed processing capabilities are designed to enable complex feature extraction within bounded time and in a responsive manner. In this article, we tackle the issue of deterministic behavior of XDense. We present a methodology that uses traffic-shaping heuristics to guarantee bounded communication delays and the fulfillment of memory requirements. We evaluate the model for varied network configurations and workload, and present a comparative performance analysis in terms of link utilization, queue size, and execution time. With the proposed traffic-shaping heuristics, we endow XDense with the capabilities required for real-time applications.
SP  - 1
EP  - 27
JF  - ACM Transactions on Cyber-Physical Systems
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3230872
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Nakayama, Ryosuke; Liu, Dan; Kakehi, Yasuaki; Gross, Mark D.; Leithinger, Daniel
TI  - Tangible and Embedded Interaction - LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces
PY  - 2020
AB  - Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.
SP  - 143
EP  - 151
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374941
ER  - 

TY  - NA
AU  - Röddiger, Tobias; Clarke, Christopher; Wolffram, Daniel; Budde, Matthias; Beigl, Michael
TI  - CHI - EarRumble: Discreet Hands- and Eyes-Free Input by Voluntary Tensor Tympani Muscle Contraction
PY  - 2021
AB  - We explore how discreet input can be provided using the tensor tympani - a small muscle in the middle ear that some people can voluntarily contract to induce a dull rumbling sound. We investigate the prevalence and ability to control the muscle through an online questionnaire (N=192) in which 43.2% of respondents reported the ability to “ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can be used to detect voluntary tensor tympani contraction in the sealed ear canal. This data was used to train a classifier based on three simple ear rumble “gestures” which achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction, grounded in three manual, dual-task application scenarios (N=8). This highlights the applicability of EarRumble as a low-effort and discreet eyes- and hands-free interaction technique that users found “magical” and “almost telepathic”.
SP  - 743
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445205
ER  - 

TY  - NA
AU  - Chi, Pei-Yu; Hu, Sen-Po; Li, Yang
TI  - CHI - Doppio: Tracking UI Flows and Code Changes for App Development
PY  - 2018
AB  - Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes.
SP  - 455
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174029
ER  - 

TY  - NA
AU  - Sra, Misha; Schmandt, Chris
TI  - MetaSpace II: Object and full-body tracking for interaction and navigation in social VR
PY  - 2015
AB  - Author(s): Sra, Misha; Schmandt, Chris | Abstract: MetaSpace II (MS2) is a social Virtual Reality (VR) system where multiple users can not only see and hear but also interact with each other, grasp and manipulate objects, walk around in space, and get tactile feedback. MS2 allows walking in physical space by tracking each user's skeleton in real-time and allows users to feel by employing passive haptics i.e., when users touch or manipulate an object in the virtual world, they simultaneously also touch or manipulate a corresponding object in the physical world. To enable these elements in VR, MS2 creates a correspondence in spatial layout and object placement by building the virtual world on top of a 3D scan of the real world. Through the association between the real and virtual world, users are able to walk freely while wearing a head-mounted device, avoid obstacles like walls and furniture, and interact with people and objects. Most current virtual reality (VR) environments are designed for a single user experience where interactions with virtual objects are mediated by hand-held input devices or hand gestures. Additionally, users are only shown a representation of their hands in VR floating in front of the camera as seen from a first person perspective. We believe, representing each user as a full-body avatar that is controlled by natural movements of the person in the real world (see Figure 1d), can greatly enhance believability and a user's sense immersion in VR.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Camisetty, Akhilesh; Chandurkar, Chaitanya; Sun, Maoyuan; Koop, David
TI  - Enhancing Web-based Analytics Applications through Provenance
PY  - 2018
AB  - Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.
SP  - 131
EP  - 141
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865039
ER  - 

TY  - NA
AU  - Katsuragawa, Keiko; Wang, Ju; Shan, Ziyang; Ouyang, Ningshan; Abari, Omid; Vogel, Daniel
TI  - UIST - Tip-Tap: Battery-free Discrete 2D Fingertip Input
PY  - 2019
AB  - We describe Tip-Tap, a wearable input technique that can be implemented without batteries using a custom RFID tag. It recognizes 2-dimensional discrete touch events by sensing the intersection between two arrays of contact points: one array along the index fingertip and the other along the thumb tip. A formative study identifies locations on the index finger that are reachable by different parts of the thumb tip, and the results determine the pattern of contacts points used for the technique. Using a reconfigurable 3x3 evaluation device, a second study shows eyes-free accuracy is 86% after a very short period, and adding bumpy or magnetic passive haptic feedback to contacts is not necessary. Finally, two battery-free prototypes using a new RFID tag design demonstrates how Tip-Tap can be implemented in a glove or tattoo form factor.
SP  - 1045
EP  - 1057
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347907
ER  - 

TY  - NA
AU  - Lindlbauer, David; Mueller, Joerg; Alexa, Marc
TI  - UIST - Changing the Appearance of Physical Interfaces Through Controlled Transparency
PY  - 2016
AB  - We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.
SP  - 425
EP  - 435
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984556
ER  - 

TY  - NA
AU  - Vadgama, Nirzaree; Steimle, Jürgen
TI  - Tangible and Embedded Interaction - Flexy: Shape-Customizable, Single-Layer, Inkjet Printable Patterns for 1D and 2D Flex Sensing
PY  - 2017
AB  - We contribute a new technique for fabricating highly customized 1D and 2D flex sensing surfaces on thin and flexible substrates. It enables designers and makers to easily, quickly and inexpensively realize thin physical objects in custom shapes with an embedded deformation sensor. The deformation sensor is digitally designed and then fabricated with a single layer of conductive material in a single pass, using an off-the-shelf inkjet printer. We establish a design space and investigate how to realize flex sensing surfaces of highly varied geometries. In a technical evaluation, we demonstrate the technical feasibility of such sensors and investigate their response. Lastly, we demonstrate the practical applicability for tangible interfaces by presenting five example applications.
SP  - 153
EP  - 162
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3024989
ER  - 

TY  - NA
AU  - Wang, Yangde; Qiu, Weidong; Xie, Yuming; Zha, Yan
TI  - PatternMonitor: a whole pipeline with a much higher level of automation for guessing Android lock pattern based on videos.
PY  - 2021
AB  - Pattern lock is a general technique used to realize identity authentication and access authorization on mobile terminal devices such as Android platform devices, but it is vulnerable to the attack proposed by recent researches that exploit information leaked by users while drawing patterns. However, the existing attacks on pattern lock are environmentally sensitive, and rely heavily on manual work, which constrains the practicability of these attack approaches. To attain a more practical attack, this paper designs the PatternMonitor, a whole pipeline with a much higher level of automation system againsts pattern lock, which extracts the guessed candidate patterns from a video containing pattern drawing: instead of manually cutting the target video and setting thresholds, it first employs recognition models to locate the target phone and keypoints of pattern drawing hand, which enables the gesture can be recognized even when the fingertips are shaded. Then, we extract the frames from the video where the drawing starts and ends. These pre-processed frames are inputs of target tracking model to generate trajectories, and further transformed into possible candidate patterns by performing our designed algorithm. To the best of our knowledge, our work is the first attack system to generate candidate patterns by only relying on hand movement instead of accurate fingertips capture. The experimental results demonstrates that our work is as accurate as previous work, which gives more than 90\% success rate within 20 attempts.
SP  - NA
EP  - NA
JF  - arXiv: Cryptography and Security
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Ma, Ying; Xu, Tianpei; Kim, Kangchul
TI  - Two-Stream Mixed Convolutional Neural Network for American Sign Language Recognition.
PY  - 2022
AB  - The Convolutional Neural Network (CNN) has demonstrated excellent performance in image recognition and has brought new opportunities for sign language recognition. However, the features undergo many nonlinear transformations while performing the convolutional operation and the traditional CNN models are insufficient in dealing with the correlation between images. In American Sign Language (ASL) recognition, J and Z with moving gestures bring recognition challenges. This paper proposes a novel Two-Stream Mixed (TSM) method with feature extraction and fusion operation to improve the correlation of feature expression between two time-consecutive images for the dynamic gestures. The proposed TSM-CNN system is composed of preprocessing, the TSM block, and CNN classifiers. Two consecutive images in the dynamic gesture are used as inputs of streams, and resizing, transformation, and augmentation are carried out in the preprocessing stage. The fusion feature map obtained by addition and concatenation in the TSM block is used as inputs of the classifiers. Finally, a classifier classifies images. The TSM-CNN model with the highest performance scores depending on three concatenation methods is selected as the definitive recognition model for ASL recognition. We design 4 CNN models with TSM: TSM-LeNet, TSM-AlexNet, TSM-ResNet18, and TSM-ResNet50. The experimental results show that the CNN models with the TSM are better than models without TSM. The TSM-ResNet50 has the best accuracy of 97.57% for MNIST and ASL datasets and is able to be applied to a RGB image sensing system for hearing-impaired people.
SP  - 5959
EP  - 5959
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 16
PB  - 
DO  - 10.3390/s22165959
ER  - 

TY  - NA
AU  - Goguey, Alix; Malacria, Sylvain; Gutwin, Carl
TI  - CHI - Improving Discoverability and Expert Performance in Force-Sensitive Text Selection for Touch Devices with Mode Gauges
PY  - 2018
AB  - Text selection on touch devices can be a difficult task for users. Letters and words are often too small to select directly, and the enhanced interaction techniques provided by the OS -- magnifiers, selection handles, and methods for selecting at the character, word, or sentence level -- often lead to as many usability problems as they solve. The introduction of force-sensitive touchscreens has added another enhancement to text selection (using force for different selection modes); however, these modes are difficult to discover and many users continue to struggle with accurate selection. In this paper we report on an investigation of the design of touch-based and force-based text selection mechanisms, and describe two novel text-selection techniques that provide improved discoverability, enhanced visual feedback, and a higher performance ceiling for experienced users. Two evaluations show that one design successfully combined support for novices and experts, was never worse than the standard iOS technique, and was preferred by participants.
SP  - 477
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174051
ER  - 

TY  - NA
AU  - Torres, César I.; Chang, Jessica; Patel, Advaita; Paulos, Eric
TI  - Conference on Designing Interactive Systems - Phosphenes: Crafting Resistive Heaters within Thermoreactive Composites
PY  - 2019
AB  - Hybrid practices are emerging that integrate creative materials like paint, clay, and cloth with intangible immaterials like computation, electricity, and heat. This work aims to expand the design potential of immaterial elements by transforming them into manipulatable, observable and intuitive materials. We explore one such immaterial, electric heat, and develop a maker-friendly fabrication pipeline and crafting support tool that allows users to experientially compose resistive heaters that generate heat spatially and temporally. These heaters are then used to couple heat and thermoreactive materials in a class of artifacts we term Thermoreactive Composites (TrCs). In a formal user study, we observe how designing fabrication workflows along dimensions of composability and perceivability better matches the working styles of material practitioners without domain knowledge of electronics. Through exemplar artifacts, we demonstrate the potential of heat as a creative material and discuss implications for immaterials used within creative practices.
SP  - 907
EP  - 919
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322375
ER  - 

TY  - JOUR
AU  - Althomali, Ibrahim; Kapfhammer, Gregory M.; McMinn, Phil
TI  - Automated visual classification of DOM-based presentation failure reports for responsive web pages
PY  - 2021
AB  - Since it is common for the users of a web page to access it through a wide variety of devices—including desktops, laptops, tablets and phones—web developers rely on responsive web design (RWD) principles and frameworks to create sites that are useful on all devices. A correctly implemented responsive web page adjusts its layout according to the viewport width of the device in use, thereby ensuring that its design suitably features the content. Since the use of complex RWD frameworks often leads to web pages with hard‐to‐detect responsive layout failures (RLFs), developers employ testing tools that generate reports of potential RLFs. Since testing tools for responsive web pages, like ReDeCheck, analyse a web page representation called the Document Object Model (DOM), they may inadvertently flag concerns that are not human visible, thereby requiring developers to manually confirm and classify each potential RLF as a true positive (TP), false positive (FP), or non‐observable issue (NOI)—a process that is time consuming and error prone. The conference version of this paper presented Viser, a tool that automatically classified three types of RLFs reported by ReDeCheck. Since Viser was not designed to automatically confirm and classify two types of RLFs that ReDeCheck's DOM‐based analysis could surface, this paper introduces Verve, a tool that automatically classifies all RLF types reported by ReDeCheck. Along with manipulating the opacity of HTML elements in a web page, as does Viser, the Verve tool also uses histogram‐based image comparison to classify RLFs in web pages. Incorporating both the 25 web pages used in prior experiments and 20 new pages not previously considered, this paper's empirical study reveals that Verve's classification of all five types of RLFs frequently agrees with classifications produced manually by humans. The experiments also reveal that Verve took on average about 4 s to classify any of the RLFs among the 469 reported by ReDeCheck. Since this paper demonstrates that classifying an RLF as a TP, FP, or NOI with Verve, a publicly available tool, is less subjective and error prone than the same manual process done by a human web developer, we argue that it is well‐suited for supporting the testing of complex responsive web pages.
SP  - NA
EP  - NA
JF  - Software Testing, Verification and Reliability
VL  - 31
IS  - 4
PB  - 
DO  - 10.1002/stvr.1756
ER  - 

TY  - NA
AU  - Nebeling, Michael; Lewis, Katy; Chang, Yu-Cheng; Zhu, Lihan; Chung, Michelle; Wang, Piaoyang; Nebeling, Janet
TI  - CHI - XRDirector: A Role-Based Collaborative Immersive Authoring System
PY  - 2020
AB  - Immersive authoring is an increasingly popular technique to design AR/VR scenes because design and testing can be done concurrently. Most existing systems, however, are single-user and limited to either AR or VR, thus constrained in the interaction techniques. We present XRDirector, a role-based collaborative immersive authoring system that enables designers to freely express interactions using AR and VR devices as puppets to manipulate virtual objects in 3D physical space. In XRDirector, we adapt roles known from filmmaking to structure the authoring process and help coordinate multiple designers in immersive authoring tasks. We study how novice AR/VR creators can take advantage of the roles and modes in XRDirector to prototype complex scenes with animated 3D characters, light effects, and camera movements, and also simulate interactive system behavior in a Wizard of Oz style. XRDirector's design was informed by case studies around complex 3D movie scenes and AR/VR games, as well as workshops with novice AR/VR creators. We show that XRDirector makes it easier and faster to create AR/VR scenes without the need for coding, characterize the issues in coordinating designers between AR and VR, and identify the strengths and weaknesses of each role and mode to mitigate the issues.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376637
ER  - 

TY  - NA
AU  - Ens, Barrett; Quigley, Aaron; Yeo, Hui-Shyong; Irani, Pourang; Billinghurst, Mark
TI  - SIGGRAPH ASIA Mobile Graphics and Interactive Applications - Multi-scale gestural interaction for augmented reality
PY  - 2017
AB  - We present a multi-scale gestural interface for augmented reality applications. With virtual objects, gestural interactions such as pointing and grasping can be convenient and intuitive, however they are imprecise, socially awkward, and susceptible to fatigue. Our prototype application uses multiple sensors to detect gestures from both arm and hand motions (macro-scale), and finger gestures (micro-scale). Micro-gestures can provide precise input through a belt-worn sensor configuration, with the hand in a relaxed posture. We present an application that combines direct manipulation with microgestures for precise interaction, beyond the capabilities of direct manipulation alone.
SP  - 3132808
EP  - NA
JF  - SIGGRAPH Asia 2017 Mobile Graphics & Interactive Applications on   - SA '17
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132787.3132808
ER  - 

TY  - NA
AU  - Samudio, David I.
TI  - VL/HCC - Idiomata: Direct manipulation of code through idiomatic views
PY  - 2017
AB  - Currently, front-end web developers spend countless hours overcoming programming challenges while debugging unexpected asynchronous behaviors, writing code to interact with a framework's API, or fixing faults. Such problems demand rethinking programming tools, and for that, we systematically analyzed 301 posts from Stack Overflow, and sought to identify the programming activities developers struggled with and find information barriers that were the primary cause of their challenges. Our results reveal that developers most commonly post questions to change behavior of existing code. Further, specific discussions or changes in code fragments that conceptually have the same intention (e.g. both document.querySelector(“x”) and $(“x”) retrieve references from the DOM), often presented reoccurring challenges that demanded different reasoning to obtain equivalent runtime information (e.g. check if a DOM reference matches what is intended in the output). Code idioms are those concepts underlying patterns in code and by categorizing them, we were able to associate their challenges to execution information barriers that prevent developers from observing relevant state and behavior at runtime (e.g. mapping a DOM reference in a code fragment to its element in the output), and selection barriers that prevent developers from finding or choosing the correct object, method, or parameter values to achieve a desired behavior (e.g. generating code that references a DOM element).
SP  - 317
EP  - 318
JF  - 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2017.8103488
ER  - 

TY  - NA
AU  - Vasylevska, Khrystyna; Kaufmann, Hannes
TI  - 3DUI - Towards efficient spatial compression in self-overlapping virtual environments
PY  - 2017
AB  - Space available for any virtual reality experience is often strictly limited and abridges the virtual world to a size of a room. To extend the amount of virtual space accessible by walking within the same real workspace the methods of spatial compression were proposed. Scene manipulation with a controlled spatial overlap has been shown to be an efficient method. However, in order to apply space compression effectively for a dynamic, scalable and robust 3D user interface, it is important to study how the human perceives different layouts with overlapping spaces. In this paper, we explore the influence of the properties of the layout used on human spatial perception in a physically impossible spatial arrangement. Our first reported study focuses on the following parameters of the path within a simple self-overlapping layout: number of turns, relative door positions, sequences of counter- and clockwise turns, symmetry and asymmetry of the path used. In addition, in the second study we explore the effect of path smoothing by substituting the right-angled corridors by smooth curves. Our studies show that usage of the smooth curved corridors is more beneficial for spatial compression than the conventional right-angled approach.
SP  - 12
EP  - 21
JF  - 2017 IEEE Symposium on 3D User Interfaces (3DUI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/3dui.2017.7893312
ER  - 

TY  - NA
AU  - Kuznetsov, Stacey; Fernando, Piyum; Ritter, Emily; Barrett, Cassandra M; Weiler, Jennifer; Rohr, Marissa
TI  - Tangible and Embedded Interaction - Screenprinting and TEI: Supporting Engagement with STEAM through DIY Fabrication of Smart Materials
PY  - 2018
AB  - This paper focuses on manual screenprinting as a DIY fabrication technique for embedding interactive behavior onto a rage of substrates such as paper, fabric, plastic, wood, or vinyl. We frame screenprinting as a process that operates at the intersection of art, technology, and material science and iteratively examine its potential in two STEAM contexts. We conducted youth and adult workshops whereby participants worked with our low-cost thermochromic, UV-sensitive, and conductive screenprinting inks to develop a range of concepts and final projects. Our findings highlight several unique features of screenprinting: it affords a low barrier to entry for smart material fabrication, supports a collaborative maker practice, and scaffolds creative engagement with STEAM concepts. By being widely-accessible and substrate-agnostic, screenprinting presents exciting opportunities for TEI: DIY fabrication of smart materials in domains such as fine arts, information visualization, and slow technology; and bridging diverse disciplines through STEAM screenprinting initiatives at youth and adult levels.
SP  - 211
EP  - 220
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173253
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Khan, Arshad; Kruttwig, Klaus; Kraus, Tobias; Steimle, Jürgen
TI  - CHI - PhysioSkin: Rapid Fabrication of Skin-Conformal Physiological Interfaces
PY  - 2020
AB  - Advances in rapid prototyping platforms have made physiological sensing accessible to a wide audience. However, off-the-shelf electrodes commonly used for capturing biosignals are typically thick, non-conformal and do not support customization. We present PhysioSkin, a rapid, do-it-yourself prototyping method for fabricating custom multi-modal physiological sensors, using commercial materials and a commodity desktop inkjet printer. It realizes ultrathin skin-conformal patches (~1μm) and interactive textiles that capture sEMG, EDA and ECG signals. It further supports fabricating devices with custom levels of thickness and stretchability. We present detailed fabrication explorations on multiple substrate materials, functional inks and skin adhesive materials. Informed from the literature, we also provide design recommendations for each of the modalities. Evaluation results show that the sensor patches achieve a high signal-to-noise ratio. Example applications demonstrate the functionality and versatility of our approach for prototyping a next generation of physiological devices that intimately couple with the human body.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376366
ER  - 

TY  - NA
AU  - Lee, Jong-In; Kim, Sunjun; Fukumoto, Masaaki; Lee, Byungjoo
TI  - UIST - Reflector: Distance-Independent, Private Pointing on a Reflective Screen
PY  - 2017
AB  - Reflector is a novel direct pointing method that utilizes hidden design space on reflective screens. By aligning a part of the user's onscreen reflection with objects rendered on the screen, Reflector enables (1) distance-independent and (2) private pointing on commodity screens. Reflector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reflector's pointing performance was compared to today's major direct input devices: eye trackers and touchscreens. We demonstrate that Reflector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reflection, Reflector shows a shoulder surfing success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.
SP  - 351
EP  - 364
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126665
ER  - 

TY  - NA
AU  - Kobayashi, Masato; Kon, Yuki; Kajimoto, Hiroyuki
TI  - AH - Detection Threshold of the Height Difference between a Visual and Physical Step
PY  - 2019
AB  - In recent years, virtual reality (VR) applications that accompany real-space walking have become popular. In these applications, the expression of steps, such as a stairway, is a technical challenge. Preparing a real step with the same scale as that of the step in the VR space is one alternative; however, it is costly and impractical. We propose using a real step, but one physical step for the expression of various steps, by manipulating the viewpoint and foot position when ascending and descending real steps. The hypothesis is that the height of a step can be complemented to some extent visually, even if the heights of the real step and that in the VR space are different. In this paper, we first propose a viewpoint and foot position manipulation algorithm. T hen we measure the detection threshold of the height difference between the visual and physical step when ascending and descending the physical step using our manipulation algorithm. As a result, we found that the difference can be detected if there is a difference of approximately 1.0 cm between the VR space and the real space, irrespective of the height of the physical step.
SP  - 8
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311857
ER  - 

TY  - CHAP
AU  - Grubert, Jens
TI  - Mixed Reality Interaction Techniques
PY  - 2021
AB  - NA
SP  - 109
EP  - 129
JF  - Springer Handbooks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-67822-7_5
ER  - 

TY  - NA
AU  - Han, Ping-Hsuan; Hsieh, Chiao-En; Chen, Yang-Sheng; Hsiao, Jui-Chun; Lee, Kong-Chang; Ko, Sheng-Fu; Chen, Kuan-Wen; Chou, Chien-Hsing; Hung, Yi-Ping
TI  - SIGGRAPH Emerging Technologies - AoEs: enhancing teleportation experience in immersive environment with mid-air haptics
PY  - 2017
AB  - To alleviate cybersickness in the immersive virtual reality (VR), teleportation is a common method of moving around in virtual spaces. Although users can receive the visual and auditory feedbacks from their first-person perspective with the advances of immersive head-mounted displays (HMD), they do not have the haptic experience when they teleport to another environment. Based on the immersive HMD, many research groups have shown that haptic feedback is one of the important key to enhance the immersive experience in the virtual reality. However, to simulate the haptic feedback from different environments e.g. desert and snow, it require many devices in the real environment to simulate the sun, airflow, humidity and temperature. In this work, our main concept is to provide a haptic tower in the room-scale VR, which allow the game designers to enhance the player experience in the immersive environments. We present Area of Elements (AoEs), an integration device for simulating immersive environments with haptics, which can provide two kinds of virtual environments simultaneously for teleportation experience.
SP  - 3
EP  - NA
JF  - ACM SIGGRAPH 2017 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3084822.3084823
ER  - 

TY  - JOUR
AU  - Alimadadi, Saba; Zhong, Di; Madsen, Magnus; Tip, Frank
TI  - Finding broken promises in asynchronous JavaScript programs
PY  - 2018
AB  - Recently, promises were added to ECMAScript 6, the JavaScript standard, in order to provide better support for the asynchrony that arises in user interfaces, network communication, and non-blocking I/O. Using promises, programmers can avoid common pitfalls of event-driven programming such as event races and the deeply nested counterintuitive control ow referred to as “callback hell”. Unfortunately, promises have complex semantics and the intricate control– and data- ow present in promise-based code hinders program comprehension and can easily lead to bugs. The promise graph was proposed as a graphical aid for understanding and debugging promise-based code. However, it did not cover all promise-related features in ECMAScript 6, and did not present or evaluate any technique for constructing the promise graphs. In this paper, we extend the notion of promise graphs to include all promise-related features in ECMAScript 6, including default reactions, exceptions, and the synchronization operations race and all. Furthermore, we report on the construction and evaluation of PromiseKeeper, which performs a dynamic analysis to create promise graphs and infer common promise anti-patterns. We evaluate PromiseKeeper by applying it to 12 open source promise-based Node.js applications. Our results suggest that the promise graphs constructed by PromiseKeeper can provide developers with valuable information about occurrences of common anti-patterns in their promise-based code, and that promise graphs can be constructed with acceptable run-time overhead.
SP  - 162
EP  - 26
JF  - Proceedings of the ACM on Programming Languages
VL  - 2
IS  - OOPSLA
PB  - 
DO  - 10.1145/3276532
ER  - 

TY  - CHAP
AU  - Khalaf, Ahmed S.; Alharthi, A.; Hamilton, Bill; Dolgov, Igor; Tran, Son; Toups, Zachary O.
TI  - HCI (2) - A Framework of Input Devices to Support Designing Composite Wearable Computers
PY  - 2020
AB  - Composite wearable computers combine multiple wearable devices to form a cohesive whole. Designing these complex systems and integrating devices to effectively leverage their affordances is nontrivial. To inform the design of composite wearable computers, we undertook a grounded theory analysis of 84 wearable input devices drawing from 197 data sources, including technical specifications, research papers, and instructional videos. The resulting prescriptive design framework consists of four axes: type of interactivity, associated output modalities, mobility, and body location. This framework informs a composition-based approach to the design of wearable computers, enabling designers to identify which devices fill particular user needs and design constraints. Using this framework, designers can understand the relationship between the wearable, the user, and the environment, identify limitations in available wearable devices, and gain insights into how to address design challenges developers will likely encounter.
SP  - 401
EP  - 427
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_28
ER  - 

TY  - BOOK
AU  - Gupta, Aakar; Pietrzak, Thomas; Yau, Cleon; Roussel, Nicolas; Balakrishnan, Ravin
TI  - ISS - Summon and Select: Rapid Interaction with Interface Controls in Mid-air
PY  - 2017
AB  - Current freehand interactions with large displays rely on point & select as the dominant paradigm. However, constant hand movement in air for pointer navigation leads to hand fatigue quickly. We introduce summon & select, a new model for freehand interaction where, instead of navigating to the control, the user summons it into focus and then manipulates it. Summon & select solves the problems of constant pointer navigation, need for precise selection, and out-of-bounds gestures that plague point & select. We describe the design and conduct two studies to evaluate the design and compare it against point & select in a multi-button selection study. The results show that summon & select is significantly faster and has less physical and mental demand than point & select.
SP  - 52
EP  - 61
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134120
ER  - 

TY  - NA
AU  - Sidenmark, Ludwig; Potts, Dominic; Bapisch, Bill; Gellersen, Hans
TI  - CHI - Radi-Eye: Hands-Free Radial Interfaces for 3D Interaction using Gaze-Activated Head-Crossing
PY  - 2021
AB  - Eye gaze and head movement are attractive for hands-free 3D interaction in head-mounted displays, but existing interfaces afford only limited control. Radi-Eye is a novel pop-up radial interface designed to maximise expressiveness with input from only the eyes and head. Radi-Eye provides widgets for discrete and continuous input and scales to support larger feature sets. Widgets can be selected with Look & Cross, using gaze for pre-selection followed by head-crossing as trigger and for manipulation. The technique leverages natural eye-head coordination where eye and head move at an offset unless explicitly brought into alignment, enabling interaction without risk of unintended input. We explore Radi-Eye in three augmented and virtual reality applications, and evaluate the effect of radial interface scale and orientation on performance with Look & Cross. The results show that Radi-Eye provides users with fast and accurate input while opening up a new design space for hands-free fluid interaction.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445697
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Hoesl, Axel; Klimczak, Alexander; Reiss, Martin; Alt, Florian; Bulling, Andreas
TI  - UIST - EyeScout: Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays
PY  - 2017
AB  - While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user's lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In "Walk then Interact" the user can walk up to an arbitrary position in front of the display and interact, while in "Walk and Interact" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kick-off time to 3.5 seconds -- a 62% improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays.
SP  - 155
EP  - 166
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126630
ER  - 

TY  - NA
AU  - Soomro, Sarmad; Cockburn, Andy
TI  - Design & Evaluation of Layout-Agnostic Tactile Guides for In-Vehicle Touchscreens
PY  - 2020
AB  - Touchscreens are commonly used to provide access to a wide range of vehicle functions. However, interacting with touchscreens can require more attention than physical controls due to their lack of tactile sensation, potentially causing driver distraction. Prior research has indicated that stencils overlays mounted on a touchscreen can ease these problems. However, the previous stencils studies used holes that were cut to the size and location of underlying interface controls, meaning that they could only be used with a single interface layout, which is unrealistic for typical in-vehicle use. In this paper, we examine the use of layout agnostic stencils that can be used with different user interface layouts, with the aim of reducing visual attention on the touchscreen while driving. We conducted an experiment in which two layout agnostic stencil designs were evaluated in comparison to a normal touchscreen during simulated driving. Contrary to our intention, the new stencil designs increased attentional demands and impaired driving performance compared to the normal touchscreen. To understand the causes of this failure, we developed a framework for understanding low-level human activities while interacting with in-vehicle controls. The framework suggests the need for improved understanding of the acuity of the human proprioceptive target approach and of the human ability to discriminate between tactile objects.
SP  - NA
EP  - NA
JF  - 2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/citisia50690.2020.9371782
ER  - 

TY  - JOUR
AU  - Koch, Eugen; Dietzel, Andreas
TI  - Surface reconstruction by means of a flexible sensor array
PY  - 2017
AB  - Abstract In recent years, an increasing popularity of flexible sensor systems has been observed, which can largely be attributed to their ability to continuously adapt the shape to deformable bodies with non-planar surfaces without losing functionality. In this paper, we present a self-sensing, ultra-thin and flexible sensor array foil, which allows for determining its actual shape by analyzing signals from 6 × 6 sensors. Raw sensor signals clearly show the dependence from strength and direction of bending. The local bending vector is determined from signals of sensors oriented in different directions using rules which are already applied for strain gauge rosettes. The algorithm for the surface reconstruction divides the sensor foil into discrete bending segments for which the bending and subsequently new coordinates of segment edges are determined. A sensor diagnostics routine intercepts failure of the complete system due to the failure of single sensors. The functionality of the sensor array and the surface reconstruction is demonstrated for a foil subsequently adapting to a tube in different orientations. The obtained surface reconstruction clearly correlates with the visually observed bending. Such surface reconstruction could provide diagnostic information and potentially be used to detect diseases like pneumothorax. It could not only help to improve medical treatments but also to monitor the structural health of technical constructions.
SP  - 293
EP  - 300
JF  - Sensors and Actuators A: Physical
VL  - 267
IS  - NA
PB  - 
DO  - 10.1016/j.sna.2017.10.023
ER  - 

TY  - JOUR
AU  - Chaqfeh, Moumena; Coke, Russell; Hu, Jacinta; Hashmi, Waleed; Subramanian, Lakshmi; Rahwan, Talal; Zaki, Yasir
TI  - JSAnalyzer: A Web Developer Tool for Simplifying Mobile Web Pages through Non-critical JavaScript Elimination
PY  - 2022
AB  - <jats:p>The amount of JavaScript used in web pages has substantially grown in the past decade, leading to large and complex pages that are computationally intensive for handheld mobile devices. Due to the increasing usage of these devices to access today’s web, and to accommodate the needs of a large number of mobile web users who solely rely on low-end devices, we propose “JSAnalyzer,” an easy-to-use tool that enables web developers to quickly optimize JavaScript usage in their pages and to generate simpler versions of these pages for mobile web users. JSAnalyzer is motivated by the widespread use of non-critical JavaScript elements, i.e., those that have negligible (if any) impact on the page’s visual content and interactive functionality. JSAnalyzer allows the developer to selectively enable or disable JavaScript elements in any given page while visually observing their impact on the page to (1) accurately identify any non-critical JavaScript elements and (2) create a simplified page with these elements removed. Our quantitative evaluation shows that, given a low-end mobile phone, JSAnalyzer achieves an increase of nearly 90% in Google’s lighthouse performance score while reducing the page load time by 30%. A qualitative study of 22 users shows that the lighter pages produced by JSAnalyzer maintain more than 90% visual similarity compared to the original pages. Moreover, JSAnalyzer was evaluated by 69 developers, showing that it scores nearly 90% in terms of usefulness and usability while retaining the page’s content and functionality. Finally, we show that JSAnalyzer outperforms state-of-the-art solutions in terms of timing speedups and resource savings.</jats:p>
SP  - 1
EP  - 31
JF  - ACM Transactions on the Web
VL  - 16
IS  - 4
PB  - 
DO  - 10.1145/3550358
ER  - 

TY  - JOUR
AU  - Wicaksono, Irmandy; Tucker, Carson I.; Sun, Tao; Guerrero, Cesar A.; Liu, Clare; Woo, Wesley M.; Pence, Eric J.; Dagdeviren, Canan
TI  - A tailored, electronic textile conformable suit for large-scale spatiotemporal physiological sensing in vivo
PY  - 2020
AB  - The rapid advancement of electronic devices and fabrication technologies has further promoted the field of wearables and smart textiles. However, most of the current efforts in textile electronics focus on a single modality and cover a small area. Here, we have developed a tailored, electronic textile conformable suit (E-TeCS) to perform large-scale, multimodal physiological (temperature, heart rate, and respiration) sensing in vivo. This platform can be customized for various forms, sizes and functions using standard, accessible and high-throughput textile manufacturing and garment patterning techniques. Similar to a compression shirt, the soft and stretchable nature of the tailored E-TeCS allows intimate contact between electronics and the skin with a pressure value of around ~25 mmHg, allowing for physical comfort and improved precision of sensor readings on skin. The E-TeCS can detect skin temperature with an accuracy of 0.1 °C and a precision of 0.01 °C, as well as heart rate and respiration with a precision of 0.0012 m/s2 through mechano-acoustic inertial sensing. The knit textile electronics can be stretched up to 30% under 1000 cycles of stretching without significant degradation in mechanical and electrical performance. Experimental and theoretical investigations are conducted for each sensor modality along with performing the robustness of sensor-interconnects, washability, and breathability of the suit. Collective results suggest that our E-TeCS can simultaneously and wirelessly monitor 30 skin temperature nodes across the human body over an area of 1500 cm2, during seismocardiac events and respiration, as well as physical activity through inertial dynamics.
SP  - 1
EP  - 13
JF  - npj Flexible Electronics
VL  - 4
IS  - 1
PB  - 
DO  - 10.1038/s41528-020-0068-y
ER  - 

TY  - JOUR
AU  - Shi, Weinan; Yu, Chun; Yi, Xin; Li, Zhen; Shi, Yuanchun
TI  - TOAST: Ten-Finger Eyes-Free Typing on Touchable Surfaces
PY  - 2018
AB  - Touch typing on flat surfaces (e.g. interactive tabletop) is challenging due to lack of tactile feedback and hand drifting. In this paper, we present TOAST, an eyes-free keyboard technique for enabling efficient touch typing on touch-sensitive surfaces. We first formalized the problem of keyboard parameter (e.g. location and size) estimation based on users' typing data. Through a user study, we then examined users' eyes-free touch typing behavior on an interactive tabletop with only asterisk feedback. We fitted the keyboard model to the typing data, results suggested that the model parameters (keyboard location and size) changed not only between different users, but also within the same user along with time. Based on the results, we proposed a Markov-Bayesian algorithm for input prediction, which considers the relative location between successive touch points within each hand respectively. Simulation results showed that based on the pooled data from all users, this model improved the top-1 accuracy of the classical statistical decoding algorithm from 86.2% to 92.1%. In a second user study, we further improved TOAST with dynamical model parameter adaptation, and evaluated users' text entry performance with TOAST using realistic text entry tasks. Participants reached a pick-up speed of 41.4 WPM with a character-level error rate of 0.6%. And with less than 10 minutes of practice, they reached 44.6 WPM without sacrificing accuracy. Participants' subjective feedback also indicated that TOAST offered a natural and efficient typing experience.
SP  - 33
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 1
PB  - 
DO  - 10.1145/3191765
ER  - 

TY  - NA
AU  - Liu, Zhe
TI  - Modeling Cumulative Arm Fatigue on Large Multi-touch Displays
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Paloschi, Davide; Bronnikov, Kirill; Korganbayev, Sanzhar; Wolf, Alexey A.; Dostovalov, Alexander; Saccomandi, Paola
TI  - 3D Shape Sensing With Multicore Optical Fibers: Transformation Matrices Versus Frenet-Serret Equations for Real-Time Application
PY  - 2021
AB  - This paper presents the characterization of an algorithm aimed at performing accurate fiber optic-based shape sensing. The measurement of the shape relies on the evaluation of the strains applied to an optic fiber in order to identify relevant spatial parameters, such as the curvature radii and bending direction, which define its shape. The measurement system is based on a 7-core multicore fiber, containing up to 9 triplets of fiber Bragg grating sensors (FBGs) organized around a central core used as reference. The proposed study aims at comparing the widely used Frenet-Serret equations with an algorithm based on the homogeneous transformation matrices that are normally used in robotics to express the position of a point in different frames, i.e. from local to global coordinates. The numerical results of the performed experiments (with different multicore fibers and setups) extensively prove the superiority of the alternative method over the Frenet-Serret equations in terms of finding a trade-off between accuracy and execution time.
SP  - 4599
EP  - 4609
JF  - IEEE Sensors Journal
VL  - 21
IS  - 4
PB  - 
DO  - 10.1109/jsen.2020.3032480
ER  - 

TY  - NA
AU  - Matsumoto, Keigo; Narumi, Takuji; Ban, Yuki; Yanase, Yohei; Tanikawa, Tomohiro; Hirose, Michitaka
TI  - VRCAI - Unlimited Corridor: A Visuo-haptic Redirection System
PY  - 2019
AB  - The Unlimited Corridor is a virtual reality system that enables users to walk in an ostensibly straight direction around a virtual corridor within a small tracked space. Unlike other redirected walking systems, the Unlimited Corridor allows users to keep walking around without interruptions or resetting phases. This is made possible by combining a redirected walking technique with visuo-haptic interaction and a path planning algorithm. The Unlimited Corridor produces passive haptic feedback using semi-circular handrails; that is, when users grip a straight handrail in the virtual environment, they simultaneously grip a corresponding curved handrail in the physical world. These stimuli enable visuo-haptic interaction, with the user perceiving the gripped handrail as straight, and this sensation enhances the effects of redirected walking. Furthermore, we developed an algorithm that dynamically modifies the amount of distortion to allow a user to walk ostensibly straight and turn at intersections in any direction. We evaluated the Unlimited Corridor using a virtual space of approximately 400 m2 in a physical space of approximately 60 m2. According to a user study, the median value of the straightness sensation of walking when users grip the handrails (5.13) was significantly larger than that of the sensation felt without gripping the handrails (3.38).
SP  - NA
EP  - NA
JF  - Proceedings of the 17th International Conference on Virtual-Reality Continuum and its Applications in Industry
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359997.3365705
ER  - 

TY  - JOUR
AU  - Gruenefeld, Uwe; Geilen, Alexander; Liebers, Jonathan; Wittig, Nick; Koelle, Marion; Schneegass, Stefan
TI  - ARm Haptics
PY  - 2022
AB  - <jats:p>Augmented Reality (AR) technology enables users to superpose virtual content onto their environments. However, interacting with virtual content while mobile often requires users to perform interactions in mid-air, resulting in a lack of haptic feedback. Hence, in this work, we present the ARm Haptics system, which is worn on the user's forearm and provides 3D-printed input modules, each representing well-known interaction components such as buttons, sliders, and rotary knobs. These modules can be changed quickly, thus allowing users to adapt them to their current use case. After an iterative development of our system, which involved a focus group with HCI researchers, we conducted a user study to compare the ARm Haptics system to hand-tracking-based interaction in mid-air (baseline). Our findings show that using our system results in significantly lower error rates for slider and rotary input. Moreover, use of the ARm Haptics system results in significantly higher pragmatic quality and lower effort, frustration, and physical demand. Following our findings, we discuss opportunities for haptics worn on the forearm.</jats:p>
SP  - 1
EP  - 18
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546728
ER  - 

TY  - NA
AU  - Gugenheimer, Jan
TI  - UIST (Adjunct Volume) - Nomadic Virtual Reality: Exploring New Interaction Concepts for Mobile Virtual Reality Head-Mounted Displays
PY  - 2016
AB  - Technical progress and miniaturization enables virtual reality (VR) head-mounted displays (HMDs) now to be solely operated using a smartphone as a display, processing unit and sensor unit. These mobile VR HMDs (e.g. Samsung GearVR) allow for a whole new interaction scenario, where users can bring their HMD with them wherever they want and immerse themselves anytime at any place (nomadic VR). However, most of the early research on interaction with VR HMDs focused around stationary setups. My research revolves around enabling new forms of interaction for these nomadic VR scenarios. In my research I choose a user-centered design approach where I build research prototypes to solve potential problems of nomadic VR and evaluate those prototypes in user studies. I am going to present three prototypes revolving around current challenges of nomadic VR (input and feedback).
SP  - 9
EP  - 12
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2984783
ER  - 

TY  - NA
AU  - Dementyev, Artem
TI  - Tangible and Embedded Interaction - Towards Self-Aware Materials
PY  - 2016
AB  - We propose a self-aware material in the form-factor of a fabric. This material contains dense sensor nodes on a flexible and stretchable substrate. It is self-configurable and can be manipulated as a traditional craft material, by cutting and joining. The complete shape of this self-sensing material can be tracked by sensing its deformation and stretch. We hope to enable artists and designers to easily make sophisticated sensor networks. This work is a continuation of the SensorTape project, which is a sensor network in the form-factor of a tape.
SP  - 685
EP  - 688
JF  - Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2839462.2854108
ER  - 

TY  - NA
AU  - Mäkelä, Ville; Radiah, Rivu; Alsherif, Saleh; Khamis, Mohamed; Xiao, Chong; Borchert, Lisa; Schmidt, Albrecht; Alt, Florian
TI  - CHI - Virtual Field Studies: Conducting Studies on Public Displays in Virtual Reality
PY  - 2020
AB  - Field studies on public displays can be difficult, expensive, and time-consuming. We investigate the feasibility of using virtual reality (VR) as a test-bed to evaluate deployments of public displays. Specifically, we investigate whether results from virtual field studies, conducted in a virtual public space, would match the results from a corresponding real-world setting. We report on two empirical user studies where we compared audience behavior around a virtual public display in the virtual world to audience behavior around a real public display. We found that virtual field studies can be a powerful research tool, as in both studies we observed largely similar behavior between the settings. We discuss the opportunities, challenges, and limitations of using virtual reality to conduct field studies, and provide lessons learned from our work that can help researchers decide whether to employ VR in their research and what factors to account for if doing so.
SP  - 1
EP  - 15
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376796
ER  - 

TY  - NA
AU  - Jeong, Yunwoo; Kim, Han-Jong; Yun, Gyeongwon; Nam, Tek-Jin
TI  - UIST - WIKA: A Projected Augmented Reality Workbench for Interactive Kinetic Art
PY  - 2020
AB  - Iterative artistic exploration, mechanism building, and interaction programming are essential processes of prototyping interactive kinetic art (IKA). However, scattered tools and interwoven workflows across digital and physical worlds make the task difficult. We present WIKA, an integrated environment supporting the whole creation process of IKA in the form of a layered picture frame in a single workspace. A projected AR system with a mobile device efficiently makes an interactive tabletop. The projected information connected with physical components (e.g. sensors and motors) enables the programming and simulation on the workspace. Physical components are applied from the initial phase of prototyping using an AR plate, and this supports the iterative trial-and-error process by bridging the workflow. A user study shows that WIKA enabled non-experts to create diverse IKA with their ideas. A tangible interaction and projected information enable the iterative and rapid creation. The method that integrates the hardware and software in the physical environment can be applied to other prototyping tools that support the creation of interactive and kinetic elements.
SP  - 999
EP  - 1009
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415880
ER  - 

TY  - JOUR
AU  - R, Sudha M; Sriraghav, K.; S, Sudar Abisheck; Jacob, Shomona Gracia; S, Manisha
TI  - Approaches and Applications of Virtual Reality and Gesture Recognition: A Review
PY  - 2017
AB  - Interaction with a computer has been the center of innovation ever since the advent of input devices. From simple punch cards to keyboards, there are number of novel ways of interaction with comput...
SP  - 1
EP  - 18
JF  - International Journal of Ambient Computing and Intelligence
VL  - 8
IS  - 4
PB  - 
DO  - 10.4018/ijaci.2017100101
ER  - 

TY  - JOUR
AU  - Sun, Qi; Wei, Li-Yi; Kaufman, Arie E.
TI  - Mapping virtual and physical reality
PY  - 2016
AB  - Real walking offers higher immersive presence for virtual reality (VR) applications than alternative locomotive means such as walking-in-place and external control gadgets, but needs to take into consideration different room sizes, wall shapes, and surrounding objects in the virtual and real worlds. Despite perceptual study of impossible spaces and redirected walking, there are no general methods to match a given pair of virtual and real scenes. We propose a system to match a given pair of virtual and physical worlds for immersive VR navigation. We first compute a planar map between the virtual and physical floor plans that minimizes angular and distal distortions while conforming to the virtual environment goals and physical environment constraints. Our key idea is to design maps that are globally surjective to allow proper folding of large virtual scenes into smaller real scenes but locally injective to avoid locomotion ambiguity and intersecting virtual objects. From these maps we derive altered rendering to guide user navigation within the physical environment while retaining visual fidelity to the virtual environment. Our key idea is to properly warp the virtual world appearance into real world geometry with sufficient quality and performance. We evaluate our method through a formative user study, and demonstrate applications in gaming, architecture walkthrough, and medical imaging.
SP  - 64
EP  - 12
JF  - ACM Transactions on Graphics
VL  - 35
IS  - 4
PB  - 
DO  - 10.1145/2897824.2925883
ER  - 

TY  - JOUR
AU  - Visentin, Francesco; Babu, Saravana Prashanth Murali; Meder, Fabian; Mazzolai, Barbara
TI  - Selective Stiffening in Soft Actuators by Triggered Phase Transition of Hydrogel-Filled Elastomers
PY  - 2021
AB  - NA
SP  - 2101121
EP  - NA
JF  - Advanced Functional Materials
VL  - 31
IS  - 32
PB  - 
DO  - 10.1002/adfm.202101121
ER  - 

TY  - JOUR
AU  - Zenner, André; Makhsadov, Akhmajon; Klingner, Sören; Liebemann, David; Krüger, Antonio
TI  - Immersive Process Model Exploration in Virtual Reality
PY  - 2020
AB  - In many professional domains, relevant processes are documented as abstract process models, such as event-driven process chains (EPCs). EPCs are traditionally visualized as 2D graphs and their size varies with the complexity of the process. While process modeling experts are used to interpreting complex 2D EPCs, in certain scenarios such as, for example, professional training or education, also novice users inexperienced in interpreting 2D EPC data are facing the challenge of learning and understanding complex process models. To communicate process knowledge in an effective yet motivating and interesting way, we propose a novel virtual reality (VR) interface for non-expert users. Our proposed system turns the exploration of arbitrarily complex EPCs into an interactive and multi-sensory VR experience. It automatically generates a virtual 3D environment from a process model and lets users explore processes through a combination of natural walking and teleportation. Our immersive interface leverages basic gamification in the form of a logical walkthrough mode to motivate users to interact with the virtual process. The generated user experience is entirely novel in the field of immersive data exploration and supported by a combination of visual, auditory, vibrotactile and passive haptic feedback. In a user study with $\mathrm{N}=27$ novice users, we evaluate the effect of our proposed system on process model understandability and user experience, while comparing it to a traditional 2D interface on a tablet device. The results indicate a tradeoff between efficiency and user interest as assessed by the UEQ novelty subscale, while no significant decrease in model understanding performance was found using the proposed VR interface. Our investigation highlights the potential of multi-sensory VR for less time-critical professional application domains, such as employee training, communication, education, and related scenarios focusing on user interest.
SP  - 2104
EP  - 2114
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2020.2973476
ER  - 

TY  - NA
AU  - Vinot, Jean-Luc; Letondal, Catherine; Pauchet, Sylvain; Chatty, Stéphane
TI  - HCI-Aero - Could tangibility improve the safety of touch-based interaction?: exploring a new physical design space for pilot-system interfaces
PY  - 2016
AB  - Touchscreen technologies will most probably replace current instrument panels in future aeronautical cockpits. However, while safety and performance require interactive instruments to maximize the perception, action and collaboration spaces offered to pilots, the literature highlights the limits of touch-based interaction regarding these aspects. Our objective is thus to explore how tangible embodied interaction (TEI), associated with a touch-based flight deck device, could address this issue. In this paper, we contribute a structured design space for pilot-system interactions based on an analysis of the design properties of physical interaction as described in the literature, and on relevant usability, safety and industrial requirements.
SP  - 8
EP  - NA
JF  - Proceedings of the International Conference on Human-Computer Interaction in Aerospace
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2950112.2964581
ER  - 

TY  - BOOK
AU  - Gruebele, Alexander M.; Zerbe, Andrew C.; Coad, Margaret M.; Okamura, Allison M.; Cutkosky, Mark R.
TI  - RoboSoft - Distributed Sensor Networks Deployed Using Soft Growing Robots
PY  - 2021
AB  - Due to their ability to move without sliding relative to their environment, soft growing robots are attractive for deploying distributed sensor networks in confined spaces. Sensing of the state of such robots would add to their capabilities as human-safe, adaptable manipulators. However, incorporation of distributed sensors onto soft growing robots is challenging because it requires an interface between stiff and soft materials, and the sensor network needs to undergo significant strain. In this work, we present a method for adding sensors to soft growing robots that uses flexible printed circuit boards with self-contained units of microcontrollers and sensors encased in a laminate armor that protects them from unsafe curvatures. We demonstrate the ability of this system to relay directional temperature and humidity information in hard-to-access spaces. We also demonstrate and characterize a method for sensing the growing robot shape using inertial measurement units deployed along its length, and develop a mathematical model to predict its accuracy. This work advances the capabilities of soft growing robots, as well as the field of soft robot sensing.
SP  - 66
EP  - 73
JF  - 2021 IEEE 4th International Conference on Soft Robotics (RoboSoft)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/robosoft51838.2021.9479345
ER  - 

TY  - BOOK
AU  - Lösch, Eva; Alt, Florian; Koch, Michael
TI  - ISS - Mirror, Mirror on the Wall: Attracting Passers-by to Public Touch Displays With User Representations
PY  - 2017
AB  - In this paper, we investigate how effectively users' representations convey interactivity and foster interaction on large information touch displays. This research is motivated by the fact that user representations have been shown to be very efficient in playful applications that support mid-air interaction. At the same time, little is known about the effects of applying this approach to settings with a different primary mode of interaction, e.g. touch. It is also unclear how the playfulness of user representations influences the interest of users in the displayed information. To close this gap, we combine a touch display with screens showing life-sized video representations of passers-by. In a deployment, we compare different spatial arrangements to understand how passers-by are attracted and enticed to interact, how they explore the application, and how they socially behave. Findings reveal that (a) opposing displays foster interaction, but (b) may also reduce interaction at the main display; (c) a large intersection between focus and nimbus helps to notice interactivity; (d) using playful elements at information displays is not counterproductive; (e) mixed interaction modalities are hard to understand.
SP  - 22
EP  - 31
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134129
ER  - 

TY  - JOUR
AU  - Lee, Jiwon; Kim, Mingyu; Kim, Jinmo
TI  - RoleVR: Multi-experience in immersive virtual reality between co-located HMD and non-HMD users
PY  - 2019
AB  - In this study, we present RoleVR, which can provide a similar high level of presence and multi experience for co-located head-mounted display (HMD) and Non-HMD users in an asymmetric virtual reality (VR) environment. The core of RoleVR is distinguishing the difference between the asymmetric environments (in terms of the system and the experience) of HMD and Non-HMD users to design optimized roles for these users. Here, we assign HMD user with spatial role that maximizes the sense of space based on three-dimensional visual information, and we assign Non-HMD user with temporal role in which they take control of communication and action, and understand the overall situation according to the flow of time. We also design an interaction for walking and a hand interface to enhance presence. This is achieved by understanding the user’s role, thereby improving the immersion. Finally, we created an asymmetric VR application that considers the interaction between roles and performed survey-based experiments to verify the basic presence and multi-experience of users in RoleVR. Through this process, we confirmed that RoleVR provides satisfactory presence for co-located HMD and Non-HMD users, and a variety of experiences specialized for each role.
SP  - 979
EP  - 1005
JF  - Multimedia Tools and Applications
VL  - 79
IS  - 1
PB  - 
DO  - 10.1007/s11042-019-08220-w
ER  - 

TY  - NA
AU  - Maekawa, Azumi; Matsubara, Seito; Wakisaka, Sohei; Uriu, Daisuke; Hiyama, Atsushi; Inami, Masahiko
TI  - CHI - Dynamic Motor Skill Synthesis with Human-Machine Mutual Actuation
PY  - 2020
AB  - This paper presents an approach for coupling robotic capability with human ability in dynamic motor skills, called "Human-Machine Mutual Actuation (HMMA)." We focus specifically on throwing motions and propose a method to control the release timing computationally. A system we developed achieves our concept, HMMA, by a robotic handheld device that acts as a release controller. We conducted user studies to validate the feasibility of the concept and clarify related technical issues to be tackled. We recognized that the system successfully performs on throwing according to the target while it exploits human ability. These empirical experiments suggest that robotic capability can be embedded into the users' motions without losing their senses of control. Throughout the user study, we also revealed several issues to be tackled in further research contributing to HMMA.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376705
ER  - 

TY  - NA
AU  - Verweij, David; Esteves, Augusto; Bakker, Saskia; Khan, Vassilis-Javed
TI  - Tangible and Embedded Interaction - Designing Motion Matching for Real-World Applications: Lessons from Realistic Deployments
PY  - 2019
AB  - Amongst the variety of (multi-modal) interaction techniques that are being developed and explored, the Motion Matching paradigm provides a novel approach to selection and control. In motion matching, users interact by rhythmically moving their bodies to track the continuous movements of different interface targets. This paper builds upon the current algorithmic and usability focused body of work by exploring the product possibilities and implications of motion matching. Through the development and qualitative study of four novel and different real-world motion matching applications --- with 20 participants --- we elaborate on the suitability of motion matching in different multi-user scenarios, the less pertinent use in home environments and the necessity for multi-modal interaction. Based on these learnings, we developed three novel motion matching based interactive lamps, which report on clear paths for further dissemination of the embodied interaction technique's experience. This paper hereby informs the design of future motion matching interfaces and products.
SP  - 645
EP  - 656
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295628
ER  - 

TY  - NA
AU  - Althomali, Ibrahim; Kapfhammer, Gregory M.; McMinn, Phil
TI  - ICST - Automatic Visual Verification of Layout Failures in Responsively Designed Web Pages
PY  - 2019
AB  - Responsively designed web pages adjust their layout according to the viewport width of the device in use. Although tools exist to help developers test the layout of a responsive web page, they often rely on humans to flag problems. Yet, the considerable number of web-enabled devices with unique viewport widths makes this manual process both time-consuming and error-prone. Capable of detecting some common responsive layout failures, the ReDeCheck tool partially automates this process. Since ReDeCheck focuses on a web page's document object model (DOM), some of the issues it finds are not observable by humans. This paper presents a tool, called Viser, that renders a ReDeCheck-reported layout issue in a browser, adjusting the opacity of certain elements and checking for a visible difference. Unless Viser classifies an issue as a human-observable layout failure, a web developer can ignore it. This paper's experiments reveal the benefit of using Viser to support automated visual verification of layout failures in responsively designed web pages. Viser automatically classified all of the 117 layout failures that ReDeCheck reported for 20 web pages, each of which had to be manually analyzed in a prior study. Viser's automated manipulation of element opacity also highlighted manual classification's subjectivity: it categorized 28 issues differently to manual analysis, including three correctly reclassified as false positives.
SP  - 183
EP  - 193
JF  - 2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icst.2019.00027
ER  - 

TY  - NA
AU  - Yoon, Sang Ho; Huo, Ke; Zhang, Yunbo; Chen, Guiming; Paredes, Luis; Chidambaram, Subramanian; Ramani, Karthik
TI  - UIST - iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing
PY  - 2017
AB  - We present iSoft, a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. We propose a low-cost and an easy way to fabricate such piezoresistive elastomer-based soft sensors for instant interactions. We employ an electrical impedance tomography (EIT) technique to estimate changes of resistance distribution on the sensor caused by fingertip contact. To compensate for the rebound elasticity of the elastomer and achieve real-time continuous contact sensing, we apply a dynamic baseline update for EIT. The baseline updates are triggered by fingertip contact and movement detections. Further, we support unidirectional stretching sensing using a model-based approach which works separately with continuous contact sensing. We also provide a software toolkit for users to design and deploy personalized interfaces with customized sensors. Through a series of experiments and evaluations, we validate the performance of contact and stretching sensing. Through example applications, we show the variety of examples enabled by iSoft.
SP  - 665
EP  - 678
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126654
ER  - 

TY  - NA
AU  - Thomas, Jerald; Pospick, Courtney Hutton; Rosenberg, Evan Suma
TI  - VRST - Towards Physically Interactive Virtual Environments: Reactive Alignment with Redirected Walking
PY  - 2020
AB  - Interactions with the physical environment, such as passive haptic feedback, have been previously shown to provide richer and more immersive virtual reality experiences. A strict correspondence between the virtual and real world coordinate systems is a staple requirement for physical interaction. However, many of the commonly employed VR locomotion techniques allow for, or even require, this relationship to change as the experience progresses. The outcome is that experience designers frequently have to choose between flexible locomotion or physical interactivity, as the two are often mutually exclusive. To address this limitation, this paper introduces reactive environmental alignment, a novel framework that leverages redirected walking techniques to achieve a desired configuration of the virtual and real world coordinate systems. This approach can transition the system from a misaligned state to an aligned state, thereby enabling the user to interact with physical proxy objects or passive haptic surfaces. Simulation-based experiments demonstrate the effectiveness of reactive alignment and provide insight into the mechanics and potential applications of the proposed algorithm. In the future, reactive environmental alignment can enhance the interactivity of virtual reality systems and inform new research vectors that combine redirected walking and passive haptics.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418966
ER  - 

TY  - NA
AU  - Vonach, Emanuel
TI  - VR - Robot Supported Virtual and Augmented Reality
PY  - 2018
AB  - In this dissertation different aspects from research in the fields of Tangible User Interfaces, encounter-type devices and Passive Haptics are combined to investigate the benefits that robots offer for providing haptic feedback in Virtual and Augmented Reality. Robotic elements like micro drives and robotic arms are employed for the actuation of passive or active physical objects. In that way physical props can be collocated with virtual counterparts to allow high fidelity, natural interaction.
SP  - 794
EP  - 795
JF  - 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2018.8446400
ER  - 

TY  - JOUR
AU  - Takada, Ryosuke; Ando, Toshiyuki; Shizuki, Buntarou; Takahashi, Shin
TI  - BaroTouch: A Technique for Touch Force Sensing Using a Waterproof Device's Built-in Barometer
PY  - 2019
AB  - NA
SP  - 106
EP  - 115
JF  - Journal of Information Processing
VL  - 27
IS  - 2
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jindal, Pawan; Khemchandani, Vineeta; Chandra, Sushil; Pandey, Vishal
TI  - A Multiplayer Shooting Game Based Simulation For Defence Training
PY  - 2021
AB  - Creating environments and dangerous scenarios for physical training is very difficult and has a very high cost in terms of money and men’s power.Virtual Reality is a technology that simulates real-life experiences and allows people to don their own cyber avatars in a virtual world and interact with it like they would in the real world. The application of VR technology in the defence paradigm is to make trainees and officers better at using equipment, navigating a mode of transport, gaining experience of potential combat situations, medical training and more. One of the advantages of VR training in defence is that it offers the functionality to immerse users in a virtual yet safe world.Our immersive system provides an intuitive way for the users to interact with the VR or AR world by physically moving around the real world and aiming freely with tangible objects. This encourages physical interaction between the players as they compete or collaborate with other players. We present a new immersive multiplayer simulation game developed for defence training. We developed three game environments which are Combat situation, Bomb defusal, and Hostage rescue, and players can see their performance based on previously played games.
SP  - NA
EP  - NA
JF  - 2021 International Conference on Computational Performance Evaluation (ComPE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/compe53109.2021.9752429
ER  - 

TY  - NA
AU  - Le, Khanh-Duy; Zhu, Kening; Fjeld, Morten
TI  - MUM - Mirrortablet: exploring a low-cost mobile system for capturing unmediated hand gestures in remote collaboration
PY  - 2017
AB  - Direct and natural images of hand gestures have been shown to benefit remote collaboration on physical tasks in several settings, including ad-hoc ones. However, to capture such unmediated hand gestures within collaborative tasks, existing approaches require stationary hardware systems or heavily instrumented mobile devices, making them unfeasible for use at ad-hoc workplaces. We present MirrorTablet, a low-cost mirror-based system taking advantage of the built-in front-facing camera of a tablet to capture the user's unmediated hand interactions on and above the screen. This system requires minimal instrumentation of the tablet and can be easily (un-)mounted, making it suitable for mobile usage. A user study with ten pairs of participants on a helper-worker setup working on construction tasks showed that MirrorTablet improved task completion time and had positive effects on participants' perceived workload when working with unfamiliar tasks compared to using a common sketch-only interface. In addition, qualitative feedback yielded design considerations on hand visualization for mobile device remote collaboration on physical tasks.
SP  - 79
EP  - 89
JF  - Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3152832.3152838
ER  - 

TY  - JOUR
AU  - Shaik, Riyaz Ali; Rufus, Elizabeth
TI  - Recent trends and role of large area flexible electronics in shape sensing application – a review
PY  - 2021
AB  - This paper aims to review the shape sensing techniques using large area flexible electronics (LAFE). Shape perception of humanoid robots using tactile data is mainly focused.,Research papers on different shape sensing methodologies of objects with large area, published in the past 15 years, are reviewed with emphasis on contact-based shape sensors. Fiber optics based shape sensing methodology is discussed for comparison purpose.,LAFE-based shape sensors of humanoid robots incorporating advanced computational data handling techniques such as neural networks and machine learning (ML) algorithms are observed to give results with best resolution in 3D shape reconstruction.,The literature review is limited to shape sensing application either two- or three-dimensional (3D) LAFE. Optical shape sensing is briefly discussed which is widely used for small area. Optical scanners provide the best 3D shape reconstruction in the noncontact-based shape sensing; here this paper focuses only on contact-based shape sensing.,Contact-based shape sensing using polymer nanocomposites is a very economical solution as compared to optical 3D scanners. Although optical 3D scanners can provide a high resolution and fast scan of the 3D shape of the object, they require line of sight and complex image reconstruction algorithms. Using LAFE larger objects can be scanned with ML and basic electronic circuitory, which reduces the price hugely.,LAFE can be used as a wearable sensor to monitor critical biological parameters. They can be used to detect shape of large body parts and aid in designing prosthetic devices. Tactile sensing in humanoid robots is accomplished by electronic skin of the robot which is a prime example of human–machine interface at workplace.,This paper reviews a unique feature of LAFE in shape sensing of large area objects. It provides insights from mechanical, electrical, hardware and software perspective in the sensor design. The most suitable approach for large object shape sensing using LAFE is also suggested.
SP  - 745
EP  - 762
JF  - Industrial Robot: the international journal of robotics research and application
VL  - 48
IS  - 5
PB  - 
DO  - 10.1108/ir-10-2020-0234
ER  - 

TY  - NA
AU  - Rivera, Michael L.; Forman, Jack; Hudson, Scott E.; Yao, Lining
TI  - CHI Extended Abstracts - Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces
PY  - 2020
AB  - The current work examines interactions that are enabled when depositing a human-safe hydrogel onto textile substrates. These hydrogel-textile composites are water-responsive, supporting reversible actuation. To enable these interactions, we describe a fabrication process using a consumer-grade 3D printer. We show how different combinations of printed hydrogel patterns and textiles create a rich actuator design space. Finally, we show an application of this approach and discuss opportunities for future work.
SP  - 1
EP  - 9
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382788
ER  - 

TY  - JOUR
AU  - Jacobson, Alec
TI  - RodSteward: A Design‐to‐Assembly System for Fabrication using 3D‐Printed Joints and Precision‐Cut Rods
PY  - 2019
AB  - We present RodSteward, a design-to-assembly system for creating furniture-scale structures composed of 3D printed joints and precision-cut rods. The RodSteward systems consists of: RSDesigner, a fabrication-aware design interface that visualizes accurate geometries during edits and identifies infeasible designs; physical fabrication of parts via novel fully automatic construction of solid 3D-printable joint geometries and automatically generated cutting plans for rods; and RSAssembler, a guided-assembly interface that prompts the user to place parts in order while showing a focus+context visualization of the assembly in progress. We demonstrate the effectiveness of our tools with a number of example constructions of varying complexity, style and parameter choices.
SP  - 765
EP  - 774
JF  - Computer Graphics Forum
VL  - 38
IS  - 7
PB  - 
DO  - 10.1111/cgf.13878
ER  - 

TY  - NA
AU  - Nebeling, Michael; Nebeling, Janet; Yu, Ao; Rumble, Rob
TI  - CHI - ProtoAR: Rapid Physical-Digital Prototyping of Mobile Augmented Reality Applications
PY  - 2018
AB  - The latest generations of smartphones with built-in AR capabilities enable a new class of mobile apps that merge digital and real-world content depending on a user's task, context, and preference. But even experienced mobile app designers face significant challenges: creating 2D/3D AR content remains difficult and time-consuming, and current mobile prototyping tools do not support AR views. There are separate tools for this; however, they require significant technical skill. This paper presents ProtoAR which supplements rapid physical prototyping using paper and Play-Doh with new mobile cross-device multi-layer authoring and interactive capture tools to generate mobile screens and AR overlays from paper sketches, and quasi-3D content from 360-degree captures of clay models. We describe how ProtoAR evolved over four design jams with students to enable interactive prototypes of mobile AR apps in less than 90 minutes, and discuss the advantages and insights ProtoAR can give designers.
SP  - 353
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173927
ER  - 

TY  - JOUR
AU  - Parizi, Farshid Salemi; Whitmire, Eric; Patel, Shwetak
TI  - AuraRing
PY  - 2022
AB  - <jats:p>Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. It has a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.</jats:p>
SP  - 85
EP  - 92
JF  - Communications of the ACM
VL  - 65
IS  - 10
PB  - 
DO  - 10.1145/3556639
ER  - 

TY  - JOUR
AU  - Parizi, Farshid Salemi; Whitmire, Eric; Patel, Shwetak N.
TI  - AuraRing: Precise Electromagnetic Finger Tracking
PY  - 2019
AB  - Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. We develop two different approaches to pose reconstruction---a first-principles iterative approach and a closed-form neural network approach. Notably, AuraRing requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 4
PB  - 
DO  - 10.1145/3369831
ER  - 

TY  - NA
AU  - Karolus, Jakob; Wozniak, Pawel W.; Chuang, Lewis L.; Schmidt, Albrecht
TI  - CHI - Robust Gaze Features for Enabling Language Proficiency Awareness
PY  - 2017
AB  - We are often confronted with information interfaces designed in an unfamiliar language, especially in an increasingly globalized world, where the language barrier inhibits interaction with the system. In our work, we explore the design space for building interfaces that can detect the user's language proficiency. Specifically, we look at how a user's gaze properties can be used to detect whether the interface is presented in a language they understand. We report a study (N=21) where participants were presented with questions in multiple languages, whilst being recorded for gaze behavior. We identified fixation and blink durations to be effective indicators of the participants' language proficiencies. Based on these findings, we propose a classification scheme and technical guidelines for enabling language proficiency awareness on information displays using gaze data.
SP  - 2998
EP  - 3010
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025601
ER  - 

TY  - NA
AU  - Zenner, André; Krüger, Antonio
TI  - CHI - Drag:on: A Virtual Reality Controller Providing Haptic Feedback Based on Drag and Weight Shift
PY  - 2019
AB  - Standard controllers for virtual reality (VR) lack sophisticated means to convey a realistic, kinesthetic impression of size, resistance or inertia. We present the concept and implementation of Drag:on, an ungrounded shape-changing VR controller that provides dynamic passive haptic feedback based on drag, i.e. air resistance, and weight shift. Drag:on leverages the airflow occurring at the controller during interaction. By dynamically adjusting its surface area, the controller changes the drag and rotational inertia felt by the user. In a user study, we found that Drag:on can provide distinguishable levels of haptic feedback. Our prototype increases the haptic realism in VR compared to standard controllers and when rotated or swung improves the perception of virtual resistance. By this, Drag:on provides haptic feedback suitable for rendering different virtual mechanical resistances, virtual gas streams, and virtual objects differing in scale, material and fill state.
SP  - 211
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300441
ER  - 

TY  - JOUR
AU  - Vatavu, Radu-Daniel
TI  - Smart-Pockets
PY  - 2017
AB  - NA
SP  - 1
EP  - 21
JF  - International Journal of Human-Computer Studies
VL  - 103
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2017.01.005
ER  - 

TY  - NA
AU  - Li, Ziheng; Lei, Zhenyuan; Yan, An; Solovey, Erin Treacy; Pahlavan, Kaveh
TI  - ICCE - ThuMouse: A Micro-gesture Cursor Input through mmWave Radar-based Interaction
PY  - 2020
AB  - In this paper, we propose ThuMouse, a novel interaction paradigm aimed to create a gesture-based and touch-free cursor interaction that accurately tracks the motion of fingers in real-time. ThuMouse enables users to move the cursor using frequency-modulated continuous-wave (FMCW) radar. While previous work with FMCW radar in human-computer-interfaces (HCI) has focused on classifying a set of predefined hand gestures, ThuMouse regressively tracks the position of a finger, which allows for finer-grained interaction. This paper presents the gesture sensing pipeline we built, with regressive tracking through deep neural networks, data augmentation for robustness, and computer vision as a training base. We also report on a proof-of-concept demonstration shows how our system can function as a mouse, and identify areas for future work. This work builds a foundation for designing finer micro gesture-based interactions, allowing the finger to emulate external input devices such as a joystick and touch-pad.
SP  - 1
EP  - 9
JF  - 2020 IEEE International Conference on Consumer Electronics (ICCE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icce46568.2020.9043082
ER  - 

TY  - JOUR
AU  - Fan, Junjun; Fan, Xiangmin; Tian, Feng; Li, Yang; Liu, Zitao; Sun, Wei; Wang, Hongan
TI  - What is That in Your Hand?: Recognizing Grasped Objects via Forearm Electromyography Sensing
PY  - 2018
AB  - Knowing the object in hand can offer essential contextual information revealing a user's fine-grained activities. In this paper, we investigate the feasibility, accuracy, and robustness of recognizing the uninstrumented object in a user's hand by sensing and decoding her forearm muscular activities via off-the-shelf electromyography (EMG) sensors. We present results from three studies to advance our fundamental understanding of the opportunities that EMG brings in object interaction recognition. In the first study, we investigated the influence of physical properties of objects such as shape, size, and weight on EMG signals. We also conducted a thorough exploration of the feature spaces and sensor positions which can provide a solid base to rely on for future designers and practitioners for such interactive technique. In the second study, we assessed the feasibility and accuracy of inferring the types of grasped objects via using forearm muscular activity as a cue. Our results indicate that the types of objects can be recognized with up to 94.2% accuracy by employing user-dependent training. In the third study, we investigated the robustness of this approach in a realistic office setting where users were allowed to interact with objects as they would naturally. Our approach achieved up to 82.5% accuracy in discriminating 15 types of objects, even when training and testing phrases were purposefully performed on different days to incorporate changes in EMG patterns over time. Overall, this work contributes a set of fundamental findings and guidelines on using EMG technologies for object-based activity tracking.
SP  - 161
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287039
ER  - 

TY  - NA
AU  - Babic, Teo; Reiterer, Harald; Haller, Michael J.
TI  - SUI - Pocket6: A 6DoF Controller Based On A Simple Smartphone Application
PY  - 2018
AB  - We propose, implement and evaluate the use of a smartphone application for real-time six-degrees-of-freedom user input. We show that our app-based approach achieves high accuracy and goes head-to-head with expensive externally tracked controllers. The strength of our application is that it is simple to implement and is highly accessible --- requiring only an off-the-shelf smartphone, without any external trackers, markers, or wearables. Due to its inside-out tracking and its automatic remapping algorithm, users can comfortably perform subtle 3D inputs everywhere (world-scale), without any spatial or postural limitations. For example, they can interact while standing, sitting or while having their hands down by their sides. Finally, we also show its use in a wide range of applications for 2D and 3D object manipulation, thereby demonstrating its suitability for diverse real-world scenarios.
SP  - 2
EP  - 10
JF  - Proceedings of the Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267782.3267785
ER  - 

TY  - NA
AU  - Je, Seungwoo; Lim, Hyunseung; Moon, Kongpyung; Teng, Shan-Yuan; Brooks, Jas; Lopes, Pedro; Bianchi, Andrea
TI  - CHI - Elevate: A Walkable Pin-Array for Large Shape-Changing Terrains
PY  - 2021
AB  - Current head-mounted displays enable users to explore virtual worlds by simply walking through them (i.e., real-walking VR). This led researchers to create haptic displays that can also simulate different types of elevation shapes. However, existing shape-changing floors are limited by their tabletop scale or the coarse resolution of the terrains they can display due to the limited number of actuators and low vertical resolution. To tackle this challenge, we introduce Elevate, a dynamic and walkable pin-array floor on which users can experience not only large variations in shapes but also the details of the underlying terrain. Our system achieves this by packing 1200 pins arranged on a 1.80 × 0.60m platform, in which each pin can be actuated to one of ten height levels (resolution: 15mm/level). To demonstrate its applicability, we present our haptic floor combined with four walkable applications and a user study that reported increased realism and enjoyment.
SP  - 1
EP  - 11
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445454
ER  - 

TY  - NA
AU  - Bajammal, Mohammad; Mesbah, Ali
TI  - ICST - Web Canvas Testing Through Visual Inference
PY  - 2018
AB  - Canvas elements are one of the major web technologies for creating high-performance graphics and visualizations in the browser. The canvas provides APIs for directly painting on the screen, but does not have a DOM state. As such, common web testing techniques that rely on the DOM cannot be applied to canvas elements. Furthermore, there has been little to no research in the literature for testing canvas elements. We propose an automated approach for testing canvas elements and their properties. Our approach performs a visual analysis of the screenshots of canvas elements and infers visual objects, their attributes, and their hierarchical relationships present on the canvas. Each inferred object is then represented as an augmented element inside the canvas element on the DOM tree. Finally, tests are generated from the augmented canvas DOM with assertions that check the inferred objects. We implement this approach in a tool, CanvaSure, and evaluate its accuracy and effectiveness for testing canvas-based applications. Our evaluation results show that CanvaSure has an accuracy of 91% for visually inferring the contents of the canvas, and is capable of correctly detecting 93% of injected visual faults on canvas applications.
SP  - 193
EP  - 203
JF  - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icst.2018.00028
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles; Haliyo, Sinan; Frey, Pascal
TI  - "Can I Touch This?": Survey of Virtual Reality Interactions via Haptic Solutions
PY  - 2021
AB  - Haptic feedback has become crucial to enhance the user experiences in Virtual Reality (VR). This justifies the sudden burst of novel haptic solutions proposed these past years in the HCI community. This article is a survey of Virtual Reality interactions, relying on haptic devices. We propose two dimensions to describe and compare the current haptic solutions: their degree of physicality, as well as their degree of actuation. We depict a compromise between the user and the designer, highlighting how the range of required or proposed stimulation in VR is opposed to the haptic interfaces flexibility and their deployment in real-life use-cases. This paper (1) outlines the variety of haptic solutions and provides a novel perspective for analysing their associated interactions, (2) highlights the limits of the current evaluation criteria regarding these interactions, and finally (3) reflects the interaction, operation and conception potentials of "encountered-type of haptic devices".
SP  - NA
EP  - NA
JF  - 32e Conférence Francophone sur l'Interaction Homme-Machine
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450522.3451323
ER  - 

TY  - NA
AU  - Thomas, Jerald; Yong, Seraphina; Rosenberg, Evan Suma
TI  - Inverse Kinematics Assistance for the Creation of Redirected Walking Paths
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00076
ER  - 

TY  - JOUR
AU  - Nejati, Javad; Balasubramanian, Aruna
TI  - WProfX: A Fine-grained Visualization Tool for Web Page Loads
PY  - 2020
AB  - Web page performance is crucial in today's Internet ecosystem, and Web developers use various developer tools to analyze their page load performance. However, existing tools cannot be used to identify the critical bottlenecks during the page load process. In this work, we design an online tool called WProfX that allows Web developers to visually identify bottlenecks in their page structure. The key to WProfX is that unlike existing Web performance tools, WProfX not only visualizes the page load activity timings, but also extracts the dependencies between the activities. Using the dependency structure, WProfX identifies the critical bottleneck activities. This lets a developer quickly identify why their page is loading slow and conduct what-if analyses to study the effect of different optimizations. WProfX uses low-level tracing information exposed by most major browsers to extract the relationship between page load activities. The result is that WProfX works with most major browsers and newer browser versions. WProfX visualizes the page load process as a dependency graph of semantically meaningful Web activities and identifies the critical bottlenecks. We evaluate WProfX with 14 Web developers who perform three what-if analysis tasks involving identifying the page load bottleneck and evaluating the effect of a page optimization. All the participants were able to complete the tasks with WProfX, compared to less than 60% when using the popular developer tools available today. WProfX is currently being used by Web developers in a large telecom and at a Silicon Valley startup.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - EICS
PB  - 
DO  - 10.1145/3394975
ER  - 

TY  - JOUR
AU  - Cheng, Tingyu; Narumi, Koya; Do, Youngwook; Zhang, Yang; Ta, Tung D.; Sasatani, Takuya; Markvicka, Eric; Kawahara, Yoshihiro; Yao, Lining; Abowd, Gregory D.; Oh, Hyunjoo
TI  - Silver Tape: Inkjet-Printed Circuits Peeled-and-Transferred on Versatile Substrates
PY  - 2020
AB  - We propose Silver Tape, a simple yet novel fabrication technique to transfer inkjet-printed silver traces from paper onto versatile substrates, without time-/space- consuming processes such as screen printing or heat sintering. This allows users to quickly implement silver traces with a variety of properties by exploiting a wide range of substrates. For instance, high flexibility can be achieved with Scotch tape, high transparency with polydimethylsiloxane (PDMS), heat durability with Kapton polyimide tape, water solubility with 3M water-soluble tape, and beyond. Many of these properties are not achievable with conventional substrates that are used for inkjet-printing conductive traces. Specifically, our technique leverages the commonly undesired low adhesion property of the inkjet printing films and repurposes these films as temporary transfer media. We describe our fabrication methods with a library of materials we can utilize, evaluate the mechanical and electrical properties of the transferred traces, and conclude with several demonstrative applications. We believe Silver Tape enriches novel interactions for the ubiquitous computing domain, by enabling digital fabrication of electronics on versatile materials, surfaces, and shapes.
SP  - 1
EP  - 17
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3381013
ER  - 

TY  - NA
AU  - Nagels, Steven; Ramakers, Raf; Luyten, Kris; Deferme, Wim
TI  - CHI - Silicone Devices: A Scalable DIY Approach for Fabricating Self-Contained Multi-Layered Soft Circuits using Microfluidics
PY  - 2018
AB  - We present a scalable Do-It-Yourself (DIY) fabrication workflow for prototyping highly stretchable yet robust devices using a CO2 laser cutter, which we call Silicone Devices. Silicone Devices are self-contained and thus embed components for input, output, processing, and power. Our approach scales to arbitrary complex devices as it supports techniques to make multi-layered stretchable circuits and buried VIAs. Additionally, high-frequency signals are supported as our circuits consist of liquid metal and are therefore highly conductive and durable. To enable makers and interaction designers to prototype a wide variety of Silicone Devices, we also contribute a stretchable sensor toolkit, consisting of touch, proximity, sliding, pressure, and strain sensors. We demonstrate the versatility and novel opportunities of our technique by prototyping various samples and exploring their use cases. Strain tests report on the reliability of our circuits and preliminary user feedback reports on the user-experience of our workflow by non-engineers.
SP  - 188
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173762
ER  - 

TY  - NA
AU  - Siddhpuria, Shaishav; Katsuragawa, Keiko; Wallace, James R.; Lank, Edward
TI  - Conference on Designing Interactive Systems - Exploring At-Your-Side Gestural Interaction for Ubiquitous Environments
PY  - 2017
AB  - Free-space gestural systems are faced with two major issues: a lack of subtlety due to explicit mid-air arm movements, and the highly effortful nature of such interactions. With an ever-growing ubiquity of interactive devices, displays, and appliances with non-standard interfaces, lower-effort and more socially acceptable interaction paradigms are essential. To address these issues, we explore at-one's-side gestural input. Within this space, we present the results of two studies that investigate the use of side-gesture input for interaction. First, we investigate end-user preference through a gesture elicitation study, present a gesture set, and validate the need for dynamic, diverse, and variable-length gestures. We then explore the feasibility of designing such a gesture recognition system, dubbed WatchTrace, which supports alphanumeric gestures of up to length three with an average accuracy of up to 82%, providing a rich, dynamic, and feasible gestural vocabulary.
SP  - 1111
EP  - 1122
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064695
ER  - 

TY  - NA
AU  - Yoon, Sang Ho; Ma, Siyuan; Lee, Woo Suk; Yadunath, Thakurdesai Shantanu; Sun, Di; Ribeiro, Flavio Protasio; Holbery, James David
TI  - UIST - HapSense: A Soft Haptic I/O Device with Uninterrupted Dual Functionalities of Force Sensing and Vibrotactile Actuation
PY  - 2019
AB  - We present HapSense, a single-volume soft haptic I/O device with uninterrupted dual functionalities of force sensing and vibrotactile actuation. To achieve both input and output functionalities, we employ a ferroelectric electroactive polymer as core functional material with a multilayer structure design. We introduce a haptic I/O hardware that supports tunable high driving voltage waveform for vibrotactile actuation while insitu sensing a change in capacitance from contact force. With mechanically soft nature of fabricated structure, HapSense can be embedded onto various object surfaces including but not limited to furniture, garments, and the human body. Through a series of experiments and evaluations, we characterized physical properties of HapSense and validated the feasibility of using soft haptic I/O with real users. We demonstrated a variety of interaction scenarios using HapSense.
SP  - 949
EP  - 961
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347888
ER  - 

TY  - NA
AU  - Cheng, Jen-Hao; Chen, Yi; Chang, Ting-Yi; Lin, Hsu-En; Wang, Po-Yao Cosmos; Cheng, Lung-Pan
TI  - VR - Impossible Staircase: Vertically Real Walking in an Infinite Virtual Tower
PY  - 2021
AB  - We present Impossible Staircase, a real-walking virtual reality system that allows users to climb an infinite virtual tower. Our set-up consists of an one-level scaffold and a lifter. A user climbs up the scaffold by real walking on a stairway while wearing a head-mounted display, and gets reset to the ground level by a lifter imperceptibly. By repeating this process, the user perceives an illusion of climbing an infinite number of levels. Our system achieves the illusion by (1) controlling the movement of the lifter to generate reverse and imperceptible motion, (2) guiding the user through the scaffold with delay mechanisms to reset the lifter in time, and (3) procedural generating overlapping structures to enlarge perceived height of each level. We built a working system and demonstrated it with a 15-min experience. With the working system, we conducted user studies to gain deeper insights into vertical motion simulation and vertical real walking in virtual reality.
SP  - 50
EP  - 56
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00025
ER  - 

TY  - JOUR
AU  - Zenner, André; Ullmann, Kristin; Krüger, Antonio
TI  - Combining Dynamic Passive Haptics and Haptic Retargeting for Enhanced Haptic Feedback in Virtual Reality
PY  - 2021
AB  - To provide immersive haptic experiences, proxy-based haptic feedback systems for virtual reality (VR) face two central challenges: (1) similarity, and (2) colocation. While to solve challenge (1), physical proxy objects need to be sufficiently similar to their virtual counterparts in terms of haptic properties, for challenge (2), proxies and virtual counterparts need to be sufficiently colocated to allow for seamless interactions. To solve these challenges, past research introduced, among others, two successful techniques: (a) Dynamic Passive Haptic Feedback (DPHF), a hardware-based technique that leverages actuated props adapting their physical state during the VR experience, and (b) Haptic Retargeting, a software-based technique leveraging hand redirection to bridge spatial offsets between real and virtual objects. Both concepts have, up to now, not ever been studied in combination. This paper proposes to combine both techniques and reports on the results of a perceptual and a psychophysical experiment situated in a proof-of-concept scenario focused on the perception of virtual weight distribution. We show that users in VR overestimate weight shifts and that, when DPHF and HR are combined, significantly greater shifts can be rendered, compared to using only a weight-shifting prop or unnoticeable hand redirection. Moreover, we find the combination of DPHF and HR to let significantly larger spatial dislocations of proxy and virtual counterpart go unnoticed by users. Our investigation is the first to show the value of combining DPHF and HR in practice, validating that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.
SP  - 2627
EP  - 2637
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067777
ER  - 

TY  - JOUR
AU  - Uzor, Stephen; Kristensson, Per Ola
TI  - An Exploration of Freehand Crossing Selection in Head-Mounted Augmented Reality
PY  - 2021
AB  - Crossing, or goal crossing, has proven useful in various selection scenarios, including pen, mouse, touch, and virtual reality (VR). However, crossing has not been exploited for freehand selection using augmented reality head-mounted displays (AR HMDs). Using the HoloLens, we explore freehand crossing for selection and compare it to the state-of-the-art “gaze and commit” (head gaze) method. We report on three studies investigating freehand crossing in multiple use cases. The first study shows that crossing outperforms head gaze in selection scenarios of varying target arrangements. The second explores crossing, head gaze, and hand pointing in radial menu and dynamic interface scenarios. The third explores crossing as a function carrier for a variety of basic interaction functions in a drawing application. This work builds on existing knowledge on the goal-crossing paradigm by demonstrating its potential as a useful interaction method in 3D AR HMD interfaces.
SP  - 1
EP  - 27
JF  - ACM Transactions on Computer-Human Interaction
VL  - 28
IS  - 5
PB  - 
DO  - 10.1145/3462546
ER  - 

TY  - NA
AU  - Alimadadi, Saba; Mesbah, Ali; Pattabiraman, Karthik
TI  - ICSE - Inferring hierarchical motifs from execution traces
PY  - 2018
AB  - Program comprehension is a necessary step for performing many software engineering tasks. Dynamic analysis is effective in producing execution traces that assist comprehension. Traces are rich sources of information regarding the behaviour of a program. However, it is challenging to gain insight from traces due to their overwhelming amount of data and complexity. We propose a generic technique for facilitating comprehension by inferring recurring execution motifs. Inspired by bioinformatics, motifs are patterns in traces that are flexible to small changes in execution, and are captured in a hierarchical model. The hierarchical nature of the model provides an overview of the behaviour at a high-level, while preserving the execution details and intermediate levels in a structured manner. We design a visualization that allows developers to observe and interact with the model. We implement our approach in an open-source tool, called Sabalan, and evaluate it through a user experiment. The results show that using Sabalan improves developers' accuracy in performing comprehension tasks by 54%.
SP  - 776
EP  - 787
JF  - Proceedings of the 40th International Conference on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3180155.3180216
ER  - 

TY  - NA
AU  - Le Magueresse, Romain; Casset, Fabrice; Giraud, Frederic; Desloges, Brigitte; David, Nadine; Kaci, Anis; Berdague, Adelaide; Colin, Mikael
TI  - Piezoelectric flexible haptic interface development
PY  - 2022
AB  - This paper presents a Finite Element Model (FEM) study and the design of a flexible haptic surface in order to produce texture rendering on a large conformable area. For this purpose, haptic pixels vibrating at ultrasonic frequencies are actuated by piezoelectric elements and embedded on a flexible matrix. The design leads to square glass plates of 10 x 10 mm <sup xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">2</sup> with a thickness of 500 µm, actuated by PZT ceramics with a thickness of 200 µm and a radius of 2.5 mm, bonded on a 100 µm thick PEEK film. Electromechanical characterizations validate the design and open the way to the realization of a flexible haptic interface able to promote haptics effects with the friction modulation principle.
SP  - NA
EP  - NA
JF  - 2022 23rd International Conference on Thermal, Mechanical and Multi-Physics Simulation and Experiments in Microelectronics and Microsystems (EuroSimE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/eurosime54907.2022.9758912
ER  - 

TY  - BOOK
AU  - Han, Jaehyun; Ahn, Sunggeun; Park, Keunwoo; Lee, Geehyuk
TI  - ISS - Designing Touch Gestures Using the Space around the Smartwatch as Continuous Input Space
PY  - 2017
AB  - Small touchscreen interfaces such as a smartwatch have usability problems due to the small screen. One solution to these problems is to utilize the space around the smartwatch as continuous input space for the touchscreen interface. We defined four steps for a gesture that starts on the touchscreen and continues in the air. The goal of this definition was to bring the experience of large touchscreen devices into a smartwatch usage. We compared design options for the four steps and made decisions for the options based on the results of four user experiments. We expect that gestures designed based on these decisions will be both easy to learn and robust.
SP  - 210
EP  - 219
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134134
ER  - 

TY  - NA
AU  - Chaqfeh, Moumena; Hu, Jacinta; Hashmi, Waleed; Coke, Russell; Subramanian, Lakshmi; Zaki, Yasir
TI  - JSAnalyzer: A Web Developer Tool for Simplifying Mobile Pages Through JavaScript Optimizations.
PY  - 2021
AB  - The amount of JavaScript embedded in Web pages has substantially grown in the past decade, leading to large and complex pages that are computationally intensive for mobile devices. In this paper, we propose JSAnalyzer, an easy-to-use tool that enables Web developers to quickly optimize and generate simpler versions of existing web pages for mobile users. JSAnalyzer can selectively enable or disable JavaScript elements in a page while visually observing their impact, such that non-critical elements can be removed without sacrificing the visual content or the interactive functionality. Our quantitative evaluation results show that JSAnalyzer achieves more than 88% relative increase in performance scoring for low-end mobile phones (i.e., from 32% to 60%), and reduces the page load time by 30%. A qualitative study of 22 users shows that JSAnalyzer maintains more than 90% visual similarity to the original pages, whereas a developer evaluation study conducted with 23 developers shows that JSAnalyzer scores more than 80% in terms of usefulness and usability while retaining the page content and functional features. Additionally, we show that JSAnalyzer outperforms state-of-the-art solutions such as JSCleaner and Google AMP.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Malinov, Yoan-Daniel Grigorov
TI  - CHI PLAY (Companion) - Characterising the Benefits of Multi-Modal Play in Virtual Reality
PY  - 2020
AB  - With the rising popularity of VR technologies, more people are experiencing what this medium has to offer. Right now, the most popular games are either single-player or online-multiplayer, leaving the people in the same room without a way of interacting with the HMD (Head Mounted Display) player. For VR to become mainstream, this problem has to be solved. A preliminary experiment was conducted in which two different ways of including a second person in the VR experience (through a PC or a Phone) were compared in terms of co-presence and immersion. Results showed that both ways are valid and can be used to add a second player --- the quantitative data gathered from two surveys (Networked Minds Measure of Social Presence for co-presence and iGroup Presence Questionnaire for Immersion) showed that there was no significant difference, and the qualitative data, which revealed 13 distinct themes divided into five categories, helped with understanding the survey results. The next steps are to concentrate on one of the categories (embodiment) and conducting a systematic review into ways of increasing it, followed by expert interviews to confirm the findings and create a definitive list of factors that affect embodiment. Finally, a second experiment will be conducted in order to confirm the validity of the factors.
SP  - 10
EP  - 11
JF  - Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3383668.3419955
ER  - 

TY  - NA
AU  - Karolus, Jakob; Bachmann, Felix; Kosch, Thomas; Schmidt, Albrecht; Woźniak, Paweł W.
TI  - MobileHCI - Facilitating Bodily Insights Using Electromyography-Based Biofeedback during Physical Activity
PY  - 2021
AB  - Physical exercises can benefit our health, but avoiding improper form and overexertion is essential. Facilitating bodily insights can encourage learning about exercise form, allowing users to gain a deeper understanding of their physiology. To investigate this, we conducted a lab experiment where amateur users performed bicep curls, and interviews with sports coaches. Participants were provided with FitBack—a system that monitors muscle activity during exercises via electromyography (EMG) and offers real-time biofeedback. Amateurs reported that they were successful in improving their exercise form and could acquire deeper bodily insights. Coaches reflected on how understanding muscle activity through EMG could be effectively used for increasing body awareness during coaching, highlighting that EMG-based biofeedback is beneficial for a diverse set of users. Our work contributes insights into using bodily sensing to help users understand their bodies. We contribute guidelines for designing systems that use EMG biofeedback effectively in physical activity.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472027
ER  - 

TY  - NA
AU  - Trotter, Ludwig; Prange, Sarah; Khamis, Mohamed; Davies, Nigel; Alt, Florian
TI  - MUM - Design Considerations for Secure and Usable Authentication on Situated Displays
PY  - 2018
AB  - Users often need to authenticate at situated displays in order to, for example, make purchases, access sensitive information, or confirm an identity. However, the exposure of interactions in public spaces introduces a large attack surface (e.g., observation, smudge or thermal attacks). A plethora of authentication models and input modalities that aim at disguising users' input has been presented in the past. However, a comprehensive analysis on the requirements for secure and usable authentication on public displays is still missing. This work presents 13 design considerations suitable to inform practitioners and researchers during the development process of authentication systems for situated displays in public spaces. It draws on a comprehensive analysis of prior literature and subsequent discussion with five experts in the fields of pervasive displays, human-computer-interaction and usable security.
SP  - 483
EP  - 490
JF  - Proceedings of the 17th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3282894.3289743
ER  - 

TY  - JOUR
AU  - Nilsson, Niels Christian; Zenner, André; Simeone, Adalberto L.; Johnsen, Kyle; Sandor, Christian; Billinghurst, Mark
TI  - Propping Up Virtual Reality With Haptic Proxies
PY  - 2021
AB  - Physical props serving as proxies for virtual objects (haptic proxies) offer a cheap, convenient, and compelling way of delivering a sense of touch in virtual reality (VR). To successfully use haptic proxies for VR, they have to be both similar to and colocated with their virtual counterparts. In this article, we introduce a taxonomy organizing techniques using haptic proxies for VR into eight categories based on when the techniques are deployed (offline or real-time), what reality is being manipulated (physical or virtual reality), and the purpose of the techniques (to affect object perception or the mapping between real and virtual objects). Finally, we discuss key advantages and limitations of the different categories of techniques.
SP  - 104
EP  - 112
JF  - IEEE computer graphics and applications
VL  - 41
IS  - 5
PB  - 
DO  - 10.1109/mcg.2021.3097671
ER  - 

TY  - NA
AU  - Lee, Byungjoo; Nancel, Mathieu; Kim, Sunjun; Oulasvirta, Antti
TI  - CHI - AutoGain: Gain Function Adaptation with Submovement Efficiency Optimization
PY  - 2020
AB  - A well-designed control-to-display gain function can improve pointing performance with indirect pointing devices like trackpads. However, the design of gain functions is challenging and mostly based on trial and error. AutoGain is a novel method to individualize a gain function for indirect pointing devices in contexts where cursor trajectories can be tracked. It gradually improves pointing efficiency by using a novel submovement-level tracking+optimization technique that minimizes aiming error (undershooting/overshooting) for each submovement. We first show that AutoGain can produce, from scratch, gain functions with performance comparable to commercial designs, in less than a half-hour of active use. Second, we demonstrate AutoGain's applicability to emerging input devices (here, a Leap Motion controller) with no reference gain functions. Third, a one-month longitudinal study of normal computer use with AutoGain showed performance improvements from participants' default functions.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376244
ER  - 

TY  - NA
AU  - Tseng, Juin-Ling
TI  - Development of Multi-User Synchronous VR System and its Performance Analysis
PY  - 2021
AB  - With the improvement of the efficiency of information and network technology, virtual reality has gradually developed in the direction of multi-user synchronization in network systems. In general, virtual reality systems have high computing costs, so performance analysis is one of the most important considerations for system execution when developing multi-user synchronous VR systems. In order to explore the execution performance of a multi-user synchronous VR system, this study uses Unity, Steam VR, Mirror Networking, to develop a multi-user synchronous VR system based on client-server architecture. In the efficiency analysis, VR systems pay attention to the ability of real-time. Therefore, this study discusses the execution performance of each client-end VR system and analyzes the required synchronous execution data with FPS.
SP  - NA
EP  - NA
JF  - 2021 International Conference on Electronic Communications, Internet of Things and Big Data (ICEIB)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iceib53692.2021.9686402
ER  - 

TY  - NA
AU  - An, Sang-Gyun; Kim, Yongkwan; Lee, Joon Hyub; Bae, Seok-Hyung
TI  - AutomotiveUI - Collaborative Experience Prototyping of Automotive Interior in VR with 3D Sketching and Haptic Helpers
PY  - 2017
AB  - Technological advances and socioeconomic disruptions such as self-driving cars, car-sharing services and artificial intelligence assistance may fundamentally alter interactions inside the future car. However, existing design tools and processes geared toward static physical authoring are ill-equipped for such interaction design. We propose a new design workflow that combines experience prototyping methods typically used by the user interface and product design communities with 3D sketching and haptic helper techniques to help automotive designers ideate, prototype, experience and evaluate multi-sensory interactions in a collaborative manner. Using our workflow, designers use 3D sketching to quickly and expressively author 3D shape and motion ideas in space; augment them with tactile and other sensory feedback through physical proxies and other available gadgets; and immediately enact and immersively experience them to progressively explore and develop them.
SP  - 183
EP  - 192
JF  - Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3122986.3123002
ER  - 

TY  - JOUR
AU  - Lee, Jiwon; Jeong, Kisung; Kim, Jinmo
TI  - MAVE: Maze‐based immersive virtual environment for new presence and experience
PY  - 2017
AB  - NA
SP  - e1756
EP  - NA
JF  - Computer Animation and Virtual Worlds
VL  - 28
IS  - 3-4
PB  - 
DO  - 10.1002/cav.1756
ER  - 

TY  - NA
AU  - Fröhlich, Thomas; Alexandrovsky, Dmitry; Stabbert, Timo; Döring, Tanja; Malaka, Rainer
TI  - CHI PLAY - VRBox: A Virtual Reality Augmented Sandbox for Immersive Playfulness, Creativity and Exploration
PY  - 2018
AB  - Augmented sandboxes have been used as playful and educative tools to create, explore and understand complex models. However, current solutions lack interactive capabilities, missing more immersive experiences such as exploring the sand landscape from a first person perspective. We extend the interaction space of augmented sandboxes into virtual reality (VR) to offer a VR-environment that contains a landscape, which the user designs via interacting with real sand while wearing a virtual reality head-mounted display (HMD). In this paper, we present our current VR-sandbox system consisting of a box with sand, triple Kinect depth sensing, a virtual reality HMD, and hand tracking, as well as an interactive world simulation use case for exploration and evaluation. Our work explores the important and timely topics how to integrate rich haptic interaction with natural materials into VR and how to track and present real physical materials in VR. In a qualitative evaluation with nine experts from computer graphics, game design, and didactics we identified potentials, limitations as well as future application scenarios.
SP  - 153
EP  - 162
JF  - Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242671.3242697
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Alt, Florian; Bulling, Andreas
TI  - UbiComp Adjunct - Challenges and design space of gaze-enabled public displays
PY  - 2016
AB  - Gaze is an attractive modality for public displays, hence the recent years saw an increase in deployments of gaze-enabled public displays. Although gaze has been thoroughly investigated for desktop scenarios, gaze-enabled public displays present new challenges that are unique to this setup. In contrast to desktop settings, public displays (1) cannot afford requiring eye tracker calibration, (2) expect users to interact from different positions, and (3) expect multiple users to interact simultaneously. In this work we discuss these challenges, and explore the design space of gaze-enabled public displays. We conclude by discussing how the current state of research stands wrt. the identified challenges, and highlight directions for future work.
SP  - 1736
EP  - 1745
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2968342
ER  - 

TY  - NA
AU  - Shahmiri, Fereshteh; Chen, Chaoyu; Waghmare, Anandghan; Zhang, Dingtian; Mittal, Shivan; Zhang, Steven L.; Wang, Yi-Cheng; Wang, Zhong Lin; Starner, Thad; Abowd, Gregory D.
TI  - CHI - Serpentine: A Self-Powered Reversibly Deformable Cord Sensor for Human Input
PY  - 2019
AB  - We introduce Serpentine, a self-powered sensor that is a reversibly deformable cord capable of sensing a variety of human input. The material properties and structural design of Serpentine allow it to be flexible, twistable, stretchable and squeezable, enabling a broad variety of expressive input modalities. The sensor operates using the principle of Triboelectric Nanogenerators (TENG), which allows it to sense mechanical deformation without an external power source. The affordances of the cord include six interactions---Pluck, Twirl, Stretch, Pinch, Wiggle and Twist. Serpentine demonstrates the ability to simultaneously recognize these inputs through a single physical interface. A 12-participant user study illustrates 95.7% accuracy for a user-dependent recognition model using a realtime system and 92.17% for user-independent offline detection. We conclude by demonstrating how Serpentine can be employed in everyday ubiquitous computing applications.
SP  - 545
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300775
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Ning, Chih-Wei; Wang, Po-Yao; Cheng, Jen-Hao; Cheng, Lung-Pan
TI  - CHI Extended Abstracts - Haptic-go-round: A Surrounding Platform for Encounter-type Haptics in Virtual Reality Experiences
PY  - 2020
AB  - We present Haptic-go-round, a surrounding platform that allows deploying props and devices to provide haptic feedbacks in any direction in virtual reality experiences. The key component of Haptic-go-round is a motorized turntable that rotates the correct haptic device to the right direction at the right time to match what users are about to touch. We implemented a working platform including plug-and-play prop cartridges and a software interface that allow experience designers to agilely add their haptic components and use the platform for their applications. We conducted technical experiments and two user studies on Haptic-go-round to evaluate its performance. We report the results and discuss our insights and limitations.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376476
ER  - 

TY  - NA
AU  - Han, Teng; Liu, Jie; Hasan, Khalad; Fan, Mingming; Kim, Junhyeok; Li, Jiannan; Fan, Xiangmin; Tian, Feng; Lank, Edward; Irani, Pourang
TI  - CHI - PinchList: Leveraging Pinch Gestures for Hierarchical List Navigation on Smartphones
PY  - 2019
AB  - Intensive exploration and navigation of hierarchical lists on smartphones can be tedious and time-consuming as it often requires users to frequently switch between multiple views. To overcome this limitation, we present PinchList, a novel interaction design that leverages pinch gestures to support seamless exploration of multi-level list items in hierarchical views. With PinchList, sub-lists are accessed with a pinch-out gesture whereas a pinch-in gesture navigates back to the previous level. Additionally, pinch and flick gestures are used to navigate lists consisting of more than two levels. We conduct a user study to refine the design parameters of PinchList such as a suitable item size, and quantitatively evaluate the target acquisition performance using pinch-in/out gestures in both scrolling and non-scrolling conditions. In a second study, we compare the performance of PinchList in a hierarchal navigation task with two commonly used touch interfaces for list browsing: pagination and expand-and-collapse interfaces. The results reveal that PinchList is significantly faster than other two interfaces in accessing items located in hierarchical list views. Finally, we demonstrate that PinchList enables a host of novel applications in list-based interaction?
SP  - 501
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300731
ER  - 

TY  - NA
AU  - Alimadadi, Saba; Mesbah, Ali; Pattabiraman, Karthik
TI  - ICSE - Understanding asynchronous interactions in full-stack JavaScript
PY  - 2016
AB  - JavaScript has become one of the most popular languages in practice. Developers now use JavaScript not only for the client-side but also for server-side programming, leading to "full-stack" applications written entirely in JavaScript. Understanding such applications is challenging for developers, due to the temporal and implicit relations of asynchronous and event-driven entities spread over the client and server side. We propose a technique for capturing a behavioural model of full-stack JavaScript applications' execution. The model is temporal and context-sensitive to accommodate asynchronous events, as well as the scheduling and execution of lifelines of callbacks. We present a visualization of the model to facilitate program understanding for developers. We implement our approach in a tool, called Sahand, and evaluate it through a controlled experiment. The results show that Sahand improves developers' performance in completing program comprehension tasks by increasing their accuracy by a factor of three.
SP  - 1169
EP  - 1180
JF  - Proceedings of the 38th International Conference on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2884781.2884864
ER  - 

TY  - NA
AU  - Knibbe, Jarrod; Schjerlund, Jonas; Petraeus, Mathias; Hornbæk, Kasper
TI  - CHI - The Dream is Collapsing: The Experience of Exiting VR
PY  - 2018
AB  - Research on virtual reality (VR) has studied users' experience of immersion, presence, simulator sickness, and learning effects. However, the momentary experience of exiting VR and transitioning back to the real-world is not well understood. Do users become self-conscious of their actions upon exit? Are users nervous of their surroundings? Using explicitation interviews, we explore the moment of exit from VR across four applications. Analysis of the interviews reveals five components of experience: space, control, sociality, time, and sensory adaptation. Participants described spatial disorientation, for example, regardless of the complexity of the VR scene. Participants also described a window across which they exit VR, for example mentally first and then physically. We present six designs for easing or heightening the exit experience, as described by the participants. Based on these findings, we further discuss the ?moment of exit' as an opportunity for designing engaging and enhanced VR experiences.
SP  - 483
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174057
ER  - 

TY  - NA
AU  - Stratmann, Tim Claudius; Kempa, Felix; Boll, Susanne
TI  - PerDis - LAME: light-controlled attention guidance for multi-monitor environments
PY  - 2019
AB  - Multi-monitor setups are common in various domains such as stock exchange, ship bridges, and emergency control centers. We developed a light display for such a multi-monitor setup, that is controlled by eye-tracker input to guide the attention of a user from the currently focused display to another display, that is in demand of attention. Together with HCI experts for light displays we designed, implemented, and evaluated three different strategies for attention guidance with our light display in a user study. We found that ambient guidance cues that begin at the current focus of the user are faster in guiding the user to a target in the periphery than ambient guidance cues in the peripheral vision of the user.
SP  - 7
EP  - NA
JF  - Proceedings of the 8th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3321335.3324935
ER  - 

TY  - NA
AU  - Hartmann, Jeremy; Holz, Christian; Ofek, Eyal; Wilson, Andrew D.
TI  - CHI - RealityCheck: Blending Virtual Environments with Situated Physical Reality
PY  - 2019
AB  - Today's virtual reality (VR) systems offer chaperone rendering techniques that prevent the user from colliding with physical objects. Without a detailed geometric model of the physical world, these techniques offer limited possibility for more advanced compositing between the real world and the virtual. We explore this using a realtime 3D reconstruction of the real world that can be combined with a virtual environment. RealityCheck allows users to freely move, manipulate, observe, and communicate with people and objects situated in their physical space without losing the sense of immersion or presence inside their virtual world. We demonstrate RealityCheck with seven existing VR titles, and describe compositing approaches that address the potential conflicts when rendering the real world and a virtual environment together. A study with frequent VR users demonstrate the affordances provided by our system and how it can be used to enhance current VR experiences.
SP  - 347
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300577
ER  - 

TY  - NA
AU  - Ahn, Sunggeun; Lee, Geehyuk
TI  - UIST - Gaze-Assisted Typing for Smart Glasses
PY  - 2019
AB  - Text entry is expected to be a common task for smart glass users, which is generally performed using a touchpad on the temple or by a promising approach using eye tracking. However, each approach has its own limitations. For more efficient text entry, we present the concept of gaze-assisted typing (GAT), which uses both a touchpad and eye tracking. We initially examined GAT with a minimal eye input load, and demonstrated that the GAT technology was 51% faster than a two-step touch input typing method (i.e.,M-SwipeBoard: 5.85 words per minute (wpm) and GAT: 8.87 wpm). We also compared GAT methods with varying numbers of touch gestures. The results showed that a GAT requiring five different touch gestures was the most preferred, although all GAT techniques were equally efficient. Finally, we compared GAT with touch-only typing (SwipeZone) and eye-only typing (adjustable dwell) using an eye-trackable head-worn display. The results demonstrate that the most preferred technique, GAT, was 25.4% faster than the eye-only typing and 29.4% faster than the touch-only typing (GAT: 11.04 wpm, eye-only typing: 8.81 wpm, and touch-only typing: 8.53 wpm).
SP  - 857
EP  - 869
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347883
ER  - 

TY  - NA
AU  - Arslan, Cagan; Rekik, Yosra; Grisoni, Laurent
TI  - Conference on Designing Interactive Systems - E-Pad: Large Display Pointing in a Continuous Interaction Space around a Mobile Device
PY  - 2019
AB  - Relative pointing through using tactile mobile device (such as tablets of phones) on a large display is a viable interaction technique (that we call Pad in this paper) which permits accurate pointing. However, limited device size has consequences on interaction. Such systems are known to often require clutching, which degrades performances. We present E-Pad, an indirect relative pointing interaction technique which takes benefit of the mobile tactile surface combined with its surrounding space. A user can perform continuous relative pointing starting on the pad then continuing in the free space around the pad, within arm's reach. As a first step toward E-Pad, we first introduce extended continuous relative pointing gestures and conduct a preliminary study to determine how people move their hand around the mobile device. We then conduct an experiment that compares the performance of E-Pad and Pad. Our findings indicate that E-Pad is faster than Pad and decreases the number of clutches without compromising accuracy. Our findings also suggest an overwhelming preference for E-Pad.
SP  - 1101
EP  - 1108
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322284
ER  - 

TY  - JOUR
AU  - Koch, Michael; Alt, Florian
TI  - Allgegenwärtige Mensch-Computer-Interaktion
PY  - 2017
AB  - NA
SP  - 147
EP  - 152
JF  - Informatik-Spektrum
VL  - 40
IS  - 2
PB  - 
DO  - 10.1007/s00287-017-1027-4
ER  - 

TY  - CHAP
AU  - Vanderdonckt, Jean; Magrofuoco, Nathan; Kieffer, Suzanne; Pérez, Jorge; Rase, Ysabelle; Roselli, Paolo; Villarreal, Santiago
TI  - HCI (19) - Head and Shoulders Gestures: Exploring User-Defined Gestures with Upper Body
PY  - 2019
AB  - This paper presents empirical results about user-defined gestures for head and shoulders by analyzing 308 gestures elicited from 22 participants for 14 referents materializing 14 different types of tasks in IoT context of use. We report an overall medium consensus but with medium variance (mean: .263, min: .138, max: .390 on the unit scale) between participants gesture proposals, while their thinking time were less similar (min: 2.45 s, max: 22.50 s), which suggests that head and shoulders gestures are not all equally easy to imagine and to produce. We point to the challenges of deciding which head and shoulders gestures will become the consensus set based on four criteria: the agreement rate, their individual frequency, their associative frequency, and their unicity.
SP  - 192
EP  - 213
JF  - Design, User Experience, and Usability. User Experience in Advanced Technological Environments
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-23541-3_15
ER  - 

TY  - JOUR
AU  - Lippert, Stefan; Koch, Michael
TI  - Operationalizing the Benefit of Information Radiators from an Awareness Point of View
PY  - 2021
AB  - In this paper, we present a grounded theory research for the benefit information radiators provided using the concept of awareness. Information radiators are ubiquitous installations that disseminate information that is likely to improve awareness in semi-public environments. We discuss potential awareness benefits of information radiators and how these can be provided. Furthermore, we examine how to address several awareness facets that correlate with the benefits to contribute to this goal. The results aim to close the research gap about the benefit of information radiators considered from an awareness point of view.
SP  - 1
EP  - 13
JF  - SN Computer Science
VL  - 3
IS  - 1
PB  - 
DO  - 10.1007/s42979-021-00928-7
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Alt, Florian; Bulling, Andreas
TI  - MobileHCI - The past, present, and future of gaze-enabled handheld mobile devices: survey and lessons learned
PY  - 2018
AB  - While first-generation mobile gaze interfaces required special-purpose hardware, recent advances in computational gaze estimation and the availability of sensor-rich and powerful devices is finally fulfilling the promise of pervasive eye tracking and eye-based interaction on off-the-shelf mobile devices. This work provides the first holistic view on the past, present, and future of eye tracking on handheld mobile devices. To this end, we discuss how research developed from building hardware prototypes, to accurate gaze estimation on unmodified smartphones and tablets. We then discuss implications by laying out 1) novel opportunities, including pervasive advertising and conducting in-the-wild eye tracking studies on handhelds, and 2) new challenges that require further research, such as visibility of the user's eyes, lighting conditions, and privacy implications. We discuss how these developments shape MobileHCI research in the future, possibly the next 20 years.
SP  - 38
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229452
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Lee, Juyoung; Bianchi, Andrea; Samboy, Alejandro; Koike, Hideki; Woo, Woontack; Quigley, Aaron
TI  - AHs - WristLens: Enabling Single-Handed Surface Gesture Interaction for Wrist-Worn Devices Using Optical Motion Sensor
PY  - 2020
AB  - WristLens is a system for surface interaction from wrist-worn wearable devices such as smartwatches and fitness trackers. It enables eyes-free, single-handed gestures on surfaces, using an optical motion sensor embedded in a wrist-strap. This allows the user to leverage any proximate surface, including their own body, for input and interaction. An experimental study was conducted to measure the performance of gesture interaction on three different body parts. Our results show that directional gestures are accurately recognized but less so for shape gestures. Finally, we explore the interaction design space enabled by WristLens, and demonstrate novel use cases and applications, such as on-body interaction, bimanual interaction, cursor control and 3D measurement.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384797
ER  - 

TY  - NA
AU  - Gugenheimer, Jan; Stemasov, Evgeny; Frommel, Julian; Hukzio, Enrico
TI  - VR - A Demonstration of ShareVR: Co-Located Experiences for Virtual Reality Between HMD and Non-HMD Users
PY  - 2018
AB  - Most current virtual reality (VR) head-mounted displays (HMD) create a highly immersive experience and are currently becoming part of the living room entertainment (e.g. PSVR). However, current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user). This results in all the bystanders (Non-HMD users) in the living room being excluded from the experience and degraded to mainly observing the HMD user. In this demonstration we show ShareVR, a VR system using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. Additionally, we implemented several experiences for the asymmetric nature of ShareVR, exploring its design space.
SP  - 755
EP  - 756
JF  - 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2018.8446551
ER  - 

TY  - JOUR
AU  - Dong, Zhi-Chao; Fu, Xiao-Ming; Zhang, Chi; Wu, Kang; Liu, Ligang
TI  - Smooth assembled mappings for large-scale real walking
PY  - 2017
AB  - Virtual reality applications prefer real walking to provide highly immersive presence than other locomotive methods. Mapping-based techniques are very effective for supporting real walking in small physical workspaces while exploring large virtual scenes. However, the existing methods for computing real walking maps suffer from poor quality due to distortion. In this paper, we present a novel divide-and-conquer method, called Smooth Assembly Mapping (SAM), to compute real walking maps with low isometric distortion for large-scale virtual scenes. First, the input virtual scene is decomposed into a set of smaller local patches. Then, a group of local patches is mapped together into a real workspace by minimizing a low isometric distortion energy with smoothness constraints between the adjacent patches. All local patches are mapped and assembled one by one to obtain a complete map. Finally, a global optimization is adopted to further reduce the distortion throughout the entire map. Our method easily handles teleportation technique by computing maps of individual regions and assembling them with teleporter conformity constraints. A large number of experiments, including formative user studies and comparisons, have shown that our method succeeds in generating high-quality real walking maps from large-scale virtual scenes to small real workspaces and is demonstrably superior to state-of-the-art methods.
SP  - 211
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 6
PB  - 
DO  - 10.1145/3130800.3130893
ER  - 

TY  - NA
AU  - Jung, Jingun; Youn, Eunhye; Lee, Geehyuk
TI  - CHI - PinPad: Touchpad Interaction with Fast and High-Resolution Tactile Output
PY  - 2017
AB  - We explored new interaction scenarios that can be realized when a touchpad outputs fast and high-resolution spatio-temporal tactile patterns to the touch-sensitive skin on the fingertips of a user. We first constructed a special tactile multi-touch touchpad called PinPad, which was capable of outputting fast and high-resolution tactile patterns using a 40 x 25 array of actuated pins. We then developed various interaction scenarios that could be realized using the prototype: 1) Tactile Target, 2) Guide and Constraint, 3) Multi-finger Output, and 4) Dynamic Partition. To evaluate the PinPad scenarios, we implemented demo applications, and conducted interviews with users to collect feedback about their experiences with PinPad and the PinPad scenarios. The participants confirmed the effectiveness of spatio-temporal outputs of PinPad in the scenarios. In particular, they provided diverse feedback regarding the unique tactile experiences of the fast and high-resolution outputs of PinPad.
SP  - 2416
EP  - 2425
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025971
ER  - 

TY  - NA
AU  - Hiraki, Takefumi; Narumi, Koya; Yatani, Koji; Kawahara, Yoshihiro
TI  - UIST (Adjunct Volume) - Phones on Wheels: Exploring Interaction for Smartphones with Kinetic Capabilities
PY  - 2016
AB  - This paper introduces novel interaction and applications using smartphones with kinetic capabilities. We develop an accessory module with robot wheels for a smartphone. With this module, the smartphone can move in a linear direction or rotate with sufficient power. The module also includes rotary encoders, allowing us to use the wheels as an input modality. We demonstrate a series of novel mobile interaction for mobile devices with kinetic capabilities through three applications.
SP  - 121
EP  - 122
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985727
ER  - 

TY  - BOOK
AU  - Casanova, Nicolas; Cabezas, Holman; Cespedes, Angie; Araque, Dario; Ospina, Daniel; Garzon-Morales, Elizabeth; Cortés-Rico, Laura; Sarmiento, Wilson J.
TI  - VR Workshops - Touch & Live. An immersive experience for acting in others’ bodies
PY  - 2020
AB  - This work shows an immersive experience for the user to feel a disability while doing a simple task. The narrative looks for that the user feels empathy with other’s conditions through living a situation multiple times, first in the body of someone without disabilities and then, each time in the body of a different person with a different disability: hearing impairment, visual disability, and reduced mobility. So, we support the experience in human-mediated-interactions, focused on triggering the animations and providing physical-haptic feedback, including a touch that triggers the virtual change of the body for re-living the situation, like a deja vu.
SP  - 507
EP  - 508
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00105
ER  - 

TY  - NA
AU  - Tu, Huawei; Huang, Jin; Liang, Hai-Ning; Skarbez, Richard; Tian, Feng; Duh, Henry Been-Lirn
TI  - CHI - Distractor Effects on Crossing-Based Interaction
PY  - 2021
AB  - Task-irrelevant distractors affect visuo-motor control for target acquisition and studying such effects has already received much attention in human-computer interaction. However, there has been little research into distractor effects on crossing-based interaction. We thus conducted an empirical study on pen-based interfaces to investigate six crossing tasks with distractor interference in comparison to two tasks without it. The six distractor-related tasks differed in movement precision constraint (directional/amplitude), target size, target distance, distractor location and target-distractor spacing. We also developed and experimentally validated six quantitative models for the six tasks. Our results show that crossing targets with distractors had longer average times and similar accuracy than that without distractors. The effects of distractors varied depending on distractor location, target-distractor spacing and movement precision constraint. When spacing is smaller than 11.27 mm, crossing tasks with distractor interference can be regarded as pointing tasks or a combination of pointing and crossing tasks, which could be better fitted with our proposed models than Fitts’ law. According to these results, we provide practical implications to crossing-based user interface design.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445340
ER  - 

TY  - BOOK
AU  - Zielasko, Daniel; Riecke, Bernhard E.
TI  - VR Workshops - Either Give Me a Reason to Stand or an Opportunity to Sit in VR
PY  - 2020
AB  - In this position paper, we want to point, with a little bit of provocation and maybe a pinch of fun, to some grievances of and also chances for today’s pool of (consumer) VR applications concerning the chosen user posture. In our opinion, the user is considered standing or required to stand in too many cases, and transitions between postures are usually entirely unsupported.
SP  - 283
EP  - 284
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00060
ER  - 

TY  - NA
AU  - Malinov, Yoan-Daniel Grigorov; Millard, David E.; Bloun, Tom
TI  - VR - StuckInSpace: Exploring the Difference Between Two Different Mediums of Play in a Multi-Modal Virtual Reality Game
PY  - 2021
AB  - With the rising popularity of Virtual Reality (VR), there is also a rising interest in co-located multiplayer experiences, as people want to play VR games together with their friends. As having multiple VR headsets is out of reach to the average consumer, we need to look into different possible ways of including multiple people in this play space. We have created a multi-modal co-located multiplayer VR game, Stuck in Space, that introduces a second player in two ways - one with a PC (the baseline that a lot of current games do), as well as a tracked Phone that can be used as a ‘window into the virtual world’. We have conducted a user study (n = 24) where we explore the difference in immersion and co-presence between the two versions using two questionnaires (IPQ and NMMoSP), as well as a thematic analysis of the subsequent interview data, from which 5 themes emerged. Surprisingly, we found no significant difference in co-presence or immersion based on the quantitative data. However, the qualitative analysis helps reveal one of the main reasons why that is - maintaining a mental model of the real world while also being in the virtual world makes it harder for the person wearing the headset to immerse themselves and feel co-present. From these themes and sub-themes we theorize that each of the two versions has positives and negatives that cancel each other out in the quantitative data, and for there to be a difference we would need to accentuate or change certain elements of the game. The results show that introducing a second player through a Phone is not detrimental in terms of co-presence and immersion and that it is a viable way of doing so, although certain design considerations would have to be taken into account to minimize the negatives.
SP  - 501
EP  - 510
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00074
ER  - 

TY  - NA
AU  - Kovacs, Robert; Ofek, Eyal; Franco, Mar Gonzalez; Siu, Alexa F.; Marwecki, Sebastian; Holz, Christian; Sinclair, Mike
TI  - UIST - Haptic PIVOT: On-Demand Handhelds in VR
PY  - 2020
AB  - We present PIVOT, a wrist-worn haptic device that renders virtual objects into the user's hand on demand. Its simple design comprises a single actuated joint that pivots a haptic handle into and out of the user's hand, rendering the haptic sensations of grasping, catching, or throwing an object anywhere in space. Unlike existing hand-held haptic devices and haptic gloves, PIVOT leaves the user's palm free when not in use, allowing users to make unencumbered use of their hand. PIVOT also enables rendering forces acting on the held virtual objects, such as gravity, inertia, or air-drag, by actively driving its motor while the user is firmly holding the handle. When wearing a PIVOT device on both hands, they can add haptic feedback to bimanual interaction, such as lifting larger objects. In our user study, participants (n=12) evaluated the realism of grabbing and releasing objects of different shape and size with mean score 5.19 on a scale from 1 to 7, rated the ability to catch and throw balls in different directions with different velocities (mean=5.5), and verified the ability to render the comparative weight of held objects with 87% accuracy for ~100g increments.
SP  - 1046
EP  - 1059
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415854
ER  - 

TY  - NA
AU  - He, Zhenyi; Zhu, Fengyuan; Perlin, Ken
TI  - UIST (Adjunct Volume) - PhyShare: Sharing Physical Interaction in Virtual Reality
PY  - 2017
AB  - We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.
SP  - 17
EP  - 19
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131795
ER  - 

TY  - NA
AU  - Hashmi, Waleed; Chaqfeh, Moumena; Subramanian, Lakshminarayanan; Zaki, Yasir
TI  - QLUE: A Computer Vision Tool for Uniform Qualitative Evaluation of Web Pages
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the ACM Web Conference 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485447.3512112
ER  - 

TY  - JOUR
AU  - Figueiredo, Lucas; Rodrigues, Eduardo; Teixeira, Joao Marcelo; Teichrieb, Veronica
TI  - A comparative evaluation of direct hand and wand interactions on consumer devices
PY  - 2018
AB  - NA
SP  - 108
EP  - 121
JF  - Computers & Graphics
VL  - 77
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2018.10.006
ER  - 

TY  - BOOK
AU  - Zhang, Yiran; Ho, Sy-Thanh; Ladeveze, Nicolas; Nguyen, Huyen; Fleury, Cédric; Bourdot, Patrick
TI  - VR Workshops - In Touch with Everyday Objects: Teleportation Techniques in Virtual Environments Supporting Tangibility
PY  - 2021
AB  - The application of virtual reality (VR) for everyday use is often limited due to the lack of tactile and kinesthetic feedback. To facilitate and expand the use of VR in daily life, it is possible to employ physical objects readily available at home as tangible objects to provide this missing feedback. For instance, a real chair can allow a user to sit within the virtual environment, even if the sitting place in the virtual world is not a chair. In home-based games, a real but not dangerous stick can provide the holding sensation of a virtual sword. These tracked objects in the real world can serve as a tangibility medium to their virtual counterparts, contributing to a higher sense of presence and immersion. However, such a solution relies on a consistent spatial relationship between the real and virtual space surrounding the user, which makes the basic use of virtual navigation techniques such as teleportation difficult. To allow the navigation on a large virtual environment while supporting a tangible interaction with real objects in a limited physical space at home, this paper explores three different teleportation techniques: to teleport the user, the object, or both of them to a new position accordingly while preserving the user and object’s spatial relationship. All of these approaches enable tangible interaction when using a teleportation technique for navigation, but each one is more or less suitable in different situations of real/virtual spatial consistency.
SP  - 278
EP  - 283
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00057
ER  - 

TY  - NA
AU  - Cherston, Juliana; Strohmeier, Paul; Paradiso, Joseph A.
TI  - Grappler: Array of Bistable Elements For Pinching Net-Like Infrastructure to Low Gravity Bodies
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - AIAA Scitech 2019 Forum
VL  - NA
IS  - NA
PB  - 
DO  - 10.2514/6.2019-0871
ER  - 

TY  - NA
AU  - Voss, Catalin; Washington, Peter; Haber, Nick; Kline, Aaron; Daniels, Jena; Fazel, Azar; De, Titas; McCarthy, Beth; Feinstein, Carl; Winograd, Terry; Wall, Dennis P.
TI  - UbiComp Adjunct - Superpower glass: delivering unobtrusive real-time social cues in wearable systems
PY  - 2016
AB  - We have developed a system for automatic facial expression recognition, which runs on Google Glass and delivers real-time social cues to the wearer. We evaluate the system as a behavioral aid for children with Autism Spectrum Disorder (ASD), who can greatly benefit from real-time non-invasive emotional cues and are more sensitive to sensory input than neurotypically developing children. In addition, we present a mobile application that enables users of the wearable aid to review their videos along with auto-curated emotional information on the video playback bar. This integrates our learning aid into the context of behavioral therapy. Expanding on our previous work describing in-lab trials, this paper presents our system and application-level design decisions in depth as well as the interface learnings gathered during the use of the system by multiple children with ASD in an at-home iterative trial.
SP  - 1218
EP  - 1226
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2968310
ER  - 

TY  - NA
AU  - Manabe, Hiroyuki; Wataru, Yamada
TI  - UIST - A Capacitive Touch Sensing Technique with Series-connected Sensing Electrodes
PY  - 2017
AB  - Touch sensing with multiple electrodes allows expressive touch interactions. The adaptability and flexibility of the sensor are important in efficiently prototyping touch based systems. The proposed technique uses capacitive touch sensing and simplifies the connections as the electrodes are connected in series via capacitors and the interface circuit is connected to the electrode array by just two wires. The touched electrode is recognized by measuring the capacitance changes while switching the polarity of the signal. We show that the technique is capable of detecting different touches through simulations and actual measurements. User tests show that ten electrodes are successfully recognized after user calibration. They also show the proposal's other novel capabilities of multi-touch (2-touch) and `capacitor-free' design. Various forms of electrodes and applications are examined to elucidate the application range.
SP  - 645
EP  - 654
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126625
ER  - 

TY  - NA
AU  - Head, Andrew; Hohman, Fred; Barik, Titus; Drucker, Steven M.; DeLine, Robert
TI  - CHI - Managing Messes in Computational Notebooks
PY  - 2019
AB  - Data analysts use computational notebooks to write code for analyzing and visualizing data. Notebooks help analysts iteratively write analysis code by letting them interleave code with output, and selectively execute cells. However, as analysis progresses, analysts leave behind old code and outputs, and overwrite important code, producing cluttered and inconsistent notebooks. This paper introduces code gathering tools, extensions to computational notebooks that help analysts find, clean, recover, and compare versions of code in cluttered, inconsistent notebooks. The tools archive all versions of code outputs, allowing analysts to review these versions and recover the subsets of code that produced them. These subsets can serve as succinct summaries of analysis activity or starting points for new analyses. In a qualitative usability study, 12 professional analysts found the tools useful for cleaning notebooks and writing analysis code, and discovered new ways to use them, like generating personal documentation and lightweight versioning.
SP  - 270
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300500
ER  - 

TY  - JOUR
AU  - Jin, Haojian; Yang, Zhijian; Kumar, Swarun; Hong, Jason
TI  - Towards Wearable Everyday Body-Frame Tracking using Passive RFIDs
PY  - 2018
AB  - We introduce RF-Wear, an accurate and wearable solution to track movements of a user's body using passive RFIDs embedded in their clothing. RF-Wear processes wireless signals reflected off these tags to a compact single-antenna RFID reader in the user's pocket. In doing so, RF-Wear enables a first-of-its-kind body-frame tracking mechanism that is lightweight and convenient for day-to-day use, without relying on external infrastructure. At the heart of RF-Wear is a novel primitive that computes angles between different parts of the user's body using the RFID tags attached to them. RF-Wear achieves this by treating groups of RFID tags as an array of antennas whose orientation can be computed accurately relative to the handheld reader. By computing the orientation of individual body parts, we demonstrate how RF-Wear reconstructs the real-time posture of the user's entire body frame. Our solution overcomes multiple challenges owing to the interactions of wireless signals with the body, the 3-D nature of human joints and the flexibility of fabric on which RFIDs are placed. We implement and evaluate a prototype of RF-Wear on commercial RFID readers and tags and demonstrate its performance in body-frame tracking. Our results reveal a mean error of 8--12° in tracking angles at joints that rotate along one degree-of-freedom, and 21°- azimuth, 8°- elevation for joints supporting two degrees-of-freedom.
SP  - 145
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161199
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Qi, Shutong; Chen, Junchi; Shang, MuJie; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong
TI  - CHI - Fabriccio: Touchless Gestural Input on Interactive Fabrics
PY  - 2020
AB  - We present Fabriccio, a touchless gesture sensing technique developed for interactive fabrics using Doppler motion sensing. Our prototype was developed using a pair of loop antennas (one for transmitting and the other for receiving), made of conductive thread that was sewn onto a fabric substrate. The antenna type, configuration, transmission lines, and operating frequency were carefully chosen to balance the complexity of the fabrication process and the sensitivity of our system for touchless hand gestures, performed at a 10 cm distance. Through a ten-participant study, we evaluated the performance of our proposed sensing technique across 11 touchless gestures as well as 1 touch gesture. The study result yielded a 92.8% cross-validation accuracy and 85.2% leave-one-session-out accuracy. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique on soft objects.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376681
ER  - 

TY  - BOOK
AU  - Kashiwagi, Naoaki; Sugiura, Yuta; Miyata, Natsuki; Tada, Mitsunori; Sugimoto, Maki; Saito, Hideo
TI  - WACV Workshops - Measuring Grasp Posture Using an Embedded Camera
PY  - 2017
AB  - This paper proposes a measurement system for grasping postures using a fish eye camera. We attached a fish eye camera to the cap ofa cylindrical object, such as a jar or a bottle. The fish eye camera was used to determine the position ofthe fingertips in 3D via image processing. The grasping posture utilized when opening or closing ajar or a bottle was then reconstructed. If an object, a model of a hand and the grasp type are given, it is possible to estimate the grasping posture of the whole hand, even using partially captured data. Preliminary experimental results show that the system is able to reconstruct the grasping posture ofvarious users.
SP  - 42
EP  - 47
JF  - 2017 IEEE Winter Applications of Computer Vision Workshops (WACVW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wacvw.2017.14
ER  - 

TY  - NA
AU  - Lim, Sarah; Hibschman, Joshua; Zhang, Haoqi; O'Rourke, Eleanor
TI  - UIST - Ply: A Visual Web Inspector for Learning from Professional Webpages
PY  - 2018
AB  - While many online resources teach basic web development, few are designed to help novices learn the CSS concepts and design patterns experts use to implement complex visual features. Professional webpages embed these design patterns and could serve as rich learning materials, but their stylesheets are complex and difficult for novices to understand. This paper presents Ply, a CSS inspection tool that helps novices use their visual intuition to make sense of professional webpages. We introduce a new visual relevance testing technique to identify properties that have visual effects on the page, which Ply uses to hide visually irrelevant code and surface unintuitive relationships between properties. In user studies, Ply helped novice developers replicate complex web features 50% faster than those using Chrome Developer Tools, and allowed novices to recognize and explain unfamiliar concepts. These results show that visual inspection tools can support learning from complex professional webpages, even for novice developers.
SP  - 991
EP  - 1002
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242660
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Soares, Gustavo; Head, Andrew; Glassman, Elena L.; Reis, Ruan; Mongiovi, Melina; D'Antoni, Loris; Hartmann, Björn
TI  - TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences
PY  - 2017
AB  - Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Khamis, Mohamed; Kienle, Anna; Alt, Florian; Bulling, Andreas
TI  - DroNet@MobiSys - GazeDrone: Mobile Eye-Based Interaction in Public Space Without Augmenting the User
PY  - 2018
AB  - Gaze interaction holds a lot of promise for seamless human-computer interaction. At the same time, current wearable mobile eye trackers require user augmentation that negatively impacts natural user behavior while remote trackers require users to position themselves within a confined tracking range. We present GazeDrone, the first system that combines a camera-equipped aerial drone with a computational method to detect sidelong glances for spontaneous (calibration-free) gaze-based interaction with surrounding pervasive systems (e.g., public displays). GazeDrone does not require augmenting each user with on-body sensors and allows interaction from arbitrary positions, even while moving. We demonstrate that drone-supported gaze interaction is feasible and accurate for certain movement types. It is well-perceived by users, in particular while interacting from a fixed position as well as while moving orthogonally or diagonally to a display. We present design implications and discuss opportunities and challenges for drone-supported gaze interaction in public.
SP  - 66
EP  - 71
JF  - Proceedings of the 4th ACM Workshop on Micro Aerial Vehicle Networks, Systems, and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3213526.3213539
ER  - 

TY  - NA
AU  - Alt, Florian; Vehns, Julia
TI  - PerDis - Opportunistic deployments: challenges and opportunities of conducting public display research at an airport
PY  - 2016
AB  - In this paper, we report on the design, development, and deployment of an interactive shopping display at a major European airport. The ability to manufacture displays in arbitrary size and form factors as well as their networking capabilities allow public displays to be deployed in almost any location and target a huge variety of audiences. At the same time, this makes it difficult for researchers to gather generalizable insights on audience behavior. Rather, findings are often very specific to a particular deployment. We argue that in order to develop a comprehensive understanding of how successful interactive display installations can be created, researchers need to explore an as large variety of situations as possible. We contribute to this understanding by providing insights from a deployment in a security critical environment and involving multiple stakeholders where the audience is encountered in different situations (waiting, passing-by). Our insights are valuable for both researchers and practitioners, operating interactive display deployments.
SP  - 106
EP  - 117
JF  - Proceedings of the 5th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2914920.2915020
ER  - 

TY  - JOUR
AU  - Yamagami, Momona; Steele, Katherine M.; Burden, Samuel A.
TI  - CHI - Decoding Intent With Control Theory: Comparing Muscle Versus Manual Interface Performance
PY  - 2020
AB  - Manual device interaction requires precise coordination which may be difficult for users with motor impairments. Muscle interfaces provide alternative interaction methods that may enhance performance, but have not yet been evaluated for simple (eg. mouse tracking) and complex (eg. driving) continuous tasks. Control theory enables us to probe continuous task performance by separating user input into intent and error correction to quantify how motor impairments impact device interaction. We compared the effectiveness of a manual versus a muscle interface for eleven users without and three users with motor impairments performing continuous tasks. Both user groups preferred and performed better with the muscle versus the manual interface for the complex continuous task. These results suggest muscle interfaces and algorithms that can detect and augment user intent may be especially useful for future design of interfaces for continuous tasks.
SP  - 1
EP  - 12
JF  - Proceedings of the SIGCHI conference on human factors in computing systems. CHI Conference
VL  - 2020
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376224
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Dachselt, Raimund
TI  - CHI Extended Abstracts - ARCord: Visually Augmented Interactive Cords for Mobile Interaction
PY  - 2018
AB  - Research on wearable controllers has shown that body-worn cords have many interesting physical affordances that make them powerful as a novel input device to control mobile applications in an unobtrusive manner. With this paper, we want to extend the interaction and application repertoire of body-worn cords by contributing the concept of visually augmented interactive cords using state-of-the-art augmented reality (AR) glasses. This novel combination of simultaneous input and output on a cord has the potential to create rich AR user interfaces that seamlessly support direct interaction and reduce cognitive burden by providing visual and tactile feedback. As a main contribution, we present a set of cord-based interaction techniques for browsing menus, selecting items, adjusting continuous values & ranges and solving advanced tasks in AR. In addition, we present our current implementation including different touch-enabled cords, its data transmission and AR visualization. Finally, we conclude with future challenges.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188456
ER  - 

TY  - JOUR
AU  - Amanzadeh, Moe; Aminossadati, Saiied M.; Kizil, Mehmet S.; Rakić, Aleksandar D.
TI  - Recent developments in fibre optic shape sensing
PY  - 2018
AB  - Abstract This paper presents a comprehensive critical review of technologies used in the development of fibre optic shape sensors (FOSSs). Their operation is based on multi-dimensional bend measurements using a series of fibre optic sensors. Optical fibre sensors have experienced tremendous growth from simple bend sensors in 1980s to full three-dimensional FOSSs using multicore fibres in recent years. Following a short review of conventional contact-based shape sensor technologies, the evolution trend and sensing principles of FOSSs are presented. This paper identifies the major optical fibre technologies used for shape sensing and provides an account of the challenges and emerging applications of FOSSs in various industries such as medical robotics, industrial robotics, aerospace and mining industry.
SP  - 119
EP  - 137
JF  - Measurement
VL  - 128
IS  - NA
PB  - 
DO  - 10.1016/j.measurement.2018.06.034
ER  - 

TY  - NA
AU  - Bajammal, Mohammad; Mazinanian, Davood; Mesbah, Ali
TI  - ASE - Generating reusable web components from mockups
PY  - 2018
AB  - The transformation of a user interface mockup designed by a graphic designer to web components in the final app built by a web developer is often laborious, involving manual and time consuming steps. We propose an approach to automate this aspect of web development by generating reusable web components from a mockup. Our approach employs visual analysis of the mockup, and unsupervised learning of visual cues to create reusable web components (e.g., React components). We evaluated our approach, implemented in a tool called VizMod, on five real-world web mockups, and assessed the transformations and generated components through comparison with web development experts. The results show that VizMod achieves on average 94% precision and 75% recall in terms of agreement with the developers' assessment. Furthermore, the refactorings yielded 22% code reusability, on average.
SP  - 601
EP  - 611
JF  - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3238147.3238194
ER  - 

TY  - JOUR
AU  - Leigh, Sang-won; Sareen, Harpreet; Kao, Hsin-Liu Cindy; Liu, Xin; Maes, Pattie
TI  - Body-Borne Computers as Extensions of Self
PY  - 2017
AB  - The opportunities for wearable technologies go well beyond always-available information displays or health sensing devices. The concept of the cyborg introduced by Clynes and Kline, along with works in various fields of research and the arts, offers a vision of what technology integrated with the body can offer. This paper identifies different categories of research aimed at augmenting humans. The paper specifically focuses on three areas of augmentation of the human body and its sensorimotor capabilities: physical morphology, skin display, and somatosensory extension. We discuss how such digital extensions relate to the malleable nature of our self-image. We argue that body-borne devices are no longer simply functional apparatus, but offer a direct interplay with the mind. Finally, we also showcase some of our own projects in this area and shed light on future challenges.
SP  - 12
EP  - NA
JF  - Computers
VL  - 6
IS  - 1
PB  - 
DO  - 10.3390/computers6010012
ER  - 

TY  - NA
AU  - Steer, Cameron; Robinson, Simon; Pearson, Jennifer; Sahoo, Deepak Ranjan; Mabbett, Ian; Jones, Matt
TI  - MobileHCI - A liquid tangible display for mobile colour mixing
PY  - 2018
AB  - Digital painting is an increasingly popular medium of expression for many artists, yet when compared to its traditional equivalents of physical brushes and viscous paint it lacks a dimension of tangibility. We conducted observations and interviews with physical and digital artists, which gave us a strong understanding of the types of interactions used to create both physical and digital art, and the important role tangibility plays within these experiences. From this, we developed a unique liquid-like tangible display for mobile, digital colour mixing. Using a chemical hydrogel that changes its viscosity depending on temperature, we are able to create some resemblances to the feeling of mixing paint with a finger. This paper documents the information gathered from working with artists, how this process informed the development of a mobile painting attachment, and an exploration of its capabilities. After returning with our prototype, we found that it provided artists with sensations of oil and acrylic paint mixing and also successfully mimicked how paints are laid out on a paint palette.
SP  - 8
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229461
ER  - 

TY  - NA
AU  - Martinez, Patricia Ivette Cornelio; De Pirro, Silvana; Thanh, Chi; Subramanian, Sriram
TI  - CHI - Agency in Mid-air Interfaces
PY  - 2017
AB  - Touchless interfaces allow users to view, control and manipulate digital content without physically touching an interface. They are being explored in a wide range of application scenarios from medical surgery to car dashboard controllers. One aspect of touchless interaction that has not been explored to date is the Sense of Agency (SoA). The SoA refers to the subjective experience of voluntary control over actions in the external world. In this paper, we investigated the SoA in touchless systems using the intentional binding paradigm. We first compare touchless systems with physical interactions and then augmented different types of haptic feedback to explore how different outcome modalities influence intentional binding. From our experiments, we demonstrated that an intentional binding effect is observed in both physical and touchless interactions with no statistical difference. Additionally, we found that haptic and auditory feedback help to increase SoA compared with visual feedback in touchless interfaces. We discuss these findings and identify design opportunities that take agency into consideration.
SP  - 2426
EP  - 2439
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025457
ER  - 

TY  - JOUR
AU  - Lee, Younghoon; Song, Won Jun; Sun, Jeong-Yun
TI  - Hydrogel soft robotics
PY  - 2020
AB  - Abstract With the rapidly growing attention to human-robot interfaces, soft robotics has attracted a great deal of interest. Soft robots have diverse advantages, including compliancy and safety, which contribute to seamless interactions with humans. To boost progress in the field, there is a need for compliant materials. Hydrogels are promising as compliant materials for soft robots because of their outstanding features, including high stretchability, transparency, ion conductivity, and biocompatibility. Furthermore, hydrogels provide innovative capabilities for soft robotics based on their unique responsiveness to stimuli. In this review, we discuss the unique features of hydrogel-based soft robots, from their fundamental working mechanisms to notable applications. Finally, we suggest perspectives on future directions that addressing potential challenges in the field of hydrogel soft robotics.
SP  - 100258
EP  - NA
JF  - Materials Today Physics
VL  - 15
IS  - NA
PB  - 
DO  - 10.1016/j.mtphys.2020.100258
ER  - 

TY  - NA
AU  - Chang, Joseph Chee; Hahn, Nathan; Kittur, Aniket
TI  - UIST - Supporting Mobile Sensemaking Through Intentionally Uncertain Highlighting
PY  - 2016
AB  - Patients researching medical diagnoses, scientist exploring new fields of literature, and students learning about new domains are all faced with the challenge of capturing information they find for later use. However, saving information is challenging on mobile devices, where the small screen and font sizes combined with the inaccuracy of finger based touch screens makes it time consuming and stressful for people to select and save text for future use. Furthermore, beyond the challenge of simply selecting a region of bounded text on a mobile device, in many learning and data exploration tasks the boundaries of what text may be relevant and useful later are themselves uncertain for the user. In contrast to previous approaches which focused on speeding up the selection process by making the identification of hard boundaries faster, we introduce the idea of intentionally supporting uncertain input in the context of saving information during complex reading and information exploration. We embody this idea in a system that uses force touch and fuzzy bounding boxes along with posthoc expandable context to support identifying and saving information in an intentionally uncertain way on mobile devices. In a two part user study we find that this approach reduced selection time and was preferred by participants over the default system text selection method.
SP  - 61
EP  - 68
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984538
ER  - 

TY  - NA
AU  - Voss, Catalin; Washington, Peter; Haber, Nick; Kline, Aaron; Daniels, Jena; Fazel, Azar; De, Titas; McCarthy, Beth; Feinstein, Carl; Winograd, Terry; Wall, Dennis P.
TI  - Superpower Glass: Delivering Unobtrusive Real-time Social Cues in Wearable Systems
PY  - 2020
AB  - We have developed a system for automatic facial expression recognition, which runs on Google Glass and delivers real-time social cues to the wearer. We evaluate the system as a behavioral aid for children with Autism Spectrum Disorder (ASD), who can greatly benefit from real-time non-invasive emotional cues and are more sensitive to sensory input than neurotypically developing children. In addition, we present a mobile application that enables users of the wearable aid to review their videos along with auto-curated emotional information on the video playback bar. This integrates our learning aid into the context of behavioral therapy. Expanding on our previous work describing in-lab trials, this paper presents our system and application-level design decisions in depth as well as the interface learnings gathered during the use of the system by multiple children with ASD in an at-home iterative trial.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Lim, Sarah
TI  - CHI Extended Abstracts - Ply: Visual Regression Pruning for Web Design Source Inspection
PY  - 2017
AB  - Despite the ease of inspecting HTML and CSS, web developers struggle to identify the code most responsible for particular stylistic effects, due to complex DOM structures and CSS property cascades. In this paper, we introduce Ply, a DOM inspection tool which augments the Chrome Developer Tools to help developers explore complex professional web designs. To compute source code relevance, we introduce a technique called visual regression pruning, which uses pixel-level screenshot comparison to help developers locate CSS responsible for a webpage's appearance. A user selects an element, and Ply computes the visual impact of each matched CSS property. If a property does not affect the webpage, Ply hides the property from the inspector. In multiple iterations of needfinding studies, developers located relevant code more quickly using Ply. In a case study with the Airbnb homepage, Ply displays an average of 49% fewer CSS properties per element, compared to the Chrome Developer Tools as a control.
SP  - 130
EP  - 135
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3048427
ER  - 

TY  - NA
AU  - Lee, Byungjoo; Nancel, Mathieu; Kim, Sunjun; Oulasvirta, Antti
TI  - AutoGain: Gain Function Adaptation with Submovement Efficiency Optimization
PY  - 2016
AB  - A well-designed control-to-display gain function can improve pointing performance with indirect pointing devices like trackpads. However, the design of gain functions is challenging and mostly based on trial and error. AutoGain is a novel method to individualize a gain function for indirect pointing devices in contexts where cursor trajectories can be tracked. It gradually improves pointing efficiency by using a novel submovement-level tracking+optimization technique that minimizes aiming error (undershooting/overshooting) for each submovement. We first show that AutoGain can produce, from scratch, gain functions with performance comparable to commercial designs, in less than a half-hour of active use. Second, we demonstrate AutoGain's applicability to emerging input devices (here, a Leap Motion controller) with no reference gain functions. Third, a one-month longitudinal study of normal computer use with AutoGain showed performance improvements from participants' default functions.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Rus, Silvia; Hammacher, Felix; von Wilmsdorff, Julian; Braun, Andreas; Grosse-Puppendahl, Tobias; Kirchbuchner, Florian; Kuijper, Arjan
TI  - AmI - Prototyping Shape-Sensing Fabrics Through Physical Simulation
PY  - 2018
AB  - Embedding sensors into fabrics can leverage substantial improvements in application areas like working safety, 3D modeling or health-care, for example to recognize the risk of developing skin ulcers. Finding a suitable setup and sensor combination for a shape-sensing fabric currently relies on the intuition of an application engineer. We introduce a novel approach: Simulating the shape-sensing fabric first and optimize the design to achieve better real-world implementations. In order to enable developers to easily prototype their shape-sensing scenario, we have implemented a framework that enables soft body simulation and virtual prototyping. To evaluate our approach, we investigate the design of a system detecting sleeping postures. We simulate potential designs first, and implement a bed cover consisting of 40 distributed acceleration sensors. The validity of our framework is confirmed by comparing the simulated and real evaluation results. We show that both approaches achieve similar performances, with an F-measure of 85% for the virtual prototype and 89% for the real-world implementation.
SP  - 147
EP  - 161
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-03062-9_12
ER  - 

TY  - NA
AU  - Patras, Cristian; Cibulskis, Mantas; Nilsson, Niels Christian
TI  - Body Warping Versus Change Blindness Remapping: A Comparison of Two Approaches to Repurposing Haptic Proxies for Virtual Reality
PY  - 2022
AB  - When using tangible props as proxies for virtual objects, it is important that these haptic proxies are similar to and co-located with their virtual counterparts. This makes it challenging to scale virtual scenarios because more proxies are needed as scenarios grow more complex. Haptic retargeting, or virtual remapping, makes it possible to repurpose the same physical prop as a proxy for multiple virtual objects. This paper details a user study comparing two techniques for repurposing haptic proxies; namely haptic retargeting based on body warping and change blindness remapping. Participants performed a simple button-pressing task, and 24 virtual buttons were mapped onto four haptic proxies with varying degrees of misalignment. Body warping and change blindness remapping were used to realign the real and virtual buttons, and the results indicate that users failed to reliably detect realignment of up to 7.9 cm for body warping and up to 9.7 cm for change blindness remapping. Moreover, change blindness remapping yielded significantly higher self-reported agency, and marginally higher ownership. Taken together these results suggest that this less explored technique has potential when it comes to repurposing haptic proxies for virtual reality.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00039
ER  - 

TY  - JOUR
AU  - Menges, Raphael; Kumar, Chandan; Staab, Steffen
TI  - Improving User Experience of Eye Tracking-Based Interaction: Introspecting and Adapting Interfaces
PY  - 2019
AB  - Eye tracking systems have greatly improved in recent years, being a viable and affordable option as digital communication channel, especially for people lacking fine motor skills. Using eye tracking as an input method is challenging due to accuracy and ambiguity issues, and therefore research in eye gaze interaction is mainly focused on better pointing and typing methods. However, these methods eventually need to be assimilated to enable users to control application interfaces. A common approach to employ eye tracking for controlling application interfaces is to emulate mouse and keyboard functionality. We argue that the emulation approach incurs unnecessary interaction and visual overhead for users, aggravating the entire experience of gaze-based computer access. We discuss how the knowledge about the interface semantics can help reducing the interaction and visual overhead to improve the user experience. Thus, we propose the efficient introspection of interfaces to retrieve the interface semantics and adapt the interaction with eye gaze. We have developed a Web browser, GazeTheWeb, that introspects Web page interfaces and adapts both the browser interface and the interaction elements on Web pages for gaze input. In a summative lab study with 20 participants, GazeTheWeb allowed the participants to accomplish information search and browsing tasks significantly faster than an emulation approach. Additional feasibility tests of GazeTheWeb in lab and home environment showcase its effectiveness in accomplishing daily Web browsing activities and adapting large variety of modern Web pages to suffice the interaction for people with motor impairment.
SP  - 37
EP  - 46
JF  - ACM Transactions on Computer-Human Interaction
VL  - 26
IS  - 6
PB  - 
DO  - 10.1145/3338844
ER  - 

TY  - JOUR
AU  - Dong, Zhi-Chao; Fu, Xiao-Ming; Yang, Zeshi; Liu, Ligang
TI  - Redirected Smooth Mappings for Multiuser Real Walking in Virtual Reality
PY  - 2019
AB  - We propose a novel technique to provide multiuser real walking experiences with physical interactions in virtual reality (VR) applications. In our system, multiple users walk freely while navigating a large virtual environment within a smaller physical workspace. These users can interact with other real users or physical props in the same physical locations. The key of our method is a redirected smooth mapping that incorporates the redirected walking technique to warp the input virtual scene with small bends and low distance distortion. Users possess a wide field of view to explore the mapped virtual environment while being redirected in the real workspace. To keep multiple users away from the overlaps of the mapped virtual scenes, we present an automatic collision avoidance technique based on dynamic virtual avatars. These avatars naturally appear, move, and disappear, producing as little influence as possible on users’ walking experiences. We evaluate our multiuser real walking system through formative user studies, and demonstrate the capability and practicability of our technique in two multiuser applications.
SP  - 1
EP  - 17
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 5
PB  - 
DO  - 10.1145/3345554
ER  - 

TY  - NA
AU  - Zhou, Qian; Fitzmaurice, George; Anderson, Fraser
TI  - In-Depth Mouse: Integrating Desktop Mouse into Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501884
ER  - 

TY  - NA
AU  - Liu, Qi Feng
TI  - Exploring the Potential of Wrist-Worn Gesture Sensing
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Ning, Chih-Wei; Wang, Po-Yao (Cosmos); Cheng, Jen-Hao; Cheng, Lung-Pan
TI  - Haptic-go-round: A Surrounding Platform for Encounter-type Haptic in Virtual Reality Experiences
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383136
ER  - 

TY  - NA
AU  - Zheng, Jingjie; Bi, Xiaojun; Li, Kun; Li, Yang; Zhai, Shumin
TI  - CHI - M3 Gesture Menu: Design and Experimental Analyses of Marking Menus for Touchscreen Mobile Interaction
PY  - 2018
AB  - Despite their learning advantages in theory, marking menus have faced adoption challenges in practice, even on today's touchscreen-based mobile devices. We address these challenges by designing, implementing, and evaluating multiple versions of M3 Gesture Menu (M3), a reimagination of marking menus targeted at mobile interfaces. M3 is defined on a grid rather than in a radial space, relies on gestural shapes rather than directional marks, and has constant and stationary space use. Our first controlled experiment on expert performance showed M3 was faster and less error-prone by a factor of two than traditional marking menus. A second experiment on learning demonstrated for the first time that users could successfully transition to recall-based execution of a dozen commands after three ten-minute practice sessions with both M3 and Multi-Stroke Marking Menu. Together, M3, with its demonstrated resolution, learning, and space use benefits, contributes to the design and understanding of menu selection in the mobile-first era of end-user computing.
SP  - 249
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173823
ER  - 

TY  - JOUR
AU  - Koch, Michael; Alt, Florian
TI  - Allgegenwärtige Mensch-Computer-Interaktion
PY  - 2015
AB  - In unserer immer mehr von digitalen Kommunikations- und Informationsangeboten bestimmten Welt entscheidet die wirkungsvolle Nutzung von Computern – die Interaktion zwischen Mensch und Computer – zunehmend uber personlichen Erfolg und gesellschaftliche Teilhabe. Anwender haben inzwischen eine Vielzahl unterschiedlicher Interaktionsgerate zum Zugang zu verschiedensten Diensten zur Verfugung, deren Nutzung auch ohne Schulungen oder das Studium von Handbuchern moglich sein muss. Ein wichtiger Aspekt bei der Gestaltung ist neben der anwendungs- und benutzergerechten Bedienbarkeit auch die Absehbarkeit der Folgen der Nutzung. Die Informatik ist herausgefordert in Zusammenarbeit mit anderen Disziplinen Formen der Mensch-Computer-Interaktion zu gestalten, die es kunftig allen Menschen ermoglichen, die allgegenwartigen Kommunikations- und Informationsangebote muhelos und selbstbestimmt zu nutzen.
SP  - 290
EP  - 295
JF  - Informatik-Spektrum
VL  - 38
IS  - 4
PB  - 
DO  - 10.1007/s00287-015-0901-1
ER  - 

TY  - JOUR
AU  - Cmentowski, Sebastian; Kievelitz, Fabian; Krueger, Jens Harald
TI  - Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games
PY  - 2022
AB  - <jats:p>The size of most virtual environments exceeds the tracking space available for physical walking. One solution to this disparity is to extend the available walking range by augmenting users' actual movements. However, the resulting increase in visual flow can easily cause cybersickness. Therefore, we present a novel augmented-walking approach for virtual reality games. Our core concept is a virtual tunnel that spans the entire travel distance when viewed from the outside. However, its interior is only a fraction as long, allowing users to cover the distance by real walking. Whereas the tunnel hides the visual flow from the applied movement acceleration, windows on the tunnel's walls still reveal the actual expedited motion. Our evaluation reveals that our approach avoids cybersickness while enhancing physical activity and preserving presence. We finish our paper with a discussion of the design considerations and limitations of our proposed locomotion technique.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CHI PLAY
PB  - 
DO  - 10.1145/3549509
ER  - 

TY  - JOUR
AU  - Lee, Jiwon; Kim, Mingyu; Kim, Jinmo
TI  - A Study on Immersion and VR Sickness in Walking Interaction for Immersive Virtual Reality Applications
PY  - 2017
AB  - This study analyzes walking interaction to enhance the immersion and minimize virtual reality (VR) sickness of users by conducting experiments. In this study, the walking interaction is composed of three steps using input devices with a simple structure that can be easily used by anyone. The first step consists of a gamepad control method, which is the most popular but has low presence. The second step consists of a hand-based walking control interface, which is mainly used for interaction in VR applications. The last step consists of a march-in-place detection simulator that interacts with the legs—the key body parts for walking. Four experiments were conducted to determine the degree of direct expression of intention by users in the course of walking interactions that can improve immersion, presence, and prevent VR sickness in VR applications. With regard to the experiments in this study, survey experiments were conducted for general users using the Wilcoxon test, a presence questionnaire, and simulator sickness questionnaire (SSQ). In addition, the technical performance of the VR scenes used in the experiment was analyzed. The experimental results showed that higher immersion was achieved when interactions that felt closer to real walking were provided in VR. Furthermore, it was found that even control methods with a simple structure could be used for walking interactions with minimal VR sickness. Finally, a satisfactory presence was found in VR if the user was able interact using his or her own legs.
SP  - 78
EP  - NA
JF  - Symmetry
VL  - 9
IS  - 5
PB  - 
DO  - 10.3390/sym9050078
ER  - 

TY  - JOUR
AU  - Lehmann, Maren; Krause, Paul; Miruchna, Viktor; von Klitzing, Regine
TI  - Tailoring PNIPAM hydrogels for large temperature-triggered changes in mechanical properties
PY  - 2019
AB  - N-isopropylacrylamide (NIPAM)-based hydrogel films are used for touch-controlled applications, where the temperature-induced change in the mechanical properties is utilized to create tactile feedback. N,N′-methylenebisacrylamide (BIS) and poly(ethylene glycol)diacrylate (PEGDA) are used as cross-linkers to study the influence of their size and concentration on the viscoelastic properties in a temperature-controlled rheology setup. The changes in water content between swollen and collapsed state of the hydrogel samples increase with decreasing cross-linking density and increasing size of the cross-linker resulting in bigger meshes in the network. The difference in the viscoelastic properties of the hydrogels increases with increasing deswelling ratio and is highest for the P(NIPAM-PEGDA) hydrogels with low cross-linking density with a 50-fold increase in the storage modulus. The deswelling ratio of these P(NIPAM-PEGDA) hydrogels is up to five times higher compared to the P(NIPAM-BIS) hydrogels of the same cross-linking density. The mesh sizes are estimated from the mechanical properties.
SP  - 633
EP  - 640
JF  - Colloid and Polymer Science
VL  - 297
IS  - 4
PB  - 
DO  - 10.1007/s00396-019-04470-0
ER  - 

TY  - NA
AU  - Cockburn, Andy; Gutwin, Carl; Palanque, Philippe; Deleris, Yannick; Trask, Catherine; Coveney, Ashley; Yung, Marcus; MacLean, Karon E.
TI  - CHI - Turbulent Touch: Touchscreen Input for Cockpit Flight Displays
PY  - 2017
AB  - Touchscreen input in commercial aircraft cockpits offers potential advantages, including ease of use, modifiability, and reduced weight. However, tolerance to turbulence is a challenge for their deployment. To better understand the impact of turbulence on cockpit input methods we conducted a comparative study of user performance with three input methods -- touch, trackball (as currently used in commercial aircraft), and a touchscreen stencil overlay designed to assist finger stabilization. These input methods were compared across a variety of interactive tasks and at three levels of simulated turbulence (none, low, and high). Results showed that performance degrades and subjective workload increases as vibration increases. Touch-based interaction was faster than the trackball when precision requirements were low (at all vibrations), but it was slower and less accurate for more precise pointing, particularly at high vibrations. The stencil did not improve touch selection times, although it did reduce errors on small targets at high vibrations, but only when finger lift-off errors had been eliminated by a timeout. Our work provides new information on the types of tasks affected by turbulence and the input mechanisms that perform best under different levels of vibration.
SP  - 6742
EP  - 6753
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025584
ER  - 

TY  - CHAP
AU  - Le Magueresse, Romain; Giraud, Frédéric; Casset, Fabrice; Kaci, Anis; Desloges, Brigitte; Colin, Mikael
TI  - Preliminary Design of a Flexible Haptic Surface
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>This paper presents the preliminary development of a flexible haptic surface in order to produce texture rendering on a large conformable area. For this purpose, Haptic Pixels vibrating at ultrasonic frequencies are actuated by piezoelectric elements and implanted on a flexible matrix. The design leads to square glass plates of 10 <jats:inline-formula><jats:alternatives><jats:tex-math>$$\times $$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:mo>×</mml:mo> </mml:math></jats:alternatives></jats:inline-formula> 10 mm<jats:inline-formula><jats:alternatives><jats:tex-math>$$^2$$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:msup> <mml:mrow /> <mml:mn>2</mml:mn> </mml:msup> </mml:math></jats:alternatives></jats:inline-formula> with a thickness of 500 <jats:inline-formula><jats:alternatives><jats:tex-math>$$\upmu $$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:mi>μ</mml:mi> </mml:math></jats:alternatives></jats:inline-formula>m, actuated by PZT ceramics with a thickness of 200 <jats:inline-formula><jats:alternatives><jats:tex-math>$$\upmu $$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:mi>μ</mml:mi> </mml:math></jats:alternatives></jats:inline-formula>m and a radius of 2.5 mm bonded on a 100 <jats:inline-formula><jats:alternatives><jats:tex-math>$$\upmu $$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:mi>μ</mml:mi> </mml:math></jats:alternatives></jats:inline-formula>m thick PEEK film. Electromechanical characterizations validate the design. The PEEK film between two pixels is exploited to separate them, to obtain the flexibility of the surface and to create an area of friction reduction with a stationary wave. Haptic evaluations are carried out to confirm the performances of the approach on a Haptic Pixel.</jats:p>
SP  - 207
EP  - 215
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06249-0_24
ER  - 

TY  - NA
AU  - Jin, Haojian; Wang, Jingxian; Yang, Zhijian; Kumar, Swarun; Hong, Jason
TI  - MobiSys - WiSh: Towards a Wireless Shape-aware World using Passive RFIDs
PY  - 2018
AB  - This paper presents WiSh, a solution that makes ordinary surfaces shape-aware, relaying their real-time geometry directly to a user's handheld device. WiSh achieves this using inexpensive, light-weight and battery-free RFID tags attached to these surfaces tracked from a compact single-antenna RFID reader. In doing so, WiSh enables several novel applications: shape-aware clothing that can detect a user's posture, interactive shape-aware toys or even shape-aware bridges that report their structural health. WiSh's core algorithm infers the shape of ordinary surfaces using the wireless channels of signals reflected off RFID tags received at a single-antenna RFID reader. Indeed, locating every RFID tag using a single channel measurement per-tag is challenging, given that neither their 3-D coordinates nor orientation are known a priori. WiSh presents a novel algorithm that models the geometric constraints between the coordinates of the RFID tags based on flexibility of the surface upon which they are mounted. By inferring surface curvature parameters rather than the locations of individual RFID tags, we greatly reduce the number of variables our system needs to compute. Further, WiSh overcomes a variety of system-level challenges stemming from signal multipath, stretching of fabric and modeling large surfaces. We implement WiSh on commodity RFID readers and tags attached to a variety of surfaces and demonstrate mm-accurate shape-tracking across various applications.
SP  - 428
EP  - 441
JF  - Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210240.3210328
ER  - 

TY  - JOUR
AU  - Andreasen, Esben; Gong, Liang; Møller, Anders; Pradel, Michael; Selakovic, Marija; Sen, Koushik; Staicu, Cristian-Alexandru
TI  - A Survey of Dynamic Analysis and Test Generation for JavaScript
PY  - 2017
AB  - JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.
SP  - 66
EP  - 36
JF  - ACM Computing Surveys
VL  - 50
IS  - 5
PB  - 
DO  - 10.1145/3106739
ER  - 

TY  - BOOK
AU  - Prakash, Jay; Yang, Zhijian; Wei, Yu-Lin; Choudhury, Romit Roy
TI  - EarComp@UbiComp - STEAR: Robust Step Counting from Earables
PY  - 2019
AB  - This paper shows that inertial measurement units (IMUs) inside earphones offer a clear advantage in counting the number of steps a user has walked. While step-count has been extensively studied in the mobile computing community, there is wide consensus that false positives are common. The main reason for false positives is due to limb and device motions producing the same periodic bounce as the human walk. However, when IMUs are at the ear, we find that many of the lower-body motions are naturally "filtered out", i.e., these noisy motions do not propagate all the way up to the ear. Hence, the earphone IMU detects a bounce produced only from walking. While head movements can still pollute this bouncing signal, we develop methods to alleviate the problem. Results show 95% step-count accuracy even in the most difficult test case -- very slow walk -- where smartphone and fitbit-like systems falter. Importantly, our system STEAR is robust to changes in walking patterns and scales well across different users. Additionally, we demonstrate how STEAR also bring opportunities for effective jump analysis, often important for exercises and injury-related rehabilitation.
SP  - 36
EP  - 41
JF  - Proceedings of the 1st International Workshop on Earable Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3345615.3361133
ER  - 

TY  - NA
AU  - Veras, Rafael; Singh, Gaganpreet; Farhadi-Niaki, Farzin; Udhani, Ritesh; Patekar, Parth Pradeep; Zhou, Wei; Irani, Pourang; Li, Wei
TI  - CHI - Elbow-Anchored Interaction: Designing Restful Mid-Air Input
PY  - 2021
AB  - We designed a mid-air input space for restful interactions on the couch. We observed people gesturing in various postures on a couch and found that posture affects the choice of arm motions when no constraints are imposed by a system. Study participants that sat with the arm rested were more likely to use the forearm and wrist, as opposed to the whole arm. We investigate how a spherical input space, where forearm angles are mapped to screen coordinates, can facilitate restful mid-air input in multiple postures. We present two controlled studies. In the first, we examine how a spherical space compares with a planar space in an elbow-anchored setup, with a shoulder-level input space as baseline. In the second, we examine the performance of a spherical input space in four common couch postures that set unique constraints to the arm. We observe that a spherical model that captures forearm movement facilitates comfortable input across different seated postures.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445546
ER  - 

TY  - NA
AU  - Lopes, Pedro; You, Sijing; Cheng, Lung-Pan; Marwecki, Sebastian; Baudisch, Patrick
TI  - CHI - Providing Haptics to Walls & Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation
PY  - 2017
AB  - We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor. In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on "prevented me from passing through" as well as "realistic". In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles.
SP  - 1471
EP  - 1482
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025600
ER  - 

TY  - NA
AU  - Wang, Sijia; Fang, Cathy Mengying; Yang, Yiyao; Lu, Kexin; Vlachostergiou, Maria; Yao, Lining
TI  - Morphace: An Integrated Approach for Designing Customizable and Transformative Facial Prosthetic Makeup
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519406
ER  - 

TY  - NA
AU  - DIckson, Terence
TI  - HybridPointing Touch: A Technique to Switch Between Absolute and Relative Pointing on Large Touch Screens
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Ens, Barrett; Lanir, Joel; Tang, Anthony; Bateman, Scott; Lee, Gun A.; Piumsomboon, Thammathip; Billinghurst, Mark
TI  - Revisiting collaboration through mixed reality: The evolution of groupware
PY  - 2019
AB  - NA
SP  - 81
EP  - 98
JF  - International Journal of Human-Computer Studies
VL  - 131
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.05.011
ER  - 

TY  - NA
AU  - Strandholt, Patrick L.; Dogaru, Oana A.; Nilsson, Niels Christian; Nordahl, Rolf; Serafin, Stefania
TI  - CHI - Knock on Wood: Combining Redirected Touching and Physical Props for Tool-Based Interaction in Virtual Reality
PY  - 2020
AB  - When physical props serve as proxies for virtual tools used to manipulate the virtual environment, it is challenging to provide appropriate haptic feedback. Redirected tool-mediated manipulation addresses this challenge by distorting the mapping between physical and virtual tools to provide a sensation of manipulating the virtual environment, when the physical tool comes into contact with another physical prop. For example, a virtual hammer's position can be offset to ensure that physical impacts accompany each strike of a virtual nail. We demonstrate the idea by showing that it can be used to create sensations of impact and resistance when driving a virtual nail into a surface, when tightening a virtual screw, and when sawing through a virtual plank. The results of a user study indicate that the proposed approach is perceived as more realistic than interaction with a single physical prop or controller and no notable detriments to precision were observed.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376303
ER  - 

TY  - JOUR
AU  - Choi, Daewoong; Cho, Hyeonjoong; Seo, Kyeongeun; Lee, Sangyub; Lee, Jaekyu; Ko, Jae-jin
TI  - Designing Hand Pose Aware Virtual Keyboard With Hand Drift Tolerance
PY  - 2019
AB  - An unintentional hand drift adversely affects the typing performance of conventional virtual keyboards. To overcome this, we proposed to utilize the typing patterns of skilled typists. First, as most typists enter the keys in the same column with a predetermined finger only, we restricted these keys to be typed by their corresponding fingers. Second, our investigation of skilled typists demonstrated that hand poses vary when the typists touch different keys. Thus, rather than locating the touch point as in the case of existing virtual keyboards, we attempted to use unique hand poses to infer the target key. Based on these two techniques, we implemented a novel hand poses aware virtual keyboard that is tolerant of hand drift. Our experimental studies yielded the following results: 1) most of the QWERTY-familiar typists who have varying typing habits were easily adaptable to the proposed keyboard design and 2) the proposed keyboard outperformed existing virtual keyboards in terms of typing speed and several error rates, and eventually achieved a typing speed of approximately 56 WPM.
SP  - 96035
EP  - 96047
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2929310
ER  - 

TY  - CHAP
AU  - Antoine, Axel; Malacria, Sylvain; Marquardt, Nicolai; Casiez, Géry
TI  - INTERACT (2) - Esquisse: Using 3D Models Staging to Facilitate the Creation of Vector-Based Trace Figures
PY  - 2019
AB  - Trace figures are contour drawings of people and objects that capture the essence of scenes without the visual noise of photos or other visual representations. Their focus and clarity make them ideal representations to illustrate designs or interaction techniques. In practice, creating those figures is a tedious task requiring advanced skills, even when creating the figures by tracing outlines based on photos. To mediate the process of creating trace figures, we introduce the open-source tool Esquisse. Informed by our taxonomy of 124 trace figures, Esquisse provides an innovative 3D model staging workflow, with specific interaction techniques that facilitate 3D staging through kinematic manipulation, anchor points and posture tracking. Our rendering algorithm (including stroboscopic rendering effects) creates vector-based trace figures of 3D scenes. We validated Esquisse with an experiment where participants created trace figures illustrating interaction techniques, and results show that participants quickly managed to use and appropriate the tool.
SP  - 496
EP  - 516
JF  - Human-Computer Interaction – INTERACT 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29384-0_30
ER  - 

TY  - NA
AU  - Han, Teng; Li, Jiannan; Hasan, Khalad; Nakamura, Keisuke; Gomez, Randy; Balakrishnan, Ravin; Irani, Pourang
TI  - CHI - PageFlip: Leveraging Page-Flipping Gestures for Efficient Command and Value Selection on Smartwatches
PY  - 2018
AB  - Selecting an item of interest on smartwatches can be tedious and time-consuming as it involves a series of swipe and tap actions. We present PageFlip, a novel method that combines into a single action multiple touch operations such as command invocation and value selection for efficient interaction on smartwatches. PageFlip operates with a page flip gesture that starts by dragging the UI from a corner of the device. We first design PageFlip by examining its key design factors such as corners, drag directions and drag distances. We next compare PageFlip to a functionally equivalent radial menu and a standard swipe and tap method. Results reveal that PageFlip improves efficiency for both discrete and continuous selection tasks. Finally, we demonstrate novel smartwatch interaction opportunities and a set of applications that can benefit from PageFlip.
SP  - 529
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174103
ER  - 

TY  - NA
AU  - Jacobson, Alec
TI  - RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed Joints and Precision-Cut Rods
PY  - 2019
AB  - We present RodSteward, a design-to-assembly system for creating furniture-scale structures composed of 3D printed joints and precision-cut rods. The RodSteward systems consists of: RSDesigner, a fabrication-aware design interface that visualizes accurate geometries during edits and identifies infeasible designs; physical fabrication of parts via novel fully automatic construction of solid 3D-printable joint geometries and automatically generated cutting plans for rods; and RSAssembler, a guided-assembly interface that prompts the user to place parts in order while showing a focus+context visualization of the assembly in progress. We demonstrate the effectiveness of our tools with a number of example constructions of varying complexity, style and parameter choices.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Long, Yong-Hao; Chen, Yan-Cheng; Chen, Xiang-Ping; Shi, Xiao-Hong; Zhou, Fan
TI  - Test-Driven Feature Extraction of Web Components
PY  - 2022
AB  - NA
SP  - 389
EP  - 404
JF  - Journal of Computer Science and Technology
VL  - 37
IS  - 2
PB  - 
DO  - 10.1007/s11390-022-0673-4
ER  - 

TY  - BOOK
AU  - Yamaguchi, Shun; Shionoiri, Hirotaka; Nakamura, Takuto; Kajimoto, Hiroyuki
TI  - ISS - An Encounter Type VR System Aimed at Exhibiting Wall Material Samples for Show House
PY  - 2018
AB  - In this research, we propose a system that can change the tactile material of the wall surface especially in the VR show house. In order to present multiple types of wall materials, an encounter type tactile sense presentation unit with a plurality of wall materials mounted on a uniaxial robot presents a specific type of wall material according to the movement of the hand of the experiencer. With this encounter type approach, users can experience tactile sensations of multiple kinds of realistic wall materials. We examined the specs necessary for such presentation, constructed the system, and conducted a user study to examine the effect of the proposed system, comparing with visual only and visual + force only conditions.
SP  - 321
EP  - 326
JF  - Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3279778.3279908
ER  - 

TY  - NA
AU  - Liu, Wanyu; Dementyev, Artem; Schwarz, Diemo; Fléty, Emmanuel; Mackay, Wendy E.; Beaudouin-Lafon, Michel; Bevilacqua, Frédéric
TI  - CHI - SonicHoop: Using Interactive Sonification to Support Aerial Hoop Practices
PY  - 2021
AB  - Aerial hoops are circular, hanging devices for both acrobatic exercise and artistic performance that let us explore the role of interactive sonification in physical activity. We present SonicHoop, an augmented aerial hoop that generates auditory feedback via capacitive touch sensing, thus becoming a digital musical instrument that performers can play with their bodies. We compare three sonification strategies through a structured observation study with two professional aerial hoop performers. Results show that SonicHoop fundamentally changes their perception and choreographic processes: instead of translating music into movement, they search for bodily expressions that compose music. Different sound designs affect their movement differently, and auditory feedback, regardless of type of sound, improves movement quality. We discuss opportunities for using SonicHoop as an aerial hoop training tool, as a digital musical instrument, and as a creative object; as well as using interactive sonification in other acrobatic practices to explore full-body vertical interaction.
SP  - 1
EP  - 16
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445539
ER  - 

TY  - NA
AU  - Li, Yichen; Li, Tianxing; Patel, Ruchir A.; Yang, Xing-Dong; Zhou, Xia
TI  - Self-Powered Gesture Recognition with Ambient Light
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242635
ER  - 

TY  - NA
AU  - Cheng, Yi Fei; Luong, Tiffany; Fender, Andreas Rene; Streli, Paul; Holz, Christian
TI  - ComforTable User Interfaces: Surfaces Reduce Input Error, Time, and Exertion for Tabletop and Mid-air User Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00029
ER  - 

TY  - NA
AU  - Hudak, Marian; Sobota, Branislav; Korecko, Stefan
TI  - Gesture Control for Cognitive Training Based on VR Technologies
PY  - 2018
AB  - Virtual reality systems create interaction between users and virtual environment. Gesture recognition has wide applicability in various industries. There are several types of I/O control devices to achieve quality between user and system interaction. Current hand-held and armband devices enhance immersion and interaction between user and virtual environment. Cognitive trainings in Virtual reality have positive effect to measure user interaction and responsiveness. In fact, there are different technologies and ways to recognize gestures for controlling Virtual environments. It can be helpful to work more naturally and intuitively with hand gesture in the future.
SP  - NA
EP  - NA
JF  - 2018 16th International Conference on Emerging eLearning Technologies and Applications (ICETA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iceta.2018.8572028
ER  - 

TY  - NA
AU  - Chen, Yan; Grossman, Tovi
TI  - UIST - Umitation: Retargeting UI Behavior Examples for Website Design
PY  - 2021
AB  - Interface designers often refer to UI behavior examples found in the wild (e.g., commercial websites) for reference or design inspiration. While past research has looked at retargeting interface and webpage design, limited work has explored the challenges in retargeting interactive visual behaviors. We introduce Umitation, a system that helps designers extract, edit, and adapt example front-end UI behaviors to target websites. Umitation can also help designers specify the desired behaviors and reconcile their intended interaction details with their existing UI. In a qualitative evaluation, we found evidence that Umitation helps participants extract and retarget dynamic front-end UI behavior examples quickly and expressively.
SP  - 922
EP  - 935
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474796
ER  - 

TY  - NA
AU  - Belo, João; Feit, Anna Maria; Feuchtner, Tiare; Grønbæk, Kaj
TI  - CHI - XRgonomics: Facilitating the Creation of Ergonomic 3D Interfaces
PY  - 2021
AB  - Arm discomfort is a common issue in Cross Reality applications involving prolonged mid-air interaction. Solving this problem is difficult because of the lack of tools and guidelines for 3D user interface design. Therefore, we propose a method to make existing ergonomic metrics available to creators during design by estimating the interaction cost at each reachable position in the user’s environment. We present XRgonomics, a toolkit to visualize the interaction cost and make it available at runtime, allowing creators to identify UI positions that optimize users’ comfort. Two scenarios show how the toolkit can support 3D UI design and dynamic adaptation of UIs based on spatial constraints. We present results from a walkthrough demonstration, which highlight the potential of XRgonomics to make ergonomics metrics accessible during the design and development of 3D UIs. Finally, we discuss how the toolkit may address design goals beyond ergonomics.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445349
ER  - 

TY  - NA
AU  - Campbell, Joey; Hogan, Trevor; Fraser, Mike
TI  - Tangible and Embedded Interaction - Feeling Virtual Worlds: An Exploration into Coupling Virtual and Kinaesthetic Experiences
PY  - 2018
AB  - In this paper we describe an exploratory study that incorporates the design, implementation and study of a system that utilises virtual reality, tangible interaction and force feedback. The approach we take is to design a VR system that incorporates a moveable tangible interface (wheelchair), which overlaps seamlessly with a 3d counterpart in the virtual world. The user interacts with the virtual environment by pushing the physical wheelchair, which simultaneously controls the virtual avatar. In the virtual world we place objects that once collided with trigger force feedback by stopping the physical wheelchair. In this paper we discuss the design rationale and technical implementation and follow by describing the next phase of this work in progress.
SP  - 279
EP  - 285
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173281
ER  - 

TY  - BOOK
AU  - Sadana, Ramik; Stasko, John
TI  - ISS - Expanding Selection for Information Visualization Systems on Tablet Devices
PY  - 2016
AB  - Selection is a fundamental operation in interactive visualization applications. Although techniques such as clicking and lassoing items of interest are sufficient for basic selections, a more sophisticated interaction mechanism is required for expressing complex queries to modify or generalize existing selections. The ability to perform these advanced selections is critical for effective analysis within visualization systems. On touch-based devices such as tablets, however, expressing advanced selections is difficult due to the absence of a cursor and modifier keys. In this work, we address this limitation by presenting new interaction techniques that leverage a person's non-dominant hand. We use these techniques for advanced selection operations such as expanding, modifying, and replicating existing selections. Further, we introduce a method for performing generalized selection on tablet devices that provides a fluid mechanism to control the attributes and parameters of selection.
SP  - 149
EP  - 158
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992157
ER  - 

TY  - BOOK
AU  - Rateau, Hanae; Rekik, Yosra; Grisoni, Laurent; Jorge, Joaquim
TI  - ISS - Talaria: Continuous Drag & Drop on a Wall Display
PY  - 2016
AB  - We present an interaction technique combining tactile actions and midair pointing to access out-of-reach content on large displays without the need to walk across the display. Users can start through a touch gesture on the display surface and finish midair by pointing to push content away or inversely to retrieve a content. The technique takes advantage of well-known semantics of pointing in human-to-human interaction. These, coupled with the semantics of proximal relations and deictic proxemics make the proposed technique very powerful as it leverages on well-understood human-human interaction modalities. Experimental results show this technique to outperform direct tactile interaction on dragging tasks. From our experience we derive four guidelines for interaction with large-scale displays.
SP  - 199
EP  - 204
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992164
ER  - 

TY  - NA
AU  - Cheng, Lung-Pan; Chang, Li; Marwecki, Sebastian; Baudisch, Patrick
TI  - CHI Extended Abstracts - iTurk: Turning Passive Haptics into Active Haptics by Making Users Reconfigure Props in Virtual Reality
PY  - 2018
AB  - We present a system that complements virtual reality experiences with passive props, yet still allows modifying the virtual world at runtime. The main contribution of our system is that it does not require any actuators; instead, our system employs the user to reconfigure and actuate otherwise passive props. We demonstrate a foldable prop that users reconfigure to represent a suitcase, a fuse cabinet, a railing, and a seat. A second prop, suspended from a long pendulum, not only stands in for inanimate objects, but also for objects that move and demonstrate proactive behavior, such as a group of flying droids that physically attack the user. Our approach conveys a sense of a living, animate world, when in reality the user is the only animate entity present in the system, complemented with only one or two physical props. In our study, participants rated their experience as more enjoyable and realistic than a corresponding no-haptics condition.
SP  - 89
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186482
ER  - 

TY  - BOOK
AU  - Delamare, William; Silpasuwanchai, Chaklam; Sarcar, Sayan; Shiraki, Toshiaki; Ren, Xiangshi
TI  - ISS - On Gesture Combination: An Exploration of a Solution to Augment Gesture Interaction
PY  - 2019
AB  - Current gesture interaction paradigm mainly involves a one-to-one gesture-command mapping. This leads to memorability issues regarding (1) the mapping - as each new command requires a new gesture, and (2) the gestures specifics (e.g., motion paths) - that can be complex to leverage the recognition of several gestures. We explore the concept of combining 3D gestures when interacting in smart environments. We first propose a design space to characterize the temporal and spatial combination aspects, and the gesture types used by the combination. We then report results from three user studies in the context of smart TV interaction. The first study reveals that end-users can create gesture sets with combinations fully optimized to reuse gestures. The second study shows that combining gestures can lead to improved memorability compared to single gestures. The third study reveals that preferences for gestures combination appear when single gestures have an abstract gesture-command mapping.
SP  - 135
EP  - 146
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359706
ER  - 

TY  - NA
AU  - Joshi, Yashas; Poullis, Charalambos
TI  - Inattentional Blindness for Redirected Walking Using Dynamic Foveated Rendering
PY  - 2019
AB  - Redirected walking is a Virtual Reality(VR) locomotion technique which enables users to navigate virtual environments (VEs) that are spatially larger than the available physical tracked space. In this work we present a novel technique for redirected walking in VR based on the psychological phenomenon of inattentional blindness. Based on the user's visual fixation points we divide the user's view into zones. Spatially-varying rotations are applied according to the zone's importance and are rendered using foveated rendering. Our technique is real-time and applicable to small and large physical spaces. Furthermore, the proposed technique does not require the use of stimulated saccades but rather takes advantage of naturally occurring saccades and blinks for a complete refresh of the framebuffer. We performed extensive testing and present the analysis of the results of three user studies conducted for the evaluation.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Karolus, Jakob; Thanheiser, Simon; Peterson, David; Viot, Nicolas; Kosch, Thomas; Schmidt, Albrecht; Wozniak, Paweł W.
TI  - Imprecise but Fun
PY  - 2022
AB  - <jats:p>Novel input methods for game design often excite users, especially if they extend the way one interacts with the system. Electromyography (EMG) has the inherent potential to provide an intuitive - yet challenging - input channel for interactive systems. While this difficulty in control often limits the scope of applications for EMG in most systems, we argue that these qualities are especially relevant for games and playful interaction. The inherently challenging qualities of EMG input make the modality a prime candidate for designing body-centric playful experiences. Yet, we still need to understand its limitations to create engaging rather than frustrating experiences for users. In this work, we investigate EMG's potential to support playful interaction through exploratory studies, deriving feasible game interactions based on EMG's technical constraints, and study their application in game design. Based on our findings, we highlight design implications and pitfalls to avoid when creating EMG-based entertainment systems.</jats:p>
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546725
ER  - 

TY  - NA
AU  - Quintal, Filipe; Esteves, Augusto; Caires, Fabio; Baptista, Vitor; Mendes, Pedro
TI  - Tangible and Embedded Interaction - Wattom: A Consumption and Grid Aware Smart Plug with Mid-air Controls
PY  - 2019
AB  - This paper presents Wattom, a highly interactive ambient eco-feedback smart plug that aims to support a more sustainable use of electricity by being tightly coupled to users' energy-related activities. We describe three use cases of the system: using Wattom to power connected appliances and understand the environmental impact of their use in real time; scheduling these power events; and presenting users with personal consumption data desegregated by device. We conclude with a user study in which the effectiveness of the plug's novel interactive capabilities is assessed (mid-air, hand-based motion matching). The study explores the effectiveness of Wattom and motion matching input in a realistic setup, where the user is not always directly ahead of the interface, and not always willing to point straight at the device (e.g., when the plug is at an uncomfortable angle). Despite not using a graphical display, our results demonstrate that our motion matching implementation was effective in line with previous work, and that participants' pointing angle did not significantly affect their performance. On the other hand, participants were more effective while pointing straight at Wattom, but reported not to finding this significantly more strenuating then when pointing to a comfortable position of their choice.
SP  - 307
EP  - 313
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295642
ER  - 

TY  - NA
AU  - Rostami, Asreen; Karlgren, Kasper; McMillan, Donald
TI  - Kintsugi VR: Designing with Fractured Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - ACM International Conference on Interactive Media Experiences
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3505284.3529966
ER  - 

TY  - JOUR
AU  - Weigel, Martin; Steimle, Jürgen
TI  - DeformWear: Deformation Input on Tiny Wearable Devices
PY  - 2017
AB  - Due to their small surfaces, wearable devices make existing techniques for touch input very challenging. This paper proposes deformation input on a tiny and soft surface as an input modality for wearable computing devices. We introduce DeformWear, tiny wearable devices that leverage single-point deformation input on various body locations. Despite the small input surface, DeformWear enables expressive and precise input using high-resolution pressure, shear, and pinch deformations. We present a first set of interaction techniques for tiny deformation-sensitive wearable devices. They enable fluid interaction in a large input space by combining multiple dimensions of deformation. We demonstrate their use in seven application examples, showing DeformWear as a standalone input device and as a companion device for smartwatches, head-mounted displays, or headphones. Results from a user study demonstrate that these tiny devices allow for precise and expressive interactions on many body locations, in standing and walking conditions.
SP  - 28
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 2
PB  - 
DO  - 10.1145/3090093
ER  - 

TY  - JOUR
AU  - Fraune, Marlena R.; Khalaf, Ahmed S.; Zemedie, Mahlet; Pianpak, Poom; NaminiMianji, Zahra; Alharthi, A.; Dolgov, Igor; Hamilton, Bill; Tran, Son; Toups, Zachary O.
TI  - Developing Future Wearable Interfaces for Human-Drone Teams through a Virtual Drone Search Game
PY  - 2021
AB  - NA
SP  - 102573
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 147
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2020.102573
ER  - 

TY  - NA
AU  - Keshavarzi, Mohammad; Zollhoefer, Michael; Yang, Allen Y.; Peluse, Patrick; Caldas, Luisa
TI  - Synthesizing Novel Spaces for Remote Telepresence Experiences
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00111
ER  - 

TY  - BOOK
AU  - Yasuda, Motoki; Ota, Arisa; Tanaka, Fumihide
TI  - RoboSoft - Development of a Variable-Softness Robot by Using Thermoresponsive Hydrogels for Haptic Interaction with Humans
PY  - 2021
AB  - It is important for social robots to be capable of changing its behavior or other capacity to sustain interaction with the user. In this paper, we discuss changing the softness of the body of a robot. The robot is supposed to be used in haptic interaction contexts such as therapy. To sustain the interest of the user, the robot changes the softness of its body elements and provide the user with variable tactile sensations depending on the haptic interaction history. In this paper, we report the design process of our creating robot prototypes by using a thermoresponsive gel that changes in viscoelasticity with temperature variations. The gel is soft in an inactive state, whereas it becomes hard when it is activated by heat. After identifying a chemical composition that was suitable for building the variable-softness robot, we created octopus-like prototypes having tentacles whose softness could be changed based on tactile sensing. User tests were conducted to check if participants could recognize such softness changes and to discuss the feasibility and prospects of this approach.
SP  - 254
EP  - 260
JF  - 2021 IEEE 4th International Conference on Soft Robotics (RoboSoft)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/robosoft51838.2021.9479205
ER  - 

TY  - NA
AU  - McGrath, William; Warner, Jeremy; Karchemsky, Mitchell; Head, Andrew; Drew, Daniel S.; Hartmann, Bjoern
TI  - UIST - WiFröst: Bridging the Information Gap for Debugging of Networked Embedded Systems
PY  - 2018
AB  - The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFrost, a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFrost automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.
SP  - 447
EP  - 455
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242668
ER  - 

TY  - JOUR
AU  - Parilusyan, Brice; Teyssier, Marc; Martinez-Missir, Valentin; Duhart, Clément; Serrano, Marcos
TI  - Sensurfaces
PY  - 2022
AB  - <jats:p>Ubiquitous touch sensing surfaces are largely influenced by touchscreens' look and feel and fail to express the physical richness of existing surrounding materials. We introduce Sensurfaces, a plug-and-play electronic module that allows to rapidly experiment with touch-sensitive surfaces while preserving the original appearance of materials. Sensurfaces is composed of plug-and-play modules that can be connected together to expand the size and number of materials composing a sensitive surface. The combination of Sensurfaces modules allows the creation of small or large multi-material sensitive surfaces that can detect multi-touch but also body proximity, pose, pass, or even human steps. In this paper, we present the design and implementation of Sensurfaces. We propose a design space describing the factors of Sensurfaces interfaces. Then, through a series of technical evaluations, we demonstrate the capabilities of our system. Finally, we report on two workshops validating the usability of our system.</jats:p>
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534616
ER  - 

TY  - CONF
AU  - Lee, Gun A.; Rudhru, Omprakash; Park, Hye Sun; Kim, Ho Won; Billinghurst, Mark
TI  - ICAT-EGVE - User interface agents for guiding interaction with augmented virtual mirrors
PY  - NA
AB  - This research investigates using user interface (UI) agents for guiding gesture based interaction with Augmented Virtual Mirrors. Compared to prior work in gesture interaction, where graphical symbols are used for guiding user interaction, we propose using UI agents. We explore two approaches for using UI agents: 1) using a UI agent as a delayed cursor and 2) using a UI agent as an interactive button. We conducted two user studies to evaluate the proposed designs. The results from the user studies show that UI agents are effective for guiding user interactions in a similar way as a traditional graphical user interface providing visual cues, while they are useful in emotionally engaging with users.
SP  - 109
EP  - 116
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.2312/egve.20171347
ER  - 

TY  - NA
AU  - Yasu, Kentaro
TI  - CHI - Magnetact: Magnetic-sheet-based Haptic Interfaces for Touch Devices
PY  - 2019
AB  - We describe a method for rapid prototyping of haptic interfaces for touch devices. A sheet-like touch interface is constructed from magnetic rubber sheets and conductive materials. The magnetic sheet is thin, and the capacitive sensor of the touch device can still detect the user's finger above the sheet because of the rubber's dielectric nature. Furthermore, tactile feedback can be customized with ease by using our magnetizing toolkit to change the magnetic patterns. Using the magnetizing toolkit, we investigated the appropriate size and thickness of haptic interfaces and demonstrated several interfaces such as buttons, sliders, switches, and dials. Our method is an easy and convenient way to customize the size, shape, and haptic feedback of a wide variety of interfaces.
SP  - 240
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300470
ER  - 

TY  - NA
AU  - Tseng, Juin-Ling; Jiang, Yan-Yi; Peng, Sheng-Jun; Wei, Hsiao-En
TI  - Development of Intelligent Tourism Information System Based On Virtual Reality
PY  - 2018
AB  - Most tourism attractions are introduced to the public by multimedia information such as text, images and videos. They are well received by visitors but nevertheless fall short in conveying more intuitive and authentic feelings. To overcome this problem, this paper develops an intelligent tourism information system. It uses virtual reality technology to show the landscape information intelligently by detecting what the user sees. The experimental results demonstrate that the system can smartly identify what the user sees and present the corresponding information in real time.
SP  - NA
EP  - NA
JF  - 2018 IEEE International Conference on Advanced Manufacturing (ICAM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/amcon.2018.8615073
ER  - 

TY  - NA
AU  - Hinckley, Ken; Heo, Seongkook; Pahud, Michel; Holz, Christian; Benko, Hrvoje; Sellen, Abigail; Banks, Richard; O'Hara, Kenton; Smyth, Gavin; Buxton, William
TI  - Pre-Touch Sensing for Mobile Interaction
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858095
ER  - 

TY  - NA
AU  - Rajaram, Shwetha; Nebeling, Michael
TI  - Paper Trail: An Immersive Authoring System for Augmented Reality Instructional Experiences
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517486
ER  - 

TY  - NA
AU  - Gupta, Aakar
TI  - Extended Hand Attributes for Touch Input, Touch Output and Touchless Interaction
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Amesaka, Takashi; Watanabe, Hiroki; Sugimoto, Masanori; Shizuki, Buntarou
TI  - Gesture Recognition Method Using Acoustic Sensing on Usual Garment
PY  - 2022
AB  - <jats:p>In this study, we show a new gesture recognition method for clothing-based gesture input methods using active and passive acoustic sensing. Our system consists of a piezoelectric speaker and a microphone. The speaker transmits ultrasonic swept sine signals, and the microphone simultaneously records the ultrasonic signals that propagate through the garment and the rubbing sounds generated by the gestures on the garment. Our method recognizes a variety of gestures, such as pinch, twist, touch, and swipe, by incorporating active and passive acoustic sensing. An important feature of our method is that it does not require a dedicated garment or embroidery embedded since our system only requires a pair of piezoelectric elements to be attached to the usual garment with a magnet. We performed recognition experiments of 11 gestures on the forearm with four types of garments made from different materials and recognition experiments of five one-handed gestures on the button of a shirt and the pocket of pants. The results of a per-user classifier confirmed that the f-scores were 83.9% and 95.9% for 11 gestures with four different types of garments and 5 gestures that were selected assuming actual use, respectively. In addition, we confirmed that the system recognizes five gestures, which can be performed with one hand, with 89.2% and 92.6% accuracy in the button and pocket sites, respectively.</jats:p>
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534579
ER  - 

TY  - JOUR
AU  - Zucco, Joanne E.; Thomas, Bruce H.
TI  - Design Guidelines for Wearable Pointing Devices
PY  - 2016
AB  - This paper presents design guidelines and recommendations for developing cursor manipulation interaction devices to be employed in a wearable context. The work presented in this paper is the culmination three usability studies designed to understand commercially available pointing (cursor manipulation) devices suitable for use in a wearable context. The set of guidelines and recommendations presented are grounded on experimental and qualitative evidence derived from three usability studies and are intended to be used in order to inform the design of future wearable input devices. In addition to guiding the design process, the guidelines and recommendations may also be used to inform users of wearable computing devices by guiding towards the selection of a suitable wearable input device. The synthesis of results derived from a series of usability studies provide insights pertaining to the choice and usability of the devices in a wearable context. That is, the guidelines form a checklist that may be utilized as a point of comparison when choosing between the different input devices available for wearable interaction.
SP  - 13
EP  - NA
JF  - Frontiers in ICT
VL  - 3
IS  - NA
PB  - 
DO  - 10.3389/fict.2016.00013
ER  - 

TY  - NA
AU  - Hite, Rebecca; Childers, Gina; Jones, M. Gail
TI  - Review of Virtual Reality Hardware Employed in K-20 Science Education
PY  - 2019
AB  - NA
SP  - 1389
EP  - 1399
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gruebele, Alexander M.; Zerbe, Andrew C.; Coad, Margaret M.; Okamura, Allison M.; Cutkosky, Mark R.
TI  - Distributed Sensor Networks Deployed Using Soft Growing Robots.
PY  - 2020
AB  - Due to their ability to move without sliding relative to their environment, soft growing robots are attractive for deploying distributed sensor networks in confined spaces. Sensing of the state of such robots would also add to their capabilities as human-safe, adaptable manipulators. However, incorporation of distributed sensors onto soft growing robots is challenging because it requires an interface between stiff and soft materials, and the sensor network needs to undergo significant strain. In this work, we present a method for adding sensors to soft growing robots that uses flexible printed circuit boards with self-contained units of microcontrollers and sensors encased in a laminate armor that protects them from unsafe curvatures. We demonstrate the ability of this system to relay directional temperature and humidity information in hard-to-access spaces. We also demonstrate and characterize a method for sensing the growing robot shape using inertial measurement units deployed along its length, and develop a mathematical model to predict its accuracy. This work advances the capabilities of soft growing robots, as well as the field of soft robot sensing.
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Chu, Shaowei; Tu, Huawei
TI  - A Comparative Evaluation of Mechanical Vibration and Ultrasonic Vibration on Smartphones in Tactile Code Perception
PY  - 2022
AB  - NA
SP  - 41038
EP  - 41046
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3167526
ER  - 

TY  - NA
AU  - Lu, Qiuyu; Mao, Chengpeng; Wang, Liyuan; Mi, Haipeng
TI  - UIST - LIME: LIquid MEtal Interfaces for Non-Rigid Interaction
PY  - 2016
AB  - Room-temperature liquid metal GaIn25 (Eutectic Gallium- Indium alloy, 75% gallium and 25% indium) has distinctive properties of reversible deformation and controllable locomotion under an external electric field stimulus. Liquid metal's newly discovered properties imply great possibilities in developing new technique for interface design. In this paper, we present LIME, LIquid MEtal interfaces for non-rigid interaction. We first discuss the interaction potential of LIME interfaces. Then we introduce the development of LIME cells and the design of some LIME widgets.
SP  - 449
EP  - 452
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984562
ER  - 

TY  - NA
AU  - Cheng, Lung-Pan; Chang, Li; Marwecki, Sebastian; Baudisch, Patrick
TI  - iTurk
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173663
ER  - 

TY  - JOUR
AU  - Jiang, Xudong; Zhu, Lifeng; Liu, Jia; Song, Aiguo
TI  - A SLAM-based 6DoF controller with smooth auto-calibration for virtual reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Visual Computer
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s00371-022-02530-1
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Wang, Po-Yao; Cheng, Jen-Hao; Ning, Chih-Wei; Wang, Ping-Yi; Cheng, Lung-Pan
TI  - SIGGRAPH Immersive Pavilion - Haptic-go-round: A Surrounding Platform for Encounter-type Haptic in Virtual Reality Experiences
PY  - 2020
AB  - We present Haptic-go-round, a surrounding platform that allows deploying props and devices to provide haptic feedbacks in any direction in virtual reality experiences. The key component of Haptic-go-round is a motorized turntable that rotates the correct haptic device to the right direction at the right time to match what users are about to touch. We implemented a working platform including plug-and-play prop cartridges and a software interface that allow experience designers to agilely add their haptic components and use the platform for their applications.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2020 Immersive Pavilion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3388536.3407886
ER  - 

TY  - CHAP
AU  - R., Sudha M.; K., Sriraghav; S., Sudar Abisheck; Jacob, Shomona Gracia; S., Manisha
TI  - Approaches and Applications of Virtual Reality and Gesture Recognition
PY  - 2018
AB  - <jats:p>Interaction with a computer has been the center of innovation ever since the advent of input devices. From simple punch cards to keyboards, there are number of novel ways of interaction with computers which influence the user experience. Communicating using gestures is perhaps one of the most natural ways of interaction. Gesture recognition as a tool for interpreting signs constitutes a pivotal area in gesture recognition research where accuracy of the algorithm and the ease of usability determine the effectiveness of the algorithm or system. Introducing gesture based interaction in Virtual reality applications has not only helped solve problems which were commonly reported in traditional Virtual Reality systems, but also gives user a more natural and enriching experience. This paper concentrates on comparison of different systems and identifying their similarities, differences, advantages and demerits which can play a key role in designing a system using such technologies.</jats:p>
SP  - 180
EP  - 199
JF  - Virtual and Augmented Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.4018/978-1-5225-5469-1.ch009
ER  - 

TY  - JOUR
AU  - Babic, Teo; Reiterer, Harald; Haller, Michael
TI  - Understanding and Creating Spatial Interactions with Distant Displays Enabled by Unmodified Off-The-Shelf Smartphones
PY  - 2022
AB  - <jats:p>Over decades, many researchers developed complex in-lab systems with the overall goal to track multiple body parts of the user for a richer and more powerful 2D/3D interaction with a distant display. In this work, we introduce a novel smartphone-based tracking approach that eliminates the need for complex tracking systems. Relying on simultaneous usage of the front and rear smartphone cameras, our solution enables rich spatial interactions with distant displays by combining touch input with hand-gesture input, body and head motion, as well as eye-gaze input. In this paper, we firstly present a taxonomy for classifying distant display interactions, providing an overview of enabling technologies, input modalities, and interaction techniques, spanning from 2D to 3D interactions. Further, we provide more details about our implementation—using off-the-shelf smartphones. Finally, we validate our system in a user study by a variety of 2D and 3D multimodal interaction techniques, including input refinement.</jats:p>
SP  - 94
EP  - 94
JF  - Multimodal Technologies and Interaction
VL  - 6
IS  - 10
PB  - 
DO  - 10.3390/mti6100094
ER  - 

TY  - NA
AU  - Jain, Harshika; Lu, Kexin; Yao, Lining
TI  - Conference on Designing Interactive Systems - Hydrogel-based DIY Underwater Morphing Artifacts: A morphing and fabrication technique to democratize the creation of controllable morphing 3D underwater structures with low-cost, easily available hydrogel beads adhered to a substrate.
PY  - 2021
AB  - Hydrogels are versatile morphing materials that have recently been adopted for creating shape-changing interfaces. However, most shape-changing interfaces require advanced material synthesis, specialized lab settings for fabrication, and technical knowledge is needed to simulate their morphing behavior. To replicate such structures, these factors become a barrier for makers. Therefore, to democratize the creation of hydrogel-based morphing artifacts and to extend their design space in HCI, we propose a water-triggered morphing mechanism that utilizes the distance between adjacent hydrogel beads adhered on a thin substrate to control their bending angle. This paper describes the bending angle quantification experiments for creating a simulator, the process of developing a computational tool along with its user-friendly workflow and demonstrates kirigami and branch-based artifacts built with the tool. Using our method, anyone can easily design and fabricate custom morphing structures.
SP  - 1242
EP  - 1252
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462136
ER  - 

TY  - NA
AU  - Chen, Qin; Perrault, Simon T.; Roy, Quentin; Wyse, Lonce
TI  - AVI - Effect of temporality, physical activity and cognitive load on spatiotemporal vibrotactile pattern recognition
PY  - 2018
AB  - Previous research demonstrated the ability for users to accurately recognize Spatiotemporal Vibrotactile Patterns (SVP): sequences of vibrations on different motors occurring either sequentially or simultaneously. However, the experiments were only run in a lab setting and the ability for users to recognize SVP in a real-world environment remains unclear. In this paper, we investigate how several factors may affect recognition: (1) physical activity (running), (2) cognitive task (i.e. primary task, typing), (3) distribution of the vibration motors across body parts and (4) temporality of the patterns. Our results suggest that physical activity has very little impact, specifically compared to cognitive task, location of the vibrations or temporality. We discuss these results and propose a set of guidelines for the design of SVPs.
SP  - 25
EP  - NA
JF  - Proceedings of the 2018 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206505.3206511
ER  - 

TY  - NA
AU  - Glassman, Elena L.; Miller, Robert C.
TI  - CSCW Companion - Leveraging Learners for Teaching Programming and Hardware Design at Scale
PY  - 2016
AB  - In a massive open online course (MOOC), a single pro-gramming or digital hardware design exercise may yield thousands of student solutions that vary in many ways, some superficial and some fundamental. Understanding large-scale variation in student solutions is a hard but important problem. For teachers, this variation can be a source of pedagogically valuable examples and expose corner cases not yet covered by autograding. For students, the variation in a large class means that other students may have struggled along a similar solution path, hit the same bugs, and can offer hints based on that earned expertise. We developed three systems to take advantage of the solu-tion variation in large classes, using program analysis and learnersourcing. All three systems have been evaluated using data or live deployments in on-campus or edX courses with thousands of students.
SP  - 37
EP  - 40
JF  - Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2818052.2874319
ER  - 

TY  - NA
AU  - Dhuliawala, Murtaza; Lee, Juyoung; Shimizu, Junichi; Bulling, Andreas; Kunze, Kai; Starner, Thad; Woo, Woontack
TI  - ICMI - Smooth eye movement interaction using EOG glasses
PY  - 2016
AB  - Orbits combines a visual display and an eye motion sensor to allow a user to select between options by tracking a cursor with the eyes as the cursor travels in a circular path around each option. Using an off-the-shelf Jins MEME pair of eyeglasses, we present a pilot study that suggests that the eye movement required for Orbits can be sensed using three electrodes: one in the nose bridge and one in each nose pad. For forced choice binary selection, we achieve a 2.6 bits per second (bps) input rate at 250ms per input. We also inntroduce Head Orbits, where the user fixates the eyes on a target and moves the head in synchrony with the orbiting target. Measuring only the relative movement of the eyes in relation to the head, this method achieves a maximum rate of 2.0 bps at 500ms per input. Finally, we combine the two techniques together with a gyro to create an interface with a maximum input rate of 5.0 bps.
SP  - 307
EP  - 311
JF  - Proceedings of the 18th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2993148.2993181
ER  - 

TY  - JOUR
AU  - Zorzal, Ezequiel Roberto; Gomes, José Miguel Campos; Sousa, Maurício; Belchior, Pedro; da Silva, Pedro Garcia; Figueiredo, Nuno; Lopes, Daniel Simões; Jorge, Joaquim
TI  - Laparoscopy with augmented reality adaptations
PY  - 2020
AB  - One of the most promising applications of Optical See-Through Augmented Reality is minimally laparoscopic surgery, which currently suffers from problems such as surgeon discomfort and fatigue caused by looking at a display positioned outside the surgeon's visual field, made worse by the length of the procedure. This fatigue is especially felt on the surgeon's neck, as it is strained from adopting unnatural postures in order to visualise the laparoscopic video feed. Throughout this paper, we will present work in Augmented Reality, as well as developments in surgery and Augmented Reality applied to both surgery in general and laparoscopy in particular to address these issues. We applied user and task analysis methods to learn about practices performed in the operating room by observing surgeons in their working environment in order to understand, in detail, how they performed their tasks and achieved their intended goals. Drawing on observations and analysis of video recordings of laparoscopic surgeries, we identified relevant constraints and design requirements. Besides proposals to approach the ergonomic issues, we present a design and implementation of a multimodal interface to enhance the laparoscopic procedure. Our method makes it more comfortable for surgeons by allowing them to keep the laparoscopic video in their viewing area regardless of neck posture. Also, our interface makes it possible to access patient imaging data without interrupting the operation. It also makes it possible to communicate with team members through a pointing reticle. We evaluated how surgeons perceived the implemented prototype, in terms of usefulness and usability, via a think-aloud protocol to conduct qualitative evaluation sessions which we describe in detail in this paper. In addition to checking the advantages of the prototype as compared to traditional laparoscopic settings, we also conducted a System Usability Scale questionnaire for measuring its usability, and a NASA Task Load Index questionnaire to rate perceived workload and to assess the prototype effectiveness. Our results show that surgeons consider that our prototype can improve surgeon-to-surgeon communication using head pose as a means of pointing. Also, surgeons believe that our approach can afford a more comfortable posture throughout the surgery and enhance hand-eye coordination, as physicians no longer need to twist their necks to look at screens placed outside the field of operation.
SP  - 103463
EP  - NA
JF  - Journal of biomedical informatics
VL  - 107
IS  - NA
PB  - 
DO  - 10.1016/j.jbi.2020.103463
ER  - 

TY  - NA
AU  - Blum, Jeffrey R.; Cauchard, Jessica R.; Cooperstock, Jeremy R.
TI  - HAPTICS - Habituation to Pseudo-Ambient Vibrotactile Patterns for Remote Awareness
PY  - 2020
AB  - Habituation is a key aspect of the human sensory processing system. This includes the sense of touch, since it allows our skin receptors to be constantly stimulated, yet largely ignored until something of interest occurs or we consciously focus our attention on the sensations and their meanings. This "ambience" is largely lacking in mobile and wearable systems today, as jarring notifications clamor for our attention. Yet, there are few longitudinal, in-the-wild studies that explore whether and how users can habituate to new ongoing haptic stimuli, especially in practical applications. We report on a three-week in-the-wild study with each participant wearing a vibrotactile device throughout every day. The device rendered two brief vibrotactile pulses every 20 seconds, and varied their durations based on a linked partner’s current activity. Some participants had little difficulty acclimating to the system from the very beginning, but practically all expressed at least some days of annoyance/distraction within the first week. Despite considerable variation among participants, we find a significant drop in both annoyance and distraction over the multiple weeks of the study. A clear majority no longer report annoyance or distraction by the end of the experiment, indicating habituation.
SP  - 657
EP  - 663
JF  - 2020 IEEE Haptics Symposium (HAPTICS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/haptics45997.2020.ras.hap20.153.550dbcba
ER  - 

TY  - NA
AU  - Gomez, Argenis Ramirez; Gellersen, Hans
TI  - FDG - KryptonEyed: Playing with Gaze Without Looking
PY  - 2020
AB  - As eye-tracking technologies become more affordable, the number of mainstream gaze-enabled games increases. These allow triggering in-game actions when the eyes focus on objects and locations of interest. Such gaze interactions follow the interaction paradigm ”what you look at is what you get”. We challenge this use of gaze interaction and propose to play without looking - with the eyes closed. We designed the game prototype KryptonEyed to introduce closing the eyes for eyes-only game control. Players are required to close their eyes and perform eye movements behind the eyelids before opening them to aim the teleportation of the main character. The game contains three levels integrating the proposed gaze mechanic in distinct game scenarios. These explore different challenges in their game dynamics and interaction metaphors to use the technique in various contexts of play.
SP  - NA
EP  - NA
JF  - International Conference on the Foundations of Digital Games
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3402942.3403017
ER  - 

TY  - NA
AU  - Schenk, Simon; Tiefenbacher, Philipp; Rigoll, Gerhard; Dorr, Michael
TI  - CHI Extended Abstracts - SPOCK: A Smooth Pursuit Oculomotor Control Kit
PY  - 2016
AB  - Gaze holds great potential for fast and intuitive hands-free user interaction. However, existing methods typically suffer from the Midas touch problem, i.e. the difficult distinction between gaze for perception and for user action; proposed solutions have required custom-tailored, application-specific user interfaces. Here, we present SPOCK, a novel gaze interaction method based on smooth pursuit eye movements requiring only minimal extensions to button-based interfaces. Upon looking at a UI element, two overlaid dynamic stimuli appear and tracking one of them triggers activation. In contrast to fixations and saccades, smooth pursuits are not only easily performed, but also easily suppressed, thus greatly reducing the Midas touch problem. We evaluated SPOCK against dwell time, the state-of-the-art gaze interaction method, in a simple target selection and a more challenging multiple-choice scenario. At higher task difficulty, unintentional target activations were reduced almost 15-fold by SPOCK, making this a promising method for gaze interaction.
SP  - 2681
EP  - 2687
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892291
ER  - 

TY  - NA
AU  - Huang, Michael Xuelin; Li, Jiajia; Ngai, Grace; Leong, Hong Va
TI  - CHI - ScreenGlint: Practical, In-situ Gaze Estimation on Smartphones
PY  - 2017
AB  - Gaze estimation has widespread applications. However, little work has explored gaze estimation on smartphones, even though they are fast becoming ubiquitous. This paper presents ScreenGlint, a novel approach which exploits the glint (reflection) of the screen on the user's cornea for gaze estimation, using only the image captured by the front-facing camera. We first conduct a user study on common postures during smartphone use. We then design an experiment to evaluate the accuracy of ScreenGlint under varying face-to-screen distances. An in-depth evaluation involving multiple users is conducted and the impact of head pose variations is investigated. ScreenGlint achieves an overall angular error of 2.44o without head pose variations, and 2.94o with head pose variations. Our technique compares favorably to state-of-the-art research works, indicating that the glint of the screen is an effective and practical cue to gaze estimation on the smartphone platform. We believe that this work can open up new possibilities for practical and ubiquitous gaze-aware applications.
SP  - 2546
EP  - 2557
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025794
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Saltuk, Ozan; Hang, Alina; Stolz, Katharina; Bulling, Andreas; Alt, Florian
TI  - UbiComp - TextPursuits: using text for pursuits-based interaction and calibration on public displays
PY  - 2016
AB  - In this paper we show how reading text on large display can be used to enable gaze interaction in public space. Our research is motivated by the fact that much of the content on public displays includes text. Hence, researchers and practitioners could greatly benefit from users being able to spontaneously interact as well as to implicitly calibrate an eye tracker while simply reading this text. In particular, we adapt Pursuits, a technique that correlates users' eye movements with moving on-screen targets. While prior work used abstract objects or dots as targets, we explore the use of Pursuits with text (read-and-pursue). Thereby we address the challenge that eye movements performed while reading interfere with the pursuit movements. Results from two user studies (N=37) show that Pursuits with text is feasible and can achieve similar accuracy as non text-based pursuit approaches. While calibration is less accurate, it integrates smoothly with reading and allows areas of the display the user is looking at to be identified.
SP  - 274
EP  - 285
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971648.2971679
ER  - 

TY  - NA
AU  - Kono, Michinari; Rekimoto, Jun
TI  - wavEMS: Improving Signal Variation Freedom of Electrical Muscle Stimulation
PY  - 2019
AB  - There has been a long history in electrical muscle stimulation (EMS), which has been used for medical and interaction purposes. Human-computer interaction (HCI) researchers are now working on various applications, including virtual reality (VR), notification, and learning. For the electric signals applied to the human body, various types of waveforms have been considered and tested. In typical applications, pulses with short duration are applied, however, many perspectives are required to be considered. In addition to the duration and polarity of the pulse/waves, the wave shapes can also be an essential factor to consider. A problem of conventional EMS toolkits and systems are that they have a limitation to the variety of signals that it can produce. For example, some may be limited to monophonic pulses. Furthermore, they are usually limited to rectangular pulses and a limited range of frequencies, and other waveforms cannot be produced. These kinds of limitations make us challenging to consider variations of EMS signals in HCI research and applications. The purpose of "{\it wavEMS}" is to encourage testing of a variety of waveforms for EMS, which can be manipulated through audio output. We believe that this can help improve HCI applications, and to open up new application areas.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Blascheck, Tanja; Bezerianos, Anastasia; Besançon, Lonni; Lee, Bongshin; Isenberg, Petra
TI  - Preparing for Perceptual Studies: Position and Orientation of Wrist-worn Smartwatches for Reading Tasks
PY  - 2018
AB  - Despite the increasing demand for data visualization on mobile devices with small displays, few guidelines exist for designing visualizations for this form factor. To conduct perceptual studies with smartwatches under realistic conditions , we first need to know how to position these devices in front of a viewer. We report the results of a study, in which we investigate how people hold their smartwatches to read information. This is the first in a series of studies we are conducting to understand the perception of visualizations on smartwatches. Our study results show that people hold their watches at a distance of 28 cm in front of them, at a pitch angle of ~50 degrees, and at an angle of ~10 degrees from the line of sight.
SP  - 1
EP  - 6
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ion, Alexandra; Kovacs, Robert; Schneider, Oliver; Lopes, Pedro; Baudisch, Patrick
TI  - Metamaterial Textures
PY  - 2018
AB  - We present metamaterial textures---3D printed surface geometries that can perform a controlled transition between two or more textures. Metamaterial textures are integrated into 3D printed objects and allow designing how the object interacts with the environment and the user's tactile sense. Inspired by foldable paper sheets ("origami") and surface wrinkling, our 3D printed metamaterial textures consist of a grid of cells that fold when compressed by an external global force. Unlike origami, however, metamaterial textures offer full control over the transformation, such as in between states and sequence of actuation. This allows for integrating multiple textures and makes them useful, e.g., for exploring parameters in the rapid prototyping of textures. Metamaterial textures are also robust enough to allow the resulting objects to be grasped, pushed, or stood on. This allows us to make objects, such as a shoe sole that transforms from flat to treaded, a textured door handle that provides tactile feedback to visually impaired users, and a configurable bicycle grip. We present an editor assists users in creating metamaterial textures interactively by arranging cells, applying forces, and previewing their deformation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173910
ER  - 

TY  - NA
AU  - Duchowski, Andrew T.
TI  - SIGGRAPH Courses - Eye-based interaction in graphical systems: 20 years later gaze applications, analytics, & interaction
PY  - 2020
AB  - The course starts with an overview of eye-tracking applications, distinguishing eye movement analysis from synthesis in virtual reality, games, and other venues including mobile eye tracking. The focus is on four forms of applications: diagnostic (off-line measurement), active (selection, look to shoot), passive (foveated rendering, a.k.a. gaze-contingent displays), and expressive (gaze synthesis). The course covers basic eye movement analytics, e.g., fixation count and dwell time within AOIs, as well as advanced analysis using ambient/focal attention modeling. The course concludes with an overview and a demo of how to build an interactive application using Python.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2020 Courses
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3388769.3407492
ER  - 

TY  - NA
AU  - Chen, Hao; Wang, Yujia; Liang, Wei
TI  - VCoach: Enabling Personalized Boxing Training in Virtual Reality
PY  - 2022
AB  - We propose a training system in virtual reality, VCoach, automati-cally generating interactive and personalized boxing training drills for individual trainees. The training drill is generated in real-time based on the trainee&#x0027;s updated performance observed through wear-able VR devices, including punch speed, reaction time, and punch pose. The training drill is visualized as a sequence of target points on a virtual heavy bag and the corresponding punch motion, as well as the performance feedback. Our experiments show that VCoach can generate personalized training drills to help trainees improve skills efficiently.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00124
ER  - 

TY  - JOUR
AU  - Lyu, Geng; Shen, Xukun; Komura, Taku; Subr, Kartic; Teng, Lijun
TI  - Widening Viewing Angles of Automultiscopic Displays Using Refractive Inserts
PY  - 2018
AB  - Displays that can portray environments that are perceivable from multiple views are known as multiscopic displays. Some multiscopic displays enable realistic perception of 3D environments without the need for cumbersome mounts or fragile head-tracking algorithms. These automultiscopic displays carefully control the distribution of emitted light over space, direction (angle) and time so that even a static image displayed can encode parallax across viewing directions (Iightfield). This allows simultaneous observation by multiple viewers, each perceiving 3D from their own (correct) perspective. Currently, the illusion can only be effectively maintained over a narrow range of viewing angles. In this paper, we propose and analyze a simple solution to widen the range of viewing angles for automultiscopic displays that use parallax barriers. We propose the use of a refractive medium, with a high refractive index, between the display and parallax barriers. The inserted medium warps the exitant lightfield in a way that increases the potential viewing angle. We analyze the consequences of this warp and build a prototype with a 93% increase in the effective viewing angle.
SP  - 1554
EP  - 1563
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 4
PB  - 
DO  - 10.1109/tvcg.2018.2794599
ER  - 

TY  - BOOK
AU  - Takahashi, Akifumi; Tanabe, Kenta; Kajimoto, Hiroyuki
TI  - AsiaHaptics - Relationship Between Force Sensation and Stimulation Parameters in Tendon Electrical Stimulation
PY  - 2017
AB  - Most haptic devices have the common problem of requiring a large hardware setup, because they must present actual force. To cope with this issue, we have proposed a method to present force sensation using tendon electrical stimulation. In this paper, we investigated whether it is possible to present force sensation by electrically stimulating the tendon through the skin surface at the wrist. We also investigated the relationship between the amount of sensation and the stimulation parameters. We found that a force sensation can be generated by electrical stimulation to the wrist, and the direction of the force sensation is opposite to the motion elicited by muscle electrical stimulation. We also found that it is possible to control the amount of the sensation by varying both pulse height and pulse frequency.
SP  - 233
EP  - 238
JF  - Lecture Notes in Electrical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-10-4157-0_40
ER  - 

TY  - NA
AU  - Kono, Michinari; Miyaki, Takashi; Rekimoto, Jun
TI  - VRST - In-pulse: inducing fear and pain in virtual experiences
PY  - 2018
AB  - Researchers have attempted to increase the realism of virtual reality (VR) applications in many ways. Combinations of the visual, auditory and haptic feedback have successfully simulated experiences in VR, however, multimedia contents may also stimulate emotions. In this paper, we especially paid attention to negative emotions that may be perceived in such experiences (e.g., fear). We hypothesized that volunteering, visual, mechanical, and electrical feedback may induce negative emotional feedback to users. In-Pulse is a novel system and approach to explore the potential of bringing this emotional feedback to users. We designed a head-mounted display (HMD) combined with mechanical and electrical muscle stimulation (EMS) actuators. A user study was performed to explore the effect of our approaches with combinations with VR contents. The results suggest that mechanical actuators and EMS can improve the experience of virtual experiences.
SP  - NA
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281506
ER  - 

TY  - NA
AU  - Lu, Qiuyu; Ou, Jifei; Wilbert, João; Haben, André; Mi, Haipeng; Ishii, Hiroshi
TI  - UIST - milliMorph -- Fluid-Driven Thin Film Shape-Change Materials for Interaction Design
PY  - 2019
AB  - This paper presents a design space, a fabrication system and applications of creating fluidic chambers and channels at millimeter scale for tangible actuated interfaces. The ability to design and fabricate millifluidic chambers allows one to create high frequency actuation, sequential control of flows and high resolution design on thin film materials. We propose a four dimensional design space of creating these fluidic chambers, a novel heat sealing system that enables easy and precise millifluidics fabrication, and application demonstrations of the fabricated materials for haptics, ambient devices and robotics. As shape-change materials are increasingly integrated in designing novel interfaces, milliMorph enriches the library of fluid-driven shape-change materials, and demonstrates new design opportunities that is unique at millimeter scale for product and interaction design.
SP  - 663
EP  - 672
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347956
ER  - 

TY  - NA
AU  - Feick, Martin; Bateman, Scott; Tang, Anthony; Miede, André; Marquardt, Nicolai
TI  - ISMAR - Tangi: Tangible Proxies For Embodied Object Exploration And Manipulation In Virtual Reality
PY  - 2020
AB  - Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi’s Manipulators and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi’s proxies outperform traditional controllers and were generally favored by participants.
SP  - 195
EP  - 206
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00042
ER  - 

TY  - JOUR
AU  - Wang, Xu; Thompson, Meredith; Yang, Kexin; Roy, Dan; Koedinger, Kenneth R.; Rosé, Carolyn Penstein; Reich, Justin
TI  - Practice-Based Teacher Questioning Strategy Training with ELK: A Role-Playing Simulation for Eliciting Learner Knowledge
PY  - 2021
AB  - Practice is essential for learning. However, for many interpersonal skills, there often are not enough opportunities and venues for novices to repeatedly practice. Role-playing simulations offer a promising framework to advance practice-based professional training for complex communication skills, in fields such as teaching. In this work, we introduce ELK (Eliciting Learner Knowledge), a role-playing simulation system that helps K-12 teachers develop effective questioning strategies to elicit learners' prior knowledge. We evaluate ELK with 75 pre-service teachers through a mixed-method study. We find that teachers demonstrate a modest increase in effective questioning strategies and develop sympathy towards students after using ELK for 3 rounds. We implement a supplementary activity in ELK in which users evaluate transcripts generated from past role-play sessions. We have tentative evidence that a combination of role-play and evaluating conversation moves may be more effective for learning. We contribute design implications of using role-play systems for communication strategy training.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
PB  - 
DO  - 10.1145/3449125
ER  - 

TY  - NA
AU  - Kono, Michinari; Rekimoto, Jun
TI  - VR - wavEMS: Improving Signal Variation Freedom of Electrical Muscle Stimulation
PY  - 2019
AB  - There has been a long history in electrical muscle stimulation (EMS), which has been used for medical and interaction purposes. Human-computer interaction (HCI) researchers are now working on various applications, including virtual reality (VR), notification, and learning. For the electric signals applied to the human body, various types of waveforms have been considered and tested. In typical applications, pulses with short duration are applied, however, many perspectives are required to be considered. In addition to the duration and polarity of the pulse/waves, the wave shapes can also be an essential factor to consider. A problem of conventional EMS toolkits and systems are that they have a limitation to the variety of signals that it can produce. For example, some may be limited to monophonic pulses. Furthermore, they are usually limited to rectangular pulses and a limited range of frequencies, and other waveforms cannot be produced. These kinds of limitations make us challenging to consider variations of EMS signals in HCI research and applications. The purpose of “wavEMS” is to encourage testing of a variety of waveforms for EMS, which can be manipulated through audio output. We believe that this can help improve HCI applications, and to open up new application areas.
SP  - 1529
EP  - 1532
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798102
ER  - 

TY  - JOUR
AU  - Ray, Rahul Kumar; Patel, Payal; Manivannan, M.
TI  - Reduction of electrotactile perception threshold using subthreshold vibrotactile stimuli
PY  - 2021
AB  - NA
SP  - 102056
EP  - NA
JF  - Displays
VL  - 69
IS  - NA
PB  - 
DO  - 10.1016/j.displa.2021.102056
ER  - 

TY  - JOUR
AU  - Shao, Xuqiang; Xiaohua, Feng; Yu, Yelu; Wu, Zhaohui; Mei, Peng
TI  - A Natural Interaction Method of Multi-Sensory Channels for Virtual Assembly System of Power Transformer Control Cabinet
PY  - 2020
AB  - The more sensory channels are equipped in a virtual assembly system, the more real users feel in the whole system, however, most of the existing virtual assembly systems are based on the natural interaction method of one or two sensory channels. Thus, this paper proposes a novel virtual assembly system integrating multi-sensory channels, including gesture interaction, Chinese speech interaction, tactile interaction, 3D display and real-time display of real environment pictures in the virtual environment. To improve the operability of the virtual environment, we analyze the parallel virtual assembly sequence on the basis of two-hand interaction, and the assembly priority is prompted based on UI interface. For ease of operation, we present a method of viewpoint control based on gesture interaction and the coordinate threshold of spatial position. A hierarchical bounding box collision detection algorithm based on volume difference is proposed to improve the efficiency of collision feedback and collision avoidance. In addition, the power equipment models are exhibited in the virtual scene, as the exhibits of the virtual roaming process. Finally, to evaluate the training effect of this system, a comparative experiment is designed to compare the participants’ experience and the effect of assembly training. The experimental results show that the virtual assembly system of natural interaction with multi-sensory channels is flexible and immersive.
SP  - 54699
EP  - 54709
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.2981539
ER  - 

TY  - NA
AU  - Pai, Yun Suen
TI  - UIST (Adjunct Volume) - Physiological Signal-Driven Virtual Reality in Social Spaces
PY  - 2016
AB  - Virtual and augmented reality are becoming the new medium that transcend the way we interact with virtual content, paving the way for many immersive and interactive forms of applications. The main purpose of my research is to create a seamless combination of physiological sensing with virtual reality to provide users with a new layer of input modality or as a form of implicit feedback. To achieve this, my research focuses in novel augmented reality (AR) and virtual reality (VR) based application for a multi-user, multi-view, multi-modal system augmented by physiological sensing methods towards an increased public and social acceptance.
SP  - 25
EP  - 28
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2984787
ER  - 

TY  - NA
AU  - Swidan, Alaaeddin; Serebrenik, Alexander; Hermans, Felienne
TI  - SCAM - How do Scratch Programmers Name Variables and Procedures
PY  - 2017
AB  - Research shows the importance of selecting good names to identifiers in software code: more meaningful names improve readability. In particular, several guidelines encourage long and descriptive variable names. A recent study analyzed the use of variable names in five programming languages, focusing on single-letter variable names, because of the apparent contradiction between their frequent use and the fact that these variables violate the aforementioned guidelines.,,In this paper, we analyze variables in Scratch, a popular block-based language aimed at children. We start by replicating the above single-letter study for Scratch. We augment this study by analyzing single-letter procedure names, and by investigating the use of Scratch specific naming patterns: spaces in variable names, numerics as variables and textual labels in procedure names.,,The results of our analysis show that Scratch programmers often prefer longer identifier names than developers in other languages, while Scratch procedure names have even longer names than Scratch variables. For the single-letter variables, the most frequent names are x, y, and i. Single-letter procedures are less popular, but show more tendency to be in upper case. When compared to the other programming languages, the usage of single uppercase letters in Scratch variables seems to be similar to the pattern found in Perl, while for the lowercase letters—to the pattern found in Java. Concerning Scratch specific features, 44% of the unique variable names and 34% of the projects in the dataset include at least one space. The usage of textual labels between parameters in procedure names appears as not common, however textual patterns used imply an influence from textual languages, for example by using brackets.,,Previous research indicate the identifier names as one significant issue in transitioning from visual block-based to textual programming languages. The naming patterns we found support this claim for Scratch programmers who may incur difficulties when transitioning to the use of mainstream textual programming languages. Those languages restrict the use of spaces in identifiers and more often divert into short and single-letter names—tendencies opposite to the naming preferences in Scratch.
SP  - 51
EP  - 60
JF  - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation (SCAM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/scam.2017.12
ER  - 

TY  - NA
AU  - Daniel, Maxime; Rivière, Guillaume; Couture, Nadine
TI  - Tangible and Embedded Interaction - CairnFORM: a Shape-Changing Ring Chart Notifying Renewable Energy Availability in Peripheral Locations
PY  - 2019
AB  - We present CairnFORM, a shape-changing cylindrical display that physicalizes forecasts of renewable energy availability. CairnFORM aims at creating and encouraging new socially-shared practices by displaying energy data in collective and public spaces, such as public places and workplaces. It is 360°-readable, and as a dynamic physical ring chart, it can change its cylindrical symmetry with quiet motion. We conducted two user studies. The first study clearly revealed the attractiveness of CairnFORM in a public place and its usability for a range task and for a compare task. Consequently, this makes CairnFORM useful to analyze renewable energy availability. The second study revealed that a non-constant motion speed is the better visualization stimulus at a workplace.
SP  - 275
EP  - 286
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295634
ER  - 

TY  - JOUR
AU  - Salido-Andres, Noelia; Rey-García, Marta; Álvarez-González, Luis Ignacio; Vázquez-Casielles, Rodolfo
TI  - Mapping the Field of Donation-Based Crowdfunding for Charitable Causes: Systematic Review and Conceptual Framework
PY  - 2020
AB  - This study compiles the main findings in the field of academic research on pure donation-based crowdfunding (DCF) soliciting monetary contributions for charitable causes. To this purpose, a systematic literature review is conducted, resulting in 92 scientific publications analyzed for the first time in this field of research. The prevailing thematic dimensions and research gaps are identified and discussed. The incipient literature on DCF, with a majority of publications from 2015 onward in the form of empirical articles using quantitative methodologies, focuses on antecedents related to individual donors, organizational promoters as main actors, and online channels and design-related features of campaigns as enablers. However, the effects of DCF on relevant stakeholders (particularly beneficiaries and society in general) remain largely obscure. Based on this analysis, an integrated conceptual framework on DCF is proposed to guide future research. This framework, susceptible of empirical evaluation, allows characterizing the DCF as a distinct and emerging type of philanthropic funding model based on specific and novel antecedents, actors, enablers and effects.
SP  - 288
EP  - 302
JF  - VOLUNTAS: International Journal of Voluntary and Nonprofit Organizations
VL  - 32
IS  - 2
PB  - 
DO  - 10.1007/s11266-020-00213-w
ER  - 

TY  - BOOK
AU  - Jungwirth, Florian; Haslgrübler, Michael; Ferscha, Alois
TI  - ETRA - Contour-guided gaze gestures: using object contours as visual guidance for triggering interactions
PY  - 2018
AB  - The eyes are an interesting modality for pervasive interactions, though their applicability for mobile scenarios is restricted by several issues so far. In this paper, we propose the idea of contour-guided gaze gestures, which overcome former constraints, like the need for calibration, by relying on unnatural and relative eye movements, as users trace the contours of objects in order to trigger an interaction. The interaction concept and the system design are described, along with two user studies, that demonstrate the method's applicability. It is shown that users were able to trace object contours to trigger actions from various positions on multiple different objects. It is further determined, that the proposed method is an easy to learn, hands-free interaction technique, that is robust against false positive activations. Results highlight low demand values and show that the method holds potential for further exploration, but also reveal areas for refinement.
SP  - 28
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204530
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Dachselt, Raimund
TI  - CHI - IllumiPaper: Illuminated Interactive Paper
PY  - 2017
AB  - Due to their simplicity and flexibility, digital pen-and-paper solutions have a promising potential to become a part of our daily work. Unfortunately, they lack dynamic visual feedback and thereby restrain advanced digital functionalities. In this paper, we investigate new forms of paper-integrated feedback, which build on emerging paper-based electronics and novel thin-film display technologies. Our approach focuses on illuminated elements, which are seamlessly integrated into standard paper. For that, we introduce an extended design space for paper-integrated illuminations. As a major contribution, we present a systematic feedback repertoire for real-world applications including feedback components for innovative paper interaction tasks in five categories. Furthermore, we contribute a fully-functional research platform including a paper-controller, digital pen and illuminated, digitally controlled papers that demonstrate the feasibility of our techniques. Finally, we report on six interviews, where experts rated our approach as intuitive and very usable for various applications, in particular educational ones.
SP  - 5605
EP  - 5618
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025525
ER  - 

TY  - BOOK
AU  - Sadana, Ramik; Setlur, Vidya; Stasko, John
TI  - ISS Companion - Redefining a Contribution for Immersive Visualization Research
PY  - 2016
AB  - Immersive computing modalities such as AR, VR, and speech-based input are regaining prominence as research threads in the visualization field due to the advancement in technology and availability of cheap consumer hardware. This renewed interest is similar to what we observed a decade ago when multitouch technology was gaining mainstream adoption. In this work, we reflect on lessons learned from designing for multitouch, with the goal of highlighting problems that may also emerge in AR/VR research. Specifically, we emphasize the need for the field to rearticulate what is expected from research efforts in the area of visualization on immersive technologies.
SP  - 41
EP  - 45
JF  - Proceedings of the 2016 ACM Companion on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3009939.3009946
ER  - 

TY  - JOUR
AU  - Ju, Qinjie; Chalon, René; Derrode, Stéphane
TI  - Assisted Music Score Reading Using Fixed-Gaze Head Movement: Empirical Experiment and Design Implications
PY  - 2019
AB  - Eye-tracking has a very strong potential in human computer interaction (HCI) as an input modality, particularly in mobile situations. However, it lacks convenient action triggering methods. In our research, we investigate the combination of eye-tracking and fixed-gaze head movement, which allows us to trigger various commands without using our hands or changing gaze direction. In this instance, we have proposed a new algorithm for fixed-gaze head movement detection using only scene images captured by the scene camera equipped in front of the head-mounted eye-tracker, for the purpose of saving computation time. To test the performance of our fixed-gaze head movement detection algorithm and the acceptance of triggering commands by these movements when the user's hands are occupied by another task, we have designed and developed an experimental application known as EyeMusic. The EyeMusic system is a music reading system, which can play the notes of a measure in a music score that the user does not understand. By making a voluntary head movement when fixing his/her gaze on the same point of a music score, the user can obtain the desired audio feedback. The design, development and usability testing of the first prototype for this application are presented in this paper. The usability of our application is confirmed by the experimental results, as 85% of participants were able to use all the head movements we implemented in the prototype. The average success rate of this application is 70%, which is partly influenced by the performance of the eye-tracker we use. The performance of our fixed-gaze head movement detection algorithm is 85%, and there were no significant differences between the performance of each head movement.
SP  - 3
EP  - 29
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - EICS
PB  - 
DO  - 10.1145/3300962
ER  - 

TY  - NA
AU  - Henrikson, Rorik; Grossman, Tovi; Trowbridge, Sean; Wigdor, Daniel; Benko, Hrvoje
TI  - CHI - Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR
PY  - 2020
AB  - This paper presents a new technique to predict the ray pointer landing position for selection movements in virtual reality (VR) environments. The technique adapts and extends a prior 2D kinematic template matching method to VR environments where ray pointers are used for selection. It builds on the insight that the kinematics of a controller and Head-Mounted Display (HMD) can be used to predict the ray's final landing position and angle. An initial study provides evidence that the motion of the head is a key input channel for improving prediction models. A second study validates this technique across a continuous range of distances, angles, and target sizes. On average, the technique's predictions were within 7.3° of the true landing position when 50% of the way through the movement and within 3.4° when 90%. Furthermore, compared to a direct extension of Kinematic Template Matching, which only uses controller movement, this head-coupled approach increases prediction accuracy by a factor of 1.8x when 40% of the way through the movement.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376489
ER  - 

TY  - NA
AU  - Signer, Madlaina; Ion, Alexandra; Sorkine-Hornung, Olga
TI  - CHI - Developable Metamaterials: Mass-fabricable Metamaterials by Laser-Cutting Elastic Structures
PY  - 2021
AB  - We propose a novel design of engineered, structured materials that leverages fast fabrication technologies, pushing them towards mass-fabrication. Specifically, our metamaterial is designed to be laser cut, to approximate the volumetric shape and allow for locally varying compliance. Traditional mechanical metamaterials consist of intricate cells arranged on a 3-dimensional grid, limiting them to 3D printing—which is slow. Our metamaterial is designed for laser cutting, which is drastically faster. Our structures are best described as ruffled strips of thin sheet material, such as paper, plastics, metals, etc. Users can interactively define the ruffles’ anisotropic stiffness directions and local density. Our computational design tool assists users by automatically optimizing the ruffle to fill the shape’s volume, and exporting the flat ruffle design ready for cutting. We demonstrate how such ruffled metamaterials can be utilized for, e.g., custom toys with locally varying compliance, custom packaging material, or lightweight formwork for architectural shells.
SP  - 674
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445666
ER  - 

TY  - CHAP
AU  - Batmaz, Anil Ufuk; Stuerzlinger, Wolfgang
TI  - When Anchoring Fails: Interactive Alignment of Large Virtual Objects in Occasionally Failing AR Systems
PY  - 2021
AB  - Augmented reality systems show virtual object models overlaid over real ones, which is helpful in many contexts, e.g., during maintenance. Assuming all geometry is known, misalignments in 3D poses will still occur without perfectly robust viewer and object 3D tracking. Such misalignments can impact the user experience and reduce the potential benefits associated with AR systems. In this paper, we implemented several interaction algorithms to make manual virtual object alignment easier, based on previously presented methods, such as HoverCam, SHOCam, and a Signed Distance Field. Our approach also simplifies the user interface for manual 3D pose alignment in 2D input systems. The results of our work indicate that our approach can reduce the time needed for interactive 3D pose alignment, which improves the user experience.
SP  - 49
EP  - 62
JF  - Proceedings of the Future Technologies Conference (FTC) 2021, Volume 1
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-89906-6_4
ER  - 

TY  - JOUR
AU  - Niwa, Kazuhiro; Tanaka, Yoshihiro; Kitamichi, Kota; Kuhara, Takumi; Uemura, Kimihiro; Saito, Takafumi
TI  - Vibrotactile Feedback System From the Fingertip to the Temples for Perceptual Enhancement of Contracture Palpation
PY  - 2021
AB  - Contractures are generally assessed by a physician or physical therapist through palpation. However, contracture palpation requires skill and experience. The frictional vibration, which has a pulse-like vibration due to sliding disturbances around the affected area during palpation, is important in assessing the degree of contracture progression. This paper aims to enhance the perceptual sensitivity of frictional vibration for contracture palpation using a vibrotactile feedback system. We previously proposed an evaluation system for palpation with a wearable skin vibration sensor that detects skin-propagated vibration, allowing touch with a bare fingertip. In this paper, we propose the vibrotactile feedback system that presents the tactile information of the fingertip detected by the wearable tactile sensor to the temples with a vibrotactile display. A stimulator that gives vibrations similar to those during the palpation, which include pulse-like vibration and small vibration, was assembled. Then, psychophysical experiments on the vibrotactile feedback system were conducted using this stimulator. The results showed that the detection sensitivity of the pulse-like vibration was significantly enhanced with the feedback.
SP  - 285
EP  - 290
JF  - IEEE transactions on haptics
VL  - 14
IS  - 2
PB  - 
DO  - 10.1109/toh.2021.3076501
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Mecke, Lukas; Rodriguez, Sarah Delgado; Hassib, Mariam; Maier, Hannah; Alt, Florian
TI  - VRST - Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality
PY  - 2020
AB  - Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418962
ER  - 

TY  - NA
AU  - Wiese, Eliane; Rafferty, Anna N.; Kopta, Daniel; Anderson, Jacqulyn M.
TI  - ICPC - Replicating novices' struggles with coding style
PY  - 2019
AB  - Good style makes code easier for others to read and modify. Control flow is one element of style where experts expect particular structure, such as conjoining conditions rather than nesting if statements. Empirical work is necessary to understand why novices use poor style, so they can be taught to use good style. Previous work shows that many students know what control flows experts prefer, but may say that novice-styled code is more readable. Yet, these same students showed similarly high comprehension across both expert- and novice-styled code. We propose a replication of that work that more fully assesses students' code comprehension and code writing. Our replication focuses on students who are earlier in their computer science courses and are less likely to be majors, to determine whether the pattern of results is particular to students who are relatively attuned to style concerns. Our pilot of the proposed replication finds that: students in this new population are less able to identify expert code; expert style may reduce comprehension for some control flows; and writing with good style does not always predict a preference for reading code with good style.
SP  - 13
EP  - 18
JF  - 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icpc.2019.00015
ER  - 

TY  - CHAP
AU  - Berna-Moya, Jose Luis; Martinez-Plasencia, Diego
TI  - INTERACT (1) - Exploring the Effects of Replicating Shape, Weight and Recoil Effects on VR Shooting Controllers
PY  - 2019
AB  - Commercial Virtual Reality (VR) controllers with realistic force feedback are becoming available, to increase the realism and immersion of first-person shooting (FPS) games in VR. These controllers attempt to mimic not only the shape and weight of real guns but also their recoil effects (linear force feedback parallel to the barrel, when the gun is shot). As these controllers become more popular and affordable, this paper investigates the actual effects that these properties (shape, weight, and especially directional force feedback) have on performance for general VR users (e.g. users with no marksmanship experience), drawing conclusions for both consumers and device manufacturers.
SP  - 763
EP  - 782
JF  - Human-Computer Interaction – INTERACT 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29381-9_45
ER  - 

TY  - BOOK
AU  - MacLean, Karon E.; Schneider, Oliver; Seifi, Hasti
TI  - The Handbook of Multimodal-Multisensor Interfaces, Volume 1 (1) - Multisensory haptic interactions: understanding the sense and designing for it
PY  - 2017
AB  - Our haptic sense comprises both taction or cutaneous information obtained through receptors in the skin, and kinesthetic awareness of body forces and motions. Broadly speaking, haptic interfaces to computing systems are anything a user touches or is touched by, to control, experience, or receive information from something with a computer in it. Keyboard and mouse, a physical button on a kitchen blender, and the glass touchscreen on your smartphone are energetically passive haptic interfaces: no external energy is pumped into the users' body from a powered actuator. Most readers will have encountered energetically active haptic feedback as a vibrotactile (VT) buzz or forces in a gaming joystick, a force feedback device in a research lab, or a physically interactive robot. Much more is possible.When we bring touch into an interaction, we invoke characteristics that are unique or accentuated relative to other modalities. Like most powerful design resources, these traits also impose constraints. The job of a haptic designer is to understand these "superpowers" and their costs and limits, and then to deploy them for an optimally enriched experience.Both jobs are relatively uncharted, even though engineers have been building devices with the explicit intention of haptic display for over 25 years, and psychophysicists have been studying this rich, complex sense for as many decades. What makes it so difficult? Our haptic sense is reallymanydifferent senses, neurally integrated; meanwhile, the technology of haptic display is anything but stable, with engineering challenges of a different nature than those for graphics and sound. In the last few years, technological advances from materials to robotics have opened new possibilities for the use of energetically active haptics in user interfaces, our primary focus here. Needs are exposed at a large scale by newly ubiquitous technology like "touch" screens crying out for physical feedback, and high-fidelity virtual reality visuals that are stalled in effectiveness without force display.
SP  - 97
EP  - 142
JF  - The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3015783.3015788
ER  - 

TY  - BOOK
AU  - Auda, Jonas; Gruenefeld, Uwe; Schneegass, Stefan
TI  - Mensch und Computer - Enabling Reusable Haptic Props for Virtual Reality by Hand Displacement
PY  - 2021
AB  - Virtual Reality (VR) enables compelling visual experiences. However, providing haptic feedback is still challenging. Previous work suggests utilizing haptic props to overcome such limitations and presents evidence that props could function as a single haptic proxy for several virtual objects. In this work, we displace users’ hands to account for virtual objects that are smaller or larger. Hence, the used haptic prop can represent several differently-sized virtual objects. We conducted a user study (N = 12) and presented our participants with two tasks during which we continuously handed them the same haptic prop but they saw in VR differently-sized virtual objects. In the first task, we used a linear hand displacement and increased the size of the virtual object to understand when participants perceive a mismatch. In the second task, we compare the linear displacement to logarithmic and exponential displacements. We found that participants, on average, do not perceive the size mismatch for virtual objects up to 50% larger than the physical prop. However, we did not find any differences between the explored different displacement. We conclude our work with future research directions.
SP  - 412
EP  - 417
JF  - Mensch und Computer 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3473856.3474000
ER  - 

TY  - NA
AU  - Yoshida, Shigeo; Sun, Yuqian; Kuzuoka, Hideaki
TI  - CHI - PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality
PY  - 2020
AB  - We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376358
ER  - 

TY  - BOOK
AU  - Hansen, John Paulin; Lund, Haakon; Biermann, Florian; Møllenbach, Emillie; Sztuk, Sebastian; San Agustin, Javier
TI  - ETRA - Wrist-worn pervasive gaze interaction
PY  - 2016
AB  - This paper addresses gaze interaction for smart home control, conducted from a wrist-worn unit. First we asked ten people to enact the gaze movements they would propose for e.g. opening a door or adjusting the room temperature. On basis of their suggestions we built and tested different versions of a prototype applying off-screen stroke input. Command prompts were given to twenty participants by text or arrow displays. The success rate achieved by the end of their first encounter with the system was 46% in average; it took them 1.28 seconds to connect with the system and 1.29 seconds to make a correct selection. Their subjective evaluations were positive with regard to the speed of the interaction. We conclude that gaze gesture input seems feasible for fast and brief remote control of smart home technology provided that robustness of tracking is improved.
SP  - 57
EP  - 64
JF  - Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2857491.2857514
ER  - 

TY  - NA
AU  - Tanaka, Yudai; Nishida, Jun; Lopes, Pedro
TI  - Demonstrating Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519904
ER  - 

TY  - NA
AU  - Harley, Daniel; Tarun, Aneesh P.; Germinario, Daniel; Mazalek, Ali
TI  - Conference on Designing Interactive Systems - Tangible VR: Diegetic Tangible Objects for Virtual Reality Narratives
PY  - 2017
AB  - We present a system for diegetic tangible objects in virtual reality (VR) narratives. The system integrates a custom-designed sensor unit, built with low-cost off-the-shelf hardware, to track objects in VR and to support a variety of custom-made and found tangibles. In its current form, the sensor unit tracks the objects' orientation and supports the authoring of specifically designed interactions for each tangible object. We contribute our design rationale, sensor unit, and four proof of concept prototypes, including a cube, a stuffed animal, a treasure chest, and a wooden boat, demonstrating how we leverage passive and active haptics to create a closer link between real and virtual worlds. For developers and users of VR, we expand interaction possibilities to include the physical characteristics of tangible objects. For the field of tangible narratives, we expand the current use of diegetic objects.
SP  - 1253
EP  - 1263
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064680
ER  - 

TY  - NA
AU  - Wu, Jason; Colglazier, Cooper; Ravishankar, Adhithya; Duan, Yuyan; Wang, Yuanbo; Ploetz, Thomas; Starner, Thad
TI  - Seesaw: rapid one-handed synchronous gesture interface for smartwatches
PY  - 2018
AB  - We present SeeSaw, a synchronous gesture interface for commodity smartwatches to support watch-hand only input with no additional hardware. Our algorithm, which uses correlation to determine whether the user is rotating their wrist in synchrony with a tactile and visual prompt, minimizes false-trigger events while maintaining fast input during situational impairments. Results from a 12 person evaluation of the system, used to respond to notifications on the watch during walking and simulated driving, show interaction speeds of 4.0 s - 5.5 s, which is comparable to the swipe-based interface control condition. SeeSaw is also evaluated as an input interface for watches used in conjunction with a head-worn display. A six subject study showed a 95% success rate in dismissing notifications and a 3.57 s mean dismissal time.
SP  - 17
EP  - 20
JF  - Proceedings of the 2018 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267242.3267251
ER  - 

TY  - NA
AU  - Alghofaili, Rawan; Sawahata, Yasuhito; Huang, Haikun; Wang, Hsueh-Cheng; Shiratori, Takaaki; Yu, Lap-Fai
TI  - CHI - Lost in Style: Gaze-driven Adaptive Aid for VR Navigation
PY  - 2019
AB  - A key challenge for virtual reality level designers is striking a balance between maintaining the immersiveness of VR and providing users with on-screen aids after designing a virtual experience. These aids are often necessary for wayfinding in virtual environments with complex paths. We introduce a novel adaptive aid that maintains the effectiveness of traditional aids, while equipping designers and users with the controls of how often help is displayed. Our adaptive aid uses gaze patterns in predicting user's need for navigation aid in VR and displays mini-maps or arrows accordingly. Using a dataset of gaze angle sequences of users navigating a VR environment and markers of when users requested aid, we trained an LSTM to classify user's gaze sequences as needing navigation help and display an aid. We validated the efficacy of the adaptive aid for wayfinding compared to other commonly-used wayfinding aids.
SP  - 348
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300578
ER  - 

TY  - BOOK
AU  - Majaranta, Päivi; Laitinen, Jari; Kangas, Jari; Isokoski, Poika
TI  - ETRA - Inducing gaze gestures by static illustrations
PY  - 2019
AB  - In gesture-based user interfaces, the effort needed for learning the gestures is a persistent problem that hinders their adoption in products. However, people's natural gaze paths form shapes during viewing. For example, reading creates a recognizable pattern. These gaze patterns can be utilized in human-technology interaction. We experimented with the idea of inducing specific gaze patterns by static drawings. The drawings included visual hints to guide the gaze. By looking at the parts of the drawing, the user's gaze composed a gaze gesture that activated a command. We organized a proof-of-concept trial to see how intuitive the idea is. Most participants understood the idea without specific instructions already on the first round of trials. We argue that with careful design the form of objects and especially their decorative details can serve as a gaze-based user interface in smart homes and other environments of ubiquitous computing.
SP  - 75
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3317956.3318151
ER  - 

TY  - NA
AU  - Tang, Jeff K. T.; Leung, Green Y. Y.; Ng, Bismarck K. L.; Hui, Jacky Hon-Kit; Kong, Anthony; Pang, Wai-Man
TI  - VR-MMA: A Virtual Reality Motion and Muscle Sensing Action Game for Personal Sport
PY  - 2016
AB  - In this paper, we proposed a serious game that not only for entertainment but also encourage people to do more exercise, in order to let them relax and refresh from the stressful working life and maintain a good health and well-being. Our game applied motion capture, muscle power sensing, action game factors and virtual reality technology (VR). In contrast to traditional VR games, our game provides a novel muscle-power sensitive gaming interface, such that special effects will be shown and a higher bonus scores could be received when the player do exercise harder and exert a greater muscle power. The preliminary experiment result reveals that the muscle sensing capability facilitates the players to pay more attention to their muscle activities when playing the game. The players also agreed that the proposed game could attract them to do exercise again.
SP  - 40
EP  - NA
JF  - Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3001773.3001814
ER  - 

TY  - JOUR
AU  - Hung, Ching-Wen; Tsai, Hsin-Ruey; Su, Chi-Chun; Chiu, Jui-Cheng; Chen, Bing-Yu
TI  - OsciHead
PY  - 2022
AB  - <jats:p>Current haptic devices are usually designed to provide one type of force feedback; however, most VR scenarios require versatile force feedback, which may require the integration of different devices to provide various types of forces. In addition, besides the main haptic effects caused by the forces, multiple types of oscillation may also commonly accompany them, which are crucial for improving VR realism and immersion. Therefore, we simulate versatile force feedback by rendering the corresponding types of oscillation as the effects caused by those forces. We take inertia and impact forces as examples in this paper, and achieve versatility using the proposed device, OsciHead, on a head-mounted display (HMD), instead of integrating different devices. By controlling elastic bands' elasticity and stored power, OsciHead uses two rotatable oscillators on both sides of the HMD, in order to render various multilevel and multidimensional oscillation feedback in 2D translation and 2D rotation directions on a head. In an exploratory study, we explored different scenarios in which multiple types of oscillation could be simulated by OsciHead. We then observed oscillation level distinguishability in two just-noticeable difference (JND) studies, and evaluated the oscillation type recognition rates in a recognition study. Based on the results, we performed a VR study, which verified that the inertia and impact feedback simulated by OsciHead enhances realism and achieves versatility.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546715
ER  - 

TY  - NA
AU  - Rivera, Michael L.; Moukperian, Melissa; Ashbrook, Daniel; Mankoff, Jennifer; Hudson, Scott E.
TI  - CHI - Stretching the Bounds of 3D Printing with Embedded Textiles
PY  - 2017
AB  - Textiles are an old and well developed technology that have many desirable characteristics. They can be easily folded, twisted, deformed, or cut; some can be stretched; many are soft. Textiles can maintain their shape when placed under tension and can even be engineered with variable stretching ability. Conversely, 3D printing is a relatively new technology that can precisely produce functional, rigid objects with custom geometry. Combining 3D printing and textiles opens up new opportunities for rapidly creating rigid objects with embedded flexibility as well as soft materials imbued with additional functionality. In this paper, we introduce a suite of techniques for integrating 3D printing with textiles during the printing process, opening up a new design space that takes inspiration from both fields. We demonstrate how the malleability, stretchability and aesthetic qualities of textiles can enhance rigid printed objects, and how textiles can be augmented with functional properties enabled by 3D printing.
SP  - 497
EP  - 508
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025460
ER  - 

TY  - NA
AU  - Zhu, Junyi; Lei, Yuxuan; Shah, Aashini; Schein, Gila; Ghaednia, Hamid; Schwab, Joseph; Harteveld, Casper; Mueller, Stefanie
TI  - MuscleRehab: Improving Unsupervised Physical Rehabilitation by Monitoring and Visualizing Muscle Engagement
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545705
ER  - 

TY  - NA
AU  - Günther, Sebastian; Makhija, Mohit; Müller, Florian; Schön, Dominik; Mühlhäuser, Max; Funk, Markus
TI  - Conference on Designing Interactive Systems - PneumAct: Pneumatic Kinesthetic Actuation of Body Joints in Virtual Reality Environments
PY  - 2019
AB  - Virtual Reality Environments (VRE) create an immersive user experience through visual, aural, and haptic sensations. However, the latter is often limited to vibrotactile sensations that are not able to actively provide kinesthetic motion actuation. Further, such sensations do not cover natural representations of physical forces, for example, when lifting a weight. We present PneumAct, a jacket to enable pneumatically actuated kinesthetic movements of arm joints in VRE. It integrates two types of actuators inflated through compressed air: a Contraction Actuator and an Extension Actuator. We evaluate our PneumAct jacket through two user studies with a total of 32 participants: First, we perform a technical evaluation measuring the contraction and extension angles of different inflation patterns and inflation durations. Second, we evaluate PneumAct in three VRE scenarios comparing our system to traditional controller-based vibrotactile and a baseline without haptic feedback.
SP  - 227
EP  - 240
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322302
ER  - 

TY  - NA
AU  - Feick, Martin; Kleer, Niko; Zenner, André; Tang, Anthony; Krüger, Antonio
TI  - CHI - Visuo-haptic Illusions for Linear Translation and Stretching using Physical Proxies in Virtual Reality
PY  - 2021
AB  - Providing haptic feedback when manipulating virtual objects is an essential part of immersive virtual reality experiences; however, it is challenging to replicate all of an object's properties and characteristics. We propose the use of visuo-haptic illusions alongside physical proxies to enhance the scope of proxy-based interactions with virtual objects. In this work, we focus on two manipulation techniques, linear translation and stretching across different distances, and investigate how much discrepancy between the physical proxy and the virtual object may be introduced without participants noticing. In a study with 24 participants, we found that manipulation technique and travel distance significantly affect the detection thresholds, and that visuo-haptic illusions impact performance and accuracy. We show that this technique can be used to enable functional proxy objects that act as stand-ins for multiple virtual objects, illustrating the technique through a showcase VR-DJ application.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445456
ER  - 

TY  - NA
AU  - Nishida, Jun; Suzuki, Kenji
TI  - CHI - bioSync: A Paired Wearable Device for Blending Kinesthetic Experience
PY  - 2017
AB  - We present a novel, paired, wearable system for combining the kinesthetic experiences of two persons. These devices allow users to sense and combine muscle contraction and joint rigidity bi-directionally. This is achieved through kinesthetic channels based on electromyogram (EMG) measurement and electrical muscle stimulation (EMS). We developed a pair of wearable kinesthetic input-output (I/O) devices called bioSync that uses specially designed electrodes to perform biosignal measurement and stimulation simultaneously on the same electrodes. In a user study, participants successfully evaluated the strength of their partners' muscle contractions while exerting their own muscles. We confirmed that the pair of devices could help participants synchronize their hand movements through tapping, without visual and auditory feedback. The proposed interpersonal kinesthetic communication system can be used to enhance interactions such as clinical gait rehabilitation and sports training, and facilitate sharing of physical experiences with Parkinson's patients, thereby enhancing understanding of the physical challenges they face in daily life.
SP  - 3316
EP  - 3327
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025829
ER  - 

TY  - JOUR
AU  - Rantala, Jussi; Majaranta, Päivi; Kangas, Jari; Isokoski, Poika; Akkil, Deepak; Špakov, Oleg; Raisamo, Roope
TI  - Gaze Interaction With Vibrotactile Feedback: Review and Design Guidelines
PY  - 2017
AB  - Vibrotactile feedback is widely used in mobile devices because it provides a discreet and private feedback channel. Gaze-based interaction, on the other hand, is useful in various applications due ...
SP  - 1
EP  - 39
JF  - Human–Computer Interaction
VL  - 35
IS  - 1
PB  - 
DO  - 10.1080/07370024.2017.1306444
ER  - 

TY  - JOUR
AU  - Lopes, Pedro; Baudisch, Patrick
TI  - Interactive Systems Based on Electrical Muscle Stimulation
PY  - 2017
AB  - Electrical muscle stimulation (EMS) has been used since the 1960s in rehabilitative medicine to regenerate lost motor functions, but in recent years researchers have started to explore new EMS applications including guided training, muscle-propelled force feedback for more immersive virtual experiences, and novel forms of information access. The authors analyze the interactive potential of EMS and compare it to more traditional mechanical actuation. The web extra at https://youtu.be/YA0mv9X9Ncw demonstrates Muscle-plotter, an interactive system that combines EMS with a motion-tracking digital pen.
SP  - 28
EP  - 35
JF  - Computer
VL  - 50
IS  - 10
PB  - 
DO  - 10.1109/mc.2017.3641627
ER  - 

TY  - NA
AU  - Zhao, Rui; Siy, Harvey; Pack, Chulwoo; Soh, Leen-Kiat; Song, Myoungkyu
TI  - An Intelligent Tutoring System for API Misuse Correction by Instant Quality Feedback
PY  - 2022
AB  - Computer science students have difficulty understanding correct usages of an Application Programming Interface (API) and programming violations that cause compilation or runtime errors. Despite high-quality documentation for programming, the students typically need an instructor&#x0027;s feedback when their programs cause bugs, crashes, and vulnerabilities. This paper presents a pedagogical approach that is based on an Intelligent Tutoring System called I<inf>NT</inf>Tu<inf>ToR</inf>. Briefly, I<inf>NT</inf>T<inf>UTOR</inf> provides novice students with instant feedback to fix their programming issues or vulnerabilities. We have implemented our approach as a plug-in application in the Integrated Development Environment (IDE) for an interactive educational environment. In our proposed evaluation, we plan to perform empirical studies with CS students to assess how effectively I<inf>NT</inf>T<inf>UTOR</inf> improves their ability to identify and fix potential bugs or vulnerabilities in the cryptography-related programming assignments.
SP  - NA
EP  - NA
JF  - 2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/compsac54236.2022.00026
ER  - 

TY  - NA
AU  - Takahashi, Akifumi; Tanabe, Kenta; Kajimoto, Hiroyuki
TI  - UIST (Adjunct Volume) - Haptic Interface Using Tendon Electrical Stimulation
PY  - 2018
AB  - This demonstration corresponds to our previous paper, which deals with our finding that a proprioceptive force sensation can be presented by electrical stimulation from the skin surface to the tendon region (Tendon Electrical Stimulation: TES). We showed that TES can elicit a force sensation, and adjusting the current parameters can control the amount of the sensation. Unlike electrical muscle stimulation (EMS), which can also present force sensation by stimulating motor nerves to contract muscles, TES is thought to present a proprioceptive force sensation by stimulating receptors or sensory nerves responsible for recognizing the magnitude of the muscle contraction existing inside the tendon. In the demo, we offer the occasion for trying TES.
SP  - 172
EP  - 173
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3271640
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Oechsner, Carl; Alt, Florian; Bulling, Andreas
TI  - AVI - VRpursuits: interaction in virtual reality using smooth pursuit eye movements
PY  - 2018
AB  - Gaze-based interaction using smooth pursuit eye movements (Pursuits) is attractive given that it is intuitive and overcomes the Midas touch problem. At the same time, eye tracking is becoming increasingly popular for VR applications. While Pursuits was shown to be effective in several interaction contexts, it was never explored in-depth for VR before. In a user study (N=26), we investigated how parameters that are specific to VR settings influence the performance of Pursuits. For example, we found that Pursuits is robust against different sizes of virtual 3D targets. However performance improves when the trajectory size (e.g., radius) is larger, particularly if the user is walking while interacting. While walking, selecting moving targets via Pursuits is generally feasible albeit less accurate than when stationary. Finally, we discuss the implications of these findings and the potential of smooth pursuits for interaction in VR by demonstrating two sample use cases: 1) gaze-based authentication in VR, and 2) a space meteors shooting game.
SP  - 18
EP  - NA
JF  - Proceedings of the 2018 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206505.3206522
ER  - 

TY  - JOUR
AU  - BermejoCarlos, ; HuiPan, 
TI  - A Survey on Haptic Technologies for Mobile Augmented Reality
PY  - 2021
AB  - Augmented reality (AR) applications have gained much research and industry attention. Moreover, the mobile counterpart—mobile augmented reality (MAR) is one of the most explosive growth areas for A...
SP  - 1
EP  - 35
JF  - ACM Computing Surveys
VL  - 54
IS  - 9
PB  - 
DO  - 10.1145/3465396
ER  - 

TY  - JOUR
AU  - Belkhiria, Chama; Boudir, Atlal; Hurter, Christophe; Peysakhovich, Vsevolod
TI  - EOG-Based Human-Computer Interface: 2000-2020 Review.
PY  - 2022
AB  - Electro-oculography (EOG)-based brain-computer interface (BCI) is a relevant technology influencing physical medicine, daily life, gaming and even the aeronautics field. EOG-based BCI systems record activity related to users' intention, perception and motor decisions. It converts the bio-physiological signals into commands for external hardware, and it executes the operation expected by the user through the output device. EOG signal is used for identifying and classifying eye movements through active or passive interaction. Both types of interaction have the potential for controlling the output device by performing the user's communication with the environment. In the aeronautical field, investigations of EOG-BCI systems are being explored as a relevant tool to replace the manual command and as a communicative tool dedicated to accelerating the user's intention. This paper reviews the last two decades of EOG-based BCI studies and provides a structured design space with a large set of representative papers. Our purpose is to introduce the existing BCI systems based on EOG signals and to inspire the design of new ones. First, we highlight the basic components of EOG-based BCI studies, including EOG signal acquisition, EOG device particularity, extracted features, translation algorithms, and interaction commands. Second, we provide an overview of EOG-based BCI applications in the real and virtual environment along with the aeronautical application. We conclude with a discussion of the actual limits of EOG devices regarding existing systems. Finally, we provide suggestions to gain insight for future design inquiries.
SP  - 4914
EP  - 4914
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 13
PB  - 
DO  - 10.3390/s22134914
ER  - 

TY  - NA
AU  - Anagnostopoulos, Vasileios Athanasios; Kiefer, Peter
TI  - UbiComp Adjunct - Towards gaze-based interaction with urban outdoor spaces
PY  - 2016
AB  - In this paper we envision gaze-based interaction in and with large-scale outdoor spaces. We propose interaction using the gaze on real-world objects located and moving in urban environments, such as buildings or cars. A novel classification scheme is introduced which describes gaze-based interaction based on whether the user and the object(s) interacted with are stationary or moving. The classification scheme can be used for exploring the design space of mobile gaze-based interaction. We discuss the challenges specific for the dimensions of the classification scheme, focussing on the recognition of the object of regard, as well as on interaction design.
SP  - 1706
EP  - 1715
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2968339
ER  - 

TY  - NA
AU  - Nakao, Takuro; Kunze, Kai; Isogai, Megumi; Shimizu, Shinya; Pai, Yun Suen
TI  - MUM - FingerFlex: Shape Memory Alloy-based Actuation on Fingers for Kinesthetic Haptic Feedback
PY  - 2020
AB  - The tactile and kinesthetic sensation of pushing a button is usually lost when interacting with modern devices like touchscreens and/or virtual reality platforms. We present FingerFlex, a standalone glove wearable actuating the metacarpophalangeal joint (MCP) of each finger via shape memory alloy (SMA). SMA actuation is subtle, silent, and light, making it ideal for actuation of the fingers which we use to simulate the sensation of pressing a button. For our first study, we evaluated the engineering performance of FingerFlex by altering the current and triggering different levels of stimuli to the user’s fingers. We show that users can perceive at least 3 levels of actuation with an accuracy of 73%. For our second study, we found FingerFlex to perform significantly better in terms of input error on a virtual numblock of a keyboard with no significant change in perceived workload.
SP  - 240
EP  - 244
JF  - 19th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3428361.3428404
ER  - 

TY  - NA
AU  - Lee, Byungjoo; Deng, Qiao; Hoggan, Eve; Oulasvirta, Antti
TI  - ICMI - Boxer: a multimodal collision technique for virtual objects
PY  - 2017
AB  - Virtual collision techniques are interaction techniques for invoking discrete events in a virtual scene, e.g. throwing, pushing, or pulling an object with a pointer. The conventional approach involves detecting collisions as soon as the pointer makes contact with the object. Furthermore, in general, motor patterns can only be adjusted based on visual feedback. The paper presents a multimodal technique based on the principle that collisions should be aligned with the most salient sensory feedback. Boxer (1) triggers a collision at the moment where the pointer's speed reaches a minimum after first contact and (2) is synchronized with vibrotactile stimuli presented to the hand controlling the pointer. Boxer was compared with the conventional technique in two user studies (with temporal pointing and virtual batting). Boxer improved spatial precision in collisions by 26.7 % while accuracy was compromised under some task conditions. No difference was found in temporal precision. Possibilities for improving virtual collision techniques are discussed.
SP  - 252
EP  - 260
JF  - Proceedings of the 19th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3136755.3136761
ER  - 

TY  - NA
AU  - Shigeyama, Jotaro; Hashimoto, Takeru; Yoshida, Shigeo; Narumi, Takuji; Tanikawa, Tomohiro; Hirose, Michitaka
TI  - CHI - Transcalibur: A Weight Shifting Virtual Reality Controller for 2D Shape Rendering based on Computational Perception Model
PY  - 2019
AB  - Humans can estimate the shape of a wielded object through the illusory feeling of the mass properties of the object obtained using their hands. Even though the shape of hand-held objects influences immersion and realism in virtual reality (VR), it is difficult to design VR controllers for rendering desired shapes according to the perceptions derived from the illusory effects of mass properties and shape perception. We propose Transcalibur, which is a hand-held VR controller that can render a 2D shape by changing its mass properties on a 2D planar area. We built a computational perception model using a data-driven approach from the collected data pairs of mass properties and perceived shapes. This enables Transcalibur to easily and effectively provide convincing shape perception based on complex illusory effects. Our user study showed that the system succeeded in providing the perception of various desired shapes in a virtual environment.
SP  - 11
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300241
ER  - 

TY  - BOOK
AU  - Ando, Ryoichi; Ando, Akihiro; Kunze, Kai; Minamizawa, Kouta
TI  - Superhuman Sports Design Challenge - Bubble jumper: enhancing the traditional japanese sport sumo with physical augmentation
PY  - 2018
AB  - This paper introduces Bubble Jumper, a super human sport enhancing your jump and enlarge your bodies circumference (grith) simulating an augmented sumo wrestler. We focus on the experience of superhuman strength in the legs and giving you the feeling of a larger body for the sport. We present the historical context of Sumo as well as our approach to augment the sport into Bubble Jumper. We show some early design prototypes and the iterative process to improve them, as well as rules and ideas for digital augmentation of the sport.
SP  - 3
EP  - NA
JF  - Proceedings of the First Superhuman Sports Design Challenge: First International Symposium on Amplifying Capabilities and Competing in Mixed Realities
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210299.3210301
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Gellersen, Hans
TI  - UIST - Gaze and Touch Interaction on Tablets
PY  - 2016
AB  - We explore how gaze can support touch interaction on tablets. When holding the device, the free thumb is normally limited in reach, but can provide an opportunity for indirect touch input. Here we propose gaze and touch input, where touches redirect to the gaze target. This provides whole-screen reachability while only using a single hand for both holding and input. We present a user study comparing this technique to direct-touch, showing that users are slightly slower but can utilise one-handed use with less physical effort. To enable interaction with small targets, we introduce CursorShift, a method that uses gaze to provide users temporal control over cursors during direct-touch interactions. Taken together, users can employ three techniques on tablets: direct-touch, gaze and touch, and cursor input. In three applications, we explore how these techniques can coexist in the same UI and demonstrate how tablet tasks can be performed with thumb-only input of the holding hand, and with it describe novel interaction techniques for gaze based tablet interaction.
SP  - 301
EP  - 311
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984514
ER  - 

TY  - NA
AU  - Lopes, Pedro; Baudisch, Patrick
TI  - SIGGRAPH Studio - Interactive systems based on electrical muscle stimulation
PY  - 2017
AB  - We provide attendees with a hands-on demonstration of several our interactive systems based on electrical muscle stimulation. These wearable devices allow attendees, for example, to transform their arms in interactive plotters, physically learn how to manipulate objects they never seen before, feel walls and forces in virtual reality, and so forth.
SP  - 4
EP  - NA
JF  - ACM SIGGRAPH 2017 Studio
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3084863.3084872
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Rekimoto, Jun
TI  - CHI Extended Abstracts - ElasticVR: Providing Multi-level Active and Passive Force Feedback in Virtual Reality Using Elasticity
PY  - 2018
AB  - We propose a light-weight, wearable device ElasticVR to provide various force feedback in multi-level in virtual reality (VR) for more immersive and realistic VR experiences. Force feedback is generally categorized into passive and active force feedback. Passive force is produced passively and continuously against the body part movement, e.g., elasticity and resistance. Active force is triggered by events and rendered actively and discretely to stimulate users, e.g., recoil and impact. ElasticVR consists of an elastic band, servo motors and mechanical locks to provide both passive and active force feedback. By changing length and extension distance of the elastic band, different elasticity levels produce various force feedback levels. By observing multi-level force feedback perception, we provide 5-level passive and 3-level active force feedback on the finger. We expect that realistic force feedback from ElasticVR enhances VR experiences.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186540
ER  - 

TY  - JOUR
AU  - Malešević, Jovana; Kostić, Miloš; Jure, Fabricio A; Spaich, Erika G; Došen, Strahinja; Ilić, Vojin; Bijelić, Goran; Štrbac, Matija
TI  - Electrotactile Communication via Matrix Electrode Placed on the Torso Using Fast Calibration, and Static vs. Dynamic Encoding.
PY  - 2022
AB  - Electrotactile stimulation is a technology that reproducibly elicits tactile sensations and can be used as an alternative channel to communicate information to the user. The presented work is a part of an effort to develop this technology into an unobtrusive communication tool for first responders. In this study, the aim was to compare the success rate (SR) between discriminating stimulation at six spatial locations (static encoding) and recognizing six spatio-temporal patterns where pads are activated sequentially in a predetermined order (dynamic encoding). Additionally, a procedure for a fast amplitude calibration, that includes a semi-automated initialization and an optional manual adjustment, was employed and evaluated. Twenty subjects, including twelve first responders, participated in the study. The electrode comprising the 3 × 2 matrix of pads was placed on the lateral torso. The results showed that high SRs could be achieved for both types of message encoding after a short learning phase; however, the dynamic approach led to a statistically significant improvement in messages recognition (SR of 93.3%), compared to static stimulation (SR of 83.3%). The proposed calibration procedure was also effective since in 83.8% of the cases the subjects did not need to adjust the stimulation amplitude manually.
SP  - 7658
EP  - 7658
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 19
PB  - 
DO  - 10.3390/s22197658
ER  - 

TY  - NA
AU  - Nakagaki, Ken
TI  - UIST (Adjunct Volume) - Mechanical Shells: Physical Add-ons for Extending and Reconfiguring the Interactivities of Actuated TUIs
PY  - 2020
AB  - In this paper, I introduce a concept of mechanical shells, which are physical add-ons that can adaptively augment, extend, and reconfigure the interactivities of self-actuated tangible user interfaces (TUIs). While a variety of research explores actuated and shape-changing interfaces for providing dynamic physical affordance and tangible displays, the concept of mechanical shell intends to overcome the constraint of existing generic actuated TUI hardware thereby enabling greater versatility and expression. This paper overviews the mechanical shell concept, describes project examples, outlines a research framework, and suggests open space for future research.
SP  - 151
EP  - 156
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415801
ER  - 

TY  - JOUR
AU  - Xu, Xinkai; Zhang, Han; Yan, Yan; Wang, Jianru; Guo, Liang
TI  - Effects of electrical stimulation on skin surface
PY  - 2021
AB  - Skin is the largest organ in the body, and directly contact with the external environment. Articles on the role of micro-current and skin have emerged in recent years. The function of micro-current is various, including introducing various drugs into the skin locally or throughout the body, stimulating skin wounds healing through various currents, suppressing pain caused by various diseases, and promoting blood circulation for postoperative muscle rehabilitation, etc. This article reviews these efforts. Compared with various physical and chemical medical therapies, micro-current stimulation provides a relatively safe, non-invasive therapy with few side effects, giving modern medicine a more suitable treatment option. At the same time, the cost of the electrical stimulation generating device is relatively low, which makes it have wider space to and more clinical application value. The current micro-current stimulation technology has become more and more mature, but there are still many problems in its research. The design of the experiment and the selection of the current parameters not standardized and rigorous. Now, clear regulations are needed to regulate this field. Micro-current skin therapy has become a robust, reliable, and well-structured system
SP  - 1
EP  - 29
JF  - Acta mechanica Sinica = Li xue xue bao
VL  - 37
IS  - 12
PB  - 
DO  - 10.1007/s10409-020-01026-2
ER  - 

TY  - NA
AU  - Groeger, Daniel; Steimle, Jürgen
TI  - CHI - LASEC: Instant Fabrication of Stretchable Circuits Using a Laser Cutter
PY  - 2019
AB  - This paper introduces LASEC, the first technique for instant do-it-yourself fabrication of circuits with custom stretchability on a conventional laser cutter and in a single pass. The approach is based on integrated cutting and ablation of a two-layer material using parametric design patterns. These patterns enable the designer to customize the desired stretchability of the circuit, to combine stretchable with non-stretchable areas, or to integrate areas of different stretchability. For adding circuits on such stretchable cut patterns, we contribute routing strategies and a real-time routing algorithm. An interactive design tool assists designers by automatically generating patterns and circuits from a high-level specification of the desired interface. The approach is compatible with off-the-shelf materials and can realize transparent interfaces. Several application examples demonstrate the versatility of the novel technique for applications in wearable computing, interactive textiles, and stretchable input devices.
SP  - 699
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300929
ER  - 

TY  - NA
AU  - Wiese, Eliane; Rafferty, Anna N.; Pyper, Jordan
TI  - Readable vs. Writable Code
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 53rd ACM Technical Symposium on Computer Science Education
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3478431.3499413
ER  - 

TY  - NA
AU  - Yan, Zeyu; Sathya, Anup; Yusuf, Sahra; Lien, Jyh-Ming; Peng, Huaishu
TI  - Fibercuit: Prototyping High-Resolution Flexible and Kirigami Circuits with a Fiber Laser Engraver
PY  - 2022
AB  - Prototyping compact devices with unique form factors often requires the PCB manufacturing process to be outsourced, which can be expensive and time-consuming. In this paper, we present Fibercuit, a set of rapid prototyping techniques to fabricate high-resolution, flexible circuits on-demand using a fiber laser engraver. We showcase techniques that can laser cut copper-based composites to form fine-pitch conductive traces, laser fold copper substrates that can form kirigami structures, and laser solder surface-mount electrical components using off-the-shelf soldering pastes. Combined with our software pipeline, an end user can design and fabricate flexible circuits which are dual-layer and three-dimensional, thereby exhibiting a wide range of form factors. We demonstrate Fibercuit by showcasing a set of examples, including a custom dice, flex cables, custom end-stop switches, electromagnetic coils, LED earrings and a circuit in the form of kirigami crane.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545652
ER  - 

TY  - JOUR
AU  - Reyes, Gabriel; Wu, Jason; Juneja, Nikita; Goldshtein, Maxim; Edwards, W. Keith; Abowd, Gregory D.; Starner, Thad
TI  - SynchroWatch: One-Handed Synchronous Smartwatch Gestures Using Correlation and Magnetic Sensing
PY  - 2018
AB  - SynchroWatch is a one-handed interaction technique for smartwatches that uses rhythmic correlation between a user's thumb movement and on-screen blinking controls. Our technique uses magnetic sensing to track the synchronous extension and reposition of the thumb, augmented with a passive magnetic ring. The system measures the relative changes in the magnetic field induced by the required thumb movement and uses a time-shifted correlation approach with a reference waveform for detection of synchrony. We evaluated the technique during three distraction tasks with varying degrees of hand and finger movement: active walking, browsing on a computer, and relaxing while watching online videos. Our initial offline results suggest that intentional synchronous gestures can be distinguished from other movement. A second evaluation using a live implementation of the system running on a smartwatch suggests that this technique is viable for gestures used to respond to notifications or issue commands. Finally, we present three demonstration applications that highlight the technique running in real-time on the smartwatch.
SP  - 158
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161162
ER  - 

TY  - NA
AU  - Lopes, Pedro
TI  - CHI Extended Abstracts - Proprioceptive Interaction: The User's Muscles as Input and Output Device
PY  - 2016
AB  - In my research, I investigate how users might interact with devices smaller than mobile or wearable devices. I argue that to achieve the intended minimal form-factor such devices will leverage the user's body as an input and output device. Users will not interact with the device but instead will interact through one of their limbs, which they share with the computer as the interface. I present four research prototypes, all of which actuate the user's limbs by means of electrical muscle stimulation and are perceived through the user's proprioceptive senses.
SP  - 223
EP  - 228
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2859014
ER  - 

TY  - CHAP
AU  - Koguchi, Yuto; Oharada, Kazuya; Takagi, Yuki; Sawada, Yoshiki; Shizuki, Buntarou; Takahashi, Shin
TI  - HCI (3) - A Mobile Command Input Through Vowel Lip Shape Recognition
PY  - 2018
AB  - Most recent smartphones are controlled by touch screens, creating a need for hands-free input techniques. Voice is a simple means of input. However, this can be stressful in public spaces, and the recognition rate is low in noisy backgrounds. We propose a touch-free input technique using lip shapes. Vowels are detected by lip shape and used as commands. This creates a touch-free operation (like voice input) without actually requiring voice. We explored the recognition accuracies of each vowel of the Japanese moras. Vowels were identified with high accuracy by means of the characteristic lip shape.
SP  - 297
EP  - 305
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-91250-9_23
ER  - 

TY  - JOUR
AU  - Kar, Pragma; Chattopadhyay, Samiran; Chakraborty, Sandip
TI  - Gestatten: Estimation of User's Attention in Mobile MOOCs From Eye Gaze and Gaze Gesture Tracking
PY  - 2020
AB  - The rapid proliferation of Massive Open Online Courses (MOOC) has resulted in many-fold increase in sharing the global classrooms through customized online platforms, where a student can participate in the classes through her personal devices, such as personal computers, smartphones, tablets, etc. However, in the absence of direct interactions with the students during the delivery of the lectures, it becomes difficult to judge their involvements in the classroom. In academics, the degree of student's attention can indicate whether a course is efficacious in terms of clarity and information. An automated feedback can hence be generated to enhance the utility of the course. The precision of discernment in the context of human attention is a subject of surveillance. However, visual patterns indicating the magnitude of concentration can be deciphered by analyzing the visual emphasis and the way an individual visually gesticulates, while contemplating the object of interest. In this paper, we develop a methodology called Gestsatten which captures the learner's attentiveness from his visual gesture patterns. In this approach, the learner's visual gestures are tracked along with the region of focus. We consider two aspects in this approach -- first, we do not transfer learner's video outside her device, so we apply in-device computing to protect her privacy; second, considering the fact that a majority of the learners use handheld devices like smartphones to observe the MOOC videos, we develop a lightweight approach for in-device computation. A three level estimation of learner's attention is performed based on these information. We have implemented and tested Gestatten over 48 participants from different age groups, and we observe that the proposed technique can capture the attention level of a learner with high accuracy (average absolute error rate is 8.68%), which meets her ability to learn a topic as measured through a set of cognitive tests.
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - EICS
PB  - 
DO  - 10.1145/3394974
ER  - 

TY  - BOOK
AU  - Reipschläger, Patrick; Dachselt, Raimund
TI  - ISS - DesignAR: Immersive 3D-Modeling Combining Augmented Reality with Interactive Displays
PY  - 2019
AB  - We present DesignAR, an augmented design workstation for creating 3D models. Our approach seamlessly integrates an interactive surface displaying 2D views with head-mounted, stereoscopic Augmented Reality (AR). This creates a combined output space that expands the screen estate and enables placing 3D objects beyond display borders. For the effective combination of 2D and 3D views, we define different levels of proximity and alignment. Regarding input, multi-touch and pen mitigate issues of precision and ergonomics commonly found in mid-air VR/AR interaction. For creating and refining 3D models, we propose a set of pen and touch techniques with immediate AR feedback, including sketching of rotational solids or tracing physical objects on the surface. To further support a designer's modeling process, we additionally propose orthographic model views and UI offloading in AR as well as freely placeable model instances with real-world reference. Based on our DesignAR prototype, we report on challenges and insights regarding this novel type of display augmentation. The combination of high-resolution, high-precision interactive surfaces with carefully aligned AR views opens up exciting possibilities for future work and design environments, a vision we call Augmented Displays.
SP  - 29
EP  - 41
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359718
ER  - 

TY  - JOUR
AU  - Khurana, Rushil; Goel, Mayank; Lyons, Kent
TI  - Detachable Smartwatch: More Than A Wearable
PY  - 2019
AB  - Glanceability and low access time are arguably the key assets of a smartwatch. However, smartwatches are currently limited to micro-interactions. They do not enable complex interactions and, in general, they do not afford continuous use for long. We believe that smartwatches can retain micro-interactions and glanceability, but also get better at long and complex interactions. We propose a smartwatch that a user can detach, and use as more than a wearable depending on their context, requirements, and preference. Detaching the watch enables it to morph into different forms, and thereby become a better interaction device, better display, and a better sensor suite. First, we interview participants to elicit usage themes for a detachable watch. Then, we build applications that showcase the range of use-cases where a detachable smartwatch offers additional functionality compared to an always-worn one, and highlights the affordances and benefits enabled due to detachability.
SP  - 50
EP  - 14
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 2
PB  - 
DO  - 10.1145/3328921
ER  - 

TY  - JOUR
AU  - Duchowski, Andrew T.
TI  - Gaze-based interaction: A 30 year retrospective
PY  - 2018
AB  - NA
SP  - 59
EP  - 69
JF  - Computers & Graphics
VL  - 73
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2018.04.002
ER  - 

TY  - JOUR
AU  - Carr, Michelle; Haar, A. J. H.; Amores, Judith; Lopes, Pedro; Bernal, Guillermo; Vega, Tomás; Rosello, Oscar; Jain, Abhinandan; Maes, Pattie
TI  - Dream engineering: Simulating worlds through sensory stimulation.
PY  - 2020
AB  - We explore the application of a wide range of sensory stimulation technologies to the area of sleep and dream engineering. We begin by emphasizing the causal role of the body in dream generation, and describe a circuitry between the sleeping body and the dreaming mind. We suggest that nearly any sensory stimuli has potential for modulating experience in sleep. Considering other areas that might afford tools for engineering sensory content in simulated worlds, we turn to Virtual Reality (VR). We outline a collection of relevant VR technologies, including devices engineered to stimulate haptic, temperature, vestibular, olfactory, and auditory sensations. We believe these technologies, which have been developed for high mobility and low cost, can be translated to the field of dream engineering. We close by discussing possible future directions in this field and the ethics of a world in which targeted dream direction and sleep manipulation are feasible.
SP  - 102955
EP  - NA
JF  - Consciousness and cognition
VL  - 83
IS  - NA
PB  - 
DO  - 10.1016/j.concog.2020.102955
ER  - 

TY  - NA
AU  - Morita, Takafumi; Kuwajima, Yu; Minaminosono, Ayato; Maeda, Shingo; Kakehi, Yasuaki
TI  - HydroMod : Constructive Modules for Prototyping Hydraulic Physical Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502096
ER  - 

TY  - NA
AU  - Tsuchiya, Keitaro; Lin, Kaiyuan; Nakao, Takuro; Kunze, Kai
TI  - MobileHCI - Watch Spaces: A Spatial User Interface for Smart Watches
PY  - 2019
AB  - We present a platform to prototype spatial user interfaces for smart watches relative to the user's body. We show the general feasibility and present 2 applications. The first application scenario enable users "pin" applications in the air around them and get back to them by moving the smart watch display at the same position again. The second application shows a zoom use case for maps or other larger displays/visualizations. The smart watch acts like a digital "magnifying glass" enabling the user to see details he cares about.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3344412
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Chen, Bing-Yu
TI  - UIST - ElastImpact: 2.5D Multilevel Instant Impact Using Elasticity on Head-Mounted Displays
PY  - 2019
AB  - Impact is a common effect in both daily life and virtual reality (VR) experiences, e.g., being punched, hit or bumped. Impact force is instantly produced, which is distinct from other force feedback, e.g., push and pull. We propose ElastImpact to provide 2.5D instant impact on a head-mounted display (HMD) for realistic and versatile VR experiences. ElastImpact consists of three impact devices, also called impactors. Each impactor blocks an elastic band with a mechanical brake using a servo motor and extending it using a DC motor to store the impact power. When releasing the brake, it provides impact instantly. Two impactors are affixed on both sides of the head and connected with the HMD to provide the normal direction impact toward the face (i.e., 0.5D in z-axis). The other impactor is connected with a proxy collider in a barrel in front of the HMD and rotated by a DC motor in the tangential plane of the face to provide 2D impact (i.e., xy-plane). By performing a just-noticeable difference (JND) study, we realize users' impact force perception distinguishability on the heads in the normal direction and tangential plane, separately. Based on the results, we combine normal and tangential impact as 2.5D impact, and performed a VR experience study to verify that the proposed 2.5D impact significantly enhances realism.
SP  - 429
EP  - 437
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347931
ER  - 

TY  - NA
AU  - Oh, Hyunjoo; Ta, Tung D.; Suzuki, Ryo; Gross, Mark D.; Kawahara, Yoshihiro; Yao, Lining
TI  - CHI - PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices
PY  - 2018
AB  - We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.
SP  - 441
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174015
ER  - 

TY  - JOUR
AU  - Groeger, Daniel; Steimle, Jürgen
TI  - ObjectSkin: Augmenting Everyday Objects with Hydroprinted Touch Sensors and Displays
PY  - 2018
AB  - Augmenting everyday objects with interactive input and output surfaces is a long-standing topic in ubiquitous computing and HCI research. Existing approaches, however, fail to leverage the objects' full potential, particularly in highly curved organic geometries and in diverse visuo-haptic surface properties. We contribute ObjectSkin, a fabrication technique for adding conformal interactive surfaces to rigid and flexible everyday objects. It enables multi-touch sensing and display output that seamlessly integrates with highly curved and irregular geometries. The approach is based on a novel water-transfer process for interactive surfaces. It leverages off-the-shelf hobbyist equipment to fabricate thin, conformal, and translucent electronic circuits that preserve the surface characteristics of everyday objects. It offers two methods, for rapid low-fidelity and versatile high-fidelity prototyping, and is applicable to a wide variety of materials. Results from a series of technical experiments provide insights into the supported object geometries, compatible object materials, and robustness. Seven example cases demonstrate how ObjectSkin makes it possible to leverage geometries, surface properties, and unconventional objects for prototyping novel interactions for ubiquitous computing.
SP  - 134
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161165
ER  - 

TY  - NA
AU  - Tennent, Paul; Marshall, Joe; Brundell, Patrick; Walker, Brendan; Benford, Steve
TI  - CHI - Abstract Machines: Overlaying Virtual Worlds on Physical Rides
PY  - 2019
AB  - Overlaying virtual worlds onto existing physical rides and altering the sensations of motion can deliver new experiences of thrill, but designing how motion is mapped between physical ride and virtual world is challenging. In this paper, we present the notion of an abstract machine, a new form of intermediate design knowledge that communicates motion mappings at the level of metaphor, mechanism and implementation. Following a performance-led, in-the-wild approach we report lessons from creating and touring VR Playground, a ride that overlays four distinct abstract machines and virtual worlds on a playground swing. We compare the artist's rationale with riders' reported experiences and analysis of their physical behaviours to reveal the distinct thrills of each abstract machine. Finally, we discuss how to make and use abstract machines in terms of heuristics for designing motion mappings, principles for virtual world design and communicating experiences to riders.
SP  - 581
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300811
ER  - 

TY  - NA
AU  - Sun, Wei; Li, Franklin Mingzhe; Steeper, Benjamin; Xu, Songlin; Tian, Feng; Zhang, Cheng
TI  - TeethTap: Recognizing Discrete Teeth Gestures Using Motion and Acoustic Sensing on an Earpiece
PY  - 2021
AB  - Teeth gestures become an alternative input modality for different situations and accessibility purposes. In this paper, we present TeethTap, a novel eyes-free and hands-free input technique, which can recognize up to 13 discrete teeth tapping gestures. TeethTap adopts a wearable 3D printed earpiece with an IMU sensor and a contact microphone behind both ears, which works in tandem to detect jaw movement and sound data, respectively. TeethTap uses a support vector machine to classify gestures from noise by fusing acoustic and motion data, and implements K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW) distance measurement using motion data for gesture classification. A user study with 11 participants demonstrated that TeethTap could recognize 13 gestures with a real-time classification accuracy of 90.9% in a laboratory environment. We further uncovered the accuracy differences on different teeth gestures when having sensors on single vs. both sides. Moreover, we explored the activation gesture under real-world environments, including eating, speaking, walking and jumping. Based on our findings, we further discussed potential applications and practical challenges of integrating TeethTap into future devices.
SP  - 161
EP  - 169
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450645
ER  - 

TY  - NA
AU  - Xia, Haijun; Herscher, Sebastian; Perlin, Ken; Wigdor, Daniel
TI  - UIST - Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality
PY  - 2018
AB  - Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users' agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.
SP  - 853
EP  - 866
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242597
ER  - 

TY  - NA
AU  - Wiehr, Frederik; Kosmalla, Felix; Daiber, Florian; Krüger, Antonio
TI  - UbiComp/ISWC Adjunct - FootStriker: an EMS-based foot strike assistant for running
PY  - 2017
AB  - In this demo, we present our FootStriker prototype in companion to the full paper. FootStriker detects heel striking while running with a pressure-sensitive insole and corrects the striking in real-time to mid-/forefoot running by applying electrical muscle stimulation (EMS) on one of the calf muscles. The device will be worn and demonstrated by the presenter but if possible, it can also be tested directly by the conference attendees. We provide them with visual realtime feedback for demonstration purposes.
SP  - 317
EP  - 320
JF  - Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123024.3123191
ER  - 

TY  - NA
AU  - Faltaous, Sarah; Koelle, Marion; Schneegass, Stefan
TI  - From Perception to Action: A Review and Taxonomy on Electrical Muscle Stimulation in HCI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3568460
ER  - 

TY  - NA
AU  - Nishida, Jun; Kasahara, Shunichi; Suzuki, Kenji
TI  - SIGGRAPH Emerging Technologies - Wired muscle: generating faster kinesthetic reaction by inter-personally connecting muscles
PY  - 2017
AB  - Instantaneously generating own body movements in response to the movement of others, such as establishing defensive posture in sports and learning kick-out timing from therapists in gait rehabilitation, is an essential aspect of interpersonal exercises and contact sports. However, ignition of movement based on a visual stimulus requires approximately 250 milliseconds (ms), which is too late for certain interpersonal physical interactions that require immediate reaction. Thus, we introduce "Wired Muscle," a system that connects muscle activities between two persons using electromyogram (EMG) measurement and electrical muscle stimulation (EMS) to generate responsive movement that are faster than those generated by the visual information-based process. Our system detects the muscle activity of a person by the EMG and triggers the EMS to drive the muscle of the other person to induce corresponding counter movements. In a pilot study using our system, the reaction time to the motion of another person could be shortened to approximately 60 ms. In addition, some participants perceive that the kinesthetic reaction was performed by their own will even though the muscle movement was electrically driven by prior stimuli. We envision that our system will enable direct connection of kinesthetic experiences among multiple persons and will form the basis for a novel paradigm of motor learning.
SP  - 26
EP  - NA
JF  - ACM SIGGRAPH 2017 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3084822.3084844
ER  - 

TY  - NA
AU  - Gong, Jun; Huang, Da-Yuan; Seyed, Teddy; Lin, Te; Hou, Tao; Liu, Xin; Yang, Molin; Yang, Boyu; Zhang, Yuhan; Yang, Xing-Dong
TI  - CHI - Jetto: Using Lateral Force Feedback for Smartwatch Interactions
PY  - 2018
AB  - Interacting with media and games is a challenging user experience on smartwatches due to their small screens. We propose using lateral force feedback to enhance these experiences. When virtual objects on the smartwatch display visually collide or push the edge of the screen, we add haptic feedback so that the user also feels the impact. This addition creates the illusion of a virtual object that is physically hitting or pushing the smartwatch, from within the device itself. Using this approach, we extend virtual space and scenes into a 2D physical space. To create realistic lateral force feedback, we first examined the minimum change in force magnitude that is detectable by users in different directions and weight levels, finding an average JND of 49% across all tested conditions, with no significant effect of weight and force direction. We then developed a proof-of-concept hardware prototype called Jetto and demonstrated its unique capabilities through a set of impact-enhanced videos and games. Our preliminary user evaluations indicated the concept was welcomed and is regarded as a worthwhile addition to smartwatch output and media experiences.
SP  - 426
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174000
ER  - 

TY  - NA
AU  - He, Liang; Wang, Ruolin; Xu, Xuhai
TI  - CHI Extended Abstracts - PneuFetch: Supporting Blind and Visually Impaired People to Fetch Nearby Objects via Light Haptic Cues
PY  - 2020
AB  - We present PneuFetch, a light haptic cue-based wearable device that supports blind and visually impaired (BVI) people to fetch nearby objects in an unfamiliar environment. In our design, we generate friendly, non-intrusive, and gentle presses and drags to deliver direction and distance cues on BVI user's wrist and forearm. As a concept of proof, we discuss our PneuFetch wearable prototype, contrast it with past work, and describe a preliminary user study.
SP  - 1
EP  - 9
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383095
ER  - 

TY  - NA
AU  - Kosch, Thomas; Hassib, Mariam; Woźniak, Paweł W.; Buschek, Daniel; Alt, Florian
TI  - CHI - Your Eyes Tell: Leveraging Smooth Pursuit for Assessing Cognitive Workload
PY  - 2018
AB  - A common objective for context-aware computing systems is to predict how user interfaces impact user performance regarding their cognitive capabilities. Existing approaches such as questionnaires or pupil dilation measurements either only allow for subjective assessments or are susceptible to environmental influences and user physiology. We address these challenges by exploiting the fact that cognitive workload influences smooth pursuit eye movements. We compared three trajectories and two speeds under different levels of cognitive workload within a user study (N=20). We found higher deviations of gaze points during smooth pursuit eye movements for specific trajectory types at higher cognitive workload levels. Using an SVM classifier, we predict cognitive workload through smooth pursuit with an accuracy of 99.5% for distinguishing between low and high workload as well as an accuracy of 88.1% for estimating workload between three levels of difficulty. We discuss implications and present use cases of how cognition-aware systems benefit from inferring cognitive workload in real-time by smooth pursuit eye movements.
SP  - 436
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174010
ER  - 

TY  - NA
AU  - Fadaei, J. Atena; Franza, Matteo; Kannape, Oliver Alan; Hara, Masayuki; Blanke, Olaf
TI  - Tactile spatial discrimination on the torso using vibrotactile and force stimulation
PY  - 2021
AB  - Abstract There is a steadily growing number of mobile communication systems that provide spatially encoded tactile information to the humans’ torso. However, the increased use of such hands-off displays is currently not matched with or supported by systematic perceptual characterization of tactile spatial discrimination on the torso. Furthermore, there are currently no data testing spatial discrimination for dynamic force stimuli applied to the torso. In the present study, we measured tactile point localization (PL) and tactile direction discrimination (DD) on the thoracic spine using two unisex torso-worn tactile vests realized with arrays of 3×3 vibrotactile or force feedback actuators. We aimed to, firstly, evaluate and compare the spatial discrimination of vibrotactile and force stimulations on the thoracic spine and, secondly, to investigate the relationship between the PL and DD results across stimulations. Thirty-four healthy participants performed both tasks with both vests. Tactile accuracies for vibrotactile and force stimulations were 60.7% and 54.6% for the PL task; 71.0% and 67.7% for the DD task, respectively. Performance correlated positively with both stimulations, although accuracies were higher for the vibrotactile than for the force stimulation across tasks, arguably due to specific properties of vibrotactile stimulations. We observed comparable directional anisotropies in the PL results for both stimulations; however, anisotropies in the DD task were only observed with vibrotactile stimulations. We discuss our findings with respect to tactile perception research as well as their implications for the design of high-resolution torso-mounted tactile displays for spatial cueing.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.1101/2021.03.26.437195
ER  - 

TY  - NA
AU  - Wessely, Michael; Tsandilas, Theophanis; Mackay, Wendy E.
TI  - UIST - Shape-Aware Material: Interactive Fabrication with ShapeMe
PY  - 2018
AB  - Makers often create both physical and digital prototypes to explore a design, taking advantage of the subtle feel of physical materials and the precision and power of digital models. We introduce ShapeMe, a novel smart material that captures its own geometry as it is physically cut by an artist or designer. ShapeMe includes a software toolkit that lets its users generate customized, embeddable sensors that can accommodate various object shapes. As the designer works on a physical prototype, the toolkit streams the artist's physical changes to its digital counterpart in a 3D CAD environment. We use a rapid, inexpensive and simple-to-manufacture inkjet printing technique to create embedded sensors. We successfully created a linear predictive model of the sensors' lengths, and our empirical tests of ShapeMe show an average accuracy of 2 to 3 mm. We present two application scenarios for modeling multi-object constructions, such as architectural models, and 3D models consisting of multiple layers stacked one on top of each other. ShapeMe demonstrates a novel technique for integrating digital and physical modeling, and suggests new possibilities for creating shape-aware materials.
SP  - 127
EP  - 139
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - 18
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242619
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Trotter, Ludwig; Tessmann, Markus; Dannhart, Christina; Bulling, Andreas; Alt, Florian
TI  - MUM - EyeVote in the wild: do users bother correcting system errors on public displays?
PY  - 2016
AB  - Although recovering from errors is straightforward on most interfaces, public display systems pose very unique design challenges. Namely, public display users interact for very short amounts of times and are believed to abandon the display when interrupted or forced to deviate from the main task. To date, it is not well understood whether public display designers should enable users to correct errors (e.g. by asking users to confirm or giving them a chance correct their input), or aim for faster interaction and rely on other types of feedback to estimate errors. To close this gap, we conducted a field study where we investigated the users willingness to correct their input on public displays. We report on our findings from an in-the-wild deployment of a public gaze-based voting system where we intentionally evoked system errors to see if users correct them. We found that public display users are willing to correct system errors provided that the correction is fast and straightforward. We discuss how our findings influence the choice of interaction methods for public displays; interaction methods that are highly usable but suffer from low accuracy can still be effective if users can "undo" their interactions.
SP  - 57
EP  - 62
JF  - Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3012709.3012743
ER  - 

TY  - NA
AU  - Yamaoka, Junichi; Dogan, Mustafa Doga; Bulovic, Katarina; Saito, Kazuya; Kawahara, Yoshihiro; Kakehi, Yasuaki; Mueller, Stefanie
TI  - CHI Extended Abstracts - FoldTronics Demo: Creating 3D Objects with Integrated Electronics Using Foldable Honeycomb Structures
PY  - 2019
AB  - We present FoldTronics, a 2D-cutting based fabrication technique to integrate electronics into 3D folded objects. The key idea is to cut and perforate a 2D sheet to make it foldable into a honeycomb structure using a cutting plotter; before folding the sheet into a 3D structure, users place the electronic components and circuitry onto the sheet. The fabrication process only takes a few minutes allowing to rapidly prototype functional interactive devices. The resulting objects are lightweight and rigid, thus allowing for weight-sensitive and force-sensitive applications. Finally, due to the nature of the honeycomb structure, the objects can be folded flat along one axis and thus can be efficiently transported in this compact form factor. We describe the structure of the foldable sheet, and present a design tool that enables users to quickly prototype the desired objects. We showcase a range of examples made with our design tool, including objects with integrated sensors and display elements.
SP  - 628
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300858
ER  - 

TY  - NA
AU  - Blum, Jeffrey R.; Cooperstock, Jeremy R.
TI  - WHC - Single-Actuator Vibrotactile Numeric Information Delivery in the Face of Distraction
PY  - 2019
AB  - We test ActiVibe, a previously reported method for communicating numeric values between 1 and 10, to determine whether it remains optimal under conditions reflective of more challenging potential real-world use cases. We thus consider vibrotactile communication in conjunction with an audio distractor task, and when conveying not just one, but three numeric values in succession. Results of a user study comparing three different rendering methods indicate that ActiVibe maintains both accuracy and subjective preference advantages vs. two different duration-based methods when conveying a single value, but largely loses these advantages when presenting three sequential values. Under conditions similar to the most difficult ones we test, a more concise duration-only approach may be preferable for some applications, requiring less power consumption and demanding attention for less total time.
SP  - 461
EP  - 466
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816082
ER  - 

TY  - NA
AU  - Pohl, Henning; Hoheisel, Franziska; Rohs, Michael
TI  - CHI Extended Abstracts - Inhibiting Freedom of Movement with Compression Feedback
PY  - 2017
AB  - Compression feedback uses inflatable straps to create uniform pressure sensations around limbs. Lower-pressure stimuli are well suited as a feedback channel for, e.g., notifications. However, operating compression feedback systems at higher pressure levels allows to physically inhibit movement. Here, we describe this modality and present a pervasive jogging game that employs physical inhibition to push runners to reach checkpoints in time.
SP  - 1962
EP  - 1969
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053081
ER  - 

TY  - NA
AU  - Lopes, Pedro; Pfeiffer, Max; Rohs, Michael; Baudisch, Patrick
TI  - CHI Extended Abstracts - Hands-on Introduction to Interactive Electrical Muscle Stimulation
PY  - 2016
AB  - In this course, participants create their own prototypes using electrical muscle stimulation. We provide a ready-to-use device and toolkit that allows for programmatically actuating the user's muscles.
SP  - 944
EP  - 947
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2856672
ER  - 

TY  - NA
AU  - Velloso, Eduardo; Wirth, Markus; Weichel, Christian; Esteves, Augusto; Gellersen, Hans
TI  - Conference on Designing Interactive Systems - AmbiGaze: Direct Control of Ambient Devices by Gaze
PY  - 2016
AB  - Eye tracking offers many opportunities for direct device control in smart environments, but issues such as the need for calibration and the Midas touch problem make it impractical. In this paper, we propose AmbiGaze, a smart environment that employs the animation of targets to provide users with direct control of devices by gaze only through smooth pursuit tracking. We propose a design space of means of exposing functionality through movement and illustrate the concept through four prototypes. We evaluated the system in a user study and found that AmbiGaze enables robust gaze-only interaction with many devices, from multiple positions in the environment, in a spontaneous and comfortable manner.
SP  - 812
EP  - 817
JF  - Proceedings of the 2016 ACM Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2901790.2901867
ER  - 

TY  - CHAP
AU  - Dionisio, Mara; Bala, Paulo; Nisi, Valentina; Oakley, Ian; Nunes, Nuno Jardim
TI  - ACE - Step by Step: Evaluating Navigation Styles in Mixed Reality Entertainment Experience
PY  - 2018
AB  - The availability of depth sensing technology in smartphones and tablets adds spatial awareness as an interaction modality to mobile entertainment experiences and showcases the potential of Mixed Reality (MR) for creating immersive and engaging experiences in real world contexts. However, the lack of design knowledge about interactions within MR represents a barrier to creating effective entertainment experiences. Faced with this challenge, we contribute a study of three navigation styles (NS) for MR experiences shown on a handheld device. The navigation styles range from fully virtual, through a mixed style that involves both on-screen and in-world activity, to fully real navigation. Our findings suggest that when designing an MR experience, the navigation style deployed should reflect the context, content and required interactions. For our MR experience, “The Old Pharmacy”, with its specific content, context and required interactions, results show that navigation styles relying on in-world activity leads to higher levels of Presence, Immersion and Flow.
SP  - 32
EP  - 45
JF  - Advances in Computer Entertainment Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-76270-8_3
ER  - 

TY  - NA
AU  - Zhang, Xucong; Sugano, Yusuke; Bulling, Andreas
TI  - Evaluation of Appearance-Based Methods and Implications for Gaze-Based Applications
PY  - 2019
AB  - Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (this http URL), the first software toolkit for appearance-based gaze estimation and interaction.
SP  - 416
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300646
ER  - 

TY  - CHAP
AU  - Liu, Chuanyi; Zhang, Jiali; Ma, Kang
TI  - ICIC (3) - Natural and Fluid 3D Operations with Multiple Input Channels of a Digital Pen
PY  - 2018
AB  - We propose six 3D operation patterns with multiple input channels of a digital pen: these patterns allow users to transfer pre-existing knowledge of physical pens to digital pens on performing 3D operations simply, naturally, intuitively, and fluidly. A prototype system was designed under the patterns and implemented. An informal user study showed that eight novices grasped to perform 3D operations with the prototype system within several minutes and gained more fun than with the typical interfaces.
SP  - 585
EP  - 598
JF  - Intelligent Computing Methodologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-95957-3_61
ER  - 

TY  - NA
AU  - Brucato, Matteo; Abouzied, Azza; Blauvelt, Chris
TI  - Redistributing Funds across Charitable Crowdfunding Campaigns
PY  - 2017
AB  - On Kickstarter only 36% of crowdfunding campaigns successfully raise sufficient funds for their projects. In this paper, we explore the possibility of redistribution of crowdfunding donations to increase the chances of success. We define several intuitive redistribution policies and, using data from a real crowdfunding platform, LaunchGood, we assess the potential improvement in campaign fundraising success rates. We find that an aggressive redistribution scheme can boost campaign success rates from 37% to 79%, but such choice-agnostic redistribution schemes come at the cost of disregarding donor preferences. Taking inspiration from offline giving societies and donor clubs, we build a case for choice preserving redistribution schemes that strike a balance between increasing the number of successful campaigns and respecting giving preference. We find that choice-preserving redistribution can easily achieve campaign success rates of 48%. Finally, we discuss the implications of these different redistribution schemes for the various stakeholders in the crowdfunding ecosystem.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kim, Yoonji; Choi, Youngkyung; Lee, Hyein; Lee, Geehyuk; Bianchi, Andrea
TI  - CHI - VirtualComponent: A Mixed-Reality Tool for Designing and Tuning Breadboarded Circuits
PY  - 2019
AB  - Prototyping electronic circuits is an increasingly popular activity, supported by researchers, who develop toolkits to improve the design, debugging, and fabrication of electronics. Although past work mainly dealt with circuit topology, in this paper we propose a system for determining or tuning the values of the circuit components. Based on the results of a formative study with seventeen makers, we designed VirtualComponent, a mixed-reality tool that allows users to digitally place electronic components on a real breadboard, tune their values in software, and see these changes applied to the physical circuit in real-time. VirtualComponent is composed of a set of plug-and-play modules containing banks of components, and a custom breadboard managing the connections and components' values. Through demonstrations and the results of an informal study with twelve makers, we show that VirtualComponent is easy to use and allows users to test components' value configurations with little effort.
SP  - 177
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300407
ER  - 

TY  - NA
AU  - Salunke, Shreeyash
TI  - IHCI - TactAlive: Context Aware Variable Tactile Key Interface
PY  - 2016
AB  - In this paper, we introduce, a bidirectional haptic I/O which varies its tactility depending on the context of the device. The keys in the interface make the user alter their applied fingertip force depending on the criticality, physicality and other attributes of the context. The interface can have many levels of tactility: soft press to hard press. Beyond the utilitarian values, the interactions are designed to alter the user behavior by making them context aware. This project attempts to expand our understanding of the context, and entirely reshape our interactions based on this understanding.
SP  - 178
EP  - 182
JF  - Proceedings of the 8th Indian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3014362.3014383
ER  - 

TY  - NA
AU  - Wang, Bryan; Grossman, Tovi
TI  - CHI - BlyncSync: Enabling Multimodal Smartwatch Gestures with Synchronous Touch and Blink
PY  - 2020
AB  - Input techniques have been drawing abiding attention along with the continual miniaturization of personal computers. In this paper, we present BlyncSync, a novel multi-modal gesture set that leverages the synchronicity of touch and blink events to augment the input vocabulary of smartwatches with a rapid gesture, while at the same time, offers a solution to the false activation problem of blink-based input. BlyncSync contributes the concept of a mutual delimiter, where two modalities are used to jointly delimit the intention of each other's input. A study shows that BlyncSync is 33% faster than using a baseline input delimiter (physical smartwatch button), with only 150ms in overhead cost compared to traditional touch events. Furthermore, our data indicates that the gesture can be tuned to elicit a true positive rate of 97% and a false positive rate of 1.68%.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376132
ER  - 

TY  - NA
AU  - Škola, Filip; Liarokapis, Fotis
TI  - CHI - Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation
PY  - 2019
AB  - Virtual reality (VR) can be immersive to such a degree that users sometimes report feeling tactile sensations based on visualization of the touch, without any actual physical contact. This effect is not only interesting for studies of human perception, but can also be leveraged to improve the quality of VR by evoking tactile sensations without usage of specialized equipment. The aim of this paper is to study brain processing of the illusory touch and its enhancement for purposes of exploitation in VR scene design. To amplify the illusory touch, transcranial direct current stimulation (tDCS) was used. Participants attended two sessions with blinded stimulation and interacted with a virtual ball using tracked hands in VR. The effects were studied using electroencephalography (EEG), that allowed us to examine stimulation-induced changes in processing of the illusory touch in the brain, as well as to identify its neural correlates. Results confirm enhanced processing of the illusory touch after the stimulation, and some of these changes were correlated to subjective rating of its magnitude.
SP  - 247
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300477
ER  - 

TY  - NA
AU  - Wang, Chiu-Hsuan; Hsieh, Chen-Yuan; Yu, Nena-Hao; Bianchi, Andrea; Chan, Liwei
TI  - VR - HapticSphere: Physical Support To Enable Precision Touch Interaction in Mobile Mixed-Reality
PY  - 2019
AB  - This work presents HapticSphere, a wearable spherical surface enabled by bridging a finger and the head-mounted display (HMD) with a passive string. Users perceive a physical support on a finger attached to a string, when extending their arm and reaching out to the string's maximum extension. This physical support assists users in precise touch interaction in the context of stationary and walking virtual or mixed-reality experiences. We propose three methods of attachment of the haptic string (directly on the head or on the body), and illustrate a novel single-step calibration algorithm that supports these configurations by estimating a grand haptic sphere, once a head-coordinated touch interaction is established. Two user studies were conducted to validate our approach and to compare the touch performance with physical support in sitting and walking conditions in the context of mobile mixed-reality scenarios. The results show that, in the walking condition, touch interaction with physical support significantly outperformed the visual-only condition.
SP  - 331
EP  - 339
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798255
ER  - 

TY  - NA
AU  - Yi, Xin; Qiu, Leping; Tang, Wenjing; Fan, Yehan; Li, Hewu; Shi, Yuanchun
TI  - DEEP: 3D Gaze Pointing in Virtual Reality Leveraging Eyelid Movement
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545673
ER  - 

TY  - BOOK
AU  - Bowers, Brook; Rukangu, Andrew; Johnsen, Kyle
TI  - VR Workshops - Making it Simple: Expanding Access and Lowering Barriers to Novel Interaction Devices for Virtual and Augmented Reality
PY  - 2020
AB  - The open source community has been an integral part in the rapid growth of the maker and DIY communities, mainly by facilitating easy exchange of information and ideas, thereby lowering the jump from ideation to a testable prototype. In this work, we add a new class of open-source tools and resources which, when used in conjunction with existing virtual reality or augmented reality systems, provide for an improved virtual experience. We focus on making our processes accessible and reproducible while keeping the costs of production of the devices as low as possible. Additionally, we describe three successful use-case projects that live in the interface between hardware and software, and we open-source the designs and software for easy reproduction by interested researchers. We hope that this paper will give AR/VR researchers the confidence in taking the next step of making virtual reality as realistic as possible and will hopefully snowball into a new class of devices/ecosystem.
SP  - 1
EP  - 6
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.0-268
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Fitzgerald, Daniel; Ma, Zhiyao; Vink, Luke; Levine, Daniel S.; Ishii, Hiroshi
TI  - Tangible and Embedded Interaction - inFORCE: Bi-directional `Force' Shape Display for Haptic Interaction
PY  - 2019
AB  - While previously proposed hardware on pin-based shape display has improved various technical aspects, there has been a clear limitation on the haptic quality of variable 'force' feedback. In this paper, we explore a novel haptic interaction design space with 'force' controlled shape display. Utilizing high performance linear actuators with current reading functionality, we built a 10 x 5 'force' shape display, named inFORCE, that can both detect and exert variable force on individual pins. By integrating closed-loop force control, our system can provide real-time variable haptic feedback in response to the way users press the pins. Our haptic interaction design space includes volumetric haptic feedback, material emulation, layer snapping, and friction. Our proposed interaction methods, for example, enables people to "press through'' computationally rendered dynamic shapes to understand the internal structure of 3D volumetric information. We also demonstrate a material property capturing functionality. Our technical evaluation and user study assesses the hardware capability and haptic perception through interaction with inFORCE. We also discuss application spaces that 'force' shape display can be used for.
SP  - 615
EP  - 623
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295621
ER  - 

TY  - NA
AU  - Shahu, Ambika; Wintersberger, Philipp; Michahelles, Florian
TI  - Scenario-based Investigation of Acceptance of Electric Muscle Stimulation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519416
ER  - 

TY  - NA
AU  - Swaminathan, Saiganesh; Ozutemiz, Kadri Bugra; Majidi, Carmel; Hudson, Scott E.
TI  - CHI - FiberWire: Embedding Electronic Function into 3D Printed Mechanically Strong, Lightweight Carbon Fiber Composite Objects
PY  - 2019
AB  - 3D printing offers significant potential in creating highly customized interactive and functional objects. However, at present ability to manufacture functional objects is limited by available materials (e.g., various polymers) and their process properties. For instance, many functional objects need stronger materials which may be satisfied with metal printers. However, to create wholly interactive devices, we need both conductors and insulators to create wiring, and electronic components to complete circuits. Unfortunately, the single material nature of metal printing, and its inherent high temperatures, preclude this. Thus, in 3D printed devices, we have had a choice of strong materials, or embedded interactivity, but not both. In this paper, we introduce a set of techniques we call FiberWire, which leverages a new commercially available capability to 3D print carbon fiber composite objects. These objects are light weight and mechanically strong, and our techniques demonstrate a means to embed circuitry for interactive devices within them. With FiberWire, we describe a fabrication pipeline takes advantage of laser etching and fiber printing between layers of carbon-fiber composite to form low resistance conductors, thereby enabling the fabrication of electronics directly embedded into mechanically strong objects. Utilizing the fabrication pipeline, we show a range of sensor designs, their performance characterization on these new materials and finally three fully printed example object that are both interactive and mechanically strong -- a bicycle handle bar with interactive controls, a swing and impact sensing golf club and an interactive game controller (Figure 1).
SP  - 567
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300797
ER  - 

TY  - NA
AU  - Groeger, Daniel; Loo, Elena Chong; Steimle, Jürgen
TI  - CHI - HotFlex: Post-print Customization of 3D Prints Using Embedded State Change
PY  - 2016
AB  - While 3D printing offers great design flexibility before the object is printed, it is very hard for end-users to customize a 3D-printed object to their specific needs after it is printed. We propose HotFlex: a new approach allowing precisely located parts of a 3D object to transition on demand from a solid into a deformable state and back. This approach enables intuitive hands-on remodeling, personalization, and customization of a 3D object after it is printed. We introduce the approach and present an implementation based on computer-controlled printed heating elements that are embedded within the 3D object. We present a set of functional patterns that act as building blocks and enable various forms of hands-on customization. Furthermore, we demonstrate how to integrate sensing of user input and visual output. A series of technical experiments and various application examples demonstrate the practical feasibility of the approach.
SP  - 420
EP  - 432
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858191
ER  - 

TY  - JOUR
AU  - Velloso, Eduardo; Carter, Marcus; Newn, Joshua; Esteves, Augusto; Clarke, Christopher; Gellersen, Hans
TI  - Motion Correlation: Selecting Objects by Matching Their Movement
PY  - 2017
AB  - Selection is a canonical task in user interfaces, commonly supported by presenting objects for acquisition by pointing. In this article, we consider motion correlation as an alternative for selection. The principle is to represent available objects by motion in the interface, have users identify a target by mimicking its specific motion, and use the correlation between the system’s output with the user’s input to determine the selection. The resulting interaction has compelling properties, as users are guided by motion feedback, and only need to copy a presented motion. Motion correlation has been explored in earlier work but only recently begun to feature in holistic interface designs. We provide a first comprehensive review of the principle, and present an analysis of five previously published works, in which motion correlation underpinned the design of novel gaze and gesture interfaces for diverse application contexts. We derive guidelines for motion correlation algorithms, motion feedback, choice of modalities, overall design of motion correlation interfaces, and identify opportunities and challenges identified for future research and design.
SP  - 22
EP  - 35
JF  - ACM Transactions on Computer-Human Interaction
VL  - 24
IS  - 3
PB  - 
DO  - 10.1145/3064937
ER  - 

TY  - NA
AU  - Wiehr, Frederik; Kosmalla, Felix; Daiber, Florian; Krüger, Antonio
TI  - MobileHCI - FootStriker: an EMS-based assistance system for real-time running style correction
PY  - 2017
AB  - Today, ambitioned amateur athletes often do not have access to professional coaching but still invest great effort in becoming faster runners. Apart from a pure increase in the quantitative training load, a change of the running technique, e.g. transitioning from heel striking to fore- or midfoot running, can be highly effective and usually prevents knee-related injuries. With this demo, we highlight factors to consider when determining EMS actuation phases for real-time running style correction in an outdoor scenario. During actuation the wearable applies electrical muscle stimulation (EMS) in the flight phase of a stride after having detected a heel-strike with force sensing resistors (FSR) in a sensor insole. To complement the original FootStriker lab prototype, we address the applicability in the field of the aforementioned real-time running style correction system.
SP  - 56
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3125444
ER  - 

TY  - NA
AU  - Lohr, Dillon J.; Komogortsev, Oleg V.
TI  - CHI Extended Abstracts - A Comparison of Smooth Pursuit- and Dwell-based Selection at Multiple Levels of Spatial Accuracy
PY  - 2017
AB  - In this paper, we present a smooth pursuit--based alternative to dwell-based selection for eye-guided user interfaces. Participants attempt to perform both dwell- and pursuit-based selections while we artificially reduce the spatial accuracy of an affordable eye tracker to see how resilient both selection methods are. We find that the time to perform a pursuit-based selection remains consistent even as spatial accuracy degrades, unlike dwell-based selection which takes considerably longer to perform the worse the spatial accuracy becomes. We argue that smooth pursuit--based selection will be important in eye-tracking systems with low spatial accuracy, such as very low cost trackers, certain self-made systems, and calibration-free systems.
SP  - 2760
EP  - 2766
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053233
ER  - 

TY  - NA
AU  - Schenk, Simon; Dreiser, Marc; Rigoll, Gerhard; Dorr, Michael
TI  - CHI - GazeEverywhere: Enabling Gaze-only User Interaction on an Unmodified Desktop PC in Everyday Scenarios
PY  - 2017
AB  - Eye tracking is becoming more and more affordable, and thus gaze has the potential to become a viable input modality for human-computer interaction. We present the GazeEverywhere solution that can replace the mouse with gaze control by adding a transparent layer on top of the system GUI. It comprises three parts: i) the SPOCK interaction method that is based on smooth pursuit eye movements and does not suffer from the Midas touch problem; ii) an online recalibration algorithm that continuously improves gaze-tracking accuracy using the SPOCK target projections as reference points; and iii) an optional hardware setup utilizing head-up display technology to project superimposed dynamic stimuli onto the PC screen where a software modification of the system is not feasible. In validation experiments, we show that GazeEverywhere's throughput according to ISO 9241-9 was improved over dwell time based interaction methods and nearly reached trackpad level. Online recalibration reduced interaction target ('button') size by about 25%. Finally, a case study showed that users were able to browse the internet and successfully run Wikirace using gaze only, without any plug-ins or other modifications.
SP  - 3034
EP  - 3044
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025455
ER  - 

TY  - BOOK
AU  - Rivu, NA; Abdrabou, Yasmeen; Mayer, Thomas E.; Pfeuffer, Ken; Alt, Florian
TI  - ETRA - GazeButton: enhancing buttons with eye gaze interactions
PY  - 2019
AB  - The button is an element of a user interface to trigger an action, traditionally using click or touch. We introduce GazeButton, a novel concept extending the default button mode with advanced gaze-based interactions. During normal interaction, users can utilise this button as a universal hub for gaze-based UI shortcuts. The advantages are: 1) easy to integrate in existing UIs, 2) complementary, as users choose either gaze or manual interaction, 3) straightforward, as all features are located in one button, and 4) one button to interact with the whole screen. We explore GazeButtons for a custom-made text reading, writing, and editing tool on a multitouch tablet device. For example, this allows the text cursor position to be set as users look at the position and tap on the GazeButton, avoiding costly physical movement. Or, users can simply gaze over a part of the text that should be selected, while holding the GazeButton. We present a design space, specific application examples, and point to future button designs that become highly expressive by unifying the user's visual and manual input.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3317956.3318154
ER  - 

TY  - NA
AU  - Kasahara, Shunichi; Nishida, Jun; Lopes, Pedro
TI  - CHI - Preemptive Action: Accelerating Human Reaction using Electrical Muscle Stimulation Without Compromising Agency
PY  - 2019
AB  - We enable preemptive force-feedback systems to speed up human reaction time without fully compromising the user's sense of agency. Typically these interfaces actuate by means of electrical muscle stimulation (EMS) or mechanical actuators; they preemptively move the user to perform a task, such as to improve movement performance (e.g., EMS-assisted drumming). Unfortunately, when using preemptive force-feedback users do not feel in control and loose their sense of agency. We address this by actuating the user's body, using EMS, within a particular time window (160 ms after visual stimulus), which we found to speed up reaction time by 80 ms in our first study. With this preemptive timing, when the user and system move congruently, the user feels that they initiated the motion, yet their reaction time is faster than usual. As our second study demonstrated, this particular timing significantly increased agency when compared to the current practice in EMS-based devices. We conclude by illustrating, using examples from the HCI literature, how to leverage our findings to provide more agency to automated haptic interfaces.
SP  - 643
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300873
ER  - 

TY  - JOUR
AU  - Blum, Jeffrey R.; Fortin, Pascal E.; Taha, Feras Al; Alirezaee, Parisa; Demers, Marc; Weill-Duflos, Antoine; Cooperstock, Jeremy R.
TI  - Getting Your Hands Dirty Outside the Lab: A Practical Primer for Conducting Wearable Vibrotactile Haptics Research
PY  - 2019
AB  - As haptics have become an ingrained part of our wearable experience, particularly through phones, smartwatches, and fitness trackers, significant research effort has been conducted to find new ways of using wearable haptics to convey information, especially while we are on-the-go. In this paper, instead of focusing on aspects of haptic information design, such as tacton encoding methods, actuators, and technical fabrication of devices, we address the more general recurring issues and “gotchas” that arise when moving from core haptic perceptual studies and in-lab wearable experiments to real world testing of wearable vibrotactile haptic systems. We summarize key issues for practitioners to take into account when designing and carrying out in-the-wild wearable haptic user studies, as well as for user studies in a lab environment that seek to simulate real-world conditions. We include not only examples from published work and commercial sources, but also hard-won illustrative examples derived from issues and failures from our own haptic studies. By providing a broad-based, accessible overview of recurring issues, we expect that both novice and experienced haptic researchers will find suggestions that will improve their own mobile wearable haptic studies.
SP  - 232
EP  - 246
JF  - IEEE transactions on haptics
VL  - 12
IS  - 3
PB  - 
DO  - 10.1109/toh.2019.2930608
ER  - 

TY  - NA
AU  - Murauer, Michaela; Haslgrübler, Michael; Ferscha, Alois
TI  - IOT - Natural pursuit calibration: using motion trajectories for unobtrusive calibration of mobile eye trackers
PY  - 2017
AB  - Although, gaze-based interaction has been investigated since the 1980s and remains a promising concept to support universal interaction within distributed IoT environments, main challenges like the Midas touch problem [6] or calibration are still frequent topics of research. In this work we present Natural Pursuit Calibration, a comfortable, unobtrusive technique enabling ongoing attention detection and eye tracker calibration in a real-world context. The user is able to perform calibration, without a digital user interface, artificial annotation of the environment and without assistance, by simply following any arbitrary moving target. Due to the characteristics of the calibration process it can be executed simultaneously to any primary task, without active user participation, resulting in a frequently updated calibration model.
SP  - 35
EP  - NA
JF  - Proceedings of the Seventh International Conference on the Internet of Things
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131542.3140271
ER  - 

TY  - JOUR
AU  - Bâce, Mihai; Staal, Sander; Sörös, Gábor; Corbellini, Giorgio
TI  - Collocated Multi-user Gestural Interactions with Unmodified Wearable Devices
PY  - 2017
AB  - Many real-life scenarios can benefit from both physical proximity and natural gesture interaction. In this paper, we explore shared collocated interactions on unmodified wearable devices. We introduce an interaction technique which enables a small group of people to interact using natural gestures. The proximity of users and devices is detected through acoustic ranging using inaudible signals, while in-air hand gestures are recognized from three-axis accelerometers. The underlying wireless communication between the devices is handled over Bluetooth for scalability and extensibility. We present (1) an overview of the interaction technique and (2) an extensive evaluation using unmodified, off-the-shelf, mobile, and wearable devices which show the feasibility of the method. Finally, we demonstrate the resulting design space with three examples of multi-user application scenarios.
SP  - 1
EP  - 14
JF  - Augmented Human Research
VL  - 2
IS  - 1
PB  - 
DO  - 10.1007/s41133-017-0009-z
ER  - 

TY  - NA
AU  - Strasnick, Evan; Follmer, Sean; Agrawala, Maneesh
TI  - CHI - Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation
PY  - 2019
AB  - Difficulties in accessing, isolating, and iterating on the components and connections of a printed circuit board (PCB) create unique challenges in PCB debugging. Manual probing methods are slow and error prone, and even dedicated PCB testing equipment remains limited by its inability to modify the circuit during testing. We present Pinpoint, a tool that facilitates in-circuit PCB debugging through techniques such as programmatically probing signals, dynamically disconnecting components and subcircuits to test in isolation, and splicing in new elements to explore potential modifications. Pinpoint automatically instruments a PCB design and generates designs for a physical jig board that interfaces the user's PCB to our custom testing hardware and to software tools. We evaluate Pinpoint's ability to facilitate the debugging of various PCB issues by instrumenting and testing different classes of boards, as well as by characterizing its technical limitations and by soliciting feedback through a guided exploration with PCB designers.
SP  - 48
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300278
ER  - 

TY  - CHAP
AU  - Wu, Connie; Chin, Erica D.; Fanton, Michael; Okamura, Allison M.
TI  - EuroHaptics (2) - Impact of Combined Stimuli on the Perception of Transient Forces
PY  - 2016
AB  - This paper discusses the development and characterization of a system for replicating the sensation of object-hand collisions. We present a novel wearable haptic band, comprised of a C2 tactor voice coil actuator and a servo-powered pressing mechanism. A virtual reality simulation in which a user actively bounces a ball on the hand was synced with the vibrations from a C2 tactor and pressure from a servo motor. User studies were conducted to determine the just-noticeable difference of pressure stimuli and realism of impact events with and without vibrations during the rendering of transient forces. We found that the presence of vibration reduced the just-noticeable difference of pressure, and that the addition of a constant vibration feedback to pressure appears to have more of an impact on the perception of the lower pressure levels than the higher pressure levels. These results indicate important considerations for the design of future devices that render transient haptic forces.
SP  - 416
EP  - 426
JF  - Haptics: Perception, Devices, Control, and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-42324-1_41
ER  - 

TY  - NA
AU  - Nuernberger, Benjamin; Höllerer, Tobias; Turk, Matthew
TI  - VRST - Hybrid orbiting-to-photos in 3D reconstructed visual reality
PY  - 2018
AB  - Virtually navigating through photos from a 3D image-based reconstruction has recently become very popular in many applications. In this paper, we consider a particular virtual travel maneuver that is important for this type of virtual navigation---orbiting to photos that can see a point-of-interest (POI). The main challenge with this particular type of orbiting is how to give appropriate feedback to the user regarding the existence and information of each photo in 3D while allowing the user to manipulate three degrees-of-freedom (DoF) for orbiting around the POI. We present a hybrid approach that combines features from two baselines---proxy plane and thumbnail approaches. Experimental results indicate that users rated our hybrid approach more favorably for several qualitative questionnaire statements, and that the hybrid approach is preferred over both baselines for outdoor scenes.
SP  - 32
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281528
ER  - 

TY  - NA
AU  - Gugenheimer, Jan; Wolf, Dennis; Eiriksson, Eythor Runar; Maes, Pattie; Rukzio, Enrico
TI  - UIST - GyroVR: Simulating Inertia in Virtual Reality using Head Worn Flywheels
PY  - 2016
AB  - We present GyroVR, head worn flywheels designed to render inertia in Virtual Reality (VR. Motions such as flying, diving or floating in outer space generate kinesthetic forces onto our body which impede movement and are currently not represented in VR. We simulate those kinesthetic forces by attaching flywheels to the users head, leveraging the gyroscopic effect of resistance when changing the spinning axis of rotation. GyroVR is an ungrounded, wireless and self contained device allowing the user to freely move inside the virtual environment. The generic shape allows to attach it to different positions on the users body. We evaluated the impact of GyroVR onto different mounting positions on the head (back and front) in terms of immersion, enjoyment and simulator sickness. Our results show, that attaching GyroVR onto the users head (front of the Head Mounted Display (HMD)) resulted in the highest level of immersion and enjoyment and therefore can be built into future VR HMDs, enabling kinesthetic forces in VR.
SP  - 227
EP  - 232
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984535
ER  - 

TY  - JOUR
AU  - Jenke, Marcus; Huppenbauer, Lena; Maier, Thomas
TI  - Investigation of continual operation procedures via a user centered gaze control by means of flexible gaze gestures
PY  - 2017
AB  - This work presents the pursued research results of a study about the continual control of virtual control elements by gaze, which was released in the proceedings of the spring congress 2017 arranged by the Society of Ergonomics and Work Science (German: GfA). The motivation to provide a more intuitive and robust gaze interaction method leads to the usage of specific gaze gestures. These gestures are based on unique gaze patterns which are distinct from typical intended as well as unintended viewing. For enabling extensive interactions through those gestures a gesture set was developed and tested in a first attempt in terms of its self-descriptiveness and learnability. The second objective of this study arises from the necessity to provide the user with vital information contents about his operation possibilities during the interaction by gaze gestures. By means of the design parameterization two different interfaces were tested concerning its design parameter shape. The objective was to come to first conclusions about the guiding method of representing specific visual interface elements to establish a satisfying gaze control. The results show that the applied gesture set can be learned quite easily and helps the user to connect the symbolic of the gesture with the corresponding execution. Furthermore, the experimental results give evidence of the preference of reference points (grid points) as the guiding elements rather than a polyline (square) shape in the gaze-gesture-controlled interface Practical Relevance: The research of gaze control is on the cutting edge. The control by gestures, for example through touch operation, has asserted themselves in various fields of application. These methods were employed to promote trends like autonomous driving, service design and individualization. Powerful and at the same time cost efficient eye tracking systems favor the extensive application of this technology beyond the recent research fields of medical diagnostic and individual interaction solutions for handicapped people. Furthermore, gaze control has great potential to help the user in fields like teleoperation, complex interfaces and cockpits as well as in the public service sector. Consequently it constitutes an innovative, intuitive and inspiring way of human system interaction which could also be useful combined with other interaction modalities. Besides all these arising opportunities in the mentioned fields, there is a big necessity to conscientiously concern the advantages and disadvantages of this technology. According to the research which has been accomplished in other, more established fields of interaction a well-defined design guideline, both for the gaze-controlled interface and the applied control characteristic is inevitable. The presented work depicts another step towards this approach.
SP  - 23
EP  - 34
JF  - Zeitschrift für Arbeitswissenschaft
VL  - 72
IS  - 1
PB  - 
DO  - 10.1007/s41449-017-0067-6
ER  - 

TY  - NA
AU  - Fortin, Pascal E.; Blum, Jeffrey R.; Cooperstock, Jeremy R.
TI  - UIST (Adjunct Volume) - Towards Consistent Haptic Coupling with HaptiStrap: Doing Better than "Tight yet Comfortable"
PY  - 2019
AB  - How firmly a haptic device, such as a smartwatch, is coupled to the body can change how its haptic effects are perceived. However, hapticians often rely on vague subjective coupling characteristics such as "strapped snugly" or "tight yet comfortable". Achieving consistent strap tightness across body sites and between participants can be challenging, since even if strap tension is consistent, differences in limb circumference alter the resulting normal force under the haptic actuator in potentially unintuitive ways. Furthermore, when participants must attach the devices on their own, e.g., during a longitudinal in-the-wild study, they may not use the same tightness each day without guidance. We present HaptiStrap, a low-cost, easily fabricated tool, as a contribution towards a standard method for ensuring that wearable haptic studies do better than vague and subjective "tight yet comfortable" guidelines.
SP  - 69
EP  - 71
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357118
ER  - 

TY  - BOOK
AU  - Verweij, David; Esteves, Augusto; Khan, Vassilis-Javed; Bakker, Saskia
TI  - ISS - Smart Home Control using Motion Matching and Smart Watches
PY  - 2017
AB  - This paper presents a prototype of a smart home control system operated through motion matching input. In motion matching, targets move continuously in a singular and pre-defined path; users interact with these targets by tracking their movement for a short period of time. Our prototype captures user input through the motion sensors embedded in off-the-shelf smartwatches while users track the moving targets with their arms and hands. The wearable nature of the tracking system makes our prototype ideal for interaction with numerous devices in a smart home.
SP  - 466
EP  - 468
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3132283
ER  - 

TY  - JOUR
AU  - Ortin, Marta; Jarabo, Adrian; Masia, Belen; Gutierrez, Diego
TI  - Analyzing Interfaces and Workflows for Light Field Editing
PY  - 2017
AB  - With the increasing number of available consumer light field cameras, such as Lytro, Raytrix, or Pelican Imaging, this new form of photography is progressively becoming more common. However, there are still very few tools for light field editing, and the interfaces to create those edits remain largely unexplored. Given the extended dimensionality of light field data, it is not clear what the most intuitive interfaces and optimal workflows are, in contrast with well-studied two-dimensional (2-D) image manipulation software. In this work, we provide a detailed description of subjects’ performance and preferences for a number of simple editing tasks, which form the basis for more complex operations. We perform a detailed state sequence analysis and hidden Markov chain analysis based on the sequence of tools and interaction paradigms users employ while editing light fields. These insights can aid researchers and designers in creating new light field editing tools and interfaces, thus helping to close the gap between 4-D and 2-D image editing.
SP  - 1162
EP  - 1172
JF  - IEEE Journal of Selected Topics in Signal Processing
VL  - 11
IS  - 7
PB  - 
DO  - 10.1109/jstsp.2017.2746263
ER  - 

TY  - JOUR
AU  - Raees, Muhammad; Ullah, Sehat
TI  - THE-3DI: Tracing head and eyes for 3D interactions
PY  - 2019
AB  - Gesture-based interfaces offer a suitable platform for interactions in Virtual Environments (VE). However, the difficulties involved in learning and making of distinct gestures affect the performance of an interactive system. By incorporating computer vision in Virtual Reality (VR), this paper presents an intuitive interaction technique where the states and positions of eyes are traced for interaction. With comparatively low cognitive load, the technique offers an easy to use interaction solution for VR applications. Unlike other gestural interfaces, interactions are performed in distinct phases where transition from one phase to another is enacted with simple blink of eyes. In an attained phase, interaction along an arbitrary axis is performed by a perceptive gesture of head; rolling, pitching or yawing. To follow the trajectory of eyes in real time, coordinates mapping is performed dynamically. The proposed technique is implemented in a case-study project; EBI (Eyes Blinking based Interaction). In the EBI project, real time detection and tracking of eyes are performed at the back-end. At the front-end, virtual scene is rendered accordingly by using the OpenGL library. To assess accuracy, usability and cognitive load of the proposed technique, the EBI project is evaluated 280 times in three different evaluation sessions. With an ordinary camera, an average accuracy of 81.4% is achieved. However, assessment made by using a high-quality camera revealed that accuracy of the system could be raised to a higher level. As a whole, findings of the evaluations support applicability of the technique in the emerging domains of VR.
SP  - 1311
EP  - 1337
JF  - Multimedia Tools and Applications
VL  - 79
IS  - 1
PB  - 
DO  - 10.1007/s11042-019-08305-6
ER  - 

TY  - NA
AU  - Sarkis, Mira; Coutrix, Céline; Nigay, Laurence; Duda, Andrzej
TI  - ICMI - WiBend: Wi-Fi for Sensing Passive Deformable Surfaces
PY  - 2019
AB  - We present WiBend, a system that recognizes bending gestures as the input modalities for interacting on non-instrumented and deformable surfaces using WiFi signals. WiBend takes advantage of off-the-shelf 802.11 (Wi-Fi) devices and Channel State Information (CSI) measurements of packet transmissions when the user is placed and interacting between a Wi-Fi transmitter and a receiver. We have performed extensive user experiments in an instrumented laboratory to obtain data for training the HMM models and for evaluating the precision of WiBend. During the experiments, participants performed 12 distinct bending gestures with three surface sizes, two bending speeds and two different directions. The performance evaluation results show that WiBend can distinguish between 12 bending gestures with a precision of 84% on average.
SP  - 339
EP  - 348
JF  - 2019 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340555.3353746
ER  - 

TY  - JOUR
AU  - Haldeman, Georgiana; Babes-Vroman, Monica; Tjang, Andrew; Nguyen, Thu D.
TI  - CSF: Formative Feedback in Autograding
PY  - 2021
AB  - Autograding systems are being increasingly deployed to meet the challenges of teaching programming at scale. Studies show that formative feedback can greatly help novices learn programming. This work extends an autograder, enabling it to provide formative feedback on programming assignment submissions. Our methodology starts with the design of a knowledge map, which is the set of concepts and skills that are necessary to complete an assignment, followed by the design of the assignment and that of a comprehensive test suite for identifying logical errors in the submitted code. Test cases are used to test the student submissions and learn classes of common errors. For each assignment, we train a classifier that automatically categorizes errors in a submission based on the outcome of the test suite. The instructor maps the errors to corresponding concepts and skills and writes hints to help students find their misconceptions and mistakes. We apply this methodology to two assignments in our Introduction to Computer Science course and find that the automatic error categorization has a 90% average accuracy. We report and compare data from two semesters, one semester when hints are given for the two assignments and one when hints are not given. Results show that the percentage of students who successfully complete the assignments after an initial erroneous submission is three times greater when hints are given compared to when hints are not given. However, on average, even when hints are provided, almost half of the students fail to correct their code so that it passes all the test cases. The initial implementation of the framework focuses on the functional correctness of the programs as reflected by the outcome of the test cases. In our future work, we will explore other kinds of feedback and approaches to automatically generate feedback to better serve the educational needs of the students.
SP  - 1
EP  - 30
JF  - ACM Transactions on Computing Education
VL  - 21
IS  - 3
PB  - 
DO  - 10.1145/3445983
ER  - 

TY  - JOUR
AU  - Williamson, John; Quek, M.; Popescu, Iulia; Ramsay, Andrew; Murray-Smith, Roderick
TI  - Efficient human-machine control with asymmetric marginal reliability input devices
PY  - 2020
AB  - Input devices such as motor-imagery brain-computer interfaces (BCIs) are often unreliable. In theory, channel coding can be used in the human-machine loop to robustly encapsulate intention through noisy input devices but standard feedforward error correction codes cannot be practically applied. We present a practical and general probabilistic user interface for binary input devices with very high noise levels. Our approach allows any level of robustness to be achieved, regardless of noise level, where reliable feedback such as a visual display is available. In particular, we show efficient zooming interfaces based on feedback channel codes for two-class binary problems with noise levels characteristic of modalities such as motor-imagery based BCI, with accuracy <75%. We outline general principles based on separating channel, line and source coding in human-machine loop design. We develop a novel selection mechanism which can achieve arbitrarily reliable selection with a noisy two-state button. We show automatic online adaptation to changing channel statistics, and operation without precise calibration of error rates. A range of visualisations are used to construct user interfaces which implicitly code for these channels in a way that it is transparent to users. We validate our approach with a set of Monte Carlo simulations, and empirical results from a human-in-the-loop experiment showing the approach operates effectively at 50-70% of the theoretical optimum across a range of channel conditions.
SP  - e0233603
EP  - NA
JF  - PloS one
VL  - 15
IS  - 6
PB  - 
DO  - 10.1371/journal.pone.0233603
ER  - 

TY  - NA
AU  - Zhang, Yufei; Liu, Bin; Liu, Zhiqiang; Huang, Jinyang; Sun, Rui
TI  - BSN - WristPress: Hand Gesture Classification with two-array Wrist-Mounted pressure sensors
PY  - 2019
AB  - This paper presents a hand gesture recognition system WristPress based on only the pressure sensors, which can reflect the different pressure changes of different hand gestures. Two arrays of force sensitive resistors (FSRs) are arranged around the wrist to capture the pressure fluctuation with the subtle muscle and tendon movements of different gestures, which can help to identify similar gestures for achieving more functions. For distinguishing more gestures with similar muscle and tendon movements, the temporal features and the spatial features of pressures are selected and designed to characterize the relation of every tiny pressure changes corresponding to the muscle and tendon movements at different positions around the wrist. In the WristPress system, 24 kinds of one-gestures, which cover not only the finger movements but also rotations around the wrist and forearm, are classified with an overall 10-fold cross validation classification accuracy of 97.40%. In addition, the WristPress prototype is non-obtrusive with a small size, and is well suited to existing wearable device forms, such as smart watches and a bracelet that are already mounted on the wrist. Our study shows that the temporal features and the spatial features of these pressures can reflect the the correlation between different pressure sensors can improve the accuracy of the hand gesture classification, and the kNN classifier has the best classification accuracy performance 97.40% with a low time complexity.
SP  - 1
EP  - 4
JF  - 2019 IEEE 16th International Conference on Wearable and Implantable Body Sensor Networks (BSN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bsn.2019.8771036
ER  - 

TY  - CHAP
AU  - Yen, Chi-Hsien; Lee, Yi-Chieh; Fu, Wai-Tat
TI  - SBP-BRiMS - Improving the efficiency of allocating crowd donations with agent-based simulation model
PY  - 2017
AB  - Crowdfunding platforms are emerging as an important online social platform to raise capital and awareness for innovative projects. When considered as a general online social system, the goal of a crowdfunding platform is to efficiently allocate a large number of small funds to promising new projects. However, the efficiency of donation allocation and the success rate of projects can be influenced by the behavior of donors, such as how they evaluate each project and choose the projects to donate. To understand how such behavior could impact crowdfunding market, we developed an agent-based model of crowdfunding to investigate three factors, i.e., project visibility, noise of perceived project quality, and donor strategies. These factors may impact the efficiency of a crowdfunding platform.
SP  - 248
EP  - 253
JF  - Social, Cultural, and Behavioral Modeling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-60240-0_30
ER  - 

TY  - NA
AU  - Davis, Josh Urban; Gong, Jun; Sun, Yunxin; Chilana, Parmit K.; Yang, Xing-Dong
TI  - UIST - CircuitStyle: A System for Peripherally Reinforcing Best Practices in Hardware Computing
PY  - 2019
AB  - Instructors of hardware computing face many challenges including maintaining awareness of student progress, allocating their time adequately between lecturing and helping individual students, and keeping students engaged even while debugging problems. Based on formative interviews with 5 electronics instructors, we found that many circuit style behaviors could help novice users prevent or efficiently debug common problems. Drawing inspiration from the software engineering practice of coding style, these circuit style behaviors consist of best-practices and guidelines for implementing circuit prototypes that do not interfere with the functionality of the circuit, but help a circuit be more readable, less error-prone, and easier to debug. To examine if these circuit style behaviors could be peripherally enforced, aid an in-person instructor's ability to facilitate a workshop, and not monopolize instructor's attention, we developed CircuitStyle, a teaching aid for in-person hardware computing workshops. To evaluate the effectiveness of our tool, we deployed our system in an in-person maker-space workshop. The instructor appreciated CircuitStyle's ability to provide a broad understanding of the workshop's progress and the potential for our system to help instructors of various backgrounds better engage and understand the needs of their classroom.
SP  - 109
EP  - 120
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347920
ER  - 

TY  - JOUR
AU  - Knibbe, Jarrod; Strohmeier, Paul; Boring, Sebastian; Hornbæk, Kasper
TI  - Automatic Calibration of High Density Electric Muscle Stimulation
PY  - 2017
AB  - Electric muscle stimulation (EMS) can enable mobile force feedback, support pedestrian navigation, or confer object affordances. To date, however, EMS is limited by two interlinked problems. (1) EMS is low resolution -- achieving only coarse movements and constraining opportunities for exploration. (2) EMS requires time consuming, expert calibration -- confining these interaction techniques to the lab. EMS arrays have been shown to increase stimulation resolution, but as calibration complexity increases exponentially as more electrodes are used, we require heuristics or automated procedures for successful calibration. We explore the feasibility of using electromyography (EMG) to auto-calibrate high density EMS arrays. We determine regions of muscle activity during human-performed gestures, to inform stimulation patterns for EMS-performed gestures. We report on a study which shows that auto-calibration of a 60-electrode array is feasible: achieving 52% accuracy across six gestures, with 82% accuracy across our best three gestures. By highlighting the electrode-array calibration problem, and presenting a first exploration of a potential solution, this work lays the foundations for high resolution, wearable and, perhaps one day, ubiquitous EMS beyond the lab.
SP  - 68
EP  - 17
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130933
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Li, Pengyu; Nith, Romain; Fonseca, Joshua; Lopes, Pedro
TI  - CHI - Touch&Fold: A Foldable Haptic Actuator for Rendering Touch in Mixed Reality
PY  - 2021
AB  - We propose a nail-mounted foldable haptic device that provides tactile feedback to mixed reality (MR) environments by pressing against the user's fingerpad when a user touches a virtual object. What is novel in our device is that it quickly tucks away when the user interacts with real-world objects. Its design allows it to fold back on top of the user's nail when not in use, keeping the user's fingerpad free to, for instance, manipulate handheld tools and other objects while in MR. To achieve this, we engineered a wireless and self-contained haptic device, which measures 24×24×41 mm and weighs 9.5 g. Furthermore, our foldable end-effector also features a linear resonant actuator, allowing it to render not only touch contacts (i.e., pressure) but also textures (i.e., vibrations). We demonstrate how our device renders contacts with MR surfaces, buttons, low- and high-frequency textures. In our first user study, we found that participants perceived our device to be more realistic than a previous haptic device that also leaves the fingerpad free (i.e., fingernail vibration). In our second user study, we investigated the participants’ experience while using our device in a real-world task that involved physical objects. We found that our device allowed participants to use the same finger to manipulate handheld tools, small objects, and even feel textures and liquids, without much hindrance to their dexterity, while feeling haptic feedback when touching MR interfaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445099
ER  - 

TY  - NA
AU  - Beruscha, Frank; Krautter, Wolfgang; So, Yong Heng; Jeon, Jin-Han
TI  - UbiComp/ISWC Adjunct - It's a PHAct: Printed Haptic Actuators for Augmented Objects and Surfaces
PY  - 2018
AB  - In this work, we discuss the application of printed haptic actuators based on Electroactive Polymer (EAP) for HCI. We envision a printed haptic layer that can be used to augment objects and surfaces with feedback. The printing process offers unique opportunities to fabricate actuators of various shapes, sizes and layouts. We show that printed actuators provide strong output that is clearly perceived even under challenging conditions. We discuss the current possibilities and limitations, and provide HCI example cases that show the benefits of printed haptic actuators.
SP  - 9
EP  - 12
JF  - Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267305.3267617
ER  - 

TY  - BOOK
AU  - Tamura, Yuto; Takemura, Kentaro
TI  - ETRA Short Papers - Estimating Point-of-Gaze using Smooth Pursuit Eye Movements without Implicit and Explicit User-Calibration
PY  - 2020
AB  - Detecting the point-of-gaze in the real world is a challenging problem in eye-tracking applications. The point-of-gaze is estimated using geometry constraints, and user-calibration is required. In addition, the distances of the focused targets are variable and large in the real world. Therefore, a calibration-free approach without geometry constraints is needed to estimate the point-of-gaze. Recent studies have investigated smooth pursuit eye movements (smooth pursuits) for human-computer interaction applications, and we consider that these smooth pursuits can also be employed in eye tracking. Therefore, we developed a method for estimating the point-of-gaze using smooth pursuits without any requirement for implicit and explicit user-calibration. In this method, interest points are extracted from the scene image, and the point-of-gaze is detected using these points, which are strongly correlated with eye movements. We performed a comparative experiment in a real environment and demonstrated the feasibility of the proposed method.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379156.3391343
ER  - 

TY  - NA
AU  - Li, Zhen; Fan, Mingming; Han, Ying; Truong, Khai N.
TI  - HUMA @ ACM Multimedia - iWink: Exploring Eyelid Gestures on Mobile Devices
PY  - 2020
AB  - Although gaze has been widely studied for mobile interactions, eyelid-based gestures are relatively understudied and limited to few basic gestures (e.g., blink). In this work, we propose a gesture grammar to construct both basic and compound eyelid gestures. We present an algorithm to detect nine eyelid gestures in real-time on mobile devices and evaluate its performance with 12 participants. Results show that our algorithm is able to recognize nine eyelid gestures with 83% and 78% average accuracy using user-dependent and user-independent models respectively. Further, we design a gesture mapping scheme to allow for navigating between and within mobile apps only using eyelid gestures. Moreover, we show how eyelid gestures can be used to enable cross-application and sensitive interactions. Finally, we highlight future research directions.
SP  - 83
EP  - 89
JF  - Proceedings of the 1st International Workshop on Human-centric Multimedia Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3422852.3423479
ER  - 

TY  - NA
AU  - Takada, Akari; Hirata, Akira; Katsura, Seiichiro
TI  - BioRob - Effect of Pulse-Width Modulation Resolution on Angle Control of Wrist Joint Using Functional Electrical Stimulation
PY  - 2020
AB  - Research on joint angle control has mainly been focused on the control method of functional electrical stimulation (FES). In such research, the modulation resolution of the control input, which is the parameter that defines the stimulation intensity, is rarely discussed. This research is based on the assumption that the control input resolution effects the tracking performance of joint angle control using FES. In order to confirm this assumption, a stimulator that can modulate the pulse width of a biphasic current pulse at a high resolution of 20 ns is introduced. The experiments discuss the effect the pulse width has on the torque output and the effect the pulse-width modulation (PWM) resolution has on angle control of the wrist joint.
SP  - 13
EP  - 18
JF  - 2020 8th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/biorob49111.2020.9224308
ER  - 

TY  - JOUR
AU  - Singh, Avinash Kumar; Chen, Hsiang-Ting; Cheng, Yu-Feng; King, Jung-Tai; Ko, Li-Wei; Gramann, Klaus; Lin, Chin-Teng
TI  - Visual Appearance Modulates Prediction Error in Virtual Reality
PY  - 2018
AB  - Different rendering styles induce different levels of agency and user behaviors in virtual reality environments. We applied an electroencephalogram-based approach to investigate how the rendering style of the users’ hands affects behavioral and cognitive responses. To this end, we introduced prediction errors due to cognitive conflicts during a 3-D object selection task by manipulating the selection distance of the target object. The results showed that, for participants with high behavioral inhibition scores, the amplitude of the negative event-related potential at approximately 50–250 ms correlated with the realism of the virtual hands. Concurring with the uncanny valley theory, these findings suggest that the more realistic the representation of the user’s hand is, the more sensitive the user becomes toward subtle errors, such as tracking inaccuracies.
SP  - 24617
EP  - 24624
JF  - IEEE Access
VL  - 6
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2832089
ER  - 

TY  - NA
AU  - Ye, Yuan-Syun; Chen, Hsin-Yu; Chan, Liwei
TI  - UIST - Pull-Ups: Enhancing Suspension Activities in Virtual Reality with Body-Scale Kinesthetic Force Feedback
PY  - 2019
AB  - We present Pull-Ups, a suspension kit that can suggest a range of body postures and thus enables various exercise styles of users perceiving the kinesthetic force feedback by suspending their weight with arm exertion during the interaction. Pull-Ups actuates the user's body to move up to 15 cm by pulling his or her hands using a pair of pneumatic artificial muscle groups. Our studies informed the discernible kinesthetic force feedbacks that were then exploited for the design of kinesthetic force feedback in three physical activities: kitesurfing, paragliding, and space invader. Our final study on user experiences suggested that a passive suspension kit alone added substantially to users' perceptions of realism and enjoyment (all above neutral) with passive physical support, while sufficient active feedback can further level them up. In addition, we found that both passive and active feedback of the suspension kit significantly reduced motion sickness in simulated kitesurfing and paragliding compared to when no suspension kit (thus no feedback) was provided. This work suggests that a passive suspension kit is cost-effective as a home exercise kit, while active feedback can further level up user experience, though at the cost of the installation (e.g., an air compressor in our prototype).
SP  - 791
EP  - 801
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347874
ER  - 

TY  - NA
AU  - Lopes, Pedro; Baudisch, Patrick
TI  - UIST (Adjunct Volume) - Demonstrating Interactive Systems based on Electrical Muscle Stimulation
PY  - 2017
AB  - We provide a hands-on demonstration of the potential of interactive systems based on electrical muscle stimulation (EMS). These wearable devices allow attendees, for example, to physically learn how to manipulate objects they never seen before, feel walls and forces in virtual reality, and so forth. In our demo we plan to not only demonstrate several of these EMS-based prototypes but also to provide instructions and free hardware for people to conduct their first projects using EMS.
SP  - 47
EP  - 49
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131806
ER  - 

TY  - CHAP
AU  - Israr, Ali; Zhao, Siyan; Schwemler, Zachary; Fritz, Adam
TI  - HCI (LBP) - Stereohaptics Toolkit for Dynamic Tactile Experiences
PY  - 2019
AB  - We introduce a toolkit for creating, editing, storing, sharing and broadcasting dynamic haptic media through current computing devices. Adapted from an audio-based infrastructure, our Stereohaptics toolkit uses off-the-shelf hardware components and available software tools to create various haptic interfaces and applications. Core of our toolkit is the Stereohaptics engine that processes inputs from embedded sensors and renders high definition haptic feedback on discrete actuator grids. We exploit sensory illusions in touch to generate the perception of objects and their motion on, around and through the user’s body and present psychophysical evidence of static and moving tactile illusions between two discretely spaced haptic actuators. The toolkit supports users to craft a variety of haptic effects on and across the body in a variety of grid configurations. We present design of the toolkit and its use in various activities and workshops. Our framework is easy-to-use and simple-to-adopt and can be applied to everyday handheld, wearable, augmented reality, virtual reality and extended reality applications.
SP  - 217
EP  - 232
JF  - HCI International 2019 – Late Breaking Papers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-30033-3_17
ER  - 

TY  - NA
AU  - Park, Kyudong; Kim, Hyeon; Han, Sung H.
TI  - Usability of the Size, Spacing, and Depth of Virtual Buttons on Head-Mounted Displays
PY  - 2018
AB  - Virtual reality (VR) allows users to see and manipulate virtual scenes and items through input devices, like head-mounted displays. In this study, the effects of button size, spacing, and depth on the usability of virtual buttons in VR environments were investigated. Task completion time, number of errors, and subjective preferences were collected to test different levels of the button size, spacing, and depth. The experiment was conducted in a desktop setting with Oculus Rift and Leap motion. A total of 18 subjects performed a button selection task. The optimal levels of button size and spacing within the experimental conditions are 25 mm and between 5 mm and 9 mm, respectively. Button sizes of 15 mm with 1-mm spacing were too small to be used in VR environments. A trend of decreasing task completion time and the number of errors was observed as button size and spacing increased. However, large size and spacing may cause fatigue, due to continuous extension of the arms. For depth effects, the touch method took a shorter task completion time. However, the push method recorded a smaller number of errors, owing to the visual push-feedback. In this paper, we discuss advantages and disadvantages in detail. The results can be applied to many different application areas with VR HMD.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Schipor, Ovidiu-Andrei; Vatavu, Radu-Daniel
TI  - GearWheels: A Software Tool to Support User Experiments on Gesture Input with Wearable Devices
PY  - 2022
AB  - NA
SP  - 1
EP  - 19
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2098907
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Nakayama, Ryosuke; Liu, Dan; Kakehi, Yasuaki; Gross, Mark D.; Leithinger, Daniel
TI  - UIST (Adjunct Volume) - LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators
PY  - 2019
AB  - This paper introduces LiftTiles, a modular and reconfigurable room-scale shape display. LiftTiles consist of an array of retractable and inflatable actuator that is compact (e.g., 15cm tall) and light (e.g., 1.8kg), while extending up to 1.5m to allow for large-scale shape transformation. Inflatable actuation also provides a robust structure that can support heavy objects (e.g., 10 kg weight). This paper describes the design and implementation of LiftTiles and explores the application space for reconfigurable room-scale shape displays.
SP  - 30
EP  - 32
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357105
ER  - 

TY  - NA
AU  - Pohl, Henning; Hornbæk, Kasper; Knibbe, Jarrod
TI  - AH - Wanding Through Space: Interactive Calibration for Electric Muscle Stimulation
PY  - 2018
AB  - Electric Muscle Stimulation (EMS) has emerged as an interaction paradigm for HCI. It has been used to confer object affordance, provide walking directions, and assist with sketching. However, the electrical signals used for EMS are multi-dimensional and require expert calibration before use. To date, this calibration has occurred as a collaboration between the experimenter, or interaction designer, and the user/participant. However, this is time-consuming, results in sampling only a limited space of possible signal configurations, and removes control from the participant. We present a calibration and signal exploration technique that both enables the user to control their own stimulation and thus comfort, and supports exploration of the continuous space of stimulation signals.
SP  - 19
EP  - NA
JF  - Proceedings of the 9th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3174910.3174948
ER  - 

TY  - NA
AU  - Fortin, Pascal E.; Sulmont, Elisabeth; Cooperstock, Jeremy R.
TI  - CHI - Detecting Perception of Smartphone Notifications Using Skin Conductance Responses
PY  - 2019
AB  - Today's smartphone notification systems are incapable of determining whether a notification has been successfully perceived without explicit interaction from the user. If the system incorrectly assumes that a notification has not been perceived, it may repeat it redundantly, disrupting the user and others (e.g., phone ringing). Or, if it incorrectly assumes that a notification was perceived, and therefore fails to repeat it, the notification will be missed altogether (e.g., text message). Results from a laboratory study confirm, for the first time, that both vibrotactile and auditory smartphone notifications induce skin conductance responses (SCR), that the induced responses differ from that of arbitrary stimuli, and that they could be employed to predict perception of smartphone notifications after their presentation using wearable sensors.
SP  - 190
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300420
ER  - 

TY  - NA
AU  - Yang, Humphrey; Yan, Zeyu; Luo, Danli; Yao, Lining
TI  - FoamFactor: Hydrogel-Foam Composite with Tunable Stiffness and Compressibility.
PY  - 2021
AB  - This paper presents FoamFactor, a novel material with tunable stiffness and compressibility between hydration states, and a tailored pipeline to design and fabricate artifacts consisting of it. This technique compounds hydrogel with open-cell foams via additive manufacturing to produce a water-responsive composite material. Enabled by the large volumetric changes of hydrogel dispersions, the material is soft and compressible when dehydrated and becomes stiffer and rather incompressible when hydrated. Leveraging this material property transition, we explore its design space in various aspects pertaining to the transition of hydration states, including multi-functional shoes, amphibious cars, mechanical transmission systems, and self-deploying robotic grippers.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Leong, Joanne; Perteneder, Florian; Jetter, Hans-Christian; Haller, Michael J.
TI  - Tangible and Embedded Interaction - What a Life!: Building a Framework for Constructive Assemblies
PY  - 2017
AB  - Constructive assemblies are tangible user interfaces (TUIs) that involve the interconnection of modular parts. As such, they offer a unique means to support us in our various and diverse activities. However, little information is available for understanding the intricacies of taking a modular, constructive assembly approach to TUI design. Based on an analysis of extensive data collected from interviews with eight world-class TUI experts, we propose a descriptive, conceptual framework to facilitate systematic investigation and critical consideration of constructive assemblies. The paper presents a lifecycle model for constructive assemblies and discusses their main design qualities and associated parameters. We demonstrate how this can be used to structure critical discussions by applying the principles to existing works and the design of our own constructive assembly.
SP  - 57
EP  - 66
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3024985
ER  - 

TY  - NA
AU  - Zhang, Cheng; Xiaoxuan, Wang; Waghmare, Anandghan; Jain, Sumeet; Ploetz, Thomas; Inan, Omer T.; Starner, Thad; Abowd, Gregory D.
TI  - ISWC - FingOrbits: interaction with wearables using synchronized thumb movements
PY  - 2017
AB  - We present FingOrbits, a wearable interaction technique using synchronized thumb movements. A thumb-mounted ring with an inertial measurement unit and a contact microphone are used to capture thumb movements when rubbing against the other fingers. Spectral information of the movements are extracted and fed into a classification backend that facilitates gesture discrimination. FingOrbits enables up to 12 different gestures through detecting three rates of movement against each of the four fingers. Through a user study with 10 participants (7 novices, 3 experts), we demonstrate that FingOrbits can distinguish up to 12 thumb gestures with an accuracy of 89% to 99% rendering the approach applicable for practical applications.
SP  - 62
EP  - 65
JF  - Proceedings of the 2017 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123021.3123041
ER  - 

TY  - NA
AU  - Wang, Chi; Huang, Da-Yuan; Hsu, Shuo-wen; Lin, Cheng-Lung; Chiu, Yeu-Luen; Hou, Chu-En; Chen, Bing-Yu
TI  - CHI - Gaiters: Exploring Skin Stretch Feedback on Legs for Enhancing Virtual Reality Experiences
PY  - 2020
AB  - We propose generating two-dimensional skin stretch feedback on the user's legs. Skin stretch is useful cutaneous feedback to induce the perception of virtual textures and illusory forces and to deliver directional cues. This feedback has been applied to the head, body, and upper limbs to simulate rich physical properties in virtual reality (VR). However, how to expand the benefit of skin stretch feedback and apply it to the lower limbs, remains to be explored. Our first two psychophysical studies examined the minimum changes in skin stretch distance and stretch angle that are perceivable by participants. We then designed and implemented Gaiters, a pair of ungrounded, leg-worn devices, each of which is able to generate multiple two-dimensional skin stretches on the skin of the user's leg. With Gaiters, we conducted an exploratory study to understand participants' experiences when coupling skin stretch patterns with various lower limb actions. The results indicate that rich haptic experiences can be created by our prototype. Finally, a user evaluation indicates that participants enjoyed the experiences when using Gaiters and considered skin stretch as compelling haptic feedback on the legs.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376865
ER  - 

TY  - JOUR
AU  - Khalghollah, Mahmood; Nayyeri, Pooyan; Najafi, Farshid
TI  - A novel method to generate the geometry of a surface actuator
PY  - 2020
AB  - This paper presents the design, evaluation, and implementation of a continuous surface actuator. A surface actuator as a shape-changing interface can facilitate the modeling process in interactive product design approaches by rendering products in 3D space for better understanding and visualization. Controlling the deformation of a continuous tactile surface using a pneumatic air pressure mechanism holds promise as a tangible haptic interface. Previous haptic actuators could only provide physical point contact with the user’s hand. In this article, a three-dimensional (3D) surface is produced to simulate the shape of a desired continuous object. To investigate the mechanical behavior of this deformable surface, the hyperelastic Yeoh model is employed to numerically simulate the experimental system behavior. The parameters of the hyperelastic material have been characterized by a tensile test. An experimental setup is developed to produce an arbitrary spherical shape containing specific deformations. For validation purposes, the surface shape has been scanned and compared with the finite element simulation results.
SP  - 211
EP  - 223
JF  - International Journal on Interactive Design and Manufacturing (IJIDeM)
VL  - 14
IS  - 1
PB  - 
DO  - 10.1007/s12008-020-00656-x
ER  - 

TY  - JOUR
AU  - Esteves, Augusto; Shin, Yonghwan; Oakley, Ian
TI  - Comparing selection mechanisms for gaze input techniques in head-mounted displays
PY  - 2020
AB  - NA
SP  - 102414
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 139
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2020.102414
ER  - 

TY  - NA
AU  - Striner, Alina
TI  - Can Multisensory Cues in VR Help Train Pattern Recognition to Citizen Scientists
PY  - 2018
AB  - As the internet of things (IoT) has integrated physical and digital technologies, designing for multiple sensory media (mulsemedia) has become more attainable. Designing technology for multiple senses has the capacity to improve virtual realism, extend our ability to process information, and more easily transfer knowledge between physical and digital environments. HCI researchers are beginning to explore the viability of integrating multimedia into virtual experiences, however research has yet to consider whether mulsemedia truly enhances realism, immersion and knowledge transfer. My work developing StreamBED, a VR training platform to train citizen science water monitors plans to consider the role of mulsemedia in immersion and learning goals. Future findings about the role of mulsemedia in learning contexts will potentially allow learners to experience, connect to, learn from spaces that are impossible to experience firsthand.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Saraiji, Mhd Yamen; Sasaki, Tomoya; Kunze, Kai; Minamizawa, Kouta; Inami, Masahiko
TI  - UIST (Adjunct Volume) - MetaArms: Body Remapping Using Feet-Controlled Artificial Arms
PY  - 2018
AB  - We introduce MetaArms, wearable anthropomorphic robotic arms and hands with six degrees of freedom operated by the user's legs and feet. Our overall research goal is to re-imagine what our bodies can do with the aid of wearable robotics using a body-remapping approach. To this end, we present an initial exploratory case study. MetaArms' two robotic arms are controlled by the user's feet motion, and the robotic hands can grip objects according to the user's toes bending. Haptic feedback is also presented on the user's feet that correlate with the touched objects on the robotic hands, creating a closed-loop system. We present formal and informal evaluations of the system, the former using a 2D pointing task according to Fitts' Law. The overall throughput for 12 users of the system is reported as 1.01 bits/s (std 0.39). We also present informal feedback from over 230 users. We find that MetaArms demonstrate the feasibility of body-remapping approach in designing robotic limbs that may help us re-imagine what the human body could do.
SP  - 140
EP  - 142
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242665
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Rekimoto, Jun; Chen, Bing-Yu
TI  - CHI - ElasticVR: Providing Multilevel Continuously-Changing Resistive Force and Instant Impact Using Elasticity for VR
PY  - 2019
AB  - Resistive force (e.g., due to object elasticity) and impact (e.g., due to recoil) are common effects in our daily life. However, resistive force continuously changes due to users' movements while impact instantly occurs when an event triggers it. These feedback are still not realistically provided by current VR haptic methods. In this paper, a wearable device, ElasticVR, which consists of an elastic band, servo motors and mechanical brakes, is proposed to provide the continuously-changing resistive force and instantly-occurring impact upon the user's hand to enhance VR realism. By changing two physical properties, length and extension distance, of the elastic band, ElasticVR provides multilevel resistive force with no delay and impact with little delay, respectively, for realistic and versatile VR applications. A force perception study was performed to observe users' force distinguishability of the resistive force and impact, and the prototype was built based on its results. A VR experience study further proves that the resistive force and impact from ElasticVR both outperform those from current approaches in realism. Applications using ElasticVR are also demonstrated.
SP  - 220
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300450
ER  - 

TY  - NA
AU  - Wang, Guanyun; Cheng, Tingyu; Do, Youngwook; Yang, Humphrey; Tao, Ye; Gu, Jianzhe; An, Byoungkwon; Yao, Lining
TI  - CHI Extended Abstracts - Demonstrating Printed Paper Actuator: A Low-cost Reversible Actuation and Sensing Method for Shape Changing Interfaces
PY  - 2018
AB  - We present a printed paper actuator as a low cost, reversible and electrical actuation and sensing method. This is a novel but easily accessible enabling technology that expands upon the library of actuation-sensing materials in HCI. By integrating three physical phenomena, including the bilayer bending actuation, the shape memory effect of the thermoplastic and the current-driven joule heating via conductive printing filament, we developed the actuator by simply printing a single layer conductive Polylactide (PLA) on a piece of copy paper via a desktop fused deposition modeling (FDM) 3D printer. This paper describes the fabrication process, the material mechanism, and the transformation primitives, followed by the electronic sensing and control methods. A software tool that assists the design, simulation and printing toolpath generation is introduced. Finally, we explored applications under four contexts: robotics, interactive art, entertainment and home environment.
SP  - 569
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186531
ER  - 

TY  - JOUR
AU  - Steil, Julian; Tonsen, Marc; Sugano, Yusuke; Bulling, Andreas
TI  - InvisibleEye: Fully Embedded Mobile Eye Tracking Using Appearance-Based Gaze Estimation
PY  - 2019
AB  - Despite their potential for a range of exciting new applications, mobile eye trackers suffer from several fundamental usability problems. InvisibleEye is an innovative approach for mobile eye tracking that uses millimeter-size RGB cameras, which can be fully embedded into normal glasses frames, as well as appearancebased gaze estimation to directly estimate gaze from the eye images. Through evaluation on three large-scale, increasingly realistic datasets, we show that InvisibleEye can achieve a person-specific gaze estimation accuracy of up to 2.04° using three camera pairs with a resolution of only 3x3 pixels.
SP  - 30
EP  - 34
JF  - GetMobile: Mobile Computing and Communications
VL  - 23
IS  - 2
PB  - 
DO  - 10.1145/3372300.3372307
ER  - 

TY  - JOUR
AU  - Wang, Guanyun; Qin, Fang; Liu, Haolin; Tao, Ye; Zhang, Yang; Zhang, Yongjie Jessica; Yao, Lining
TI  - MorphingCircuit: An Integrated Design, Simulation, and Fabrication Workflow for Self-morphing Electronics
PY  - 2020
AB  - Manufacturing nonplanar electronics often requires the integration of functions and forms through embedding circuit boards into three-dimensional (3D) shapes. While most popular solutions rely on cavities where electronics reside in forms of rigid circuit boards, other alternative approaches leverage 3D printing or layer lamination to create 3D electronics that often require expensive manufacturing processes and materials. Furthermore, many conventional methods are incompatible with complex geometries (e.g., surfaces that twist or have local minima). In response, we introduce MorphingCircuit, an integrated design, simulation, and fabrication workflow that combines electronic functions with forms through four-dimensional (4D) printing, which effectively reduces cost, production time, and e-waste. Specifically, we start by printing a flat substrate and assembling functional electronics on top of it. The flat structure will then self-morph into a preprogrammed 3D shape when triggered by external heating. Overall, our comprehensive 3D electronics fabrication pipeline encompasses the design, simulation, fabrication, and transformation, with which we hope to inspire designers, researchers, and makers to create conformal electronics on complex substrate geometries that were previously difficult or impossible to design or manufacture.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432232
ER  - 

TY  - NA
AU  - Gehrke, Lukas; Akman, Sezen; Lopes, Pedro; Chen, Albert; Singh, Avinash Kumar; Chen, Hsiang-Ting; Lin, Chin-Teng; Gramann, Klaus
TI  - CHI - Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials
PY  - 2019
AB  - Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions. Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials. We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.
SP  - 427
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300657
ER  - 

TY  - NA
AU  - Tseng, Wen-Jie; Wang, Li-Yang; Chan, Liwei
TI  - UIST - FaceWidgets: Exploring Tangible Interaction on Face with Head-Mounted Displays
PY  - 2019
AB  - We present FaceWidgets, a device integrated with the backside of a head-mounted display (HMD) that enables tangible interactions using physical controls. To allow for near range-to-eye interactions, our first study suggested displaying the virtual widgets at 20 cm from the eye positions, which is 9 cm from the HMD backside. We propose two novel interactions, widget canvas and palm-facing gesture, that can help users avoid double vision and allow them to access the interface as needed. Our second study showed that displaying a hand reference improved performance of face widgets interactions. We developed two applications of FaceWidgets, a fixed-layout 360 video player and a contextual input for smart home control. Finally, we compared four hand visualizations against the two applications in an exploratory study. Participants considered the transparent hand as the most suitable and responded positively to our system.
SP  - 417
EP  - 427
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347946
ER  - 

TY  - JOUR
AU  - Chen, Bo-Han; Wong, Sai-Keung; Chang, Wei-Che; Fan, Roy Ping-Hao
TI  - LAGH: Towards Asymmetrical Collaborative Bodily Play between 1st and 2nd Person Perspectives
PY  - 2022
AB  - <jats:p>This paper investigated to what extent social interactions and empathy of users could be induced when different control mechanisms were used in an asymmetric collaboration. We conducted a user study to explore the user experience under one decentralized and two centralized control conditions via using the proposed two-player asymmetric collaborative bodily play, LAGH, which supports perspective-taking through the integration with the first- and second-perspectives and shared objects. The two players have complementary views and controls to each other in an immersive environment. The results indicate that participant pairs were encouraged by the asymmetric collaboration interface to share their emotional and physiological perspectives with each other. When their control abilities were balanced, they were more motivated to perform information sharing and interact with each other, thereby enhancing closeness and stimulating empathy. Furthermore, users could improve the collaboration efficiency.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555548
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Alt, Florian; Hassib, Mariam; von Zezschwitz, Emanuel; Hasholzner, Regina; Bulling, Andreas
TI  - CHI Extended Abstracts - GazeTouchPass: Multimodal Authentication Using Gaze and Touch on Mobile Devices
PY  - 2016
AB  - We propose a multimodal scheme, GazeTouchPass, that combines gaze and touch for shoulder-surfing resistant user authentication on mobile devices. GazeTouchPass allows passwords with multiple switches between input modalities during authentication. This requires attackers to simultaneously observe the device screen and the user's eyes to find the password. We evaluate the security and usability of GazeTouchPass in two user studies. Our findings show that GazeTouchPass is usable and significantly more secure than single-modal authentication against basic and even advanced shoulder-surfing attacks.
SP  - 2156
EP  - 2164
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892314
ER  - 

TY  - NA
AU  - Gupta, Aakar; Yang, Jiushan; Balakrishnan, Ravin
TI  - UIST - Asterisk and Obelisk: Motion Codes for Passive Tagging
PY  - 2018
AB  - Machine readable passive tags for tagging physical objects are ubiquitous today. We propose Motion Codes, a passive tagging mechanism that is based on the kinesthetic motion of the user's hand. Here, the tag comprises of a visual pattern that is displayed on a physical surface. To scan the tag and receive the encoded information, the user simply traces their finger over the pattern. The user wears an inertial motion sensing (IMU) ring on the finger that records the traced pattern. We design two motion code schemes, Asterisk and Obelisk that rely on directional vector data processed from the IMU. We evaluate both schemes for the effects of orientation, size, and data density on their accuracies. We further conduct an in-depth analysis of the sources of motion deviations in the ring data as compared to the ground truth finger movement data. Overall, Asterisk achieves a 95% accuracy for an information capacity of 16.8 million possible sequences.
SP  - 725
EP  - 736
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242637
ER  - 

TY  - NA
AU  - Wang, Guanyun; Cheng, Tingyu; Do, Youngwook; Yang, Humphrey; Tao, Ye; Gu, Jianzhe; An, Byoungkwon; Yao, Lining
TI  - Printed Paper Actuator
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174143
ER  - 

TY  - JOUR
AU  - Song, Kahye; Kim, Sohyun; Cha, Youngsu
TI  - Soft electromagnetic actuator for assembly robots
PY  - 2020
AB  - Robots equipped with an assembly system have significant potential and value because they allow simple modules to be assembled in various forms. A new type of soft assembly robot was recently created by combining a soft robot and an assembly system. In this paper, we introduce an assemblable electromagnetic soft actuator made of silicone. Silicone is very soft and stretchable, and is made to react with electromagnets by mixing with iron oxide. The motion is controlled by the force applied to the electromagnets attached to the floor. A fast and accurate response is displayed according to the input amplitude and frequency. In addition, the range of motion is significant. When the power applied by a single actuator is 9V 1A, the motion is approximately 2.62mm, which is 93.6 % of the entire actuator height, the maximum value that can be moved structurally. Above all, the actuator can be connected to the y- and z-axes without additional line connections to create the desired assembly structure. An additional wire connection is required for the x-axis connection. At the moment a connection is made, the actuators move in accordance with the input signal and all movements are conducted under little voltages.
SP  - 067001
EP  - NA
JF  - Smart Materials and Structures
VL  - 29
IS  - 6
PB  - 
DO  - 10.1088/1361-665x/ab84b8
ER  - 

TY  - NA
AU  - Fortin, Pascal E.; Blum, Jeffrey R.; Cooperstock, Jeremy R.
TI  - UIST (Adjunct Volume) - Raising the Heat: Electrical Muscle Stimulation for Simulated Heat Withdrawal Response
PY  - 2017
AB  - Virtual Reality (VR) has numerous mechanisms for making a virtual scene more compellingly real. Most effort has been focused on visual and auditory techniques for immersive environments, although some commercial systems now include relatively crude haptic effects through handheld controllers or haptic suits. We present results from a pilot experiment demonstrating the use of Electrical Muscle Stimulation (EMS) to trick participants into thinking a surface is dangerously hot even though it is below 50C. This is accomplished by inducing an artificial heat withdrawal reflex by contracting the participant's bicep shortly after contact with the virtual hot surface. Although the effects of multiple experimental confounds need to be quantified in future work, results so far suggest that EMS could potentially be used to modify temperature perception in VR and AR contexts. Such an illusion has applications for VR gaming as well as emergency response and workplace training and simulation, in addition to providing new insights into the human perceptual system.
SP  - 137
EP  - 139
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131828
ER  - 

TY  - JOUR
AU  - Wolf, Dennis; Rietzler, Michael; Hnatek, Leo; Rukzio, Enrico
TI  - Face/On: Multi-Modal Haptic Feedback for Head-Mounted Displays in Virtual Reality
PY  - 2019
AB  - While the real world provides humans with a huge variety of sensory stimuli, virtual worlds most of all communicate their properties by visual and auditory feedback due to the design of current head mounted displays (HMDs). Since HMDs offer sufficient contact area to integrate additional actuators, prior works utilised a limited amount of haptic actuators to integrate respective information about the virtual world. With the Face/On prototype complex feedback patterns are introduced that combine a high number of vibration motors with additional thermal sources to transport multi-modal and spatial information. A pre-study determining the boundaries of the feedbacks' intensities as well as a user study showing a significant increase of presence and enjoyment validate Face/On's approach.
SP  - 3169
EP  - 3177
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2019.2932215
ER  - 

TY  - JOUR
AU  - Sachyani, Ela; Layani, Michael; Tibi, Gal; Avidan, Tal; Degani, Amir; Magdassi, Shlomo
TI  - Enhanced movement of CNT-based actuators by a three-Layered structure with controlled resistivity
PY  - 2017
AB  - NA
SP  - 1071
EP  - 1077
JF  - Sensors and Actuators B: Chemical
VL  - 252
IS  - NA
PB  - 
DO  - 10.1016/j.snb.2017.06.104
ER  - 

TY  - CHAP
AU  - Kim, Taeyong; Blum, Jeffrey R.; Alirezaee, Parisa; Arnold, Andre G.; Fortin, Pascal E.; Cooperstock, Jeremy R.
TI  - Usability of Foot-Based Interaction Techniques for Mobile Solutions
PY  - 2018
AB  - Although hand-based interaction dominates mobile applications, this can be unsuitable for use by motor-impaired individuals or in situations such as musical performance or surgery, where the hands are otherwise occupied. The alternative of foot-based interaction, the subject of this chapter, has been shown to offer reasonable performance in such conditions and offers benefits in terms of diversity of input techniques, wide applicability, and social acceptability. This chapter also describes potential applications of foot-based interfaces, with an emphasis on factors related to usability. We aim to inspire designers and developers to consider the potential for leveraging interaction through the feet as a replacement for, or complement to, more traditional application designs.
SP  - 309
EP  - 329
JF  - Mobile Solutions and Their Usefulness in Everyday Life
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-93491-4_16
ER  - 

TY  - BOOK
AU  - Wei, Mian; Singh, Karan
TI  - SCF - Bend-a-rule: a fabrication-based workflow for 3D planar contour acquisition
PY  - 2017
AB  - Bend-a-rule is a physical tool and workflow that enables the robust acquisition of planar contours of 3D shape. Our work exemplifies the design of physical artifacts that subsequently aid in digital design and fabrication. Bend-a-rule is a ruler, fabricated by laser-cutting a periodic pattern on a rigid board. The ruler has unidirectional flexibility, and readily bends to conform to the shape of curved planar contours on physical 3D objects. We present a novel workflow, by which this curved planar contour can be digitally acquired from a single image of the physical ruler. The acquired contour is then used to design laser-cut accessory shapes that attach to physcial 3D objects along the digitally acquired contour. We describe the construction of Bend-a-rule, propose an automatic algorithm for the extraction of a to-scale, planar 3D curve from a Bend-a-rule image, evaluate the resulting curves in comparison to ground truth data, and show example physical 3D objects augmented using Bend-a-rule.
SP  - 5
EP  - NA
JF  - Proceedings of the 1st Annual ACM Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3083157.3083164
ER  - 

TY  - JOUR
AU  - Jouybari, Atena Fadaei; Franza, Matteo; Kannape, Oliver Alan; Hara, Masayuki; Blanke, Olaf
TI  - Tactile spatial discrimination on the torso using vibrotactile and force stimulation.
PY  - 2021
AB  - There is a steadily growing number of mobile communication systems that provide spatially encoded tactile information to the humans’ torso. However, the increased use of such hands-off displays is currently not matched with or supported by systematic perceptual characterization of tactile spatial discrimination on the torso. Furthermore, there are currently no data testing spatial discrimination for dynamic force stimuli applied to the torso. In the present study, we measured tactile point localization (LOC) and tactile direction discrimination (DIR) on the thoracic spine using two unisex torso-worn tactile vests realized with arrays of 3 × 3 vibrotactile or force feedback actuators. We aimed to, first, evaluate and compare the spatial discrimination of vibrotactile and force stimulations on the thoracic spine and, second, to investigate the relationship between the LOC and DIR results across stimulations. Thirty-four healthy participants performed both tasks with both vests. Tactile accuracies for vibrotactile and force stimulations were 60.7% and 54.6% for the LOC task; 71.0% and 67.7% for the DIR task, respectively. Performance correlated positively with both stimulations, although accuracies were higher for the vibrotactile than for the force stimulation across tasks, arguably due to specific properties of vibrotactile stimulations. We observed comparable directional anisotropies in the LOC results for both stimulations; however, anisotropies in the DIR task were only observed with vibrotactile stimulations. We discuss our findings with respect to tactile perception research as well as their implications for the design of high-resolution torso-mounted tactile displays for spatial cueing.
SP  - 3175
EP  - 3188
JF  - Experimental brain research
VL  - 239
IS  - 11
PB  - 
DO  - 10.1007/s00221-021-06181-x
ER  - 

TY  - BOOK
AU  - Bace, Mihai; Becker, Vincent; Wang, Chenyang; Bulling, Andreas
TI  - ETRA - Combining Gaze Estimation and Optical Flow for Pursuits Interaction
PY  - 2020
AB  - Pursuit eye movements have become widely popular because they enable spontaneous eye-based interaction. However, existing methods to detect smooth pursuits require special-purpose eye trackers. We propose the first method to detect pursuits using a single off-the-shelf RGB camera in unconstrained remote settings. The key novelty of our method is that it combines appearance-based gaze estimation with optical flow in the eye region to jointly analyse eye movement dynamics in a single pipeline. We evaluate the performance and robustness of our method for different numbers of targets and trajectories in a 13-participant user study. We show that our method not only outperforms the current state of the art but also achieves competitive performance to a consumer eye tracker for a small number of targets. As such, our work points towards a new family of methods for pursuit interaction directly applicable to an ever-increasing number of devices readily equipped with cameras.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391315
ER  - 

TY  - NA
AU  - Gong, Jun; Li, Lan; Vogel, Daniel; Yang, Xing-Dong
TI  - CHI - Cito: An Actuated Smartwatch for Extended Interactions
PY  - 2017
AB  - We propose and explore actuating a smartwatch face to enable extended interactions. Five face movements are defined: rotation, hinging, translation, rising, and orbiting. These movements are incorporated into interaction techniques to address limitations of a fixed watch face. A 20-person study uses concept videos of a passive low fidelity prototype to confirm the usefulness of the actuated interaction techniques. A second 20-person study uses 3D rendered animations to access social acceptability and perceived comfort for different actuation dynamics and usage contexts. Finally, we present Cito, a high-fidelity proof-of-concept hardware prototype that investigates technical challenges.
SP  - 5331
EP  - 5345
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025568
ER  - 

TY  - NA
AU  - Faltaous, Sarah; Hubert, Anna; Karolus, Jakob; Villa, Steeven; Kosch, Thomas; Wozniak, Pawel W.
TI  - EMStriker: Potentials of Enhancing the Training Process of Racket-based Sports via Electrical Muscle Stimulation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505578
ER  - 

TY  - NA
AU  - Zhu, Junyi; Zhu, Yunyi; Cui, Jiaming; Cheng, Leon; Snowden, Jackson C.; Chounlakone, Mark; Wessely, Michael; Mueller, Stefanie
TI  - UIST - MorphSensor: A 3D Electronic Design Tool for Reforming Sensor Modules
PY  - 2020
AB  - MorphSensor is a 3D electronic design tool that enables designers to morph existing sensor modules of pre-defined two-dimensional shape into free-form electronic component arrangements that better integrate with the three-dimensional shape of a physical prototype. MorphSensor builds onto existing sensor module schematics that already define the electronic components and the wiring required to build the sensor. Since MorphSensor maintains the wire connections throughout the editing process, the sensor remains fully functional even when designers change the electronic component layout on the prototype geometry. We detail the MorphSensor editor that supports designers in re-arranging the electronic components, and discuss a fabrication pipeline based on customized PCB footprints for making the resulting freeform sensor. We then demonstrate the capabilities of our system by morphing a range of sensor modules of different complexity and provide a technical evaluation of the quality of the resulting free-form sensors.
SP  - 541
EP  - 553
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415898
ER  - 

TY  - JOUR
AU  - Jeung, Sein; Hilton, Christopher; Berg, Timotheus; Gehrke, Lukas; Gramann, Klaus
TI  - Virtual Reality for Spatial Navigation.
PY  - 2022
AB  - Immersive virtual reality (VR) allows its users to experience physical space in a non-physical world. It has developed into a powerful research tool to investigate the neural basis of human spatial navigation as an embodied experience. The task of wayfinding can be carried out by using a wide range of strategies, leading to the recruitment of various sensory modalities and brain areas in real-life scenarios. While traditional desktop-based VR setups primarily focus on vision-based navigation, immersive VR setups, especially mobile variants, can efficiently account for motor processes that constitute locomotion in the physical world, such as head-turning and walking. When used in combination with mobile neuroimaging methods, immersive VR affords a natural mode of locomotion and high immersion in experimental settings, designing an embodied spatial experience. This in turn facilitates ecologically valid investigation of the neural underpinnings of spatial navigation.
SP  - NA
EP  - NA
JF  - Current topics in behavioral neurosciences
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/7854_2022_403
ER  - 

TY  - BOOK
AU  - Outram, Benjamin I.; Pai, Yun Suen; Person, Tanner; Minamizawa, Kouta; Kunze, Kai
TI  - ETRA - Anyorbit: orbital navigation in virtual environments with eye-tracking
PY  - 2018
AB  - Gaze-based interactions promise to be fast, intuitive and effective in controlling virtual and augmented environments. Yet, there is still a lack of usable 3D navigation and observation techniques. In this work: 1) We introduce a highly advantageous orbital navigation technique, AnyOrbit, providing an intuitive and hands-free method of observation in virtual environments that uses eye-tracking to control the orbital center of movement; 2) The versatility of the technique is demonstrated with several control schemes and use-cases in virtual/augmented reality head-mounted-display and desktop setups, including observation of 3D astronomical data and spectator sports.
SP  - 99
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204555
ER  - 

TY  - NA
AU  - Fikri, Muhammad Ainul; Putra, Iqbal Kurniawan Asmar; Wibirama, Sunu
TI  - Object Selection Using LSTM Networks for Spontaneous Gaze-Based Interaction
PY  - 2022
AB  - Two years on with Covid-19, touchless technology has evolved from a device that symbolizes luxury to something that is necessary. Eye tracker is one type of touchless technologies that uses user's gaze to interact with computer without touching the screen. Development of spontaneous gazebased interaction is progressing very rapidly. Researchers have developed various object selection methods without prior gazeto-screen calibration. Recently, the conventional approach of setting threshold was developed as a gaze-based object selection method. However, the use of threshold values is considered non-adaptive and requires additional data pre-processing to handle noises. To overcome this problem, deep learning is used as an object selection method for spontaneous gaze-based interaction. Deep learning does not require any data preprocessing method to achieve accurate object selection results. Out of five deep learning algorithms that were evaluated, LSTM (Long Short-Term Memory) and BiLSTM (Bidirectional Long Short-Term Memory) networks achieved comparable accuracy of $95.17 \pm 0.95$% and $95.15 \pm 1.17$%, respectively. In future, our research is promising for development of real-time object selection technique for touchless public display.
SP  - NA
EP  - NA
JF  - 2022 14th International Conference on Information Technology and Electrical Engineering (ICITEE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icitee56407.2022.9954104
ER  - 

TY  - NA
AU  - Saini, Aryan; Huang, Haotian; Patibanda, Rakesh; Overdevest, Nathalie; Van Den Hoven, Elise; Mueller, Florian Floyd
TI  - SomaFlatables: Supporting Embodied Cognition through Pneumatic Bladders
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558705
ER  - 

TY  - BOOK
AU  - Newn, Joshua; Velloso, Eduardo; Carter, Marcus; Vetere, Frank
TI  - ISS - Multimodal Segmentation on a Large Interactive Tabletop: Extending Interaction on Horizontal Surfaces with Gaze
PY  - 2016
AB  - Eye tracking is a promising input modality for interactive tabletops. However, issues such as eyelid occlusion and the viewing angle at distant positions present significant challenges for remote gaze tracking in this setting. We present the results of two studies that explore the way gaze interaction can be enabled. Our first study contributes the results from an empirical investigation of gaze accuracy on a large horizontal surface, finding gaze to be unusable close to the user (due to eyelid occlusion), accurate at arm's length, and only precise horizontally at large distances. In consideration of these results, we propose two solutions for the design of interactive systems that utilise remote gaze-tracking on the tabletop; multimodal segmentation and the use of X-Gaze-our novel technique-to interact with out-of-reach objects. Our second study evaluates and validates both these solutions in a Video-on-Demand application, presenting immediate opportunities for remote-gaze interaction on horizontal surfaces.
SP  - 251
EP  - 260
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992179
ER  - 

TY  - NA
AU  - Dobbelstein, David; Herrdum, Steffen; Rukzio, Enrico
TI  - ISWC - inScent: a wearable olfactory display as an amplification for mobile notifications
PY  - 2017
AB  - We introduce inScent, a wearable olfactory display that can be worn in mobile everyday situations and allows the user to receive personal scented notifications, i.e. scentifications. Olfaction, i.e. the sense of smell, is used by humans as a sensorial information channel as an element for experiencing the environment. Olfactory sensations are closely linked to emotions and memories, but also notify about personal dangers such as fire or foulness. We want to utilize the properties of smell as a notification channel by amplifying received mobile notifications with artificially emitted scents. We built a wearable olfactory display that can be worn as a pendant around the neck and contains up to eight different scent aromas that can be inserted and quickly exchanged via small scent cartridges. Upon emission, scent aroma is vaporized and blown towards the user. A hardware - and software framework is presented that allows developers to add scents to their mobile applications. In a qualitative user study, participants wore the inScent wearable in public. We used subsequent semi-structured interviews and grounded theory to build a common understanding of the experience and derived lessons learned for the use of scentifications in mobile situations.
SP  - 130
EP  - 137
JF  - Proceedings of the 2017 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123021.3123035
ER  - 

TY  - NA
AU  - Clarke, Christopher; Bellino, Alessio; Esteves, Augusto; Velloso, Eduardo; Gellersen, Hans
TI  - UbiComp - TraceMatch: a computer vision technique for user input by tracing of animated controls
PY  - 2016
AB  - Recent works have explored the concept of movement correlation interfaces, in which moving objects can be selected by matching the movement of the input device to that of the desired object. Previous techniques relied on a single modality (e.g. gaze or mid-air gestures) and specific hardware to issue commands. TraceMatch is a computer vision technique that enables input by movement correlation while abstracting from any particular input modality. The technique relies only on a conventional webcam to enable users to produce matching gestures with any given body parts, even whilst holding objects. We describe an implementation of the technique for acquisition of orbiting targets, evaluate algorithm performance for different target sizes and frequencies, and demonstrate use of the technique for remote control of graphical as well as physical objects with different body parts.
SP  - 298
EP  - 303
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971648.2971714
ER  - 

TY  - NA
AU  - Clarke, Christopher; Gellersen, Hans
TI  - UIST - MatchPoint: Spontaneous Spatial Coupling of Body Movement for Touchless Pointing
PY  - 2017
AB  - Pointing is a fundamental interaction technique where user movement is translated to spatial input on a display. Conventionally, this is based on a rigid configuration of a display coupled with a pointing device that determines the types of movement that can be sensed, and the specific ways users can affect pointer input. Spontaneous spatial coupling is a novel input technique that instead allows any body movement, or movement of tangible objects, to be appropriated for touchless pointing on an ad hoc basis. Pointer acquisition is facilitated by the display presenting graphical objects in motion, to which users can synchronise to define a temporary spatial coupling with the body part or tangible object they used in the process. The technique can be deployed using minimal hardware, as demonstrated by MatchPoint, a generic computer vision-based implementation of the technique that requires only a webcam. We explore the design space of spontaneous spatial coupling, demonstrate the versatility of the technique with application examples, and evaluate MatchPoint performance using a multi-directional pointing task.
SP  - 179
EP  - 192
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126626
ER  - 

TY  - NA
AU  - Mizuhara, Ryo; Takahashi, Akifumi; Kajimoto, Hiroyuki
TI  - AH - Enhancement of Subjective Mechanical Tactile Intensity via Electrical Stimulation
PY  - 2019
AB  - Naturalistic tactile sensations can be elicited by mechanical stimuli because mechanical stimulation reproduces a natural physical phenomenon. However, a mechanical stimulation that is too strong may cause injury. Although electrical stimulation can elicit strong tactile sensations without damaging the skin, electrical stimulation is inferior in terms of naturalness. Here, we propose and validate a haptic method for presenting naturalistic and intense sensations by combining electrical and mechanical stimulation. Prior to the main experiment, we measured the appropriate temporal gap between the two stimuli such that they are perceived as simultaneous, since nerve activity directly elicited by electrical stimulation is generally considered to be perceived faster than mechanical stimulation. We confirmed that enhancement of subjective strength took place when two stimuli were given simultaneously. The main experiment with simultaneous electrical and mechanical stimulation confirmed that addition of electrical stimulation enhances the sensation of mechanical stimulation, and participants' comments implied that electrical stimulation was interpreted as part of the mechanical stimulation.
SP  - 9
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311836
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Umapathi, Udayan; Leithinger, Daniel; Ishii, Hiroshi
TI  - Conference on Designing Interactive Systems - AnimaStage: Hands-on Animated Craft on Pin-based Shape Displays
PY  - 2017
AB  - In this paper, we present AnimaStage: a hands-on animated craft platform based on an actuated stage. Utilizing a pin-based shape changing display, users can animate their crafts made from various materials. Through this system, we intend to lower the barrier for artists and designers to create actuated objects and to contribute to interaction design using shape changing interfaces for inter-material interactions. We introduce a three-phase design process for AnimaStage with examples of animated crafts. We implemented the system with several control modalities that allow users to manipulate the motion of the crafts so that they could easily explore their desired motion through an iterative process. To complement the animated crafts, dynamic landscapes can also be rendered. We conducted a user study to observe the subject and process by which people make crafts using AnimaStage. We invited participants with different backgrounds to design and create crafts using multiple materials and craft techniques. A variety of outcomes and application spaces were found in this study.
SP  - 1093
EP  - 1097
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064670
ER  - 

TY  - JOUR
AU  - Kono, Michinari; Takahashi, Takumi; Nakamura, Hiromi; Miyaki, Takashi; Rekimoto, Jun
TI  - Design Guideline for Developing Safe Systems that Apply Electricity to the Human Body
PY  - 2018
AB  - The human body has unique electrical characteristics. These characteristics have been investigated in various studies in human-computer interaction (HCI) and related research fields. Such studies include applications for using the body as a conductive lead for transmission or electric field sensing and activating human muscles or organs. However, electricity is not completely safe for the human body; therefore, to avoid harming users, careful consideration is essential when developing such devices. The knowledge required for such consideration is spread throughout a large number research fields, and it can be difficult for researchers in the HCI field to comprehend all of them. The purpose of this article is to support researchers in developing systems that apply electricity to the human body and to serve as a basis for further research. This article reviews previous research pertaining to HCI in which users come into contact with electricity. In addition, considerations of how and where this type of research can be expanded, along with guidelines grounded in other fields for designing systems safely and addressing ethical concerns, are presented. An understanding of the field and of the related safety issues will enhance the understanding of limitations and potential and can clarify the design space.
SP  - 19
EP  - 36
JF  - ACM Transactions on Computer-Human Interaction
VL  - 25
IS  - 3
PB  - 
DO  - 10.1145/3184743
ER  - 

TY  - NA
AU  - Yen, Chi-Hsien; Lee, Yi-Chieh; Fu, Wai-Tat
TI  - IUI - Visible Hearts, Visible Hands: A Smart Crowd Donation Platform
PY  - 2018
AB  - On existing crowdfunding platforms, the allocation of money is often not regulated, which leads to less-than-ideal distribution of resources. For example, recent donations to hurricane victims through their crowdfunding campaigns often lead to overfunding of certain victims while underfunding others. Inspired by algorithms from economic theories, we proposed a Smart Crowd Donate system encourages donors to express preferences to multiple projects and reallocates funds dynamically across these preferences over time. We conducted a user study in which recruited 452 participants to simulate a small scale of crowdfunding. The findings of our user study supported the idea that the Smart Crowd Donate system has potential to efficiently distribute funds to projects and allows more projects to receive the amount of money they need.
SP  - 385
EP  - 395
JF  - 23rd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3172944.3172971
ER  - 

TY  - NA
AU  - Wang, Yuntao; Ding, Jiexin; Chatterjee, Ishan; Salemi Parizi, Farshid; Zhuang, Yuzhou; Yan, Yukang; Patel, Shwetak; Shi, Yuanchun
TI  - FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones
PY  - 2022
AB  - Face orientation can often indicate users' intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user's earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user's face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri's performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track the user's head orientation with a median absolute error of 10.9 mm in the distance, 3.7 degrees in yaw, and 5.8 degrees in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interaction.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517698
ER  - 

TY  - NA
AU  - Sidenmark, Ludwig; Clarke, Christopher; Zhang, Xuesong; Phu, Jenny; Gellersen, Hans
TI  - CHI - Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality
PY  - 2020
AB  - In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376438
ER  - 

TY  - NA
AU  - Wang, Chiuan; Yeh, Hsuan-Ming; Wang, Bryan; Wu, Te-Yen; Tsai, Hsin-Ruey; Liang, Rong-Hao; Hung, Yi-Ping; Chen, Mike Y.
TI  - UIST - CircuitStack: Supporting Rapid Prototyping and Evolution of Electronic Circuits
PY  - 2016
AB  - For makers and developers, circuit prototyping is an integral part of building electronic projects. Currently, it is common to build circuits based on breadboard schematics that are available on various maker and DIY websites. Some breadboard schematics are used as is without modification, and some are modified and extended to fit specific needs. In such cases, diagrams and schematics merely serve as blueprints and visual instructions, but users still must physically wire the breadboard connections, which can be time-consuming and error-prone. We present CircuitStack, a system that combines the flexibility of breadboarding with the correctness of printed circuits, for enabling rapid and extensible circuit construction. This hybrid system enables circuit reconfigurability, component reusability, and high efficiency at the early stage of prototyping development.
SP  - 687
EP  - 695
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984527
ER  - 

TY  - NA
AU  - Lee, Juyoung; Aggarwal, Shaurye; Wu, Jason; Starner, Thad; Woo, Woontack
TI  - SelfSync: exploring self-synchronous body-based hotword gestures for initiating interaction
PY  - 2019
AB  - SelfSync enables rapid, robust initiation of a gesture interface using synchronized movement of different body parts. SelfSync is the gestural equivalent of a hotword such as OK-Google in a speech interface and is enabled by the increasing trend where a user wears two or more wearables, such as a smartwatch, wireless earbuds, or a smartphone. In a user study comparing five potential SelfSync gestures in isolation, our system averages 96%, 98% and 88% for user dependent, user adapted, and user independent accuracy, respectively. For when the user has a phone in a pocket and a smart-watch, we suggest twisting the hand about the wrist while moving the leg with the phone in synchrony left and right. When the user has a head worn device and a smartwatch, we suggest twisting the hand while twisting the head left and right.
SP  - 123
EP  - 128
JF  - Proceedings of the 23rd International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341163.3347745
ER  - 

TY  - NA
AU  - Rifat, Rashidujjaman; Siddique, Aysha; Abouzied, Azza; Chen, Jay
TI  - ICTD - From Alley to Landfill: Challenges of and Design Opportunities for Cleaning Dhaka's Communal Trash
PY  - 2016
AB  - Garbage is an endemic problem in developing cities due to the continual influx of migrants from rural areas coupled with deficient municipal capacity planning. In cities like Dhaka, open waste dumps contribute to the prevalence of disease, environmental contamination, catastrophic flooding, and deadly fires. Recent interest in the garbage problem has prompted cursory proposals to introduce technology solutions for mapping and fundraising. Yet, the role of technology and its potential benefits are unexplored in this large-scale problem. In this paper, we contribute to the understanding of the waste ecology in Dhaka and how the various actors acquire, perform, negotiate, and coordinate their roles. Within this context, we explore design opportunities for using computing technologies to support collaboration between waste pickers and residents of these communities. We find opportunities in the presence of technology and the absence of mechanisms to facilitate coordination of community funding and crowd work.
SP  - 9
EP  - NA
JF  - Proceedings of the Eighth International Conference on Information and Communication Technologies and Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909609.2909648
ER  - 

TY  - JOUR
AU  - Hassan, Mahmoud; Daiber, Florian; Wiehr, Frederik; Kosmalla, Felix; Krüger, Antonio
TI  - FootStriker: An EMS-based Foot Strike Assistant for Running
PY  - 2017
AB  - In running, knee-related injuries are very common. The main cause are high impact forces when striking the ground with the heel first. Mid- or forefoot running is generally known to reduce impact loads and to be a more efficient running style. In this paper, we introduce a wearable running assistant, consisting of an electrical muscle stimulation (EMS) device and an insole with force sensing resistors. It detects heel striking and actuates the calf muscles during the flight phase to control the foot angle before landing. We conducted a user study, in which we compared the classical coaching approach using slow motion video analysis as a terminal feedback to our proposed real-time EMS feedback. The results show that EMS actuation significantly outperforms traditional coaching, i.e., a decreased average heel striking rate, when using the system. As an implication, EMS feedback can generally be beneficial for the motor learning of complex, repetitive movements.
SP  - 2
EP  - 18
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 1
PB  - 
DO  - 10.1145/3053332
ER  - 

TY  - NA
AU  - Drewes, Heiko; Khamis, Mohamed; Alt, Florian
TI  - MUM - DialPlates: enabling pursuits-based user interfaces with large target numbers
PY  - 2019
AB  - In this paper we introduce a novel approach for smooth pursuits eye movement detection and demonstrate that it allows up to 160 targets to be distinguished. With this work we advance the well-established smooth pursuits technique, which allows gaze interaction without calibration. The approach is valuable for researchers and practitioners, since it enables novel user interfaces and applications to be created that employ a large number of targets, for example, a pursuits-based keyboard or a smart home where many different objects can be controlled using gaze. We present findings from two studies. In particular, we compare our novel detection algorithm based on linear regression with the correlation method. We quantify its accuracy for around 20 targets on a single circle and up to 160 targets on multiple circles. Finally, we implemented a pursuits-based keyboard app with 108 targets as proof-of-concept.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365610.3365626
ER  - 

TY  - JOUR
AU  - Blascheck, Tanja; Besançon, Lonni; Bezerianos, Anastasia; Lee, Bongshin; Isenberg, Petra
TI  - Glanceable Visualization: Studies of Data Comparison Performance on Smartwatches
PY  - 2018
AB  - We present the results of two perception studies to assess how quickly people can perform a simple data comparison task for small-scale visualizations on a smartwatch. The main goal of these studies is to extend our understanding of design constraints for smartwatch visualizations. Previous work has shown that a vast majority of smartwatch interactions last under 5 s. It is still unknown what people can actually perceive from visualizations during such short glances, in particular with such a limited display space of smartwatches. To shed light on this question, we conducted two perception studies that assessed the lower bounds of task time for a simple data comparison task. We tested three chart types common on smartwatches: bar charts, donut charts, and radial bar charts with three different data sizes: 7, 12, and 24 data values. In our first study, we controlled the differences of the two target bars to be compared, while the second study varied the difference randomly. For both studies, we found that participants performed the task on average in <300 ms for the bar chart, <220 ms for the donut chart, and in < 1780 ms for the radial bar chart. Thresholds in the second study per chart type were on average 1.14-1.35× higher than in the first study. Our results show that bar and donut charts should be preferred on smartwatch displays when quick data comparisons are necessary.
SP  - 630
EP  - 640
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865142
ER  - 

TY  - NA
AU  - Mohan, Pallavi; Goh, Wooi Boon; Fu, Chi-Wing; Yeung, Sai-Kit
TI  - ISMAR Adjunct - DualGaze: Addressing the Midas Touch Problem in Gaze Mediated VR Interaction
PY  - 2018
AB  - With the increasing acceptance of eye tracking as a viable interaction method for Virtual Reality (VR) headsets, thoughtful gaze interaction methods need to be carefully designed to meet common challenges such as the Midas Touch problem, where users unintentionally select onscreen objects by gazing upon them. This paper presents DualGaze, a novel interaction method in which users perform a distinctive two-step gaze gesture for object selection. Once users gaze upon an object that they wish to select, a confirmation flag pops up next to the object at a location where the users' gaze just passed through. This trajectory-adaptive flag placement strategy reduces the chance of unintentional confirmation by requiring a returning gaze back to the flag. We conducted a user study to compare the accuracy and selection speed of DualGaze and the popular gaze fixation method on a simple gaze-typing task. Our results show that DualGaze is significantly more accurate while maintaining a comparable selection speed that was observed to improve with familiarity of use.
SP  - 79
EP  - 84
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2018.00039
ER  - 

TY  - NA
AU  - Wang, Li-Yang; Han, Ping-Hsuan; Chan, Liwei
TI  - Push-Ups: Enhancing Kinesthetic Experience with Shape-Forming Devices on the Feet Soles
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501333
ER  - 

TY  - NA
AU  - Brunvand, Erik
TI  - ACM Great Lakes Symposium on VLSI - Extending Student Labs with SMT Circuit Implementation
PY  - 2019
AB  - Computer Science and Computer Engineering classes related to digital circuits, embedded systems, Human Computer Interaction (HCI), and a wide variety of "maker" subjects, would often like to include physical computing projects. Extending these physical computing ideas to physical realization of circuits is the next logical step, and has traditionally happened with low-cost non-soldered prototyping such as breadboards. However, many modern circuit components are now available only in tiny surface-mount technology (SMT) packages. Using these components essentially requires that students are able to design printed circuit boards (PCBs) specifically for their module. In this paper we describe an experience report of how we enhanced a student lab in a cost-effective way to enable students to develop, assemble, and solder custom PCBs that contain SMT components. This allows modern circuit components to be used, offers an enriched hands-on experience, and enables an expansion of the scope and complexity of student projects.
SP  - 231
EP  - 236
JF  - Proceedings of the 2019 on Great Lakes Symposium on VLSI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3299874.3317968
ER  - 

TY  - NA
AU  - Head, Andrew; Glassman, Elena L.; Soares, Gustavo; Suzuki, Ryo; Figueredo, Lucas; D'Antoni, Loris; Hartmann, Björn
TI  - L@S - Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis
PY  - 2017
AB  - In large introductory programming classes, teacher feedback on individual incorrect student submissions is often infeasible. Program synthesis techniques are capable of fixing student bugs and generating hints automatically, but they lack the deep domain knowledge of a teacher and can generate functionally correct but stylistically poor fixes. We introduce a mixed-initiative approach which combines teacher expertise with data-driven program synthesis techniques. We demonstrate our novel approach in two systems that use different interaction mechanisms. Our systems use program synthesis to learn bug-fixing code transformations and then cluster incorrect submissions by the transformations that correct them. The MistakeBrowser system learns transformations from examples of students fixing bugs in their own submissions. The FixPropagator system learns transformations from teachers fixing bugs in incorrect student submissions. Teachers can write feedback about a single submission or a cluster of submissions and propagate the feedback to all other submissions that can be fixed by the same transformation. Two studies suggest this approach helps teachers better understand student bugs and write reusable feedback that scales to a massive introductory programming classroom.
SP  - 89
EP  - 98
JF  - Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3051457.3051467
ER  - 

TY  - NA
AU  - Cassidy, Caitlin; Goldman, Max; Miller, Robert C.
TI  - L@S - Glanceable code history: visualizing student code for better instructor feedback
PY  - 2018
AB  - Immediate, individualized feedback on their code helps students learning to program. However, even in short, focused exercises in active learning, teachers do not have much time to write feedback. In addition, only looking at a student's final code hides a lot of the students' learning and discovering process. We created a glanceable code history visualization that enables teachers to view a student's entire coding history quickly and efficiently. A preliminary user study shows that this visualization captures previously unseen information that allows teachers to give students better grades and give students longer feedback and better feedback that focuses not just on their final code, but all their code in between.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifth Annual ACM Conference on Learning at Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3231644.3231680
ER  - 

TY  - JOUR
AU  - Zeng, Zhe; Neuer, Elisabeth Sumithra; Roetting, Matthias; Siebert, Felix Wilhelm
TI  - A One-Point Calibration Design for Hybrid Eye Typing Interface
PY  - 2022
AB  - NA
SP  - 1
EP  - 14
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2101186
ER  - 

TY  - JOUR
AU  - Gil, Hyunjae; Kim, Hongmin; Oakley, Ian
TI  - Fingers and Angles: Exploring the Comfort of Touch Input on Smartwatches
PY  - 2018
AB  - Smartwatches present a unique touch input context: small, fixed to one wrist and approachable from a limited range of angles by the touching hand. Techniques to expand their input expressivity often involve variations in how a watch must be touched, such as with different fingers, poses or from specific angles. While objective performance with such systems is commonly reported, subjective qualities such as comfort remain overlooked. We argue that techniques that involve uncomfortable input will be of limited value and contribute the first data on the comfort of input on smartwatches via two studies that combine subjective ratings of comfort with objective performance data. We examine both static and dynamic touches and three finger poses. Based on the study results, we contribute a set of design recommendations for comfortable, effective smartwatch input. We close by instantiating the recommendations in interface prototypes that we evaluate in a final qualitative study.
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287042
ER  - 

TY  - NA
AU  - Nabil, Sara; Plötz, Thomas; Kirk, David
TI  - Tangible and Embedded Interaction - Interactive Architecture: Exploring and Unwrapping the Potentials of Organic User Interfaces
PY  - 2017
AB  - Organic User Interfaces (OUIs) are flexible, actuated interfaces characterized by being aesthetically pleasing, intuitively manipulated and ubiquitously embedded in our daily life. In this paper, we critically survey the state-of-the-art for OUIs in interactive architecture research at two levels: 1) Architecture and Landscape; and 2) Interior Design. We postulate that OUIs have specific qualities that offer great potential for building interactive interiors and entire architectures that have the potential to -finally- transform the vision of smart homes and ubiquitous computing environments (calm computing) into reality. We formulate a manifesto for OUI Architecture in both exterior and interior design, arguing that OUIs should be at the core of a new interdisciplinary field driving research and practice in architecture. Based on this research agenda we propose concerted efforts to be made to begin addressing the challenges and opportunities of OUIs. This agenda offers us the strongest means through which to deliver a future of interactive architecture.
SP  - 89
EP  - 100
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3024981
ER  - 

TY  - NA
AU  - Niijima, Arinobu; Isezaki, Takashi; Aoki, Ryosuke; Watanabe, Tomoki; Yamada, Tomohiro
TI  - CHI - Controlling Maximal Voluntary Contraction of the Upper Limb Muscles by Facial Electrical Stimulation
PY  - 2018
AB  - In this paper, we propose to use facial electrical stimulation to control maximal voluntary contraction (MVC) of the upper limbs. The method is based on a body mechanism in which the contraction of the masseter muscles enhances MVC of the limb muscles. Facial electrical stimulation is applied to the masseter muscles and the lips. The former is to enhance the MVC by causing involuntary contraction of the masseter muscles, and the latter is to suppress the MVC by interfering with voluntary contraction of the masseter muscles. In a user study, we used electromyography sensors on the upper limbs to evaluate the effects of the facial electrical stimulation on the MVC of the upper limbs. The experimental results show that the MVC was controlled by the facial electrical stimulation. We assume that the proposed method is useful for sports athletes because the MVC is linked to sports performance.
SP  - 394
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173968
ER  - 

TY  - NA
AU  - Tiab, John; Boring, Sebastian; Strohmeier, Paul; Markussen, Anders; Alexander, Jason; Hornbæk, Kasper
TI  - AVI - Tiltstacks: composing shape-changing interfaces using tilting and stacking of modules
PY  - 2018
AB  - Many shape-changing interfaces use an array of actuated rods to create a display surface; each rod working as a pixel. However, this approach only supports pixel height manipulation and cannot produce more radical shape changes of each pixel (and thus of the display). Examples of such changes include non-horizontal pixels, pixels that overhang other pixels, or variable gaps between pixels. We present a concept for composing shape-changing interfaces by vertically stacking tilt-enabled modules. Together, stacking and tilting allow us to create a more diverse range of display surfaces than using arrays. We demonstrate this concept through TiltStacks, a shape-changing prototype built using stacked linear actuators and displays. Each tiltable module provides three degrees of freedom (z-movement, roll, and pitch); two more degrees of freedom are added through stacking modules (i.e., planar x- and y-movement).
SP  - 44
EP  - NA
JF  - Proceedings of the 2018 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206505.3206530
ER  - 

TY  - JOUR
AU  - Hoppe, Matthias; Oskina, Daria; Schmidt, Albrecht; Kosch, Thomas
TI  - Odin's Helmet: A Head-Worn Haptic Feedback Device to Simulate G-Forces on the Human Body in Virtual Reality
PY  - 2021
AB  - Virtual Reality (VR) experiences have massively improved in the mediation of feedback. However, the simulation of forces is still limited. This paper presents Odin's Helmet, a head-worn device to simulate g-forces that act on the human head in real-life situations. Odin's Helmet uses four head-mounted propellers as actuators to simulate g-forces through pushing and pulling the user's head while being immersed in VR. Odin's Helmet's goal is to increase presence and manipulate the user's perception of the otolith organ in the vestibular system. The user's perception will be tricked to experience a sensation of self-movement in VR. A technical evaluation shows Odin's Helmet's applicability to apply perceivable g-forces to the user's head. We conclude with future use cases of Odin's Helmet, such as redirected walking by controlling the user's head orientation, attention guidance, and wind simulations through Odin's Helmet.
SP  - 1
EP  - 15
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3461734
ER  - 

TY  - NA
AU  - Lu, Kevin; Brombacher, Aarnout
TI  - Tangible and Embedded Interaction - Haptic Feedback in Running: Is It Possible for Information Transfer through Electrical Muscle Signalling?
PY  - 2020
AB  - A haptic feedback mechanism is explored for personalized data interaction. Electrical muscle stimulation under the level of full contraction, in this paper described as electrical muscle signalling (EMS), is used for on-body and live data interactions as simplified cognitive processes for running purposes, such as data-assisted coaching, personalized feedback and injury prevention. In this research, we defined haptic electrical muscle signalling as feedback mechanism and the results concluded that (i.) muscle signalling under the level of contraction can be noticed in the form of pre-cramps, similar to a vibrating/contracting type of feedback on the skin, (ii.) feedback is able to trigger cognitive processes while running (iii.) and it does not negatively impact running performance or comfort. This is on-going research and future work is already in progress.
SP  - 479
EP  - 485
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374976
ER  - 

TY  - JOUR
AU  - Rafael, Ballagas; Ghosh, Sarthak; Landay, James A.
TI  - The Design Space of 3D Printable Interactivity
PY  - 2018
AB  - The capabilities of 3D printers are rapidly progressing towards fabrication of fully interactive products. For designers to reason about the best way to achieve their interaction design goals, it is helpful to not only know what exists in the literature, but to also understand the design space of options. Such an understanding can help in comparison, analysis, selection of suitable technology, and also in the generation of new ideas. In this article, we survey the state of the art in 3D printing fully functional sensors and actuators to support explicit interaction techniques. We classify and organize the surveyed works around the following parameters: mechanism, designed affordances, interaction primitives, and output modality. The design space is presented in the form of a multidimensional matrix known as a Zwicky box. Using the tables, we can make observations about the existing literature and also identify gaps in this design space. Many such gaps can potentially lead to exciting new research or engineering opportunities.
SP  - 61
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 2
PB  - 
DO  - 10.1145/3214264
ER  - 

TY  - NA
AU  - Duente, Tim; Pfeiffer, Max; Rohs, Michael
TI  - MobileHCI - Zap++: a 20-channel electrical muscle stimulation system for fine-grained wearable force feedback
PY  - 2017
AB  - Electrical muscle stimulation (EMS) has been used successfully in HCI to generate force feedback and simple movements both in stationary and mobile settings. However, many natural limb movements require the coordinated actuation of multiple muscles. Off-the-shelf EMS devices are typically limited in their ability to generate fine-grained movements, because they only have a low number of channels and do not provide full control over the EMS parameters. More capable medical devices are not designed for mobile use or still have a lower number of channels and less control than is desirable for HCI research. In this paper we present the concept and a prototype of a 20-channel mobile EMS system that offers full control over the EMS parameters. We discuss the requirements of wearable multi-electrode EMS systems and present the design and technical evaluation of our prototype. We further outline several application scenarios and discuss safety and certification issues.
SP  - 1
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3098546
ER  - 

TY  - NA
AU  - Gomez, Argenis Ramirez
TI  - Exploration of smooth pursuit eye movements for gaze calibration in games
PY  - 2017
AB  - Eye tracking offers opportunities to extend novel interfaces and promises new ways of interaction for gameplay. However, gaze has been found challenging to use in dynamic interfaces involving motion. Moving targets are hard to select with state of the art gaze input methods and gaze estimation requires calibration in order to be accurate when offering a successful interaction experience. Smooth pursuit eye movements have been used to solve this new paradigm, but there is not enough information on the behavior of the eyes when performing such eye movement. In this work, we tried to understand the relationship between gaze and motion when performing smooth pursuit movements through the integration of calibration within a videogame. In our rst study, we propose to leverage the attentive gaze behavior of the eyes during gameplay for implicit and continuous re-calibration. We demonstrated this with GazeBall, a retro-inspired version of Atari's BreakOut game in which we continually calibrate gaze based on the ball's movement and the player's inevitable ocular pursuit on the ball. Continuous calibration enabled the extension of the game with a gaze-based `power-up'. In the evaluation of GazeBall, we show that our approach is effective in maintaining highly accurate gaze input over time, while re-calibration remains invisible to the player. GazeBall raised awareness on the lack of information about smooth pursuit for interaction. Therefore, in our second study, we focused on gaining more understanding on the behavior of the eyes. By testing different motion directions and speeds we found anticipatory predictions during gaze trajectory that indicates that the common reaction of the eyes when a moving target is present is not only following but trying to predict and advance the displayed movement.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kolvenbag, Jay; Bruns, Miguel; Winters, Amy
TI  - Rapid Prototyping Dynamic Robotic Fibers for Tunable Movement
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558696
ER  - 

TY  - NA
AU  - Elsden, Chris; Trotter, Ludwig; Harding, Mike; Davies, Nigel; Speed, Chris; Vines, John
TI  - CHI - Programmable Donations: Exploring Escrow-Based Conditional Giving
PY  - 2019
AB  - This paper reports on a co-speculative interview study with charitable donors to explore the future of programmable, conditional and data-driven donations. Responding to the rapid emergence of blockchain-based and AI-supported financial technologies, we specifically examine the potential of automated, third-party 'escrows', where donations are held before they are released or returned based on specified rules and conditions. To explore this we conducted pilot workshops with 9 participants and an interview study in which 14 further participants were asked about their experiences of donating money, and invited to co-speculate on a service for programmable giving. The study elicited how data-driven conditionality and automation could be leveraged to create novel donor experiences, however also illustrated the inherent tensions and challenges involved in giving programmatically. Reflecting on these findings, our paper contributes implications both for the design of programmable aid platforms, and the design of escrow-based financial services in general.
SP  - 379
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300609
ER  - 

TY  - NA
AU  - Shimizu, Junichi; Lee, Juyoung; Dhuliawala, Murtaza; Bulling, Andreas; Starner, Thad; Woo, Woontack; Kunze, Kai
TI  - UbiComp Adjunct - Solar system: smooth pursuit interactions using EOG glasses
PY  - 2016
AB  - Solar System implements smooth pursuit eye movement interactions on commercial smart glasses using electrooculography. The system requires no calibration and little to no training. We present a prototype implementation, describe initial user tests and show several application scenarios for hands-free eye gaze interactions.
SP  - 369
EP  - 372
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2971376
ER  - 

TY  - NA
AU  - Vallgårda, Anna; Boer, Laurens; Tsaknaki, Vasiliki; Svanæs, Dag
TI  - NordiCHI - Material Programming: a Design Practice for Computational Composites
PY  - 2016
AB  - In this paper we propose the notion of material programming as a future design practice for computational composites. Material programming would be a way for the interaction designer to better explore the dynamic potential of computational materials at hand and through that familiarity be able to compose more sophisticated and complex temporal forms in their designs. The contribution of the paper is an analysis of qualities that we find a material programming practice would and should support: designs grounded in material properties and experiences, embodied programming practice, real-time on-site explorations, and finally a reasonable level of complexity in couplings between input and output. We propose material programming knowing that the technology and materials are not entirely ready to support this practice yet, however, we are certain they will be and that the interaction design community will need to find new ways of relating to such computational materials.
SP  - 46
EP  - NA
JF  - Proceedings of the 9th Nordic Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971485.2971554
ER  - 

TY  - NA
AU  - Nascimento, Marcos; Araújo, Eliane; Serey, Dalton; de Figueiredo, Jorge C. A.
TI  - FIE - The Role of Source Code Vocabulary in Programming Teaching and Learning
PY  - 2020
AB  - This full paper, categorized as Research-to-Practice, presents an investigation on the role of source code vocabulary in introductory programming learning practices. According to software engineering literature, source code vocabulary is a fundamental aspect of software quality as it contributes to software readability and eases maintenance and evolution tasks. Furthermore, some studies show that identifier naming and comments in the code tend to use terms that reflect the software problem domain specification and can be used, for instance, to localize feature. In this paper, we build upon this knowledge of software engineering and investigate how we can take advantage of it in computer science education, specially on the teaching of programming. We conducted a twofold empirical study in an introductory programming course to investigate: (1) How to give automated feedback about identifier naming, aiming to improve software readability, and (2) To what extent source code vocabulary reflects problem specification comprehension, which is crucial to effective programming problem-solving. In the first study, we found that 51.7% of the students improved their source code vocabulary, after receiving such feedback. In the second study, the results showed that students tend to manage to better comprehend the programming problem being solved when their code identifier names are connected to the related description. It is a promising indicative that we can use this information to assess the problem requirements comprehension, which is a fundamental step in the programming problem-solving cycle. The main contribution of this paper is to shed light on the source code vocabulary and promote its role in the programming teaching and learning scenario. We present some evidence that students’ source code vocabulary is a rich information source about their understanding and we can use it to produce formative feedback.
SP  - 1
EP  - 8
JF  - 2020 IEEE Frontiers in Education Conference (FIE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/fie44824.2020.9274137
ER  - 

TY  - NA
AU  - Esteves, Augusto; Verweij, David; Suraiya, Liza; Islam, Rasel; Lee, Youryang; Oakley, Ian
TI  - UIST - SmoothMoves: Smooth Pursuits Head Movements for Augmented Reality
PY  - 2017
AB  - SmoothMoves is an interaction technique for augmented reality (AR) based on smooth pursuits head movements. It works by computing correlations between the movements of on-screen targets and the user's head while tracking those targets. The paper presents three studies. The first suggests that head based input can act as an easier and more affordable surrogate for eye-based input in many smooth pursuits interface designs. A follow-up study grounds the technique in the domain of augmented reality, and captures the error rates and acquisition times on different types of AR devices: head-mounted (2.6%, 1965ms) and hand-held (4.9%, 2089ms). Finally, the paper presents an interactive lighting system prototype that demonstrates the benefits of using smooth pursuits head movements in interaction with AR interfaces. A final qualitative study reports on positive feedback regarding the technique's suitability for this scenario. Together, these results indicate show SmoothMoves is viable, efficient and immediately available for a wide range of wearable devices that feature embedded motion sensing.
SP  - 167
EP  - 178
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126616
ER  - 

TY  - NA
AU  - Degraen, Donald; Piovarči, Michal; Bickel, Bernd; Krüger, Antonio
TI  - UIST - Capturing Tactile Properties of Real Surfaces for Haptic Reproduction
PY  - 2021
AB  - Tactile feedback of an object’s surface enables us to discern its material properties and affordances. This understanding is used in digital fabrication processes by creating objects with high-resolution surface variations to influence a user’s tactile perception. As the design of such surface haptics commonly relies on knowledge from real-life experiences, it is unclear how to adapt this information for digital design methods. In this work, we investigate replicating the haptics of real materials. Using an existing process for capturing an object’s microgeometry, we digitize and reproduce the stable surface information of a set of 15 fabric samples. In a psychophysical experiment, we evaluate the tactile qualities of our set of original samples and their replicas. From our results, we see that direct reproduction of surface variations is able to influence different psychophysical dimensions of the tactile perception of surface textures. While the fabrication process did not preserve all properties, our approach underlines that replication of surface microgeometries benefits fabrication methods in terms of haptic perception by covering a large range of tactile variations. Moreover, by changing the surface structure of a single fabricated material, its material perception can be influenced. We conclude by proposing strategies for capturing and reproducing digitized textures to better resemble the perceived haptics of the originals.
SP  - 954
EP  - 971
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474798
ER  - 

TY  - BOOK
AU  - George, Ceenu; Buschek, Daniel; Ngao, Andrea; Khamis, Mohamed
TI  - AVR (1) - GazeRoomLock: Using Gaze and Head-Pose to Improve the Usability and Observation Resistance of 3D Passwords in Virtual Reality
PY  - 2020
AB  - Authentication has become an important component of Immersive Virtual Reality (IVR) applications, such as virtual shopping stores, social networks, and games. Recent work showed that compared to traditional graphical and alphanumeric passwords, a more promising form of passwords for IVR is 3D passwords. This work evaluates four multimodal techniques for entering 3D passwords in IVR that consist of multiple virtual objects selected in succession. Namely, we compare eye gaze and head pose for pointing, and dwell time and tactile input for selection. A comparison of a) usability in terms of entry time, error rate, and memorability, and b) resistance to real world and offline observations, reveals that: multimodal authentication in IVR by pointing at targets using gaze, and selecting them using a handheld controller significantly improves usability and security compared to the other methods and to prior work. We discuss how the choice of pointing and selection methods impacts the usability and security of 3D passwords in IVR.
SP  - 61
EP  - 81
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58465-8_5
ER  - 

TY  - JOUR
AU  - Esteves, Augusto; Bouquet, Elizabeth; Pfeuffer, Ken; Alt, Florian
TI  - One-handed Input for Mobile Devices via Motion Matching and Orbits Controls
PY  - 2022
AB  - <jats:p>We introduce a novel one-handed input technique for mobile devices that is not based on pointing, but on motion matching -where users select a target by mimicking its unique animation. Our work is motivated by the findings of a survey (N=201) on current mobile use, from which we identify lingering opportunities for one-handed input techniques. We follow by expanding on current motion matching implementations - previously developed in the context of gaze or mid-air input - so these take advantage of the affordances of touch-input devices. We validate the technique by characterizing user performance via a standard selection task (N=24) where we report success rates (&gt;95%), selection times (~1.6 s), input footprint, grip stability, usability, and subjective workload - in both phone and tablet conditions. Finally, we present a design space that illustrates six ways in which motion matching can be embedded into mobile interfaces via a camera prototype application.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534624
ER  - 

TY  - JOUR
AU  - Marandi, Ramtin Zargari; Gazerani, Parisa
TI  - Aging and eye tracking: in the quest for objective biomarkers
PY  - 2019
AB  - <jats:p> Recent applications of eye tracking for diagnosis, prognosis and follow-up of therapy in age-related neurological or psychological deficits have been reviewed. The review is focused on active aging, neurodegeneration and cognitive impairments. The potential impacts and current limitations of using characterizing features of eye movements and pupillary responses (oculometrics) as objective biomarkers in the context of aging are discussed. A closer look into the findings, especially with respect to cognitive impairments, suggests that eye tracking is an invaluable technique to study hidden aspects of aging that have not been revealed using any other noninvasive tool. Future research should involve a wider variety of oculometrics, in addition to saccadic metrics and pupillary responses, including nonlinear and combinatorial features as well as blink- and fixation-related metrics to develop biomarkers to trace age-related irregularities associated with cognitive and neural deficits. </jats:p>
SP  - FNL33
EP  - NA
JF  - Future Neurology
VL  - 14
IS  - 4
PB  - 
DO  - 10.2217/fnl-2019-0012
ER  - 

TY  - NA
AU  - Chen, Bo-Han; Wong, Sai-Keung; Chang, Wei-Che; Fan, Roy Ping-Hao
TI  - UIST (Adjunct Volume) - Towards Social Interaction between 1st and 2nd Person Perspectives on Bodily Play
PY  - 2021
AB  - Bodily play, which is a productive social interaction for bonding social relationships, has positive impacts on self-efficacy, acute cognitive benefit, and emotion. However, most bodily play encourages players to enjoy their own experiences. There are limited researches on sharing players' perspectives to enhance players' empathy for understanding others. Thus, we propose an asymmetric two-person game in an immersive environment. This bodily play, which supports perspective-taking via the integration with the first- and second-perspectives, has a collaborative interface that allows users to share their physiological and emotional perspectives. Initial testing of the system shows that players can not only understand well the feeling and problems encountered by each other through sharing perspectives and information but also increases the social closeness of players and stimulates empathy after the interplay.
SP  - 1
EP  - 3
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480211
ER  - 

TY  - NA
AU  - Lopes, Pedro; You, Sijing; Ion, Alexandra; Baudisch, Patrick
TI  - CHI - Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation
PY  - 2018
AB  - We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.
SP  - 446
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174020
ER  - 

TY  - NA
AU  - Barreto, Mary; Casado-Mansilla, Diego; Esteves, Augusto; Magno de Gouveia Quintal, Filipe
TI  - Designing Smart Plugs for Interactivity and Energy Sustainability via a Survey and Thematic Analysis
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546155.3546681
ER  - 

TY  - BOOK
AU  - Barz, Michael; Daiber, Florian; Sonntag, Daniel; Bulling, Andreas
TI  - ETRA - Error-aware gaze-based interfaces for robust mobile gaze interaction
PY  - 2018
AB  - Gaze estimation error can severely hamper usability and performance of mobile gaze-based interfaces given that the error varies constantly for different interaction positions. In this work, we explore error-aware gaze-based interfaces that estimate and adapt to gaze estimation error on-the-fly. We implement a sample error-aware user interface for gaze-based selection and different error compensation methods: a naive approach that increases component size directly proportional to the absolute error, a recent model by Feit et al. that is based on the two-dimensional error distribution, and a novel predictive model that shifts gaze by a directional error estimate. We evaluate these models in a 12-participant user study and show that our predictive model significantly outperforms the others in terms of selection rate, particularly for small gaze targets. These results underline both the feasibility and potential of next generation error-aware gaze-based user interfaces.
SP  - 24
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204536
ER  - 

TY  - JOUR
AU  - Park, Kyudong; Kim, Do-Hyeon; Han, Sung H.
TI  - Usability of the size, spacing, and operation method of virtual buttons with virtual hand on head-mounted displays
PY  - 2020
AB  - NA
SP  - 102939
EP  - NA
JF  - International Journal of Industrial Ergonomics
VL  - 76
IS  - NA
PB  - 
DO  - 10.1016/j.ergon.2020.102939
ER  - 

TY  - NA
AU  - Tamaki, Emi; Chan, Terence; Iwasaki, Ken
TI  - UIST (Adjunct Volume) - UnlimitedHand: Input and Output Hand Gestures with Less Calibration Time
PY  - 2016
AB  - Numerous devices that either track hand gestures or provide haptic feedback have been developed with the aim of manipulating objects within Virtual Reality(VR) and Augmented Reality(AR) environments. However, these devices implement lengthy calibration processes to ease out individual differences. In this research, a wearable device that simultaneously recognizes hand gestures and outputs haptic feedback: UnlimitedHand is suggested. Photo-reflectors are placed over specific muscle groups on the forearm to read in hand gestures. For output, electrodes are placed over the same muscles to control the user's hand movements. Both sensors and electrodes target main muscle groups responsible for moving the hand. Since the positions of these muscle groups are common between humans, UnlimitedHand is able to reduce the time spent on performing calibration.
SP  - 163
EP  - 165
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985743
ER  - 

TY  - NA
AU  - Jain, Shubham; Sharma, Shubham; Babbar, Dhawal
TI  - Tangible and Embedded Interaction - Star-Force: A Playful Implementation of the Jedi-force
PY  - 2017
AB  - We introduce Star-force, which is a playful implementation of sci-fi concept The Force, as envisioned in the Star Wars saga. This is achieved via a battle environment instantiated in mixed reality using a video see-through head-mounted display. The "weapons", i.e., the Light-Sabers, Blasters are based on the ESP8266 microcontroller and tracked using makers and external cameras. Through their headsets, users see the virtual Light-Sabers as beams of light emanating from their handheld controllers. Depending on the actions, such as contact between two Light Sabers, we trigger force-feedback on the player's hands via electrical muscle stimulation. This emulates the sensation of the two powerful beams of light hitting each other.
SP  - 761
EP  - 766
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3041098
ER  - 

TY  - JOUR
AU  - McKenzie, Ross M.; Roberts, Jamie O.; Sayed, Mohammed E.; Stokes, Adam A.
TI  - PeriSim: A Simulator for Optimizing Peristaltic Table Control
PY  - 2019
AB  - NA
SP  - 1900070
EP  - NA
JF  - Advanced Intelligent Systems
VL  - 1
IS  - 8
PB  - 
DO  - 10.1002/aisy.201900070
ER  - 

TY  - JOUR
AU  - Hassoumi, Almoctar; Peysakhovich, Vsevolod; Hurter, Christophe
TI  - Improving eye-tracking calibration accuracy using symbolic regression.
PY  - 2019
AB  - Eye tracking systems have recently experienced a diversity of novel calibration procedures, including smooth pursuit and vestibulo-ocular reflex based calibrations. These approaches allowed collecting more data compared to the standard 9-point calibration. However, the computation of the mapping function which provides planar gaze positions from pupil features given as input is mostly based on polynomial regressions, and little work has investigated alternative approaches. This paper fills this gap by providing a new calibration computation method based on symbolic regression. Instead of making prior assumptions on the polynomial transfer function between input and output records, symbolic regression seeks an optimal model among different types of functions and their combinations. This approach offers an interesting perspective in terms of flexibility and accuracy. Therefore, we designed two experiments in which we collected ground truth data to compare vestibulo-ocular and smooth pursuit calibrations based on symbolic regression, both using a marker or a finger as a target, resulting in four different calibrations. As a result, we improved calibration accuracy by more than 30%, with reasonable extra computation time.
SP  - e0213675
EP  - NA
JF  - PloS one
VL  - 14
IS  - 3
PB  - 
DO  - 10.1371/journal.pone.0213675
ER  - 

TY  - NA
AU  - Almoctar, Hassoumi; Irani, Pourang; Peysakhovich, Vsevolod; Hurter, Christophe
TI  - ICMI - Path Word: A Multimodal Password Entry Method for Ad-hoc Authentication Based on Digits' Shape and Smooth Pursuit Eye Movements
PY  - 2018
AB  - We present PathWord (PATH passWORD), a multimodal digit entry method for ad-hoc authentication based on known digits shape and user relative eye movements. PathWord is a touch-free, gaze-based input modality, which attempts to decrease shoulder surfing attacks when unlocking a system using PINs. The system uses a modified web camera to detect the user's eye. This enables suppressing direct touch, making it difficult for passer-bys to be aware of the input digits, thus reducing shoulder surfing and smudge attacks. In addition to showing high accuracy rates (Study 1: 87.1% successful entries) and strong confidentiality through detailed evaluations with 42 participants (Study 2), we demonstrate how PathWord considerably diminishes the potential of stolen passwords (on average 2.38% stolen passwords with PathWord vs. over 90% with traditional PIN screen). We show use-cases of PathWord and discuss its advantages over traditional input modalities. We envision PathWord as a method to foster confidence while unlocking a system through gaze gestures.
SP  - 268
EP  - 277
JF  - Proceedings of the 20th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242969.3243008
ER  - 

TY  - NA
AU  - Torres, César I.
TI  - Hybrid Aesthetics: Bridging Material Practices and Digital Fabrication through Computational Crafting Proxies
PY  - 2019
AB  - Author(s): Torres, Cesar Armando | Advisor(s): Paulos, Eric | Abstract: Creative technologies like digital fabrication led to the rise of the Maker movement, engendering grassroots innovation in education, manufacturing, and healthcare. Today, these creative technologies stand at a crossroads – despite a significant rise in participation, a deeper engagement with design and material is absent from traditional computer-aided design workflows. In this thesis, I will motivate the need for creative technologies to support the morphogenetic model of making, a thinking and working style characteristic of how practitioners work with physical materials but difficult to access in digital design tools. To communicate my findings, I introduce the concept of a Crafting Proxy, an intermediary between a practitioner and a material that can be used to facilitate the interpretation, manipulation, and evaluation of a material as a part of a creative process. In these works, I employ a Research through Design (RtD) methodology to construct intermediate-level knowledge around the design, implementation, and evaluation of Crafting Proxies. I’ll demonstrate how Crafting Proxies can be enacted within physical materials, physical tools, and physical practices to support morphogenetic workflows in domains such as light and heater design, and metalworking. As a result, this work contributes a design method for creating crafting proxies and a set of design principles that inform how new materials and digital fabrication technologies can foreground the existing knowledge and practices of material practitioners and generate new forms and aesthetics that can alter the trajectory of the Maker movement towards a New Making Renaissance.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - An, Byoungkwon; Tao, Ye; Gu, Jianzhe; Cheng, Tingyu; Chen, Xiang 'Anthony'; Zhang, Xiaoxiao; Zhao, Wei; Do, Youngwook; Takahashi, Shigeo; Wu, Hsiang-Yun; Zhang, Teng; Yao, Lining
TI  - CHI - Thermorph: Democratizing 4D Printing of Self-Folding Materials and Interfaces
PY  - 2018
AB  - We develop a novel method printing complex self-folding geometries. We demonstrated that with a desktop fused deposition modeling (FDM) 3D printer, off-the-shelf printing filaments and a design editor, we can print flat thermoplastic composites and trigger them to self-fold into 3D with arbitrary bending angles. This is a suitable technique, called Thermorph, to prototype hollow and foldable 3D shapes without losing key features. We describe a new curved folding origami design algorithm, compiling given arbitrary 3D models to 2D unfolded models in G-Code for FDM printers. To demonstrate the Thermorph platform, we designed and printed complex self-folding geometries (up to 70 faces), including 15 self-curved geometric primitives and 4 self-curved applications, such as chairs, the simplified Stanford Bunny and flowers. Compared to the standard 3D printing, our method saves up to 60% - 87% of the printing time for all shapes chosen.
SP  - 260
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173834
ER  - 

TY  - NA
AU  - Do, Seungwon; Lee, Byungjoo
TI  - CHI - Improving Reliability of Virtual Collision Responses: A Cue Integration Technique
PY  - 2020
AB  - In virtual reality (VR), a user's virtual avatar can interact with a virtual object by colliding with it. If collision responses do not occur in the direction that the user expects, the user experiences degradation of accuracy and precision in applications such as VR sports games. In determining the response of a virtual collision, existing physics engines have not considered the direction in which the user perceived and estimated the collision. Based on the cue integration theory, this study presents a statistical model explaining how users estimate the direction of a virtual collision from their body's orientation and velocity vectors. The accuracy and precision of virtual collisions can be improved by 8.77% and 30.29%, respectively, by setting the virtual collision response in the direction that users perceive.
SP  - 3376819
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376819
ER  - 

TY  - BOOK
AU  - Jones, Lee; McClelland, John C.; Thongsouksanoumane, Phonesavanh; Girouard, Audrey
TI  - ISS - Ambient Notifications with Shape Changing Circuits in Peripheral Locations
PY  - 2017
AB  - Calm technologies help us avoid distraction by embedding notifications in our surroundings with peripheral updates. However, users also lose out on the passive awareness that comes from more overt notifications. In our paper, we present an initial study setup on shape changing circuits as notifications. We compare near and far peripheral locations to determine the optimal location for these notifications by assigning a primary task of arithmetic questions, and a secondary task of responding to bend notifications. Our demonstration will show the set-up of our study to encourage discussion on possible applications of shape changing notifications in peripheral locations.
SP  - 405
EP  - 408
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3132291
ER  - 

TY  - JOUR
AU  - Niu, Yafeng; Li, Xin; Yang, Wenjun; Xue, Chengqi; Peng, Ningyue; Jin, Tao
TI  - Smooth Pursuit Study on an Eye-Control System for Continuous Variable Adjustment Tasks
PY  - 2021
AB  - NA
SP  - 23
EP  - 33
JF  - International Journal of Human–Computer Interaction
VL  - 39
IS  - 1
PB  - 
DO  - 10.1080/10447318.2021.2012979
ER  - 

TY  - JOUR
AU  - Ricca, Aylen; Chellali, Amine; Otmane, Samir
TI  - Comparing touch-based and head-tracking navigation techniques in a virtual reality biopsy simulator
PY  - 2020
AB  - Recently, virtual reality (VR) technologies started gaining momentum in surgical simulation-based training by allowing clinicians to practice their skills before performing real procedures. The design of such simulators is usually focused on the primary operative tasks to be taught, but little attention is paid to secondary tasks that the user needs to perform, such as changing his/her point of view when manipulating the surgical instruments. More particularly, it is not clear how to design appropriate interaction techniques for those tasks, and how the fidelity of these interactions can impact the user’s performance on such systems. In this paper, we compare two viewpoint changing techniques having two different levels of interaction fidelity during needle insertion in a semi-immersive VR (SIVR) biopsy trainer. These techniques were designed based on observing clinicians performing actual biopsy procedures. The first technique is based on tracking the user’s head position (high interaction fidelity), while the second technique is touch-based with the user utilizing his/her non-dominant hand fingers to manipulate the point of view on a touch screen (moderate interaction fidelity). A user study was carried out to investigate the impact of the interaction fidelity of the viewpoint changing task (secondary task) on the user’s performance during the needle insertion task (main task). Twenty-one novice participants were asked to perform several trials of a needle insertion task while using the navigation techniques (within-subject design). Objective and subjective measures were recorded to compare the task performance (time to accomplish the task, precision of the tumor sampling, and errors) and user experience for both techniques. The results show that the touch-based viewpoint changing technique improves the users’ task completion performance during needle insertion while maintaining a similar level of needle manipulation accuracy as compared to the head-tracking technique. These results suggest that high interaction fidelity is not always necessary when designing surgical trainers. This also highlights the importance of designing appropriate interactions for secondary tasks because they can influence the user’s primary task performance in VR simulators.
SP  - 191
EP  - 208
JF  - Virtual Reality
VL  - 25
IS  - 1
PB  - 
DO  - 10.1007/s10055-020-00445-7
ER  - 

TY  - NA
AU  - Tatsuno, Sho; Hayakawa, Tomohiko; Ishikawa, Masatoshi
TI  - AH - Trajectory adjustment system for learning based on electrical stimulation
PY  - 2017
AB  - Electrical stimulation is a well-known technology in medicine used for stimulating human muscles that has been applied in rehabilitation. Recently, electrical stimulation has been spotlighted for generating haptic sensations in human interface applications. Existing methods of generating haptic sensations are mainly mechanical. However, electrical stimulation can generate much stronger stimulation than mechanical force generators with the same energy. In this study, we consider applications of electrical stimulation to skill learning. Through simple tasks to learn trajectory, we assessed the learning rate using electrical stimulation compared with learning using vibration.
SP  - 28
EP  - NA
JF  - Proceedings of the 8th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3041164.3041197
ER  - 

TY  - JOUR
AU  - Pan, Wei; Chen, Lujie; Dritsas, Stylianos
TI  - Pick-and-place process sequencing for transformation of rasterized 3D structures
PY  - 2017
AB  - NA
SP  - 56
EP  - 64
JF  - Automation in Construction
VL  - 75
IS  - NA
PB  - 
DO  - 10.1016/j.autcon.2016.12.007
ER  - 

TY  - NA
AU  - Fast, Ethan; Bernstein, Michael S.
TI  - UIST - Meta: Enabling Programming Languages to Learn from the Crowd
PY  - 2016
AB  - Collectively authored programming resources such as Q&A sites and open-source libraries provide a limited window into how programs are constructed, debugged, and run. To address these limitations, we introduce Meta: a language extension for Python that allows programmers to share functions and track how they are used by a crowd of other programmers. Meta functions are shareable via URL and instrumented to record runtime data. Combining thousands of Meta functions with their collective runtime data, we demonstrate tools including an optimizer that replaces your function with a more efficient version written by someone else, an auto-patcher that saves your program from crashing by finding equivalent functions in the community, and a proactive linter that warns you when a function fails elsewhere in the community. We find that professional programmers are able to use Meta for complex tasks (creating new Meta functions that, for example, cross-validate a logistic regression), and that Meta is able to find 44 optimizations (for a 1.45 times average speedup) and 5 bug fixes across the crowd.
SP  - 259
EP  - 270
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984532
ER  - 

TY  - NA
AU  - Aghayi, Emad; LaToza, Thomas D.; Surendra, Paurav; Abolghasemi, Seyedmeysam
TI  - Implementing Microservices through Microtasks.
PY  - 2019
AB  - Microtask programming is a form of crowdsourcing for programming in which implementation work is decomposed into short, self-contained microtasks. Each microtask offers a specific goal (e.g., write a unit test) as well as all of the required context and environment support necessary to accomplish this goal. Key to microtasking is the choice of workflow, which delineates the microtasks developers may complete and how contributions from each are aggregated to generate the final software product. Existing approaches either rely on a single developer to manually generate all microtasks, limiting their potential scalability, or impose coordination requirements which limit their effectiveness. Inspired by behavior-driven development, we describe a novel workflow for decomposing programming into microtasks in which each microtask involves identifying, testing, implementing, and debugging an individual behavior within a single function. We apply this approach to the implementation of microservices, demonstrating the first approach for implementing a microservice through microtasks. To evaluate our approach, we conducted a user study in which a small crowd worked to implement a simple microservice and test suite. We found that the crowd was able to use a behavior-driven microtask workflow to successfully complete 350 microtasks and implement 13 functions, quickly onboard and submit their first microtask in less than 24 minutes, contribute new behaviors in less than 5 minutes, and together implement a functioning microservice with only four defects. We discuss these findings and their implications for incorporating microtask work into open source projects.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yi, Xin; Lu, Yiqin; Cai, Ziyin; Wu, Zihan; Wang, Yuntao; Shi, Yuanchun
TI  - GazeDock: Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu
PY  - 2022
AB  - Gaze-only input techniques in VR face the challenge of avoiding false triggering due to continuous eye tracking while maintaining interaction performance. In this paper, we proposed GazeDock, a technique for enabling fast and robust gaze-based menu selection in VR. GazeDock features a view-fixed peripheral menu layout that automatically triggers appearing and selection when the user&#x2019;s gaze approaches and leaves the menu zone, thus facilitating interaction speed and minimizing the false triggering rate. We built a dataset of 12 participants&#x2019; natural gaze movements in typical VR applications. By analyzing their gaze movement patterns, we designed the menu UI personalization and optimized selection detection algorithm of GazeDock. We also examined users&#x2019; gaze selection precision for targets on the peripheral menu and found that 4&#x2013;8 menu items yield the highest throughput when considering both speed and accuracy. Finally, we validated the usability of GazeDock in a VR navigation game that contains both scene exploration and menu selection. Results showed that GazeDock achieved an average selection time of 471ms and a false triggering rate of 3.6%. And it received higher user preference ratings compared with dwell-based and pursuit-based techniques.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00105
ER  - 

TY  - NA
AU  - Braley, Sean; Rubens, Calvin; Merritt, Timothy; Vertegaal, Roel
TI  - UIST - GridDrones: A Self-Levitating Physical Voxel Lattice for Interactive 3D Surface Deformations
PY  - 2018
AB  - We present GridDrones, a self-levitating programmable matter platform that can be used for representing 2.5D voxel grid relief maps capable of rendering unsupported structures and 3D transformations. GridDrones consists of cube-shaped nanocopters that can be placed in a volumetric 1xnxn mid-air grid, which is demonstrated here with 15 voxels. The number of voxels and scale is only limited by the size of the room and budget. Grid deformations can be applied interactively to this voxel lattice by manually selecting a set of voxels, then assigning a continuous topological relationship between voxel sets that determines how voxels move in relation to each other and manually drawing out selected voxels from the lattice structure. Using this simple technique, it is possible to create unsupported structures that can be translated and oriented freely in 3D. Shape transformations can also be recorded to allow for simple physical shape morphing animations. This work extends previous work on selection and editing techniques for 3D user interfaces.
SP  - 87
EP  - 98
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242658
ER  - 

TY  - BOOK
AU  - Pfeiffer, Max; George, Niklas; Degbelo, Auriol
TI  - Mensch &amp; Computer - EMS-based actuated output gestures: a design process for novices
PY  - 2020
AB  - Electrical Muscle Stimulation (EMS) has been gaining increasing attention in Human-Computer Interaction (HCI), owing to its potential to generate more expressive haptic feedback. Despite many hardware and software prototypes developed to study specific interaction aspects, designing EMS-based actuated gestures remains a complex endeavor. In this work, we present a design process for actuated gestures and a mobile app to support researchers and gesture designers during EMS-based gesture creation. The app covers the individual calibration of muscles, the composition of single movements into more complex gestures, and the playback of the pre-calibrated gestures. A user study with 12 participants, mostly non-familiar with EMS, has shown that the app was successful in supporting participants test actuated gestures. Our main contributions include a design process for actuated gestures, as well as an open-source app to support the approach and its first evaluation.
SP  - 409
EP  - 413
JF  - Proceedings of the Conference on Mensch und Computer
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3404983.3409995
ER  - 

TY  - JOUR
AU  - Singh, Ronal; Miller, Tim; Newn, Joshua; Velloso, Eduardo; Vetere, Frank; Sonenberg, Liz
TI  - Combining gaze and AI planning for online human intention recognition
PY  - 2020
AB  - NA
SP  - 103275
EP  - NA
JF  - Artificial Intelligence
VL  - 284
IS  - NA
PB  - 
DO  - 10.1016/j.artint.2020.103275
ER  - 

TY  - JOUR
AU  - Kiziroglou, Michail E.; Temelkuran, Burak; Yeatman, Eric M.; Yang, Guang-Zhong
TI  - Micro Motion Amplification–A Review
PY  - 2020
AB  - Many motion-active materials have recently emerged, with new methods of integration into actuator components and systems-on-chip. Along with established microprocessors, interconnectivity capabilities and emerging powering methods, they offer a unique opportunity for the development of interactive millimeter and micrometer scale systems with combined sensing and actuating capabilities. The amplification of nanoscale material motion to a functional range is a key requirement for motion interaction and practical applications, including medical micro-robotics, micro-vehicles and micro-motion energy harvesting. Motion amplification concepts include various types of leverage, flextensional mechanisms, unimorphs, micro-walking /micro-motor systems, and structural resonance. A review of the research state-of-art and product availability shows that the available mechanisms offer a motion gain in the range of 10. The limiting factor is the aspect ratio of the moving structure that is achievable in the microscale. Flexures offer high gains because they allow the application of input displacement in the close vicinity of an effective pivotal point. They also involve simple and monolithic fabrication methods allowing combination of multiple amplification stages. Currently, commercially available motion amplifiers can provide strokes as high as 2% of their size. The combination of high-force piezoelectric stacks or unimorph beams with compliant structure optimization methods is expected to make available a new class of high-performance motion translators for microsystems.
SP  - 64037
EP  - 64055
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.2984606
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Liu, Yingda; Nelson-Arzuaga, Chloe; Ishii, Hiroshi
TI  - Tangible and Embedded Interaction - TRANS-DOCK: Expanding the Interactivity of Pin-based Shape Displays by Docking Mechanical Transducers
PY  - 2020
AB  - This paper introduces TRANS-DOCK, a docking system for pin-based shape displays that enhances their interaction capabilities for both the output and input. By simply interchanging the transducer module, composed of passive mechanical structures, to be docked on a shape display, users can selectively switch between different configurations including display sizes, resolutions, and even motion modalities to allow pins moving in a linear motion to rotate, bend and inflate. We introduce a design space consisting of several mechanical elements and enabled interaction capabilities. We then explain the implementation of the docking system and transducer design components. Our implementation includes providing the limitations and characteristics of each motion transmission method as design guidelines. A number of transducer examples are then shown to demonstrate the range of interactivity and application space achieved with the approach of TRANS-DOCK. Potential use cases to take advantage of the interchangeability of our approach are discussed. Through this paper we intend to expand expressibility, adaptability and customizability of a single shape display for dynamic physical interaction. By converting arrays of linear motion to several types of dynamic motion in an adaptable and flexible manner, we advance shape displays to enable versatile embodied interactions.
SP  - 131
EP  - 142
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374933
ER  - 

TY  - JOUR
AU  - Evripidou, Salomi; Amanatiadis, Angelos; Christodoulou, Klitos; Chatzichristofis, Savvas A.
TI  - Introducing Algorithmic Thinking and Sequencing Using Tangible Robots
PY  - 2021
AB  - Today, in the era of robotics, different types of educational robots have been used extensively in school classrooms to facilitate teaching activities related to a variety of computer science concepts. Numerous studies have been performed that attempt to examine the effects of using tangible interfaces to enhance collaborative learning experiences. In most of these studies, feedback, which is a vital function for a successful game activity, is mainly provided by the trainers. However, this kind of feedback can be considered as static and general, while each trainee seeks clear, consistent, and even personalized feedback. This article proposes an interactive learning tool for introducing algorithmic thinking and sequencing using educational robots suitable for elementary and intermediate students. In more detail, in this article, we leverage a fuzzy-rule-based system and computer vision techniques to provide immediate, personalized feedback and recommendations to young students while they perform a series of activities using tangible robots. These activities relate to teaching programming skills and improve the algorithmic thinking of students. Experimental results revealed that participants were able to increase their algorithmic/programming thinking skills while developing a positive attitude toward programming. The interactive gaming factor that is embedded in the use of tangible robots, while participating in the activities, was proved to be a compelling and a rewarding experience. The article concludes that the use of the proposed feedback mechanism, when placed in a robot game environment, can lead to a positive and more effective learning process.
SP  - 93
EP  - 105
JF  - IEEE Transactions on Learning Technologies
VL  - 14
IS  - 1
PB  - 
DO  - 10.1109/tlt.2021.3058060
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Alexander, Jason; Gellersen, Hans
TI  - MUM - GazeArchers: playing with individual and shared attention in a two-player look&shoot tabletop game
PY  - 2016
AB  - Gaze can complement touch on surfaces for fast target selection and occlusion-free input. In this work, we look beyond single-user application of gaze and touch and explore how gaze can be leveraged for collaborative use. We present the design of a two-player shooter game in which targets are gaze-aware and able to react differently to attention by one of the players versus shared attention of both players. The game-play, evaluated in a study with 14 users, encourages users to adopt different strategies switching between individual and shared attention to achieve their collaborative goal.
SP  - 213
EP  - 216
JF  - Proceedings of the 15th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3012709.3012717
ER  - 

TY  - NA
AU  - Muender, Thomas; Bonfert, Michael; Reinschluessel, Anke Verena; Malaka, Rainer; Döring, Tanja
TI  - Haptic Fidelity Framework: Defining the Factors of Realistic Haptic Feedback for Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501953
ER  - 

TY  - NA
AU  - Fuchs, Alexandra; Sturdee, Miriam; Schöning, Johannes
TI  - NordiCHI - Foldwatch: using origami-inspired paper prototypes to explore the extension of output space in smartwatches
PY  - 2018
AB  - Smartwatches are highly portable, ubiquitous devices, allowing rich interaction at a small scale. However, the display size can hinder user engagement, limit information display, and presentation style. Most research focuses on exploring ways in which the interaction area of smartwatches can be extended, although this mainly entails simple fold-out displays or additional screens. Conversely, added weight and size can hinder the wearable experience. In response, we took inspiration from origami and explored the design space for new types of lightweight, highly foldable smartwatch, by developing complex paper-prototypes which demonstrate novel ways of extending screen space. We collected data on potential input and output interaction with complex folded smartwatch displays during workshops with expert and non-expert users, discovering application ideas and additional input/output functionality. These insights were used to produce and evaluate a concept video for the FoldWatch prototype.
SP  - 47
EP  - 59
JF  - Proceedings of the 10th Nordic Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3240167.3240173
ER  - 

TY  - NA
AU  - Sukawati, Irawan Dharma; Wibirama, Sunu; Setiawan, Noor Akhmad; Amin, Muhamad Kamal Mohammed
TI  - A Survey of Signal Processing Filters, Calibration, and Interactive Applications based on Smooth Pursuit Eye Movement
PY  - 2019
AB  - Eye tracking has been considered as an important modality of interaction between human and computer. Three types of eye movement commonly used in interactive applications are fixation, saccadic, and smooth pursuit. Fixational eye movement has been considered as the most common technique in gaze-based interactive applications. Nevertheless, fixation-based interaction requires conventional eye tracking calibration, which limits its practicality when the interactive applications have to be used in a public space. In the last decade, smooth pursuit-based interaction has emerged as a new interaction technique. This method enables spontaneous interaction with minimum calibration targets, allowing the users to be calibrated faster or even interacting with the applications without prior calibration. Despite of its emerging popularity, there is no a review study that observes various techniques behind this interaction technology. To fill this gap, this paper presents a review on signal processing filters, calibration methods, and different interactive applications based on smooth pursuit eye movement. This review may be used as a guideline to start a new research project on interactive application based on smooth pursuit eye movement.
SP  - 1
EP  - 6
JF  - 2019 5th International Conference on Science and Technology (ICST)
VL  - 1
IS  - NA
PB  - 
DO  - 10.1109/icst47872.2019.9166280
ER  - 

TY  - NA
AU  - Pfeiffer, Max; Kröger, Tobias; Seifert, Jens; Somaskantharajan, Sulaxan; Jahnich, Lukas; Steinblum, Tobias; Speckamp, Jan; Medrano, Samuel Navas
TI  - CHI Extended Abstracts - WONDER -- Enhancing VR Training with Electrical Muscle Stimulation
PY  - 2019
AB  - Training employees on workplace procedures in virtual environments (VEs) is becoming popular since it reduces cost and risks. Although haptic enhancements with force feedback make such VEs more realistic and increase performance. Such enhancements are only available for 'spatial' scenarios. One potential enhancement for low-cost VEs is electrical muscle stimulation (EMS), but it remains open how EMS can be used to support trainees. Therefore we present WONDER: A virtual training environment with an EMS feedback enhancing layer. In an initial study, we show the feasibility of the approach and that it can successfully support trainees in remembering workflows. We test feedback that supports participants by pushing their hand towards a button or pulling their hand away from it. Participants preferred a combination of both feedback types.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312780
ER  - 

TY  - NA
AU  - Zhu, Yeshuang; Yue, Shichao; Yu, Chun; Shi, Yuanchun
TI  - CSCW - CEPT: Collaborative Editing Tool for Non-Native Authors
PY  - 2017
AB  - Due to language deficiencies, individual non-native speakers (NNS) face many difficulties while writing. In this paper, we propose to build a collaborative editing system that aims to facilitate the sharing of language knowledge among non-native co-authors, with the ultimate goal of improving writing quality. We describe CEPT, which allows individual co-authors to generate their own revisions as well as incorporating edits from others to achieve mutual inspiration. The main technical challenge is how to aggregate edits of multiple co-authors and present them in an easy-to-understand way. After iterative design, CEPT highlights three novel features: 1) cross-version sentence mapping for edit tracking, 2) summarization of edits from multiple co-authors, and 3) a collaborative editing interface that enables co-authors to examine, comment on, and borrow edits of others. A preliminary lab study showed that CEPT could significantly improve both the language quality and collaboration experience of NNS writers, due to its efficacy for sharing language knowledge.
SP  - 273
EP  - 285
JF  - Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2998181.2998306
ER  - 

TY  - CHAP
AU  - Khakurel, Jayden; Knutas, Antti; Melkas, Helinä; Penzenstadler, Birgit; Fu, Bo; Porras, Jari
TI  - HCI (7) - Categorization Framework for Usability Issues of Smartwatches and Pedometers for the Older Adults
PY  - 2018
AB  - In recent years various usability issues related to device characteristics of quantified-self wearables such as smartwatches and pedometers have been identified which appear likely to impact device adoption among the older adults. However, an overall framework has not yet been developed to provide a comprehensive set of usability issues related to smartwatches and pedometers. This study used a two-stage research approach with 33 older participants, applying contextual action theory and usability evaluation methods both to determine perceived usability issues and to formulate a usability categorization framework based on identified issues. Additionally, we prioritized the predominant usability issues of smartwatches and pedometers that warrant immediate attention from technology designers, the research community, and application developers. Results revealed predominant usability issues related to the following device characteristics of smartwatches: user interface (font size, interaction techniques such as notification, button location) and hardware (screen size); and of pedometers: user interface (font size, interaction techniques such as notification, button location, and tap detection) and hardware (screen size).
SP  - 91
EP  - 106
JF  - Universal Access in Human-Computer Interaction. Methods, Technologies, and Users
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-92049-8_7
ER  - 

TY  - NA
AU  - Verweij, David; Esteves, Augusto; Khan, Vassilis-Javed; Bakker, Saskia
TI  - CHI Extended Abstracts - WaveTrace: Motion Matching Input using Wrist-Worn Motion Sensors
PY  - 2017
AB  - We present WaveTrace, a novel interaction technique based on selection by motion matching. In motion matching systems, targets move continuously in a singular and pre-defined path -- users interact with these by performing a synchronous bodily movement that matches the movement of one of the targets. Unlike previous work which tracks user input through optical systems, WaveTrace is arguably the first motion matching technique to rely on motion data from inertial measurement units readily available in many wrist-worn wearable devices such as smart watches. To evaluate the technique, we conducted a user study in which we varied: hand; degrees of visual angle; target speed; and number of concurrent targets. Preliminary results indicate that the technique supports up to eight concurrent targets; and that participants could select targets moving at speeds between 180 and 270/s (mean acquisition time of 2237ms, and average success rate of 91%).
SP  - 2180
EP  - 2186
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053161
ER  - 

TY  - NA
AU  - Li, Richard; Whitmire, Eric; Stengel, Michael; Boudaoud, Ben; Kautz, Jan; Luebke, David; Patel, Shwetak N.; Akşit, Kaan
TI  - ISMAR - Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors
PY  - 2020
AB  - Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of $2. 67^{\circ}$ at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of $1. 57^{\circ}$ at 250 Hz using 800 mW.
SP  - 117
EP  - 126
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00033
ER  - 

TY  - JOUR
AU  - Juravle, Georgiana; Binsted, Gordon; Spence, Charles
TI  - Tactile suppression in goal-directed movement.
PY  - 2016
AB  - Sharing numerous characteristics with suppression in the other senses, tactile suppression is a reliable phenomenon that accompanies movement. By investigating the simplest of movements (e.g., finger flexions), early research tried to explain the origins of the phenomenon in terms of motor command generation together with sensory reafference. Here, we review recent research that has delved into (naturalistic) goal-directed movements. In connection with goal-directed movement, tactile suppression is evident as a decrease in behavioural performance measured shortly prior to, and during, movement execution. It is also reflected in a consistent response bias highlighting the (perceptual) uncertainty of the movement. Goal-directed movement supports the forward model and establishes contextual influences as the defining influences on tactile suppression. Depending on the task at hand, people prioritize a certain percept during movement. Future research, we argue, should focus on studying naturalistic movements, or sequences of movements, that share a common meaning or goal.
SP  - 1060
EP  - 1076
JF  - Psychonomic bulletin & review
VL  - 24
IS  - 4
PB  - 
DO  - 10.3758/s13423-016-1203-6
ER  - 

TY  - NA
AU  - Newn, Joshua; Tag, Benjamin; Singh, Ronal; Velloso, Eduardo; Vetere, Frank
TI  - UbiComp/ISWC Adjunct - AI-mediated gaze-based intention recognition for smart eyewear: opportunities & challenges
PY  - 2019
AB  - Commercial smart eyewear products designed for day-today use have drastically improved in recent years. However, the current utility and applications of smart eyewear can be easily substituted with other smart wearables. In this paper, we propose the integration of an artificial agent capable of performing gaze-based intention recognition on smart eyewear to extend the device capabilities. As smart eyewear affords unobtrusive tracking of the user's gaze while the user interacts naturally with the world, it serves as a perfect platform to discreetly identify the user's intentions through gaze, allowing the agent to provide relevant, personalised and proactive assistance. We believe that integration our proposed agent in smart eyewear is an achievable goal in the coming years with the rapid progressions in computer vision, wearable technology and socially interactive artificial agents. This paper, therefore, discusses our proof-of-concept intention-aware agent, followed by its future opportunities and existing challenges for its integration in smart eyewear.
SP  - 637
EP  - 642
JF  - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341162.3348387
ER  - 

TY  - JOUR
AU  - Saga, Satoshi; Ikeda, Naoto
TI  - Dynamic Brake Control for a Wearable Impulsive Force Display by a String and a Brake System
PY  - 2021
AB  - <jats:p>In recent years, it has become possible to experience sports in the virtual reality (VR) space. Although many haptic displays in the VR environment currently use vibrators as the mainstream, the vibrators’ presentation is not suitable to express ball-receiving in the VR sports experience. Therefore, we have developed a novel haptic display that reproduces an impulsive force by instantaneously applying traction to the palm using a string and wearable brake system. This paper proposes a method to present various reaction forces by dynamic control of the braking system and report the quantitative evaluation of the device’s physical and psychological usability.</jats:p>
SP  - 1075
EP  - 1081
JF  - Journal of Robotics and Mechatronics
VL  - 33
IS  - 5
PB  - 
DO  - 10.20965/jrm.2021.p1075
ER  - 

TY  - NA
AU  - Outram, Benjamin I.; Pai, Yun Suen; Person, Tanner; Minamizawa, Kouta; Kunze, Kai
TI  - Anyorbit
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3209579
ER  - 

TY  - JOUR
AU  - Isomoto, Toshiya; Yamanaka, Shota; Shizuki, Buntarou
TI  - Dwell Selection with ML-based Intent Prediction Using Only Gaze Data
PY  - 2022
AB  - <jats:p>We developed a dwell selection system with ML-based prediction of a user's intent to select. Because a user perceives visual information through the eyes, precise prediction of a user's intent will be essential to the establishment of gaze-based interaction. Our system first detects a dwell to roughly screen the user's intent to select and then predicts the intent by using an ML-based prediction model. We created the intent prediction model from the results of an experiment with five different gaze-only tasks representing everyday situations. The intent prediction model resulted in an overall area under the curve (AUC) of the receiver operator characteristic curve of 0.903. Moreover, it could perform independently of the user (AUC=0.898) and the eye-tracker (AUC=0.880). In a performance evaluation experiment with real interactive situations, our dwell selection method had both higher qualitative and quantitative performance than previously proposed dwell selection methods.</jats:p>
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550301
ER  - 

TY  - BOOK
AU  - Gomez, Argenis Ramirez; Gellersen, Hans
TI  - ETRA - Smooth-i: smart re-calibration using smooth pursuit eye movements
PY  - 2018
AB  - Eye gaze for interaction is dependent on calibration. However, gaze calibration can deteriorate over time affecting the usability of the system. We propose to use motion matching of smooth pursuit eye movements and known motion on the display to determine when there is a drift in accuracy and use it as input for re-calibration. To explore this idea we developed Smooth-i, an algorithm that stores calibration points and updates them incrementally when inaccuracies are identified. To validate the accuracy of Smooth-i, we conducted a study with five participants and a remote eye tracker. A baseline calibration profile was used by all participants to test the accuracy of the Smooth-i re-calibration following interaction with moving targets. Results show that Smooth-i is able to manage re-calibration efficiently, updating the calibration profile only when inaccurate data samples are detected.
SP  - 10
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204585
ER  - 

TY  - NA
AU  - Bailey, Gavin; Sahoo, Deepak Ranjan; Jones, Matt
TI  - Conference on Designing Interactive Systems - Digital Bookmark: Seamless Switching Between Printed and Electronic Books
PY  - 2020
AB  - Recently, people prefer to read books via a combination of formats - E-Books along with printed books. We conducted a scoping survey and a lab-study which informed various inhibiting factors associated with switching between formats to conveniently use multiple formats. To improve the switching experience, we present Digital Bookmark that synchronises the current page location digitally between both printed and e-books. The page number of the printed book is electronically read using a conductive tag and transmitted to the e-book via the internet. The current location on the e-book is converted to the corresponding page number of the printed book and presented on the display of the Digital Bookmark. We present the results of a controlled lab-study to assess the parameters of switching between printed and electronic books. The initial feedback from a local reading group suggests that our Digital Bookmark would encourage multi-format reading and improve their user experience.
SP  - 885
EP  - 894
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395557
ER  - 

TY  - NA
AU  - Palonen, Tuomo
TI  - Augmented Reality Based Human Machine Interface for Semiautonomous Work Machines
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wang, Xi; Ley, Andreas; Koch, Sebastian; Lindlbauer, David; Hays, James; Holmqvist, Kenneth; Alexa, Marc
TI  - CHI - The Mental Image Revealed by Gaze Tracking
PY  - 2019
AB  - Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.
SP  - 609
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300839
ER  - 

TY  - NA
AU  - Sudo, Naoki; Fujiwara, Sei-etsu; Isoyama, Takashi; Fukayama, Osamu
TI  - Rattractor - Instant guidance of a rat into a virtual cage using a deep brain stimulation
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>We developed “Rattractor” (rat attractor), a system to apply electrical stimuli to the deep brain of a rat as it stays in a specified region or a virtual cage to demonstrate an instant electrophysiological feedback controller for animals.</jats:p><jats:p>Two wire electrodes were implanted in the brains of nine rats. The electrodes targeted the medial forebrain bundle (MFB), which is a part of the reward system in the deep brain. Following the recovery period, the rats were placed in a plain field where they could move freely, but wired to a stimulation circuit. An image sensor installed over the field detected the subject’s position, which triggered the stimulator such that the rat remained within the virtual cage. We conducted a behavioral experiment to evaluate the sojourn ratio of rats residing in the region. Thereafter, a histological analysis of the rat brain was performed to confirm the position of the stimulation sites in the brain.</jats:p><jats:p>Seven rats survived the surgery and the recovery period without technical failures such as connector breaks. We observed that three of them tended to stay in the virtual cage during stimulation, and this effect was maintained for two weeks. Histological analysis revealed that the electrode tips were correctly placed in the MFB region of the rats. The other four subjects showed no apparent preference for the virtual cage. In these rats, we did not find electrode tips in the MFB, or could not determine their positions.</jats:p><jats:p>Almost half of the rats tended to remain inside the virtual cage when position-related reward stimuli were triggered in the MFB region. Notably, our system did not require previous training or sequential interventions to affect the behavioral preferences of subjects. This process is similar to the situation in which sheep are chased by a shepherd dog in the desired direction.</jats:p>
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.1101/2022.10.07.511260
ER  - 

TY  - JOUR
AU  - Zhang, Tengxiang; Yi, Xin; Wang, Ruolin; Gao, Jiayuan; Wang, Yuntao; Yu, Chun; Li, Simin; Shi, Yuanchun
TI  - Facilitating Temporal Synchronous Target Selection through User Behavior Modeling
PY  - 2019
AB  - Temporal synchronous target selection is an association-free selection technique: users select a target by generating signals (e.g., finger taps and hand claps) in sync with its unique temporal pattern. However, classical pattern set design and input recognition algorithm of such techniques did not leverage users' behavioral information, which limits their robustness to imprecise inputs. In this paper, we improve these two key components by modeling users' interaction behavior. In the first user study, we asked users to tap a finger in sync with blinking patterns with various period and delay, and modeled their finger tapping ability using Gaussian distribution. Based on the results, we generated pattern sets for up to 22 targets that minimized the possibility of confusion due to imprecise inputs. In the second user study, we validated that the optimized pattern sets could reduce error rate from 23% to 7% for the classical Correlation recognizer. We also tested a novel Bayesian, which achieved higher selection accuracy than the Correlation recognizer when the input sequence is short. The informal evaluation results show that the selection technique can be effectively scaled to different modalities and sensing techniques.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 4
PB  - 
DO  - 10.1145/3369839
ER  - 

TY  - NA
AU  - Elsayed, Hesham; Machuca, Mayra Donaji Barrera; Schaarschmidt, Christian; Marky, Karola; Müller, Florian; Riemann, Jan; Matviienko, Andrii; Schmitz, Martin; Weigel, Martin; Mühlhäuser, Max
TI  - VRST - VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments
PY  - 2020
AB  - Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen – a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1) pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2) vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1) pneumatic, (2) vibrotactile and (3) a combination of both with (4) snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418953
ER  - 

TY  - JOUR
AU  - Tonsen, Marc; Steil, Julian; Sugano, Yusuke; Bulling, Andreas
TI  - InvisibleEye: Mobile Eye Tracking Using Multiple Low-Resolution Cameras and Learning-Based Gaze Estimation
PY  - 2017
AB  - Analysis of everyday human gaze behaviour has significant potential for ubiquitous computing, as evidenced by a large body of work in gaze-based human-computer interaction, attentive user interfaces, and eye-based user modelling. However, current mobile eye trackers are still obtrusive, which not only makes them uncomfortable to wear and socially unacceptable in daily life, but also prevents them from being widely adopted in the social and behavioural sciences. To address these challenges we present InvisibleEye, a novel approach for mobile eye tracking that uses millimetre-size RGB cameras that can be fully embedded into normal glasses frames. To compensate for the cameras’ low image resolution of only a few pixels, our approach uses multiple cameras to capture different views of the eye, as well as learning-based gaze estimation to directly regress from eye images to gaze directions. We prototypically implement our system and characterise its performance on three large-scale, increasingly realistic, and thus challenging datasets: 1) eye images synthesised using a recent computer graphics eye region model, 2) real eye images recorded of 17 participants under controlled lighting, and 3) eye images recorded of four participants over the course of four recording sessions in a mobile setting. We show that InvisibleEye achieves a top person-specific gaze estimation accuracy of 1.79° using four cameras with a resolution of only 5 × 5 pixels. Our evaluations not only demonstrate the feasibility of this novel approach but, more importantly, underline its significant potential for finally realising the vision of invisible mobile eye tracking and pervasive attentive user interfaces.
SP  - 106
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130971
ER  - 

TY  - NA
AU  - Cao, Yuanzhi; Xu, Zhuangying; Glenn, Terrell; Huo, Ke; Ramani, Karthik
TI  - Tangible and Embedded Interaction - Ani-Bot: A Modular Robotics System Supporting Creation, Tweaking, and Usage with Mixed-Reality Interactions
PY  - 2018
AB  - Ani-Bot is a modular robotics system that allows users to control their DIY robots using Mixed-Reality Interaction (MRI). This system takes advantage of MRI to enable users to visually program the robot through the augmented view of a Head-Mounted Display (HMD). In this paper, we first explain the design of the Mixed-Reality (MR) ready modular robotics system, which allows users to instantly perform MRI once they finish assembling the robot. Then, we elaborate the augmentations provided by the MR system in the three primary phases of a construction kit's lifecycle: Creation, Tweaking, and Usage. Finally, we demonstrate Ani-Bot with four application examples and evaluate the system with a two-session user study. The results of our evaluation indicate that Ani-Bot does successfully embed MRI into the lifecycle (Creation, Tweaking, Usage) of DIY robotics and that it does show strong potential for delivering an enhanced user experience.
SP  - 419
EP  - 428
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173226
ER  - 

TY  - NA
AU  - Rubens, Calvin
TI  - BitDrones: Design of a Tangible Drone Swarm as a Programmable Matter Interface
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Kato, Kunihiro; Ikematsu, Kaori; Kawahara, Yoshihiro; Siio, Itiro
TI  - CircWood: Laser Printed Circuit Boards and Sensors for Affordable DIY Woodworking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501317
ER  - 

TY  - JOUR
AU  - Jin, Yincheng; Gao, Yang; Xu, Xuhai; Choi, Seokmin; Li, Jiyang; Liu, Feng; Li, Zhengxiong; Jin, Zhanpeng
TI  - EarCommand
PY  - 2022
AB  - <jats:p>Intelligent speech interfaces have been developing vastly to support the growing demands for convenient control and interaction with wearable/earable and portable devices. To avoid privacy leakage during speech interactions and strengthen the resistance to ambient noise, silent speech interfaces have been widely explored to enable people's interaction with mobile/wearable devices without audible sounds. However, most existing silent speech solutions require either restricted background illuminations or hand involvement to hold device or perform gestures. In this study, we propose a novel earphone-based, hand-free silent speech interaction approach, named EarCommand. Our technique discovers the relationship between the deformation of the ear canal and the movements of the articulator and takes advantage of this link to recognize different silent speech commands. Our system can achieve a WER (word error rate) of 10.02% for word-level recognition and 12.33% for sentence-level recognition, when tested in human subjects with 32 word-level commands and 25 sentence-level commands, which indicates the effectiveness of inferring silent speech commands. Moreover, EarCommand shows high reliability and robustness in a variety of configuration settings and environmental conditions. It is anticipated that EarCommand can serve as an efficient, intelligent speech interface for hand-free operation, which could significantly improve the quality and convenience of interactions.</jats:p>
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534613
ER  - 

TY  - CHAP
AU  - Lee, Yi-Chieh; Yen, Chi-Hsien; Fu, Wai-Tat
TI  - SBP-BRiMS - Improving Donation Distribution for Crowdfunding: An Agent-Based Model
PY  - 2016
AB  - Donation-based crowdfunding has the potential to democratize capital raising by soliciting donations directly from the public through the Web and social media. These crowdfunding platforms, however, often function as unregulated open markets, in which there is minimal intervention to influence donation distribution across projects. In fact, research on crowdfunding hints that donation distribution in most crowdfunding platforms are suboptimal: while the overall success rates of crowdfunding projects are often low, a significant proportion of projects receive donations way over their targets. In this paper, we propose a new donation distributing system that aim to (a) distribute donations more effectively among the projects, and (b) align the allocation of donations with the preferences of donors. An agent-based model was developed to test the proposed system. Results showed that the proposed system not only increased the overall success rates of projects, but also led to more successes for projects preferred by donors. Implications to future crowdfunding platforms are discussed.
SP  - 3
EP  - 12
JF  - Social, Cultural, and Behavioral Modeling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-39931-7_1
ER  - 

TY  - CHAP
AU  - Sakashita, Mose; Hashizume, Satoshi; Ochiai, Yoichi
TI  - HCI (2) - Wrist-Mounted Haptic Feedback for Support of Virtual Reality in Combination with Electrical Muscle Stimulation and Hanger Reflex
PY  - 2019
AB  - Hanger reflex and electrical muscle stimulation (EMS) have been explored in previous haptic device research as novel methods for providing force sensations. This paper proposes a method that combines EMS and hanger reflex for haptic feedback on the wrist. Hanger reflex was used to elicit supination and pronation, and EMS was uses to cause flexion and extension. A virtual reality (VR) application was also implemented that gave users haptic feedback to their wrists. A user study was also conducted to investigate how the combined haptic feedback enhanced the VR experience in terms of enjoyment and realism. The results showed that the proposed haptic device allowed users a more realistic and enjoyable VR experience.
SP  - 544
EP  - 553
JF  - Human-Computer Interaction. Recognition and Interaction Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-22643-5_43
ER  - 

TY  - JOUR
AU  - Knibbe, Jarrod; Alsmith, Adrian J. T.; Hornbæk, Kasper
TI  - Experiencing Electrical Muscle Stimulation
PY  - 2018
AB  - Electrical Muscle Stimulation (EMS) offers rich opportunities for interaction. By varying stimulation parameters (amplitudes, pulse widths and frequencies), EMS can be used to either trigger muscle contractions, and so convey object affordances or guide user movements, or provide rich haptic feedback. However, the way users' experience changes with these parameters, and EMS in general, is poorly understood. Using a phenomenologically inspired interview technique, the explicitation interview, we study fifteen users' experience of EMS across 48 combinations of stimulation parameters. We synthesize the descriptions of EMS and relate stimulation parameters to categories of experience, such as 'temperature', 'motion', and 'sensitivity'. From the interviews, we explore more general topics in body-based interfaces, including the experience of control, metaphors for having your body actuated, and the relation between EMS parameters and perceived depth and location of sensations. These findings provide a vocabulary of EMS experience, and an insight into the relationship between specific parameters and associated sensations. In turn, this can help designers consider the user experience of EMS when developing interfaces.
SP  - 118
EP  - 14
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 3
PB  - 
DO  - 10.1145/3264928
ER  - 

TY  - NA
AU  - Tatsuno, Sho; Hayakawa, Tomohiko; Ishikawa, Masatoshi
TI  - WHC - Supportive training system for sports skill acquisition based on electrical stimulation
PY  - 2017
AB  - A supporting training system using a haptic device is considered to contribute more to motion learning than other stimuli that stimulate the human somatosensory system. However, the apparatus required for haptic stimulation is large, although the output is small. Electrical stimulation has been gaining attention in the field of human interfaces in recent years as a potential solution to this problem. Electrical stimulation is superior to existing haptic interfaces in that a large output can be generated using a few batteries, which can drive muscles to contract. Therefore, in this study, we developed a training system using electrical stimulation. This system teaches the rotation of arms, which is difficult to achieve by means of other haptic stimuli. We confirmed that the form in the rotation direction was successfully modified by using this system. The form was improved by 32% through electrical stimulation during training, and the form correction was achieved via the somatosensory system, which is difficult with the existing method. In addition, we verified that the skill was memorized in a short period of time and retained in the short term.
SP  - 466
EP  - 471
JF  - 2017 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2017.7989946
ER  - 

TY  - NA
AU  - C, Varun Perumal; Wigdor, Daniel
TI  - CHI - Foldem: Heterogeneous Object Fabrication via Selective Ablation of Multi-Material Sheets
PY  - 2016
AB  - Foldem, a novel method of rapid fabrication of objects with multi-material properties is presented. Our specially formulated Foldem sheet allows users to fabricate and easily assemble objects with rigid, bendable, and flexible properties using a standard laser-cutter. The user begins by creating his designs in a vector graphics software package. A laser cutter is then used to fabricate the design by selectively ablating/vaporizing one or more layers of the Foldem sheet to achieve the desired physical properties for each joint. Herein the composition of the Foldem sheet, as well as various design considerations taken into account while building and designing the method, are described. Sample objects made with Foldem are demonstrated, each showcasing the unique attributes of Foldem. Additionally, a novel method for carefully calibrating a laser cutter for precise ablation is presented.
SP  - 5765
EP  - 5775
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858135
ER  - 

TY  - NA
AU  - Yan, Zeyu; Sathya, Anup; Yusuf, Sahra; Lien, Jyh-Ming; Peng, Huaishu
TI  - Demonstration of Fibercuit: Techniques to Prototype High-Resolution Flexible and Kirigami Circuits with a Fiber Laser Engraver
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3565583
ER  - 

TY  - NA
AU  - Yoon, Sang Ho; Sung, Youjin; Shao, Yitian; Kim, Rachel
TI  - Exploring Vibration Intensity Map Of Hand Postures For Haptic Rendering In XR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565672
ER  - 

TY  - NA
AU  - Faltaous, Sarah; Neuwirth, Joshua; Gruenefeld, Uwe; Schneegass, Stefan
TI  - MUM - SaVR: Increasing Safety in Virtual Reality Environments via Electrical Muscle Stimulation
PY  - 2020
AB  - One of the main benefits of interactive Virtual Reality (VR) applications is that they provide a high sense of immersion. As a result, users lose their sense of real-world space which makes them vulnerable to collisions with real-world objects. In this work, we propose a novel approach to prevent such collisions using Electrical Muscle Stimulation (EMS). EMS actively prevents the movement that would result in a collision by actuating the antagonist muscle. We report on a user study comparing our approach to the commonly used feedback modalities: audio, visual, and vibro-tactile. Our results show that EMS is a promising modality for restraining user movement and, at the same time, rated best in terms of user experience.
SP  - 254
EP  - 258
JF  - 19th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3428361.3428389
ER  - 

TY  - NA
AU  - Purnendu, NA; Novack, Sasha M; Acome, Eric; Keplinger, Christoph; Alistar, Mirela; Gross, Mark D.; Bruns, Carson J.; Leithinger, Daniel
TI  - Conference on Designing Interactive Systems - Electriflow: Soft Electrohydraulic Building Blocks for Prototyping Shape-changing Interfaces
PY  - 2021
AB  - We present Electriflow: a new class of soft electrohydraulic actuators as building blocks for prototyping shape-changing interfaces. These actuators are silent and fast in operation and can be fabricated with commodity materials. Electriflow generates an immediate hydraulic force upon electrostatic activation without an external fluid supply source, enabling a simple and compact self-contained design. This paper describes the materials and mechanisms of these shape-changing building blocks, as well as the underlying fabrication process, which includes a software tool that assists in their design, shape visualization and construction. Finally, we explore four classes of application prototypes: tangible animation, actuating origami creases, shape-changing phone, and shape-changing bowl.
SP  - 1280
EP  - 1290
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462093
ER  - 

TY  - NA
AU  - Saviot, Léa; Brudy, Frederik; Houben, Steven
TI  - CHI Extended Abstracts - WRISTBAND.IO: Expanding Input and Output Spaces of a Smartwatch
PY  - 2017
AB  - Smartwatches are characterized by their small size designed for wearability, discretion, and mobile interactions. Most of the interactivity, however, is limited to the size of the display, introducing issues such as screen occlusion and limited information density. We introduce Wristband.io, a smartwatch with an extended interaction space along the wristband, enabling (i) back-of-band interaction using a touchpad, (ii) a low resolution ambient watchband display for off-screen notification, and (iii) tangible buttons for quick, eyes-free input. Insights gained through a study show that back-of-band input increases accuracy and task completion rates for smaller on-screen targets. We probe the design space of Wristband.io with three applications.
SP  - 2025
EP  - 2033
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053132
ER  - 

TY  - NA
AU  - Zhang, Zhuoming; Héron, Robin; Lecolinet, Eric; Détienne, Françoise; Safin, Stéphane
TI  - ICMI - VisualTouch: Enhancing Affective Touch Communication with Multi-modality Stimulation
PY  - 2019
AB  - As one of the most important non-verbal communication channel, touch plays an essential role in interpersonal affective communication. Although some researchers have started exploring the possibility of using wearable devices for conveying emotional information, most of the existing devices still lack the capability to support affective and dynamic touch in interaction. In this paper, we explore the effect of dynamic visual cues on the emotional perception of vibrotactile signals. For this purpose, we developed VisualTouch, a haptic sleeve consisting of a haptic layer and a visual layer. We hypothesized that visual cues would enhance the interpretation of tactile cues when both types of cues are congruent. We first carried out an experiment and selected 4 stimuli producing substantially different responses. Based on that, a second experiment was conducted with 12 participants rating the valence and arousal of 36 stimuli using SAM scales.
SP  - 114
EP  - 123
JF  - 2019 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340555.3353733
ER  - 

TY  - JOUR
AU  - Crain, Patrick; Bailey, Brian
TI  - Easier Said or Easier Done? Exploring the Relative Merits of Common Feedback Presentations
PY  - 2022
AB  - <jats:p>Feedback such as rubrics, pre-authored statements, and free-form comments are widely deployed in classrooms and feedback exchange communities. Despite these formats having different levels of detail and requiring different composition processes, little research has related these compositional processes for the provider to the feedback perceptions and revision outcomes for the recipient. In an online experiment, we investigated how novice participants (N=285) revised short stories in response to expert feedback composed with four common processes: rubrics, open comments, rubrics with open comments, and rubrics with comments on each rubric item (per-criterion comments). We also surveyed the feedback providers (N=4) to contrast the costs of composing feedback with the benefits of that feedback for the recipients. We found rubrics with per-criterion comments led to the highest revision quality, providers believed rubrics with open comments were most helpful to recipients, and rubrics alone took the least time to compose. Recipients' revision quality and perceptions of feedback helpfulness and credibility increased with feedback detail. We contribute an emergent framework for selecting feedback composition techniques based on attributes of interest and insights linking the composition costs with the benefits of the resulting feedback.</jats:p>
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512933
ER  - 

TY  - NA
AU  - Shimizu, Shuntaro; Hashimoto, Takeru; Yoshida, Shigeo; Matsumura, Reo; Narumi, Takuji; Kuzuoka, Hideaki
TI  - VR - Unident: Providing Impact Sensations on Handheld Objects via High-Speed Change of the Rotational Inertia
PY  - 2021
AB  - Several virtual reality (VR) proxies have been developed that can emulate impact sensations by generating actual forces on the hand. Although these proxies contribute to increasing the reality of VR, they still have some limitations, such as high latency, high power consumption, and low frequency to provide impact sensations. To overcome these limitations, we first propose a method to provide an impact sensation without actual force generation by quickly changing the rotational inertia of a handheld proxy while users are swinging it. Then, we developed Unident, a handheld proxy capable of changing its rotational inertia by moving a weight along one axis at a high speed. Two experiments were conducted to evaluate the ability of Unident to provide users with impact sensations. In the first experiment, we demonstrate that Unident can physically provide an impact sensation applied to a handheld object by analyzing the pressure on the user's palm. The second experiment shows that Unident can provide an impact sensation with various magnitudes depending on the amount of rotational inertia to be changed. Finally, we present an application that can be enabled by Unident.
SP  - 11
EP  - 20
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00021
ER  - 

TY  - JOUR
AU  - Startsev, Mikhail; Dorr, Michael
TI  - Supersaliency: A Novel Pipeline for Predicting Smooth Pursuit-Based Attention Improves Generalisability of Video Saliency
PY  - 2020
AB  - Predicting attention is a popular topic at the intersection of human and computer vision. However, even though most of the available video saliency data sets and models claim to target human observers' fixations, they fail to differentiate them from smooth pursuits (SPs), a major eye movement type that is unique to perception of dynamic scenes. In this work, we strive for a more meaningful prediction and conceptual understanding of saliency in general. Because of the higher attentional selectivity of smooth pursuit compared to fixations modelled in traditional saliency research, we refer to the problem of SP prediction as “supersaliency”. To make this distinction explicit, we (i) use algorithmic and manual annotations of SPs and fixations for two well-established video saliency data sets, (ii) train Slicing Convolutional Neural Networks for saliency prediction on either fixation- or SP-salient locations, and (iii) evaluate our and 26 publicly available dynamic saliency models on three data sets against traditional saliency and supersaliency ground truth. Overall, our models outperform the state of the art in both the new supersaliency and the traditional saliency problem settings, for which literature models are optimised. Importantly, on two independent data sets, our supersaliency model shows greater generalisation ability than its counterpart saliency model and outperforms all other models, even for fixation prediction. Furthermore, we tested an end-to-end video saliency model, which also showed systematic improvements when smooth pursuit was predicted either exclusively or together with fixations, with the best performance achieved when the model was trained for the supersaliency problem. This demonstrates the practical benefits and the potential of principled training data selection based on eye movement analysis.
SP  - 1276
EP  - 1289
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2961835
ER  - 

TY  - NA
AU  - Qin, Fang; Cheng, Huai-Yu; Sneeringer, Rachel; Vlachostergiou, Maria; Acharya, Sampada; Liu, Haolin; Majidi, Carmel; Islam, Mohammad; Yao, Lining
TI  - CHI Extended Abstracts - ExoForm: Shape Memory and Self-Fusing Semi-Rigid Wearables
PY  - 2021
AB  - Semi-rigid and rigid structures have been utilized in many on-body applications including musculoskeletal support (e.g., braces and splints). However, most of these support structures are not very compliant, so effortless custom fitting becomes a unique design challenge. Furthermore, the weight and space needed to transport these structures impede adoption in mobile environments. Here, we introduce ExoForm, a compact, customizable, and semi-rigid wearable material system with self-fusing edges that can semi-autonomously assemble on-demand while providing integrated sensing, control, and mobility. We present a comprehensive and holistic engineering strategy that includes optimized material composition, computationally-guided design and fabrication, semi-autonomous self-morphing assembly and fusing steps, heating control, and sensing for our easy-to-wear ExoForm. Finally, we fabricate wearable braces using the ExoForm method as a demonstration along with preliminary evaluation of ExoForm's performance.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451818
ER  - 

TY  - NA
AU  - Startsev, Mikhail; Dorr, Michael
TI  - Supersaliency: A Novel Pipeline for Predicting Smooth Pursuit-Based Attention Improves Generalizability of Video Saliency
PY  - 2018
AB  - Predicting attention is a popular topic at the intersection of human and computer vision. However, even though most of the available video saliency data sets and models claim to target human observers' fixations, they fail to differentiate them from smooth pursuit (SP), a major eye movement type that is unique to perception of dynamic scenes. In this work, we highlight the importance of SP and its prediction (which we call supersaliency, due to greater selectivity compared to fixations), and aim to make its distinction from fixations explicit for computational models. To this end, we (i) use algorithmic and manual annotations of SP and fixations for two well-established video saliency data sets, (ii) train Slicing Convolutional Neural Networks for saliency prediction on either fixation- or SP-salient locations, and (iii) evaluate our and 26 publicly available dynamic saliency models on three data sets against traditional saliency and supersaliency ground truth. Overall, our models outperform the state of the art in both the new supersaliency and the traditional saliency problem settings, for which literature models are optimized. Importantly, on two independent data sets, our supersaliency model shows greater generalization ability and outperforms all other models, even for fixation prediction.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Schipor, Ovidiu Andrei; Vatavu, Radu-Daniel; Vanderdonckt, Jean
TI  - Euphoria: A Scalable, event-driven architecture for designing interactions across heterogeneous devices in smart environments
PY  - 2019
AB  - NA
SP  - 43
EP  - 59
JF  - Information and Software Technology
VL  - 109
IS  - 109
PB  - 
DO  - 10.1016/j.infsof.2019.01.006
ER  - 

TY  - NA
AU  - Fortin, Pascal E.; Cooperstock, Jeremy
TI  - Understanding Smartphone Notifications' Activity Disruption via In Situ Wrist Motion Monitoring
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519695
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Yamaoka, Junichi; Leithinger, Daniel; Yeh, Tom; Gross, Mark D.; Kawahara, Yoshihiro; Kakehi, Yasuaki
TI  - UIST - Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation
PY  - 2018
AB  - This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.
SP  - 99
EP  - 111
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242659
ER  - 

TY  - NA
AU  - Han, Teng; Wang, Sirui; Wang, Sijia; Fan, Xiangmin; Liu, Jie; Tian, Feng; Fan, Mingming
TI  - CHI - Mouillé: Exploring Wetness Illusion on Fingertips to Enhance Immersive Experience in VR
PY  - 2020
AB  - Providing users with rich sensations is beneficial to enhance their immersion in Virtual Reality (VR) environments. Wetness is one such imperative sensation that affects users' sense of comfort and helps users adjust grip force when interacting with objects. Researchers have recently begun to explore ways to create wetness illusions, primarily on a user's face or body skin. In this work, we extended this line of research by creating wetness illusion on users' fingertips. We first conducted a user study to understand the effect of thermal and tactile feedback on users' perceived wetness sensation. Informed by the findings, we designed and evaluated a prototype---Mouille---that provides various levels of wetness illusions on fingertips for both hard and soft items when users squeeze, lift, or scratch it. Study results indicated that users were able to feel wetness with different levels of temperature changes and they were able to distinguish three levels of wetness for simulated VR objects. We further presented applications that simulated an ice cube, an iced cola bottle, and a wet sponge, etc, to demonstrate its use in VR.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376138
ER  - 

TY  - NA
AU  - Masuda, Taiki; Sawahashi, Ryunosuke; Komatsu, Jonah; Okui, Manabu; Nishihama, Rie; Nakamura, Taro
TI  - Prototype of an exoskeletal lower limb force-feedback device for moving extensively in VR space
PY  - 2022
AB  - In recent years, force feedback devices have been developed to improve the reality of virtual reality (VR) experiences. However, most of the general force feedback devices target the upper limbs. By realizing force feedback to the lower limbs, it is possible to realize an experience in VR space that cannot be achieved by force feedback to the upper limbs alone. Therefore, in this study, we developed a lower limb force feedback device based on a physical model of the human lower limb. The output torque of the developed device that met the target torque by the experiment was confirmed. The device was then experimented with to determine if it could reproduce an impact force similar to that of a soccer pass as an initial study. The results suggested that even a low torque of 10 Nm could reproduce a soccer pass.
SP  - NA
EP  - NA
JF  - IECON 2022 – 48th Annual Conference of the IEEE Industrial Electronics Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iecon49645.2022.9968915
ER  - 

TY  - NA
AU  - Fernandez, Misahael; Mathis, Florian; Khamis, Mohamed
TI  - NordiCHI - GazeWheels: Comparing Dwell-time Feedback and Methods for Gaze Input
PY  - 2020
AB  - We present an evaluation and comparison of GazeWheels: techniques for dwell time gaze input and feedback. In GazeWheel, visual feedback is shown to the user in the form of a wheel that is filled. When completely filled, a selection is made where the user is gazing. We compare three methods for responding to the user when gazing away from the target: Resetting GazeWheel, Pause-and-Resume GazeWheel, and Infinite GazeWheel. We also compare the position of the GazeWheel; Co-located Feedback: shown on the target being gazed at, and Remote Feedback: shown at the top of the interface. To this end, we report on results of a user study (N=19) that investigates the benefits and drawbacks of each method at different dwell times: 500ms, 800ms, and 1000ms. Results show that Infinite GazeWheel and Pause-and-Resume GazeWheel are more error prone but significantly faster than Resetting GazeWheel when using 800-1000 ms dwell time, even when including the time for correcting errors.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3419249.3420122
ER  - 

TY  - NA
AU  - Chang, Hong-Yu; Tseng, Wen-Jie; Tsai, Chia-En; Chen, Hsin-Yu; Peiris, Roshan Lalintha; Chan, Liwei
TI  - UIST - FacePush: Introducing Normal Force on Face with Head-Mounted Displays
PY  - 2018
AB  - This paper presents FacePush, a Head-Mounted Display (HMD) integrated with a pulley system to generate normal forces on a user's face in virtual reality (VR). The mechanism of FacePush is obtained by shifting torques provided by two motors that press upon a user's face via utilization of a pulley system. FacePush can generate normal forces of varying strengths and apply those to the surface of the face. To inform our design of FacePush for noticeable and discernible normal forces in VR applications, we conducted two studies to iden- tify the absolute detection threshold and the discrimination threshold for users' perception. After further consideration in regard to user comfort, we determined that two levels of force, 2.7 kPa and 3.375 kPa, are ideal for the development of the FacePush experience via implementation with three applications which demonstrate use of discrete and continuous normal force for the actions of boxing, diving, and 360 guidance in virtual reality. In addition, with regards to a virtual boxing application, we conducted a user study evaluating the user experience in terms of enjoyment and realism and collected the user's feedback.
SP  - 927
EP  - 935
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242588
ER  - 

TY  - NA
AU  - Winters, Amy; Bekkers, Iris; Nayeb Ghanbar Hosseini, Dorsa; Vekemans, Verindi; Weima, Samuël; Bruns, Miguel
TI  - Dynamic Robotic Fibers: Liquid Crystal Elastomers for Programmable and Reversible Shape-Changing Behaviors
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519769
ER  - 

TY  - BOOK
AU  - Schrader, Anderson; Gebhart, Isabella; Garrison, Drew; Duchowski, Andrew T.; Lapadatescu, Martian C.; Feng, Weiyu; Thabit, Mahmoud; Wang, Fang; Krejtz, Krzysztof; Petty, Daniel D.
TI  - ETRA Full Papers - Toward Eye-Tracked Sideline Concussion Assessment in eXtended Reality
PY  - 2021
AB  - As there is no currently available portable, visuomotor assessment of concussion at the sidelines, we present preliminary development of an approach based on Predictive Visual Tracking (PVT) suitable for the sidelines. Previous work has shown PVT sensitivity and specificity of 0.85 and 0.73, respectively, for standard deviation of radial error for normal and acute concussion (mild Traumatic Brain Injury, or mTBI), using a simple orbiting target stimulus. We propose new variants of the radial and tangential error metrics and conduct preliminary evaluation in Virtual Reality when applied to two different target motions (orbit and pendulum). Our new local visualization is intuitive, especially when considering evaluation of the pendulum target. Initial results indicate promise for baseline-related, personalized concussion testing in extended reality.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448017.3457378
ER  - 

TY  - NA
AU  - Bermejo, Carlos; Hui, Pan
TI  - A survey on haptic technologies for mobile augmented reality
PY  - 2017
AB  - Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention these days. The mobile nature of MAR applications limits users' interaction capabilities such as inputs, and haptic feedbacks. This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey first presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic: manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR applications and their challenges.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Leong, Joanne; Tappa, Jordan L.; Wilbert, João; Ishii, Hiroshi
TI  - UIST - HERMITS: Dynamically Reconfiguring the Interactivity of Self-propelled TUIs with Mechanical Shell Add-ons
PY  - 2020
AB  - We introduce HERMITS, a modular interaction architecture for self-propelled Tangible User Interfaces (TUIs) that incorporates physical add-ons, referred to as mechanical shells. The mechanical shell add-ons are intended to be dynamically reconfigured by utilizing the locomotion capability of self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed a proof-of-concept system that demonstrates this novel architecture using two-wheeled robots and a variety of mechanical shell examples. These mechanical shell add-ons are passive physical attatchments that extend the primitive interactivities (e.g. shape, motion and light) of the self-propelled robots. The paper proposes the architectural design, interactive functionality of HERMITS as well as design primitives for mechanical shells. The paper also introduces the prototype implementation that is based on an off-the-shelf robotic toy with a modified docking mechanism. A range of applications is demonstrated with the prototype to motivate the collective and dynamically reconfigurable capability of the modular architecture, such as an interactive mobility simulation, an adaptive home/desk environment, and a story-telling narrative. Lastly, we discuss the future research opportunity of HERMITS to enrich the interactivity and adaptability of actuated and shape changing TUIs.
SP  - 882
EP  - 896
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415831
ER  - 

TY  - NA
AU  - Fortin, Pascal E.; Sulmont, Elisabeth; Cooperstock, Jeremy R.
TI  - UIST (Adjunct Volume) - SweatSponse: Closing the Loop on Notification Delivery Using Skin Conductance Responses
PY  - 2018
AB  - Today"s smartphone notification systems are incapable of determining whether a notification has been successfully perceived without explicit interaction from the user. When the system incorrectly assumes that a notification has not been perceived, it may repeat it redundantly, disrupting the user (e.g., phone ringing). Or, when it assumes that a notification was perceived, and therefore fails to repeat it, the notification will be missed altogether (e.g., text message). We introduce SweatSponse, a feedback loop using skin conductance responses (SCR) to infer the perception of smartphone notifications just after their presentation. Early results from a laboratory study suggest that notifications induce SCR and that they could be used to better infer perception of smartphone notifications in real-time.
SP  - 7
EP  - 9
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266084
ER  - 

TY  - NA
AU  - del Castillo, Juan Angel Lorenzo; Couture, Nadine
TI  - HCI-Aero - The aircraft of the future: towards the tangible cockpit
PY  - 2016
AB  - The future of the cockpit is undeniably tactile. To make this vision become a reality, several usability issues must be first addressed, being the most important one the eyes-free interaction. In fact, different ways of interaction (tactile, physical) will coexist, and it is paramount to identify those elements in the cockpit that can become tactile and those that must remain as tangible (i.e. physical) ones. This work intends to analyze the current situation and the requirements from the point of view of Human-Machine Interaction. In this regard, we propose a new approach that, leading to the concept of "tangibilisation of the cockpit", can facilitate the coexistence between tactile and physical actuators in the cockpit. We believe that this approach will foster and inspire the development of a tangible cockpit in the near future.
SP  - 11
EP  - NA
JF  - Proceedings of the International Conference on Human-Computer Interaction in Aerospace
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2950112.2964582
ER  - 

TY  - NA
AU  - Chen, Yuxin; Yang, Zhuolin; Abbou, Ruben; Lopes, Pedro; Zhao, Ben Y.; Zheng, Haitao
TI  - CHI Extended Abstracts - Demonstrating User Authentication via Electrical Muscle Stimulation
PY  - 2021
AB  - We propose a novel modality for active biometric authentication: electrical muscle stimulation (EMS). To explore this, we engineered an interactive system, which we call ElectricAuth, that stimulates the user’s forearm muscles with a sequence of electrical impulses (i.e., EMS challenge) and measures the user’s involuntary finger movements (i.e., response to the challenge). ElectricAuth leverages EMS’s intersubject variability, where the same electrical stimulation results in different movements in different users because everybody’s physiology is unique (e.g., differences in bone and muscular structure, skin resistance and composition, etc.). As such, ElectricAuth allows users to login without memorizing passwords or PINs. ElectricAuth’s challenge-response structure makes it secure against data breaches and replay attacks, a major vulnerability facing today’s biometrics such as facial recognition and fingerprints. Furthermore, ElectricAuth never reuses the same challenge twice in authentications – in just one second of stimulation it encodes one of 68M possible challenges.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451563
ER  - 

TY  - NA
AU  - Carter, Marcus; Velloso, Eduardo; Downs, John; Sellen, Abigail; O'Hara, Kenton; Vetere, Frank
TI  - CHI - PathSync: Multi-User Gestural Interaction with Touchless Rhythmic Path Mimicry
PY  - 2016
AB  - In this paper, we present PathSync, a novel, distal and multi-user mid-air gestural technique based on the principle of rhythmic path mimicry; by replicating the movement of a screen-represented pattern with their hand, users can intuitively interact with digital objects quickly, and with a high level of accuracy. We present three studies that each contribute (1) improvements to how correlation is calculated in path-mimicry techniques necessary for touchless interaction, (2) a validation of its efficiency in comparison to existing techniques, and (3) a demonstration of its intuitiveness and multi-user capacity 'in the wild'. Our studies consequently demonstrate PathSync's potential as an immediately legitimate alternative to existing techniques, with key advantages for public display and multi-user applications.
SP  - 3415
EP  - 3427
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858284
ER  - 

TY  - NA
AU  - Velloso, Eduardo; Morimoto, Carlos H.
TI  - A Probabilistic Interpretation of Motion Correlation Selection Techniques
PY  - 2021
AB  - Motion correlation interfaces are those that present targets moving in different patterns, which the user can select by matching their motion. In this paper, we re-formulate the task of target selection as a probabilistic inference problem. We demonstrate that previous interaction techniques can be modelled using a Bayesian approach and that how modelling the selection task as transmission of information can help us make explicit the assumptions behind similarity measures. We propose ways of incorporating uncertainty into the decision-making process and demonstrate how the concept of entropy can illuminate the measurement of the quality of a design. We apply these techniques in a case study and suggest guidelines for future work.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445184
ER  - 

TY  - NA
AU  - Chen, Yuxin; Yang, Zhuolin; Abbou, Ruben; Lopes, Pedro; Zhao, Ben Y.; Zheng, Haitao
TI  - CHI - User Authentication via Electrical Muscle Stimulation
PY  - 2021
AB  - We propose a novel modality for active biometric authentication: electrical muscle stimulation (EMS). To explore this, we engineered an interactive system, which we call ElectricAuth, that stimulates the user’s forearm muscles with a sequence of electrical impulses (i.e., EMS challenge) and measures the user’s involuntary finger movements (i.e., response to the challenge). ElectricAuth leverages EMS’s intersubject variability, where the same electrical stimulation results in different movements in different users because everybody’s physiology is unique (e.g., differences in bone and muscular structure, skin resistance and composition, etc.). As such, ElectricAuth allows users to login without memorizing passwords or PINs. ElectricAuth’s challenge-response structure makes it secure against data breaches and replay attacks, a major vulnerability facing today’s biometrics such as facial recognition and fingerprints. Furthermore, ElectricAuth never reuses the same challenge twice in authentications – in just one second of stimulation it encodes one of 68M possible challenges. In our user studies, we found that ElectricAuth resists: (1) impersonation attacks (false acceptance rate: 0.17% at 5% false rejection rate); (2) replay attacks (false acceptance rate: 0.00% at 5% false rejection rate); and, (3) synthesis attacks (false acceptance rates: 0.2-2.5%). Our longitudinal study also shows that ElectricAuth produces consistent results over time and across different humidity and muscle conditions.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445441
ER  - 

TY  - NA
AU  - Kangas, Jari; Špakov, Oleg; Isokoski, Poika; Akkil, Deepak; Rantala, Jussi; Raisamo, Roope
TI  - AH - Feedback for Smooth Pursuit Gaze Tracking Based Control
PY  - 2016
AB  - Smart glasses, like Google Glass or Microsoft HoloLens, can be used as interfaces that expand human perceptual, cognitive, and actuation capabilities in many everyday situations. Conventional manual interaction techniques, however, are not convenient with smart glasses whereas eye trackers can be built into the frames. This makes gaze tracking a natural input technology for smart glasses. Not much is known about interaction techniques for gaze-aware smart glasses. This paper adds to this knowledge, by comparing feedback modalities (visual, auditory, haptic, none) in a continuous adjustment technique for smooth pursuit gaze tracking. Smooth pursuit based gaze tracking has been shown to enable flexible and calibration free method for spontaneous interaction situations. Continuous adjustment, on the other hand, is a technique that is needed in many everyday situations such as adjusting the volume of a sound system or the intensity of a light source. We measured user performance and preference in a task where participants matched the shades of two gray rectangles. The results showed no statistically significant differences in performance, but clear user preference and acceptability for haptic and audio feedback.
SP  - 6
EP  - NA
JF  - Proceedings of the 7th Augmented Human International Conference 2016
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2875194.2875209
ER  - 

TY  - NA
AU  - Hashizume, Satoshi; Takazawa, Kazuki; Koike, Amy; Ochiai, Yoichi
TI  - WHC - Cross-field haptics: Multiple direction haptics combined with magnetic and electrostatic fields
PY  - 2017
AB  - We present a new method of rendering haptic textures that utilizes electrostatic and magnetic fields. In conventional research, a single physical quantity is used to render haptic textures. By contrast, our method combines multiple fields (electrostatic and magnetic). Although these fields have no direct interference, combining them provides benefits such as the ability to produce multi-resolution haptic images and synergistic effects on haptic perception. We investigate the increase in the variation of texture by comparing each single field method. Furthermore, we conduct user experiments and quantitative measurements.
SP  - 370
EP  - 375
JF  - 2017 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2017.7989930
ER  - 

TY  - NA
AU  - Jones, Michael; Seppi, Kevin D.; Forsyth, Jared; Anderson, Zann
TI  - PerCom Workshops - Decoupling Screen Size and Gesture Size for Wrist Worn Devices
PY  - 2018
AB  - Touch gestures on the screen of a wrist worn device are constrained by the size of the screen. Decoupling the gesture size from the screen size allows for larger gestures on smaller devices. Other approaches to decoupling screen size from gesture size on wrist worn devices support only a small set of gestures. We decouple screen size from gesture size by using an optical flow sensor. The user generates gestures by moving a finger over the optical flow sensor. Gestures can feasibly be detected through a small round window with a diameter of 3 mm. This window “dot” could be embedded in small wrist worn devices. A random forest trained on the EdgeWrite alphabet achieved 93% accuracy on 27 gestures generated using the optical flow sensor. We discuss the motivation for such a system and prove its feasibility using a prototype, noting that additional engineering work is needed to produce a small wrist-worn device based on a small optical flow sensor package.
SP  - 324
EP  - 329
JF  - 2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/percomw.2018.8480406
ER  - 

TY  - NA
AU  - Du, Jiachun; Markopoulos, Panos; Wang, Qi; Toeters, Marina; Gong, Ting
TI  - Tangible and Embedded Interaction - ShapeTex: Implementing Shape-Changing Structures in Fabric for Wearable Actuation
PY  - 2018
AB  - Research in smart textiles and garments has mostly focused on integrating sensing technology. In order to make garments that are truly interactive it is also essential to develop technologies for actuating smart garments and textiles. This paper introduces ShapeTex, a thermal shape changing fabric that uses laminate thermal expansion to actuate textiles. We present the design process and rationale for ShapeTex; we explain the fabrication process we have developed for making ShapeTex accessible to fashion designers and interaction designers. Based on co-creation sessions with designers we discuss requirements derived from this material. Finally we present a number of concept prototypes created to explore and illustrate the potential applications of ShapeTex.
SP  - 166
EP  - 176
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173245
ER  - 

TY  - BOOK
AU  - Velloso, Eduardo; Coutinho, Flávio Luiz; Kurauchi, Andrew; Morimoto, Carlos H.
TI  - ETRA - Circular orbits detection for gaze interaction using 2D correlation and profile matching algorithms
PY  - 2018
AB  - Recently, interaction techniques in which the user selects screen targets by matching their movement with the input device have been gaining popularity, particularly in the context of gaze interaction (e.g. Pursuits, Orbits, AmbiGaze, etc.). However, though many algorithms for enabling such interaction techniques have been proposed, we still lack an understanding of how they compare to each other. In this paper, we introduce two new algorithms for matching eye movements: Profile Matching and 2D Correlation, and present a systematic comparison of these algorithms with two other state-of-the-art algorithms: the Basic Correlation algorithm used in Pursuits and the Rotated Correlation algorithm used in PathSync. We also examine the effects of two thresholding techniques and post-hoc filtering. We evaluated the algorithms on a user dataset and found the 2D Correlation with one-level thresholding and post-hoc filtering to be the best performing algorithm.
SP  - 25
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204524
ER  - 

TY  - NA
AU  - Kan, Viirj; Vargo, Emma; Machover, Noa; Ishii, Hiroshi; Pan, Serena; Chen, Weixuan; Kakehi, Yasuaki
TI  - CHI - Organic Primitives: Synthesis and Design of pH-Reactive Materials using Molecular I/O for Sensing, Actuation, and Interaction
PY  - 2017
AB  - In this paper we present Organic Primitives, an enabling toolbox that expands upon the library of input-output devices in HCI and facilitates the design of interactions with organic, fluid-based systems. We formulated color, odor and shape changing material primitives which act as sensor-actuators that convert pH signals into human-readable outputs. Food-grade organic molecules anthocyanin, vanillin, and chitosan were employed as dopants to synthesize materials which output a spectrum of colors, degrees of shape deformation, and switch between odorous and non-odorous states. We evaluated the individual output properties of our sensor-actuators to assess the rate, range, and reversibility of the changes as a function of pH 2-10. We present a design space with techniques for enhancing the functionality of the material primitives, and offer passive and computational methods for controlling the material interfaces. Finally, we explore applications enabled by Organic Primitives under four contexts: environmental, cosmetic, edible, and interspecies.
SP  - 989
EP  - 1000
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025952
ER  - 

TY  - BOOK
AU  - Gomez, Argenis Ramirez; Clarke, Christopher; Sidenmark, Ludwig; Gellersen, Hans
TI  - ETRA Full Papers - Gaze+Hold: Eyes-only Direct Manipulation with Continuous Gaze Modulated by Closure of One Eye
PY  - 2021
AB  - The eyes are coupled in their gaze function and therefore usually treated as a single input channel, limiting the range of interactions. However, people are able to open and close one eye while still gazing with the other. We introduce Gaze+Hold as an eyes-only technique that builds on this ability to leverage the eyes as separate input channels, with one eye modulating the state of interaction while the other provides continuous input. Gaze+Hold enables direct manipulation beyond pointing which we explore through the design of Gaze+Hold techniques for a range of user interface tasks. In a user study, we evaluated performance, usability and user’s spontaneous choice of eye for modulation of input. The results show that users are effective with Gaze+Hold. The choice of dominant versus non-dominant eye had no effect on performance, perceived usability and workload. This is significant for the utility of Gaze+Hold as it affords flexibility for mapping of either eye in different configurations.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448017.3457381
ER  - 

TY  - NA
AU  - Nith, Romain; Teng, Shan-Yuan; Li, Pengyu; Tao, Yujie; Lopes, Pedro
TI  - UIST - DextrEMS: Increasing Dexterity in Electrical Muscle Stimulation by Combining it with Brakes
PY  - 2021
AB  - Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user's fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.
SP  - 414
EP  - 430
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474759
ER  - 

TY  - NA
AU  - Tsai, Ching-Yi; Tsai, I-Lun; Lai, Chao-Jung; Chow, Derrek; Wei, Lauren; Cheng, Lung-Pan; Chen, Mike Y.
TI  - AirRacket: Perceptual Design of Ungrounded, Directional Force Feedback to Improve Virtual Racket Sports Experiences
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502034
ER  - 

TY  - BOOK
AU  - Špakov, Oleg; Isokoski, Poika; Kangas, Jari; Akkil, Deepak; Majaranta, Päivi
TI  - ETRA - PursuitAdjuster: an exploration into the design space of smooth pursuit --based widgets
PY  - 2016
AB  - In a study with 12 participants we compared two smooth pursuit based widgets and one dwell time based widget in adjusting a continuous value. The circular smooth pursuit widget was found to be about equally efficient as the dwell based widget in our color matching task. The scroll bar shaped smooth pursuit widget exhibited lower performance and lower user ratings.
SP  - 287
EP  - 290
JF  - Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2857491.2857526
ER  - 

TY  - NA
AU  - Dobbelstein, David
TI  - Near-body interaction for wearable interfaces
PY  - NA
AB  - Smart devices and mobile interfaces are getting evermore pervasive and available to users, so that nowadays information access is possible almost anywhere and anytime. With the frequency of mobile interactions increasing, access time and always availability are becoming more and more important. As a result, interfaces are moving closer to the user’s body. While smart phones enabled users to access and interact with information with reach from the user’s pocket, wearable devices such as smart eyewear and smart watches are further advancing this trend. By being able to always display information within or near the user’s field of view, a faster access time to information can be enabled. When it comes to interaction with wearable interfaces, however, no interaction techniques could be established as state-of-the-art yet. Building on that, this thesis contributes to the field of human-computer interaction (HCI) by investigating important properties for near-body interaction with wearable interfaces. For user input, novel concepts for near-body touch techniques are introduced, using the user’s body as an interaction delimiter. The body of research includes social implications of wearable interaction techniques and the internal perception of unobtrusiveness, the input expressiveness of touch gestures in mobile contexts, and suitable on- and off-body input locations. For user output, current modalities for wearable devices are mostly limited to visual and haptic feedback. In this thesis, the capabilities of haptic feedback are extended to positional feedback, using not only the temporal domain of vibrational feedback, but also positional continuous feedback by self-actuation. Furthermore, the concept of scent-based feedback is explored by introducing a wearable olfactory display that can emit multiple scents as an emotional channel for mobile notifications
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.18725/oparu-29531
ER  - 

TY  - BOOK
AU  - Isomoto, Toshiya; Ando, Toshiyuki; Shizuki, Buntarou; Takahashi, Shin
TI  - ETRA - Dwell time reduction technique using Fitts' law for gaze-based target acquisition
PY  - 2018
AB  - We present a dwell time reduction technique for gaze-based target acquisition. We adopt Fitts' Law to achieve the dwell time reduction. Our technique uses both the eye movement time for target acquisition estimated using Fitts' Law (Te) and the actual eye movement time (Ta) for target acquisition; a target is acquired when the difference between Te and Ta is small. First, we investigated the relation between the eye movement for target acquisition and Fitts' Law; the result indicated a correlation of 0.90 after error correction. Then we designed and implemented our technique. Finally, we conducted a user study to investigate the performance of our technique; an average dwell time of 86.7 ms was achieved, with a 10.0% Midas-touch rate.
SP  - 26
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204532
ER  - 

TY  - JOUR
AU  - Burdick, Kendall J.; Jorgensen, Seiver K.; Combs, Taylor N.; Holmberg, Megan O.; Kultgen, Samantha P.; Schlesinger, Joseph J.
TI  - SAVIOR ICU: sonification and vibrotactile interface for the operating room and intensive care unit
PY  - 2019
AB  - Alarm fatigue is an issue for healthcare providers in the intensive care unit, and may result from desensitization of overbearing and under-informing alarms. To directly increase the overall identification of medical alarms and potentially contribute to a downstream decrease in the prevalence of alarm fatigue, we propose advancing alarm sonification by combining auditory and tactile stimuli to create a multisensory alarm. Participants completed four trials—two multisensory (auditory and tactile) and two unisensory (auditory). Analysis compared the unisensory trials to the multisensory trials based on the percentage of correctly identified point of change, direction of change and identity of three physiological parameters (indicated by different instruments): heart rate (drums), blood pressure (piano), blood oxygenation (guitar). A repeated-measures of ANOVA yielded a significant improvement in performance for the multisensory group compared to the unisensory group (p < 0.05). Specifically, the multisensory group had better performance in correctly identifying parameter (p < 0.05) and point of change (p < 0.05) compared to the unisensory group. Participants demonstrated a higher accuracy of identification with the use of multisensory alarms. Therefore, multisensory alarms may relieve the auditory burden of the medical environment and increase the overall quality of care and patient safety.
SP  - 787
EP  - 796
JF  - Journal of clinical monitoring and computing
VL  - 34
IS  - 4
PB  - 
DO  - 10.1007/s10877-019-00381-1
ER  - 

TY  - JOUR
AU  - Parisay, Mohsen; Poullis, Charalambos; Kersten-Oertel, Marta
TI  - EyeTAP: Introducing a multimodal gaze-based technique using voice inputs with a comparative analysis of selection techniques
PY  - 2021
AB  - Abstract One of the main challenges of gaze-based interactions is the ability to distinguish normal eye function from a deliberate interaction with the computer system, commonly referred to as ‘Midas touch’. In this paper we propose EyeTAP (Eye tracking point-and-select by Targeted Acoustic Pulse) a contact-free multimodal interaction method for point-and-select tasks. We evaluated the prototype in four user studies with 33 participants and found that EyeTAP is applicable in the presence of ambient noise, results in a faster movement time, and faster task completion time, and has a lower cognitive workload than voice recognition. In addition, although EyeTAP did not generally outperform the dwell-time method, it did have a lower error rate than the dwell-time in one of our experiments. Our study shows that EyeTAP would be useful for users for whom physical movements are restricted or not possible due to a disability or in scenarios where contact-free interactions are necessary. Furthermore, EyeTAP has no specific requirements in terms of user interface design and therefore it can be easily integrated into existing systems.
SP  - 102676
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 154
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2021.102676
ER  - 

TY  - NA
AU  - Feldman, Molly Q.; Wang, Yiting; Byrd, William E.; Guimbretière, François; Andersen, Erik
TI  - Towards answering “Am I on the right track?” automatically using program synthesis
PY  - 2019
AB  - Students learning to program often need help completing assignments and understanding why their code does not work as they expect it to. One common place where they seek such help is at teaching assistant office hours. We found that teaching assistants in introductory programming (CS1) courses frequently answer some variant of the question ``Am I on the Right Track?''. The goal of this work is to develop an automated tool that provides similar feedback for students in real-time from within an IDE as they are writing their program. Existing automated tools lack the generality that we seek, often assuming a single approach to a problem, using hand-coded error models, or applying sample fixes from other students. In this paper, we explore the use of program synthesis to provide less constrained automated answers to ``Am I on the Right Track'' (AIORT) questions. We describe an observational study of TA-student interactions that supports targeting AIORT questions, as well as the development of and design considerations behind a prototype integrated development environment (IDE). The IDE uses an existing program synthesis engine to determine if a student is on the right track and we present pilot user studies of its use.
SP  - NA
EP  - NA
JF  - Proceedings of the 2019 ACM SIGPLAN Symposium on SPLASH-E
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3358711.3361626
ER  - 

TY  - JOUR
AU  - Jiang, Chutian; Chen, Yanjun; Fan, Mingming; Wang, Liuping; Shen, Luyao; Li, Nianlong; Sun, Wei; Zhang, Yu; Tian, Feng; Han, Teng
TI  - Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality
PY  - 2021
AB  - The imitation of pain sensation in Virtual Reality is considered valuable for safety education and training but has been seldom studied. This paper presents Douleur, a wearable haptic device that renders intensity-adjustable pain sensations with chemical stimulants. Different from mechanical, thermal, or electric stimulation, chemical-induced pain is more close to burning sensations and long-lasting. Douleur consists of a microfluidic platform that precisely emits capsaicin onto the skin and a microneedling component to help the stimulant penetrate the epidermis layer to activate the trigeminal nerve efficiently. Moreover, it embeds a Peltier module to apply the heating or cooling stimulus to the affected area to adjust the level of pain on the skin. To better understand how people would react to the chemical stimulant, we conducted a first study to quantify the enhancement of the sensation by changing the capsaicin concentration, skin temperature, and time and to determine suitable capsaicin concentration levels. In the second study, we demonstrated that Douleur could render a variety of pain sensations in corresponding virtual reality applications. In sum, Douleur is the first wearable prototype that leverages a combination of capsaicin and Peltier to induce rich pain sensations and opens up a wide range of applications for safety education and more.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463527
ER  - 

TY  - NA
AU  - Rasel, Islam
TI  - EdgeGlass: Exploring Tapping Performance on Smart Glasses while Sitting and Walking
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Onishi, Yuki; Takashima, Kazuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - VR - BouncyScreen: Physical Enhancement of Pseudo-Force Feedback
PY  - 2021
AB  - We explore BouncyScreen, an actuated 1D display system that enriches indirect interaction with a virtual object by pseudo-haptic feedback mechanics enhanced through the screen's physical movements. We configured a proof-of-concept prototype of BouncyScreen with a flat-screen mounted on a mobile robot. When the user manipulates a virtual object using virtual reality (VR) controllers, the screen moves in accordance with the virtual object. We conducted psychophysical studies to examine how BouncyScreen's physical movements would affect users' pseudo-haptic perceptions and interaction experiences. Our preliminary study using a weight discrimination task for object pushing interaction showed that BouncyScreen offers identical pseudo-force feedback to the vision-based pseudo-haptic technique. We conducted a follow-up psychophysical study using a weight magnitude estimation task for object pushing and bumping interactions. The results reveal that a users' perceived weight magnitude is enhanced by the screen's physical motion under different characteristics depending on interaction styles (i.e., pushing and bumping). Our study also confirmed that the screen's synchronous physical motions significantly enhance the reality of the interaction and the sense of presence. We close this paper with some applications and use suggestions for BouncyScreen in future HMD-free flat-screen 3D user interface systems.
SP  - 363
EP  - 372
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00059
ER  - 

TY  - NA
AU  - Delamare, William; Han, Teng; Irani, Pourang
TI  - MobileHCI - Designing a gaze gesture guiding system
PY  - 2017
AB  - We propose the concept of a guiding system specifically designed for semaphoric gaze gestures, i.e. gestures defining a vocabulary to trigger commands via the gaze modality. Our design exploration considers fundamental gaze gesture phases: Exploration, Guidance, and Return. A first experiment reveals that Guidance with dynamic elements moving along 2D paths is efficient and resistant to visual complexity. A second experiment reveals that a Rapid Serial Visual Presentation of command names during Exploration allows for more than 30% faster command retrievals than a standard visual search. To resume the task where the guide was triggered, labels moving from the outward extremity of 2D paths toward the guide center leads to efficient and accurate origin retrieval during the Return phase. We evaluate our resulting Gaze Gesture Guiding system, G3, for interacting with distant objects in an office environment using a head-mounted display. Users report positively on their experience with both semaphoric gaze gestures and G3.
SP  - 26
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3098561
ER  - 

TY  - NA
AU  - Yan, Yukang; Yu, Chun; Ma, Xiaojuan; Yi, Xin; Sun, Ke; Shi, Yuanchun
TI  - CHI - VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval
PY  - 2018
AB  - We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.
SP  - 78
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173652
ER  - 

TY  - NA
AU  - Zhang, Qing; Huang, Yifei; Chernyshov, George; Li, Juling; Pai, Yun Suen; Kunze, Kai
TI  - GazeSync: Eye Movement Transfer Using an Optical Eye Tracker and Monochrome Liquid Crystal Displays
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 27th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490100.3516469
ER  - 

TY  - BOOK
AU  - Hassoumi, Almoctar; Peysakhovich, Vsevolod; Hurter, Christophe
TI  - ETRA - EyeFlow: pursuit interactions using an unmodified camera
PY  - 2019
AB  - We investigate the smooth pursuit eye movement based interaction using an unmodified off-the-shelf RGB camera. In each pair of sequential video frames, we compute the indicative direction of the eye movement by analyzing flow vectors obtained using the Lucas-Kanade optical flow algorithm. We discuss how carefully selected low vectors could replace the traditional pupil centers detection in smooth pursuit interaction. We examine implications of unused features in the eye camera imaging frame as potential elements for detecting gaze gestures. This simple approach is easy to implement and abstains from many of the complexities of pupil based approaches. In particular, EyeFlow does not call for either a 3D pupil model or 2D pupil detection to track the pupil center location. We compare this method to state-of-the-art approaches and ind that this can enable pursuit interactions with standard cameras. Results from the evaluation with 12 users data yield an accuracy that compares to previous studies. In addition, the benefit of this work is that the approach does not necessitate highly matured computer vision algorithms and expensive IR-pass cameras.
SP  - 1
EP  - 10
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319820
ER  - 

TY  - NA
AU  - Pai, Yun Suen; Outram, Benjamin I.; Vontin, Noriyasu; Kunze, Kai
TI  - UIST (Adjunct Volume) - Transparent Reality: Using Eye Gaze Focus Depth as Interaction Modality
PY  - 2016
AB  - We present a novel, eye gaze based interaction technique, using focus depth as an input modality for virtual reality (VR) applications. We also show custom hardware prototype implementation. Comparing the focus depth based interaction to a scroll wheel interface, we find no statistically significant difference in performance (the focus depth works slightly better) and a subjective preference of the users in a user study with 10 participants playing a simple VR game. This indicates that it is a suitable interface modality that should be further explored. Finally, we give some application scenarios and guidelines for using focus depth interactions in VR applications.
SP  - 171
EP  - 172
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2984754
ER  - 

TY  - JOUR
AU  - Lopes, Pedro; Baudisch, Patrick
TI  - Immense Power in a Tiny Package: Wearables Based on Electrical Muscle Stimulation
PY  - 2017
AB  - Electrical muscle stimulation (EMS) devices have been used in rehabilitation medicine since the 1960s to regenerate lost motor functions, but, more recently, researchers have started to experiment with EMS to create interactive systems. EMS miniaturizes well, easily lending itself to pervasive computing use cases--particularly those involving mobile and wearable devices. As the authors discuss, EMS provides researchers with the technical means to create devices even smaller than current wearable devices. The authors illustrate this by comparing some of their own prototypes based on EMS with traditional approaches involving mechanical actuators.
SP  - 12
EP  - 16
JF  - IEEE Pervasive Computing
VL  - 16
IS  - 3
PB  - 
DO  - 10.1109/mprv.2017.2940953
ER  - 

TY  - NA
AU  - Misztal, Sebastian; Carbonell, Guillermo; Schild, Jonas
TI  - Experiencing Age-Related Movement Impairment Through Visual Delegation in VR Can Substitute Haptic Impairments of an Age Simulation Suit
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE 10th International Conference on Serious Games and Applications for Health(SeGAH)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/segah54908.2022.9978590
ER  - 

TY  - NA
AU  - Hirzle, Teresa; Gugenheimer, Jan; Geiselhart, Florian; Bulling, Andreas; Rukzio, Enrico
TI  - CHI - A Design Space for Gaze Interaction on Head-mounted Displays
PY  - 2019
AB  - Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a user's gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision.
SP  - 625
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300855
ER  - 

TY  - NA
AU  - Gotsch, Daniel; Zhang, Xujing; Merritt, Timothy; Vertegaal, Roel
TI  - CHI - TeleHuman2: A Cylindrical Light Field Teleconferencing System for Life-size 3D Human Telepresence
PY  - 2018
AB  - For telepresence to support the richness of multiparty conversations, it is important to convey motion parallax and stereoscopy without head-worn apparatus. TeleHuman2 is a "hologrammatic" telepresence system that conveys full-body 3D video of interlocutors using a human-sized cylindrical light field display. For rendering, the system uses an array of projectors mounted above the heads of participants in a ring around a retroreflective cylinder. Unique angular renditions are calculated from streaming depth video captured at the remote location. Projected images are retro-reflected into the eyes of local participants, at 1.3o intervals providing angular renditions simultaneously for left and right eyes of all onlookers, which conveys motion parallax and stereoscopy without head-worn apparatus or head tracking. Our technical evaluation of the angular accuracy of the system demonstrates that the error in judging the angle of a remote arrow object represented in TeleHuman2 is within 1 degree, and not significantly different from similar judgments of a collocated arrow object.
SP  - 522
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174096
ER  - 

TY  - NA
AU  - Ahn, Sunggeun; Santosa, Stephanie; Parent, Mark; Wigdor, Daniel; Grossman, Tovi; Giordano, Marcello
TI  - CHI - StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR
PY  - 2021
AB  - This work explores the design of marking menus for gaze-based AR/VR menu selection by expert and novice users. It first identifies and explains the challenges inherent in ocular motor control and current eye tracking hardware, including overshooting, incorrect selections, and false activations. Through three empirical studies, we optimized and validated design parameters to mitigate these errors while reducing completion time, task load, and eye fatigue. Based on the findings from these studies, we derived a set of design guidelines to support gaze-based marking menus in AR/VR. To overcome the overshoot errors found with eye-based expert marking menu behaviour, we developed StickyPie, a marking menu technique that enables scale-independent marking input by estimating saccade landing positions. An evaluation of StickyPie revealed that StickyPie was easier to learn than the traditional technique (i.e., RegularPie) and was 10% more efficient after 3 sessions.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445297
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Horak, Tom; Dachselt, Raimund
TI  - CHI - Watch+Strap: Extending Smartwatches with Interactive StrapDisplays
PY  - 2020
AB  - While smartwatches are widely adopted these days, their input and output space remains fairly limited by their screen size. We present StrapDisplays-interactive watchbands with embedded display and touch technologies-that enhance commodity watches and extend their input and output capabilities. After introducing the physical design space of these StrapDisplays, we explore how to combine a smartwatch and straps in a synergistic Watch+Strap system. Specifically, we propose multiple interface concepts that consider promising content distributions, interaction techniques, usage types, and display roles. For example, the straps can enrich watch apps, display visualizations, provide glanceable feedback, or help avoiding occlusion issues. Further, we provide a modular research platform incorporating three StrapDisplay prototypes and a flexible web-based software architecture, demonstrating the feasibility of our approach. Early brainstorming sessions with 15 participants informed our design process, while later interviews with six experts supported our concepts and provided valuable feedback for future developments.
SP  - 1
EP  - 15
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376199
ER  - 

TY  - JOUR
AU  - Cheng, Tingyu; Li, Bu; Zhang, Yang; Li, Yunzhi; Ramey, Charles; Jung, Eui Min; Cui, Yepu; Swaminathan, Saiganesh; Do, Youngwook; Tentzeris, Manos M.; Abowd, Gregory D.; Oh, Hyunjoo
TI  - Duco: Autonomous Large-Scale Direct-Circuit-Writing (DCW) on Vertical Everyday Surfaces Using A Scalable Hanging Plotter
PY  - 2021
AB  - Human environments are filled with large open spaces that are separated by structures like walls, facades, glass windows, etc. Most often, these structures are largely passive offering little to no interactivity. In this paper, we present Duco, a large-scale electronics fabrication robot that enables room-scale & building-scale circuitry to add interactivity to vertical everyday surfaces. Duco negates the need for any human intervention by leveraging a hanging robotic system that automatically sketches multi-layered circuity to enable novel large-scale interfaces. The key idea behind Duco is that it achieves single-layer or multi-layer circuit fabrication on 2D surfaces as well as 2D cutouts that can be assembled into 3D objects by loading various functional inks (e.g., conductive, dielectric, or cleaning) to the wall-hanging drawing robot, as well as employing an optional laser cutting head as a cutting tool. Our technical evaluation shows that Duco's mechanical system works reliably on various surface materials with a wide range of roughness and surface morphologies. The system achieves superior mechanical tolerances (0.1mm XY axis resolution and 1mm smallest feature size). We demonstrate our system with five application examples, including an interactive piano, an IoT coffee maker controller, an FM energy-harvester printed on a large glass window, a human-scale touch sensor and a 3D interactive lamp.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478118
ER  - 

TY  - NA
AU  - Auda, Jonas; Pascher, Max; Schneegass, Stefan
TI  - CHI - Around the (Virtual) World: Infinite Walking in Virtual Reality Using Electrical Muscle Stimulation
PY  - 2019
AB  - Virtual worlds are infinite environments in which the user can move around freely. When shifting from controller-based movement to regular walking as an input, the limitation of the real world also limits the virtual world. Tackling this challenge, we propose the use of electrical muscle stimulation to limit the necessary real-world space to create an unlimited walking experience. We thereby actuate the users` legs in a way that they deviate from their straight route and thus, walk in circles in the real world while still walking straight in the virtual world. We report on a study comparing this approach to vision shift - the state of the art approach - as well as combining both approaches. The results show that particularly combining both approaches yield high potential to create an infinite walking experience.
SP  - 431
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300661
ER  - 

TY  - NA
AU  - Guo, Wei; Wang, Jingtao
TI  - ICMI - Towards Attentive Speed Reading on Small Screen Wearable Devices
PY  - 2018
AB  - Smart watches can enrich everyday interactions by providing both glanceable information and instant access to frequent tasks. However, reading text messages on a 1.5-inch small screen is inherently challenging, especially when a user's attention is divided. We present SmartRSVP, an attentive speed-reading system to facilitate text reading on small-screen wearable devices. SmartRSVP leverages camera-based visual attention tracking and implicit physiological signal sensing to make text reading via Rapid Serial Visual Presentation (RSVP) more enjoyable and practical on smart watches. Through a series of three studies involving 40 participants, we found that 1) SmartRSVP can achieve a significantly higher comprehension rate (57.5% vs. 23.9%) and perceived comfort (3.8 vs. 2.1) than traditional RSVP; 2) Users prefer SmartRSVP over traditional reading interfaces when they walk and read; 3) SmartRSVP can predict users' cognitive workloads and adjust the reading speed accordingly in real-time with 83.3% precision.
SP  - 278
EP  - 287
JF  - Proceedings of the 20th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242969.3243009
ER  - 

TY  - BOOK
AU  - Geisler, David; Castner, Nora; Kasneci, Gjergji; Kasneci, Enkelejda
TI  - ETRA - A MinHash approach for fast scanpath classification
PY  - 2020
AB  - The visual scanpath describes the shift of visual attention over time. Characteristic patterns in the attention shifts allow inferences about cognitive processes, performed tasks, intention, or expertise. To analyse such patterns, the scanpath is often represented as a sequence of symbols that can be used to calculate a similarity score to other scanpaths. However, as the length of the scanpath or the number of possible symbols increases, established methods for scanpath similarity become inefficient, both in terms of runtime and memory consumption. We present a MinHash approach for efficient scanpath similarity calculation. Our approach shows competitive results in clustering and classification of scanpaths compared to established methods such as Needleman-Wunsch, but at a fraction of the required runtime. Furthermore, with time complexity of and constant memory consumption, our approach is ideally suited for real-time operation or analyzing large amounts of data.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391325
ER  - 

TY  - NA
AU  - Günther, Sebastian; Schön, Dominik; Müller, Florian; Mühlhäuser, Max; Schmitz, Martin
TI  - CHI Extended Abstracts - PneumoVolley: Pressure-based Haptic Feedback on the Head through Pneumatic Actuation
PY  - 2020
AB  - Haptic Feedback brings immersion and presence in Virtual Reality (VR) to the next level. While research proposes the usage of various tactile sensations, such as vibration or ultrasound approaches, the potential applicability of pressure feedback on the head is still underexplored. In this paper, we contribute concepts and design considerations for pressure-based feedback on the head through pneumatic actuation. As a proof-of-concept implementing our pressure-based haptics, we further present PneumoVolley: A VR experience similar to the classic Volleyball game but played with the head. In an exploratory user study with 9 participants, we evaluated our concepts and identified a significantly increased involvement compared to a no-haptics baseline along with high realism and enjoyment ratings using pressure-based feedback on the head in VR.
SP  - 1
EP  - 10
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382916
ER  - 

TY  - NA
AU  - Velloso, Eduardo; Carter, Marcus
TI  - CHI PLAY - The Emergence of EyePlay: A Survey of Eye Interaction in Games
PY  - 2016
AB  - As eye trackers become cheaper, smaller, more robust, and more available, they finally leave research labs and enter the home environment. In this context, gaming arises as a promising application domain for eye interaction. The goal of this survey is to categorise the different ways in which the eyes can be incorporated into games and play in general as a resource for future design. We reviewed the literature on the topic, as well as other game prototypes that employ the eyes. We compiled a list of eye-enabled game mechanics and derived a taxonomy that classifies them according to the eye movements they involve, the input type they provide, and the game mechanics that they implement. Based on our findings we articulate the value of gaming for future HCI gaze research and outline a research program around eye interaction in gaming.
SP  - 171
EP  - 185
JF  - Proceedings of the 2016 Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2967934.2968084
ER  - 

TY  - NA
AU  - Carneiro, Alex Torquato S.; Elmadjian, Carlos; Gonzales, Candy; Coutinho, Flávio Luiz; Morimoto, Carlos H.
TI  - SIBGRAPI - PursuitPass: A Visual Pursuit-Based User Authentication System
PY  - 2019
AB  - As our lives get more deeply submerged in digital format, ubiquitous access to sensitive data requires more secure and efficient user authentication procedures. Methods that solely relied on password entry were lately enhanced with the use of biometrics. Yet, these techniques can still be tricked by, for example, recordings of the face, voice, and fingerprint cloning. In this paper we introduce PursuitPass, a compact, robust, and efficient visual pursuit-based authentication system. PursuitPass is a user calibration-free method that requires the user to enter a password by visually pursuing moving targets on a small screen, such as a public ATM or a personal mobile phone. Because eye movements are used as input, passwords are better protected against shoulder surfing. Also, since targets can potentially move in unpredictable ways, it naturally imposes a liveness feature that cannot be counterfeited by recordings of the eyes. We investigated four pattern-matching algorithms to match visual pursuit user data with the movement of the targets. Two experiments were conducted. The first experiment aimed to define the best performing matching algorithm and configuration for PursuitPass. The second experiment aimed to evaluate the performance of our prototype. PursuitPass achieved a 96.82% accuracy with an average time of 10.42 s on a series of 4-digit PIN entry trials.
SP  - 226
EP  - 233
JF  - 2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sibgrapi.2019.00038
ER  - 

TY  - JOUR
AU  - Teng, Shan-Yuan; Lopes, Pedro
TI  - XR needs "mixed feelings"
PY  - 2022
AB  - <jats:p>Haptic devices allow us to feel virtual worlds through touch and forces; yet they are incompatible with haptics present in our everyday life. This urges us to re-think how to engineer a wave of new haptic devices for extended reality.</jats:p>
SP  - 44
EP  - 47
JF  - XRDS: Crossroads, The ACM Magazine for Students
VL  - 29
IS  - 1
PB  - 
DO  - 10.1145/3558194
ER  - 

TY  - NA
AU  - Drewes, Heiko; Khamis, Mohamed; Alt, Florian
TI  - DialPlate: Enhancing the Detection of Smooth Pursuits Eye Movements Using Linear Regression
PY  - 2018
AB  - We introduce and evaluate a novel approach for detecting smooth pursuit eye movements that increases the number of distinguishable targets and is more robust against false positives. Being natural and calibration-free, Pursuits has been gaining popularity in the past years. At the same time, current implementations show poor performance when more than eight on-screen targets are being used, thus limiting its applicability. Our approach (1) leverages the slope of a regression line, and (2) introduces a minimum signal duration that improves both the new and the traditional detection method. After introducing the approach as well as the implementation, we compare it to the traditional correlation-based Pursuits detection method. We tested the approach up to 24 targets and show that, if accepting a similar error rate, nearly twice as many targets can be distinguished compared to state of the art. For fewer targets, accuracy increases significantly. We believe our approach will enable more robust pursuit-based user interfaces, thus making it valuable for both researchers and practitioners.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cui, Dixuan; Mousas, Christos
TI  - Estimating the Just Noticeable Difference of Tactile Feedback in Oculus Quest 2 Controllers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00013
ER  - 

TY  - NA
AU  - Tessier, Matthieu; Ura, Masahiro; Miyata, Kazunori
TI  - Effect of Surface Geometry on Affordance and Cognitive Load: A Comparison of Different Age Groups
PY  - 2016
AB  - With the ever increasing number of interfaces and tangible interfaces a person might be interacting with on a daily basis, we are eager to understand how the shape of those artifacts play a role in the interaction process. This study aims at understanding differences in the analysis of a volume and its affordances between different age groups: children (8 years old), and young adults (25 years old). Each participant was asked to associate one of three given tangible displays (each with a unique geometry) to a common task (usually done on computers or tablets). The tests were done with each display showing a single color (white) to evaluate the level of abstraction each age group was able to achieve. The children group was given a second test, with identical questions, but using video projected content on the three displays to compare their answers between an abstract representation and a direct visual representation. Despite the limited amount of test participants, these results indicate that higher levels of abstraction are understandable by older participants. However, the children showed they can perceive the ergonomic features of a volume and link a volume with the visualization and interaction of digital content. The results of the second test with the children also indicate that video projected images drastically modify their appreciation of a volume.
SP  - 164
EP  - 170
JF  - 2016 Nicograph International (NicoInt)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/nicoint.2016.49
ER  - 

TY  - NA
AU  - Wiese, Eliane; Yen, Michael; Chen, Antares; Santos, Lucas A.; Fox, Armando
TI  - L@S - Teaching Students to Recognize and Implement Good Coding Style
PY  - 2017
AB  - Teaching students to write code with good style is important but difficult: in-depth feedback currently requires a human. AutoStyle, a style tutor that scales, offers adaptive, real-time holistic style feedback and hints as students improve their code. An in-situ study with 103 undergraduate students in a CS class compared AutoStyle to a control tutor which only offered ABC score. While students improved the style of their code in both cases, students working with AutoStyle were more likely to use an appropriate language idiom and to improve their recognition of good style. However, students struggled to implement style improvements, even when hints recommended specific functions.
SP  - 41
EP  - 50
JF  - Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3051457.3051469
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Liao, Yu-So; Tsai, Chieh
TI  - ImpactVest: Rendering Spatio-Temporal Multilevel Impact Force Feedback on Body in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501971
ER  - 

TY  - NA
AU  - Heo, Seongkook; Chung, Christina; Lee, Geehyuk; Wigdor, Daniel
TI  - CHI Extended Abstracts - Thor's Hammer: An Ungrounded Force Feedback Device Utilizing Propeller-Induced Propulsive Force
PY  - 2018
AB  - We present a new handheld haptic device, Thor's Hammer, which uses propeller propulsion to generate ungrounded, 3-DOF force feedback. Thor's Hammer has six motors and propellers that generates strong thrusts of air without the need for physical grounding or heavy air compressors. With its location and orientation tracked by an optimal tracking system, the system can exert forces in arbitrary directions regardless of the device's orientation. Our technical evaluation shows that Thor's Hammer can apply up to 4 N of force in arbitrary directions with less than 0.11 N and 3.9° of average magnitude and orientation errors. We also present virtual reality applications that can benefit from the force feedback provided by Thor's Hammer. Using these applications, we conducted a preliminary user study and participants felt the experience more realistic and immersive with the force feedback.
SP  - 525
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186544
ER  - 

TY  - NA
AU  - Bonfert, Michael; Porzel, Robert; Malaka, Rainer
TI  - VR - Get a Grip! Introducing Variable Grip for Controller-Based VR Systems
PY  - 2019
AB  - We propose an approach to facilitate adjustable grip for object interaction in virtual reality. It enables the user to handle objects with loose and firm grip using conventional controllers. Pivotal design properties were identified and evaluated in a qualitative pilot study. Two revised interaction designs with variable grip were compared to the status quo of invariable grip in a quantitative study. The users performed placing actions with all interaction modes. Performance, clutching, task load, and usability were measured. While the handling time increased slightly using variable grip, the usability score was significantly higher. No substantial differences were measured in positioning accuracy. The results lead to the conclusion that variable grip can be useful and improve realism depending on tasks, goals, and user preference.
SP  - 604
EP  - 612
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797824
ER  - 

TY  - JOUR
AU  - Tajima, Daisuke; Nishida, Jun; Lopes, Pedro; Kasahara, Shunichi
TI  - <i>Whose Touch is This?</i>
            : Understanding the Agency Trade-Off Between User-Driven Touch vs. Computer-Driven Touch
PY  - 2022
AB  - <jats:p>Force-feedback enhances digital touch by enabling users to share non-verbal aspects such as rhythm, poses, and so on. To achieve this, interfaces actuate the user’s to touch involuntarily (using exoskeletons or electrical-muscle-stimulation); we refer to this as computer-driven touch. Unfortunately, forcing users to touch causes a loss of their sense of agency. While researchers found that delaying the timing of computer-driven touch preserves agency, they only considered the naïve case when user-driven touch is aligned with computer-driven touch. We argue this is unlikely as it assumes we can perfectly predict user-touches. But, what about all the remainder situations: when the haptics forces the user into an outcome they did not intend or assists the user in an outcome they would not achieve alone? We unveil, via an experiment, what happens in these novel situations. From our findings, we synthesize a framework that enables researchers of digital-touch systems to trade-off between haptic-assistance vs. sense-of-agency.</jats:p>
SP  - 1
EP  - 27
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3489608
ER  - 

TY  - NA
AU  - Aghayi, Emad; LaToza, Thomas D.; Surendra, Paurav; Abolghasemi, Seyedmeysam
TI  - Crowdsourced Behavior-Driven Development: Implementing Microservices through Microtasks.
PY  - 2019
AB  - Key to the effectiveness of crowdsourcing approaches for software engineering is workflow design, describing how complex work is organized into small, relatively independent microtasks. In this paper, we introduce a Behavior-Driven Development (BDD) workflow for accomplishing programming work through self-contained microtasks, implemented as a preconfigured environment called Crowd Microservices. In our approach, a client, acting on behalf of a software team, describes a microservice as a set of endpoints with paths, requests, and responses. A crowd then implements the endpoints, identifying individual endpoint behaviors which they test, implement, and debug, creating new functions and interacting with persistence APIs as needed. To evaluate our approach, we conducted a feasibility study in which a small crowd worked to implement a small ToDo microservice. The crowd created an implementation with only four defects, completing 350 microtasks and implementing 13 functions. We discuss the implications of these findings for incorporating crowdsourced programming contributions into traditional software projects.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mayer, Sven; Laput, Gierad; Harrison, Chris
TI  - CHI - Enhancing Mobile Voice Assistants with WorldGaze
PY  - 2020
AB  - Contemporary voice assistants require that objects of inter-est be specified in spoken commands. Of course, users are often looking directly at the object or place of interest ? fine-grained, contextual information that is currently unused. We present WorldGaze, a software-only method for smartphones that provides the real-world gaze location of a user that voice agents can utilize for rapid, natural, and precise interactions. We achieve this by simultaneously opening the front and rear cameras of a smartphone. The front-facing camera is used to track the head in 3D, including estimating its direction vector. As the geometry of the front and back cameras are fixed and known, we can raycast the head vector into the 3D world scene as captured by the rear-facing camera. This allows the user to intuitively define an object or region of interest using their head gaze. We started our investigations with a qualitative exploration of competing methods, before developing a functional, real-time implementation. We conclude with an evaluation that shows WorldGaze can be quick and accurate, opening new multimodal gaze+voice interactions for mobile voice agents.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376479
ER  - 

TY  - NA
AU  - Zhang, Qing; Barbareschi, Giulia; Huang, Yifei; Li, Juling; Pai, Yun Suen; Ward, Jamie; Kunze, Kai
TI  - Seeing our Blind Spots: Smart Glasses-based Simulation to Increase Design Students' Awareness of Visual Impairment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545687
ER  - 

TY  - NA
AU  - Wu, Po-Chen; Wang, Robert Y.; Kin, Kenrick; Twigg, Christopher D.; Han, Shangchen; Yang, Ming-Hsuan; Chien, Shao-Yi
TI  - UIST - DodecaPen: Accurate 6DoF Tracking of a Passive Stylus
PY  - 2017
AB  - We propose a system for real-time six degrees of freedom (6DoF) tracking of a passive stylus that achieves sub-millimeter accuracy, which is suitable for writing or drawing in mixed reality applications. Our system is particularly easy to implement, requiring only a monocular camera, a 3D printed dodecahedron, and hand-glued binary square markers. The accuracy and performance we achieve are due to model-based tracking using a calibrated model and a combination of sparse pose estimation and dense alignment. We demonstrate the system performance in terms of speed and accuracy on a number of synthetic and real datasets, showing that it can be competitive with state-of-the-art multi-camera motion capture systems. We also demonstrate several applications of the technology ranging from 2D and 3D drawing in VR to general object manipulation and board games.
SP  - 365
EP  - 374
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126664
ER  - 

TY  - JOUR
AU  - Khoshtinat, Shiva; Carvelli, Valter; Marano, Claudia
TI  - Characterization and modeling the hygroscopic behavior of cellulose acetate membranes
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Exploiting materials with the ability to respond to the environmental stimuli is experiencing an enormous research interest. In particular, polymers that are sensitive to the changes of humidity levels attract great attention as self-actuators. The sensitivity of these materials to the level of moisture is expressed by their hygroscopic properties, namely, the coefficient of hygroscopic expansion. In this context, this study details the effect of moisture absorption on cellulose acetate membranes, as potential material for humidity-responsive self-actuators. The aim is two-fold. The first deals with the evaluation of the coefficient of hygroscopic expansion (α) through the determination of the absorbed moisture concentration at saturation (<jats:italic>C</jats:italic><jats:sub><jats:italic>sat</jats:italic></jats:sub>) and the relevant moisture absorption induced strain (<jats:italic>ε</jats:italic><jats:sub><jats:italic>hygro</jats:italic></jats:sub>). The second assesses the accuracy of a finite element modeling in describing the coupling of moisture absorption in cellulose acetate membranes and the corresponding dimensional variation, using the material properties experimentally measured. The experimentally measured <jats:italic>C</jats:italic><jats:sub><jats:italic>sat</jats:italic></jats:sub> and <jats:italic>ε</jats:italic><jats:sub><jats:italic>hygro</jats:italic></jats:sub> resulted a non-linear dependency on relative humidity. Also the coefficient of hygroscopic expansion (<jats:italic>α</jats:italic> = <jats:italic>C</jats:italic><jats:sub><jats:italic>sat</jats:italic></jats:sub><jats:italic> /ε</jats:italic><jats:sub><jats:italic>hygro</jats:italic></jats:sub>) resulted to have a non-linear dependency on the relative humidity, as well. By this input, numerical simulations were performed for different relative humidity levels, showing accurate description of experimental data.</jats:p>
SP  - 2175
EP  - 2186
JF  - Cellulose
VL  - 29
IS  - 4
PB  - 
DO  - 10.1007/s10570-022-04450-8
ER  - 

TY  - BOOK
AU  - Takeuchi, Yuichiro; Suwa, Shunichi; Nagamine, Kunihiko
TI  - ISS - AnyLight: Programmable Ambient Illumination via Computational Light Fields
PY  - 2016
AB  - In this paper we describe AnyLight, a lighting device that uses computational light fields to offer highly programmable illumination for architectural environments. Relying on integral imaging--a technique commonly used to realize stereoscopic 3D displays--AnyLight can mimic the illumination effects of various light sources, both real and imagined. A combination of high-output, narrow-beam LED backlight and custom, 3D printed optics give the device the capacity to shoot out strong, targeted light rays from its surface in arbitrary directions. The paper will provide extensive discussions covering the device's technical details, usage scenarios, and possibilities for future extensions; quantitative and qualitative results from our initial evaluation sessions will be reported as well.
SP  - 39
EP  - 48
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992188
ER  - 

TY  - NA
AU  - Mizuhara, Ryo; Takahashi, Akifumi; Kajimoto, Hiroyuki
TI  - VRCAI - Combination of Mechanical and Electrical Stimulation for an Intense and Realistic Tactile Sensation
PY  - 2019
AB  - Naturalistic tactile sensations can be elicited by mechanical stimuli because mechanical stimulations reproduce natural physical phenomena. However, a mechanical stimulation that is too strong may cause injury. Although electrical stimulation can elicit strong tactile sensations without damaging the skin, it is inferior in terms of naturalness. We propose and validate a haptic method for presenting naturalistic and intense sensations by combining electrical and mechanical stimulation. We validate the proposed method by verifying whether both enhancement of the subjective strength of mechanical stimulation through electrical stimulation and elimination of the bizarre sensation of electrical stimulation through mechanical stimulation can be achieved. We find that the proposed method increases subjective intensity by 25% on average across participants compared with mechanical stimulus alone and decreases the bizarre sensation compared with the presentation of the electrical stimulus alone. The method can be used to enhance the experience of virtual-reality content but has room for improvement especially in terms of intensity enhancement.
SP  - NA
EP  - NA
JF  - Proceedings of the 17th International Conference on Virtual-Reality Continuum and its Applications in Industry
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359997.3365714
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Hung, Ching-Wen; Wu, Tzu-Chun; Chen, Bing-Yu
TI  - CHI - ElastOscillation: 3D Multilevel Force Feedback for Damped Oscillation on VR Controllers
PY  - 2020
AB  - Force feedback from damped oscillation is a common effect in our daily lives, especially when shaking an elastic object, an object hanging or containing other stuff, or a container with liquid, e.g., casting with a fishing pole or wine-swirling. Such a force, affected by complex physical variations and collisions, is difficult to properly simulate using current force feedback methods. Therefore, we propose ElastOscillation on a virtual reality (VR) controller to provide 3D multilevel force feedback for damped oscillation to enhance VR experiences. ElastOscillation consists of a proxy, six elastic bands and DC motors. It leverages the motors to control the bands' elasticity to restrain the movement of the proxy, which is connected with the bands. Therefore, when users shake the ElastOscillation device, the proxy shakes or moves in corresponding ranges of movement. The users then perceive the force from oscillation at different levels. In addition, elastic force from the bands further reinforces the oscillation force feedback. We conducted a force perception study to understand users' distinguishability for perceiving oscillation forces in 1D and 2D movement, respectively. Based on the results, we performed a VR experience study to show that the force feedback provided by ElastOscillation enhances VR realism.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376408
ER  - 

TY  - NA
AU  - Pai, Yun Suen; Chen, Zikun; Chan, Liwei; Isogai, Megumi; Kimata, Hideaki; Kunze, Kai
TI  - MobileHCI - Pinchmove: improved accuracy of user mobility for near-field navigation in virtual environments
PY  - 2018
AB  - Navigation and mobility mechanics for virtual environments aim to be realistic or fun, but rarely prioritize the accuracy of movement. We propose PinchMove, a highly accurate navigation mechanic utilizing pinch gestures and manipulation of the viewport for confined environments that prefers accurate movement. We ran a pilot study to first determine the degree of simulator sickness caused by this mechanic, and a comprehensive user study to evaluate its accuracy in a virtual environment. We found that utilizing an 80° tunneling effect at a maximum speed of 15.18° per second was deemed suitable for PinchMove in reducing motion sickness. We also found our system to be at average, more accurate in enclosed virtual environments when compared to conventional methods. This paper makes the following three contributions: 1) We propose a navigation solution in near-field virtual environments for accurate movement, 2) we determined the appropriate tunneling effect for our method to minimize motion sickness, and 3) We validated our proposed solution by comparing it with conventional navigation solutions in terms of accuracy of movement. We also propose several use- case scenarios where accuracy in movement is desirable and further discuss the effectiveness of PinchMove.
SP  - 7
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229470
ER  - 

TY  - NA
AU  - Drewes, Heiko; Sakel, Sophia; Hussmann, Heinrich
TI  - User Perception of Smooth Pursuit Target Speed
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529234
ER  - 

TY  - NA
AU  - Akkil, Deepak; Lucero, Andrés; Kangas, Jari; Jokela, Tero; Salmimaa, Marja; Raisamo, Roope
TI  - NordiCHI - User Expectations of Everyday Gaze Interaction on Smartglasses
PY  - 2016
AB  - Gaze tracking technology is increasingly seen as a viable and practical input modality in a variety of everyday contexts, such as interacting with computers, mobile devices, public displays and wearables (e.g. smartglasses). We conducted an exploratory study consisting of six focus group sessions to understand people's expectations towards everyday gaze interaction on smartglasses. Our results provide novel insights into the role of use-context and social conventions regarding gaze behavior in acceptance of gaze interaction, various social and personal issues that need to be considered while designing gaze-based applications and user preferences of various gaze-based interaction techniques. Our results have many practical design implications and serve towards human-centric design and development of everyday gaze interaction technologies.
SP  - 24
EP  - NA
JF  - Proceedings of the 9th Nordic Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971485.2971496
ER  - 

TY  - NA
AU  - Drewes, Heiko; Khamis, Mohamed; Alt, Florian
TI  - MUM - Smooth Pursuit Target Speeds and Trajectories
PY  - 2018
AB  - In this paper we present an investigation of how the speed and trajectory of smooth pursuits targets impact on detection rates in gaze interfaces. Previous work optimized these values for the specific application for which smooth pursuit eye movements were employed. However, this may not always be possible. For example UI designers may want to minimize distraction caused by the stimulus, integrate it with a certain UI element (e.g., a button), or limit it to a certain area of the screen. In these cases an in-depth understanding of the interplay between speed, trajectory, and accuracy is required. To achieve this, we conducted a user study with 15 participants who had to follow targets with different speeds and on different trajectories using their gaze. We evaluated the data with respect to detectability. As a result, we obtained reasonable ranges for target speeds and demonstrate the effects of trajectory shapes. We show that slow moving targets are hard to detect by correlation and that introducing a delay improves the detection rate for fast moving targets. Our research is complemented by design rules which enable designers to implement better pursuit detectors and pursuit-based user interfaces.
SP  - 139
EP  - 146
JF  - Proceedings of the 17th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3282894.3282913
ER  - 

TY  - NA
AU  - Zhong, Ce; Wakkary, Ron; Odom, William; Chen, Amy Yo Sue; Yoo, MinYoung; Oogjes, Doenja
TI  - On the Design of deformTable: Attending to Temporality and Materiality for Supporting Everyday Interactions with a Shape-Changing Artifact
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533501
ER  - 

TY  - NA
AU  - Tamura, Yuto; Takemura, Kentaro
TI  - UIST (Adjunct Volume) - Estimating Focused Object using Smooth Pursuit Eye Movements and Interest Points in the Real World
PY  - 2019
AB  - User calibration is a significant problem in eye-based interaction. To overcome this, several solutions, such as the calibration-free method and implicit user calibration, have been proposed. Pursuits-based interaction is another such solution that has been studied for public screens and virtual reality. It has been applied to select graphical user interfaces (GUIs) because the movements in a GUI can be designed in advance. Smooth pursuit eye movements (smooth pursuits) occur when a user looks at objects in the physical space as well and thus, we propose a method to identify the focused object by using smooth pursuits in the real world. We attempted to determine the focused objects without prior information under several conditions by using the pursuits-based approach and confirmed the feasibility and limitations of the proposed method through experimental evaluations.
SP  - 21
EP  - 23
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357102
ER  - 

TY  - NA
AU  - Pfeiffer, Max; Duente, Tim; Rohs, Michael
TI  - MobileHCI - Let your body move: a prototyping toolkit for wearable force feedback with electrical muscle stimulation
PY  - 2016
AB  - Electrical muscle stimulation (EMS) is a promising wearable haptic output technology as it can be miniaturized considerably and delivers a wide range of haptic output. However, prototyping EMS applications is challenging. It requires detailed knowledge and skills about hardware, software, and physiological characteristics. To simplify prototyping with EMS in mobile and wearable situations we present the Let Your Body Move toolkit. It consists of (1) a hardware control module with Bluetooth communication that uses off-the-shelf EMS devices as signal generators, (2) a simple communications protocol to connect mobile devices, and (3) a set of control applications as starting points for EMS prototyping. We describe EMS-specific parameters, electrode placements on the skin, and user calibration. The toolkit was evaluated in a workshop with 10 researchers in haptics. The results show that the toolkit allows to quickly generate non-trivial prototypes. The hardware schematics and software components are available as open source software.
SP  - 418
EP  - 427
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2935334.2935348
ER  - 

TY  - NA
AU  - Sanada, Hanamichi; Kobayashi, Masato; Kon, Yuki; Kajimoto, Hiroyuki
TI  - AHs - Investigation of Effective Parts for Rotation and Translation of the Legs Using Hanger Reflex
PY  - 2020
AB  - When wearing a wire hanger on the head, an illusory force sense called the Hanger Reflex will occur. This phenomenon transpires not only at the head but also at the waists, wrists, and ankles. Dynamic control of the phenomenon by using pneumatic balloons has been proposed for the waist-type Hanger Reflex to enable direct rotation and translation of the user's body. However, this method is unsuitable for teaching different directions for the right and left legs, which is considered necessary for sports applications. In this study, we investigated which part of the leg is most effective for the direct rotation and translation of the leg. We also investigated the angle of the leg that was rotated when the Hanger Reflex device was attached to the thigh, knee, shin, and ankle, and the amount of translation. As a result, it was found that the most effective way to rotate and translate the leg was to rotate in the knee region, and that a greater extent of leg rotation was possible here than in other studies. These results suggest that a Hanger Reflex device is a possible candidate to rotate and translate the leg.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384786
ER  - 

TY  - JOUR
AU  - Su, Dan; Li, Youfu; Chen, Hao
TI  - Toward Precise Gaze Estimation for Mobile Head-Mounted Gaze Tracking Systems
PY  - 2019
AB  - The gaze estimation in the mobile scenario often suffers from the extrapolation and parallax errors. In this paper, we propose a novel calibration framework to achieve the precise gaze estimation for head-mounted gaze trackers. Our proposed framework consists of two steps to learn a point-to-point and a point-to-line relations, respectively. The aim of step I is to infer the relation between pupil centers and spatially constrained points of regard. By adopting the “CalibMe” gaze data acquisition method, a sparse Gaussian Process using pseudo-inputs is used to capture the smooth residual field unmodeled by the polynomial function. Meanwhile, a distraction detection criterion is introduced to identify the moment when user's attention is taken away from the calibration point thereby removing outliers. By combining with the point-to-point relation inferred in step I, the observed parallax errors are leveraged in step II to obtain a point-to-line relation, i.e., each pupil center will correspond to an epipolar line. Thus, the real image gaze point projected from different depths is predicted as the intersection of two epipolar lines inferred from binocular data. The simulation and experimental results show the effectiveness of our proposed calibration framework for head-mounted gaze trackers.
SP  - 2660
EP  - 2672
JF  - IEEE Transactions on Industrial Informatics
VL  - 15
IS  - 5
PB  - 
DO  - 10.1109/tii.2018.2867952
ER  - 

TY  - NA
AU  - Du, Ruofei; He, Liang
TI  - CHI Extended Abstracts - VRSurus: Enhancing Interactivity and Tangibility of Puppets in Virtual Reality
PY  - 2016
AB  - We present VRSurus, a smart device designed to recognize the puppeteer's gestures and render tactile feedback to enhance the interactivity of physical puppets in virtual reality (VR). VRSurus is wireless, self-contained, and small enough to be mounted upon any physical puppets. Using machine-learning techniques, VRSurus is able to recognize three gestures: swiping, shaking and thrusting. Actuators (e.g., solenoids, servos and vibration motors) assist with the puppetry visible to the audience and provide tactile feedback on the puppeteer's forearm. As a proof of concept, we implemented a tangible serious VR game using VRSurus that aimed at inspiring children to protect the environment and demonstrated it at the ACM UIST 2015 Student Innovation Contest. Our 3D models, circuitry and the source code are publicly available at www.vrsurus.com.
SP  - 2454
EP  - 2461
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892290
ER  - 

TY  - NA
AU  - Wang, Yanan; Luo, Shijian; Lu, Yujia; Gong, Hebo; Zhou, Yexing; Liu, Shuai; Hansen, Preben
TI  - Conference on Designing Interactive Systems - AnimSkin: Fabricating Epidermis with Interactive, Functional and Aesthetic Color Animation
PY  - 2017
AB  - Human epidermis, as the largest organ on body, has become a new design platform for wearable computing. The availability of miniature electronics makes more possibilities for on-skin designs. In this paper, we present AnimSkin, a thin-film interface, which will emit dynamic color animations directly on skin. This is done by using thermochromic material embedded with transparent electrode acting as a capacitive sensor. Moreover, an accessible and low-cost fabrication method is introduced. Individuals could also customize aesthetic graphic designs by following the detailed fabrication process to achieve personalized patterns. We propose four different dynamic types of color animation by applying certain voltage to the heating circuitry. With two examples, Email Reminder and Light Control System, we demonstrate how AnimSkin can be integrated into everyday life, and specifically, we show how AnimSkin can benefit areas such as on-skin design, thin-film interface and beauty technology.
SP  - 397
EP  - 401
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064687
ER  - 

TY  - JOUR
AU  - Clarke, Christopher; Bellino, Alessio; Esteves, Augusto; Gellersen, Hans
TI  - Remote Control by Body Movement in Synchrony with Orbiting Widgets: an Evaluation of TraceMatch
PY  - 2017
AB  - In this work we consider how users can use body movement for remote control with minimal effort and maximum flexibility. TraceMatch is a novel technique where the interface displays available controls as circular widgets with orbiting targets, and where users can trigger a control by mimicking the displayed motion. The technique uses computer vision to detect circular motion as a uniform type of input, but is highly appropriable as users can produce matching motion with any part of their body. We present three studies that investigate input performance with different parts of the body, user preferences, and spontaneous choice of movements for input in realistic application scenarios. The results show that users can provide effective input with their head, hands and while holding objects, that multiple controls can be effectively distinguished by the difference in presented phase and direction of movement, and that users choose and switch modes of input seamlessly.
SP  - 45
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130910
ER  - 

TY  - CONF
AU  - Singh, Ronal; Miller, Tim; Newn, Joshua; Sonenberg, Liz; Velloso, Eduardo; Vetere, Frank
TI  - AAMAS - Combining Planning with Gaze for Online Human Intention Recognition
PY  - 2018
AB  - Intention recognition is the process of using behavioural cues to infer an agent's goals or future behaviour. People use many behavioural cues to infer others' intentions, such as deliberative actions, facial expressions, eye gaze, and gestures. In artificial intelligence, two approaches for intention recognition, among others, are gaze-based and model-based intention recognition. Approaches in the former class use gaze to determine which parts of a space a person looks at more often to infer a person's intention. Approaches in the latter use models of possible future behaviour to rate intentions as more likely if they are a better 'fit' to observed actions. In this paper, we propose a novel model of human intention recognition that combines gaze and model-based approaches for online human intention recognition. Gaze data is used to build probability distributions over a set of possible intentions, which are then used as priors in a model-based intention recognition algorithm. In human-behavioural experiments ( n =20) involving a multi-player board game, we found that adding gaze-based priors to model-based intention recognition more accurately determined intentions ( p
SP  - 488
EP  - 496
JF  - NA
VL  - 1
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Heo, Seongkook; Chung, Christina; Lee, Geehyuk; Wigdor, Daniel
TI  - Thor's Hammer
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174099
ER  - 

TY  - CHAP
AU  - Soucy, Nicholas; Ranasinghe, Nimesha; Rossow, Avery; James, Meetha Nesam; Peiris, Roshan Lalintha
TI  - THED: A Wrist-Worn Thermal Display to Perceive Spatial Thermal Sensations in Virtual Reality
PY  - 2020
AB  - This paper presents THED, a wearable thermal display to perceive spatial thermal sensations within a virtual reality (VR) environment. THED consists of a wrist-worn thermal stimulation module and a control module utilizing BluetoothTM communication to connect with the VR environment. To demonstrate THED, we have developed a VR environment showing a virtual campfire in a snowy climate where participants were able to experience the virtual campfire in different predetermined distances. We have conducted a user experiment to 1) determine the distance based perception of spatial thermal sensations in a VR setting (VR only), 2) determine the differences of thermal stimuli on participants’ wrists (thermal only), and 3) evaluate the effects of combined thermal stimuli towards their expected spatial thermal stimuli (VR + thermal). Our primary aim of this study is to learn how humans spatially perceive thermal sensations on their hands (utilizing only one hand vs. both hands) when given a wrist-worn thermal source coupled with a virtual reality scenario. Our findings show that different thermal stimuli utilized by THED were able to provide thermal sensations in virtual reality that closely mirrored participants’ expected thermal sensations in respective real world scenarios.
SP  - 809
EP  - 829
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-63089-8_53
ER  - 

TY  - NA
AU  - Shishkin, Sergei L.; Velichkovsky, Boris M.; Melnichuk, Eugeny V.; Dubynin, Ignat A.; Zhao, Darisy G.; Isachenko, Andrey V.
TI  - SMC - The Pursuing Gaze Beats Mouse in Non-Pop-Out Target Selection
PY  - 2018
AB  - Demonstration of faster target selection by gaze compared to computer mouse so far was limited to targets attracting attention due to their visual saliency. This task, however, can be performed much faster with modern computer vision systems. Can gaze be faster than mouse in a more "intentional" selection task: when targets and non-targets do not significantly differ by their visual features? We propose that this may be the case when targets are moving at speeds beneficial for smooth pursuit eye movements. 16 healthy participants were asked to select 20 balls numbered 1 to 20 in numerical order. Balls were moving linearly at a screen in different directions at 12°/s speed. We compared selection made using a consumer grade eye tracker and a simple smooth pursuit detection algorithm with selection made using a computer mouse, either with clicks or pursuit. Compared to both mouse selection techniques, gaze selection was significantly faster and was experienced as more convenient by all participants.
SP  - 3518
EP  - 3523
JF  - 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smc.2018.00595
ER  - 

TY  - NA
AU  - Breitenfellner, Marcel; Jungwirth, Florian; Ferscha, Alois
TI  - UbiComp/ISWC Adjunct - Towards 3D smooth pursuit interaction
PY  - 2019
AB  - In this position paper, we encourage the use of novel 3D gaze tracking possibilities in the field of gaze-based interaction. Smooth pursuit offers great benefits over other gaze interaction approaches, like the ability to work with uncalibrated eye trackers, but also has disadvantages like the produced visual clutter in more complex user interfaces. We examine the basic concept of smooth pursuits, its hardware and algorithmic requirements and how this can be applied to real world problems. Then we evaluate how the recent change in availability of 3D eye tracking hardware can be used to approach the challenges of 2D smooth pursuit interaction. We take a look at different research opportunities, show concrete ideas and discuss why they are relevant for future research.
SP  - 619
EP  - 623
JF  - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341162.3348385
ER  - 

TY  - NA
AU  - Marshall, Joe; Benford, Steve; Byrne, Richard; Tennent, Paul
TI  - CHI - Sensory Alignment in Immersive Entertainment
PY  - 2019
AB  - When we use digital systems to stimulate the senses, we typically stimulate only a subset of users' senses, leaving other senses stimulated by the physical world. This creates potential for misalignment between senses, where digital and physical stimulation give conflicting signals to users. We synthesize knowledge from HCI, traditional entertainments, and underlying sensory science research relating to how senses work when given conflicting signals. Using this knowledge we present a design dimension of sensory alignment, and show how this dimension presents opportunities for a range of creative strategies ranging from full alignment of sensory stimulation, up to extreme conflict between senses.
SP  - 700
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300930
ER  - 

TY  - NA
AU  - Chen, Zikun; Peiris, Roshan Lalintha; Minamizawa, Kouta
TI  - Tangible and Embedded Interaction - A Thermal Pattern Design for Providing Dynamic Thermal Feedback on the Face with Head Mounted Displays
PY  - 2017
AB  - This paper presents our preliminary exploration of providing dynamic thermal haptic feedback on the face. We have designed a thermal feedback prototype that integrates Peltier modules directly with a head mounted display which is able to provide thermal output directly on users' faces. We explored the possibility of providing dynamic thermal feedback which could present information and enhance user experience through designing four initial dynamic thermal patterns of our prototype. We also conducted a preliminary user experiment and analysis of the design of different patterns ("dynamic information") to evaluate the design on three aspects, the accuracy of recognition of dynamic stimuli, smoothness of the movement and comfort level. Our results indicate that the accuracy of recognition of dynamic stimuli was approximately 71.84%.
SP  - 381
EP  - 388
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3025060
ER  - 

TY  - BOOK
AU  - Mattusch, Thomas; Mirzamohammad, Mahsa; Khamis, Mohamed; Bulling, Andreas; Alt, Florian
TI  - ETRA - Hidden pursuits: evaluating gaze-selection via pursuits when the stimuli's trajectory is partially hidden
PY  - 2018
AB  - The idea behind gaze interaction using Pursuits is to leverage the human's smooth pursuit eye movements performed when following moving targets. However, humans can also anticipate where a moving target would reappear if it temporarily hides from their view. In this work, we investigate how well users can select targets using Pursuits in cases where the target's trajectory is partially invisible (HiddenPursuits): e.g., can users select a moving target that temporarily hides behind another object? Although HiddenPursuits was not studied in the context of interaction before, understanding how well users can perform HiddenPursuits presents numerous opportunities, particularly for small interfaces where a target's trajectory can cover area outside of the screen. We found that users can still select targets quickly via Pursuits even if their trajectory is up to 50% hidden, and at the expense of longer selection times when the hidden portion is larger. We discuss how gaze-based interfaces can leverage HiddenPursuits for an improved user experience.
SP  - 27
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204569
ER  - 

TY  - JOUR
AU  - Zhang, Tengxiang; Yi, Xin; Wang, Ruolin; Wang, Yuntao; Yu, Chun; Lu, Yiqin; Shi, Yuanchun
TI  - Tap-to-Pair: Associating Wireless Devices with Synchronous Tapping
PY  - 2018
AB  - Ad-hoc wireless device pairing enables impromptu interactions in smart spaces, such as resource sharing and remote control. The pairing experience is mainly determined by the device association process, during which users express their pairing intentions between the advertising device and the scanning device. Currently, most wireless devices are associated by selecting the advertiser's name from a list displayed on the scanner's screen, which becomes less efficient and often misplaced as the number of wireless devices increases. In this paper, we propose Tap-to-Pair, a spontaneous device association mechanism that initiates pairing from advertising devices without hardware or firmware modifications. Tapping an area near the advertising device's antenna can change its signal strength. Users can then associate two devices by synchronizing taps on the advertising device with the blinking pattern displayed by the scanning device. By leveraging the wireless transceiver for sensing, Tap-to-Pair does not require additional resources from advertising devices and needs only a binary display (e.g. LED) on scanning devices. We conducted a user study to test users' synchronous tapping ability and demonstrated that Tap-to-Pair can reliably detect users' taps. We ran simulations to optimize parameters for the synchronization recognition algorithm and provide pattern design guidelines. We used a second user study to evaluate the on-chip performance of Tap-to-Pair. The results show that Tap-to-Pair can achieve an overall successful pairing rate of 93.7% with three scanning devices at different distances.
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287079
ER  - 

TY  - NA
AU  - Fortin, Pascal E.
TI  - MobileHCI - Methods and Interfaces for Closed Loop Smartphone Notifications
PY  - 2019
AB  - Our smartphones are constantly fighting to capture our attention, oftentimes causing significant disruption in professional and social contexts. In contrast with prior smart notification systems work focused on external contextual information (e.g., environment, user activity, etc.), my research explores how the notification experience could be enhanced by providing smartphones with a better awareness of their user's psycho-physiological state both prior to, but more importantly immediately after the presentation of alerts. This paper first summarizes findings from the evaluation of a novel notification perception classification technique based on wearable physiological sensing, and a non-intrusive mobile journaling mechanisms adapted to modern smartphone usage. From there, a tentative sequence of studies is presented, aiming to answer the project's remaining research questions.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3344422
ER  - 

TY  - JOUR
AU  - Saveljev, Vladimir; Son, Jung-Young; Yim, Choonsik; Heo, Gwanghee
TI  - Three-dimensional interactive cursor based on voxel patterns for autostereoscopic displays
PY  - 2022
AB  - NA
SP  - 137
EP  - 150
JF  - Journal of Information Display
VL  - 23
IS  - 2
PB  - 
DO  - 10.1080/15980316.2022.2029591
ER  - 

TY  - NA
AU  - Kasahara, Shunichi; Takada, Kazuma; Nishida, Jun; Shibata, Kazuhisa; Shimojo, Shinsuke; Lopes, Pedro
TI  - CHI - Preserving Agency During Electrical Muscle Stimulation Training Speeds up Reaction Time Directly After Removing EMS
PY  - 2021
AB  - Abstract: Force feedback devices, such as motor-based exoskeletons or wearables based on electrical muscle stimulation (EMS), have the unique potential to accelerate users’ own reaction time (RT). However, this speedup has only been explored while the device is attached to the user. In fact, very little is known regarding whether this faster reaction time still occurs after the user removes the device from their bodies–this is precisely what we investigated by means of a simple reaction time (RT) experiment, in which participants were asked to tap as soon as they saw an LED flashing. Participants experienced this in three EMS conditions: (1) fast-EMS, the electrical impulses were synced with the LED; (2) agency-EMS, the electrical impulse was delivered 40ms faster than the participant’s own RT, which prior work has shown to preserve one’s sense of agency over this movement; and, (3) late-EMS: the impulse was delivered after the participant’s own RT. Our results revealed that the participants’ RT was significantly reduced by approximately 8ms (up to 20ms) only after training with the agency-EMS condition. This finding suggests that the prioritizing agency during EMS training is key to motor-adaptation, i.e., it enables a faster motor response even after the user has removed the EMS device from their body.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445147
ER  - 

TY  - NA
AU  - Ishimaru, Takaya; Saga, Satoshi
TI  - Subjective sensation and force characteristics of electrical muscle stimulation due to waveform changes
PY  - 2022
AB  - Electrical muscle stimulation (EMS), which has been often used in the medical field, such as rehabilitation, has also come to be used as a haptic feedback technique. However, electrical muscle stimulation has a unique sense of electricity and pain. We aim to suppress this undesired sensation by changing the electrical muscle stimulation waveform. This study investigated the change in sensation due to the electrical muscle stimulation waveform change and describes the results. Further, we evaluated the subjective sensation of force by comparing the sensation with physical force.
SP  - NA
EP  - NA
JF  - 2022 IEEE/SICE International Symposium on System Integration (SII)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sii52469.2022.9708898
ER  - 

TY  - NA
AU  - Shtarbanov, Ali
TI  - AirTap : a multimodal interactive interface platform with free-space cutaneous haptic feedback via toroidal air-vortices
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhou, Dongbo; Hayakawa, Wataru; Nakajima, Yoshikazu; Tadano, Kotaro
TI  - Development of a Wearable Haptic Glove Presenting Haptic Sensation by Electrical Stimulation.
PY  - 2022
AB  - Most haptic devices generate haptic sensation using mechanical actuators. However, the workload and limited workspace handicap the operator from operating freely. Electrical stimulation is an alternative approach to generate haptic sensations without using mechanical actuators. The light weight of the electrodes adhering to the body brings no limitations to free motion. Because a real haptic sensation consists of feelings from several areas, mounting the electrodes to several different body areas can make the sensations more realistic. However, simultaneously stimulating multiple electrodes may result in "noise" sensations. Moreover, the operators may feel tingling because of unstable stimulus signals when using the dry electrodes to help develop an easily mounted haptic device using electrical stimulation. In this study, we first determine the appropriate stimulation areas and stimulus signals to generate a real touch sensation on the forearm. Then, we propose a circuit design guideline for generating stable electrical stimulus signals using a voltage divider resistor. Finally, based on the aforementioned results, we develop a wearable haptic glove prototype. This haptic glove allows the user to experience the haptic sensations of touching objects with five different degrees of stiffness.
SP  - 431
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 23
IS  - 1
PB  - 
DO  - 10.3390/s23010431
ER  - 

TY  - CHAP
AU  - Kane, Shaun K.
TI  - Wearables
PY  - 2019
AB  - NA
SP  - 701
EP  - 714
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-1-4471-7440-0_35
ER  - 

TY  - BOOK
AU  - Rey, Stéphanie; Brock, Anke; Bortolaso, Christophe; Derras, Mustapha; Couture, Nadine
TI  - ETIS - Guiding visitors in museums with calm interactions
PY  - 2020
AB  - We present two design solutions and an experimental platform that highlight the benefits of tangible interfaces in guiding visitors in museums while ensuring a better distribution of their attention between the exhibition and the guidance. We explore the use of the interactionattention continuum of Bakker et al. to design interfaces that allow the visitor to regulate his attention at different times of the visit. The Visiting Stick draws on the walking habits and the Marauder's Brochure extends the use of a recurring museum object, the visit brochure, by augmenting it with a dynamic display and various tactile and kinesthetic modalities. We have thus designed and built a physical experimental platform using several sensory channels: visual, audio and haptic (using heat, vibration and change of shape) to guide visitors. This platform will allow us in future work to compare the different modalities and their combination for guiding in the museum.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhao, Yiran; Tao, Yujie; Le, Grace; Maki, Rui; Adams, Alexander; Lopes, Pedro; Choudhury, Tanzeem
TI  - Affective Touch as Immediate and Passive Wearable Intervention
PY  - 2022
AB  - <jats:p>We investigated affective touch as a new pathway to passively mitigate in-the-moment anxiety. While existing mobile interventions offer great promises for health and well-being, they typically focus on achieving long-term effects such as shifting behaviors. As such, most mobile interventions are not applicable to provide immediate help in acute conditions -- when a user experiences a high anxiety level during ongoing events (e.g., completing high-stake tasks or mitigating interpersonal conflicts). A few works have developed passive interventions that are effective in-the-moment by leveraging breathing regulations and biofeedback. In this paper, we drew on neuroscientific findings on affective touch, the slow stroking on hairy skin that can elicit innate pleasantness and evaluated affective touch as a mobile health intervention. To induce affective touch, we first engineered a wearable device that renders a soft stroking sensation on the user's forearm. Then, we conducted a between-group experiment, in which participants underwent high-stress situations with/without receiving affective touch and post-experiment interviews, with 24 participants. Our results showed that participants who received affective touch experienced lower state anxiety and the same physiological stress response level compared to the control group participants. We also found that affective touch facilitated emotion regulation by rendering pleasantness, providing emotional support, and shifting attention. Finally, we discussed the immediate effect of affective touch on anxiety and physiological stress, the benefits of affective touch as a passive intervention, and the implementation considerations to use affective touch in just-in-time systems.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569484
ER  - 

TY  - NA
AU  - Tanaka, Yudai; Horie, Arata; Chen, Xiang 'Anthony'
TI  - VRST - DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback
PY  - 2020
AB  - We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418964
ER  - 

TY  - NA
AU  - Zhang, Xucong
TI  - Gaze estimation and interaction in real-world environments
PY  - NA
AB  - Human eye gaze has been widely used in human-computer interaction, as it is a promising modality for natural, fast, pervasive, and non-verbal interaction between humans and computers. As the foundation of gaze-related interactions, gaze estimation has been a hot research topic in recent decades. In this thesis, we focus on developing appearance-based gaze estimation methods and corresponding attentive user interfaces with a single webcam for challenging real-world environments. First, we collect a large-scale gaze estimation dataset, MPIIGaze, the first of its kind, outside of controlled laboratory conditions. Second, we propose an appearance-based method that, in stark contrast to a long-standing tradition in gaze estimation, only takes the full face image as input. Second, we propose an appearance-based method that, in stark contrast to a long-standing tradition in gaze estimation, only takes the full face image as input. Third, we study data normalisation for the first time in a principled way, and propose a modification that yields significant performance improvements. Fourth, we contribute an unsupervised detector for human-human and human-object eye contact. Finally, we study personal gaze estimation with multiple personal devices, such as mobile phones, tablets, and laptops.Der Blick des menschlichen Auges wird in Mensch-Computer-Interaktionen verbreitet eingesetzt, da dies eine vielversprechende Möglichkeit für natürliche, schnelle, allgegenwärtige und nonverbale Interaktion zwischen Mensch und Computer ist. Als Grundlage von blickbezogenen Interaktionen ist die Blickschätzung in den letzten Jahrzehnten ein wichtiges Forschungsthema geworden. In dieser Arbeit konzentrieren wir uns auf die Entwicklung Erscheinungsbild-basierter Methoden zur Blickschätzung und entsprechender “attentive user interfaces” (die Aufmerksamkeit des Benutzers einbeziehende Benutzerschnittstellen) mit nur einer Webcam für anspruchsvolle natürliche Umgebungen. Zunächst sammeln wir einen umfangreichen Datensatz zur Blickschätzung, MPIIGaze, der erste, der außerhalb von kontrollierten Laborbedingungen erstellt wurde. Zweitens schlagen wir eine Erscheinungsbild-basierte Methode vor, die im Gegensatz zur langjährigen Tradition in der Blickschätzung nur eine vollständige Aufnahme des Gesichtes als Eingabe verwendet. Drittens untersuchen wir die Datennormalisierung erstmals grundsätzlich und schlagen eine Modifizierung vor, die zu signifikanten Leistungsverbesserungen führt. Viertens stellen wir einen unüberwachten Detektor für Augenkontakte zwischen Mensch und Mensch und zwischen Mensch und Objekt vor. Abschließend untersuchen wir die persönliche Blickschätzung mit mehreren persönlichen Geräten wie Handy, Tablet und Laptop
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.22028/d291-27366
ER  - 

TY  - JOUR
AU  - Fernandez, Misahael; Mathis, Florian; Khamis, Mohamed
TI  - GazeWheels: Recommendations for using wheel widgets for feedback during dwell-time gaze input
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>We present GazeWheels: a series of visual feedback methods for dwell-based gaze input in the form of a wheel that is filled gradually until target selection. We evaluate three variations: Resetting, Pause &amp; Resume and Infinite GazeWheel, and study how dwell duration and visual feedback position (co-located vs remote) impact performance. Findings from a user study (N = 19) show that Infinite and Pause &amp; Resume GazeWheels are error prone but significantly faster than Resetting GazeWheel even when including error correction time. We conclude with five design recommendations.</jats:p>
SP  - 145
EP  - 156
JF  - it - Information Technology
VL  - 63
IS  - 3
PB  - 
DO  - 10.1515/itit-2020-0042
ER  - 

TY  - NA
AU  - Reiter, Katharina; Pfeuffer, Ken; Esteves, Augusto; Mittermeier, Tim; Alt, Florian
TI  - Look & Turn: One-handed and Expressive Menu Interaction by Gaze and Arm Turns in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529233
ER  - 

TY  - NA
AU  - Cmentowski, Sebastian; Krüger, Jens
TI  - Effects of Task Type and Wall Appearance on Collision Behavior in Virtual Environments.
PY  - 2021
AB  - Driven by the games community, virtual reality setups have lately evolved into affordable and consumer-ready mobile headsets. However, despite these promising improvements, it remains challenging to convey immersive and engaging VR games as players are usually limited to experience the virtual world by vision and hearing only. One prominent example of such open challenges is the disparity between the real surroundings and the virtual environment. As virtual obstacles usually do not have a physical counterpart, players might walk through walls enclosing the level. Thus, past research mainly focussed on multisensory collision feedback to deter players from ignoring obstacles. However, the underlying causative reasons for such unwanted behavior have mostly remained unclear. Our work investigates how task types and wall appearances influence the players' incentives to walk through virtual walls. Therefore, we conducted a user study, confronting the participants with different task motivations and walls of varying opacity and realism. Our evaluation reveals that players generally adhere to realistic behavior, as long as the experience feels interesting and diverse. Furthermore, we found that opaque walls excel in deterring subjects from cutting short, whereas different degrees of realism had no significant influence on walking trajectories. Finally, we use collected player feedback to discuss individual reasons for the observed behavior.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Heo, Seongkook; Annett, Michelle; Lafreniere, Benjamin; Grossman, Tovi; Fitzmaurice, George
TI  - Graphics Interface - No Need to Stop What You’re Doing: Exploring No-Handed Smartwatch Interaction
PY  - 2017
AB  - Smartwatches have the potential to enable quick micro-interactions throughout daily life. However, because they require both hands to operate, their full potential is constrained, particularly in situations where the user is actively performing a task with their hands. We investigate the space of no-handed interaction with smartwatches in scenarios where one or both hands are not free. Specifically, we present a taxonomy of scenarios in which standard touchscreen interaction with smartwatches is not possible, and discuss the key constraints that limit such interaction. We then implement a set of interaction techniques and evaluate them via two user studies: one where participants viewed video clips of the techniques and another where participants used the techniques in simulated hand-constrained scenarios. Our results found a preference for foot-based interaction and reveal novel design considerations to be mindful of when designing for no-handed smartwatch interaction scenarios.
SP  - 107
EP  - 114
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Tanaka, Yudai; Nishida, Jun; Lopes, Pedro
TI  - Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501910
ER  - 

TY  - NA
AU  - Feick, Martin; Bateman, Scott; Tang, Anthony; Miede, André; Marquardt, Nicolai
TI  - TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation in Virtual Reality
PY  - 2020
AB  - Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit, and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives, and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators, and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi's proxies outperform traditional controllers, and were generally favored by participants.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhao, Maozheng; Huang, Henry; Li, Zhi; Liu, Rui; Cui, Wenzhe; Toshniwal, Kajal; Goel, Ananya; Wang, Andrew; Zhao, Xia; Rashidian, Sina; Baig, Furqan; Phi, Khiem; Zhai, Shumin; Ramakrishnan, IV; Wang, Fusheng; Bi, Xiaojun
TI  - EyeSayCorrect: Eye Gaze and Voice Based Hands-free Text Correction for Mobile Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 27th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490099.3511103
ER  - 

TY  - JOUR
AU  - Zhang, Yunbo; Kwok, Tsz-Ho
TI  - Design and Interaction Interface using Augmented Reality for Smart Manufacturing
PY  - 2018
AB  - Abstract In this paper, we apply Augmented Reality (AR) technologies to develop a design and interaction interface for Smart Manufacturing (SmartMFG). This work is motivated by the lack of appropriate human-machine-interaction (HMI) tools to support interaction and customization in SmartMFG environment. Trying to address this research problem, we hypothesize that AR-based design interfaces that communicate with Machine Control Unit (MCU) directly will increase the degree of interaction and the complexity of instructions performed in Manual Data Input (MDI) systems. To test this hypothesis, we developed a prototyping system consisting of an AR-tablet device as the input interface and an Ultimaker 3 printer as the machine tool. Firstly, this AR-based system has sensing, design and control capabilities to interact and communicate with the machine tool via Wifi. Secondly, a set of sketch-based computational tools is developed for users to design shapes on existing objects easily and efficiently within the AR environment. Finally, The customized design is converted to machine code, which is also customized based on the machine tool and the registration of the virtual model and the existing object. We tested our system by designing two customized shapes onto an existing shape in the AR environment and generating the G-code to control the printer to fabricate them onto the physical object.
SP  - 1278
EP  - 1286
JF  - Procedia Manufacturing
VL  - 26
IS  - NA
PB  - 
DO  - 10.1016/j.promfg.2018.07.140
ER  - 

TY  - NA
AU  - Zorzal, Ezequiel R.; Sousa, Mauricio; Belchior, Pedro; Pereira, Joao Madeiras; Figueiredo, Nuno; Jorge, Joaquim A.
TI  - Design requirements to improve laparoscopy via XR
PY  - 2022
AB  - Laparoscopic surgery has the advantage of avoiding large open in-cisions and thereby decreasing blood loss, pain, and discomfort to patients. However, on the other side, it is hampered by restricted workspace, ambiguous communication, and surgeon fatigue caused by non-ergonomic head positioning. We aimed to identify critical problems and suggest design requirements and solutions. We used user and task analysis methods to learn about practices performed in an operating room by observing surgeons in their working environment to understand how they performed tasks and achieved their intended goals. Drawing on observations and analysis from recorded laparoscopic surgeries, we have identified several constraints and design requirements to propose potential solutions to address the issues. Surgeons operate in a dimly lit environment, surrounded by monitors, and communicate through verbal commands and pointing gestures. Therefore, performing user and task analysis allowed us to understand the existing problems in laparoscopy better while identifying several communication constraints and design requirements, which a solution has to follow to address those problems. Our contributions include identifying design requirements for laparoscopy surgery through a user and task analysis. These requirements propose design solutions towards improved surgeons&#x0027; comfort and make the surgical procedure less laborious.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00093
ER  - 

TY  - NA
AU  - Tseng, Wen-Jie; Bonnail, Elise; McGill, Mark; Khamis, Mohamed; Lecolinet, Eric; Huron, Samuel; Gugenheimer, Jan
TI  - The Dark Side of Perceptual Manipulations in Virtual Reality
PY  - 2022
AB  - "Virtual-Physical Perceptual Manipulations" (VPPMs) such as redirected walking and haptics expand the user's capacity to interact with Virtual Reality (VR) beyond what would ordinarily physically be possible. VPPMs leverage knowledge of the limits of human perception to effect changes in the user's physical movements, becoming able to (perceptibly and imperceptibly) nudge their physical actions to enhance interactivity in VR. We explore the risks posed by the malicious use of VPPMs. First, we define, conceptualize and demonstrate the existence of VPPMs. Next, using speculative design workshops, we explore and characterize the threats/risks posed, proposing mitigations and preventative recommendations against the malicious use of VPPMs. Finally, we implement two sample applications to demonstrate how existing VPPMs could be trivially subverted to create the potential for physical harm. This paper aims to raise awareness that the current way we apply and publish VPPMs can lead to malicious exploits of our perceptual vulnerabilities.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517728
ER  - 

TY  - JOUR
AU  - Shen, Xiyuan; Yan, Yukang; Yu, Chun; Shi, Yuanchun
TI  - ClenchClick
PY  - 2022
AB  - <jats:p>We propose to explore teeth-clenching-based target selection in Augmented Reality (AR), as the subtlety in the interaction can be beneficial to applications occupying the user's hand or that are sensitive to social norms. To support the investigation, we implemented an EMG-based teeth-clenching detection system (ClenchClick), where we adopted customized thresholds for different users. We first explored and compared the potential interaction design leveraging head movements and teeth clenching in combination. We finalized the interaction to take the form of a Point-and-Click manner with clenches as the confirmation mechanism. We evaluated the taskload and performance of ClenchClick by comparing it with two baseline methods in target selection tasks. Results showed that ClenchClick outperformed hand gestures in workload, physical load, accuracy and speed, and outperformed dwell in work load and temporal load. Lastly, through user studies, we demonstrated the advantage of ClenchClick in real-world tasks, including efficient and accurate hands-free target selection, natural and unobtrusive interaction in public, and robust head gesture input.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550327
ER  - 

TY  - NA
AU  - McClelland, John
TI  - Haptic Feedback in Virtual Reality with Deformation and Shape-Change
PY  - 2018
AB  - Past Virtual Reality (VR) research shows that haptic feedback increases presence and improves users' task performance. However, providing haptic feedback for multiple virtual objects usually requires complex, immobile systems, or multiple haptic props. We present a new approach that applies deformable, shape-changing devices to VR haptics, leveraging the dominance of human vision in VR to provide realistic haptic feedback with physical shape approximations. Our first study evaluates our HaptoBend prototype through an elicitation study. Results support the use of physical shape approximations and reveal important user preferences. We translate these results and past work into a Design Criteria to inform our second prototype, Adaptic. In our second study, we compare docking performance and adherence to our Design Criteria with Adaptic, a Razor Hydra Controller, and haptic props. We found Adaptic did well in satisfying our Design Criteria and had little difference in performance compared to the other haptic approaches.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Hossain, Tamzid; Islam, Md. Fahimul; Delamare, William; Chowdhury, Farida; Hasan, Khalad
TI  - Exploring Social Acceptability and Users' Preferences of Head- and Eye-Based Interaction with Mobile Devices
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3490636
ER  - 

TY  - BOOK
AU  - Drewes, Heiko; Pfeuffer, Ken; Alt, Florian
TI  - ETRA - Time- and space-efficient eye tracker calibration
PY  - 2019
AB  - One of the obstacles to bring eye tracking technology to everyday human computer interactions is the time consuming calibration procedure. In this paper we investigate a novel calibration method based on smooth pursuit eye movement. The method uses linear regression to calculate the calibration mapping. The advantage is that users can perform the calibration quickly in a few seconds and only use a small calibration area to cover a large tracking area. We first describe the theoretical background on establishing a calibration mapping and discuss differences of calibration methods used. We then present a user study comparing the new regression-based method with a classical nine-point and with other pursuit-based calibrations. The results show the proposed method is fully functional, quick, and enables accurate tracking of a large area. The method has the potential to be integrated into current eye tracking systems to make them more usable in various use cases.
SP  - 7
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319818
ER  - 

TY  - NA
AU  - Kosch, Thomas-Andreas
TI  - Workload-aware systems and interfaces for cognitive augmentation
PY  - 2020
AB  - In today's society, our cognition is constantly influenced by information intake, attention switching, and task interruptions. This increases the difficulty of a given task, adding to the existing workload and leading to compromised cognitive performances. The human body expresses the use of cognitive resources through physiological responses when confronted with a plethora of cognitive workload. This temporarily mobilizes additional resources to deal with the workload at the cost of accelerated mental exhaustion. We predict that recent developments in physiological sensing will increasingly create user interfaces that are aware of the user’s cognitive capacities, hence able to intervene when high or low states of cognitive workload are detected. In this thesis, we initially focus on determining opportune moments for cognitive assistance. Subsequently, we investigate suitable feedback modalities in a user-centric design process which are desirable for cognitive assistance. We present design requirements for how cognitive augmentation can be achieved using interfaces that sense cognitive workload. We then investigate different physiological sensing modalities to enable suitable real-time assessments of cognitive workload. We provide empirical evidence that the human brain is sensitive to fluctuations in cognitive resting states, hence making cognitive effort measurable. Firstly, we show that electroencephalography is a reliable modality to assess the mental workload generated during the user interface operation. Secondly, we use eye tracking to evaluate changes in eye movements and pupil dilation to quantify different workload states. The combination of machine learning and physiological sensing resulted in suitable real-time assessments of cognitive workload. The use of physiological sensing enables us to derive when cognitive augmentation is suitable. Based on our inquiries, we present applications that regulate cognitive workload in home and work settings. We deployed an assistive system in a field study to investigate the validity of our derived design requirements. Finding that workload is mitigated, we investigated how cognitive workload can be visualized to the user. We present an implementation of a biofeedback visualization that helps to improve the understanding of brain activity. A final study shows how cognitive workload measurements can be used to predict the efficiency of information intake through reading interfaces. Here, we conclude with use cases and applications which benefit from cognitive augmentation. This thesis investigates how assistive systems can be designed to implicitly sense and utilize cognitive workload for input and output. To do so, we measure cognitive workload in real-time by collecting behavioral and physiological data from users and analyze this data to support users through assistive systems that adapt their interface according to the currently measured workload. Our overall goal is to extend new and existing context-aware applications by the factor cognitive workload. We envision Workload-Aware Systems and Workload-Aware Interfaces as an extension in the context-aware paradigm. To this end, we conducted eight research inquiries during this thesis to investigate how to design and create workload-aware systems. Finally, we present our vision of future workload-aware systems and workload-aware interfaces. Due to the scarce availability of open physiological data sets, reference implementations, and methods, previous context-aware systems were limited in their ability to utilize cognitive workload for user interaction. Together with the collected data sets, we expect this thesis to pave the way for methodical and technical tools that integrate workload-awareness as a factor for context-aware systems.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Shahu, Ambika; Wintersberger, Philipp; Michahelles, Florian
TI  - Would Users Accept Electric Muscle Stimulation Controlling their Body? Insights from a Scenario-based Investigation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519693
ER  - 

TY  - NA
AU  - Harris, Maxwell T.; McCarty, Mitchell; Montes, Andre; Celik, Ozkan
TI  - Enhancing Haptic Effects Displayed via Neuromuscular Electrical Stimulation
PY  - 2016
AB  - <jats:p>This paper presents an experimental setup and results on enhancing sensations of a common haptic effect -a virtual wall-induced via neuromuscular electrical stimulation (NMES). A single degree of freedom (DOF) elbow platform with position sensing was constructed. This platform supports the arm in the horizontal plane while elbow flexion and extension torques are generated by stimulation of triceps brachii or the biceps brachii muscles. The response of the system was experimentally characterized by determining the latency, and the relationship between stimulation pulse width, stimulation current, joint position and generated output torques. After system characterization, stimulation control methods to enhance haptic sensations were designed, implemented and pilot tested under a variety of virtual wall hit scenarios. Our results indicate that the wall hit trajectories and interaction were improved by control laws that initiated low intensity stimulation prior to the wall hit and utilized co-contraction for damping. The “priming” of the muscle with low intensity stimulation prior to the main stimulation improved the responsiveness of muscle contractions.</jats:p>
SP  - NA
EP  - NA
JF  - Volume 1: Advances in Control Design Methods, Nonlinear and Optimal Control, Robotics, and Wind Energy Systems; Aerospace Applications; Assistive and Rehabilitation Robotics; Assistive Robotics; Battery and Oil and Gas Systems; Bioengineering Applications; Biomedical and Neural Systems Modeling, Diagnostics and Healthcare; Control and Monitoring of Vibratory Systems; Diagnostics and Detection; Energy Harvesting; Estimation and Identification; Fuel Cells/Energy Storage; Intelligent Transportation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1115/dscc2016-9823
ER  - 

TY  - NA
AU  - Li, Jiahao; Samoylov, Alexis; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms
PY  - 2022
AB  - One important vision of robotics is to provide physical assistance by manipulating different everyday objects, e.g., hand tools, kitchen utensils. However, many objects designed for dexterous hand-control are not easily manipulable by a single robotic arm with a generic parallel gripper. Complementary to existing research on developing grippers and control algorithms, we present Roman, a suite of hardware design and software tool support for robotic engineers to create 3D printable mechanisms attached to everyday handheld objects, making them easier to be manipulated by conventional robotic arms. The Roman hardware comes with a versatile magnetic gripper that can snap on/off handheld objects and drive add-on mechanisms to perform tasks. Roman also provides software support to register and author control programs. To validate our approach, we designed and fabricated Roman mechanisms for 14 everyday objects/tasks presented within a design space and conducted expert interviews with robotic engineers indicating that Roman serves as a practical alternative for enabling robotic manipulation of everyday objects.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501818
ER  - 

TY  - NA
AU  - Yasu, Kentaro
TI  - MagneShape: A Non-electrical Pin-Based Shape-Changing Display
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545645
ER  - 

TY  - NA
AU  - Hashizume, Satoshi; Koike, Amy; Hoshi, Takayuki; Ochiai, Yoichi
TI  - Sonovortex: Aerial Haptic Layer Rendering by Aerodynamic Vortex and Focused Ultrasound
PY  - 2019
AB  - In this paper, a method of rendering aerial haptics that uses an aerodynamic vortex and focused ultrasound is presented. Significant research has been conducted on haptic applications based on multiple phenomena such as magnetic and electric fields, focused ultrasound, and laser plasma. By combining multiple physical quantities; the resolution, distance, and magnitude of force are enhanced. To combine multiple tactile technologies, basic experiments on resolution and discrimination threshold are required. Separate user studies were conducted using aerodynamic and ultrasonic haptics. Moreover, the perception of their superposition, in addition to their resolution, was tested. Although these fields cause no direct interference, the system enables the simultaneous perception of the tactile feedback of both stimuli. The results of this study are expected to contribute to expanding the expression of aerial haptic displays based on several principles.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Velloso, Eduardo; Morimoto, Carlos H.
TI  - CHI - A Probabilistic Interpretation of Motion Correlation Selection Techniques
PY  - 2021
AB  - Motion correlation interfaces are those that present targets moving in different patterns, which the user can select by matching their motion. In this paper, we re-formulate the task of target selection as a probabilistic inference problem. We demonstrate that previous interaction techniques can be modelled using a Bayesian approach and that how modelling the selection task as transmission of information can help us make explicit the assumptions behind similarity measures. We propose ways of incorporating uncertainty into the decision-making process and demonstrate how the concept of entropy can illuminate the measurement of the quality of a design. We apply these techniques in a case study and suggest guidelines for future work.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Startsev, Mikhail; Dorr, Michael
TI  - Supersaliency: Predicting Smooth Pursuit-Based Attention with Slicing CNNs Improves Fixation Prediction for Naturalistic Videos
PY  - 2018
AB  - Predicting attention is a popular topic at the intersection of human and computer vision, but video saliency prediction has only recently begun to benefit from deep learning-based approaches. Even though most of the available video-based saliency data sets and models claim to target human observers' fixations, they fail to differentiate them from smooth pursuit (SP), a major eye movement type that is unique to perception of dynamic scenes. In this work, we aim to make this distinction explicit, to which end we (i) use both algorithmic and manual annotations of SP traces and other eye movements for two well-established video saliency data sets, (ii) train Slicing Convolutional Neural Networks (S-CNN) for saliency prediction on either fixation- or SP-salient locations, and (iii) evaluate ours and over 20 popular published saliency models on the two annotated data sets for predicting both SP and fixations, as well as on another data set of human fixations. Our proposed model, trained on an independent set of videos, outperforms the state-of-the-art saliency models in the task of SP prediction on all considered data sets. Moreover, this model also demonstrates superior performance in the prediction of "classical" fixation-based saliency. Our results emphasize the importance of selectively approaching training set construction for attention modelling.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mesihovic, Mirza; Ljubovic, Vedran; Muharemovic, Ibrahim
TI  - MIPRO - Using WebIDE as a distance learning tool for high school programming
PY  - 2020
AB  - During COVID-19 pandemics, not unlike their peers in other parts of the world, the high-school students in Bosnia and Herzegovina needed to get accustomed to learning from their homes. This proved to be particularly difficult for those who were scheduled to learn basic C++ programming, not only the inperson support by their teachers is invaluable in these early stages of learning, but many of them did not have access to adequate hardware or software. A web-based integrated development environment (WebIDE) was deployed, and a number of sample assignments were created. Altogether, over 130 students from 5 schools participated actively, solving a total of 65 assignments (as of June 1st). Based on questionnaire results, overall the experiment was deemed successful, and the continued use of WebIDE after life returns to normal was justified.
SP  - 893
EP  - 898
JF  - 2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/mipro48935.2020.9245263
ER  - 

TY  - NA
AU  - Kato, Hiroyuki; Takemura, Kentaro
TI  - UbiComp Adjunct - Hand pose estimation based on active bone-conducted sound sensing
PY  - 2016
AB  - Estimating hand poses is essential to achieve intuitive user interfaces. In Virtual Reality, an infrared (IR) camera is used for hand tracking, and direct manipulation can be accomplished by using hands. Additionally, wearable devices have also attracted attention because of their portability. We have developed a method based on the use of a wearable device to estimate the joint angle, which can be determined using the amplitude of vibration. However, the joint angle, which can be estimated, is limited to particular joints. Therefore, our proposed method determines the hand pose based on active bone-conducted sound sensing toward intuitive user interfaces. We employed the power spectral density as a feature, thereby enabling the hand pose to be classified with a support vector machine. We confirmed the recognition accuracy and the feasibility of our proposed method through evaluation experiments.
SP  - 109
EP  - 112
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2971403
ER  - 

TY  - JOUR
AU  - González-Manzano, Lorena; de Fuentes, José María; Ribagorda, Arturo
TI  - Leveraging User-related Internet of Things for Continuous Authentication: A Survey
PY  - 2019
AB  - Among all Internet of Things (IoT) devices, a subset of them are related to users. Leveraging these user-related IoT elements, it is possible to ensure the identity of the user for a period of time, thus avoiding impersonation. This need is known as Continuous Authentication (CA). Since 2009, a plethora of IoT-based CA academic research and industrial contributions have been proposed. We offer a comprehensive overview of 58 research papers regarding the main components of such a CA system. The status of the industry is studied as well, covering 32 market contributions, research projects, and related standards. Lessons learned, challenges, and open issues to foster further research in this area are finally presented.
SP  - 53
EP  - 38
JF  - ACM Computing Surveys
VL  - 52
IS  - 3
PB  - 
DO  - 10.1145/3314023
ER  - 

TY  - NA
AU  - Abdelrahman, Yomna; Khamis, Mohamed; Schneegass, Stefan; Alt, Florian
TI  - CHI - Stay Cool! Understanding Thermal Attacks on Mobile-based User Authentication
PY  - 2017
AB  - PINs and patterns remain among the most widely used knowledge-based authentication schemes. As thermal cameras become ubiquitous and affordable, we foresee a new form of threat to user privacy on mobile devices. Thermal cameras allow performing thermal attacks, where heat traces, resulting from authentication, can be used to reconstruct passwords. In this work we investigate in details the viability of exploiting thermal imaging to infer PINs and patterns on mobile devices. We present a study (N=18) where we evaluated how properties of PINs and patterns influence their thermal attacks resistance. We found that thermal attacks are indeed viable on mobile devices; overlapping patterns significantly decrease successful thermal attack rate from 100% to 16.67%, while PINs remain vulnerable (>72% success rate) even with duplicate digits. We conclude by recommendations for users and designers of authentication schemes on how to resist thermal attacks.
SP  - 3751
EP  - 3763
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025461
ER  - 

TY  - NA
AU  - Bigman, Maxwell; Roy, Ethan; Garcia, Jorge E.; Suzara, Miroslav; Wang, Kaili; Piech, Chris
TI  - SIGCSE - PearProgram: A More Fruitful Approach to Pair Programming
PY  - 2021
AB  - In this paper we present PearProgram, a hybrid learning and research tool that helps introductory Computer Science (CS) students learn how to pair program, including in remote learning environments. Grounded in theory from the Learning Sciences, the tool -- a collaborative, online IDE -- has two primary goals: 1) to help introductory CS students achieve pair programming success; and 2) to research what factors contribute to pairs that have beneficial outcomes. We present our learnings from the use of PearProgram in three remote introductory CS courses: a CS1 course, and two large international courses, including one for high school students. Teacher and student users responded positively to PearProgram, and use of the tool was associated with beneficial learning outcomes in these online learning environments. Our research opens many future research directions for (remote) pair programming, and indicates practices that may prove useful for CS educators at all levels.
SP  - 900
EP  - 906
JF  - Proceedings of the 52nd ACM Technical Symposium on Computer Science Education
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3408877.3432517
ER  - 

TY  - NA
AU  - Sato, Munehiko; Puri, Rohan S.; Olwal, Alex; Ushigome, Yosuke; Franciszkiewicz, Lukas; Chandra, Deepak; Poupyrev, Ivan; Raskar, Ramesh
TI  - CHI - Zensei: Embedded, Multi-electrode Bioimpedance Sensing for Implicit, Ubiquitous User Recognition
PY  - 2017
AB  - Interactions and connectivity is increasingly expanding to shared objects and environments, such as furniture, vehicles, lighting, and entertainment systems. For transparent personalization in such contexts, we see an opportunity for embedded recognition, to complement traditional, explicit authentication. We introduce Zensei, an implicit sensing system that leverages bio-sensing, signal processing and machine learning to classify uninstrumented users by their body's electrical properties. Zensei could allow many objects to recognize users. E.g., phones that unlock when held, cars that automatically adjust mirrors and seats, or power tools that restore user settings. We introduce wide-spectrum bioimpedance hardware that measures both amplitude and phase. It extends previous approaches through multi-electrode sensing and high-speed wireless data collection for embedded devices. We implement the sensing in devices and furniture, where unique electrode configurations generate characteristic profiles based on user's unique electrical properties. Finally, we discuss results from a comprehensive longitudinal 22-day data collection experiment with 46 subjects. Our analysis shows promising classification accuracy and low false acceptance rate.
SP  - 3972
EP  - 3985
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025536
ER  - 

TY  - NA
AU  - Albashaireh, Rasha; Ming, Hua
TI  - A Survey of Online Learning Platforms with Initial Investigation of Situation-Awareness to Facilitate Programming Education
PY  - 2018
AB  - With the advancement of today's ubiquitous technology, and due to the increasing number of technologies supported by the Internet, a variety of Online Learning Platforms have rapidly grown as modern learning methods. This fast-emerging learning option interests researchers to study and investigate the main features and functionality of the most popular Online Learning Platforms. This paper surveys the state-of-the-art Online Learning Platforms that aim to teach computer programming, in terms of principles, design, and implementations. In addition, the paper investigates the feasibility of incorporating human-oriented Situation-Awareness as the driving factor to facilitate the delivery of improved user learning experiences.
SP  - NA
EP  - NA
JF  - 2018 International Conference on Computational Science and Computational Intelligence (CSCI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/csci46756.2018.00126
ER  - 

TY  - NA
AU  - Drey, Tobias; Albus, Patrick; der Kinderen, Simon; Milo, Maximilian; Segschneider, Thilo; Chanzab, Linda; Rietzler, Michael; Seufert, Tina; Rukzio, Enrico
TI  - Towards Collaborative Learning in Virtual Reality: A Comparison of Co-Located Symmetric and Asymmetric Pair-Learning
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517641
ER  - 

TY  - NA
AU  - Arimatsu, Kazuyuki; Mori, Hideki
TI  - CHI - Evaluation of Machine Learning Techniques for Hand Pose Estimation on Handheld Device with Proximity Sensor
PY  - 2020
AB  - Tracking finger movement for natural interaction using hand is commonly studied. For vision-based implementations of finger tracking in virtual reality (VR) application, finger movement is occluded by a handheld device which is necessary for auxiliary input, thus tracking finger movement using cameras is still challenging. Finger tracking controllers using capacitive proximity sensors on the surface are starting to appear. However, research on estimating articulated hand pose from curved capacitance sensing electrodes is still immature. Therefore, we built a prototype with 62 electrodes and recorded training datasets using an optical tracking system. We have introduced 2.5D representation to apply convolutional neural network methods on a capacitive image of the curved surface, and two types of network architectures based on recent achievements in the computer vision field were evaluated with our dataset. We also implemented real-time interactive applications using the prototype and demonstrated the possibility of intuitive interaction using fingers in VR applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376712
ER  - 

TY  - JOUR
AU  - Seidel, Stefan; Berente, Nicholas; Martinez, Benoit; Lindberg, Aron; Lyytinen, Kalle; Nickerson, Jeffrey V.
TI  - Autonomous Tools in System Design: Reflective Practice in Ubisofts Ghost Recon Wildlands Project
PY  - 2018
AB  - Ubisoft’s game designers successfully used autonomous tools to develop an innovative virtual world. The authors discuss the reflective practices underlying this success and how autonomous tools enable more complex system design.
SP  - 16
EP  - 23
JF  - Computer
VL  - 51
IS  - 10
PB  - 
DO  - 10.1109/mc.2018.3971341
ER  - 

TY  - NA
AU  - Zhang, Yang; Yang, Chouchang (Jack); Hudson, Scott E.; Harrison, Chris; Sample, Alanson
TI  - Wall++
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173847
ER  - 

TY  - NA
AU  - Gomi, Hidehito; Yamaguchi, Shuji; Tsubouchi, Kota; Sasaya, Naomi
TI  - TrustCom/BigDataSE - Continuous Authentication System Using Online Activities
PY  - 2018
AB  - This paper poses the question, "Is it possible to identify users with just a set of Web activity logs?" The answer is yes: we can provide a continuous authentication system that does not require explicit actions by the users while monitoring their interactions regarding what they do for services as well as how they use their devices. We propose an activity-based authentication (ABA) system for active authentication that continuously verifies the identity of a user accessing multiple online services by means of their activity histories. ABA involves a machine-learning technique for authentication with a bagging-data-summarization approach, as it is difficult to identify users by using small logs. We assessed the performance and effect of various activity features extracted from the activity logs of 1,000 users of commercially deployed Web sites. Our findings provide valuable insights to guide the development of an authentication system utilizing the online activities of users.
SP  - 522
EP  - 532
JF  - 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/trustcom/bigdatase.2018.00080
ER  - 

TY  - NA
AU  - Laput, Gierad; Harrison, Chris
TI  - CHI - SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences
PY  - 2019
AB  - IoT appliances are gaining consumer traction, from smart thermostats to smart speakers. These devices generally have limited user interfaces, most often small buttons and touchscreens, or rely on voice control. Further, these devices know little about their surroundings unaware of objects, people and activities happening around them. Consequently, interactions with these "smart" devices can be cumbersome and limited. We describe SurfaceSight, an approach that enriches IoT experiences with rich touch and object sensing, offering a complementary input channel and increased contextual awareness. For sensing, we incorporate LIDAR into the base of IoT devices, providing an expansive, ad hoc plane of sensing just above the surface on which devices rest. We can recognize and track a wide array of objects, including finger input and hand gestures. We can also track people and estimate which way they are facing. We evaluate the accuracy of these new capabilities and illustrate how they can be used to power novel and contextually-aware interactive experiences.
SP  - 329
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300559
ER  - 

TY  - NA
AU  - Nguyen, Phuc; Muncuk, Ufuk; Ashok, Ashwin; Chowdhury, Kaushik R.; Gruteser, Marco; Vu, Tam
TI  - SenSys - Battery-Free Identification Token for Touch Sensing Devices
PY  - 2016
AB  - This paper proposes the design and implementation of low-- energy tokens for smart interaction with capacitive touch-- enabled devices by associating the token's identity with its contact, or touch. The proposed token's design features two key novel technical components: (1) a through--touch--sensor low--energy communication method for token identification and (2) a touch--sensor energy harvesting technique. The communication mechanism involves the token transmitting its identity (ID) directly through the touch--sensor by artificially modifying the effective capacitance between the touch-- sensor and token surfaces. This approach consumes significantly lower energy compared to traditional electrical signal modulation approaches. By enabling the token to harvest energy from touch--screen sensors or touch--surfaces the token is rendered battery--free. Through experimental evaluations using a prototype implementation, the proposed design is shown to achieve at least 95% identification accuracy. It is also shown to consume less energy than competitive techniques (NFC P2P and Bluetooth Low--Energy) for communicating a short ID sequence. The adoption of this technology among users is evaluated through a user study on 12 subjects.
SP  - 109
EP  - 122
JF  - Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2994551.2994566
ER  - 

TY  - NA
AU  - Laput, Gierad; Xiao, Robert; Harrison, Chris
TI  - UIST - ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers
PY  - 2016
AB  - Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Their position on the wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch's existing accelerometer to 4 kHz. Using this new source of high-fidelity data, we uncovered a wide range of applications. For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps, which combine with on-device motion tracking to create a wide range of expressive input modalities. Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling passive object recognition that can augment everyday experiences with context-aware functionality. Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body. Overall, our contributions unlock user interface techniques that previously relied on special-purpose and/or cumbersome instrumentation, making such interactions considerably more feasible for inclusion in future consumer devices.
SP  - 321
EP  - 333
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984582
ER  - 

TY  - NA
AU  - Surale, Hemant Bhaskar; Tham, Yu Jiang; Smith, Brian A.; Vaish, Rajan
TI  - ARcall: Real-Time AR Communication using Smartphones and Smartglasses
PY  - 2022
AB  - Augmented Reality (AR) smartglasses are increasingly regarded as the next generation personal computing platform. However, there is a lack of understanding about how to design communication systems using them. We present ARcall, a novel Augmented Reality-based real-time communication system that enables an immersive, delightful, and privacy-preserving experience between a smartphone user and a smartglasses wearer. ARcall allows a remote friend (Friend) to send and project AR content to a smartglasses wearer (Wearer). The ARcall system was designed with the practical limits of existing AR glasses in mind, including shorter battery life and a reduced field of view. We conduct a qualitative evaluation of the three main components of ARcall: Drop-In, ARaction, and Micro-Chat. Our results provide novel insights for building future AR-based communication methods, including, the importance of context priming, user control over AR content placement, and the feeling of co-presence while conversing.
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519398
ER  - 

TY  - NA
AU  - Du, Xiaoyu; Hargreaves, Christopher; Sheppard, John; Anda, Felix; Sayakkara, Asanka; Le-Khac, Nhien-An; Scanlon, Mark
TI  - SoK: Exploring the State of the Art and the Future Potential of Artificial Intelligence in Digital Forensic Investigation
PY  - 2020
AB  - Multi-year digital forensic backlogs have become commonplace in law enforcement agencies throughout the globe. Digital forensic investigators are overloaded with the volume of cases requiring their expertise compounded by the volume of data to be processed. Artificial intelligence is often seen as the solution to many big data problems. This paper summarises existing artificial intelligence based tools and approaches in digital forensics. Automated evidence processing leveraging artificial intelligence based techniques shows great promise in expediting the digital forensic analysis process while increasing case processing capacities. For each application of artificial intelligence highlighted, a number of current challenges and future potential impact is discussed.
SP  - NA
EP  - NA
JF  - Proceedings of the 15th International Conference on Availability, Reliability and Security
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3407023.3407068
ER  - 

TY  - JOUR
AU  - Piumsomboon, Thammathip; Dey, Arindam; Ens, Barrett; Lee, Gun A.; Billinghurst, Mark
TI  - The Effects of Sharing Awareness Cues in Collaborative Mixed Reality.
PY  - 2019
AB  - Augmented and Virtual Reality provide unique capabilities for Mixed Reality collaboration. This paper explores how different combinations of virtual awareness cues can provide users with valuable information about their collaborator's attention and actions. In a user study (n = 32, 16 pairs), we compared different combinations of three cues: Field-of-View (FoV) frustum, Eye-gaze ray, and Head-gaze ray against a baseline condition showing only virtual representations of each collaborator's head and hands. Through a collaborative object finding and placing task, the results showed that awareness cues significantly improved user performance, usability, and subjective preferences, with the combination of the FoV frustum and the Head-gaze ray being best. This work establishes the feasibility of room-scale MR collaboration and the utility of providing virtual awareness cues.
SP  - 5
EP  - 5
JF  - Frontiers in robotics and AI
VL  - 6
IS  - NA
PB  - 
DO  - 10.3389/frobt.2019.00005
ER  - 

TY  - JOUR
AU  - Semsar, Azin; McGowan, Hannah; Feng, Yuanyuan; Zahiri, H. Reza; Park, Adrian; Kleinsmith, Andrea; Mentis, Helena M.
TI  - How Trainees Use the Information from Telepointers in Remote Instruction
PY  - 2019
AB  - Researchers have shown both performance drawbacks and benefits of using telepointers or similar display overlay-technologies in remote instruction; however, there is not a clear understanding of why there are these performance effects. This poses a challenge in knowing how and when to successfully use or design telepointing technologies in remote instruction. A better understanding is needed with the rise of remote workers in a wide array of industries from oil rig repair to surgery, and the proliferation of heads-up displays or telecommunications devices to support these future work practices. In this study, we explore how the information conveyed through a telepointer is taken up and acted upon by surgical trainees in a laparoscopic surgical telementoring setting. We collected audio and video data of 12 surgical trainees who performed standard laparoscopic surgical tasks on a physical model under the guidance of a surgical trainer. We investigated both action and talk to determine how the telepointer-based information was used. Our findings reveal three main challenges in using the instructional information conveyed through the telepointer including the trainees' tendency of attending to the telepointer instruction as the primary source of information. We argue that the found challenges are socio-technical in nature and require a redesign of the mentoring context as well as the technological tools.
SP  - 93
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359195
ER  - 

TY  - NA
AU  - Wang, Peng; Min, Haitao; Bai, Xiaoliang; Billinghurst, Mark; Zhang, Shusheng; Han, Dechuan; Lv, Hao; He, Weiping; Yan, Yuxiang; Zhang, Xiangyu
TI  - ISMAR Adjunct - An MR Remote Collaborative Platform Based on 3D CAD Models for Training in Industry
PY  - 2019
AB  - In this paper, we describe a new Mixed Reality (MR) remote collaborative platform making use of 3D CAD models for training in the manufacturing industry. It enables a remote expert in Virtual Reality (VR) to train a local worker in a physical assembly task. For the local site, we use Spatial Augmented Reality (SAR) to enable the local worker see virtual cues without wearing any AR devices, leaving their user hands free to easily manipulate the physical parts. For the remote expert, we construct a 3D virtual environment using virtual replicas of the physical parts. We also report on the results of a usability study of the prototype.
SP  - 91
EP  - 92
JF  - 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2019.00038
ER  - 

TY  - NA
AU  - Dobinson, Rhett; Teyssier, Marc; Steimle, Jürgen; Fruchard, Bruno
TI  - MicroPress: Detecting Pressure and Hover Distance in Thumb-to-Finger Interactions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567698
ER  - 

TY  - JOUR
AU  - Wang, Peng; Bai, Xiaoliang; Billinghurst, Mark; Zhang, Shusheng; Zhang, Xiangyu; Wang, Shuxia; He, Weiping; Yan, Yuxiang; Ji, Hongyu
TI  - AR/MR Remote Collaboration on Physical Tasks: A Review
PY  - 2021
AB  - NA
SP  - 102071
EP  - NA
JF  - Robotics and Computer-Integrated Manufacturing
VL  - 72
IS  - NA
PB  - 
DO  - 10.1016/j.rcim.2020.102071
ER  - 

TY  - NA
AU  - Piumsomboon, Thammathip; Lee, Gun A.; Irlitti, Andrew; Ens, Barrett; Thomas, Bruce H.; Billinghurst, Mark
TI  - CHI - On the Shoulder of the Giant: A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible Interaction
PY  - 2019
AB  - We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction.
SP  - 228
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300458
ER  - 

TY  - NA
AU  - Villar, Nicolas; Cletheroe, Daniel; Saul, Greg; Holz, Christian; Regan, Tim; Salandin, Oscar; Sra, Misha; Yeo, Hui-Shyong; Field, William; Zhang, Haiyan
TI  - CHI - Project Zanzibar: A Portable and Flexible Tangible Interaction Platform
PY  - 2018
AB  - We present Project Zanzibar: a flexible mat that can locate, uniquely identify and communicate with tangible objects placed on its surface, as well as sense a user's touch and hover hand gestures. We describe the underlying technical contributions: efficient and localised Near Field Communication (NFC) over a large surface area; object tracking combining NFC signal strength and capacitive footprint detection, and manufacturing techniques for a rollable device form-factor that enables portability, while providing a sizable interaction area when unrolled. In addition, we detail design patterns for tangibles of varying complexity and interactive capabilities, including the ability to sense orientation on the mat, harvest power, provide additional input and output, stack, or extend sensing outside the bounds of the mat. Capabilities and interaction modalities are illustrated with self-generated applications. Finally, we report on the experience of professional game developers building novel physical/digital experiences using the platform.
SP  - 515
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174089
ER  - 

TY  - NA
AU  - Han, Teng; Han, Qian; Annett, Michelle; Anderson, Fraser; Huang, Da-Yuan; Yang, Xing-Dong
TI  - UIST - Frictio: Passive Kinesthetic Force Feedback for Smart Ring Output
PY  - 2017
AB  - Smart rings have a unique form factor suitable for many applications, however, they offer little opportunity to provide the user with natural output. We propose passive kinesthetic force feedback as a novel output method for rotational input on smart rings. With this new output channel, friction force profiles can be designed, programmed, and felt by a user when they rotate the ring. This modality enables new interactions for ring form factors. We demonstrate the potential of this new haptic output method though Frictio, a prototype smart ring. In a controlled experiment, we determined the recognizability of six force profiles, including Hard Stop, Ramp-Up, Ramp-Down, Resistant Force, Bump, and No Force. The results showed that participants could distinguish between the force profiles with 94% accuracy. We conclude by presenting a set of novel interaction techniques that Frictio enables, and discuss insights and directions for future research.
SP  - 131
EP  - 142
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126622
ER  - 

TY  - NA
AU  - Chan, Edwin
TI  - User-defined Single-hand Microgestures
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.11575/prism/28612
ER  - 

TY  - JOUR
AU  - Khan, Shahroz; Kaklis, Panagiotis
TI  - From regional sensitivity to intra-sensitivity for parametric analysis of free-form shapes: application to ship design
PY  - 2021
AB  - NA
SP  - 101314
EP  - NA
JF  - Advanced Engineering Informatics
VL  - 49
IS  - NA
PB  - 
DO  - 10.1016/j.aei.2021.101314
ER  - 

TY  - CHAP
AU  - Scaffidi, Christopher
TI  - ICITS - On-Ramps to Learning: The Progression of Learners Through Topics in the Online LabVIEW Forum.
PY  - 2019
AB  - Online forums can facilitate collaborative learning in situations where instructors impose structure promoting constructive interaction among students. This paper presents an investigation of how well an online forum, such as the LabVIEW programmer forum, supports learning in the absence of instructor-imposed structure. This study focuses first on whether specific topics served to draw users into the community and second on whether users displayed evidence of learning over time. Unsupervised machine learning on 475,094 posts in the LabVIEW forum identified 974 topical clusters among these posts, and statistical analysis confirmed that a minority (30%) of clusters accounted for over 70% of users’ initial posts. Linear regression revealed that subsequent posts by each user were indeed more likely to be flagged by the community as valuable, offering potential evidence of learning. However, this trend was not strong or uniform, suggesting the need for additional innovations in information technologies to support independent learning.
SP  - 791
EP  - 801
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-11890-7_74
ER  - 

TY  - NA
AU  - Agarwal, Sonu; Mondal, Arindam; Joshi, Gurdeepak; Gupta, Gaurav
TI  - AH - GestGlove: a wearable device with gesture based touchless interaction
PY  - 2017
AB  - Wearable devices have gained significant popularity in recent years and have enabled new modes of human computer interaction. Wearables have been explored to track user movements and gestures but typically require large arm motions. In this paper we propose a glove based wearable that is designed for active users like cyclists, motorists and skiers who typically wear a glove during their activity. The GestGlove has a gesture based user interface that enables single-handed touchless interaction. Inertial and bend sensors mounted on the device track subtle wrist and finger movements which are intuitive and convenient. We use the popular approach of Dynamic Time Warping (DTW) for the classification of this multivariate time series data. A Mahalanobis distance measure for DTW is proposed that uses the DTW warp path iteratively for metric calculation, an aspect that has not been explored in prior work.
SP  - 3
EP  - NA
JF  - Proceedings of the 8th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3041164.3041172
ER  - 

TY  - NA
AU  - Utzig, Sebastian; Kaps, Robert; Azeem, Syed Muhammad; Gerndt, Andreas
TI  - Augmented Reality for Remote Collaboration in Aircraft Maintenance Tasks
PY  - 2019
AB  - In this paper, we present a concept study to facilitate maintenance of an operating aircraft based on its lifelong collected data, called Digital Twin. It demonstrates a damage assessment scenario on a real aircraft component. We propose a graphical user interface that contains menu-guided instructions and inspection documentation to increase the efficiency of manual processes. Furthermore, experts located at different sites can join via a virtual session. By inspecting a 3D model of the aircraft component, they can see synchronized information from a Digital Twin database. With Augmented Reality glasses, the Microsoft HoloLens, a Digital Twin can be experienced personally. In the inspector's view, the 3D model of the Digital Twin is directly superimposed on the physical component. This Mixed Reality Vision can be used for inspection purposes. Any inspection related information can be directly attached to the component. For example, damage locations are marked by the inspector on the component's surface and are stored in the Digital Twin database. Our scenario demonstrates how new information can be derived from the combination of collected data and analyses from the Digital Twin database. This information is used to maintain the continued airworthiness of the aircraft. Feedback from domain related engineers confirm that our interface has an enormous potential for solving current maintenance problems in the aviation industry. Additionally, our study provides ideas for the integration of further analysis functions into the interface.
SP  - 1
EP  - 10
JF  - 2019 IEEE Aerospace Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aero.2019.8742228
ER  - 

TY  - BOOK
AU  - Shen, Ruiqi; Wohn, Donghee Yvette; Lee, Michael J.
TI  - CompEd - Comparison of Learning Programming Between Interactive Computer Tutors and Human Teachers
PY  - 2019
AB  - People typically learn programming from teachers in in-person courses or online tutorials. Interactive computer tutors---systems that deliver learning content interactively---have become more prevalent in online settings for teaching skills such as computer programming. Research has shown the efficiency and effectiveness of learning programming from teachers, interactive computer tutors, and a combination of both. However, there is limited understanding of learners' comparative perspectives about their experience learning from these different resources. We conducted an exploratory study using semi-structured interviews, recruiting 20 participants that had experience learning programming from both teachers and interactive computer tutors. We identified factors that learners like and dislike from both learning methods and discussed the strengths and weaknesses of them. Based on our findings, we propose suggestions for designers of interactive computer tutors, and for programming educators.
SP  - 2
EP  - 8
JF  - Proceedings of the ACM Conference on Global Computing Education
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3300115.3309506
ER  - 

TY  - CHAP
AU  - Freiwald, Jann Philipp; Gollek, Sünje; Steinicke, Frank
TI  - INTERACT (1) - VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity
PY  - 2021
AB  - Virtual Reality (VR) is a promising immersive technology, which provides users with place and plausibility illusions in a virtual environment (VE). However, current immersive experiences are often limited to those users wearing a VR head-mounted display (HMD). In this paper we present VR Invite, a project-independent smartphone app, which allows multiple non-immersive bystanders to observe and interact with the VE and the HMD users. Our system renders multiple view ports of the scene on a host computer, and transmits the data via wireless local network to the mobile devices. Furthermore, the position and orientation of the smartphones is tracked to change the viewpoints accordingly.
SP  - 352
EP  - 370
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85623-6_22
ER  - 

TY  - JOUR
AU  - Kwon, Jin Uk; Hwang, Jae-In; Park, Jaeyoung; Ahn, Sang Chul
TI  - Fully Asymmetric Remote Collaboration System
PY  - 2019
AB  - In this paper, we investigate the requirements for fully asymmetric remote collaboration and present a prototype system. Unlike previously studied asymmetric remote collaboration systems, fully asymmetric remote collaboration handles the cases, where a user of one side is a complete novice having difficulty in explaining his/her problems and unfamiliar with a collaboration system, and thus the remote expert of the other side must locate the local user's problem and teach the local user by having full control of the remote collaboration system. These cases can frequently occur when we collaborate with elderly people. For a fully asymmetric remote collaboration system, there are some requirements which are a little bit different from those of an asymmetric one. We itemize them in the three phases of collaboration: identifying a problem, finding a solution, and showing the solution to a local user. We also design and implement a prototype of a fully asymmetric collaboration system to meet the requirements. Our system consists of a robot with cameras and a projector for a local user and an HMD (head mounted display) with a controller for a remote expert. We also design an efficient HMD-based interaction method for fully asymmetric remote collaboration with our system. We show the efficiency of our prototype system in a fully asymmetric remote collaboration situation by user evaluation.
SP  - 54155
EP  - 54166
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2912397
ER  - 

TY  - JOUR
AU  - Ryohei, Matsui; Tanuma, Iwao; Kawahara, Ryotaro; Ushio, Naoko; Yoshimoto, Hiroyuki; Kawamura, Tetsufumi; Sugii, Nobuyuki
TI  - Identifying Handwork with Machine Learning Data Sets from Sensors Built into Gloves
PY  - 2021
AB  - NA
SP  - 284
EP  - 291
JF  - IEEJ Transactions on Sensors and Micromachines
VL  - 141
IS  - 8
PB  - 
DO  - 10.1541/ieejsmas.141.284
ER  - 

TY  - NA
AU  - Gong, Jun; Zhang, Yang; Zhou, Xia; Yang, Xing-Dong
TI  - UIST - Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing
PY  - 2017
AB  - We present Pyro, a micro thumb-tip gesture recognition technique based on thermal infrared signals radiating from the fingers. Pyro uses a compact, low-power passive sensor, making it suitable for wearable and mobile applications. To demonstrate the feasibility of Pyro, we developed a self-contained prototype consisting of the infrared pyroelectric sensor, a custom sensing circuit, and software for signal processing and machine learning. A ten-participant user study yielded a 93.9% cross-validation accuracy and 84.9% leave-one-session-out accuracy on six thumb-tip gestures. Subsequent lab studies demonstrated Pyro's robustness to varying light conditions, hand temperatures, and background motion. We conclude by discussing the insights we gained from this work and future research questions.
SP  - 553
EP  - 563
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126615
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Tan, Lu; Zhang, Yuji; Seyed, Teddy; Yang, Xing-Dong
TI  - UIST - Capacitivo: Contact-Based Object Recognition on Interactive Fabrics using Capacitive Sensing
PY  - 2020
AB  - We present Capacitivo, a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace. To demonstrate our technique, we created a prototype composed of a 12 x 12 grid of electrodes, made from conductive fabric attached to a textile substrate. We designed the size and separation between the electrodes to maximize the sensing area and sensitivity. We then used a 10-person study to evaluate the performance of our sensing technique using 20 different objects, which yielded a 94.5% accuracy rate. We conclude this work by presenting several different application scenarios to demonstrate unique interactions that are enabled by our technique on fabrics.
SP  - 649
EP  - 661
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415829
ER  - 

TY  - JOUR
AU  - Incel, Ozlem Durmaz; Gunay, Secil; Akan, Yasemin; Barlas, Yunus; Basar, Okan Engin; Alptekin, Gülfem Isiklar; Isbilen, Mustafa
TI  - DAKOTA: Sensor and Touch Screen-Based Continuous Authentication on a Mobile Banking Application
PY  - 2021
AB  - Authenticating a user in the right way is essential to IT systems, where the risks are becoming more and more complex. Especially in the mobile world, banking applications are among the most delicate systems requiring strict rules and regulations. Existing approaches often require point-of-entry authentication accompanied by a one-time password as a second-factor authentication. However, this requires active participation of the user and there is continuous authentication during a session. In this paper, we investigate whether it is possible to continuously authenticate users via behavioral biometrics with a certain performance on a mobile banking application. A currently used mobile banking application in Turkey is chosen as the case, and we developed a continuous authentication scheme, named DAKOTA, on top of this application. The DAKOTA system records data from the touch screen and the motion sensors on the phone to monitor and model the user’s behavioral patterns. Forty-five participants completed the predefined banking transactions. This data is used to train seven different classification algorithms. The results reveal that binary-SVM with RBF kernel reaches the lowest error scores, 3.5% equal error rate (EER). Using the end-to-end DAKOTA system, we investigate the performance in real-time, both in terms of authentication accuracy and resource usage. We show that it does not bring extra overhead in terms of power and memory usage compared to the original banking application and we can achieve a 90% true positive recognition rate, on average.
SP  - 38943
EP  - 38960
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3063424
ER  - 

TY  - NA
AU  - Ito, Hiroki; Shimakawa, Hiromitsu; Harada, Fumiko
TI  - FedCSIS - Comprehension analysis considering programming thinking ability using code puzzle
PY  - 2020
AB  - In programming education, the instructor tries to find out the learners who needs help by grasping the learners’ development of understanding using tests that require knowledge. However, in reality, not many learners will acquire the skill of writing source codes. This kind of current situation implies that programming ability of learners cannot be measured by tests that require knowledge. This paper focuses on not only the knowledge items required for programming but also the programming thinking (computational thinking), which is the ability to combine the constituent elements of the program. In this paper, we propose a method to estimate the learner’s understanding from the learner’s process to solve the code puzzles that require programming thinking as well as knowledge. We developed the interface to realize the proposed method. The experimental result with the interface showed that the proposed method could estimate with the accuracy of 80% or more.
SP  - 609
EP  - 618
JF  - Proceedings of the 2020 Federated Conference on Computer Science and Information Systems
VL  - 21
IS  - NA
PB  - 
DO  - 10.15439/2020f44
ER  - 

TY  - NA
AU  - Kolkmeier, Jan; Harmsen, Emiel; Giesselink, Sander; Reidsma, Dennis; Theune, Mariët; Heylen, Dirk
TI  - VRST - With a little help from a holographic friend: the OpenIMPRESS mixed reality telepresence toolkit for remote collaboration systems
PY  - 2018
AB  - Remote mixed reality (MR) collaboration systems allow for multimodal, real-time support from remote experts. We present our open toolkit that provides a flexible end-to-end solution for building such systems using off-the-shelf hardware. From related work, three core design aspects have been identified: 1) the independence of the viewpoint that the visitor (the remote expert) can take in relation to position and viewpoint of the visitee, 2) the immersiveness of the presentation technology for visitor and visitee, and 3) the extent to which the visitor's body is represented in the visitee's environment. This paper describes the implementation of our system, which includes these aspects. In a study aimed at validating whether we implemented these core aspects to good effect, conducted with a collaborative puzzle application built with our toolkit, we examine how variations of these aspects contribute to usability, performance and social presence related metrics.
SP  - 26
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281542
ER  - 

TY  - JOUR
AU  - Chen, Wei; Fuge, Mark
TI  - Beyond the Known: Detecting Novel Feasible Domains Over an Unbounded Design Space
PY  - 2017
AB  - <jats:p>To solve a design problem, sometimes it is necessary to identify the feasible design space. For design spaces with implicit constraints, sampling methods are usually used. These methods typically bound the design space; that is, limit the range of design variables. But bounds that are too small will fail to cover all possible designs, while bounds that are too large will waste sampling budget. This paper tries to solve the problem of efficiently discovering (possibly disconnected) feasible domains in an unbounded design space. We propose a data-driven adaptive sampling technique—ε-margin sampling, which learns the domain boundary of feasible designs and also expands our knowledge on the design space as available budget increases. This technique is data-efficient, in that it makes principled probabilistic trade-offs between refining existing domain boundaries versus expanding the design space. We demonstrate that this method can better identify feasible domains on standard test functions compared to both random and active sampling (via uncertainty sampling). However, a fundamental problem when applying adaptive sampling to real world designs is that designs often have high dimensionality and thus require (in the worst case) exponentially more samples per dimension. We show how coupling design manifolds with ε-margin sampling allows us to actively expand high-dimensional design spaces without incurring this exponential penalty. We demonstrate this on real-world examples of glassware and bottle design, where our method discovers designs that have different appearance and functionality from its initial design set.</jats:p>
SP  - 111405
EP  - NA
JF  - Journal of Mechanical Design
VL  - 139
IS  - 11
PB  - 
DO  - 10.1115/1.4037306
ER  - 

TY  - JOUR
AU  - Hsia, Tzu-Hsuan; Okamoto, Shogo; Akiyama, Yasuhiro; Yamada, Yoji
TI  - One-touch calibration of hum-noise-based touch sensor for unknown users utilizing models trained by different users
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Hum-noise-based touch sensors (HumTouch) are capable of recognizing human touch on semiconductive materials using the current leaking from the finger to the surface. Thus far, calibration for these hum-noise-based touch sensors has been performed for individual users because of the individual differences in hum-driven electric currents in human bodies. However, for applications designed for unknown users, time-consuming calibration for individual users is not preferred, and a new user should be able to use the sensor immediately. For this purpose, we propose a new calibration method for HumTouch. In this method, learning datasets collected from multiple people and a few extra samples from a new user are collectively used to establish a touch localization estimator. The estimator is computed using the kernel regression method with weighted samples from the new user. For a 20 <jats:inline-formula><jats:alternatives><jats:tex-math>$$\times $$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:mo>×</mml:mo> </mml:math></jats:alternatives></jats:inline-formula> 18 cm<jats:inline-formula><jats:alternatives><jats:tex-math>$$^2$$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:msup> <mml:mrow /> <mml:mn>2</mml:mn> </mml:msup> </mml:math></jats:alternatives></jats:inline-formula> paper, the mean localization error is reduced from 1.24 cm to 0.90 cm with only one sample from the new user. Hence, a new user can establish a semipersonalized localization estimator by touching only one point on the surface. This method improves the localization performance of HumTouch sensors in an easy-to-access manner.</jats:p>
SP  - NA
EP  - NA
JF  - ROBOMECH Journal
VL  - 9
IS  - 1
PB  - 
DO  - 10.1186/s40648-022-00238-4
ER  - 

TY  - JOUR
AU  - Gratch, Jonathan
TI  - The Promise and Peril of Automated Negotiators
PY  - 2021
AB  - NA
SP  - 13
EP  - 34
JF  - Negotiation Journal
VL  - 37
IS  - 1
PB  - 
DO  - 10.1111/nejo.12348
ER  - 

TY  - NA
AU  - Piumsomboon, Thammathip; Lee, Gun A.; Hart, Jonathon D.; Ens, Barrett; Lindeman, Robert W.; Thomas, Bruce H.; Billinghurst, Mark
TI  - CHI - Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration
PY  - 2018
AB  - We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.
SP  - 46
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173620
ER  - 

TY  - NA
AU  - Nguyen, Viet; Ibrahim, Mohamed; Truong, Hoang; Nguyen, Phuc; Gruteser, Marco; Howard, Richard; Vu, Tam
TI  - MobiCom - Body-Guided Communications: A Low-power, Highly-Confined Primitive to Track and Secure Every Touch
PY  - 2018
AB  - The growing number of devices we interact with require a convenient yet secure solution for user identification, authorization and authentication. Current approaches are cumbersome, susceptible to eavesdropping and relay attacks, or energy inefficient. In this paper, we propose a body-guided communication mechanism to secure every touch when users interact with a variety of devices and objects. The method is implemented in a hardware token worn on user's body, for example in the form of a wristband, which interacts with a receiver embedded inside the touched device through a body-guided channel established when the user touches the device. Experiments show low-power (uJ/bit) operation while achieving superior resilience to attacks, with the received signal at the intended receiver through the body channel being at least 20dB higher than that of an adversary in cm range.
SP  - 353
EP  - 368
JF  - Proceedings of the 24th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3241539.3241550
ER  - 

TY  - JOUR
AU  - Merrill, Nick; Curran, Max T.; Gandhi, Swapan; Chuang, John
TI  - One-Step, Three-Factor Passthought Authentication With Custom-Fit, In-Ear EEG.
PY  - 2019
AB  - In-ear EEG offers a promising path toward usable, discreet brain-computer interfaces (BCIs) for both healthy individuals and persons with disabilities. To test the promise of this modality, we produced a brain-based authentication system using custom-fit EEG earpieces. In a sample of N=7 participants, we demonstrated that our system has high accuracy, higher than prior work using non-custom earpieces. We demonstrated that both inherence and knowledge factors contribute to authentication accuracy, and performed a simulated attack to show our system's robustness against impersonation. From an authentication standpoint, our system provides three factors of authentication in a single step. From a usability standpoint, our system does not require a cumbersome, head-worn device.
SP  - 354
EP  - 354
JF  - Frontiers in neuroscience
VL  - 13
IS  - NA
PB  - 
DO  - 10.3389/fnins.2019.00354
ER  - 

TY  - NA
AU  - Zhang, Cheng; Xue, Qiuyue; Waghmare, Anandghan; Meng, Ruichen; Jain, Sumeet; Han, Yizeng; Li, Xinyu; Cunefare, Kenneth A.; Ploetz, Thomas; Starner, Thad; Inan, Omer T.; Abowd, Gregory D.
TI  - CHI - FingerPing: Recognizing Fine-grained Hand Poses using Active Acoustic On-body Sensing
PY  - 2018
AB  - FingerPing is a novel sensing technique that can recognize various fine-grained hand poses by analyzing acoustic resonance features. A surface-transducer mounted on a thumb ring injects acoustic chirps (20Hz to 6,000Hz) to the body. Four receivers distributed on the wrist and thumb collect the chirps. Different hand poses of the hand create distinct paths for the acoustic chirps to travel, creating unique frequency responses at the four receivers. We demonstrate how FingerPing can differentiate up to 22 hand poses, including the thumb touching each of the 12 phalanges on the hand as well as 10 American sign language poses. A user study with 16 participants showed that our system can recognize these two sets of poses with an accuracy of 93.77% and 95.64%, respectively. We discuss the opportunities and remaining challenges for the widespread use of this input technique.
SP  - 437
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174011
ER  - 

TY  - JOUR
AU  - Ahmed, Faez; Fuge, Mark
TI  - Ranking Ideas for Diversity and Quality
PY  - 2017
AB  - When selecting ideas or trying to find inspiration, designers often must sift through hundreds or thousands of ideas. This paper provides an algorithm to rank design ideas such that the ranked list simultaneously maximizes the quality and diversity of recommended designs. To do so, we first define and compare two diversity measures using Determinantal Point Processes (DPP) and additive sub-modular functions. We show that DPPs are more suitable for items expressed as text and that a greedy algorithm diversifies rankings with both theoretical guarantees and empirical performance on what is otherwise an NP-Hard problem. To produce such rankings, this paper contributes a novel way to extend quality and diversity metrics from sets to permutations of ranked lists. These rank metrics open up the use of multi-objective optimization to describe trade-offs between diversity and quality in ranked lists. We use such trade-off fronts to help designers select rankings using indifference curves. However, we also show that rankings on trade-off front share a number of top-ranked items; this means reviewing items (for a given depth like the top 10) from across the entire diversity-to-quality front incurs only a marginal increase in the number of designs considered. While the proposed techniques are general purpose enough to be used across domains, we demonstrate concrete performance on selecting items in an online design community (OpenIDEO), where our approach reduces the time required to review diverse, high-quality ideas from around 25 hours to 90 minutes. This makes evaluation of crowd-generated ideas tractable for a single designer. Our code is publicly accessible for further research.
SP  - 011101
EP  - NA
JF  - Journal of Mechanical Design
VL  - 140
IS  - 1
PB  - 
DO  - 10.1115/1.4038070
ER  - 

TY  - NA
AU  - Fetter, Mirko; Bimamisa, David; Gross, Tom
TI  - CHI Extended Abstracts - Task-Based Focus and AdHoc-Focus-Territory: Novel Concepts for Shared Interactive Surfaces
PY  - 2016
AB  - Shared Interactive Surfaces allow co-located users to collaboratively work on a task. As current technology often is not able to distinguish between different users, there is a potential for concurrent conflicting actions of multiple users, leading to unwanted results and accordingly frustration. With our concepts for Task-Based Focus and AdHoc-Focus-Territory we provide light-weight solutions Integrated in our toolkit TUIOFX - for designers of multi-user, multi-touch applications. Our solution helps to overcome some of the problems of anonymous touch input, without an immediate need for more heavy-weight mechanisms like user identification.
SP  - 1193
EP  - 1200
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892446
ER  - 

TY  - NA
AU  - Grosse-Puppendahl, Tobias; Holz, Christian; Cohn, Gabe; Wimmer, Raphael; Bechtold, Oskar; Hodges, Steve; Reynolds, Matthew S.; Smith, Joshua R.
TI  - CHI - Finding Common Ground: A Survey of Capacitive Sensing in Human-Computer Interaction
PY  - 2017
AB  - For more than two decades, capacitive sensing has played a prominent role in human-computer interaction research. Capacitive sensing has become ubiquitous on mobile, wearable, and stationary devices - enabling fundamentally new interaction techniques on, above, and around them. The research community has also enabled human position estimation and whole-body gestural interaction in instrumented environments. However, the broad field of capacitive sensing research has become fragmented by different approaches and terminology used across the various domains. This paper strives to unify the field by advocating consistent terminology and proposing a new taxonomy to classify capacitive sensing approaches. Our extensive survey provides an analysis and review of past research and identifies challenges for future work. We aim to create a common understanding within the field of human-computer interaction, for researchers and practitioners alike, and to stimulate and facilitate future research in capacitive sensing.
SP  - 3293
EP  - 3315
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025808
ER  - 

TY  - NA
AU  - Hartmann, Stefan; Weinmann, Michael; Wessel, Raoul; Klein, Reinhard
TI  - StreetGAN: towards road network synthesis with generative adversarial networks
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Matthies, Denys J.C.; Bieber, Gerald; Kaulbars, Uwe
TI  - iWOAR - AGIS: automated tool detection & hand-arm vibration estimation using an unmodified smartwatch
PY  - 2016
AB  - Over the past three decades, it has been known that long-lasting and intense hand-arm vibrations (HAV) can cause serious diseases, such as the Raynaud- / White Finger-Syndrome. In order to protect workers nowadays, the long-term use of tools such as a drill, grinder, rotary hammer etc. underlie strict legal regulations. However, users rarely comply with these regulations because it is quite hard to manually estimate vibration intensity throughout the day. Therefore, we propose a wearable system that automatically counts the daily HAV exposure doses due to the fact that we are able to determine the currently used tool. With the implementation of AGIS, we demonstrate the technical feasibility of using the integrated microphone and accelerometer from a commercial smartwatch. In contrast to prior works, our approach does not require a technical modification of the smartwatch nor an instrumentation of the environment or the tool. A pilot study shows our proof-of-concept to be applicable in real workshop environments.
SP  - 8
EP  - NA
JF  - Proceedings of the 3rd International Workshop on Sensor-based Activity Recognition and Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2948963.2948971
ER  - 

TY  - NA
AU  - Chen, Yan; Herskovitz, Jaylin; Matute, Gabriel; Wang, April Yi; Lee, Sang Won; Lasecki, Walter S.; Oney, Steve
TI  - VL/HCC - EdCode: Towards Personalized Support at Scale for Remote Assistance in CS Education
PY  - 2020
AB  - Programming support methods, like discussion fo-rums and office hours, are important in CS education, but difficult to scale. In this paper, we introduce EdCode, a system that allows students to seek remote instructional support within their IDE in a way that resembles in-person support. It also allows instructors to provide contextualized responses by referencing students’ code, and curate and publish their answers for an entire class by selecting only the relevant part of the code referenced, thereby helping to avoid plagiarism. We evaluated EdCode with a series of usability studies and identified benefits and challenges for its use in programming courses. Students found that the perceived quality of support from EdCode was comparable to that of support from in-person office hours, and both students and instructors found publishing and viewing other students’ answers helpful.
SP  - 1
EP  - 5
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127260
ER  - 

TY  - NA
AU  - Sousa, Maurício; Anjos, Rafael Kuffner dos; Mendes, Daniel; Billinghurst, Mark; Jorge, Joaquim
TI  - CHI - Warping Deixis: Distorting Gestures to Enhance Collaboration
PY  - 2019
AB  - When engaged in communication, people often rely on pointing gestures to refer to out-of-reach content. However, observers frequently misinterpret the target of a pointing gesture. Previous research suggests that to perform a pointing gesture, people place the index finger on or close to a line connecting the eye to the referent, while observers interpret pointing gestures by extrapolating the referent using a vector defined by the arm and index finger. In this paper we present Warping Deixis, a novel approach to improving the perception of pointing gestures and facilitate communication in collaborative Extended Reality environments. By warping the virtual representation of the pointing individual, we are able to match the pointing expression to the observer's perception. We evaluated our approach in a co-located side by side virtual reality scenario. Results suggest that our approach is effective in improving the interpretation of pointing gestures in shared virtual environments.
SP  - 608
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300838
ER  - 

TY  - NA
AU  - Sahdev, Sidharth; Forlines, Clifton; Jota, Ricardo; Araujo, Bruno; Moseley, Braon; Deber, Jonathan; Sanders, Steven Leonard; Leigh, Darren; Wigdor, Daniel
TI  - CHI - GhostID: Enabling Non-Persistent User Differentiation in Frequency-Division Capacitive Multi-Touch Sensors
PY  - 2017
AB  - Current touch devices are adept at tracking finger touches, but cannot distinguish if multiple touches are caused by different fingers on a single hand, by fingers from both hands of a single user, or by different users. This limitation significantly reduces the possibilities for interaction techniques in touch interfaces. We present GhostID, a capacitive sensor that can differentiate the origins of multiple simultaneous touches. Our approach analyzes the signal ghosting, already present as an artifact in a frequency-division touch controller, to differentiate touches from the same hand or different hands of a single user (77% reliability at 60 fps) or from two different users (95% reliability at 60 fps). In addition to GhostID, we also develop a framework of user-differentiation capabilities for touch input devices, and illustrate a set of interaction techniques enabled by GhostID.
SP  - 15
EP  - 27
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025719
ER  - 

TY  - NA
AU  - Chung, John Joon Young; Shin, Hijung Valentina; Xia, Haijun; Wei, Li-Yi; Kazi, Rubaiat Habib
TI  - CHI - Beyond Show of Hands: Engaging Viewers via Expressive and Scalable Visual Communication in Live Streaming
PY  - 2021
AB  - Live streaming is gaining popularity across diverse application domains in recent years. A core part of the experience is streamer-viewer interaction, which has been mainly text-based. Recent systems explored extending viewer interaction to include visual elements with richer expression and increased engagement. However, understanding expressive visual inputs becomes challenging with many viewers, primarily due to the relative lack of structure in visual input. On the other hand, adding rigid structures can limit viewer interactions to narrow use cases or decrease the expressiveness of viewer inputs. To facilitate the sensemaking of many visual inputs while retaining the expressiveness or versatility of viewer interactions, we introduce a visual input management framework (VIMF) and a system, VisPoll, that help streamers specify, aggregate, and visualize many visual inputs. A pilot evaluation indicated that VisPoll can expand the types of viewer interactions. Our framework provides insights for designing scalable and expressive visual communication for live streaming.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445419
ER  - 

TY  - NA
AU  - Piech, Chris; Malik, Ali Ahmad; Jue, Kylie; Sahami, Mehran
TI  - SIGCSE - Code in Place: Online Section Leading for Scalable Human-Centered Learning
PY  - 2021
AB  - Could it be the case that the number of people who want to teach computer science, and have the potential, is roughly proportional to the number of people who want to learn? During the time of COVID-19 we offered a free CS1 class to people around the world. Well-aware of the high drop-out rates reported in many massive open-access online courses (MOOCs), we augmented our course with a scalable, human-centered solution: section leading. Section leaders teach small, weekly interactive learning sessions. We hypothesize that the personalized attention adds a sense of responsibility for both student and teacher which drives learning. We recruited over 900 volunteer section leaders and more than 10,000 students in the class. To our knowledge this is the largest group of section leaders in a single CS1 course offering and the most small group interactions. The completion rate in our class was more than 10 times that usually reported for similar MOOCs. Additionally, 99% of the volunteer section leaders taught through the entire span of the course, showing the potential for large scale volunteer-driven education, and the benefit that teachers themselves derive. We also discovered the potential for replication of this model, as 34% of students in a representative-sample survey indicated they would serve as section leaders for a future offering of the course. This level of participation would be more than sufficient to field additional offerings of the course sustainably. We believe this is an intriguing case study of a model for significantly scaling human-centric CS education for all.
SP  - 973
EP  - 979
JF  - Proceedings of the 52nd ACM Technical Symposium on Computer Science Education
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3408877.3432562
ER  - 

TY  - JOUR
AU  - Wang, Zhuo; Bai, Xiaoliang; Zhang, Shusheng; Weiping, He; Wang, Yang; Han, Dechuan; Wei, Sili; Wei, Bingzhao; Chen, Chengkun
TI  - M-AR: A Visual Representation of Manual Operation Precision in AR Assembly
PY  - 2021
AB  - The research of augmented reality physical tasks for human-computer collaboration is still an attractive dynamic research field. In recent years, AR Instruction supporting collaborative assembly ha...
SP  - 1799
EP  - 1814
JF  - International Journal of Human–Computer Interaction
VL  - 37
IS  - 19
PB  - 
DO  - 10.1080/10447318.2021.1909278
ER  - 

TY  - NA
AU  - Schlosser, Paul; Matthews, Ben; Salisbury, Isaac; Sanderson, Penelope; Hayes, Sass
TI  - CHI - Head-Worn Displays for Emergency Medical Services Staff: Properties of Prehospital Work, Use Cases, and Design Considerations
PY  - 2021
AB  - Head-worn displays (HWDs) offer their users high mobility, hands-free operation, and “see-what-I-see” features. In the prehospital environment, emergency medical services (EMS) staff could benefit from the unique characteristics of HWDs. We conducted a field study to analyze work practices of EMS staff and the potential of HWDs to support their activities. Based on our observations and the comments of EMS staff, we propose three use cases for HWDs in the prehospital environment. They are (1) enhanced communication between different care providers, (2) hands-free access to clinical monitoring and imaging, (3) and improved realism of training scenarios. We conclude with a set of design considerations and suggest that for the successful implementation of HWDs in EMS environments, researchers, designers, and clinical stakeholders should consider the harsh outdoor environment in which HWDs will be used, the extensive workload of staff, the complex collaboration performed, privacy requirements, and the high variability of work.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445614
ER  - 

TY  - NA
AU  - Deng, Yufeng; Wang, Dong; Zhang, Qian; Zhao, Run
TI  - SECON - MType: A Magnetic Field-based Typing System on the Hand for Around-Device Interaction
PY  - 2019
AB  - Smart wearable devices have become pervasive as they are portable and intelligent. The popular method to interact with it is touch-screen, which is error-prone and cumbersome due to its limited size. There are a few innovative works designing a virtual dial plate on the hand back, which need special-purpose sensors or microphones which may suffer from privacy leak. We propose MType, a system only employs sensors already built in the commercial-off-the-shelf (COTS) device with a magnetic ring to expand the interaction space between users and wearable devices. The core idea is to leverage the gravity sensor, linear accelerometer and magnetometer embedded in the standard smartwatch to detect gestures, capture input events and locate keystrokes on the opisthenar and palm. Besides, MType designs a runtime adaptation mechanism to handle the cold start problem and adapt to the variations over the time of usage. We implement MType on the COTS smartwatch and our extensive experiments in different scenarios show that the average accuracy of keystroke localization can reach 93% with a small size initial training set (3 samples for each key) at a low sampling rate (51Hz). Furthermore, when turning on the runtime adaptation mechanism and enlarging the training set, the accuracy can achieve 98%.
SP  - 1
EP  - 9
JF  - 2019 16th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sahcn.2019.8824942
ER  - 

TY  - NA
AU  - Sun, Hongling; Zhang, Zhenliang; Liu, Yue; Duh, Henry Been-Lirn
TI  - OZCHI - OptoBridge: assisting skill acquisition in the remote experimental collaboration
PY  - 2016
AB  - In this paper an experimental teaching platform named OptoBridge is presented which supports the sharing of the collaborative space for spatially distributed users to assist skill acquisition. The development of OptoBridge is based on augmented reality (AR) and integrates free-hand gesture interactions with the video mediated communication. The prototype is preliminarily applied in the optics field to promote skill execution in the case of the Michelson interferometer. OptoBridge enables the remote teacher to monitor the experimental scenario as well as the detailed optical phenomena through the transmitted video captured on the local side. Meanwhile the local learner equipped with the optical see-through head-mounted display (OSTHMD) can be indicated by virtual hands and augmented annotations controlled by the teacher's gestures and follow the guidance to get their skills practiced. The implementation of OptoBridge is also presented, aimed at providing a more engaging and efficient approach for remote skill teaching.
SP  - 195
EP  - 199
JF  - Proceedings of the 28th Australian Conference on Computer-Human Interaction - OzCHI '16
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3010915.3010975
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Withana, Anusha; Pourjafarian, Narjes; Steimle, Jürgen
TI  - CHI - Multi-Touch Skin: A Thin and Flexible Multi-Touch Sensor for On-Skin Input
PY  - 2018
AB  - Skin-based touch input opens up new opportunities for direct, subtle, and expressive interaction. However, existing skin-worn sensors are restricted to single-touch input and limited by a low resolution. We present the first skin overlay that can capture high-resolution multi-touch input. Our main contributions are: 1) Based on an exploration of functional materials, we present a fabrication approach for printing thin and flexible multi-touch sensors for on-skin interactions. 2) We present the first non-rectangular multi-touch sensor overlay for use on skin and introduce a design tool that generates such sensors in custom shapes and sizes. 3) To validate the feasibility and versatility of our approach, we present four application examples and empirical results from two technical evaluations. They confirm that the sensor achieves a high signal-to-noise ratio on the body under various grounding conditions and has a high spatial accuracy even when subjected to strong deformations.
SP  - 33
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173607
ER  - 

TY  - NA
AU  - Arakawa, Riku; Maekawa, Azumi; Kashino, Zendai; Inami, Masahiko
TI  - SUI - Hand with Sensing Sphere: Body-Centered Spatial Interactions with a Hand-Worn Spherical Camera
PY  - 2020
AB  - We propose a novel body-centered interaction system making use of a spherical camera attached to a hand. Its broad and unique field of view enables an all-in-one approach to sensing multiple pieces of contextual information in hand-based spatial interactions: (i) hand location on the body surface, (ii) hand posture, (iii) hand keypoints in certain postures, and (iv) the near-hand environment. The proposed system makes use of a deep-learning approach to perform hand location and posture recognition. The proposed system is capable of achieving high hand location and posture recognition accuracy, 85.0 % and 88.9 % respectively, after collecting sufficient data and training. Our result and example demonstrations show the potential of utilizing 360° cameras for vision-based sensing in context-aware body-centered spatial interactions.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418450
ER  - 

TY  - NA
AU  - Guinea, Alejandro Sanchez; Boytsov, Andrey; Mouline, Ludovic; Le Traon, Yves
TI  - MobiQuitous - Continuous Identification in Smart Environments Using Wrist-Worn Inertial Sensors
PY  - 2018
AB  - In this paper, we propose a new approach capable of performing continuous identification of users in home and office environments based on hand and arm motion patterns obtained from a wrist-worn inertial measurement unit (IMU). Different from state-of-the-art methods, our approach is not constrained to particular types of movements, gestures, or activities, thus allowing users to perform freely and unconstrained their daily routines while the identification takes place. We evaluate our approach by conducting an in the lab study and two in-situ studies, one in home environment and one in office environment. Our studies involved a total of 29 different participants and the data collected corresponds to approximately 256 hours. The results obtained in the studies indicate that our approach is able to perform continuous user identification with an accuracy of 0.88 for office environments and 0.71 for the average size of a household.
SP  - 87
EP  - 96
JF  - Proceedings of the 15th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3286978.3287001
ER  - 

TY  - JOUR
AU  - Zhang, Tengxiang; Yi, Xin; Yu, Chun; Wang, Yuntao; Becker, Nicholas; Shi, Yuanchun
TI  - TouchPower: Interaction-based Power Transfer for Power-as-needed Devices
PY  - 2017
AB  - The trend toward ubiquitous deployment of electronic devices demands novel low maintenance power schemes to decrease the burden of maintaining such a large number of devices. In this paper, we propose Interaction-based Power Transfer (IPT): a novel power scheme for power-as-needed devices (i.e., devices that only require power during interaction). IPT allows for the removal of built-in batteries on these devices, and to instead be powered up through direct contact interaction with the user (e.g. gripping a mouse, holding a pen). We prove the concept and show the potential of IPT through our TouchPower prototype. TouchPower transfers on-body power to off-body power-as-needed devices through contact between electrodes on a glove worn by the user and those on the target device during the interaction process. We design TouchPower to automatically detect the contact topology at runtime to supply power accordingly, and place electrodes on the glove so that TouchPower is compatible with various interactions with different objects. We also show the methodology of placing electrodes on the device-end, and evaluate it on a mouse and a remote controller. Results show that during interaction, TouchPower is able to provide stable power supply to these devices with only a small sacrifice in regards to interaction naturalness. At last we demonstrate six applications of TouchPower, and discuss the limitations and potential of TouchPower and IPT systems.
SP  - 121
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130986
ER  - 

TY  - JOUR
AU  - Goudswaard, Mark; Forbes, Hannah; Kent, Lee; Snider, Chris; Hicks, Ben
TI  - Different approaches to democratise design - are they equal?
PY  - 2019
AB  - The democratisation of design permits greater stakeholder involvement in what has traditionally been a domain reserved for experts; the design process itself. This is enabled by technological advances in fields such as 3D printing, virtual reality and high-speed peer to peer communication technologies which have fuelled the development of new and innovative design methods. This paper compares and contrasts different approaches to the democratisation of design, and in particular, those that aim to involve wider stakeholders in the design process itself. Three different approaches(design by play, design by generation and crowdsourcing for design) are defined and contextualised within existing design frameworks and their respective suitabilities to democratise different design phases are considered. An exemplar use case of each approach is presented in order to assess how stakeholder engagement is affected by each democratising strategy. The discussion compares and contrasts the approaches with respect to their applicability and utility for different stages of the design process and how the power dynamics of the design process are altered when the different approaches are employed.
SP  - 119
EP  - 128
JF  - Proceedings of the Design Society: International Conference on Engineering Design
VL  - 1
IS  - 1
PB  - 
DO  - 10.1017/dsi.2019.15
ER  - 

TY  - NA
AU  - Gomi, Hidehito; Yamaguchi, Shuji; Wataru, Ogami; Teraoka, Teruhiko; Higurashi, Tatsuru
TI  - TrustCom/BigDataSE - Context-Aware Authentication Using Co-Located Devices
PY  - 2019
AB  - This paper proposes a context-aware authentication mechanism that verifies the identity of a user via the data generated by detecting short-range radio signals from nearby devices. Many projects on user authentication have aimed to improve security and usability. Although these projects have been successful in solving individual problems, authentication using passwords remains the most dominant authentication mechanism. So far, however, only a few attempts have been made at pursuing authentication using information about co-located devices, even though an increasingly large number of wearable and Internet-of-Things (IoT) devices is appearing in our computing environments. We explore the feasibility of the proposed mechanism by conducting in-house experiments. Our experimental results suggest that the information on co-location with nearby devices has good potential for implicitly and continuously authenticating a user while modulating the authentication accuracy.
SP  - 304
EP  - 311
JF  - 2019 18th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/13th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/trustcom/bigdatase.2019.00048
ER  - 

TY  - JOUR
AU  - Lee, Lik Hang; Hui, Pan
TI  - Interaction Methods for Smart Glasses: A Survey
PY  - 2018
AB  - Since the launch of Google Glass in 2014, smart glasses have mainly been designed to support micro-interactions. The ultimate goal for them to become an augmented reality interface has not yet been attained due to an encumbrance of controls. Augmented reality involves superimposing interactive computer graphics images onto physical objects in the real world. This survey reviews current research issues in the area of human–computer interaction for smart glasses. The survey first studies the smart glasses available in the market and afterwards investigates the interaction methods proposed in the wide body of literature. The interaction methods can be classified into hand-held, touch, and touchless input. This paper mainly focuses on the touch and touchless input. Touch input can be further divided into on-device and on-body, while touchless input can be classified into hands-free and freehand. Next, we summarize the existing research efforts and trends, in which touch and touchless input are evaluated by a total of eight interaction goals. Finally, we discuss several key design challenges and the possibility of multi-modal input for smart glasses.
SP  - 28712
EP  - 28732
JF  - IEEE Access
VL  - 6
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2831081
ER  - 

TY  - NA
AU  - Liang, Chen; Yu, Chun; Xiaoying, Wei; Xu, Xuhai; Hu, Yongquan; Wang, Yuntao; Shi, Yuanchun
TI  - CHI - Auth+Track: Enabling Authentication Free Interaction on Smartphone by Continuous User Tracking
PY  - 2021
AB  - We propose Auth+Track, a novel authentication model that aims to reduce redundant authentication in everyday smartphone usage. By sparse authentication and continuous tracking of the user’s status, Auth+Track eliminates the “gap” authentication between fragmented sessions and enables “Authentication Free when User is Around”. To instantiate the Auth+Track model, we present PanoTrack, a prototype that integrates body and near field hand information for user tracking. We install a fisheye camera on the top of the phone to achieve a panoramic vision that can capture both user’s body and on-screen hands. Based on the captured video stream, we develop an algorithm to extract 1) features for user tracking, including body keypoints and their temporal and spatial association, near field hand status, and 2) features for user identity assignment. The results of our user studies validate the feasibility of PanoTrack and demonstrate that Auth+Track not only improves the authentication efficiency but also enhances user experiences with better usability.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445624
ER  - 

TY  - NA
AU  - Wang, Yu-Xiang; Tsai, Yu-Ju; Huang, Yu-Hsuan; Yang, Wan-Ling; Yu, Tzu-Chieh; Chiu, Yu-Kai; Ouhyoung, Ming
TI  - SIGGRAPH Posters - ThirdEye: a coaxial feature tracking system for stereoscopic video see-through augmented reality
PY  - 2016
AB  - For stereoscopic augmented reality (AR) system, continuous feature tracking of the observing target is required to generate a virtual object in the real world coordinate. Besides, dual cameras have to be placed with proper distance to obtain correct stereo images for video see-through applications. Both higher resolution and frame rate per second (FPS) can improve the user experience. However, feature tracking could be the bottleneck with high resolution images and the latency would increase if image processing was done before tracking.
SP  - 22
EP  - NA
JF  - ACM SIGGRAPH 2016 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2945078.2945100
ER  - 

TY  - NA
AU  - Shimobayashi, Hideki; Sasaki, Tomoya; Horie, Arata; Arakawa, Riku; Kashino, Zendai; Inami, Masahiko
TI  - AHs - Independent Control of Supernumerary Appendages Exploiting Upper Limb Redundancy
PY  - 2021
AB  - In the field of physical augmentation, researchers have attempted to extend human capabilities by expanding the number of human appendages. To fully realize the potential of having an additional appendage, supernumerary appendages should be independently controllable without interfering with the functionality of existing appendages. Herein, we propose a novel approach for controlling supernumerary appendages by exploiting upper limb redundancy. We present a headphone-style visual sensing device and a recognition system to estimate shoulder movement. Through a set of user experiments, we evaluate the feasibility of our system and reveal the potential of independent control using upper limb redundancy. Our results indicate that participants are able to intentionally give commands through their shoulder motions. Finally, we demonstrate the wide range of supernumerary appendage control applications that our novel approach enables and discuss future prospects for our work.
SP  - 19
EP  - 30
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458980
ER  - 

TY  - JOUR
AU  - Gong, Taesik; Cho, Hyunsung; Lee, Bowon; Lee, Sung-Ju
TI  - Knocker: Vibroacoustic-based Object Recognition with Smartphones
PY  - 2019
AB  - While smartphones have enriched our lives with diverse applications and functionalities, the user experience still often involves manual cumbersome inputs. To purchase a bottle of water for instance, a user must locate an e-commerce app, type the keyword for a search, select the right item from the list, and finally place an order. This process could be greatly simplified if the smartphone identifies the object of interest and automatically executes the user preferred actions for the object. We present Knocker that identifies the object when a user simply knocks on an object with a smartphone. The basic principle of Knocker is leveraging a unique set of responses generated from the knock. Knocker takes a multimodal sensing approach that utilizes microphones, accelerometers, and gyroscopes to capture the knock responses, and exploits machine learning to accurately identify objects. We also present 15 applications enabled by Knocker that showcase the novel interaction method between users and objects. Knocker uses only the built-in smartphone sensors and thus is fully deployable without specialized hardware or tags on either the objects or the smartphone. Our experiments with 23 objects show that Knocker achieves an accuracy of 98% in a controlled lab and 83% in the wild.
SP  - 82
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3351240
ER  - 

TY  - NA
AU  - Laput, Gierad
TI  - UIST (Adjunct Volume) - Fading into the Background: Unleashing Ubiquitous and Unobtrusive Context Sensing
PY  - 2017
AB  - As computing becomes increasingly embedded into the fabric of everyday life, systems that understand people's context of use are of paramount importance. Regardless of whether the platform is a mobile device, wearable, or smart infrastructure, context offers an implicit dimension that is vital to increasing the richness of human-computer interaction. In my thesis work, I introduce multiple enabling technologies that greatly enhance context awareness in highly dynamic platforms, all without costly or invasive instrumentation. My systems have been deployed across long periods and multiple environments, the results of which show the versatility, accuracy and potential for robust context sensing. By combining novel sensing with machine learning, my work transforms raw signals into intelligent abstractions that can power rich, context-sensitive applications, unleashing the potential of next-generation computing platforms.
SP  - 87
EP  - 90
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131839
ER  - 

TY  - JOUR
AU  - Zhang, Cheng; Waghmare, Anandghan; Kundra, Pranav; Pu, Yiming; Gilliland, Scott; Ploetz, Thomas; Starner, Thad; Inan, Omer T.; Abowd, Gregory D.
TI  - FingerSound: Recognizing unistroke thumb gestures using a ring
PY  - 2017
AB  - We introduce FingerSound, an input technology to recognize unistroke thumb gestures, which are easy to learn and can be performed through eyes-free interaction. The gestures are performed using a thumb-mounted ring comprising a contact microphone and a gyroscope sensor. A K-Nearest-Neighbor(KNN) model with a distance function of Dynamic Time Warping (DTW) is built to recognize up to 42 common unistroke gestures. A user study, where the real-time classification results were given, shows an accuracy of 92%-98% by a machine learning model built with only 3 training samples per gesture. Based on the user study results, we further discuss the opportunities, challenges and practical limitations of FingerSound when deploying it to real-world applications in the future.
SP  - 120
EP  - 19
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130985
ER  - 

TY  - JOUR
AU  - Liu, Jialin; Snodgrass, Sam; Khalifa, Ahmed; Risi, Sebastian; Yannakakis, Georgios N.; Togelius, Julian
TI  - Deep Learning for Procedural Content Generation
PY  - 2020
AB  - Procedural content generation in video games has a long history. Existing procedural content generation methods, such as search-based, solver-based, rule-based and grammar-based methods have been applied to various content types such as levels, maps, character models, and textures. A research field centered on content generation in games has existed for more than a decade. More recently, deep learning has powered a remarkable range of inventions in content production, which are applicable to games. While some cutting-edge deep learning methods are applied on their own, others are applied in combination with more traditional methods, or in an interactive setting. This article surveys the various deep learning methods that have been applied to generate game content directly or indirectly, discusses deep learning methods that could be used for content generation purposes but are rarely used today, and envisages some limitations and potential future directions of deep learning for procedural content generation.
SP  - 19
EP  - 37
JF  - Neural Computing and Applications
VL  - 33
IS  - 1
PB  - 
DO  - 10.1007/s00521-020-05383-8
ER  - 

TY  - NA
AU  - Teo, Theophilus; Lawrence, Louise; Lee, Gun A.; Billinghurst, Mark; Adcock, Matt
TI  - CHI - Mixed Reality Remote Collaboration Combining 360 Video and 3D Reconstruction
PY  - 2019
AB  - Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement.
SP  - 201
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300431
ER  - 

TY  - JOUR
AU  - Kato, Hiroyuki; Takemura, Kentaro
TI  - 能動的振動入力による手形状識別;能動的振動入力による手形状識別;Hand Pose Identification by Emitting Vibration
PY  - 2018
AB  - NA
SP  - 62
EP  - 68
JF  - Transactions of the Society of Instrument and Control Engineers
VL  - 54
IS  - 1
PB  - 
DO  - 10.9746/sicetr.54.62
ER  - 

TY  - NA
AU  - Dewan, Prasun; Joyce, Blake L.; Merchant, Nirav
TI  - VL/HCC - Human-Centric Programming in the Large - Command Languages to Scalable Cyber Training
PY  - 2018
AB  - Programming in the large allows composition of processes executing code written using programming in the small. Traditionally, systems supporting programming in the large have included interpreters of OS command languages, but today, with the emergence of collaborative “big data” science, these systems also include cyberinfrastructures, which allow computations to be carried out on remote machines in the “cloud”. The rationale for these systems, even the traditional command interpreters, is human-centric computing, as they are designed to support quick, interactive development and execution of process workflows. Some cyberinfrastructures extend this human-centricity by also providing manipulation of visualizations of these workflows. To further increase the human-centricity of these systems, we have started a new project on cyber training - instruction in the use of command languages and visual components of cyberinfrastructures. Our objective is to provide scalable remote awareness of trainees' progress and difficulties, as well as collaborative and automatic resolution of their difficulties. Our current plan is to provide awareness based on a subway workflow metaphor, allow a trainer to collaborate with multiple trainees using a single instance of a command interpreter, and combine research in process and interaction workflows to support automatic help. These research directions can be considered an application of the general principle of integrating programming in the small and large
SP  - 295
EP  - 297
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506564
ER  - 

TY  - NA
AU  - Zhang, Tengxiang; Zeng, Xin; Zhang, Yinshuai; Sun, Ke; Wang, Yuntao; Chen, Yiqiang
TI  - CHI - ThermalRing: Gesture and Tag Inputs Enabled by a Thermal Imaging Smart Ring
PY  - 2020
AB  - The heterogeneous and ubiquitous input demands in smart spaces call for an input device that can enable rich and spontaneous interactions. We propose ThermalRing, a thermal imaging smart ring using low-resolution thermal camera for identity-anonymous, illumination-invariant, and power-efficient sensing of both dynamic and static gestures. We also design ThermalTag, thin and passive thermal imageable tags that reflect the heat from the human hand. ThermalTag can be easily made and applied onto everyday objects by users. We develop sensing techniques for three typical input demands: drawing gestures for device pairing, click and slide gestures for device control, and tag scan gestures for quick access. The study results show that ThermalRing can recognize nine drawing gestures with an overall accuracy of 90.9%, detect click gestures with an accuracy of 94.9%, and identify among six ThermalTags with an overall accuracy of 95.0%. Finally, we show the versatility and potential of ThermalRing through various applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376323
ER  - 

TY  - NA
AU  - Colley, Ashley; Inget, Virve; Lappalainen, Tuomas; Häkkilä, Jonna
TI  - ISWC - Ring form factor: a design space for interaction
PY  - 2017
AB  - We present a design space for interaction with a ring, worn on the user's finger. Whilst prior work has studied the ring as a means of attachment for hand motion sensors, we investigate the interaction possibilities provided by the natural affordance of the ring form factor itself. Interactions include, for example, changing the placement of the ring on the fingers, moving the ring along or around a finger and spinning of the ring on a surface.
SP  - 178
EP  - 179
JF  - Proceedings of the 2017 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123021.3123055
ER  - 

TY  - JOUR
AU  - Whitmire, Eric; Jain, Mohit; Jain, Divye; Nelson, Greg L.; Karkar, Ravi; Patel, Shwetak N.; Goel, Mayank
TI  - DigiTouch: Reconfigurable Thumb-to-Finger Input and Text Entry on Head-mounted Displays
PY  - 2017
AB  - Input is a significant problem for wearable systems, particularly for head mounted virtual and augmented reality displays. Existing input techniques either lack expressive power or may not be socially acceptable. As an alternative, thumb-to-finger touches present a promising input mechanism that is subtle yet capable of complex interactions. We present DigiTouch, a reconfigurable glove-based input device that enables thumb-to-finger touch interaction by sensing continuous touch position and pressure. Our novel sensing technique improves the reliability of continuous touch tracking and estimating pressure on resistive fabric interfaces. We demonstrate DigiTouch’s utility by enabling a set of easily reachable and reconfigurable widgets such as buttons and sliders. Since DigiTouch senses continuous touch position, widget layouts can be customized according to user preferences and application needs. As an example of a real-world application of this reconfigurable input device, we examine a split-QWERTY keyboard layout mapped to the user’s fingers. We evaluate DigiTouch for text entry using a multi-session study. With our continuous sensing method, users reliably learned to type and achieved a mean typing speed of 16.0 words per minute at the end of ten 20-minute sessions, an improvement over similar wearable touch systems.
SP  - 113
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130978
ER  - 

TY  - NA
AU  - Jain, Aryamaan; Sunkara, Jyoti; Shah, Ishaan; Sharma, Avinash; Rajan, K S
TI  - Automated tree generation using grammar & particle system
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490035.3490285
ER  - 

TY  - JOUR
AU  - Friedel, Joseph E.; Holzer, Thomas H.; Sarkani, Shahryar
TI  - Development, Optimization, and Validation of Unintended Radiated Emissions Processing System for Threat Identification
PY  - 2020
AB  - There is an on-going need for faster, more accurate, and easier to implement threat identification systems for concealed electronics, to thwart terrorism and espionage attempts. Common electronics are used in the design of improvised explosive devices (IEDs), targeting military, and civilian populations, while concealed recording devices steal proprietary/confidential data, compromising both government and industrial resources. This paper discusses a novel, nonintrusive, repeatable, reliable, expandable, and simple-to-implement detection and identification (D&I) system for identifying threats by using unintended radiated emissions (URE). This paper differs from current URE proposals, by avoiding energy radiation. The proposed process also addresses big data problems involved in capturing and building databases of URE characteristics. Issue interpretation is utilized with URE data to distinguish between threat/nonthreat devices using multiple criteria decision analysis and other decision-making techniques to determine model and mode of hidden devices. System optimization, verified by testing, is used to improve the speed and accuracy of the identification algorithm. The system is developed with data from 130 devices and validated with 33 separate devices, which are representative of IED and espionage threats. Due to slow progress in the URE D&I field, the properties and potential of a more effective D&I system, compared to current methods, will be of interest to the explosive ordnance disposal, security services, electronic systems manufacturing, automated inventory, mobile application development communities, and potentially others as well. For a field that is not well documented, this paper provides a clear, concise, and detailed model for future research and applications of URE D&I.
SP  - 2208
EP  - 2219
JF  - IEEE Transactions on Systems, Man, and Cybernetics: Systems
VL  - 50
IS  - 6
PB  - 
DO  - 10.1109/tsmc.2018.2810305
ER  - 

TY  - JOUR
AU  - Huang, Haibin; Kalogerakis, Evangelos; Yumer, Ersin; Mech, Radomir
TI  - Shape Synthesis from Sketches via Procedural Models and Convolutional Networks
PY  - 2016
AB  - Procedural modeling techniques can produce high quality visual content through complex rule sets. However, controlling the outputs of these techniques for design purposes is often notoriously difficult for users due to the large number of parameters involved in these rule sets and also their non-linear relationship to the resulting content. To circumvent this problem, we present a sketch-based approach to procedural modeling. Given an approximate and abstract hand-drawn 2D sketch provided by a user, our algorithm automatically computes a set of procedural model parameters, which in turn yield multiple, detailed output shapes that resemble the user's input sketch. The user can then select an output shape, or further modify the sketch to explore alternative ones. At the heart of our approach is a deep Convolutional Neural Network (CNN) that is trained to map sketches to procedural model parameters. The network is trained by large amounts of automatically generated synthetic line drawings. By using an intuitive medium, i.e., freehand sketching as input, users are set free from manually adjusting procedural model parameters, yet they are still able to create high quality content. We demonstrate the accuracy and efficacy of our method in a variety of procedural modeling scenarios including design of man-made and organic shapes.
SP  - 2003
EP  - 2013
JF  - IEEE transactions on visualization and computer graphics
VL  - 23
IS  - 8
PB  - 
DO  - 10.1109/tvcg.2016.2597830
ER  - 

TY  - JOUR
AU  - Nash, Charlie; Williams, Christopher
TI  - The shape variational autoencoder: A deep generative model of part-segmented 3D objects
PY  - 2017
AB  - We introduce a generative model of part-segmented 3D objects: the shape variational auto-encoder (ShapeVAE). The ShapeVAE describes a joint distribution over the existence of object parts, the locations of a dense set of surface points, and over surface normals associated with these points. Our model makes use of a deep encoder-decoder architecture that leverages the part-decomposability of 3D objects to embed high-dimensional shape representations and sample novel instances. Given an input collection of part-segmented objects with dense point correspondences the ShapeVAE is capable of synthesizing novel, realistic shapes, and by performing conditional inference enables imputation of missing parts or surface normals. In addition, by generating both points and surface normals, our model allows for the use of powerful surface-reconstruction methods for mesh synthesis. We provide a quantitative evaluation of the ShapeVAE on shape-completion and test-set log-likelihood tasks and demonstrate that the model performs favourably against strong baselines. We demonstrate qualitatively that the ShapeVAE produces plausible shape samples, and that it captures a semantically meaningful shape-embedding. In addition we show that the ShapeVAE facilitates mesh reconstruction by sampling consistent surface normals.
SP  - 1
EP  - 12
JF  - Computer Graphics Forum
VL  - 36
IS  - 5
PB  - 
DO  - 10.1111/cgf.13240
ER  - 

TY  - NA
AU  - Jones, Michael; Johnson, Naomi; Seppi, Kevin D.; Thatcher, Lawrence
TI  - UbiComp/ISWC Adjunct - Understanding How Non-Experts Collect and Annotate Activity Data
PY  - 2018
AB  - Training classifiers for human activity recognition systems often relies on large corpora of annotated sensor data. Crowd sourcing is one way to collect and annotate large amounts of sensor data. Crowd sourcing often depends on unskilled workers to collect and annotate the data. In this paper we explore machine learning of classifiers based on human activity data collected and annotated by non-experts. We consider the entire process starting from data collection through annotation including machine learning and ending with the final application implementation. We focus on three issues 1) can non-expert annotators overcome the technical challenges of data acquisition and annotation, 2) can they annotate reliably, and 3) to what extent might we expect their annotations to yield accurate and generalizable event classifiers. Our results suggest that non-expert users can collect video and data as well as produce annotations which are suitable for machine learning.
SP  - 1424
EP  - 1433
JF  - Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267305.3267507
ER  - 

TY  - NA
AU  - Sharma, Adwait; Roo, Joan Sol; Steimle, Jürgen
TI  - CHI - Grasping Microgestures: Eliciting Single-hand Microgestures for Handheld Objects
PY  - 2019
AB  - Single-hand microgestures have been recognized for their potential to support direct and subtle interactions. While pioneering work has investigated sensing techniques and presented first sets of intuitive gestures, we still lack a systematic understanding of the complex relationship between microgestures and various types of grasps. This paper presents results from a user elicitation study of microgestures that are performed while the user is holding an object. We present an analysis of over 2,400 microgestures performed by 20 participants, using six different types of grasp and a total of 12 representative handheld objects of varied geometries and size. We expand the existing elicitation method by proposing statistical clustering on the elicited gestures. We contribute detailed results on how grasps and object geometries affect single-hand microgestures, preferred locations, and fingers used. We also present consolidated gesture sets for different grasps and object size. From our findings, we derive recommendations for the design of microgestures compatible with a large variety of handheld objects.
SP  - 402
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300632
ER  - 

TY  - NA
AU  - Brudy, Frederik; Holz, Christian; Rädle, Roman; Wu, Chi-Jui; Houben, Steven; Klokmose, Clemens Nylandsted; Marquardt, Nicolai
TI  - CHI - Cross-Device Taxonomy: Survey, Opportunities and Challenges of Interactions Spanning Across Multiple Devices
PY  - 2019
AB  - Designing interfaces or applications that move beyond the bounds of a single device screen enables new ways to engage with digital content. Research addressing the opportunities and challenges of interactions with multiple devices in concert is of continued focus in HCI research. To inform the future research agenda of this field, we contribute an analysis and taxonomy of a corpus of 510 papers in the cross-device computing domain. For both new and experienced researchers in the field we provide: an overview, historic trends and unified terminology of cross-device research; discussion of major and under-explored application areas; mapping of enabling technologies; synthesis of key interaction techniques spanning across multiple devices; and review of common evaluation strategies. We close with a discussion of open issues. Our taxonomy aims to create a unified terminology and common understanding for researchers in order to facilitate and stimulate future cross-device research.
SP  - 562
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300792
ER  - 

TY  - JOUR
AU  - Zhang, Jian; Wang, Changbo; Li, Chen; Qin, Hong
TI  - Example-based rapid generation of vegetation on terrain via CNN-based distribution learning
PY  - 2019
AB  - Modeling large-scale vegetation on terrain is an important and challenging task in computer games, movie production and other digital entertainment applications. In this work, we propose a novel example-based method for rapid generation of vegetation in outdoor natural environments. Its central idea is to learn the vegetation distribution on terrain via deep convolution neural networks. We first use a pre-trained deep neural network to extract rich local information from the terrain pertinent to vegetation distribution. Second, we produce the initial features of the target vegetation distribution based on patch matching and further introduce a network that generates a vegetation density map based on the initial features. Third, during the synthesis stage, we propose a procedural method to generate the vegetation distribution data corresponding to the terrain data. Our research work confirms that the image features extracted by the pre-trained deep neural network could be utilized to explore the connection between vegetation and terrain. We validate our new method over various outdoor scenes, including procedural generated scenes and scenes with manual control on tree patterns. The experimental results demonstrate that our method can rapidly produce new realistic scenes for outdoor natural environments, which relies on the mechanism of learning correlationship between vegetation distribution and terrain data.
SP  - 1181
EP  - 1191
JF  - The Visual Computer
VL  - 35
IS  - 6
PB  - 
DO  - 10.1007/s00371-019-01667-w
ER  - 

TY  - NA
AU  - Johnson, Janet G.; Gasques, Danilo; Sharkey, Tommy; Schmitz, Evan; Weibel, Nadir
TI  - CHI - Do You Really Need to Know Where “That” Is? Enhancing Support for Referencing in Collaborative Mixed Reality Environments
PY  - 2021
AB  - Mixed Reality has been shown to enhance remote guidance and is especially well-suited for physical tasks. Conversations during these tasks are heavily anchored around task objects and their spatial relationships in the real world, making referencing - the ability to refer to an object in a way that is understood by others - a crucial process that warrants explicit support in collaborative Mixed Reality systems. This paper presents a 2x2 mixed factorial experiment that explores the effects of providing spatial information and system-generated guidance to task objects. It also investigates the effects of such guidance on the remote collaborator’s need for spatial information. Our results show that guidance increases performance and communication efficiency while reducing the need for spatial information, especially in unfamiliar environments. Our results also demonstrate a reduced need for remote experts to be in immersive environments, making guidance more scalable, and expertise more accessible.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445246
ER  - 

TY  - NA
AU  - Xiao, Robert; Laput, Gierad; Zhang, Yang; Harrison, Chris
TI  - CHI - Deus EM Machina: On-Touch Contextual Functionality for Smart IoT Appliances
PY  - 2017
AB  - Homes, offices and many other environments will be increasingly saturated with connected, computational appliances, forming the "Internet of Things" (IoT). At present, most of these devices rely on mechanical inputs, webpages, or smartphone apps for control. However, as IoT devices proliferate, these existing interaction methods will become increasingly cumbersome. Will future smart-home owners have to scroll though pages of apps to select and dim their lights? We propose an approach where users simply tap a smartphone to an appliance to discover and rapidly utilize contextual functionality. To achieve this, our prototype smartphone recognizes physical contact with uninstrumented appliances, and summons appliance-specific interfaces. Our user study suggests high accuracy 98.8% recognition accuracy among 17 appliances. Finally, to underscore the immediate feasibility and utility of our system, we built twelve example applications, including six fully functional end-to-end demonstrations.
SP  - 4000
EP  - 4008
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025828
ER  - 

TY  - NA
AU  - Roggen, Daniel; Yazdan, Arash Pour; Morales, Francisco Javier Ordonez; Prance, Robert J.; Prance, Helen
TI  - ISWC - Electric field phase sensing for wearable orientation and localisation applications
PY  - 2016
AB  - We show how to sense the phase of the ambient electric field from a body-worn sensor with respect to a reference and discuss how phase information could contribute to relative orientation sensing and indoor localisation. Our system uses 7mW and can be enclosed in a plastic case which makes it suitable for new wearable devices.
SP  - 52
EP  - 53
JF  - Proceedings of the 2016 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971763.2971774
ER  - 

TY  - JOUR
AU  - Joshi, Siddharth; Kim, Chul; Cauwenberghs, Gert
TI  - A 6.5- $\mu \text{W}$ /MHz Charge Buffer With 7-fF Input Capacitance in 65-nm CMOS for Noncontact Electropotential Sensing
PY  - 2016
AB  - This brief presents a CMOS charge buffer with femtofarad-range input capacitance for applications in capacitive electropotential sensing. We analyze and verify a feedback mechanism to negate parasitic capacitances seen at the input of a CMOS amplifier. Measurements are presented from a prototype fabricated in 65-nm CMOS occupying an active area of 193 $ {\mu }\text{m}^{{2}}$ with an efficiency of 6.5 $ {\mu }\text{W}$ /MHz. Over-the-air measurements validate its applicability to electropotential sensing.
SP  - 1161
EP  - 1165
JF  - IEEE Transactions on Circuits and Systems II: Express Briefs
VL  - 63
IS  - 12
PB  - 
DO  - 10.1109/tcsii.2016.2623591
ER  - 

TY  - CHAP
AU  - Chan, Cheng-Sheng; Chen, Shou-Zhong; Xie, Pei-Xuan; Chang, Chiung-Chih; Sun, Min
TI  - ECCV (4) - Recognition from Hand Cameras: A Revisit with Deep Learning
PY  - 2016
AB  - We revisit the study of a wrist-mounted camera system (referred to as HandCam) for recognizing activities of hands. HandCam has two unique properties as compared to egocentric systems (referred to as HeadCam): (1) it avoids the need to detect hands; (2) it more consistently observes the activities of hands. By taking advantage of these properties, we propose a deep-learning-based method to recognize hand states (free vs. active hands, hand gestures, object categories), and discover object categories. Moreover, we propose a novel two-streams deep network to further take advantage of both HandCam and HeadCam. We have collected a new synchronized HandCam and HeadCam dataset with 20 videos captured in three scenes for hand states recognition. Experiments show that our HandCam system consistently outperforms a deep-learning-based HeadCam method (with estimated manipulation regions) and a dense-trajectory-based HeadCam method in all tasks. We also show that HandCam videos captured by different users can be easily aligned to improve free vs. active recognition accuracy (\(3.3\,\%\) improvement) in across-scenes use case. Moreover, we observe that finetuning Convolutional Neural Network consistently improves accuracy. Finally, our novel two-streams deep network combining HandCam and HeadCam achieves the best performance in four out of five tasks. With more data, we believe a joint HandCam and HeadCam system can robustly log hand states in daily life.
SP  - 505
EP  - 521
JF  - Computer Vision – ECCV 2016
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-46493-0_31
ER  - 

TY  - JOUR
AU  - Liu, Guanhong; Gu, Yizheng; Yin, Yiwen; Yu, Chun; Wang, Yuntao; Mi, Haipeng; Shi, Yuanchun
TI  - Keep the Phone in Your Pocket: Enabling Smartphone Operation with an IMU Ring for Visually Impaired People
PY  - 2020
AB  - Previous studies have shown that visually impaired users face a unique set of pain points in smartphone interaction including locating and removing the phone from a pocket, two-handed interaction while holding a cane, and keeping personal data private in a public setting. In this paper, we present a ring-based input interaction that enables in-pocket smartphone operation. By wearing a ring with an Inertial Measurement Unit on the index finger, users can perform gestures on any surface (e.g., tables, thighs) using subtle, one-handed gestures and receive auditory feedback via earphones. We conducted participatory studies to obtain a set of versatile commands and corresponding gestures. We subsequently trained an SVM model to recognize these gestures and achieved a mean accuracy of 95.5% on 15 classifications. Evaluation results showed that our ring interaction is more efficient than some baseline phone interactions and is easy, private, and fun to use.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397308
ER  - 

TY  - NA
AU  - Schoop, Eldon; Nguyen, Michelle L.T.; Lim, Daniel; Savage, Valkyrie; Follmer, Sean; Hartmann, Björn
TI  - CHI Extended Abstracts - Drill Sergeant: Supporting Physical Construction Projects through an Ecosystem of Augmented Tools
PY  - 2016
AB  - Mapping techniques from software tutorials onto physical craft processes can assist novices in building multi-material assemblies. By providing in-situ step instructions and progress tracking, generating dynamic feedback on technique, and adapting tutorial content to a user's specific context and preferences, an ecosystem of smart tools can guide users through complete project tutorials. We demonstrate how such techniques can be enabled by augmenting common workshop tools (drill/driver, saw, router) with measurement, state sensing and interactive feedback; and by sequencing instructions across multiple tools. We validate the benefits of a smart tool ecosystem through reflections on a series of author-created design examples and informal feedback from four fab lab users.
SP  - 1607
EP  - 1614
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892429
ER  - 

TY  - NA
AU  - Schor, Nadav; Katzir, Oren; Zhang, Hao; Cohen-Or, Daniel
TI  - ICCV - CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition
PY  - 2019
AB  - Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to enable a model to generate a rich variety of samples from the entire target distribution, rather than only from a distribution confined to the training data. In other words, we would like the generative model to go beyond the observed samples and learn to generate ``unseen'', yet still plausible, data. In our work, we present CompoNet, a generative neural network for 2D or 3D shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions. Treating a shape not as an unstructured whole, but as a (re-)composable set of deformable parts, adds a combinatorial dimension to the generative process to enrich the diversity of the output, encouraging the generator to venture more into the ``unseen''. We show that our part-based model generates richer variety of plausible shapes compared with baseline generative models. To this end, we introduce two quantitative metrics to evaluate the diversity of a generative model and assess how well the generated data covers both the training data and unseen data from the same target distribution.
SP  - 8759
EP  - 8768
JF  - 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccv.2019.00885
ER  - 

TY  - CHAP
AU  - Li, Xiangdong; Chen, Wenqian; Zhou, Yunzhan; Athalye, Surabhi; Chin, Wai Kit Daniel; Kit, Russell Goh Wei; Setiawan, Vincent; Hansen, Preben
TI  - HCI (2) - Mobile Phone-Based Device for Personalised Tutorials of 3D Printer Assembly
PY  - 2019
AB  - There are a number of studies exploring materials and mechanisms of 3D printers that can help product designers develop and evaluate interactive systems efficiently. As 3D printers are increasingly adopted, designers are more likely to encounter difficulties in assembling 3D printers on their own, as the assembly process involves specialised skills and knowledge of fitting components in right positions. Conventional solutions use text and video manuals but still requires high understandings of the assembly. We designed and evaluated the mobile phone-based device for personalised tutorials of 3D printer assembly. The device consists of a modified dongle and mobile phone application. The former detects electromagnetic signals upon physical contacts with the components and the latter displays tutorials accordingly. The contributions include the device design with electromagnetic signal-based object detection and importantly, the approach to integrating component touching with component detection for personalised interactions. Generalising implications for the approach are discussed.
SP  - 37
EP  - 48
JF  - Human-Computer Interaction. Recognition and Interaction Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-22643-5_4
ER  - 

TY  - NA
AU  - He, Zhenyi; Rosenberg, Karl; Perlin, Ken
TI  - CHI Extended Abstracts - Exploring Configuration of Mixed Reality Spaces for Communication
PY  - 2019
AB  - Mixed Reality (MR) enables users to explore scenarios not realizable in the physical world. This allows users to communicate with the help of digital content. We investigate how different configurations of participants and content affect communication in a shared immersive environment. We designed and implemented side-by-side, mirrored face-to-face and eyes-free configurations in our multi-user MR environment and conducted a preliminary user study for our mirrored face-to-face configuration, evaluating with respect to one-to-one interaction, smooth focus shifts and eye contact within a 3D presentation using the interactive Chalktalk system. We provide experimental results and interview responses.
SP  - 3312761
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312761
ER  - 

TY  - NA
AU  - Dziubak, Volodymyr; Lafreniere, Ben; Grossman, Tovi; Bunt, Andrea; Fitzmaurice, George
TI  - UIST - Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops
PY  - 2018
AB  - Instructors of 3D design workshops for children face many challenges, including maintaining awareness of students' progress, helping students who need additional attention, and creating a fun experience while still achieving learning goals. To help address these challenges, we developed Maestro, a workshop orchestration system that visualizes students' progress, automatically detects and draws attention to common challenges faced by students, and provides mechanisms to address common student challenges as they occur. We present the design of Maestro, and the results of a case-study evaluation with an experienced facilitator and 13 children. The facilitator appreciated Maestro's real-time indications of which students were successfully following her tutorial demonstration, and recognized the system's potential to "extend her reach" while helping struggling students. Participant interaction data from the study provided support for our follow-along detection algorithm, and the capability to remind students to use 3D navigation.
SP  - 287
EP  - 298
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242606
ER  - 

TY  - NA
AU  - Karasawa, Hiroyuki; Fukui, Rui; Watanabe, Masahiko; Warisawa, Shin'ichi
TI  - AIM - Simultaneous Recognition of Hand Shape and Two-Axis Wrist Bending Using Wearable Wrist Contour Measuring Device
PY  - 2019
AB  - Hand gestures can express various kinds of information. However, existing techniques present difficulties for practical use such as environmental limitations and obstructions to natural gestures. To address these difficulties, our earlier work has developed a wrist contour measuring device that recognizes a hand shape and pronation angle. An earlier report has described the necessity of increasing the number of recognizable hand motions to realize a highly usable screen operation. This paper reports a trial of simultaneous recognition of hand shape and wrist-bending state. After observing the wrist contour and then clarifying the phenomenon, we design features, and construct a recognizer. The simultaneous recognition rate of four hand shapes and 14 wrist-bending states achieved, in this research, is 79.7%. As described in this paper, a trial of a user interface using hand gestures recognized by the device is also conducted. Several problems related to practical use are discussed.
SP  - 1550
EP  - 1555
JF  - 2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aim.2019.8868434
ER  - 

TY  - NA
AU  - Herscher, Sebastian; DeFanti, Connor; Vitovitch, Nicholas Gregory; Brenner, Corinne; Xia, Haijun; Layng, Kris; Perlin, Ken
TI  - UIST - CAVRN: An Exploration and Evaluation of a Collective Audience Virtual Reality Nexus Experience
PY  - 2019
AB  - The virtual reality ecosystem has gained momentum in the gaming, entertainment, and enterprise markets, but is hampered by limitations in concurrent user count, throughput, and accessibility to mass audiences. Based on our analysis of the current state of the virtual reality ecosystem and relevant aspects of traditional media, we propose a set of design hypotheses for practical and effective seated virtual reality experiences of scale. Said hypotheses manifest in the Collective Audience Virtual Reality Nexus (CAVRN), a framework and management system for large-scale (30+ user) virtual reality deployment in a theater-like physical setting. A mixed methodology study of CAVE, an experience implemented using CAVRN, generated rich insights into the proposed hypotheses. We discuss the implications of our findings on content design, audience representation, and audience interaction.
SP  - 1137
EP  - 1150
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347929
ER  - 

TY  - JOUR
AU  - Li, Tianxing; Xiong, Xi; Xie, Yifei; Hito, George; Yang, Xing-Dong; Zhou, Xia
TI  - Reconstructing Hand Poses Using Visible Light
PY  - 2017
AB  - Free-hand gestural input is essential for emerging user interactions. We present Aili, a table lamp reconstructing a 3D hand skeleton in real time, requiring neither cameras nor on-body sensing devices. Aili consists of an LED panel in a lampshade and a few low-cost photodiodes embedded in the lamp base. To reconstruct a hand skeleton, Aili combines 2D binary blockage maps from vantage points of different photodiodes, which describe whether a hand blocks light rays from individual LEDs to all photodiodes. Empowering a table lamp with sensing capability, Aili can be seamlessly integrated into the existing environment. Relying on such low-level cues, Aili entails lightweight computation and is inherently privacy-preserving. We build and evaluate an Aili prototype. Results show that Aili’s algorithm reconstructs a hand pose within 7.2 ms on average, with 10.2° mean angular deviation and 2.5-mm mean translation deviation in comparison to Leap Motion. We also conduct user studies to examine the privacy issues of Leap Motion and solicit feedback on Aili’s privacy protection. We conclude by demonstrating various interaction applications Aili enables.
SP  - 71
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130937
ER  - 

TY  - NA
AU  - Varga, Virag; Wyss, Marc; Vakulya, Gergely; Sample, Alanson P.; Gross, Thomas R.
TI  - UIST - Designing Groundless Body Channel Communication Systems: Performance and Implications
PY  - 2018
AB  - Novel interactions that capacitively couple electromagnetic (EM) fields between devices and the human body are gaining more attention in the human-computer interaction community. One class of these techniques is Body Channel Communication (BCC), a method that overlays physical touch with digital information. Despite the number of published capacitive sensing and communication prototypes, there exists no guideline on how to design such hardware or what are the application limitations and possibilities. Specifically, wearable (groundless) BCC has been proven in the past to be extremely challenging to implement. Additionally, the exact behavior of the human body as an EM-field medium is still not fully understood today. Consequently, the application domain of BCC technology could not be fully explored. This paper addresses this problem. Based on a recently published general purpose wearable BCC system, we first present a thorough evaluation of the impact of various technical parameter choices and an exhaustive channel characterization of the human body as a host for BCC. Second, we discuss the implications of these results for the application design space and present guidelines for future wearable BCC systems and their applications. Third, we point out an important observation of the measurements, namely that BCC can employ the whole body as user interface (and not just hands or feet). We sketch several applications with these novel interaction modalities.
SP  - 683
EP  - 695
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242622
ER  - 

TY  - NA
AU  - Gong, Taesik; Cho, Hyunsung; Lee, Bowon; Lee, Sung-Ju
TI  - CHI Extended Abstracts - Identifying Everyday Objects with a Smartphone Knock
PY  - 2018
AB  - We use smartphones and their apps for almost every daily activity. For instance, to purchase a bottle of water online, a user has to unlock the smartphone, find the right e-commerce app, search the name of the water product, and finally place an order. This procedure requires manual, often cumbersome, input of a user, but could be significantly simplified if the smartphone can identify an object and automatically process this routine. We present Knocker, an object identification technique that only uses commercial off-the-shelf smartphones. The basic idea of Knocker is to leverage a unique set of responses that occur when a user knocks on an object with a smartphone, which consist of the generated sound from the knock and the changes in accelerometer and gyroscope values. Knocker employs a machine learning classifier to identify an object from the knock responses. A user study was conducted to evaluate the feasibility of Knocker with 14 objects in both quiet and noisy environments. The result shows that Knocker identifies objects with up to 99.7% accuracy.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188514
ER  - 

TY  - JOUR
AU  - Koyama, Yuki; Sato, Issei; Goto, Masataka
TI  - Sequential gallery for interactive visual design optimization
PY  - 2020
AB  - Visual design tasks often involve tuning many design parameters. For example, color grading of a photograph involves many parameters, some of which non-expert users might be unfamiliar with. We propose a novel user-in-the-loop optimization method that allows users to efficiently find an appropriate parameter set by exploring such a high-dimensional design space through much easier two-dimensional search subtasks. This method, called sequential plane search, is based on Bayesian optimization to keep necessary queries to users as few as possible. To help users respond to plane-search queries, we also propose using a gallery-based interface that provides options in the two-dimensional subspace arranged in an adaptive grid view. We call this interactive framework Sequential Gallery since users sequentially select the best option from the options provided by the interface. Our experiment with synthetic functions shows that our sequential plane search can find satisfactory solutions in fewer iterations than baselines. We also conducted a preliminary user study, results of which suggest that novices can effectively complete search tasks with Sequential Gallery in a photo-enhancement scenario.
SP  - 88
EP  - NA
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 4
PB  - 
DO  - 10.1145/3386569.3392444
ER  - 

TY  - NA
AU  - Bergstrom-Lehtovirta, Joanna; Boring, Sebastian; Hornbæk, Kasper
TI  - CHI - Placing and Recalling Virtual Items on the Skin
PY  - 2017
AB  - The human skin provides an ample, always-on surface for input to smart watches, mobile phones, and remote displays. Using touch on bare skin to issue commands, however, requires users to recall the location of items without direct visual feedback. We present an in-depth study in which participants placed 30 items on the hand and forearm and attempted to recall their locations. We found that participants used a variety of landmarks, personal associations, and semantic groupings in placing the items on the skin. Although participants most frequently used anatomical landmarks (e.g., fingers, joints, and nails), recall rates were higher for items placed on personal landmarks, including scars and tattoos. We further found that personal associations between items improved recall, and that participants often grouped important items in similar areas, such as family members on the nails. We conclude by discussing the implications of our findings for design of skin-based interfaces.
SP  - 1497
EP  - 1507
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3026030
ER  - 

TY  - JOUR
AU  - Sun, Lu; Osman, Hussein Al; Lang, Jochen
TI  - An Augmented Reality Online Assistance Platform for Repair Tasks
PY  - 2021
AB  - Our augmented reality online assistance platform enables an expert to specify 6DoF movements of a component and apply the geometrical and physical constraints in real-time. We track the real components on the expert’s side to monitor the operations of an expert. We leverage a remote rendering technique that we proposed previously to relieve the rendering burden of the augmented reality end devices. By conducting a user study, we show that the proposed method outperforms conventional instructional videos and sketches. The answers to the questionnaires show that the proposed method receives higher recommendation than sketching, and, compared to conventional instructional videos, is outstanding in terms of instruction clarity, preference, recommendation, and confidence of task completion. Moreover, as to the overall user experience, the proposed method has an advantage over the video method.
SP  - 1
EP  - 23
JF  - ACM Transactions on Multimedia Computing, Communications, and Applications
VL  - 17
IS  - 2
PB  - 
DO  - 10.1145/3429285
ER  - 

TY  - NA
AU  - Singh, Shubham; Ma, Zengou; Giunchi, Daniele; Steed, Anthony
TI  - Real-time Collaboration Between Mixed Reality Users in Geo-referenced Virtual Environment.
PY  - 2020
AB  - Collaboration using mixed reality technology is an active area of research, where significant research is done to virtually bridge physical distances. There exist a diverse set of platforms and devices that can be used for a mixed-reality collaboration, and is largely focused for indoor scenarios, where, a stable tracking can be assumed. We focus on supporting collaboration between VR and AR users, where AR user is mobile outdoors, and VR user is immersed in true-sized digital twin. This cross-platform solution requires new user experiences for interaction, accurate modelling of the real-world, and working with noisy outdoor tracking sensor such as GPS. In this paper, we present our results and observations of real-time collaboration between cross-platform users, in the context of a geo-referenced virtual environment. We propose a solution for using GPS measurement in VSLAM to localize the AR user in an outdoor environment. The client applications enable VR and AR user to collaborate across the heterogeneous platforms seamlessly. The user can place or load dynamic contents tagged to a geolocation and share their experience with remote users in real-time.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Sun, Hongling; Liu, Yue; Zhang, Zhenliang; Liu, Xiaoxu; Wang, Yongtian
TI  - CCHI - Employing Different Viewpoints for Remote Guidance in a Collaborative Augmented Environment
PY  - 2018
AB  - This paper details the design, implementation and an initial evaluation of a collaborative platform named OptoBridge, which is aimed at enhancing remote guidance and skill acquisition for spatially distributed users. OptoBridge integrates augmented reality (AR), gesture interaction with video mediated communication and is preliminarily applied to the experimental teaching of the adjustment task with Michelson interferometer. An exploratory study has been conducted to qualitatively and quantitatively evaluate the extent to which different viewpoints affect the student's sense of presence, task performance, learning outcomes and subjective feelings in the remote collaborative augmented environment. 16 students from local universities have participated in the evaluation. The result shows the influence of two different viewpoints and indicates that OptoBridge can effectively support remote guidance and enhance the collaborators' experience.
SP  - 64
EP  - 70
JF  - Proceedings of the Sixth International Symposium of Chinese CHI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3202667.3202676
ER  - 

TY  - NA
AU  - Wang, Edward; Garrison, Jake; Whitmire, Eric; Goel, Mayank; Patel, Shwetak N.
TI  - UIST - Carpacio: Repurposing Capacitive Sensors to Distinguish Driver and Passenger Touches on In-Vehicle Screens
PY  - 2017
AB  - Standard vehicle infotainment systems often include touch screens that allow the driver to control their mobile phone, navigation, audio, and vehicle configurations. For the driver's safety, these interfaces are often disabled or simplified while the car is in motion. Although this reduced functionality aids in reducing distraction for the driver, it also disrupts the usability of infotainment systems for passengers. Current infotainment systems are unaware of the seating position of their user and hence, cannot adapt. We present Carpacio, a system that takes advantage of the capacitive coupling created between the touchscreen and the electrode present in the seat when the user touches the capacitive screen. Using this capacitive coupling phenomenon, a car infotainment system can intelligently distinguish who is interacting with the screen seamlessly, and adjust its user interface accordingly. Manufacturers can easily incorporate Carpacio into vehicles since the included seat occupancy detection sensor or seat heating coils can be used as the seat electrode. We evaluated Carpacio in eight different cars and five mobile devices and found that it correctly detected over 2600 touches with an accuracy of 99.4%.
SP  - 49
EP  - 55
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126623
ER  - 

TY  - NA
AU  - Sun, Wei; Li, Franklin Mingzhe; Huang, Congshu; Lei, Zhenyu; Steeper, Benjamin; Tao, Songyun; Tian, Feng; Zhang, Cheng
TI  - MobileHCI - ThumbTrak: Recognizing Micro-finger Poses Using a Ring with Proximity Sensing
PY  - 2021
AB  - ThumbTrak is a novel wearable input device that recognizes 12 micro-finger poses in real-time. Poses are characterized by the thumb touching each of the 12 phalanges on the hand. It uses a thumb-ring, built with a flexible printed circuit board, which hosts nine proximity sensors. Each sensor measures the distance from the thumb to various parts of the palm or other fingers. ThumbTrak uses a support-vector-machine (SVM) model to classify finger poses based on distance measurements in real-time. A user study with ten participants showed that ThumbTrak could recognize 12 micro finger poses with an average accuracy of 93.6%. We also discuss potential opportunities and challenges in applying ThumbTrak in real-world applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472060
ER  - 

TY  - JOUR
AU  - Argudo, Oscar; Andujar, Carlos; Chica, Antoni
TI  - Image-based tree variations
PY  - 2019
AB  - The automatic generation of realistic vegetation closely reproducing the appearance of specific plant species is still a challenging topic in computer graphics. In this paper, we present a new approach to generate new tree models from a small collection of frontal RGBA images of trees. The new models are represented either as single billboards (suitable for still image generation in areas such as architecture rendering) or as billboard clouds (providing parallax effects in interactive applications). Key ingredients of our method include the synthesis of new contours through convex combinations of exemplar countours, the automatic segmentation into crown/trunk classes and the transfer of RGBA colour from the exemplar images to the synthetic target. We also describe a fully automatic approach to convert a single tree image into a billboard cloud by extracting superpixels and distributing them inside a silhouette-defined 3D volume. Our algorithm allows for the automatic generation of an arbitrary number of tree variations from minimal input, and thus provides a fast solution to add vegetation variety in outdoor scenes.Peer ReviewedPostprint (author's final draft
SP  - 174
EP  - 184
JF  - Computer Graphics Forum
VL  - 39
IS  - 1
PB  - 
DO  - 10.1111/cgf.13752
ER  - 

TY  - BOOK
AU  - Sikora, Axel; Nyemkova, Elena; Lakh, Yuriy
TI  - IDAACS-SWS - Accuracy Improvements of Identification and Authentication of Devices by EM-Measurements
PY  - 2020
AB  - The authentication method of electronic devices, based on individual forms of correlograms of their internal electric noises, is well-known. Specific physical differences in the components – for example, caused by variations in production quality – cause specific electrical signals, i.e. electric noise, in the electronic device. It is possible to obtain this information and to identify the specific differences of the individual devices using an embedded analog-to-digital converter (ADC). These investigations confirm the possibility to identify and authenticate electronic devices using bit templates, calculated from the sequence of values of the normalized autocorrelation function of noise. Experiments have been performed using personal computers. The probability of correct identification and authentication increases with increasing noise recording duration. As a result of these experiments, an accuracy of 98.1% was achieved for a 1 second-long registration of EM for a set of investigated computers.
SP  - 1
EP  - 5
JF  - 2020 IEEE 5th International Symposium on Smart and Wireless Systems within the Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/idaacs-sws50031.2020.9297071
ER  - 

TY  - NA
AU  - Yoshida, Takatoshi; Ogawa, Junichi; Choi, Kyung Yun; Bushnaq, Sanad; Nakagaki, Ken; Ishii, Hiroshi
TI  - TEI - inDepth: Force-based Interaction with Objects beyond A Physical Barrier
PY  - 2021
AB  - We propose inDepth, a novel system that enables force-based interaction with objects beyond a physical barrier by using scalable force sensor modules. inDepth transforms a physical barrier (eg. glass showcase or 3D display) to a tangible input interface that enables users to interact with objects out of reach, by applying finger pressure on the barrier’s surface. To achieve this interaction, our system tracks the applied force as a directional vector by using three force sensors installed underneath the barrier. Meanwhile, our force-to-depth conversion algorithm translates force intensity into a spatial position along its direction beyond the barrier. Finally, the system executes various operations on objects in that position based on the type of application. In this paper, we introduce inDepth concept and its design space. We also demonstrate example applications, including selecting items in showcases and manipulating 3D rendered models.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3442447
ER  - 

TY  - NA
AU  - Park, Keunwoo; Kim, Sunbum; Yoon, Youngwoo; Kim, Tae-Kyun; Lee, Geehyuk
TI  - UIST - DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera
PY  - 2020
AB  - Near-surface multi-finger tracking (NMFT) technology expands the input space of touchscreens by enabling novel interactions such as mid-air and finger-aware interactions. We present DeepFisheye, a practical NMFT solution for mobile devices, that utilizes a fisheye camera attached at the bottom of a touchscreen. DeepFisheye acquires the image of an interacting hand positioned above the touchscreen using the camera and employs deep learning to estimate the 3D position of each fingertip. We created two new hand pose datasets comprising fisheye images, on which our network was trained. We evaluated DeepFisheye's performance for three device sizes. DeepFisheye showed average errors with approximate value of 20 mm for fingertip tracking across the different device sizes. Additionally, we created simple rule-based classifiers that estimate the contact finger and hand posture from DeepFisheye's output. The contact finger and hand posture classifiers showed accuracy of approximately 83 and 90%, respectively, across the device sizes.
SP  - 1132
EP  - 1146
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415818
ER  - 

TY  - NA
AU  - Ueno, Michihiko; Satoh, Shin'ichi
TI  - IUI - Continuous and Gradual Style Changes of Graphic Designs with Generative Model
PY  - 2021
AB  - Creating a high-quality layout design from scratch is difficult for novices. Therefore, novices often consult the works of other skilled designers for ideas regarding layout designs. Researchers have previously investigated methods to support the layout design process; these works mainly focused on retrieval methods for similar layout designs, or refinement of existing layouts. To enhance user creativity in designing layouts, assistance is needed for exploring various designs. Herein, we propose a novel deep generative model that enables the generation of various layout designs and guarantees continuous and gradual changes in layouts, for effectively exploring graphic designs. Accordingly, we present an adversarial training method with dual critic networks; we trained our model by a public graphic design dataset. We developed another interaction method that allows the user to change the graphic designs between two different layout styles and categories parametrically. We demonstrated the efficacy of the proposed method in generating rich layout variations with representation of latent space by comparing the layout designs generated by our model with by an existing model.
SP  - 280
EP  - 289
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450666
ER  - 

TY  - CHAP
AU  - Pang, Yijie; Wu, Shaochun; Zhu, Honghao
TI  - ICONIP (1) - The Sample Selection Model Based on Improved Autoencoder for the Online Questionnaire Investigation
PY  - 2017
AB  - This paper presents the sample selection model based on improved autoencoder to solve low response rate in the online questionnaire investigation industry. This model utilizes the improved autoencoder to extract the samples’ features and uses the softmax classifier to predict the samples’ loyalty. Furthermore, the autoencoder is improved with three steps: first, the number of middle hidden layer nodes is determined by Singular Value Decomposition (SVD); second, the loss function of the autoencoder is improved with the information gain ratio; finally, the concept of Random Denoising Autoencoder (RDA) is introduced to enhance the robustness of the model. Through the selection model, samples with high loyalty will be picked out to answer the questionnaire so that the response rate can be improved. Experiments are performed to determine the feasibility and effectiveness of the model. Compared with the BP neural networks, the prediction accuracy of our model is totally improved about 8.5% and the success rate of sending questionnaires is also improved about 15%.
SP  - 175
EP  - 184
JF  - Neural Information Processing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-70087-8_19
ER  - 

TY  - BOOK
AU  - Soliman, Mohamed; Mueller, Franziska; Hegemann, Lena; Roo, Joan Sol; Theobalt, Christian; Steimle, Jürgen
TI  - ISS - FingerInput: Capturing Expressive Single-Hand Thumb-to-Finger Microgestures
PY  - 2018
AB  - Single-hand thumb-to-finger microgestures have shown great promise for expressive, fast and direct interactions. However, pioneering gesture recognition systems each focused on a particular subset of gestures. We are still in lack of systems that can detect the set of possible gestures to a fuller extent. In this paper, we present a consolidated design space for thumb-to-finger microgestures. Based on this design space, we present a thumb-to-finger gesture recognition system using depth sensing and convolutional neural networks. It is the first system that accurately detects the touch points between fingers as well as the finger flexion. As a result, it can detect a broader set of gestures than the existing alternatives, while also providing high-resolution information about the contact points. The system shows an average accuracy of 91% for the real-time detection of 8 demanding thumb-to-finger gesture classes. We demonstrate the potential of this technology via a set of example applications.
SP  - 177
EP  - 187
JF  - Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3279778.3279799
ER  - 

TY  - NA
AU  - Raina, Ayush; McComb, Christopher; Cagan, Jonathan
TI  - Learning to Design From Humans: Imitating Human Designers Through Deep Learning
PY  - 2019
AB  - Humans as designers have quite versatile problem-solving strategies. Computer agents on the other hand can access large scale computational resources to solve certain design problems. Hence, if agents can learn from human behavior, a synergetic human-agent problem solving team can be created. This paper presents an approach to extract human design strategies and implicit rules, purely from historical human data, and use that for design generation. A two-step framework that learns to imitate human design strategies from observation is proposed and implemented. This framework makes use of deep learning constructs to learn to generate designs without any explicit information about objective and performance metrics. The framework is designed to interact with the problem through a visual interface as humans did when solving the problem. It is trained to imitate a set of human designers by observing their design state sequences without inducing problem-specific modelling bias or extra information about the problem. Furthermore, an end-to-end agent is developed that uses this deep learning framework as its core in conjunction with image processing to map pixel-to-design moves as a mechanism to generate designs. Finally, the designs generated by a computational team of these agents are then compared to actual human data for teams solving a truss design problem. Results demonstrates that these agents are able to create feasible and efficient truss designs without guidance, showing that this methodology allows agents to learn effective design strategies.
SP  - 1
EP  - 24
JF  - Volume 2A: 45th Design Automation Conference
VL  - 141
IS  - 11
PB  - 
DO  - 10.1115/detc2019-97399
ER  - 

TY  - NA
AU  - Sugiura, Yuta; Nakamura, Fumihiko; Kawai, Wataru; Kikuchi, Takashi; Sugimoto, Maki
TI  - Behind the palm: Hand gesture recognition through measuring skin deformation on back of hand by using optical sensors
PY  - 2017
AB  - We propose a system consisting of a wearable device equipped with photo-reflective sensors arranged in an array. Hand gestures are recognized by measuring the skin deformation of the back of the hand. Since the muscles and bones on the back of the hand are linked to the fingers, finger movement can be clearly observed. Skin deformation is measured using several photo-reflective sensors. Skin deformation can be determined by measuring the distance between the device and skin with these sensors. The system estimates hand gestures with a support vector machine using the sensor data. Since this system simultaneously records the hand shape using Leap Motion in the learning phase, a user can freely register gestures. The system further displays a reconstructed digital hand as a gesture-recognition result.
SP  - 1082
EP  - 1087
JF  - 2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/sice.2017.8105457
ER  - 

TY  - JOUR
AU  - Liang, Chen; Hsia, Chi; Yu, Chun; Yan, Yukang; Wang, Yuntao; Shi, Yuanchun
TI  - DRG-Keyboard
PY  - 2022
AB  - <jats:p>We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.</jats:p>
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569463
ER  - 

TY  - NA
AU  - Burnap, Alexander
TI  - Crowdsourcing for Engineering Design: Objective Evaluations and Subjective Preferences
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wolling, Florian; Van Laerhoven, Kristof
TI  - IBSync: Intra-body synchronization and implicit contextualization of wearable devices using artificial ECG landmarks
PY  - 2022
AB  - <jats:p>With a smaller form factor and a larger set of applications, body-worn devices have evolved into a collection of simultaneously deployed hardware units, rather than into a single all-round wearable. The sensor data, logged by such devices across the user's body, contains a wealth of information but is often difficult to synchronize. Especially the application of machine learning techniques, e.g., for activity recognition, suffers from the inaccuracy of the devices' internal clocks. In recent years, intra-body communication emerged as a promising alternative to the traditional wired and wireless communication techniques. Distributed wearable systems will notably benefit from its advantages, such as a superior energy efficiency. However, due to the absence of commercially available platforms, applications using this innovative technique remain rare and underinvestigated. With <jats:italic>IBSync</jats:italic>, we present a novel concept in which artificial landmark signals are received by body-worn devices on touching, approaching, or passing certain areas, surfaces, or objects with embedded transmitter beacons. The landmark signals enable both the wearables' intentional or incidental synchronization as well as the implicit contextualization using supplementary information about the beacons' situational context. For the detection of the landmarks, we propose to repurpose the on-board ECG sensor front-end available in recent commercial wearable devices. Evaluated on a total of 215 min of recordings from two devices, we demonstrate the concept's feasibility and a promising synchronization error of 0.80±1.79 samples or 6.25±14.00 ms at a device's sampling rate of 128 Hz.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Computer Science
VL  - 4
IS  - NA
PB  - 
DO  - 10.3389/fcomp.2022.915448
ER  - 

TY  - NA
AU  - Burnap, Alex; Liu, Ye; Pan, Yanxin; Lee, Honglak; Gonzalez, Richard; Papalambros, Panos Y.
TI  - Estimating and exploring the product form design space using deep generative models
PY  - 2016
AB  - <jats:p>Product forms in quantitative design methods are typically expressed with a mathematical representation such as vectors, trees, graphs, and grammars. Such formal representations are restrictive in terms of realism or flexibility, and this limits their utility for human designers who typically create product forms in a design space that is restricted by the medium (e.g., free-hand sketching) and by their cognitive skills (e.g., creativity and experience). To increase the value of formal representations to human designers, this paper proposes to represent the design space as designs sampled from a statistical distribution of form and estimate a generative model of this distribution using a large set of images and design attributes of previous designs. This statistical representation approach is both flexible and realistic, and is estimated using a deep (multi-layer) generative model. The value of the representation is demonstrated in a study of two-dimensional automobile body forms. Using 180,000 form data of automobile designs over the past decade, we can morph a vehicle form into different body types and brands, thus offering human designers potential insights on realistic new design possibilities.</jats:p>
SP  - NA
EP  - NA
JF  - Volume 2A: 42nd Design Automation Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1115/detc2016-60091
ER  - 

TY  - JOUR
AU  - Yu, Kevin; Eck, Ulrich; Pankratz, Frieder; Lazarovici, Marc; Wilhelm, Dirk; Navab, Nassir
TI  - Duplicated Reality for Co-located Augmented Reality Collaboration.
PY  - 2022
AB  - When two or more users attempt to collaborate in the same space with Augmented Reality, they often encounter conflicting intentions regarding the occupation of the same working area and self-positioning around such without mutual interference. Augmented Reality is a powerful tool for communicating ideas and intentions during a co-assisting task that requires multi-disciplinary expertise. To relax the constraint of physical co-location, we propose the concept of Duplicated Reality, where a digital copy of a 3D region of interest of the users' environment is reconstructed in real-time and visualized in-situ through an Augmented Reality user interface. This enables users to remotely annotate the region of interest while being co-located with others in Augmented Reality. We perform a user study to gain an in-depth understanding of the proposed method compared to an in-situ augmentation, including collaboration, effort, awareness, usability, and the quality of the task. The result indicates almost identical objective and subjective results, except a decrease in the consulting user's awareness of co-located users when using our method. The added benefit from duplicating the working area into a designated consulting area opens up new interaction paradigms to be further investigated for future co-located Augmented Reality collaboration systems.
SP  - 2190
EP  - 2200
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2022.3150520
ER  - 

TY  - NA
AU  - Xu, Zheer; Wong, Pui Chung; Gong, Jun; Wu, Te-Yen; Nittala, Aditya Shekhar; Bi, Xiaojun; Steimle, Jürgen; Fu, Hongbo; Zhu, Kening; Yang, Xing-Dong
TI  - UIST - TipText: Eyes-Free Text Entry on a Fingertip Keyboard
PY  - 2019
AB  - In this paper, we propose and investigate a new text entry technique using micro thumb-tip gestures. Our technique features a miniature QWERTY keyboard residing invisibly on the first segment of the user's index finger. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The keyboard layout was optimized for eyes-free input by utilizing a spatial model reflecting the users' natural spatial awareness of key locations on the index finger. We present our approach of designing and optimizing the keyboard layout through a series of user studies and computer simulated text entry tests over 1,146,484 possibilities in the design space. The outcome is a 2×3 grid with the letters highly confining to the alphabetic and spatial arrangement of QWERTY. Our user evaluation showed that participants achieved an average text entry speed of 11.9 WPM and were able to type as fast as 13.3 WPM towards the end of the experiment.
SP  - 883
EP  - 899
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347865
ER  - 

TY  - NA
AU  - Roeschlin, Marc; Sluganovic, Ivo; Martinovic, Ivan; Tsudik, Gene; Rasmussen, Kasper Bonne
TI  - WPES@CCS - Generating Secret Keys from Biometric Body Impedance Measurements
PY  - 2016
AB  - Growing numbers of ubiquitous electronic devices and services motivate the need for effortless user authentication and identification. While biometrics are a natural means of achieving these goals, their use poses privacy risks, due mainly to the difficulty of preventing theft and abuse of biometric data. One way to minimize information leakage is to derive biometric keys from users' raw biometric measurements. Such keys can be used in subsequent security protocols and ensure that no sensitive biometric data needs to be transmitted or permanently stored.This paper is the first attempt to explore the use of human body impedance as a biometric trait for deriving secret keys. Building upon Randomized Biometric Templates as a key generation scheme, we devise a mechanism that supports consistent regeneration of unique keys from users' impedance measurements. The underlying set of biometric features are found using a feature learning technique based on Siamese networks. Compared to prior feature extraction methods, the proposed technique offers significantly improved recognition rates in the context of key generation.Besides computing experimental error rates, we tailor a known key guessing approach specifically to the used key generation scheme and assess security provided by the resulting keys. We give a very conservative estimate of the number of guesses an adversary must make to find a correct key. Results show that the proposed key generation approach produces keys comparable to those obtained by similar methods based on other biometrics.
SP  - 59
EP  - 69
JF  - Proceedings of the 2016 ACM on Workshop on Privacy in the Electronic Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2994620.2994626
ER  - 

TY  - JOUR
AU  - Jorgensen, Erik J.; Werner, Frank T.; Prvulovic, Milos; Zajic, Alenka
TI  - Deep Learning Classification of Motherboard Components by Leveraging EM Side-Channel Signals
PY  - 2021
AB  - We show that a broad range of motherboard components can be classified based on their electromagnetic (EM) emanations using a lightweight convolutional neural network (CNN) architecture. Our random bootstrap sampling approach to cross-validation allows us to select robust models that achieve strong accuracy when classifying processor, memory, power, and ethernet integrated circuits (ICs) on internet of things (IoT) devices in two different excitation states. Our method also performs equal to or better than a k-Nearest Neighbors (k-NN) classifier baseline in all tested cases. We develop an anomaly detection procedure to flag the types of IC models not seen by the CNN in training. An analysis of signals passing through our end-to-end trained model allows us to generate insights about the discriminative features in the emanations of different components. We analyze how the convolution layers’ outputs pass through linear transformation layers to understand which of the input features are most important. Our results demonstrate that a CNN architecture can be used to classify ICs accurately using a diverse set of EM emanations while providing insights about the relevant discriminative features between components.
SP  - 114
EP  - 126
JF  - Journal of Hardware and Systems Security
VL  - 5
IS  - 2
PB  - 
DO  - 10.1007/s41635-021-00116-2
ER  - 

TY  - NA
AU  - Gheran, Bogdan-Florin; Vatavu, Radu-Daniel; Vanderdonckt, Jean
TI  - Conference on Designing Interactive Systems (Companion Volume) - Ring x2: Designing Gestures for Smart Rings using Temporal Calculus
PY  - 2018
AB  - We introduce "Ring×2," a design space for gesture input with two smart rings. Wearing two rings at once opens new input opportunities, unexplored by the community so far, such as bimanual ring gestures or shifting input from one smart ring to the other to effectively manage situational impairments, such as encumbrance. To enable such developments and explorations, we present a formal description of designing two-ring gestures from the perspective of temporal calculus, a logic-based framework for reasoning about events and actions occurring in time, which we exemplify on a dataset of 83 bimanual gestures performed by 24 participants with two rings. We hope that our exploration of two-ring gestures and our design approach rooted in temporal calculus will be provocative and inspiring for the community, leading to new designs of input techniques for smart rings.
SP  - 117
EP  - 122
JF  - Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3197391.3205422
ER  - 

TY  - JOUR
AU  - del Amo, Iñigo Fernández; Erkoyuncu, John Ahmet; Vrabič, Rok; Frayssinet, Romain; Reynel, Cristina Vazquez; Roy, Rajkumar
TI  - Structured authoring for AR-based communication to enhance efficiency in remote diagnosis for complex equipment
PY  - 2020
AB  - Abstract Remote diagnosis procedures are prone to communication errors due to varying levels of experience and knowledge between expert maintainers and technicians. These result in inefficiencies that delay the diagnosis process. The aim of the paper is to develop a Structured-Message Authoring framework for Augmented Reality (AR) Remote Communication (SMAARRC) and to evaluate its ability to enhance the efficiency of remote diagnosis services. The framework proposes a message structure and automatic AR content creation rules for it that enable data capture and sharing within a remote context. Laboratory experiments present an average time reduction of 56% for remote calls while maintaining same quality compared to traditional remote communication methods (phone calls and emails). Remote experts feedback evidence the usability and feasibility of this framework to work in real-life conditions.
SP  - 101096
EP  - NA
JF  - Advanced Engineering Informatics
VL  - 45
IS  - NA
PB  - 
DO  - 10.1016/j.aei.2020.101096
ER  - 

TY  - NA
AU  - Elvezio, Carmine; Sukan, Mengu; Oda, Ohan; Feiner, Steven; Tversky, Barbara
TI  - SIGGRAPH VR Village - Remote collaboration in AR and VR using virtual replicas
PY  - 2017
AB  - In many complex tasks, a remote subject-matter expert may need to assist a local user, to guide their actions on objects in the local user's environment. However, effective spatial referencing and action demonstration in a remote physical environment can be challenging. We demonstrate an approach that uses Virtual Reality (VR) or Augmented Reality (AR) for the remote expert, and AR for the local user, each wearing a stereo head-worn display (HWD). Our approach allows the remote expert to create and manipulate virtual replicas of physical objects in the local environment to refer to parts of those physical objects and to indicate actions on them. This can be especially useful for parts that are occluded or difficult to access. The remote expert can demonstrate actions in 3D by manipulating virtual replicas, supported by constraints and annotations, and point in 3D to portions of virtual replicas to annotate them.
SP  - 13
EP  - NA
JF  - ACM SIGGRAPH 2017 VR Village
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3089269.3089281
ER  - 

TY  - JOUR
AU  - Danhaive, Renaud; Mueller, Caitlin
TI  - Design subspace learning: Structural design space exploration using performance-conditioned generative modeling
PY  - 2021
AB  - NA
SP  - 103664
EP  - NA
JF  - Automation in Construction
VL  - 127
IS  - NA
PB  - 
DO  - 10.1016/j.autcon.2021.103664
ER  - 

TY  - NA
AU  - Yan, Zhenyu; Song, Qun; Tan, Rui; Li, Yang; Kong, Adams Wai Kin
TI  - Towards Touch-to-Access Device Authentication Using Induced Body Electric Potentials
PY  - 2019
AB  - This paper presents TouchAuth, a new touch-to-access device authentication approach using induced body electric potentials (iBEPs) caused by the indoor ambient electric field that is mainly emitted from the building's electrical cabling. The design of TouchAuth is based on the electrostatics of iBEP generation and a resulting property, i.e., the iBEPs at two close locations on the same human body are similar, whereas those from different human bodies are distinct. Extensive experiments verify the above property and show that TouchAuth achieves high-profile receiver operating characteristics in implementing the touch-to-access policy. Our experiments also show that a range of possible interfering sources including appliances' electromagnetic emanations and noise injections into the power network do not affect the performance of TouchAuth. A key advantage of TouchAuth is that the iBEP sensing requires a simple analog-to-digital converter only, which is widely available on microcontrollers. Compared with existing approaches including intra-body communication and physiological sensing, TouchAuth is a low-cost, lightweight, and convenient approach for authorized users to access the smart objects found in indoor environments.
SP  - 23
EP  - NA
JF  - The 25th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3300061.3300118
ER  - 

TY  - CHAP
AU  - Lee, Lik-Hang; Braud, Tristan; Hui, Pan
TI  - Embodied Interaction on Constrained Interfaces for Augmented Reality
PY  - 2021
AB  - NA
SP  - 239
EP  - 271
JF  - Springer Handbooks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-67822-7_10
ER  - 

TY  - JOUR
AU  - Kyriakou, Harris; Nickerson, Jeffrey V.; Sabnis, Gaurav
TI  - Knowledge reuse for customization: metamodels in an open design community for 3D printing
PY  - 2017
AB  - Theories of knowledge reuse posit two distinct processes: reuse for replication and reuse for innovation. We identify another distinct process, reuse for customization. Reuse for customization is a process in which designers manipulate the parameters of metamodels to produce models that fulfill their personal needs. We test hypotheses about reuse for customization in Thingiverse, a community of designers that shares files for three-dimensional printing. 3D metamodels are reused more often than the 3D models they generate. The reuse of metamodels is amplified when the metamodels are created by designers with greater community experience. Metamodels make the community's design knowledge available for reuse for customization—or further extension of the metamodels, a kind of reuse for innovation.
SP  - 315
EP  - 332
JF  - MIS Quarterly
VL  - 41
IS  - 1
PB  - 
DO  - 10.25300/misq/2017/41.1.17
ER  - 

TY  - JOUR
AU  - Sathya, Anup; Li, Jiasheng; Rahman, Tauhidur; Gao, Ge; Peng, Huaishu
TI  - Calico
PY  - 2022
AB  - <jats:p>We explore Calico, a miniature relocatable wearable system with fast and precise locomotion for on-body interaction, actuation and sensing. Calico consists of a two-wheel robot and an on-cloth track mechanism or "railway," on which the robot travels. The robot is self-contained, small in size, and has additional sensor expansion options. The track system allows the robot to move along the user's body and reach any predetermined location. It also includes rotational switches to enable complex routing options when diverging tracks are presented. We report the design and implementation of Calico with a series of technical evaluations for system performance. We then present a few application scenarios, and user studies to understand the potential of Calico as a dance trainer and also explore the qualitative perception of our scenarios to inform future research in this space.</jats:p>
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550323
ER  - 

TY  - NA
AU  - Kokubu, Harutoshi; Tominaga, Koki; Shizuki, Buntarou
TI  - One–Handed Control for Smartwatches Using Thumb Gestures to Ring
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Asian HCI Symposium'22
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3516492.3558790
ER  - 

TY  - JOUR
AU  - Demir, Ilke; Aliaga, Daniel G.
TI  - Guided proceduralization: Optimizing geometry processing and grammar extraction for architectural models
PY  - 2018
AB  - Abstract We describe a guided proceduralization framework that optimizes geometry processing on architectural input models to extract target grammars. We aim to provide efficient artistic workflows by creating procedural representations from existing 3D models, where the procedural expressiveness is controlled by the user. Architectural reconstruction and modeling tasks have been handled as either time consuming manual processes or procedural generation with difficult control and artistic influence. We bridge the gap between creation and generation by converting existing manually modeled architecture to procedurally editable parametrized models, and carrying the guidance to procedural domain by letting the user define the target procedural representation. Additionally, we propose various applications of such procedural representations, including guided completion of point cloud models, controllable 3D city modeling, and other benefits of procedural modeling.
SP  - 257
EP  - 267
JF  - Computers & Graphics
VL  - 74
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2018.05.013
ER  - 

TY  - NA
AU  - Gong, Jun; Wu, Yu; Yan, Lei; Seyed, Teddy; Yang, Xing-Dong
TI  - UIST - Tessutivo: Contextual Interactions on Interactive Fabrics with Inductive Sensing
PY  - 2019
AB  - We present Tessutivo, a contact-based inductive sensing technique for contextual interactions on interactive fabrics. Our technique recognizes conductive objects (mainly metallic) that are commonly found in households and workplaces, such as keys, coins, and electronic devices. We built a prototype containing six by six spiral-shaped coils made of conductive thread, sewn onto a four-layer fabric structure. We carefully designed the coil shape parameters to maximize the sensitivity based on a new inductance approximation formula. Through a ten-participant study, we evaluated the performance of our proposed sensing technique across 27 common objects. We yielded 93.9% real-time accuracy for object recognition. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique.
SP  - 29
EP  - 41
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347897
ER  - 

TY  - JOUR
AU  - Fages, Arthur; Fleury, Cédric; Tsandilas, Theophanis
TI  - Understanding Multi-View Collaboration between Augmented Reality and Remote Desktop Users
PY  - 2022
AB  - <jats:p>Establishing an effective collaboration between augmented-reality (AR) and remote desktop users is a challenge because collaborators do not share a common physical space and equipment. Yet, such asymmetrical collaboration configurations are common today for many design tasks, due to the geographical distance of people or unusual circumstances such as a lockdown. We conducted a first study to investigate trade-offs of three remote representations of an AR workspace: a fully virtual representation, a first-person view, and an external view. Building on our findings, we designed ARgus, a multi-view video-mediated communication system that combines these representations through interactive tools for navigation, previewing, pointing, and annotation. We report on a second user study that observed how 12 participants used ARgus to provide remote instructions for an AR furniture arrangement task. Participants extensively used its view transition tools, while the system reduced their reliance on verbal instructions.</jats:p>
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555607
ER  - 

TY  - JOUR
AU  - Radu, Iulian; Joy, Tugce; Bowman, Yiran Z; Bott, Ian; Schneider, Bertrand
TI  - A Survey of Needs and Features for Augmented Reality Collaborations in Collocated Spaces
PY  - 2021
AB  - In this paper we contribute a literature review and organization framework for classifying the collaboration needs and features that should be considered when designing headset-based augmented reality (AR) experiences for collocated settings. In recent years augmented reality technology has been experiencing significant growth through the emergence of headsets that allow gestural interaction, and AR designers are increasingly interested in using this technology to enhance collaborative activities in a variety of physical environments. However, collaborative AR applications need to contain features that enhance collaboration and satisfy needs that are present during the group activities. When AR designers lack an understanding of what collaborators need during an interaction, or what features have already been designed to solve those needs, then AR creators will spend time redesigning features that have already been created, or worse, create applications that do not contain necessary features. While much work has been done on designing virtual reality (VR) collaborative environments, AR environments are a relatively newer design space, and designers are lacking a comprehensive framework for describing needs that arise during collaborative activities and the features that could be designed into AR applications to satisfy those needs. In this paper we contribute a literature review of 92 papers in the areas of augmented reality and virtual reality, and we contribute a list of design features and needs that are helpful to consider when designing for headset-based collaborative AR experiences.
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
PB  - 
DO  - 10.1145/3449243
ER  - 

TY  - JOUR
AU  - Zhou, Yushuang; Sharma, Ashutosh; Shabaz, Mohammad
TI  - Feature recognition of state signal of electromechanical integration railway turnout over health parameters using CMOS area array technology
PY  - 2022
AB  - NA
SP  - 715
EP  - 724
JF  - The Journal of Engineering
VL  - 2022
IS  - 7
PB  - 
DO  - 10.1049/tje2.12154
ER  - 

TY  - CHAP
AU  - Ienaga, Naoto; Kawai, Wataru; Fujita, Koji; Miyata, Natsuki; Sugiura, Yuta; Saito, Hideo
TI  - ACCV Workshops - A Thumb Tip Wearable Device Consisting of Multiple Cameras to Measure Thumb Posture
PY  - 2019
AB  - Today, cameras have become smaller and cheaper and can be utilized in various scenes. We took advantage of that to develop a thumb tip wearable device to estimate joint angles of a thumb as measuring human finger postures is important in terms of human-computer interface and to analyze human behavior. The device we developed consists of three small cameras attached at different angles so the cameras can capture the four fingers. We assumed that the appearance of the four fingers would change depending on the joint angles of the thumb. We made a convolutional neural network learn a regression relationship between the joint angles of the thumb and the images taken by the cameras. In this paper, we captured the keypoint positions of the thumb with a USB sensor device and calculated the joint angles to construct a dataset. The root mean squared error of the test data was 6.23\(^\circ \) and 4.75\(^\circ \).
SP  - 31
EP  - 38
JF  - Computer Vision – ACCV 2018 Workshops
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-21074-8_3
ER  - 

TY  - NA
AU  - Guo, Philip J.
TI  - CHI - Non-Native English Speakers Learning Computer Programming: Barriers, Desires, and Design Opportunities
PY  - 2018
AB  - People from nearly every country are now learning computer programming, yet the majority of programming languages, libraries, documentation, and instructional materials are in English. What barriers do non-native English speakers face when learning from English-based resources? What desires do they have for improving instructional materials? We investigate these questions by deploying a survey to a programming education website and analyzing 840 responses spanning 86 countries and 74 native languages. We found that non-native English speakers faced barriers with reading instructional materials, technical communication, reading and writing code, and simultaneously learning English and programming. They wanted instructional materials to use simplified English without culturally-specific slang, to use more visuals and multimedia, to use more culturally-agnostic code examples, and to embed inline dictionaries. Programming also motivated some to learn English better and helped clarify logical thinking about natural languages. Based on these findings, we recommend learner-centered design improvements to programming-related instructional resources and tools to make them more accessible to people around the world.
SP  - 396
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173970
ER  - 

TY  - JOUR
AU  - Li, Zhen; Annett, Michelle; Hinckley, Ken; Wigdor, Daniel
TI  - SMAC: A Simplified Model of Attention and Capture in Multi-Device Desk-Centric Environments
PY  - 2019
AB  - Prior research has demonstrated that users are increasingly employing multiple devices during daily work. Currently, devices such as keyboards, cell phones, and tablets remain largely unaware of their role within a user's workflow. As a result, transitioning between devices is tedious, often to the degree that users are discouraged from taking full advantage of the devices they have within reach. This work explores the device ecologies used in desk-centric environments and complies the insights observed into SMAC, a simplified model of attention and capture that emphasizes the role of user-device proxemics, as mediated by hand placement, gaze, and relative body orientation, as well as inter-device proxemics. SMAC illustrates the potential of harnessing the rich, proxemic diversity that exists between users and their device ecologies, while also helping to organize and synthesize the growing body of literature on distributed user interfaces. An evaluation study using SMAC demonstrated that users could easily understand the tenants of user- and inter-device proxemics and found them to be valuable within their workflows.
SP  - 1
EP  - 47
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - EICS
PB  - 
DO  - 10.1145/3300961
ER  - 

TY  - JOUR
AU  - Wang, Zhuo; Bai, Xiaoliang; Zhang, Shusheng; He, Weiping; Zhang, Xiangyu; Zhang, Li; Wang, Peng; Han, Dechuan; Yan, Yuxiang
TI  - Information-level AR instruction: a novel assembly guidance information representation assisting user cognition
PY  - 2019
AB  - Recently, there are some problems in AR instruction research in assembly field, such as irregular instruction design form and redundant display of instruction content. The reason is that there is no AR instruction design rule for AR assembly. The goal of this rule should be to maximize users’ cognitive efficiency of guided content. In order to solve this problem, this paper carries out relevant research work. Firstly, the definition of AR instruction at geometric level and information level is given, and the corresponding design rules of AR instruction, namely, geometric level visualization (GLV) and information-level visualization (ILV), are proposed under these two definitions. Then, a data processing model oriented to the above rules is established, and the relationship and difference between them are elaborated. And then, according to GLV and ILV, four visual interfaces are designed to guide AR assembly. A case study was designed to test the performance of the four interfaces under the two specifications in terms of assembly time, operation experience (including enjoyment, concentration, self-confidence, natural intuition, feasibility, effectiveness, availability, and comprehensibility). Finally, through the test results of each interface, the influencing factors of ILV on user’s assembly efficiency, cognitive efficiency, and understanding are determined, and three implications of ILV are summarized. The results show that ILV with MBD design elements can improve user’s operating experience better than GLV.
SP  - 603
EP  - 626
JF  - The International Journal of Advanced Manufacturing Technology
VL  - 106
IS  - 1
PB  - 
DO  - 10.1007/s00170-019-04538-9
ER  - 

TY  - JOUR
AU  - Vyas, Shantanu; Chen, Ting-Ju; Mohanty, Ronak R.; Jiang, Peng; Krishnamurthy, Vinayak R.
TI  - Latent Embedded Graphs for Image and Shape Interpolation
PY  - 2021
AB  - NA
SP  - 103091
EP  - NA
JF  - Computer-Aided Design
VL  - 140
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2021.103091
ER  - 

TY  - NA
AU  - Jansen, Pascal; Fischbach, Fabian; Gugenheimer, Jan; Stemasov, Evgeny; Frommel, Julian; Rukzio, Enrico
TI  - UIST - ShARe: Enabling Co-Located Asymmetric Multi-User Interaction for Augmented Reality Head-Mounted Displays
PY  - 2020
AB  - Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.
SP  - 459
EP  - 471
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415843
ER  - 

TY  - BOOK
AU  - Feiz, Shirin; Ramakrishnan, I. V.
TI  - W4A - Exploring feasibility of wrist gestures for non-visual interactions with wearables
PY  - 2019
AB  - The emergence of wearable devices such as smartwatches is spurring new user interface designs and interaction modalities for these devices. One such new input modality for interacting with smartwatches is wrist gestures. Smartwatches are beginning to support a set of wrist gestures using which users can do a range of one-handed interactions with these devices. Wrist gestures are particularly appealing for people with vision impairments (PVIs) as touch-based interaction with smartwatch screens is quite challenging for them. However, the question of how accessible are wrist gestures for PVIs to interact with wearable devices, remains unexplored. To this end, we conducted a user study to explore this question. The study reveals the accessibility barriers of the current generation of wrist gestures and sheds insight on how to make them accessible for PVIs.
SP  - NA
EP  - NA
JF  - The 16th Web for All Conference : San Francisco, 2019. Web for All Conference (16th : 2019 : San Francisco, Calif.)
VL  - 2019
IS  - NA
PB  - 
DO  - 10.1145/3315002.3317570
ER  - 

TY  - NA
AU  - Reis, Ruan; Soares, Gustavo; Mongiovi, Melina; Andrade, Wilkerson L.
TI  - FIE - Evaluating Feedback Tools in Introductory Programming Classes
PY  - 2019
AB  - This Research Full Paper presents a study on the evaluation of feedback tools in introductory programming classes. Recently, several tools have been proposed in order to provide guidance and help students overcome conceptual difficulties in programming education. Some tools leverage clustering algorithms and program repair techniques to automatically generate personalized hints for students’ incorrect programs. In contrast, some teachers choose to present students with program visualization tools to help them understand the dynamic execution of a source code. These tools are used to help students get correct solutions for programming assignments. However, due to limitations in assessments, it is still unclear how effective the feedback provided by these tools is. In this study, we analyzed the effectiveness of a tool for generating personalized hints and a tool for visualizing programs. To do so, we conducted a user study in which students, assisted by these tools, implemented solutions for three programming problems. Our results show that personalized hints can significantly reduce student’s effort to get correct solutions. In addition, personalized hints can provide students with an understanding of problem solving similar to when using test cases. However, students who used the program visualization tool got lower post-test performance than using other tools.
SP  - 1
EP  - 7
JF  - 2019 IEEE Frontiers in Education Conference (FIE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/fie43999.2019.9028418
ER  - 

TY  - NA
AU  - Asiri, Somayah; Alzahrani, Ahmad A.
TI  - The Effectiveness of Mixed Reality Environment-Based Hand Gestures in Distributed Collaboration
PY  - 2019
AB  - Mixed reality (MR) technologies are widely used in distributed collaborative learning scenarios and have made learning and training more flexible and intuitive. However, there are many challenges in the use of MR due to the difficulty in creating a physical presence, particularly when a physical task is being performed collaboratively. We therefore developed a novel MR system to overcomes these limitations and enhance the distributed collaboration user experience. The primary objective of this paper is to explore the potential of a MR-based hand gestures system to enhance the conceptual architecture of MR in terms of both visualization and interaction in distributed collaboration. We propose a synchronous prototype named MRCollab as an immersive collaborative approach that allows two or more users to communicate with a peer based on the integration of several technologies such as video, audio, and hand gestures.
SP  - NA
EP  - NA
JF  - 2019 2nd International Conference on Computer Applications & Information Security (ICCAIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cais.2019.8769588
ER  - 

TY  - NA
AU  - Hachisu, Taku; Bourreau, Baptiste; Suzuki, Kenji
TI  - CHI - EnhancedTouchX: Smart Bracelets for Augmenting Interpersonal Touch Interactions
PY  - 2019
AB  - EnhancedTouchX, a bracelet-type interpersonal body area network device, not only detects but also quantifies interpersonal hand-to-hand touch interactions. Without any wired connection, it can identify the direction and gestures of a touch. The developed device can connect to an external device via Bluetooth Low Energy for monitoring and logging where, when, how long, who, and how the touch interactions occurred. These daily augmented touch interactions provided by such contextual information would offer a variety of applications to facilitate social interactions. Our experiment, conducted with several pairs of participants, demonstrates that the devices can identify the direction of a touch (from one initiating the touch (active touch) to the one being touched (passive touch)) with 95% accuracy. In addition, the devices are also capable of identifying four types of touch gestures with 85% accuracy using a simple threshold classifier.
SP  - 321
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300551
ER  - 

TY  - JOUR
AU  - Syed, Toqeer Ali; Siddiqui, Muhammad Shoaib; Abdullah, Hurria Binte; Jan, Salman; Namoun, Abdallah; Alzahrani, Ali; Nadeem, Adnan; Alkhodre, Ahmad B
TI  - In-Depth Review of Augmented Reality: Tracking Technologies, Development Tools, AR Displays, Collaborative AR, and Security Concerns.
PY  - 2022
AB  - Augmented reality (AR) has gained enormous popularity and acceptance in the past few years. AR is indeed a combination of different immersive experiences and solutions that serve as integrated components to assemble and accelerate the augmented reality phenomena as a workable and marvelous adaptive solution for many realms. These solutions of AR include tracking as a means for keeping track of the point of reference to make virtual objects visible in a real scene. Similarly, display technologies combine the virtual and real world with the user's eye. Authoring tools provide platforms to develop AR applications by providing access to low-level libraries. The libraries can thereafter interact with the hardware of tracking sensors, cameras, and other technologies. In addition to this, advances in distributed computing and collaborative augmented reality also need stable solutions. The various participants can collaborate in an AR setting. The authors of this research have explored many solutions in this regard and present a comprehensive review to aid in doing research and improving different business transformations. However, during the course of this study, we identified that there is a lack of security solutions in various areas of collaborative AR (CAR), specifically in the area of distributed trust management in CAR. This research study also proposed a trusted CAR architecture with a use-case of tourism that can be used as a model for researchers with an interest in making secure AR-based remote communication sessions.
SP  - 146
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 23
IS  - 1
PB  - 
DO  - 10.3390/s23010146
ER  - 

TY  - NA
AU  - Markel, Julia M.; Guo, Philip J.
TI  - Designing the Future of Experiential Learning Environments for a Post-COVID World: A Preliminary Case Study
PY  - 2020
AB  - ABSTRACT Experiential learning environments such as computer labs and design studios are critical for learning hands-on skills like programming, design, and data science. However, during and even after a global pandemic many people will not be willing to congregate in such high-density physical spaces due to safety concerns. How can we design experiential learning environments of the future to make them broadly accessible regardless of whether students can be physically present? To spur discussion about this question, we performed a preliminary case study where we compared in-person and online versions of software-based design courses created before and during the early-2020 COVID pandemic, respectively. We found that online tools such as videoconferencing, chat, and screensharing worked better than their in-person counterparts for certain user needs, but they cannot foster as much ambient awareness or spontaneous interaction. We then offer design recommendations for how to improve remote learning tools to potentially match or exceed face-to-face interactions. We hope our work can inspire discussion at this symposium about how we can design better learning environments to prepare people for the new future of work in a post-COVID world. Keywords remote learning technologies, experiential learning ABOUT THE AUTHOR/S Julia M. Markel UC San Diego jmarkel@ucsd.edu Philip J. Guo UC San Diego pg@ucsd.edu Philip Guo is an associate professor of cognitive science at UC San Diego. His research spans human-computer interaction, programming tools, and online learning. He currently studies what motivates people to learn programming and builds tools to help people better understand code and data (e.g., http://pythontutor.com/). Academic website: https://pg.ucsd.edu/ New Future of Work 2020, August 3–5, 2020 © 2020 Copyright held by the owner/author(s).
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chen, Yan; Lee, Sang Won; Xie, Yin; Yang, Yiwei; Lasecki, Walter S.; Oney, Steve
TI  - CHI - Codeon: On-Demand Software Development Assistance
PY  - 2017
AB  - Software developers rely on support from a variety of resources---including other developers---but the coordination cost of finding another developer with relevant experience, explaining the context of the problem, composing a specific help request, and providing access to relevant code is prohibitively high for all but the largest of tasks. Existing technologies for synchronous communication (e.g. voice chat) have high scheduling costs, and asynchronous communication tools (e.g. forums) require developers to carefully describe their code context to yield useful responses. This paper introduces Codeon, a system that enables more effective task hand-off between end-user developers and remote helpers by allowing asynchronous responses to on-demand requests. With Codeon, developers can request help by speaking their requests aloud within the context of their IDE. Codeon automatically captures the relevant code context and allows remote helpers to respond with high-level descriptions, code annotations, code snippets, and natural language explanations. Developers can then immediately view and integrate these responses into their code. In this paper, we describe Codeon, the studies that guided its design, and our evaluation that its effectiveness as a support tool. In our evaluation, developers using Codeon completed nearly twice as many tasks as those who used state-of-the-art synchronous video and code sharing tools, by reducing the coordination costs of seeking assistance from other developers.
SP  - 6220
EP  - 6231
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025972
ER  - 

TY  - NA
AU  - Elvezio, Carmine; Ling, Frank; Liu, Jen-Shuo; Feiner, Steven
TI  - UIST (Adjunct Volume) - Collaborative Virtual Reality for Low-Latency Interaction
PY  - 2018
AB  - In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.
SP  - 179
EP  - 181
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3271643
ER  - 

TY  - BOOK
AU  - Ito, Hiroki; Shimakawa, Hiromitsu; Harada, Fumiko
TI  - ISM@FedCSIS - Advanced Comprehension Analysis Using Code Puzzle
PY  - 2021
AB  - In programming education, the instructor tries to find out the learner who needs help by grasping the understanding using a written test and e-learning. However, in reality, not many learners will acquire the skill of writing source codes. This kind of current situation implies that programming ability of learners cannot be measured by tests that require knowledge. This paper focuses on not only the knowledge items required for programming but also the programming thinking (computational thinking), which is the ability to combine the constituent elements of the program. In this paper, we propose a method to estimate the learner’s understanding from the learner’s process to solve the code puzzles that require programming thinking as well as knowledge. The experimental result with the interface showed that the proposed method could estimate with the accuracy of 80% or more. The accurate measurement of the learner’s programming ability contributes to developing the learner’s true programming ability, which cannot measured by only the score of written tests. In addition, the importance of each variable in the behavior analysis leads to the identification of learner’s misunderstanding factors and the improvement of class contents. This study shows that it is possible to estimate the comprehension level of a programming thinking ability from only behavior of code puzzle, without sensors. The ability of learners to actually write programs is more important than their grades. This research can be developed to help develop the Information Technology talent we need in this era.
SP  - 45
EP  - 64
JF  - Lecture Notes in Business Information Processing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-71846-6_3
ER  - 

TY  - NA
AU  - Maruyama, Yusuke; Kono, Yasuyuki
TI  - AH - AR Pottery Wheel-Throwing by Attaching Omnidirectional Cameras to the Center of a User's Palms
PY  - 2019
AB  - This research describes our system for AR pottery wheel-throwing employing an HMD and omnidirectional cameras each of which is attached to the center of a user's palm. The omnidirectional cameras enable the user's finger postures and the three-dimensional relative position and orientation between the user's hands and virtual clay model on the wheel to be estimated. Our system detects a marker on the desk and the wheel is set on its coordinate system along with the finger posture estimation in real time. The system then simulates the collision between the virtual clay model and the left/right hand model based on the above information. Pottery wheel-throwing is reproduced in Unity software environment by deforming the clay model by contact with hand models in this simulation.
SP  - 47
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311856
ER  - 

TY  - JOUR
AU  - Liang, Chen; Yu, Chun; Qin, Yue; Wang, Yuntao; Shi, Yuanchun
TI  - DualRing: Enabling Subtle and Expressive Hand Interaction with Dual IMU Rings
PY  - 2021
AB  - We present DualRing, a novel ring-form input device that can capture the state and movement of the user's hand and fingers. With two IMU rings attached to the user's thumb and index finger, DualRing can sense not only the absolute hand gesture relative to the ground but also the relative pose and movement among hand segments. To enable natural thumb-to-finger interaction, we develop a high-frequency AC circuit for on-body contact detection. Based on the sensing information of DualRing, we outline the interaction space and divide it into three sub-spaces: within-hand interaction, hand-to-surface interaction, and hand-to-object interaction. By analyzing the accuracy and performance of our system, we demonstrate the informational advantage of DualRing in sensing comprehensive hand gestures compared with single-ring-based solutions. Through the user study, we discovered the interaction space enabled by DualRing is favored by users for its usability, efficiency, and novelty.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478114
ER  - 

TY  - JOUR
AU  - Grubert, Jens; Kranz, Matthias; Quigley, Aaron
TI  - Challenges in Mobile Multi-Device Ecosystems
PY  - 2016
AB  - Coordinated multi-display environments from the desktop, second-screen to gigapixel display walls are increasingly common. Personal and intimate mobile and wearable devices such as head-mounted displays, smartwatches, smartphones and tablets are rarely part of such multi-device ecosystems. With this paper, we contribute to a better understanding about factors that impede the creation and use of such mobile multi-device ecosystems. We base our findings on literature research and an expert survey. Specifically, we present grounded challenges relevant for the design, development and use of mobile multi-device environments.
SP  - 5
EP  - NA
JF  - mUX: The Journal of Mobile User Experience
VL  - 5
IS  - 1
PB  - 
DO  - 10.1186/s13678-016-0007-y
ER  - 

TY  - NA
AU  - Teo, Theophilus; Lee, Gun A.; Billinghurst, Mark; Adcock, Matt
TI  - OZCHI - Hand gestures and visual annotation in live 360 panorama-based mixed reality remote collaboration
PY  - 2018
AB  - In this paper, we investigate hand gestures and visual annotation cues overlaid in a live 360 panorama-based Mixed Reality remote collaboration. The prototype system captures 360 live panorama video of the surroundings of a local user and shares it with another person in a remote location. The two users wearing Augmented Reality or Virtual Reality head-mounted displays can collaborate using augmented visual communication cues such as virtual hand gestures, ray pointing, and drawing annotations. Our preliminary user evaluation comparing these cues found that using visual annotation cues (ray pointing and drawing annotation) helps local users perform collaborative tasks faster, easier, making less errors and with better understanding, compared to using only virtual hand gestures.
SP  - 406
EP  - 410
JF  - Proceedings of the 30th Australian Conference on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3292147.3292200
ER  - 

TY  - CHAP
AU  - Le Goc, Mathieu; Zhao, Allen; Wang, Ye; Dietz, Griffin; Semmens, Robert; Follmer, Sean
TI  - Investigating Active Tangibles and Augmented Reality for Creativity Support in Remote Collaboration
PY  - 2019
AB  - Physical manipulation is a key part of externalizing representations of knowledge and the creative process. However, contemporary tools for remote collaboration ignore physical manipulation and the haptic modality. We are interested in exploring remote physical manipulation in the context of ideation and brainstorming. Augmented Reality provides much of the benefits of spatial representation of remote participants, yet AR does not allow for rich physical manipulation and haptic feedback. Thus, we propose to use pairs of multi-robot system to provide synchronized haptic proxies in conjunction with the AR system. These small, tangible robots can be used directly as handles for digital models. We share insights gathered during experimentation to help design platforms combining AR and actuated tangibles, and present several application scenarios to illustrate their potential for remote collaboration.
SP  - 185
EP  - 200
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-28960-7_12
ER  - 

TY  - JOUR
AU  - Irlitti, Andrew; Piumsomboon, Thammathip; Jackson, Daniel; Thomas, Bruce H.
TI  - Conveying spatial awareness cues in xR collaborations
PY  - 2019
AB  - Spatial Augmented Reality (SAR) systems can be suitably combined with other existing Extended Reality (xR) technologies to support collaboration. In existing strategies, users unencumbered by a viewing technology, such as a tablet interface or a head-mounted display, must rely on the transmission of their collaborators' positioning through interpreting a first-person camera view. This design creates a seam between a user's experience of the augmented physical environment in SAR, and their collaborators' experience inside the virtual environment. To assist in development and evaluation of spatial cues to support spatial awareness in SAR environments, an egocentric spatial-communication taxonomy is presented given two determining dimensions, a cue's attachment (physical/virtual) and animation (local/world). We developed four egocentric cues which characterize the four independent dimensions of the matrix: arrow, path, glow , and radial , and a single exocentric world in miniature visualization. Our study shows that virtual attachment cues are preferred, providing the highest accuracy, highest performance when collaborators are occluded, and produce the least mental effort when used with a single virtual collaborator. For multiple collaborators however, the virtual attached, world animated radial cue produces significant increases in mental load and reductions in preference, demonstrating the impact of visual augmentation clutter. The single exocentric visualization produced higher levels of head movement, and poorer accuracy, however the novelty of the visualization produced positive qualitative results.
SP  - 3178
EP  - 3189
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2019.2932173
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Wu, Erwin; Lee, Juyoung; Quigley, Aaron; Koike, Hideki
TI  - UIST - Opisthenar: Hand Poses and Finger Tapping Recognition by Observing Back of Hand Using Embedded Wrist Camera
PY  - 2019
AB  - We introduce a vision-based technique to recognize static hand poses and dynamic finger tapping gestures. Our approach employs a camera on the wrist, with a view of the opisthenar (back of the hand) area. We envisage such cameras being included in a wrist-worn device such as a smartwatch, fitness tracker or wristband. Indeed, selected off-the-shelf smartwatches now incorporate a built-in camera on the side for photography purposes. However, in this configuration, the fingers are occluded from the view of the camera. The oblique angle and placement of the camera make typical vision-based techniques difficult to adopt. Our alternative approach observes small movements and changes in the shape, tendons, skin and bones on the opisthenar area. We train deep neural networks to recognize both hand poses and dynamic finger tapping gestures. While this is a challenging configuration for sensing, we tested the recognition with a real-time user test and achieved a high recognition rate of 89.4% (static poses) and 67.5% (dynamic gestures). Our results further demonstrate that our approach can generalize across sessions and to new users. Namely, users can remove and replace the wrist-worn device while new users can employ a previously trained system, to a certain degree. We conclude by demonstrating three applications and suggest future avenues of work based on sensing the back of the hand.
SP  - 963
EP  - 971
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347867
ER  - 

TY  - JOUR
AU  - Martinovic, Ivan; Rasmussen, Kasper Bonne; Roeschlin, Marc; Tsudik, Gene
TI  - Pulse-Response: Exploring Human Body Impedance for Biometric Recognition
PY  - 2017
AB  - Biometric characteristics are often used as a supplementary component in user authentication and identification schemes. Many biometric traits, both physiological and behavioral, offering a wider range of security and stability, have been explored. We propose a new physiological trait based on the human body’s electrical response to a square pulse signal, called pulse-response, and analyze how this biometric characteristic can be used to enhance security in the context of two example applications: (1) an additional authentication mechanism in PIN entry systems and (2) a means of continuous authentication on a secure terminal. The pulse-response biometric recognition is effective because each human body exhibits a unique response to a signal pulse applied at the palm of one hand and measured at the palm of the other. This identification mechanism integrates well with other established methods and could offer an additional layer of security, either on a continuous basis or at log-in time. We build a proof-of-concept prototype and perform experiments to assess the feasibility of pulse-response for biometric authentication. The results are very encouraging, achieving an equal error rate of 2% over a static dataset and 9% over a dataset with samples taken over several weeks. We also quantize resistance to attack by estimating individual worst-case probabilities for zero-effort impersonation in different experiments.
SP  - 6
EP  - 31
JF  - ACM Transactions on Privacy and Security
VL  - 20
IS  - 2
PB  - 
DO  - 10.1145/3064645
ER  - 

TY  - NA
AU  - Wong, Pui Chung; Fu, Hongbo; Zhu, Kening
TI  - SIGGRAPH ASIA Mobile Graphics and Interactive Applications - Back-Mirror: back-of-device one-handed interaction on smartphones
PY  - 2016
AB  - We present Back-Mirror, a low-cost camera-based approach for widening the interaction space on the back surface of a smartphone by using mirror reflection. Back-Mirror consists of two main parts: a smartphone accessory with a mirror that can reflect the back surface to the rear-facing camera of the phone, and a computer-vision algorithm for gesture recognition based on the visual pattern on the back surface. Our approach captures the finger position on the back surface, and tracks finger movement with higher resolution than the previous methods. We further designed a set of intuitive gestures that can be recognized by Back-Mirror, including swiping up, down, left and right, tapping left, middle, right, and holding gestures. Furthermore, we created applications of Back-of-device, such as game, media player, photo gallery, and unlock mechanism, allowing users to experience the use of Back-Mirror gestures in the real-life scenarios.
SP  - 10
EP  - NA
JF  - SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2999508.2999512
ER  - 

TY  - NA
AU  - He, Zhenyi; Du, Ruofei; Perlin, Ken
TI  - ISMAR - CollaboVR: A Reconfigurable Framework for Creative Collaboration in Virtual Reality
PY  - 2020
AB  - Writing or sketching on whiteboards is an essential part of collaborative discussions in business meetings, reading groups, design sessions, and interviews. However, prior work in collaborative virtual reality (VR) systems has rarely explored the design space of multi-user layouts and interaction modes with virtual whiteboards. In this paper, we present CollaboVR, a reconfigurable framework for both co-located and geographically dispersed multi-user communication in VR. Our system unleashes users’ creativity by sharing freehand drawings, converting 2D sketches into 3D models, and generating procedural animations in real-time. To minimize the computational expense for VR clients, we leverage a cloud architecture in which the computational expensive application (Chalktalk) is hosted directly on the servers, with results being simultaneously streamed to clients. We have explored three custom layouts – integrated, mirrored, and projective – to reduce visual clutter, increase eye contact, or adapt different use cases. To evaluate CollaboVR, we conducted a within-subject user study with 12 participants. Our findings reveal that users appreciate the custom configurations and real-time interactions provided by CollaboVR. We have open sourced CollaboVR at https://github.com/snowymo/CollaboVR to facilitate future research and development of natural user interfaces and real-time collaborative systems in virtual and augmented reality.
SP  - 542
EP  - 554
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00082
ER  - 

TY  - NA
AU  - AlDuaij, Naser; Nieh, Jason
TI  - MobiSys - Tap: an app framework for dynamically composable mobile systems
PY  - 2021
AB  - As smartphones and tablets have become ubiquitous, there is a growing demand for apps that can enable users to collaboratively use multiple mobile systems. We present Tap, a framework that makes it easy for users to dynamically compose collections of mobile systems and developers to write apps that make use of those impromptu collections. Tap users control the composition by simply tapping systems together for discovery and authentication. The physical interaction mimics and supports ephemeral user interactions without the need for tediously exchanging user contact information such as phone numbers or email addresses. Tapping triggers a simple NFC-based mechanism to exchange connectivity information and security credentials that works across heterogeneous networks and requires no user accounts or cloud infrastructure support. Tap makes it possible for apps to use existing mobile platform APIs across multiple mobile systems by virtualizing data sources so that local and remote data sources can be combined together upon tapping. Virtualized data sources can be hardware or software features, including media, clipboard, calendar events, and devices such as cameras and microphones. Leveraging existing mobile platform APIs makes it easy for developers to write apps that use hardware and software features across dynamically composed collections of mobile systems. We have implemented a Tap prototype that allows apps to make use of both unmodified Android and iOS systems. We have modified and implemented various apps using Tap to demonstrate that it is easy to use and can enable apps to provide powerful new functionality by leveraging multiple mobile systems. Our results show that Tap has good performance, even for high-bandwidth features, and is user and developer friendly.
SP  - 336
EP  - 349
JF  - Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458864.3467678
ER  - 

TY  - NA
AU  - Aan, Hojun; Han, Sangsun; Kim, Hyeonkyu; Kim, Jimoon; Yoon, Pilhyoun; Kim, Kibum
TI  - Remote Virtual Showdown: A Collaborative Virtual Reality Game for People with Visual Impairments.
PY  - 2021
AB  - Many researchers have developed VR systems for people with visual impairments by using various audio feedback techniques. However, there has been much less study of collaborative VR systems in which people with visual impairments and people with able-body can participate together. Therefore, we developed a VR showdown game which is similar to a real Showdown game in which two players can play together in the same virtual environment. We incorporate auditory distance perception using the HRTF (Head Related Transform Function) based on a spatial position in VR. We developed two modes in the showdown game. One is the PVA (Player vs. Agent) mode in which people with visual impairments can play alone and the PVP (Player vs. Player) mode in which people with visual impairments can play with another player in the network environment. We conducted our user studies by comparing the performances of people with visual impairments and people with able-body. The user study results show that people with visual impairments won 67.6% of the games when competing against people with able-body. This paper reports an example of a collaborative VR system for people with visual impairments and also design guideline for developing VR systems for people with visual impairments.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kim, Daehwa; Park, Keunwoo; Lee, Geehyuk
TI  - CHI - AtaTouch: Robust Finger Pinch Detection for a VR Controller Using RF Return Loss
PY  - 2021
AB  - Handheld controllers are an essential part of VR systems. Modern sensing techniques enable them to track users’ finger movements to support natural interaction using hands. The sensing techniques, however, often fail to precisely determine whether two fingertips touch each other, which is important for the robust detection of a pinch gesture. To address this problem, we propose AtaTouch, which is a novel, robust sensing technique for detecting the closure of a finger pinch. It utilizes a change in the coupled impedance of an antenna and human fingers when the thumb and finger form a loop. We implemented a prototype controller in which AtaTouch detects the finger pinch of the grabbing hand. A user test with the prototype showed a finger-touch detection accuracy of 96.4%. Another user test with the scenarios of moving virtual blocks demonstrated low object-drop rate (2.75%) and false-pinch rate (4.40%). The results and feedback from the participants support the robustness and sensitivity of AtaTouch.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445442
ER  - 

TY  - NA
AU  - Xi, Mingze; Adcock, Matt; McCulloch, John
TI  - Future Agriculture Farm Management using Augmented Reality
PY  - 2018
AB  - Augmented reality (AR) technology is blooming in the past few years with a growing number of low-cost AR devices becoming available to the general public. AR techniques have demonstrated the capacity to optimise task efficiency in a broad range of industries and provide engaging entertainment and education experiences. However, the potential of AR has not yet been fully explored. One of the extremely underexplored areas is its application in broad agriculture sector. As a major source of food, agriculture has always been a national priority. Agriculture farming is highly labour-intensive and heavily relies on individual farmer’s expertise, resulting in challenging farm management issues. We argue that AR can make critical contributions to the optimum management of agriculture farms. We take aquaculture ponds as an example, and presented three use cases to show how AR can potentially support more efficient farm management activities: water quality management, remote collaboration, and boardroom discussion.
SP  - 1
EP  - 3
JF  - 2018 IEEE Workshop on Augmented and Virtual Realities for Good (VAR4Good)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/var4good.2018.8576887
ER  - 

TY  - JOUR
AU  - Sereno, Mickael; Wang, Xiyao; Besançon, Lonni; McGuffin, Michael J.; Isenberg, Tobias
TI  - Collaborative Work in Augmented Reality: A Survey.
PY  - 2022
AB  - In Augmented Reality (AR), users perceive virtual content anchored in the real world. It is used in medicine, education, games, navigation, maintenance, product design, and visualization, in both single-user and multi-user scenarios. Multi-user AR has received limited attention from researchers, even though AR has been in development for more than two decades. We present the state of existing work at the intersection of AR and Computer-Supported Collaborative Work (AR-CSCW), by combining a systematic survey approach with an exploratory, opportunistic literature search. We categorize 65 papers along the dimensions of space, time, role symmetry (whether the roles of users are symmetric), technology symmetry (whether the hardware platforms of users are symmetric), and output and input modalities. We derive design considerations for collaborative AR environments, and identify under-explored research topics. These include the use of heterogeneous hardware considerations and 3D data exploration research areas. This survey is useful for newcomers to the field, readers interested in an overview of CSCW in AR applications, and domain experts seeking up-to-date information.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 01
PB  - 
DO  - 10.1109/tvcg.2020.3032761
ER  - 

TY  - CHAP
AU  - Höhner, Nils; Pfeiffer, Anke; Reverberi, Davide; Mints, Mark Oliver; Rodewald, Julien
TI  - Connecting Spatially Separated Laboratory Environments by Combining Virtual and Augmented Reality Technology
PY  - 2022
AB  - AbstractThis paper proposes a technical solution to connect an existing Virtual Reality laboratory to a real-world laboratory by combining Augmented Reality glasses (Microsoft HoloLens 2) and a Virtual Reality Headset (HTC Vive Pro) in a single networking application. Furthermore, it investigates educational opportunities and challenges of combining both a virtual and a real lab for higher engineering education.
SP  - 509
EP  - 520
JF  - Artificial Intelligence and Online Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-17091-1_51
ER  - 

TY  - NA
AU  - Lee, DoYoung; Lee, SooHwan; Oakley, Ian
TI  - CHI - Nailz: Sensing Hand Input with Touch Sensitive Nails
PY  - 2020
AB  - Touches between the fingers of an unencumbered hand represent a ready-to-use, eyes-free and expressive input space suitable for interacting with wearable devices such as smart glasses or watches. While prior work has focused on touches to the inner surface of the hand, touches to the nails, a practical site for mounting sensing hardware, have been comparatively overlooked. We extend prior implementations of single touch sensing nails to a full set of five and explore their potential for wearable input. We present design ideas and an input space of 144 touches (taps, flicks and swipes) derived from an ideation workshop. We complement this with data from two studies characterizing the subjective comfort and objective characteristics (task time, accuracy) of each touch. We conclude by synthesizing this material into a set of 29 viable nail touches, assessing their performance in a final study and illustrating how they could be used by presenting, and qualitatively evaluating, two example applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376778
ER  - 

TY  - NA
AU  - Liu, Jen-Shuo; Wang, Portia; Tversky, Barbara; Feiner, Steven
TI  - Adaptive Visual Cues for Guiding a Bimanual Unordered Task in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00059
ER  - 

TY  - CHAP
AU  - Watanabe, Kyosuke; Oka, Makoto; Mori, Hirohiko
TI  - Effects of Immediate Feedback in Operating Information Device by Finger Tap Gesture
PY  - 2022
AB  - NA
SP  - 166
EP  - 182
JF  - Human Interface and the Management of Information: Visual and Information Design
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06424-1_13
ER  - 

TY  - NA
AU  - Huang, Kevin; Li, Jiannan; Sousa, Mauricio; Grossman, Tovi
TI  - immersivePOV: Filming How-To Videos with a Head-Mounted 360° Action Camera
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517468
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Wu, Te-Yen; Huang, Da-Yuan; Hsiu, Min-Chieh; Hsiao, Jui-Chun; Hung, Yi-Ping; Chen, Mike Y.; Chen, Bing-Yu
TI  - CHI Extended Abstracts - SegTouch: Enhancing Touch Input While Providing Touch Gestures on Screens Using Thumb-To-Index-Finger Gestures
PY  - 2017
AB  - Insufficient input modality on touchscreens causes icons, toolbars and mode switching steps required to perform different functions. Although various methods are proposed to increase touchscreen input modality, touch gestures (e.g., swipe), usually used in touch input, are not provided in previous methods (e.g., Force Touch on iPhone 6s). This still restricts the input modality on touchscreens. Hence, we propose SegTouch to enhance touch input while providing touch gestures. SegTouch uses thumb-to-index-finger gestures, i.e., the thumb slides on the index finger, to define various touch purposes. Based on a pilot study, the middle and base segments on the index finger are suitable input areas for SegTouch. To observe how users leverage the proprioception and natural haptic feedback from index finger landmarks to perform SegTouch, different layouts on the index finger segments were examined in the eyes-free. Including the normal touch without thumb-to-index-finger gesture, SegTouch provides 9 input modality and touch gestures on the screen, so novel applications are enabled.
SP  - 2164
EP  - 2171
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053109
ER  - 

TY  - NA
AU  - Kulkarni, Chinmay
TI  - L@S - Design Perspectives of Learning at Scale: Scaling Efficiency and Empowerment
PY  - 2019
AB  - How do we design technology for learning at scale? Based on an examination of a large number of influential systems for learning at scale, I argue that designing for scale is not an amorphous design undertaking. Instead, it builds on two distinct perspectives on scale. Systems built with a scaling through efficiency perspective make learning more efficient and allow the same number of instructors to help a much larger set of learners. Systems with a scaling through empowerment perspective empower a larger number of people to assist learners effectively. I outline how these simple differences in design perspective lead to large differences in design concerns, techniques, and evaluation criteria. Articulating prevalent design perspectives should make overlooked design opportunities more salient, help systems designers design for scale more deliberately and understand their tradeoffs, and open up new opportunities to designers who shift their perspectives.
SP  - 18
EP  - NA
JF  - Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3330430.3333620
ER  - 

TY  - CONF
AU  - Du, Xiaoyu; Hargreaves, Christopher; Sheppard, John; Anda, Felix; Sayakkara, Asanka; Le-Khac, Nhien-An; Scanlon, Mark
TI  - ARES - SoK: exploring the state of the art and the future potential of artificial intelligence in digital forensic investigation
PY  - 2020
AB  - Multi-year digital forensic backlogs have become commonplace in law enforcement agencies throughout the globe. Digital forensic investigators are overloaded with the volume of cases requiring their expertise compounded by the volume of data to be processed. Artificial intelligence is often seen as the solution to many big data problems. This paper summarises existing artificial intelligence based tools and approaches in digital forensics. Automated evidence processing leveraging artificial intelligence based techniques shows great promise in expediting the digital forensic analysis process while increasing case processing capacities. For each application of artificial intelligence highlighted, a number of current challenges and future potential impact is discussed.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yi, Jooyong; Ahmed, Umair Z.; Karkare, Amey; Tan, Shin Hwei; Roychoudhury, Abhik
TI  - ESEC/SIGSOFT FSE - A feasibility study of using automated program repair for introductory programming assignments
PY  - 2017
AB  - Despite the fact an intelligent tutoring system for programming (ITSP) education has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying intelligent programming tutoring and APR. We perform our feasibility study with four state-of-the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while novice students do not seem to know how to effectively make use of generated repairs as hints, the graders do seem to gain benefits from repairs.
SP  - 740
EP  - 751
JF  - Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3106237.3106262
ER  - 

TY  - JOUR
AU  - Marques, Bernardo; Teixeira, Antônio Lúcio; Silva, Samuel; Alves, João; Dias, Paulo; Santos, Beatriz Sousa
TI  - A critical analysis on remote collaboration mediated by Augmented Reality: Making a case for improved characterization and evaluation of the collaborative process
PY  - 2022
AB  - NA
SP  - 619
EP  - 633
JF  - Computers & Graphics
VL  - 102
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.08.006
ER  - 

TY  - NA
AU  - Liang, Rong-Hao; Yang, Shun-Yao; Chen, Bing-Yu
TI  - InDexMo: exploring finger-worn RFID motion tracking for activity recognition on tagged objects
PY  - 2019
AB  - This work explores and evaluates the designs of finger-worn radio-frequency identification (RFID) motion tracking for activity recognition on tagged objects. We propose an index-finger-worn device that consists of a short-range (~2cm) RFID reader and a pair of two inertial measurement units (IMUs), which are mounted at the locations where an artificial nail and a ring are worn. The short-range RFID reader recognizes the tagged object on finger touch, and then the IMU data are used for activity recognition. Data collected from the user of this device allows for a post-hoc analysis, which informs the activity recognition performance in various RFID+IMU and IMU-only configurations on the same task. The results of a ten-participant user study show that when the objects have similar physical form factors, the hybrid RFID motion tracking significantly outperforms the IMU-only tracking, especially in a larger-number set of objects. In our test, three IMU configurations (i.e., NailOnly, RingOnly, and Nail+Ring) achieved comparable action recognition performances, i.e., ≥90% accuracy, with 500 ms recognition time, though the NailOnly RFID+IMU configuration provided the highest wearability. The practical challenges toward a real-world deployment of a finger-worn RFID motion tracking system are also discussed.
SP  - 129
EP  - 134
JF  - Proceedings of the 23rd International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341163.3347724
ER  - 

TY  - BOOK
AU  - Ladwig, Philipp; Dewitz, Bastian; Preu, Hendrik; Säger, Mitja
TI  - Mensch &amp; Computer - Remote Guidance for Machine Maintenance Supported by Physical LEDs and Virtual Reality
PY  - 2019
AB  - Machines that are used in industry often require dedicated technicians to fix them in case of defects. This involves travel expenses and certain amount of time, both of which may be significantly reduced by installing small extensions on a machine as we describe in this paper. The goal is that an authorized local worker, guided by a remote expert, can fix the problem on the real machine himself. Our approach is to equip a machine with multiple inexpensive LEDs (light-emitting diodes) and a simple micro controller, which is connected to the internet, to remotely light up certain LEDs, that are close to machine parts of interest. The blinking of an LED can be induced on a virtual 3D model (digital twin) of the machine by a remote expert using a virtual reality application to draw the local worker's attention to certain areas. We conducted an initial user study on this concept with 36 participants and found significantly shorter completion times and less errors for our approach compared to only voice guidance with no visual LED feedback.
SP  - 255
EP  - 262
JF  - Proceedings of Mensch und Computer 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340764.3340780
ER  - 

TY  - NA
AU  - Liang, Rong-Hao; Hsieh, Meng-Ju; Ke, Jheng-You; Guo, Jr-Ling; Chen, Bing-Yu
TI  - UIST - RFIMatch: Distributed Batteryless Near-Field Identification Using RFID-Tagged Magnet-Biased Reed Switches
PY  - 2018
AB  - This paper presents a technique enabling distributed batteryless near-field identification (ID) between two passive radio frequency ID (RFID) tags. Each conventional ultra-high-frequency (UHF) RFID tag is modified by connecting its antenna and chip to a reed switch and then attaching a magnet to one of the reed switch's terminals, thus transforming it into an always-on switch. When the two modules approach each other, the magnets counteract each other and turn off both switches at the same time. The coabsence of IDs thus indicates a unique interaction event. In addition to sensing, the module also provides native haptic feedback through magnetic repulsion force, enabling users to perceive the system's state eyes-free, without physical constraints. Additional visual feedback can be provided through an energy-harvesting module and a light emitting diode. This specific hardware design supports contactless, orientation-invariant sensing, with a form factor compact enough for embedded and wearable use in ubiquitous computing applications.
SP  - 473
EP  - 483
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242620
ER  - 

TY  - NA
AU  - Hessar, Mehrdad; Iyer, Vikram; Gollakota, Shyamnath
TI  - UbiComp - Enabling on-body transmissions with commodity devices
PY  - 2016
AB  - We show for the first time that commodity devices can be used to generate wireless data transmissions that are confined to the human body. Specifically, we show that commodity input devices such as fingerprint sensors and touchpads can be used to transmit information to only wireless receivers that are in contact with the body. We characterize the propagation of the resulting transmissions across the whole body and run experiments with ten subjects to demonstrate that our approach generalizes across different body types and postures. We also evaluate our communication system in the presence of interference from other wearable devices such as smartwatches and nearby metallic surfaces. Finally, by modulating the operations of these input devices, we demonstrate bit rates of up to 50 bits per second over the human body.
SP  - 1100
EP  - 1111
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971648.2971682
ER  - 

TY  - NA
AU  - Chen, Wei; Chazan, Jonah; Fuge, Mark
TI  - How Designs Differ: Non-Linear Embeddings Illuminate Intrinsic Design Complexity
PY  - 2016
AB  - This paper shows how to measure the complexity and reduce the dimensionality of a geometric design space. It assumes that high-dimensional design parameters actually lie in a much lower-dimensional space that represents semantic attributes. Past work has shown how to embed designs using techniques like autoencoders; in contrast, this paper quantifies when and how various embeddings are better than others. It captures the intrinsic dimensionality of a design space, the performance of recreating new designs for an embedding, and the preservation of topology of the original design space. We demonstrate this with both synthetic superformula shapes of varying non-linearity and real glassware designs. We evaluate multiple embeddings by measuring shape reconstruction error, topology preservation, and required semantic space dimensionality. Our work generates fundamental knowledge about the inherent complexity of a design space and how designs differ from one another. This deepens our understanding of design complexity in general.Copyright © 2016 by ASME
SP  - NA
EP  - NA
JF  - Volume 2A: 42nd Design Automation Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1115/detc2016-60112
ER  - 

TY  - NA
AU  - Oh, Seungjae; Yun, Gyeore; Park, Chaeyong; Kim, Jin-Soo; Choi, Seungmoon
TI  - CHI - VibEye: Vibration-Mediated Object Recognition for Tangible Interactive Applications
PY  - 2019
AB  - We present VibEye: a vibration-mediated recognition system of objects for tangible interaction. A user holds an object between two fingers wearing VibEye. VibEye triggers a vibration from one finger, and the vibration that has propagated through the object is sensed at the other finger. This vibration includes information about the object's identity, and we represent it using a spectrogram. Collecting the spectrograms of many objects, we formulate the object recognition problem to a classical classification problem among the images. This simple method, when tested with 20 users, shows 92.5% accuracy for 16 objects of the same shape with various materials. This material-based classifier is also extended to the recognition of everyday objects. Lastly, we demonstrate several tangible applications where VibEye provides the needed functionality while enhancing user experiences. VibEye is particularly effective for recognizing objects made of different materials, which is difficult to distinguish by other means such as light and sound.
SP  - 676
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300906
ER  - 

TY  - CHAP
AU  - Li, Xiangdong A.; Hansen, Preben; Lou, Xiaolong; Geng, Weidong; Peng, Ren
TI  - HCI (21) - Design and Evaluation of Cross-Objects User Interface for Whiteboard Interaction
PY  - 2017
AB  - Whiteboard has long been an important tool for education and communication, and nowadays it embraces display functions and other interactive features such as pen pointing and selecting of digital contents. Despite the enhanced interactivity, it is often time- and cost-consuming to implement specific apparatus for different whiteboard interactions. Therefore, we aimed at incorporating physical-world objects (e.g. Lego Rubik’s cubes) as the cross-objects user interface for multiple whiteboard interaction tasks without incurring heavy development work. The user interface utilised electromagnetic technique to extract electromechanical signals and recognised normal objects, thus extended the generality. To further understand effectiveness of the user interface, we implemented a low-fidelity prototype and conducted within-subject evaluation. The results showed the cross-objects user interface was natural, responsive, and easy of learning as the conventional whiteboard. Moreover, the user interface outperformed over the conventional one in the perspectives of configuration efficiency and versatility of multiple interaction tasks. Given these findings, practical implications for future tangible user interface design for whiteboard interactions are discussed.
SP  - 180
EP  - 191
JF  - Distributed, Ambient and Pervasive Interactions
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-58697-7_13
ER  - 

TY  - CHAP
AU  - Hoppe, Adrian Heinrich; Westerkamp, Kai; Maier, Sebastian; van de Camp, Florian; Stiefelhagen, Rainer
TI  - HCI (29) - Multi-user Collaboration on Complex Data in Virtual and Augmented Reality
PY  - 2018
AB  - With increasing task and system complexity, it becomes necessary to support workers, e.g. performing repair tasks, from a remote location. Current approaches utilize images or a video stream combined with annotations and speech to allow collaboration with remote users. We propose a technique that gives the remote supporter the ability to see a high fidelity point cloud of a real world object in Virtual Reality (VR). The VR user can indicate points of interest via a laser pointer. The local worker sees these indications on top of the real object with an Augmented Reality (AR) headset. A preliminary user study shows that the proposed method is faster and less error-prone regarding the comprehension of the object and the communication between the users. In addition to that, the system has a higher usability. This work shows that even non-virtual, collaborative tasks can be supported by new forms of user interaction using different technologies like VR and AR.
SP  - 258
EP  - 265
JF  - HCI International 2018 – Posters' Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-92279-9_35
ER  - 

TY  - JOUR
AU  - Zhang, Zhenning; Pan, Zhigeng; Li, Weiqing; Su, Zhiyong
TI  - Imitative Collaboration: A mirror-neuron inspired mixed reality collaboration method with remote hands and local replicas
PY  - 2022
AB  - NA
SP  - 103600
EP  - 103600
JF  - Journal of Visual Communication and Image Representation
VL  - 88
IS  - NA
PB  - 
DO  - 10.1016/j.jvcir.2022.103600
ER  - 

TY  - JOUR
AU  - Yamamoto, Kazuhiko; Igarashi, Takeo
TI  - Fully perceptual-based 3D spatial sound individualization with an adaptive variational autoencoder
PY  - 2017
AB  - To realize 3D spatial sound rendering with a two-channel headphone, one needs head-related transfer functions (HRTFs) tailored for a specific user. However, measurement of HRTFs requires a tedious and expensive procedure. To address this, we propose a fully perceptual-based HRTF fitting method for individual users using machine learning techniques. The user only needs to answer pairwise comparisons of test signals presented by the system during calibration. This reduces the efforts necessary for the user to obtain individualized HRTFs. Technically, we present a novel adaptive variational AutoEncoder with a convolutional neural network. In the training, this AutoEncoder analyzes publicly available HRTFs dataset and identifies factors that depend on the individuality of users in a nonlinear space. In calibration, the AutoEncoder generates high-quality HRTFs fitted to a specific user by blending the factors. We validate the feasibilities of our method through several quantitative experiments and a user study.
SP  - 212
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 6
PB  - 
DO  - 10.1145/3130800.3130838
ER  - 

TY  - NA
AU  - Zhang, Yang; Iravantchi, Yasha; Jin, Haojian; Kumar, Swarun; Harrison, Chris
TI  - UIST - Sozu: Self-Powered Radio Tags for Building-Scale Activity Sensing
PY  - 2019
AB  - Robust, wide-area sensing of human environments has been a long-standing research goal. We present Sozu, a new low-cost sensing system that can detect a wide range of events wirelessly, through walls and without line of sight, at whole-building scale. To achieve this in a battery-free manner, Sozu tags convert energy from activities that they sense into RF broadcasts, acting like miniature self-powered radio stations. We describe the results from a series of iterative studies, culminating in a deployment study with 30 instrumented objects. Results show that Sozu is very accurate, with true positive event detection exceeding 99%, with almost no false positives. Beyond event detection, we show that Sozu can be extended to detect richer signals, such as the state, intensity, count, and rate of events.
SP  - 973
EP  - 985
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347952
ER  - 

TY  - JOUR
AU  - Gunpinar, Erkan; Gunpinar, Serkan
TI  - A shape sampling technique via particle tracing for CAD models
PY  - 2018
AB  - NA
SP  - 11
EP  - 29
JF  - Graphical Models
VL  - 96
IS  - NA
PB  - 
DO  - 10.1016/j.gmod.2018.01.003
ER  - 

TY  - NA
AU  - Piumsomboon, Thammathip; Day, Arindam; Ens, Barrett; Lee, Youngho; Lee, Gun A.; Billinghurst, Mark
TI  - SIGGRAPH ASIA Mobile Graphics and Interactive Applications - Exploring enhancements for remote mixed reality collaboration
PY  - 2017
AB  - In this paper, we explore techniques for enhancing remote Mixed Reality (MR) collaboration in terms of communication and interaction. We created CoVAR, a MR system for remote collaboration between an Augmented Reality (AR) and Augmented Virtuality (AV) users. Awareness cues and AV-Snap-to-AR interface were proposed for enhancing communication. Collaborative natural interaction, and AV-User-Body-Scaling were implemented for enhancing interaction. We conducted an exploratory study examining the awareness cues and the collaborative gaze, and the results showed the benefits of the proposed techniques for enhancing communication and interaction.
SP  - 16
EP  - NA
JF  - SIGGRAPH Asia 2017 Mobile Graphics & Interactive Applications on - SA '17
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132787.3139200
ER  - 

TY  - NA
AU  - Laput, Gierad; Harrison, Chris
TI  - CHI - Sensing Fine-Grained Hand Activity with Smartwatches
PY  - 2019
AB  - Capturing fine-grained hand activity could make computational experiences more powerful and contextually aware. Indeed, philosopher Immanuel Kant argued, "the hand is the visible part of the brain." However, most prior work has focused on detecting whole-body activities, such as walking, running and bicycling. In this work, we explore the feasibility of sensing hand activities from commodity smartwatches, which are the most practical vehicle for achieving this vision. Our investigations started with a 50 participant, in-the-wild study, which captured hand activity labels over nearly 1000 worn hours. We then studied this data to scope our research goals and inform our technical approach. We conclude with a second, in-lab study that evaluates our classification stack, demonstrating 95.2% accuracy across 25 hand activities. Our work highlights an underutilized, yet highly complementary contextual channel that could unlock a wide range of promising applications.
SP  - 338
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300568
ER  - 

TY  - JOUR
AU  - Guo, Philip J.
TI  - Building tools to help students learn to program
PY  - 2017
AB  - <jats:p> The <jats:italic>Communications</jats:italic> Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of <jats:italic>Communications</jats:italic> , we'll publish selected posts or excerpts. </jats:p> <jats:p> <jats:bold>twitter</jats:bold> </jats:p> <jats:p>Follow us on Twitter at http://twitter.com/blogCACM</jats:p> <jats:p>http://cacm.acm.org/blogs/blog-cacm</jats:p> <jats:p>Philip Guo summarizes his first three years of research into building tools to support those learning computer programming.</jats:p>
SP  - 8
EP  - 9
JF  - Communications of the ACM
VL  - 60
IS  - 12
PB  - 
DO  - 10.1145/3148245
ER  - 

TY  - NA
AU  - Feick, Martin; Tang, Anthony; Bateman, Scott
TI  - UIST (Adjunct Volume) - Mixed-Reality for Object-Focused Remote Collaboration
PY  - 2018
AB  - In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators' perspectives on the object as well as understand one another's perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other's perspective; (2) explore objects independently from one another, and (3) render gestures in the remote's workspace. In this work, we focus on the expert's role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.
SP  - 63
EP  - 65
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266102
ER  - 

TY  - NA
AU  - Gong, Jun; Yang, Xin; Seyed, Teddy; Davis, Josh Urban; Yang, Xing-Dong
TI  - UIST - Indutivo: Contact-Based, Object-Driven Interactions with Inductive Sensing
PY  - 2018
AB  - We present Indutivo, a contact-based inductive sensing technique for contextual interactions. Our technique recognizes conductive objects (metallic primarily) that are commonly found in households and daily environments, as well as their individual movements when placed against the sensor. These movements include sliding, hinging, and rotation. We describe our sensing principle and how we designed the size, shape, and layout of our sensor coils to optimize sensitivity, sensing range, recognition and tracking accuracy. Through several studies, we also demonstrated the performance of our proposed sensing technique in environments with varying levels of noise and interference conditions. We conclude by presenting demo applications on a smartwatch, as well as insights and lessons we learned from our experience.
SP  - 321
EP  - 333
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242662
ER  - 

TY  - CHAP
AU  - Kim, Joy; Agrawala, Maneesh; Bernstein, Michael S.
TI  - Mosaic: Designing Online Creative Communities for Sharing Works-in-Progress
PY  - 2017
AB  - NA
SP  - 105
EP  - 129
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-60967-6_6
ER  - 

TY  - JOUR
AU  - Zhang, Xiangyu; Bai, Xiaoliang; Zhang, Shusheng; He, Weiping; Wang, Peng; Wang, Zhuo; Yan, Yuxiang; Yu, Quan
TI  - Real-time 3D video-based MR remote collaboration using gesture cues and virtual replicas
PY  - 2022
AB  - NA
SP  - 7697
EP  - 7719
JF  - The International Journal of Advanced Manufacturing Technology
VL  - 121
IS  - 11-12
PB  - 
DO  - 10.1007/s00170-022-09654-7
ER  - 

TY  - JOUR
AU  - Youn, Eunhye; Lee, Sangyoon; Kim, Sunbum; Shim, Youngbo Aram; Chan, Liwei; Lee, Geehyuk
TI  - WristDial: An Eyes-Free Integer-Value Input Method by Quantizing the Wrist Rotation
PY  - 2021
AB  - Smartwatches are a convenient substitute for smartphones in hand-busy situations, but there is limited interactivity to one-handed use of wrist-worn devices. In this paper, we propose WristDial, a ...
SP  - 1607
EP  - 1624
JF  - International Journal of Human–Computer Interaction
VL  - 37
IS  - 17
PB  - 
DO  - 10.1080/10447318.2021.1898848
ER  - 

TY  - NA
AU  - Henley, Austin Z.; Ball, Julian; Klein, Benjamin; Rutter, Aiden; Lee, Dylan
TI  - ICSE (SEET) - An Inquisitive Code Editor for Addressing Novice Programmers' Misconceptions of Program Behavior
PY  - 2021
AB  - Novice programmers face numerous barriers while attempting to learn how to code that may deter them from pursuing a computer science degree or career in software development. In this work, we propose a tool concept to address the particularly challenging barrier of novice programmers holding misconceptions about how their code behaves. Specifically, the concept involves an inquisitive code editor that: (1) identifies misconceptions by periodically prompting the novice programmer with questions about their program's behavior, (2) corrects the misconceptions by generating explanations based on the program's actual behavior, and (3) prevents further misconceptions by inserting test code and utilizing other educational resources. We have implemented portions of the concept as plugins for the Atom code editor and conducted informal surveys with students and instructors. Next steps include deploying the tool prototype to students enrolled in introductory programming courses.
SP  - 165
EP  - 170
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse-seet52601.2021.00026
ER  - 

TY  - JOUR
AU  - Shrestha, Prakash; Saxena, Nitesh
TI  - An Offensive and Defensive Exposition of Wearable Computing
PY  - 2017
AB  - Wearable computing is rapidly getting deployed in many—commercial, medical, and personal—domains of day-to-day life. Wearable devices appear in various forms, shapes, and sizes and facilitate a wide variety of applications in many domains of life. However, wearables raise unique security and privacy concerns. Wearables also hold the promise to help enhance the existing security, privacy, and safety paradigms in unique ways while preserving the system’s usability. The contribution of this research literature survey is threefold. First, as a background, we identify a wide range of existing as well as upcoming wearable devices and investigate their broad applications. Second, we provide an exposition of the security and privacy of wearable computing, studying dual aspects, that is, both attacks and defenses. Third, we provide a comprehensive study of the potential security, privacy, and safety enhancements to existing systems based on the emergence of wearable technology. Although several research works have emerged exploring different offensive and defensive uses of wearables, there is a lack of a broad and precise literature review systematizing all those security and privacy aspects and the underlying threat models. This research survey also analyzes current and emerging research trends and provides directions for future research.
SP  - 92
EP  - 39
JF  - ACM Computing Surveys
VL  - 50
IS  - 6
PB  - 
DO  - 10.1145/3133837
ER  - 

TY  - JOUR
AU  - Garg, Kapil; Gergle, Darren; Zhang, Haoqi
TI  - Understanding the Practices and Challenges of Networked Orchestration in Research Communities of Practice
PY  - 2022
AB  - <jats:p>Work and learning communities have become increasingly networked to support their members in developing the skills to solve complex, real-world problems. Though disciplinary knowledge remains important to tackle these problems, working effectively in these modern-day communities of practice demands the ability for one to learn how to access networked support (e.g., venues, tools, resource guides, or peers) throughout the community for one's needs. Against this backdrop, we study networked orchestration--how community members access and learn to access networked supports--in a community of practice for undergraduate research training. Through field observations and in-depth interviews, we find that students in the networked research community dynamically engage with their mentors and peers across multiple venues throughout the week in order to identify, clarify, and resolve their needs. Mentors in the community monitor how students are engaging with the supports available in the network, and provide coaching on effective strategies when students are ineffective on their own. Finally, we surface the challenges involved in each of these processes and offer practical insights for future ecosystem-level networked orchestration technologies that have an understanding of the interactions occurring across the venues and tools in a community, and can support the learning and practice of effective access strategies. Our paper presents important insights for supporting people's work and learning needs in networked future workplaces and learning communities, and provides guidance on designing new technologies for supporting networked ways of working and learning.</jats:p>
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555764
ER  - 

TY  - NA
AU  - Li, Hanchuan
TI  - UIST (Adjunct Volume) - Enabling Sensing and Interaction with Everyday Objects
PY  - 2017
AB  - The Internet of Things (IoT) promises to revolutionize the way people interact with their environment and the objects within it by creating a ubiquitous network of physical devices. However, current advancement in IoT has been heavily targeted towards creating battery-powered electronics. There remains a huge gap between the collection of smart devices integrated into the IoT and the massive number of everyday physical objects. The goal of my research is to bridge this gap in the current IoT framework to enable novel sensing and interactive applications with daily objects.
SP  - 103
EP  - 106
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131843
ER  - 

TY  - JOUR
AU  - Werner, Frank T.; Yilmaz, Baki Berkay; Prvulovic, Milos; Zajic, Alenka
TI  - Leveraging EM Side-Channels for Recognizing Components on a Motherboard
PY  - 2021
AB  - This article proposes leveraging EM side-channels to recognize/authenticate electronic components integrated onto a motherboard. By focusing on components on a motherboard, our method provides an opportunity to authenticate devices assembled by third parties. This method identifies components based on the modulated signals emanated while they are excited in a controlled manner. When testing an unknown component, the spectrum is compared to previously recorded training signatures. To improve efficiency, the size of the spectrum is reduced by projecting it into a vector space generated from training signatures. The identity of the tested component is then determined using a k-nearest neighbors algorithm. This method successfully classified memory, processor, and Ethernet transceiver components integrated on seven types of Internet-of-Things devices. Since manufacturers commonly use the same components in multiple designs, cross-type testing of motherboards is conducted. Collecting the training signatures on one motherboard and testing components from different motherboards speeds up the process and decreases the cost. Using measurements taken while exciting the components for 1 s, our method achieves a classification accuracy greater than 96 $\%$ across all components tested. These results demonstrate that this method can recognize components based on their emanations, even if the components are integrated onto completely different motherboards.
SP  - 502
EP  - 515
JF  - IEEE Transactions on Electromagnetic Compatibility
VL  - 63
IS  - 2
PB  - 
DO  - 10.1109/temc.2020.3016892
ER  - 

TY  - NA
AU  - Sukan, Mengu; Elvezio, Carmine; Feiner, Steven; Tversky, Barbara
TI  - SUI - Providing Assistance for Orienting 3D Objects Using Monocular Eyewear
PY  - 2016
AB  - Many tasks require that a user rotate an object to match a specific orientation in an external coordinate system. This includes tasks in which one object must be oriented relative to a second prior to assembly and tasks in which objects must be held in specific ways to inspect them. Research has investigated guidance mechanisms for some 6DOF tasks, using wide--field-of-view, stereoscopic virtual and augmented reality head-worn displays (HWDs). However, there has been relatively little work directed toward smaller field-of-view lightweight monoscopic HWDs, such as Google Glass, which may remain more comfortable and less intrusive than stereoscopic HWDs in the near future. We have designed and implemented a novel visualization approach and three additional visualizations representing different paradigms for guiding unconstrained manual 3DOF rotation, targeting these monoscopic HWDs. We describe our exploration of these paradigms and present the results of a user study evaluating the relative performance of the visualizations and showing the advantages of our new approach.
SP  - 89
EP  - 98
JF  - Proceedings of the 2016 Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2983310.2985764
ER  - 

TY  - NA
AU  - Huo, Ke; Wang, Tianyi; Paredes, Luis; Villanueva, Ana M.; Cao, Yuanzhi; Ramani, Karthik
TI  - UIST - SynchronizAR: Instant Synchronization for Spontaneous and Spatial Collaborations in Augmented Reality
PY  - 2018
AB  - We present SynchronizAR, an approach to spatially register multiple SLAM devices together without sharing maps or involving external tracking infrastructures. SynchronizAR employs a distance based indirect registration which resolves the transformations between the separate SLAM coordinate systems. We attach an Ultra-Wide Bandwidth~(UWB) based distance measurements module on each of the mobile AR devices which is capable of self-localization with respect to the environment. As users move on independent paths, we collect the positions of the AR devices in their local frames and the corresponding distance measurements. Based on the registration, we support to create a spontaneous collaborative AR environment to spatially coordinate users' interactions. We run both technical evaluation and user studies to investigate the registration accuracy and the usability towards spatial collaborations. Finally, we demonstrate various collaborative AR experience using SynchronizAR.
SP  - 19
EP  - 30
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242595
ER  - 

TY  - JOUR
AU  - Zhang, Yufei; Liu, Bin; Liu, Zhiqiang
TI  - Recognizing Hand Gestures With Pressure-Sensor-Based Motion Sensing
PY  - 2019
AB  - This paper proposes a novel framework to process pressure signals for real-time and robust gesture recognition, which includes an innovative segmentation scheme, a gesture recognition scheme and a pressure-parameter adaptive updating strategy. A prototype system, including a wearable gesture sensing device with four pressure sensors and the corresponding algorithmic framework, is developed to realize real-time gesture-based interaction. With the device worn on the wrist, the user can interact with the computer using 8 predefined gestures. Experimental results show that the delay of gesture recognition is about 100 ms, with the average accuracy of 95.28% in the experienced-user test and 86.20% in the inexperienced-user test. Finally, the system is evaluated by a mouse-controlling interaction task and performs well. Both experienced and inexperienced people can easily and quickly complete interactive tasks. These results demonstrate that a pressure-sensor based wristband can be used to classify hand gestures well and to control the mouse interaction. This approach provides an interactive way to replace the mouse for decreasing the risk of the carpal tunnel syndrome (CTS).
SP  - 1425
EP  - 1436
JF  - IEEE transactions on biomedical circuits and systems
VL  - 13
IS  - 6
PB  - 
DO  - 10.1109/tbcas.2019.2940030
ER  - 

TY  - JOUR
AU  - Koyama, Yuki; Igarashi, Takeo
TI  - Oxford Scholarship Online - Computational Design with Crowds
PY  - 2018
AB  - Computational design is aimed at supporting or automating design processes using computational techniques. However, some classes of design tasks involve criteria that are difficult to handle only with computers. For example, visual design tasks seeking to fulfill aesthetic goals are difficult to handle purely with computers. One promising approach is to leverage human computation; that is, to incorporate human input into the computation process. Crowdsourcing platforms provide a convenient way to integrate such human computation into a working system. In this chapter, we discuss such computational design with crowds in the domain of parameter tweaking tasks in visual design. Parameter tweaking is often performed to maximize the aesthetic quality of designed objects. Computational design powered by crowds can solve this maximization problem by leveraging human computation. We discuss the opportunities and challenges of computational design with crowds with two illustrative examples: (1) estimating the objective function (specifically, preference learning from crowds' pairwise comparisons) to facilitate interactive design exploration by a designer and (2) directly searching for the optimal parameter setting that maximizes the objective function (specifically, crowds-in-the-loop Bayesian optimization).
SP  - NA
EP  - NA
JF  - Oxford Scholarship Online
VL  - NA
IS  - NA
PB  - 
DO  - 10.1093/oso/9780198799603.003.0007
ER  - 

TY  - CHAP
AU  - Ito, Hiroki; Shimakawa, Hiromitsu; Harada, Fumiko
TI  - Estimation of Programming Understanding by Time Series Analysis of Code Puzzles
PY  - 2022
AB  - AbstractIn programming education, it is desirable for instructors to stand beside learners and monitor their answering process to assess the individual’s actual ability. However, it seems to be impossible in large-group lectures at educational institutions or newcomer education at companies. Therefore, instructors attempt to grasp the understanding status of many learners at once by using written tests and e-learning to find out the learners who need instruction. They examine the learner’s knowledge such as algorithm and syntax. However, in reality, not a few learners fail to acquire the skill of writing source codes. This kind of situation implies that the programming ability of learners cannot be measured only by knowledge tests or the data obtained from answer results. The purpose of this study is to estimate the understanding of programming, focusing on the thinking process. This paper analyzes a time series of operations of learners working on code puzzles, where they arrange code fragments. Since we assumed learners with low understanding are different from those with high in terms of the consistency of blocks of code fragments to be touched, we modeled it using a hidden Markov model. The proposed method estimates their perspectives on how fragments are built up to achieve given requirements. The results of an experiment have shown that the calculated hidden Markov model produces meaningful interpretable values. Furthermore, the values show significant indices that machine learning models can explain the understanding of learners.KeywordsProgramming educationLearning analyticsComputational thinkingProcess orientedCode puzzleTime seriesRemote tutoring
SP  - 485
EP  - 498
JF  - Proceedings of Seventh International Congress on Information and Communication Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-19-2394-4_44
ER  - 

TY  - CHAP
AU  - Casas, Sergio; Portalés, Cristina; García-Pereira, Inma; Gimeno, Jesús
TI  - Mixing Different Realities in a Single Shared Space: Analysis of Mixed-Platform Collaborative Shared Spaces
PY  - 2019
AB  - <jats:p>The concept of internet of everything involves an intelligent connection of people, processes, data, and things. In this sense, shared spaces aimed to connect different users that collaborate following a common purpose are of relevance to the field. Many computer-based collaborative environments have been proposed in recent years. However, the design of mixed-platform collaborative spaces, in which different paradigms—such as augmented reality (AR) and virtual reality (VR)—are blended, is still uncommon. This chapter aims to analyze the benefits and features of these systems, reviewing existing related works and proposing a series of features for the design of effective mixed-platform collaborative shared spaces. In particular, the authors propose five setups with different levels of immersion/interaction, which are aligned to the current state of the art. These systems will be analyzed with respect to navigation, user representation, interaction, and annotation, among others. Finally, some applications are proposed within the given framework. </jats:p>
SP  - 175
EP  - 192
JF  - Harnessing the Internet of Everything (IoE) for Accelerated Innovation Opportunities
VL  - NA
IS  - NA
PB  - 
DO  - 10.4018/978-1-5225-7332-6.ch008
ER  - 

TY  - NA
AU  - Tomlinson, William J.; Banou, Stella; Yu, Christopher C.; Nogueira, Michele; Chowdhury, Kaushik R.
TI  - INFOCOM - Secure On-skin Biometric Signal Transmission using Galvanic Coupling
PY  - 2019
AB  - Increasing threats of malicious eavesdropping raise concerns in confidential data reporting by body-worn sensors. We propose a secure, body-guided transmission channel through the use of galvanic coupling (GC). This method involves injecting weak electrical current into the body, which propagates primarily through the skin. The proposed approach makes the transmission of biometric data impervious to sniffing attacks, enabling the body to serve as a waveguide. This paper makes the following contributions: (i) An analytical channel model using a tissue equivalent circuit of the human arm-wrist-palm GC-propagation path is formulated and empirically verified. (ii) A simulation study is conducted for a comparative analysis of various modulation schemes, leveraging the validated GC-channel behavior. (iii) A GC-transceiver with optimized communication parameters (modulation, frequency, power) is designed and implemented using a dielectrically equivalent tissue phantom, and (iv) through experimental trials, resilience to over-the-air susceptibility (i.e., likelihood of adversarial eavesdropping) of the GC-signal and similar body communication techniques are demonstrated. Performance results of the GC-transceiver prototype yield a bit error rate of $10^{-6}$ with a transmit power of -2dBm, in addition to over 7x reduction of signal radiation outside the body compared to capacitive coupling.
SP  - 1135
EP  - 1143
JF  - IEEE INFOCOM 2019 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom.2019.8737540
ER  - 

TY  - NA
AU  - Chen, Lei; Liu, Yilin; Li, Yue; Yu, Lingyun; Gao, BoYu; Caon, Maurizio; Yue, Yong; Liang, Hai-Ning
TI  - SUI - Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration
PY  - 2021
AB  - Visual cues are essential in computer-mediated communication. It is especially important when communication happens in a collaboration scenario that requires focusing several users’ attention on a specific object among other similar ones. This paper explores the effect of visual cues on pointing tasks in co-located Augmented Reality (AR) collaboration. A user study (N = 32, 16 pairs) was conducted to compare two types of visual cues: Pointing Line (PL) and Moving Track (MT). Both are head-based visual techniques. Through a series of collaborative pointing tasks on objects with different states (static and dynamic) and density levels (low, medium and high), the results showed that PL was better on task performance and usability, but MT was rated higher on social presence and user preference. Based on our results, some design implications are provided for pointing tasks in co-located AR collaboration.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485297
ER  - 

TY  - NA
AU  - Chidambaram, Subramanian; Huang, Hank; He, Fengming; Qian, Xun; Villanueva, Ana M.; Redick, Thomas S.; Stuerzlinger, Wolfgang; Ramani, Karthik
TI  - Conference on Designing Interactive Systems - ProcessAR: An augmented reality-based tool to create in-situ procedural 2D/3D AR Instructions
PY  - 2021
AB  - Augmented reality (AR) is an efficient form of delivering spatial information and has great potential for training workers. However, AR is still not widely used for such scenarios due to the technical skills and expertise required to create interactive AR content. We developed ProcessAR, an AR-based system to develop 2D/3D content that captures subject matter expert’s (SMEs) environment-object interactions in situ. The design space for ProcessAR was identified from formative interviews with AR programming experts and SMEs, alongside a comparative design study with SMEs and novice users. To enable smooth workflows, ProcessAR locates and identifies different tools/objects through computer vision within the workspace when the author looks at them. We explored additional features such as embedding 2D videos with detected objects and user-adaptive triggers. A final user evaluation comparing ProcessAR and a baseline AR authoring environment showed that, according to our qualitative questionnaire, users preferred ProcessAR.
SP  - 234
EP  - 249
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462126
ER  - 

TY  - JOUR
AU  - Schlosser, Paul; Matthews, Ben; Sanderson, Penelope
TI  - Head-worn displays for healthcare and industry workers: a review of applications and design
PY  - 2021
AB  - NA
SP  - 102628
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 154
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2021.102628
ER  - 

TY  - NA
AU  - Martin-Gomez, Alejandro; Eck, Ulrich; Navab, Nassir
TI  - VR - Visualization Techniques for Precise Alignment in VR: A Comparative Study
PY  - 2019
AB  - Many studies explored the effectiveness of augmented, virtual, and mixed reality for object placement tasks. Two main approaches for assisting users during object alignment exist: static visualization techniques and interactive guides. This paper presents a comparative evaluation of four static visualization techniques used to render virtual objects when precise alignment in 6 degrees of freedom (DoF) is required. The selection of these techniques is based on the amount of occlusion caused by the visual guides during the alignment task. To the best of our knowledge, no previous work exists that evaluates which visualization technique is most suitable to support users while precisely aligning objects in virtual environments. We designed a virtual reality scenario considering two conditions -with and without time constraints- in which users aligned pairs of objects. To evaluate the users performance, quantitative and qualitative scores were collected. Our results suggest that visualization techniques with low levels of occlusion can improve alignment performance and increase user acceptance.
SP  - 735
EP  - 741
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798135
ER  - 

TY  - JOUR
AU  - Al-Naji, Fatimah Hussain; Zagrouba, Rachid
TI  - A survey on continuous authentication methods in Internet of Things environment
PY  - 2020
AB  - NA
SP  - 109
EP  - 133
JF  - Computer Communications
VL  - 163
IS  - NA
PB  - 
DO  - 10.1016/j.comcom.2020.09.006
ER  - 

TY  - JOUR
AU  - Bergström, Joanna; Hornbæk, Kasper
TI  - Human--Computer Interaction on the Skin
PY  - 2019
AB  - The skin offers exciting possibilities for human--computer interaction by enabling new types of input and feedback. We survey 42 research papers on interfaces that allow users to give input on their skin. Skin-based interfaces have developed rapidly over the past 8 years but most work consists of individual prototypes, with limited overview of possibilities or identification of research directions. The purpose of this article is to synthesize what skin input is, which technologies can sense input on the skin, and how to give feedback to the user. We discuss challenges for research in each of these areas.
SP  - 77
EP  - 14
JF  - ACM Computing Surveys
VL  - 52
IS  - 4
PB  - 
DO  - 10.1145/3332166
ER  - 

TY  - NA
AU  - Takahashi, Ryo; Fukumoto, Masaaki; Han, Changyo; Sasatani, Takuya; Narusue, Yoshiaki; Kawahara, Yoshihiro
TI  - UIST - TelemetRing: A Batteryless and Wireless Ring-shaped Keyboard using Passive Inductive Telemetry
PY  - 2020
AB  - TelemetRing is a batteryless and wireless ring-shaped keyboard that supports command and text entry in daily lives by detecting finger typing on various surfaces. The proposed inductive telemetry approach eliminates bulky batteries or capacitors from the ring part. Each ring consists of a sensor coil (the ring part itself), 1-DoF piezoelectric accelerometer, and varactor diode; moreover, it has different resonant frequencies. Typing shocks slightly shift the resonant frequency, and these are detected by a wrist-mounted readout coil. 5-bit chord keyboard is realized by attaching five sensor rings on five fingers. Our evaluation shows that the prototype achieved the tiny (6 g, 3.5 cm^3) ring sensor and 89.7% of typing detection ratio.
SP  - 1161
EP  - 1168
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415873
ER  - 

TY  - NA
AU  - Zillner, Jakob; Mendez, Erick; Wagner, Daniel
TI  - ISMAR Adjunct - Augmented Reality Remote Collaboration with Dense Reconstruction
PY  - 2018
AB  - This paper describes an Augmented Reality remote collaboration system leveraging high-fidelity, dense scene reconstruction for intuitive and precise remote guidance. A local worker in need of help can use our system to automatically generate a 3D mesh of the surrounding and stream it to a remote expert. The remote expert can navigate and explore the reconstructed environment independently of the local worker in six degrees of freedom. World-stabilized text- and image-annotations can be placed in the scene and strokes drawn on surfaces are intelligently positioned in the world. In addition, the reconstruction allows the remote expert to segment colored objects from the mesh and use the resulting 3D model to create simple animations in order to convey precise instructions.
SP  - 38
EP  - 39
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2018.00028
ER  - 

TY  - NA
AU  - Streli, Paul; Holz, Christian
TI  - CHI - CapContact: Super-resolution Contact Areas from Capacitive Touchscreens
PY  - 2021
AB  - Touch input is dominantly detected using mutual-capacitance sensing, which measures the proximity of close-by objects that change the electric field between the sensor lines. The exponential drop-off in intensities with growing distance enables software to detect touch events, but does not reveal true contact areas. In this paper, we introduce CapContact, a novel method to precisely infer the contact area between the user’s finger and the surface from a single capacitive image. At 8 × super-resolution, our convolutional neural network generates refined touch masks from 16-bit capacitive images as input, which can even discriminate adjacent touches that are not distinguishable with existing methods. We trained and evaluated our method using supervised learning on data from 10 participants who performed touch gestures. Our capture apparatus integrates optical touch sensing to obtain ground-truth contact through high-resolution frustrated total internal reflection. We compare our method with a baseline using bicubic upsampling as well as the ground truth from FTIR images. We separately evaluate our method’s performance in discriminating adjacent touches. CapContact successfully separated closely adjacent touch contacts in 494 of 570 cases (87%) compared to the baseline’s 43 of 570 cases (8%). Importantly, we demonstrate that our method accurately performs even at half of the sensing resolution at twice the grid-line pitch across the same surface area, challenging the current industry-wide standard of a ∼ 4 mm sensing pitch. We conclude this paper with implications for capacitive touch sensing in general and for touch-input accuracy in particular.
SP  - 289
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445621
ER  - 

TY  - NA
AU  - Kim, Chang-Min; Nam, Tek-Jin
TI  - Exploration on Everyday Objects as an IoT Control Interface
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533472
ER  - 

TY  - NA
AU  - Gong, Jun; Anderson, Fraser; Fitzmaurice, George; Grossman, Tovi
TI  - CHI - Instrumenting and Analyzing Fabrication Activities, Users, and Expertise
PY  - 2019
AB  - The recent proliferation of fabrication and making activities has introduced a large number of users to a variety of tools and equipment. Monitored, reactive and adaptive fabrication spaces are needed to provide personalized information, feedback and assistance to users. This paper explores the sensorization of making and fabrication activities, where the environment, tools, and users were considered to be separate entities that could be instrumented for data collection. From this exploration, we present the design of a modular system that can capture data from the varied sensors and infer contextual information. Using this system, we collected data from fourteen participants with varying levels of expertise as they performed seven representative making tasks. From the collected data, we predict which activities are being performed, which users are performing the activities, and what expertise the users have. We present several use cases of this contextual information for future interactive fabrication spaces.
SP  - 324
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300554
ER  - 

TY  - NA
AU  - Buddhika, Thisum; Zhang, Haimo; Weerasinghe, Chamod; Nanayakkara, Suranga; Zimmermann, Roger
TI  - AH - OSense: Object-activity Identification Based on Gasping Posture and Motion
PY  - 2019
AB  - Observing that, how we grasp objects is highly correlated with geometric shapes and interactions, we propose the use of hand postures and motions as an indirect source of inputs for object-activity recognition. This paradigm treats the human hand as an always-available sensor, and transforms all sensing problems to the data analysis for the "sensor hand". We envision this paradigm to be generalizable for all objects regardless of whether they are acoustically or electromagnetically active, and that it detects different motions while holding the same object. Our proof-of-concept setup consists of six IMU sensors mounted on the fingers and back of the hand. Our experiments show that when the posture is combined with the motion, the personalized object-activity detection accuracy increases from 80% to 87%.
SP  - 13
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311841
ER  - 

TY  - NA
AU  - Shi, Lei; Ashoori, Maryam; Zhang, Yunfeng; Azenkot, Shiri
TI  - MobileHCI - Knock knock, what's there: converting passive objects into customizable smart controllers
PY  - 2018
AB  - Knocking is a way of interacting with everyday objects. We introduce BeatIt, a novel technique that allows users to use passive, everyday objects to control a smart environment by recognizing the sounds generated from knocking on the objects. BeatIt uses a BeatSet, a series of percussive sound samples, to represent the sound signature of knocking on an object. A user associates a BeatSet with an event. For example, a user can associate the BeatSet of knocking on a door with the event of turning on the lights. Decoder, a signal-processing module, classifies the sound signals into one of the recorded BeatSets, and then triggers the associated event. Unlike prior work, BeatIt can be implemented on microphone-enabled commodity devices. Our user studies with 12 participants showed that our proof-of-concept implementation based on a smartwatch could accurately classify eight BeatSets using a user-independent classifier.
SP  - 31
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229453
ER  - 

TY  - NA
AU  - Rantala, Inka; Colley, Ashley; Häkkilä, Jonna
TI  - PerDis - Smart Jewelry: Augmenting Traditional Wearable Self-Expression Displays
PY  - 2018
AB  - In this paper, we address smart jewelry, with particular focus on the enhancement of traditional jewelry. We chart people's perceptions on smart jewelry through focus group and online survey user studies. We then present and evaluate a necklace design including a prototype mobile augmented reality application. Our salient findings show that the technical concept and visual design should not conflict with the physical form and aesthetic of the jewelry, and that the personal nature and emotional bonding with the jewelry should be reflected also in the digital extensions. Our work progresses the understanding of people's attitudes and preferences towards smart jewelry, and extends the body of research on augmented wearables, which is under-explored but has a great potential for the future.
SP  - 22
EP  - NA
JF  - Proceedings of the 7th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3205873.3205891
ER  - 

TY  - JOUR
AU  - Zhang, Cheng; Xue, Qiuyue; Waghmare, Anandghan; Jain, Sumeet; Pu, Yiming; Hersek, Sinan; Lyons, Kent; Cunefare, Kenneth A.; Inan, Omer T.; Abowd, Gregory D.
TI  - SoundTrak: Continuous 3D Tracking of a Finger Using Active Acoustics
PY  - 2017
AB  - The small size of wearable devices limits the efficiency and scope of possible user interactions, as inputs are typically constrained to two dimensions: the touchscreen surface. We present SoundTrak, an active acoustic sensing technique that enables a user to interact with wearable devices in the surrounding 3D space by continuously tracking the finger position with high resolution. The user wears a ring with an embedded miniature speaker sending an acoustic signal at a specific frequency (e.g., 11 kHz), which is captured by an array of miniature, inexpensive microphones on the target wearable device. A novel algorithm is designed to localize the finger’s position in 3D space by extracting phase information from the received acoustic signals. We evaluated SoundTrak in a volume of space (20cm × 16cm × 11cm) around a smartwatch, and show an average accuracy of 1.3 cm. We report on results from a Fitts’ Law experiment with 10 participants as the evaluation of the real-time prototype. We also present a set of applications which are supported by this 3D input technique, and show the practical challenges that need to be addressed before widespread use.
SP  - 30
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 2
PB  - 
DO  - 10.1145/3090095
ER  - 

TY  - NA
AU  - Huh, Seungyeon; Muralidharan, Shapna; Ko, Heedong; Yoo, Byounghyun
TI  - Web3D - XR Collaboration Architecture based on Decentralized Web
PY  - 2019
AB  - The web has been an extremely effective collaboration platform, enabling services like Wikipedia article co-authoring, blogging, social messaging, video conferencing, and many others. However, the collaboration should ideally occur Peer to Peer (P2P) among the participants instead of going through a centralized server as in the current centralized web, which acts as a mediator as well as a repository of data, especially for face-to-face collaboration in 3D XR context. Most notable XR applications like MMORPG have been developed in a dedicated application platform with their own centralized game servers. Nowadays, the decentralized web is being promoted as the next web architecture in numerous fronts such as blockchain in cryptocurrency, reviving P2P storage, and networking technologies as the next web, Web 3.0. It would be beneficial if we could make an XR collaboration framework based on the recent developments of the decentralized web. This paper explores one possible amalgamation of the decentralized web technology stack toward a webized XR collaboration framework.
SP  - 1
EP  - 9
JF  - The 24th International Conference on 3D Web Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3329714.3338137
ER  - 

TY  - JOUR
AU  - Sayakkara, Asanka; Le-Khac, Nhien-An; Scanlon, Mark
TI  - A Survey of Electromagnetic Side-Channel Attacks and Discussion on their Case-Progressing Potential for Digital Forensics
PY  - 2019
AB  - The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices produced by various vendors often posses limited standard interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require, and must result in, no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed - selected on the basis of their applicability in IoT device investigation scenarios. The insight gained from the background study is used to identify promising future applications of the technique for digital forensic analysis on IoT devices - potentially progressing a wide variety of currently hindered digital investigations.
SP  - 43
EP  - 54
JF  - Digital Investigation
VL  - 29
IS  - 1
PB  - 
DO  - 10.1016/j.diin.2019.03.002
ER  - 

TY  - NA
AU  - Huang, Haibin
TI  - DEEP-LEARNED GENERATIVE REPRESENTATIONS OF 3D SHAPE FAMILIES
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.7275/10677221.0
ER  - 

TY  - CHAP
AU  - Johnson, Emmanuel; Lucas, Gale M.; Kim, Peter H.; Gratch, Jonathan
TI  - AIED (2) - Intelligent Tutoring System for Negotiation Skills Training.
PY  - 2019
AB  - Intelligent tutoring systems have proven very effective at teaching hard skills such as math and science, but less research has examined how to teach “soft” skills such as negotiation. In this paper, we introduce an effective approach to teaching negotiation tactics. Prior work showed that students can improve through practice with intelligent negotiation agents. We extend this work by proposing general methods of assessment and feedback that could be applied to a variety of such agents. We evaluate these techniques through a human subject study. Our study demonstrates that personalized feedback improves students’ use of several foundational tactics.
SP  - 122
EP  - 127
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-23207-8_23
ER  - 

TY  - NA
AU  - Huang, Da-Yuan; Chan, Liwei; Yang, Shuo; Wang, Fan; Liang, Rong-Hao; Yang, De-Nian; Hung, Yi-Ping; Chen, Bing-Yu
TI  - CHI - DigitSpace: Designing Thumb-to-Fingers Touch Interfaces for One-Handed and Eyes-Free Interactions
PY  - 2016
AB  - Thumb-to-fingers interfaces augment touch widgets on fingers, which are manipulated by the thumb. Such interfaces are ideal for one-handed eyes-free input since touch widgets on the fingers enable easy access by the stylus thumb. This study presents DigitSpace, a thumb-to-fingers interface that addresses two ergonomic factors: hand anatomy and touch precision. Hand anatomy restricts possible movements of a thumb, which further influences the physical comfort during the interactions. Touch precision is a human factor that determines how precisely users can manipulate touch widgets set on fingers, which determines effective layouts of the widgets. Buttons and touchpads were considered in our studies to enable discrete and continuous input in an eyes-free manner. The first study explores the regions of fingers where the interactions can be comfortably performed. According to the comfort regions, the second and third studies explore effective layouts for button and touchpad widgets. The experimental results indicate that participants could discriminate at least 16 buttons on their fingers. For touchpad, participants were asked to perform unistrokes. Our results revealed that since individual participant performed a coherent writing behavior, personalized $1 recognizers could offer 92% accuracy on a cross-finger touchpad. A series of design guidelines are proposed for designers, and a DigitSpace prototype that uses magnetic-tracking methods is demonstrated.
SP  - 1526
EP  - 1537
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858483
ER  - 

TY  - JOUR
AU  - Kharrufa, Ahmed; Ploetz, Thomas; Olivier, Patrick
TI  - A Unified Model for User Identification on Multi-Touch Surfaces: A Survey and Meta-Analysis
PY  - 2017
AB  - User identification on interactive surfaces is a desirable feature that is not inherently supported by existing technologies. We have conducted an extensive survey of existing identification techniques, which led us to formulate a unified model for user identification. We start by introducing this model that (1) classifies existing user identification approaches in five categories according to the identification technology, (2) identifies eight characteristic identification system parameters, and (3) proposes a way for visualizing the system's characteristics as points on a radar chart to allow for quick comparison and contrast between systems. This model is then used to present our survey of existing user identification approaches and visualize their characteristics, highlighting their strengths and limitations. The model also makes it possible to visually represent requirements of systems that require user identification, identify existing approaches that can meet an application's requirements, and help report on and evaluate new approaches to user identification systematically.
SP  - 39
EP  - 39
JF  - ACM Transactions on Computer-Human Interaction
VL  - 24
IS  - 6
PB  - 
DO  - 10.1145/3144569
ER  - 

TY  - NA
AU  - Grosse-Puppendahl, Tobias; Dellangnol, Xavier; Hatzfeld, Christian; Fu, Biying; Kupnik, Mario; Kuijper, Arjan; Hastall, Matthias R.; Scott, James; Gruteser, Marco
TI  - MobiSys - Platypus: Indoor Localization and Identification through Sensing of Electric Potential Changes in Human Bodies
PY  - 2016
AB  - Platypus is the first system to localize and identify people by remotely and passively sensing changes in their body electric potential which occur naturally during walking. While it uses three or more electric potential sensors with a maximum range of 2 m, as a tag-free system it does not require the user to carry any special hardware. We describe the physical principles behind body electric potential changes, and a predictive mathematical model of how this affects a passive electric field sensor. By inverting this model and combining data from sensors, we infer a method for localizing people and experimentally demonstrate a median localization error of 0.16 m. We also use the model to remotely infer the change in body electric potential with a mean error of 8.8 % compared to direct contact-based measurements. We show how the reconstructed body electric potential differs from person to person and thereby how to perform identification. Based on short walking sequences of 5 s, we identify four users with an accuracy of 94 %, and 30 users with an accuracy of 75 %. We demonstrate that identification features are valid over multiple days, though change with footwear.
SP  - 17
EP  - 30
JF  - Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2906388.2906402
ER  - 

TY  - JOUR
AU  - Vegas, Jesús; Llamas, César; González, Manuel Á.; Hernández, Carmen
TI  - Identifying users from the interaction with a door handle
PY  - 2021
AB  - Abstract Ambient intelligence pursues the integration of intelligent approaches on an IoT infrastructure, mainly using everyday objects of the environment. The main hypothesis of the work is that the way in which a user interacts with a door handle is suitable to be used in the identification task. Our proposal contributes with a new method to identify persons in a seamless and unobstrusive way, suitable to be used in a smart building scenery without the need to bring any additional device. In this case, we embed accelerometers and gyroscopes in a door handle in order to obtain a data set comprising samples of 47 individuals. A parametric approximation is adopted to reduce each sample to a feature vector by using a dynamic time warping technique. A study has been made of the outcomes of different classification techniques over six different feature sets in order to assess the feasibility of this identification challenge. The AUC values observed with the selected feature set show promising results above 0.90 using neural networks and SVM classifiers.
SP  - 101293
EP  - NA
JF  - Pervasive and Mobile Computing
VL  - 70
IS  - NA
PB  - 
DO  - 10.1016/j.pmcj.2020.101293
ER  - 

TY  - NA
AU  - Lee, Geonsun; Kang, HyeongYeop; Lee, JongMin; Han, JungHyun
TI  - VR - A User Study on View-sharing Techniques for One-to-Many Mixed Reality Collaborations
PY  - 2020
AB  - In a one-to-many mixed reality collaboration environment, where multiple local users wearing AR headsets are supervised by a remote expert wearing a VR HMD, we evaluated three view-sharing techniques: 2D video, 360 video, and 3D model augmented with 2D video. Through a pilot test, the weaknesses of the techniques were identified, and additional features were integrated into them. Then, their performances were compared in two different collaboration scenarios based on search and assembling. In the first scenario, a local user performed both search and assembling. In the second scenario, two local users had dedicated roles, one for search and the other for assembling. The experiment results showed that the 3D model augmented with 2D video was time-efficient, usable, less demanding and most preferred in one-to-many mixed reality collaborations.
SP  - 343
EP  - 352
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581166222244
ER  - 

TY  - CHAP
AU  - Breitkreuz, David; Müller, Maike; Stegelmeyer, Dirk; Mishra, Rakesh
TI  - Augmented Reality Remote Maintenance in Industry: A Systematic Literature Review
PY  - 2022
AB  - AbstractAugmented reality (AR) is a promising technology for supporting industrial maintenance applications. Two major types of AR technology are used for maintenance applications. One of those is AR remote maintenance, a technology that connects remote experts to on-site technicians to work collaboratively on industrial maintenance applications. This seems especially valuable for nonstandardized tasks. Although several recent systematic literature reviews (SLRs) on AR for maintenance applications have been published, the growing body of literature calls for an ever-differentiated view of the knowledge base of AR remote maintenance. Therefore, this paper aims to map AR remote maintenance literature by conducting an SLR, characterizing the literature, describing applications in industry, and making suggestions for further research. Based on the analysis of 89 articles from the last two decades, this paper contributes the following findings: 1) the research field has a strong engineering focus on system development. 2) scholars share a common understanding of AR remote maintenance, despite using heterogeneous terminology; 3) the prevailing study design only allows for limited comparison of prototypes and applications; 4) transferability to industrial maintenance professionals is limited, due to the study design; 5) AR remote maintenance appears to raise business model opportunities for product-service systems; and 6) the diversity of AR remote maintenance applications indicates the technology’s industrial versatility. Overall, the maturity of the research field is increasing; however, it is still at an early stage. Based on these findings, we made two proposals for advancing the AR remote maintenance research field.KeywordsAugmented realityRemote maintenanceTele-assistanceMobile collaborationProduct-service system
SP  - 287
EP  - 305
JF  - Extended Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-15553-6_21
ER  - 

TY  - NA
AU  - Varga, Virag; Vakulya, Gergely; Buergisser, Benjamin; Riopelle, Nathan; Zünd, Fabio; Sumner, Robert W.; Gross, Thomas R.; Sample, Alanson P.
TI  - TEI - Real-Time Capture of Holistic Tangible Interactions
PY  - 2021
AB  - When digital applications aim to blend virtual and real worlds, understanding the actual physical actions of users becomes an important task; the precise timing of these tangible interaction events is needed, along with the identity, and possibly location and history, of all involved actors/objects. With multiple actors or objects, it is difficult to identify who touches which object and when. Instrumenting objects for Body Channel Communication (BCC) allows message exchange around the human body between instrumented objects and the user themselves. In this paper we show how BCC can be utilized to perform under real-time conditions so that we can directly notice touch events (and the identity of actors). TangibleID is a framework that unifies tangible interaction capture for objects and users based on wearable BCC. TangibleID provides identification and communication with tagged objects/users in less than 120 ms and supports a variety of tangible interactions, without the need to restrict user (hand) movements or to maintain line-of-sight connection to cameras. When an AR application is combined with TangibleID, a new tangible mixed reality experience is achieved, as demonstrated in the “Haunted Castle” showcase. The paper presents an end-to-end technical evaluation including trade-offs regarding robustness and speed of touch recognition, outlines the breadth of interaction modalities, and reports on an initial user assessment.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440658
ER  - 

TY  - NA
AU  - Wilhelm, Mathias; Lechler, Jan-Peter; Krakowczyk, Daniel; Albayrak, Sahin
TI  - IUI - Ring-based finger tracking using capacitive sensors and long short-term memory
PY  - 2020
AB  - We present a ring-shaped interaction device, called PeriSense, utilizing capacitive sensing in order to enable finger tracking. The finger angles and its adjacent fingers are sensed by measuring capacitive proximity between electrodes and human skin. To map the capacitive measurements to the finger angles, we use long short-term memory (LSTM). By wearing the ring on the middle finger, the angles of the index, middle and ring finger can be determined from the capacitive measurements. We collected sample data from 17 users using a Leap Motion camera, which provided the reference data. In a leave-one-user-out cross-validation test, we revealed a mean absolute error of 13.02 degrees over all finger angles. The motion of the little finger and the thumb are out of the capacitive measurement range or covered by the index and ring finger, respectively. With natural finger movements, the LSTM estimates the angles of the little finger and thumb based on the movement pattern of the other fingers.
SP  - 551
EP  - 555
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377535
ER  - 

TY  - JOUR
AU  - Hong, Sanghwa; Heo, Seongkook; Lee, Byungjoo
TI  - MaterialSense: Estimating and utilizing material properties of contact objects in multi-touch interaction
PY  - NA
AB  - NA
SP  - 102985
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 172
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2022.102985
ER  - 

TY  - JOUR
AU  - Guo, Jianwei; Jiang, Haiyong; Benes, Bedrich; Deussen, Oliver; Zhang, Xiaopeng; Lischinski, Dani; Huang, Hui
TI  - Inverse Procedural Modeling of Branching Structures by Inferring L-Systems
PY  - 2020
AB  - We introduce an inverse procedural modeling approach that learns L-system representations of pixel images with branching structures. Our fully automatic model generates a compact set of textual rewriting rules that describe the input. We use deep learning to discover atomic structures such as line segments or branchings. Orientation and scaling of these structures are determined and the detected structures are combined into a tree. The initial representation is analyzed, and repeating parts are encoded into a small grammar by using greedy optimization while the user can control the size of the detected rules. The output is an L-system that represents the input image as a simple text and a set of terminal symbols. We apply our approach to a variety of examples, demonstrate its robustness against noise and blur, and we show that it can detect user sketches and complex input structures.
SP  - 1
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 5
PB  - 
DO  - 10.1145/3394105
ER  - 

TY  - NA
AU  - Ohshima, Yoshiki; Warth, Alessandro; Freudenberg, Bert; Lunzer, Aran; Kay, Alan
TI  - Towards Making a Computer Tutor for Children of All Ages: A Memo
PY  - 2016
AB  - One of the primary goals of our research group is to improve education through computing. We are interested in unleashing the power of the computer to create automated "intelligent" tutor systems (ITS). This paper presents ideas that may guide the design of such a system, targeting the problem of computer-programming education in particular. We also outline a research and development plan to build this system. While this plan is just a straw-man (there is a lot of uncertainty), our hope is to get a discussion started on this important topic.
SP  - 21
EP  - 25
JF  - Proceedings of the Programming Experience 2016 (PX/16) Workshop
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984380.2984383
ER  - 

TY  - NA
AU  - Lee, DoYoung; Lee, Youryang; Shin, Yonghwan; Oakley, Ian
TI  - UIST - Designing Socially Acceptable Hand-to-Face Input
PY  - 2018
AB  - Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable hand-to-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.
SP  - 711
EP  - 723
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242642
ER  - 

TY  - CHAP
AU  - Nyemkova, Elena; Shandra, Zynovii; Klos-Witkowska, Aleksandra; Więcław, Łukasz
TI  - CISIM - Network Electronic Devices Authentication by Internal Electrical Noise
PY  - 2018
AB  - The article is devoted to dynamic authentication method of electronic network devices with built-in analog-to-digital converters (ADCs) based on authentication templates. The following results were obtained: the authentication of each electronic device can be carried out uniquely by its internal electrical noise (like biometric authentication of a person). Uniqueness of authentication is provided by the invariants of the noise signal such as the shape of the graph of the autocorrelation function of noise and the set of resonance frequencies of the device. The electronic device authentication template is obtained from the sequence of values of the autocorrelation function of the noise. It consists from the bit template and the amplitude template. The technique of obtaining an authentication template is presented. The required duration of the noise signal is 0.5 s for reliable authentication at a sampling frequency of 44.1 kHz. The results of authentication of several computers are presented.
SP  - 474
EP  - 485
JF  - Computer Information Systems and Industrial Management
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-99954-8_39
ER  - 

TY  - NA
AU  - Gong, Jun; Gupta, Aakar; Benko, Hrvoje
TI  - UIST - Acustico: Surface Tap Detection and Localization using Wrist-based Acoustic TDOA Sensing
PY  - 2020
AB  - In this paper, we present Acustico, a passive acoustic sensing approach that enables tap detection and 2D tap localization on uninstrumented surfaces using a wrist-worn device. Our technique uses a novel application of acoustic time differences of arrival (TDOA) analysis. We adopt a sensor fusion approach by taking both 'surface waves' (i.e., vibrations through surface) and 'sound waves' (i.e., vibrations through air) into analysis to improve sensing resolution. We carefully design a sensor configuration to meet the constraints of a wristband form factor. We built a wristband prototype with four acoustic sensors, two accelerometers and two microphones. Through a 20-participant study, we evaluated the performance of our proposed sensing technique for tap detection and localization. Results show that our system reliably detects taps with an F1-score of 0.9987 across different environmental noises and yields high localization accuracies with root-mean-square-errors of 7.6mm (X-axis) and 4.6mm (Y-axis) across different surfaces and tapping techniques.
SP  - 406
EP  - 419
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415901
ER  - 

TY  - JOUR
AU  - Kim, Seong-heum; Tai, Yu-Wing; Lee, Joon-Young; Park, Jaesik; Kweon, In So
TI  - Category-Specific Salient View Selection via Deep Convolutional Neural Networks
PY  - 2017
AB  - In this paper, we present a new framework to determine up front orientations and detect salient views of 3D models. The salient viewpoint to human preferences is the most informative projection with correct upright orientation. Our method utilizes two Convolutional Neural Network (CNN) architectures to encode category-specific information learnt from a large number of 3D shapes and 2D images on the web. Using the first CNN model with 3D voxel data, we generate a CNN shape feature to decide natural upright orientation of 3D objects. Once a 3D model is upright-aligned, the front projection and salient views are scored by category recognition using the second CNN model. The second CNN is trained over popular photo collections from internet users. In order to model comfortable viewing angles of 3D models, a category-dependent prior is also learnt from the users. Our approach effectively combines category-specific scores and classical evaluations to produce a data-driven viewpoint saliency map. The best viewpoints from the method are quantitatively and qualitatively validated with more than 100 objects from 20 categories. Our thumbnail images of 3D models are the most favoured among those from different approaches.
SP  - 313
EP  - 328
JF  - Computer Graphics Forum
VL  - 36
IS  - 8
PB  - 
DO  - 10.1111/cgf.13082
ER  - 

TY  - JOUR
AU  - Ayeswarya, S.; Norman, Jasmine
TI  - A survey on different continuous authentication systems
PY  - 2019
AB  - There has been significant research in the provision of trustworthy initial login user authentication, however, there is still need for continuous authentication during a user session. Most mobile devices and computer systems authenticate a user only at the initial login session and do not take steps to recognise whether the present user is still the initial authorised user or an imposter pretending to be a valid user. Therefore, a system to check the identity of the user continuously throughout the whole session is necessary. To ensure the authenticity of the user during their whole login session, a continuous user authentication mechanism is required. In this paper, an overview of different continuous authentication methods is presented along with a discussion on the merits and demerits of the available approaches. This paper also discusses the understanding of the emerging necessities and open problems in continuous user authentication system.
SP  - 67
EP  - 99
JF  - International Journal of Biometrics
VL  - 11
IS  - 1
PB  - 
DO  - 10.1504/ijbm.2019.10016811
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Salehi, Niloufar; Lam, Michelle S.; Marroquin, Juan C.; Bernstein, Michael S.
TI  - CHI - Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships
PY  - 2016
AB  - Expert crowdsourcing marketplaces have untapped potential to empower workers' career and skill development. Currently, many workers cannot afford to invest the time and sacrifice the earnings required to learn a new skill, and a lack of experience makes it difficult to get job offers even if they do. In this paper, we seek to lower the threshold to skill development by repurposing existing tasks on the marketplace as mentored, paid, real-world work experiences, which we refer to as micro-internships. We instantiate this idea in Atelier, a micro-internship platform that connects crowd interns with crowd mentors. Atelier guides mentor-intern pairs to break down expert crowdsourcing tasks into milestones, review intermediate output, and problem-solve together. We conducted a field experiment comparing Atelier's mentorship model to a non-mentored alternative on a real-world programming crowdsourcing task, finding that Atelier helped interns maintain forward progress and absorb best practices.
SP  - 2645
EP  - 2656
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858121
ER  - 

TY  - NA
AU  - Ito, Hiroki; Shimakawa, Hiromitsu; Harada, Fumiko
TI  - Estimating Learner's Perspective in Programming: Analysis of Operation Time Series in Code Puzzles
PY  - 2021
AB  - In programming education, the instructor tries to find out the learner who needs help by grasping the understanding using a written test and e-learning. However, in reality, not many learners will acquire the skill of writing source codes. This kind of situation implies that programming ability of learners cannot be measured only by knowledge tests or the data obtained from answer results. This paper discusses understanding analysis that focuses on the thought process of programming. The study analyses time series of operations of learners struggling with a code puzzle, where they arrange code fragments. The proposed method aims to estimate their perspectives on how fragments are organized to achieve given requirements. As a result of the experiment, we were able to represent the learner’s perspective as a hidden state of the hidden Markov model.
SP  - NA
EP  - NA
JF  - 2021 16th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iske54062.2021.9755407
ER  - 

TY  - NA
AU  - Kim, Joy; Agrawala, Maneesh; Bernstein, Michael S.
TI  - Mosaic: Designing Online Creative Communities for Sharing Works-in-Progress
PY  - 2017
AB  - Online creative communities allow creators to share their work with a large audience, maximizing opportunities to showcase their work and connect with fans and peers. However, sharing in-progress work can be technically and socially challenging in environments designed for sharing completed pieces. We propose an online creative community where sharing process, rather than showcasing outcomes, is the main method of sharing creative work. Based on this, we present Mosaic---an online community where illustrators share work-in-progress snapshots showing how an artwork was completed from start to finish. In an online deployment and observational study, artists used Mosaic as a vehicle for reflecting on how they can improve their own creative process, developed a social norm of detailed feedback, and became less apprehensive of sharing early versions of artwork. Through Mosaic, we argue that communities oriented around sharing creative process can create a collaborative environment that is beneficial for creative growth.
SP  - 246
EP  - 258
JF  - Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2998181.2998195
ER  - 

TY  - NA
AU  - Bianchi, Andrea; 제승우, NA; 이혜립, NA; Oakley, Ian
TI  - Enhancing Spatial Input on the Body with a Smart-Ring
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Mariano, Laura J.; Aubuchon, Alexander; Lau, Troy; Ozdemir, Onur; Lazovich, Tomo; Coakley, John
TI  - Classification of Electronic Devices and Software Processes via Unintentional Electronic Emissions With Neural Decoding Algorithms
PY  - 2020
AB  - Electronic and electromechanical devices continuously emit electromagnetic (EM) signals while in use. These EM emissions (EMEs) contain unique spectral characteristics that can be leveraged for a variety of purposes, including identification of the device's unique EM “fingerprint,” and characterization of software processes running on the device. In this study, we implemented a novel method for automatic identification and characterization of these EMEs inspired by a classification/decoding scheme used to extract neural correlates of brain state from magnetoencephalographic data. Utilizing a sparse bilinear formulation of logistic regression as our “neural” decoder, we extracted device- and software-specific spectrospatial patterns from five identical Arduino Uno prototyping boards as they cycled through five program states. In the device fingerprinting task, we were able to discriminate all five boards from each other with near-perfect accuracy using this method. For software characterization, within a single device, we detected all five programs with 90% accuracy, and across devices, we were able to identify 3/5 programs with 99% accuracy, and achieving 77% accuracy for the other two. Overall, this neural-decoding approach performed well in all scenarios tested, and the corresponding EME “maps” it provides quantify the differences between device and software-specific EMEs in an easily interpretable way.
SP  - 470
EP  - 477
JF  - IEEE Transactions on Electromagnetic Compatibility
VL  - 62
IS  - 2
PB  - 
DO  - 10.1109/temc.2019.2903232
ER  - 

TY  - NA
AU  - Elvezio, Carmine; Sukan, Mengu; Feiner, Steven
TI  - CHI - Mercury: A Messaging Framework for Modular UI Components
PY  - 2018
AB  - In recent years, the entity--component--system pattern has become a fundamental feature of the software architectures of game-development environments such as Unity and Unreal, which are used extensively in developing 3D user interfaces. In these systems, UI components typically respond to events, requiring programmers to write application-specific callback functions. In some cases, components are organized in a hierarchy that is used to propagate events among vertically connected components. When components need to communicate horizontally, programmers must connect those components manually and register/unregister events as needed. Moreover, events and callback signatures may be incompatible, making modular UIs cumbersome to build and share within or across applications. To address these problems, we introduce a messaging framework, Mercury, to facilitate communication among components. We provide an overview of Mercury, outline its underlying protocol and how it propagates messages to responders using relay nodes, describe a reference implementation in Unity, and present example systems built using Mercury to explain its advantages.
SP  - 588
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174162
ER  - 

TY  - JOUR
AU  - Raina, Ayush; McComb, Christopher; Cagan, Jonathan
TI  - Learning to design from humans: Imitating human designers through deep learning
PY  - 2019
AB  - Humans as designers have quite versatile problem-solving strategies. Computer agents on the other hand can access large scale computational resources to solve certain design problems. Hence, if agents can learn from human behavior, a synergetic human-agent problem solving team can be created. This paper presents an approach to extract human design strategies and implicit rules, purely from historical human data, and use that for design generation. A two-step framework that learns to imitate human design strategies from observation is proposed and implemented. This framework makes use of deep learning constructs to learn to generate designs without any explicit information about objective and performance metrics. The framework is designed to interact with the problem through a visual interface as humans did when solving the problem. It is trained to imitate a set of human designers by observing their design state sequences without inducing problem-specific modelling bias or extra information about the problem. Furthermore, an end-to-end agent is developed that uses this deep learning framework as its core in conjunction with image processing to map pixel-to-design moves as a mechanism to generate designs. Finally, the designs generated by a computational team of these agents are then compared to actual human data for teams solving a truss design problem. Results demonstrates that these agents are able to create feasible and efficient truss designs without guidance, showing that this methodology allows agents to learn effective design strategies.
SP  - NA
EP  - NA
JF  - Journal of Mechanical Design
VL  - 141
IS  - 11
PB  - 
DO  - 10.1115/1.4044256
ER  - 

TY  - JOUR
AU  - Maddali, Hanuma Teja; Irlitti, Andrew; Lazar, Amanda
TI  - Probing the Potential of Extended Reality to Connect Experts and Novices in the Garden
PY  - 2022
AB  - <jats:p>As extended reality (XR) systems become increasingly available, XR-based remote instruction is being adopted for diverse purposes in professional settings such as surgery and field servicing. Hobbyists have been well-studied in HCI and may similarly benefit from remote skill-sharing. However, little is known about how XR technologies might support expert-novice collaboration for skilled hobby activities. This paper examines the potential and limitations of XR to connect experts and novices for one such activity: gardening. Through two studies involving 27 expert and novice gardeners, we designed prototypes to understand 1) practitioner perceptions of XR and remote skill-sharing in the garden and 2) what kinds of interactions can be supported in XR for expert-novice groups. We discuss design opportunities and challenges for XR systems in supporting informal connecting interactions and meaningful sensory interactions with a remote environment during skill-sharing.</jats:p>
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555211
ER  - 

TY  - NA
AU  - von Zadow, Ulrich; Reipschläger, Patrick; Bösel, Daniel; Sellent, Anita; Dachselt, Raimund
TI  - AVI - YouTouch! Low-Cost User Identification at an Interactive Display Wall
PY  - 2016
AB  - We present YouTouch!, a system that tracks users in front of an interactive display wall and associates touches with users. With their large size, display walls are inherently suitable for multi-user interaction. However, current touch recognition technology does not distinguish between users, making it hard to provide personalized user interfaces or access to private data. In our system we place a commodity RGB + depth camera in front of the wall, allowing us to track users and correlate them with touch events. While the camera's driver is able to track people, it loses the user's ID whenever she is occluded or leaves the scene. In these cases, we re-identify the person by means of a descriptor comprised of color histograms of body parts and skeleton-based biometric measurements. Additional processing reliably handles short-term occlusion as well as assignment of touches to occluded users. YouTouch! requires no user instrumentation nor custom hardware, and there is no registration nor learning phase. Our system was thoroughly tested with data sets comprising 81 people, demonstrating its ability to re-identify users and correlate them to touches even under adverse conditions.
SP  - 144
EP  - 151
JF  - Proceedings of the International Working Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909132.2909258
ER  - 

TY  - NA
AU  - Pena-Rios, Anasol; Hagras, Hani; Gardner, Michael; Owusu, Gilbert
TI  - FUZZ-IEEE - A fuzzy logic based system for geolocated augmented reality field service support
PY  - 2017
AB  - In recent years, Augmented Reality (AR) started transitioning from an experimental technology to a more mature area, with new types of applications in entertainment, marketing, education, retail, transportation, manufacturing, construction, and other industries. One of the main challenges for AR-based field service tools is to help users to correctly locate company's assets and infrastructure in the field. This paper presents an AR system using private maps to find company's assets to support field workforce tasks. The AR system is based on fuzzy logic mechanisms to provide the user with directions for asset location by comparing his/her current position with assets' location in real-time. Auditory and visual feedback is provided via a head mounted display (HMD), enhancing user's perception to achieve human augmentation.
SP  - 1
EP  - 6
JF  - 2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/fuzz-ieee.2017.8015477
ER  - 

TY  - NA
AU  - Yu, Kevin; Winkler, Alexander; Pankratz, Frieder; Lazarovici, Marc; Wilhelm, Dirk; Eck, Ulrich; Roth, Daniel; Navab, Nassir
TI  - VR - Magnoramas: Magnifying Dioramas for Precise Annotations in Asymmetric 3D Teleconsultation
PY  - 2021
AB  - When users create hand-drawn annotations in Virtual Reality they often reach their physical limits in terms of precision, especially if the region to be annotated is small. One intuitive solution employs magnification beyond natural scale. However, scaling the whole environment results in wrong assumptions about the coherence between physical and virtual space. In this paper, we introduce Mag-noramas, a novel interaction method for selecting and extracting a region of interest that the user can subsequently scale and transform inside the virtual space. Our technique enhances the user's capabilities to perform supernaturally precise virtual annotations on virtual objects. We explored our technique in a user study within asimplified clinical scenario of a teleconsultation-supported craniectomy procedure that requires accurate annotations on a human head. Teleconsultation was performed asymmetrically between a remote expert in Virtual Reality that collaborated with a local user through Augmented Reality. The remote expert operates inside a reconstructed environment, captured from RGB-D sensors at the local site, and is embodied by an avatar to establish co-presence. The results show that Magnoramas significantly improve the precision of annotations while preserving usability and perceived presence measures compared to the baseline method. By hiding the 3D reconstruction while keeping the Magnorama, users can intentionally choose to lower their perceived social presence and focus on their tasks.
SP  - 392
EP  - 401
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00062
ER  - 

TY  - JOUR
AU  - Wang, April Yi; Mittal, Anant; Brooks, Christopher; Oney, Steve
TI  - How Data Scientists Use Computational Notebooks for Real-Time Collaboration
PY  - 2019
AB  - Effective collaboration in data science can leverage domain expertise from each team member and thus improve the quality and efficiency of the work. Computational notebooks give data scientists a convenient interactive solution for sharing and keeping track of the data exploration process through a combination of code, narrative text, visualizations, and other rich media. In this paper, we report how synchronous editing in computational notebooks changes the way data scientists work together compared to working on individual notebooks. We first conducted a formative survey with 195 data scientists to understand their past experience with collaboration in the context of data science. Next, we carried out an observational study of 24 data scientists working in pairs remotely to solve a typical data science predictive modeling problem, working on either notebooks supported by synchronous groupware or individual notebooks in a collaborative setting. The study showed that working on the synchronous notebooks improves collaboration by creating a shared context, encouraging more exploration, and reducing communication costs. However, the current synchronous editing features may lead to unbalanced participation and activity interference without strategic coordination. The synchronous notebooks may also amplify the tension between quick exploration and clear explanations. Building on these findings, we propose several design implications aimed at better supporting collaborative editing in computational notebooks, and thus improving efficiency in teamwork among data scientists.
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359141
ER  - 

TY  - NA
AU  - Ahuja, Karan; Shen, Vivian; Fang, Cathy Mengying; Riopelle, Nathan; Kong, Andy; Harrison, Chris
TI  - ControllerPose: Inside-Out Body Capture with VR Controller Cameras
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502105
ER  - 

TY  - NA
AU  - Liu, Jen-Shuo; Tversky, Barbara; Feiner, Steven
TI  - Precueing Sequential Rotation Tasks in Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565641
ER  - 

TY  - NA
AU  - Li, Dingzeyu; Nair, Avinash S.; Nayar, Shree K.; Zheng, Changxi
TI  - UIST - AirCode: Unobtrusive Physical Tags for Digital Fabrication
PY  - 2017
AB  - We present AirCode, a technique that allows the user to tag physically fabricated objects with given information. An AirCode tag consists of a group of carefully designed air pockets placed beneath the object surface. These air pockets are easily produced during the fabrication process of the object, without any additional material or postprocessing. Meanwhile, the air pockets affect only the scattering light transport under the surface, and thus are hard to notice to our naked eyes. But, by using a computational imaging method, the tags become detectable. We present a tool that automates the design of air pockets for the user to encode information. AirCode system also allows the user to retrieve the information from captured images via a robust decoding algorithm. We demonstrate our tagging technique with applications for metadata embedding, robotic grasping, as well as conveying object affordances.
SP  - 449
EP  - 460
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126635
ER  - 

TY  - NA
AU  - Holz, Christian; Bentley, Frank
TI  - CHI - On-Demand Biometrics: Fast Cross-Device Authentication
PY  - 2016
AB  - We explore the use of a new way to log into a web service, such as email or social media. Using on-demand biometrics, users sign in from a browser on a computer using just their name, which sends a request to their phone for approval. Users approve this request by authenticating on their phone using their fingerprint, which completes the login in the browser. On-demand biometrics thus replace passwords or temporary access codes found in two-step verification with the ease of use of biometrics. We present the results of an interview study on the use of on-demand biometrics with a live login backend. Participants perceived our system as convenient and fast to use and also expressed their trust in fingerprint authentication to keep their accounts safe. We motivate the design of on-demand biometrics, present an analysis of participants' use and responses around general account security and authentication, and conclude with implications for designing fast and easy cross-device authentication.
SP  - 3761
EP  - 3766
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858139
ER  - 

TY  - NA
AU  - Kim, Insu; Park, Keunwoo; Yoon, Youngwoo; Lee, Geehyuk
TI  - UIST (Adjunct Volume) - Touch180: Finger Identification on Mobile Touchscreen using Fisheye Camera and Convolutional Neural Network
PY  - 2018
AB  - We present Touch180, a computer vision based solution for identifying fingers on a mobile touchscreen with a fisheye camera and deep learning algorithm. As a proof-of-concept research, this paper focused on robustness and high accuracy of finger identification. We generated a new dataset for Touch180 configuration, which is named as Fisheye180. We trained a CNN (Convolutional Neural Network)-based network utilizing touch locations as auxiliary inputs. With our novel dataset and deep learning algorithm, finger identification result shows 98.56% accuracy with VGG16 model. Our study will serve as a step stone for finger identification on a mobile touchscreen.
SP  - 29
EP  - 32
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266091
ER  - 

TY  - NA
AU  - Kienzle, Wolf; Whitmire, Eric; Rittaler, Chris; Benko, Hrvoje
TI  - CHI - ElectroRing: Subtle Pinch and Touch Detection with a Ring
PY  - 2021
AB  - We present ElectroRing, a wearable ring-based input device that reliably detects both onset and release of a subtle finger pinch, and more generally, contact of the fingertip with the user’s skin. ElectroRing addresses a common problem in ubiquitous touch interfaces, where subtle touch gestures with little movement or force are not detected by a wearable camera or IMU. ElectroRing’s active electrical sensing approach provides a step-function-like change in the raw signal, for both touch and release events, which can be easily detected using only basic signal processing techniques. Notably, ElectroRing requires no second point of instrumentation, but only the ring itself, which sets it apart from existing electrical touch detection methods. We built three demo applications to highlight the effectiveness of our approach when combined with a simple IMU-based 2D tracking system.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445094
ER  - 

TY  - NA
AU  - Ritchie, Daniel; Thomas, Anna E.; Hanrahan, Pat; Goodman, Noah D.
TI  - Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks
PY  - 2016
AB  - Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jasche, Florian; Kirchhübel, Jasmin; Ludwig, Thomas; Tolmie, Peter
TI  - C&amp;T - BeamLite: Diminishing Ecological Fractures of Remote Collaboration through Mixed Reality Environments
PY  - 2021
AB  - Developing systems to support remote collaboration usually involves creating new environments in which non-co-located participants produce actions that are, at least in part, accessible to one another. However, this typically fractures the relationship between those actions and the sense of a shared environment, engendering difficulties that can render even the simplest of activities problematic. This becomes more pronounced as the activities become more complex and involve physical artifacts. Although mixed reality seems to offer promising ways of overcoming these troubles, there is still a risk of replicating the fractured ecology problem. We report on an empirical study and the development of a mixed reality prototype called BeamLite that seeks to bypass such issues by providing participants with the illusion of them sharing a single familiar place. Although our evaluation revealed possibilities for evading some troubles associated with artifact-focused remote collaboration, it exposed the need for virtual toolboxes that dynamically support specific work practices and the importance of virtual artifacts embedded within the physical environment to further diminish the sense of ecological fracture.
SP  - 200
EP  - 211
JF  - C&T '21: Proceedings of the 10th International Conference on Communities & Technologies - Wicked Problems in the Age of Tech
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461564.3461566
ER  - 

TY  - NA
AU  - Sharma, Rhea; Nair, Atira; Guo, Ana; Palea, Dustin; Lee, David T.
TI  - "It's usually not worth the effort unless you get really lucky": Barriers to Undergraduate Research Experiences from the Perspective of Computing Faculty
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Conference on International Computing Education Research V.1
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3501385.3543976
ER  - 

TY  - CHAP
AU  - Ladwig, Philipp; Geiger, Christian
TI  - REV - A Literature Review on Collaboration in Mixed Reality
PY  - 2018
AB  - Mixed Reality is defined as a combination of Reality, Augmented Reality, Augmented Virtuality and Virtual Reality. This innovative technology can aid with the transition between these stages. The enhancement of reality with synthetic images allows us to perform tasks more easily, such as the collaboration between people who are at different locations. Collaborative manufacturing, assembly tasks or education can be conducted remotely, even if the collaborators do not physically meet. This paper reviews both past and recent research, identifies benefits and limitations, and extracts design guidelines for the creation of collaborative Mixed Reality applications in technical settings.
SP  - 591
EP  - 600
JF  - Lecture Notes in Networks and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-95678-7_65
ER  - 

TY  - JOUR
AU  - Gu, Yizheng; Yu, Chun; Li, Zhipeng; Li, Zhaoheng; Xiaoying, Wei; Shi, Yuanchun
TI  - QwertyRing: Text Entry on Physical Surfaces Using a Ring
PY  - 2020
AB  - The software keyboard is widely used on digital devices such as smartphones, computers, and tablets. The software keyboard operates via touch, which is efficient, convenient, and familiar to users. However, some emerging technology devices such as AR/VR headsets and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing, a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement Unit) ring. Users wear the ring on the middle phalanx of the index finger and type on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing, users do not focus on monitoring the hand motions. They receive text feedback on a separate screen, e.g., an AR/VR headset or a digital device display, such as a computer monitor. The basic idea of QwertyRing is to detect touch events and predict users' desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing through a five-day user study. Participants achieved a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based techniques [24, 30, 45, 68] and is 86.48% of the speed of typing on a smartphone with an index finger. The results show that QwertyRing enables efficient touch-based text entry on physical surfaces.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432204
ER  - 

TY  - NA
AU  - Zhang, Qian; Wang, Dong; Zhao, Run; Yu, Yinggang
TI  - IUI - MyoSign: enabling end-to-end sign language recognition with wearables
PY  - 2019
AB  - Automatic sign language recognition is an important milestone in facilitating the communication between the deaf community and hearing people. Existing approaches are either intrusive or susceptible to ambient environments and user diversity. Moreover, most of them perform only isolated word recognition, not sentence-level sequence translation. In this paper, we present MyoSign, a deep learning based system that enables end-to-end American Sign Language (ASL) recognition at both word and sentence levels. We leverage a lightweight wearable device which can provide inertial and electromyography signals to non-intrusively capture signs. First, we propose a multimodal Convolutional Neural Network (CNN) to abstract representations from inputs of different sensory modalities. Then, a bidirectional Long Short Term Memory (LSTM) is exploited to model temporal dependences. On the top of the networks, we employ Connectionist Temporal Classification (CTC) to get around temporal segments and achieve end-to-end continuous sign language recognition. We evaluate MyoSign on 70 commonly used ASL words and 100 ASL sentences from 15 volunteers. Our system achieves an average accuracy of 93.7% at word-level and 93.1% at sentence-level in user-independent settings. In addition, MyoSign can recognize sentences unseen in the training set with 92.4% accuracy. The encouraging results indicate that MyoSign can be a meaningful buildup in the advancement of sign language recognition.
SP  - 650
EP  - 660
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302296
ER  - 

TY  - NA
AU  - Weng, Yueting; Chun, Yu; Shi, Yingtian; Zhao, Yuhang; Yan, Yukang; Shi, Yuanchun
TI  - CHI - FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision
PY  - 2021
AB  - We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445484
ER  - 

TY  - JOUR
AU  - Tomlinson, William J.; Banou, Stella; Blechinger-Slocum, Shay; Yu, Christopher C.; Chowdhury, Kaushik R.
TI  - Body-Guided Galvanic Coupling Communication for Secure Biometric Data
PY  - 2019
AB  - In a world dominated by the wearable IoT devices, malicious security threats have become a common concern in system design. Guided by this notion, we propose a secure transmission system through body-guided channels using Galvanic Coupling (GC). The GC-method injects weak electrical current into human tissue, primarily propagating through the skin. The proposed design provides impermeability to malicious attacks, (e.g. side-channel sniffing) when sending biometric data, as the body behaves as a natural waveguide. The following contributions are: 1) the analytical formulation and empirical verification of a 3D tissue equivalent circuit model for GC-signal propagation of the human arm-wrist-palm channel; 2) the simulation study of numerous modulation schemes, drawn from the validated results of the GC-channel model; 3) the design and implementation of a transceiver prototype using optimal communication parameters (modulation, frequency, power) for transmission on a dielectrically equivalent tissue phantom; and 4) through the experimental trials, we demonstrate the eavesdropping susceptibility of the GC-signals, and similar body communication techniques, over-the-air and while in direct contact with the medium. The performance results of the GC-transceiver prototype yield a bit error rate of 10−6 with a transmit power of −2 dBm, in addition to over 7 $\times$ reduction of signal radiation outside the body compared to capacitive coupling.
SP  - 4143
EP  - 4156
JF  - IEEE Transactions on Wireless Communications
VL  - 18
IS  - 8
PB  - 
DO  - 10.1109/twc.2019.2921964
ER  - 

TY  - JOUR
AU  - Welsford-Ackroyd, Finn; Chalmers, Andrew; Anjos, Rafael Kuffner dos; Medeiros, Daniel; Kim, Hyejin; Rhee, Taehyun
TI  - Spectator View: Enabling Asymmetric Interaction between HMD Wearers and Spectators with a Large Display
PY  - 2021
AB  - In this paper, we present a system that allows a user with a head-mounted display (HMD) to communicate and collaborate with spectators outside of the headset. We evaluate its impact on task performance, immersion, and collaborative interaction. Our solution targets scenarios like live presentations or multi-user collaborative systems, where it is not convenient to develop a VR multiplayer experience and supply each user (and spectator) with an HMD. The spectator views the virtual world on a large-scale tiled video wall and is given the ability to control the orientation of their own virtual camera. This allows spectators to stay focused on the immersed user's point of view or freely look around the environment. To improve collaboration between users, we implemented a pointing system where a spectator can point at objects on the screen, which maps an indicator directly onto the objects in the virtual world. We conducted a user study to investigate the influence of rotational camera decoupling and pointing gestures in the context of HMD-immersed and non-immersed users utilizing a large-scale display. Our results indicate that camera decoupling and pointing positively impacts collaboration. A decoupled view is preferable in situations where both users need to indicate objects of interest in the scene, such as presentations and joint-task scenarios, as it requires a shared reference space. A coupled view, on the other hand, is preferable in synchronous interactions such as remote-assistant scenarios.
SP  - 1
EP  - 17
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3486951
ER  - 

TY  - CHAP
AU  - Johnson, Naomi; Jones, Michael; Seppi, Kevin; Thatcher, Lawrence
TI  - Understanding How Non-experts Collect and Annotate Activity Data
PY  - 2019
AB  - NA
SP  - 91
EP  - 110
JF  - Human Activity Sensing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-13001-5_7
ER  - 

TY  - NA
AU  - Yu, Xiaojing; Zhou, Zhijun; Xu, Mingxue; You, Xuanke; Li, Xiang-Yang
TI  - PerCom - ThumbUp: Identification and Authentication by Smartwatch using Simple Hand Gestures
PY  - 2020
AB  - The widespread creative application and smart devices call for convenient and secure interaction with human users. We propose, design, and implement a smartwatch-based two-factor real-time identification and authentication system named ThumbUp, where smartwatch users can identify and authenticate themselves by some simple hand and finger gestures, such as thumb-up. ThumbUp leverages the signal collected from the Inertial Measurement Unit (IMU) in Commercial Off-The-Shelf (COTS) smart devices and discovers the unique fingerprint pattern produced by each user’s simple hand gestures using a carefully crafted deep learning model. We implement our system and conduct extensive experiments to evaluate its efficacy and efficiency with 65 different users over a period of more than 3 months. It reaches an accuracy of 97% for identification, and EER 0.014 for authentication using only one simple gesture. We also survey the users’ acceptance of our system and discuss how the proficiency of gestures affects authentication accuracy.
SP  - 1
EP  - 10
JF  - 2020 IEEE International Conference on Pervasive Computing and Communications (PerCom)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/percom45495.2020.9127367
ER  - 

TY  - NA
AU  - Ye, Xinfeng; Manoharan, Sathiamoorthy
TI  - AIIPCC - Providing automated grading and personalized feedback
PY  - 2019
AB  - Practicing helps students reinforce the concepts that they have learned. It is equally important that personalized feedback are given to the students to help the student to learn from their mistakes. Traditionally, marking assessments and providing personalized feedback are carried out manually by teachers. However, both of these tasks become increasingly difficult as class size becomes large. To cope with this problem, we utilize natural language processing and machine learning techniques to mark essay type assessments and to help teachers provide personalized feedback promptly. The scheme determines the correctness of students' answers according to the meaning of the answers instead of comparing the words in the answers with the ones in the specimen answer. The scheme partitions the students' answers into groups according to their semantic meanings. Teachers provide feedback to a few answers in each group, and this feedback covers the typical issues demonstrated by the answers in the group. The feedback is propagated to the other answers in the same group. The proposed schemes showed reasonable marking accuracy.
SP  - NA
EP  - NA
JF  - Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3371425.3371453
ER  - 

TY  - JOUR
AU  - Stearns, Lee; Oh, Uran; Findlater, Leah; Froehlich, Jon E.
TI  - TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments
PY  - 2018
AB  - On-body interaction, which employs the user's own body as an interactive surface, offers several advantages over existing touchscreen devices: always-available control, an expanded input space, and additional proprioceptive and tactile cues that support non-visual use. While past work has explored a variety of approaches such as wearable depth cameras, bio-acoustics, and infrared reflectance (IR) sensors, these systems do not instrument the gesturing finger, do not easily support multiple body locations, and have not been evaluated with visually impaired users (our target). In this paper, we introduce TouchCam, a finger wearable to support location-specific, on-body interaction. TouchCam combines data from infrared sensors, inertial measurement units, and a small camera to classify body locations and gestures using supervised learning. We empirically evaluate TouchCam's performance through a series of offline experiments followed by a realtime interactive user study with 12 blind and visually impaired participants. In our offline experiments, we achieve high accuracy (>96%) at recognizing coarse-grained touch locations (e.g., palm, fingers) and location-specific gestures (e.g., tap on wrist, left swipe on thigh). The follow-up user study validated our real-time system and helped reveal tradeoffs between various on-body interface designs (e.g., accuracy, convenience, social acceptability). Our findings also highlight challenges to robust input sensing for visually impaired users and suggest directions for the design of future on-body interaction systems.
SP  - 164
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161416
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Flamich, Gergely; Schrempf, Patrick; Harris-Birtill, David; Quigley, Aaron
TI  - UIST - RadarCat: Radar Categorization for Input & Interaction
PY  - 2016
AB  - In RadarCat we present a small, versatile radar-based system for material and object classification which enables new forms of everyday proximate interaction with digital devices. We demonstrate that we can train and classify different types of materials and objects which we can then recognize in real time. Based on established research designs, we report on the results of three studies, first with 26 materials (including complex composite objects), next with 16 transparent materials (with different thickness and varying dyes) and finally 10 body parts from 6 participants. Both leave one-out and 10-fold cross-validation demonstrate that our approach of classification of radar signals using random forest classifier is robust and accurate. We further demonstrate four working examples including a physical object dictionary, painting and photo editing application, body shortcuts and automatic refill based on RadarCat. We conclude with a discussion of our results, limitations and outline future directions.
SP  - 833
EP  - 841
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984515
ER  - 

TY  - JOUR
AU  - Tangsali, Kaustubh; Krishnamurthy, Vinayak R.; Hasnain, Zohaib
TI  - Generalizability of Convolutional Encoder–Decoder Networks for Aerodynamic Flow-Field Prediction Across Geometric and Physical-Fluidic Variations
PY  - 2020
AB  - <jats:title>Abstract</jats:title> <jats:p>The generalizability of a convolutional encoder–decoder based model in predicting aerodynamic flow field across various flow regimes and geometric variation is assessed. A rich master dataset consisting of 11,000+ simulations including cambered, uncambered, thin, and thick airfoils simulated at varying angles of attack is generated. The various Mach and Reynolds number (Re) chosen allows analysis across compressible, incompressible, low, and high Re flow regimes. Multiple studies are carried out with the model trained on datasets that are categorized based on the aforementioned parameters. In each study, the loss of prediction accuracy by training the model on a larger dataset (generalizability), versus a smaller categorically sorted dataset, is evaluated. Largely disparate flow features across the Re range lead to a 25.56% loss, while the generalization across Mach range led to an average of 23.95% loss. However, flow-field changes induced due to geometric variation exhibited a better generalization potential, through an increased accuracy of 12.4%. The encoder–decoder architecture allows extraction of relevant geometric features from largely different geometries (geometric generalization) providing a better out-of-sample prediction accuracy in comparison to physics-based generalization. It is shown that, through user-informed choice of training data (removal of geometrically similar samples), computational costs incurred in generating training data can be reduced. This is important for the application of such methods in the design optimization of platforms and components that require the analysis of the fluid flows.</jats:p>
SP  - NA
EP  - NA
JF  - Journal of Mechanical Design
VL  - 143
IS  - 5
PB  - 
DO  - 10.1115/1.4048221
ER  - 

TY  - NA
AU  - Teo, Theophilus; Hayati, Ashkan F.; Lee, Gun A.; Billinghurst, Mark; Adcock, Matt
TI  - VRST - A Technique for Mixed Reality Remote Collaboration using 360 Panoramas in 3D Reconstructed Scenes
PY  - 2019
AB  - Mixed Reality (MR) remote collaboration provides an enhanced immersive experience where a remote user can provide verbal and nonverbal assistance to a local user to increase the efficiency and performance of the collaboration. This is usually achieved by sharing the local user's environment through live 360 video or a 3D scene, and using visual cues to gesture or point at real objects allowing for better understanding and collaborative task performance. While most of prior work used one of the methods to capture the surrounding environment, there may be situations where users have to choose between using 360 panoramas or 3D scene reconstruction to collaborate, as each have unique benefits and limitations. In this paper we designed a prototype system that combines 360 panoramas into a 3D scene to introduce a novel way for users to interact and collaborate with each other. We evaluated the prototype through a user study which compared the usability and performance of our proposed approach to live 360 video collaborative system, and we found that participants enjoyed using different ways to access the local user's environment although it took them longer time to learn to use our system. We also collected subjective feedback for future improvements and provide directions for future research.
SP  - 23
EP  - NA
JF  - 25th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359996.3364238
ER  - 

TY  - NA
AU  - Tian, Zhiqiang; Gao, Peng; Yang, Lu; Liu, Junjian; Zhang, Xi; Zhan, Junyao; Lai, Yuandu; Zhou, Jiawang
TI  - Augmented Reality in Telecom Industry: Concepts, Technologies and Applications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00012
ER  - 

TY  - NA
AU  - Chan, Edwin; Seyed, Teddy; Stuerzlinger, Wolfgang; Yang, Xing-Dong; Maurer, Frank
TI  - CHI - User Elicitation on Single-hand Microgestures
PY  - 2016
AB  - Gestural interaction has become increasingly popular, as enabling technologies continue to transition from research to retail. The mobility of miniaturized (and invisible) technologies introduces new uses for gesture recognition. This paper investigates single-hand microgestures (SHMGs), detailed gestures in a small interaction space. SHMGs are suitable for the mobile and discrete nature of interactions for ubiquitous computing. However, there has been a lack of end-user input in the design of such gestures. We performed a user-elicitation study with 16 participants to determine their preferred gestures for a set of referents. We contribute an analysis of 1,632 gestures, the resulting gesture set, and prevalent conceptual themes amongst the elicited gestures. These themes provide a set of guidelines for gesture designers, while informing the designs of future studies. With the increase in hand-tracking and electronic devices in our surroundings, we see this as a starting point for designing gestures suitable to portable ubiquitous computing.
SP  - 3403
EP  - 3414
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858589
ER  - 

TY  - NA
AU  - He, Zhenyi; Rosenberg, Karl; Perlin, Ken
TI  - Exploring Configurations for Multi-user Communication in Virtual Reality
PY  - 2019
AB  - Virtual Reality (VR) enables users to collaborate while exploring scenarios not realizable in the physical world. We propose CollabVR, a distributed multi-user collaboration environment, to explore how digital content improves expression and understanding of ideas among groups. To achieve this, we designed and examined three possible configurations for participants and shared manipulable objects. In configuration (1), participants stand side-by-side. In (2), participants are positioned across from each other, mirrored face-to-face. In (3), called "eyes-free," participants stand side-by-side looking at a shared display, and draw upon a horizontal surface. We also explored a "telepathy" mode, in which participants could see from each other's point of view. We implemented "3DSketch" visual objects for participants to manipulate and move between virtual content boards in the environment. To evaluate the system, we conducted a study in which four people at a time used each of the three configurations to cooperate and communicate ideas with each other. We have provided experimental results and interview responses.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Boldu, Roger; Dancu, Alexandru; Matthies, Denys J.C.; Cascón, Pablo Gallego; Ransir, Shanaka; Nanayakkara, Suranga
TI  - SUI - Thumb-In-Motion: Evaluating Thumb-to-Ring Microgestures for Athletic Activity
PY  - 2018
AB  - Spatial User Interfaces, such as wearable fitness trackers are widely used to monitor and improve athletic performance. However, most fitness tracker interfaces require bimanual interactions, which significantly impacts the user's gait and pace. This paper evaluated a one-handed thumb-to-ring gesture interface to quickly access information without interfering with physical activity, such as running. By a pilot study, the most minimal gesture set was selected, particularly those that could be executed reflexively to minimize distraction and cognitive load. The evaluation revealed that among the selected gestures, the tap, swipe-down, and swipe-left were the most 'easy to use'. Interestingly, motion does not have a significant effect on the ease of use or on the execution time. However, interacting in motion was subjectively rated as more demanding. Finally, the gesture set was evaluated in real-world applications, while the user performed a running exercise and simultaneously controlled a lap timer, a distance counter, and a music player.
SP  - 150
EP  - 157
JF  - Proceedings of the Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267782.3267796
ER  - 

TY  - NA
AU  - Gong, Taesik; Cho, Hyunsung; Lee, Bowon; Lee, Sung-Ju
TI  - MobiSys - Real-Time Object Identification with a Smartphone Knock (video)
PY  - 2019
AB  - We propose Knocker, a real-time object identification technique with smartphones. Knocker leverages unique impulse signals that are generated by knocking on an object with a smartphone. Knocker does not require any special augmentation for both smartphones and objects.
SP  - 703
EP  - 704
JF  - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3307334.3328593
ER  - 

TY  - NA
AU  - Zhang, Yang; Kienzle, Wolf; Yanjun, Ma; Ng, Shiu S.; Benko, Hrvoje; Harrison, Chris
TI  - UIST - ActiTouch: Robust Touch Detection for On-Skin AR/VR Interfaces
PY  - 2019
AB  - Contemporary AR/VR systems use in-air gestures or handheld controllers for interactivity. This overlooks the skin as a convenient surface for tactile, touch-driven interactions, which are generally more accurate and comfortable than free space interactions. In response, we developed ActiTouch, a new electrical method that enables precise on-skin touch segmentation by using the body as an RF waveguide. We combine this method with computer vision, enabling a system with both high tracking precision and robust touch detection. Our system requires no cumbersome instrumentation of the fingers or hands, requiring only a single wristband (e.g., smartwatch) and sensors integrated into an AR/VR headset. We quantify the accuracy of our approach through a user study and demonstrate how it can enable touchscreen-like interactions on the skin.
SP  - 1151
EP  - 1159
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347869
ER  - 

TY  - NA
AU  - Erdelyi, Viktor; Rizk, Hamada; Yamaguchi, Hirozumi; Higashino, Teruo
TI  - ICDCN (Adjunct Volume) - Learn to See: A Microwave-based Object Recognition System Using Learning Techniques
PY  - 2021
AB  - The capability to recognize nearby objects automatically has numerous applications including asset tracking, lifestyle analysis, and navigation assistance for blind people. In recent years, several approaches were proposed, but they are either limited to electric objects or objects instrumented with tags, which cannot scale. There are also acoustic or vision-based techniques for recognizing uninstrumented objects, but they may have privacy issues. In this paper, we present a microwave-based object detection and recognition approach. Specifically, the proposed system leverages Universal Software Radio Peripherals (USRPs) to transmit microwave signals through the target object and capture them on the opposite side. To reduce the privacy impact, we use a single antenna for receiving a single-pixel “image”. Then, a Random Forest classifier learns the characteristics of the received signals altered by a given object, enabling object recognition. Using a wide range of microwave frequencies, we evaluated the proposed system’s capability to detect and differentiate between four different objects of different materials. The evaluation results show that, using only a signal, the system can correctly detect the presence of the object 98.7% of the time. The system can also differentiate between different objects 92% of the time.
SP  - 145
EP  - 150
JF  - Adjunct Proceedings of the 2021 International Conference on Distributed Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3427477.3429459
ER  - 

TY  - BOOK
AU  - Carter, Jason; Dewan, Prasun
TI  - SEmotion@ICSE - Contextualizing inferred programming difficulties: extended abstract
PY  - 2018
AB  - Communicating1 automatically inferred programming difficulty to appropriate observers can promote help if they have enough context to determine whether they should and can offer help. Buffered workspace awareness keeps a segment of the developer's actions around an inferred difficulty as context, and allows the recording to be played a single or multiple times. Semi-structured interviews with mentor-intern pairs in a large company showed that they liked the general idea of buffered workspace awareness. We performed a two-phase user study where observers used both single- and multi-pass awareness to determine the problems developers' had and the solution to those problems. Almost all solutions required a one-line fix. We could find no statistical difference in the correctness of their solutions in the two conditions, though the observers overwhelmingly preferred and were more confident using the multi-pass mechanism, and made use of its rewind and pause commands. Both kinds of mechanisms allowed the observers to solve the majority of problems by looking only at 5 minutes of the workers' interaction before the difficulty. The time spent by them processing the context was a small fraction of the time spent by the developers on the difficulties. The time wasted on abandoned difficulties was a small fraction of the time spent on difficulties.
SP  - 32
EP  - 38
JF  - Proceedings of the 3rd International Workshop on Emotion Awareness in Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3194932.3194937
ER  - 

TY  - JOUR
AU  - Thanyadit, Santawat; Punpongsanon, Parinya; Piumsomboon, Thammathip; Pong, Ting-Chuen
TI  - XR-LIVE: Enhancing Asynchronous Shared-Space Demonstrations with Spatial-temporal Assistive Toolsets for Effective Learning in Immersive Virtual Laboratories
PY  - 2022
AB  - <jats:p>An immersive virtual laboratory (VL) could offer flexibility of time and space, as well as safety, for remote students to conduct laboratory activities through online experiential learning. Recording an instructor's demonstration inside a VL is an approach that allows students to learn directly from a demonstration. However, students have to learn from a recording while controlling the playback, which requires attention spent on additional spatial and temporal cues. This additional cognitive load could lead to errors during the laboratory procedure. To address these challenges, we have identified four design requirements to reduce attention load in VLs; namely, organized learning steps, improved student sense of co-presence, reduction of task-instructor split-attention, and learning independent of interpersonal distance. Based on these requirements, we have designed and implemented spatial-temporal assistive toolsets for laboratories in a virtual environment, namely XR-LIVE, to reduce cognitive load and enhance learning in an asynchronous shared-space demonstration, implemented based on the setup of a standard civil engineering laboratory. We also analyzed students' behavior in the VL demonstration to design guidelines applicable to generic VLs.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512983
ER  - 

TY  - NA
AU  - Huo, Ke; Cao, Yuanzhi; Yoon, Sang Ho; Xu, Zhuangying; Chen, Guiming; Ramani, Karthik
TI  - CHI - Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes
PY  - 2018
AB  - The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.
SP  - 219
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173793
ER  - 

TY  - NA
AU  - Gong, Jun; Xu, Zheer; Guo, Qifan; Seyed, Teddy; Chen, Xiang 'Anthony'; Bi, Xiaojun; Yang, Xing-Dong
TI  - CHI - WrisText: One-handed Text Entry on Smartwatch using Wrist Gestures
PY  - 2018
AB  - We present WrisText - a one-handed text entry technique for smartwatches using the joystick-like motion of the wrist. A user enters text by whirling the wrist of the watch hand, towards six directions which each represent a key in a circular keyboard, and where the letters are distributed in an alphabetical order. The design of WrisText was an iterative process, where we first conducted a study to investigate optimal key size, and found that keys needed to be 55o or wider to achieve over 90% striking accuracy. We then computed an optimal keyboard layout, considering a joint optimization problem of striking accuracy, striking comfort, word disambiguation. We evaluated the performance of WrisText through a five-day study with 10 participants in two text entry scenarios: hand-up and hand-down. On average, participants achieved a text entry speed of 9.9 WPM across all sessions, and were able to type as fast as 15.2 WPM by the end of the last day.
SP  - 181
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173755
ER  - 

TY  - NA
AU  - Kaya, Sinem Guven; Zhou, Bing; R, Arora Rohan; Zheutlin, Noah; Randolph, Vanloo Gerard; Eyigöz, Elif
TI  - ISMAR Adjunct - Dynamic Content Generation for Augmented Technical Support
PY  - 2021
AB  - In the hardware technical support domain, scaling technician skills remains a prevalent problem. Given the large portfolio of hardware products service providers need to maintain, it is not possible for every technician to be expert at repairing every product. Augmented reality addresses this problem through virtual procedures, which are interactive 3D visual representations of text-based knowledge articles that describe how to perform step-by-step repair actions. Virtual procedures, thus, equip the technicians with the skills they need to support a wide range of hardware products. In this paper, we present a novel and scalable approach to dynamically construct virtual procedures, and we demonstrate the feasibility of our approach through a real-life implementation.
SP  - 441
EP  - 446
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct54149.2021.00101
ER  - 

TY  - JOUR
AU  - Wang, Peng; Bai, Xiaoliang; Billinghurst, Mark; Zhang, Shusheng; Wei, Sili; Xu, Guangyao; He, Weiping; Zhang, Xiangyu; Zhang, Jie
TI  - 3DGAM: using 3D gesture and CAD models for training on mixed reality remote collaboration
PY  - 2020
AB  - As Virtual Reality(VR), Augmented Reality(AR), Mixed Reality(MR) technology becomes more accessible, it is important to explore VR/AR/MR technologies that can be used for remote collaboration on physical tasks. Previous research has shown that gesture-based interaction is intuitive and expressive for remote collaboration, and using 3D CAD models can provide clear instructions for assembly tasks. In this paper, therefore, we describe a new MR remote collaboration system which combines the use of gesture and CAD models in a complementary manner. The prototype system enables a remote expert in VR to provide instructions based on 3D gesture and CAD models (3DGAM) for a local worker who uses AR to see these instructions. Using this interface, we conducted a formal user study to explore the effect of sharing 3D gesture and CAD models in an assembly training task. We found that the combination of 3D gesture and CAD models can improve remote collaboration on an assembly task with respect to the performance time and user experience. Finally, we provide some conclusions and directions for future research.
SP  - 31059
EP  - 31084
JF  - Multimedia Tools and Applications
VL  - 80
IS  - 20
PB  - 
DO  - 10.1007/s11042-020-09731-7
ER  - 

TY  - JOUR
AU  - Chen, Taizhou; Li, Tianpei; Yang, Xingyu; Zhu, Kening
TI  - EFRing
PY  - 2022
AB  - <jats:p>We present EFRing, an index-finger-worn ring-form device for detecting thumb-to-index-finger (T2I) microgestures through the approach of electric-field (EF) sensing. Based on the signal change induced by the T2I motions, we proposed two machine-learning-based data-processing pipelines: one for recognizing/classifying discrete T2I microgestures, and the other for tracking continuous 1D T2I movements. Our experiments on the EFRing microgesture classification showed an average within-user accuracy of 89.5% and an average cross-user accuracy of 85.2%, for 9 discrete T2I microgestures. For the continuous tracking of 1D T2I movements, our method can achieve the mean-square error of 3.5% for the generic model and 2.3% for the personalized model. Our 1D-Fitts'-Law target-selection study shows that the proposed tracking method with EFRing is intuitive and accurate for real-time usage. Lastly, we proposed and discussed the potential applications for EFRing.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569478
ER  - 

TY  - NA
AU  - Han, Hao; Ma, Xiaojun; Oyama, Keizo
TI  - ICIS - Towards detecting and predicting fall events in elderly care using bidirectional electromyographic sensor network
PY  - 2016
AB  - Falling is one of the most serious life-threatening events for the elders, and the ICT-based solution plays a key role in addressing this problem prevalently. In this paper, four principles are proposed as fundamental criteria for designing a sensor network for elder-oriented fall detection and prediction. According to these criteria, a bidirectional electromyographic sensor network model is experimentally constructed, and qualitative analysis is conducted to explain that this solution performs more realistically and rationally.
SP  - 1
EP  - 6
JF  - 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icis.2016.7550897
ER  - 

TY  - NA
AU  - Kim, Meejin; Lee, Sung-Hee
TI  - Web3D - Deictic Gesture Retargeting for Telepresence Avatars in Dissimilar Object and User Arrangements
PY  - 2020
AB  - The avatar-mediated 3D telepresence system aims to enable a user in local space to interact with a remote user through a virtual avatar representing the remote user. In this system, the avatar’s movement should convey the meaning of the remote user’s movement to the local space. If the local and remote spaces have different spatial configurations, the remote user’s placement and motion should be adapted to the local space so as to preserve the motion semantics of the remote user. To this end, this paper presents a method to generate the placement and deictic gestures of the avatar. First, we develop a method to predict the probability of the avatar’s placement in the local space according to the placement of the remote user. From this, we find the optimal placement of the avatar given the configuration of the local space and the remote user’s placement. Second, we develop a simple yet effective method to retarget the remote user’s deictic gesture to his/her avatar. User study shows that our method improves user engagement and social presence in the tested telepresence scenarios.
SP  - NA
EP  - NA
JF  - The 25th International Conference on 3D Web Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3424616.3424693
ER  - 

TY  - NA
AU  - Buddhika, Thisum; Zhang, Haimo; Chan, Samantha W. T.; Dissanayake, Vipula; Nanayakkara, Suranga; Zimmermann, Roger
TI  - AH - fSense: Unlocking the Dimension of Force for Gestural Interactions using Smartwatch PPG Sensor
PY  - 2019
AB  - While most existing gestural interfaces focus on the static posture or the dynamic action of the hand, few have investigated the feasibility of using the forces that are exerted while performing gestures. Using the photoplethysmogram (PPG) sensor of off-the-shelf smartwatches, we show that, it is possible to recognize the force of a gesture as an independent channel of input. Based on a user study with 12 participants, we found that users were able to reliably produce two levels of force across several types of common gestures. We demonstrate a few interaction scenarios where the force is either used as a standalone input or to complement existing input modalities.
SP  - 11
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311839
ER  - 

TY  - CHAP
AU  - Yamamoto, Kazuhiko; Igarashi, Takeo
TI  - 3D Spatial Sound Individualization with Perceptual Feedback
PY  - 2021
AB  - Designing an interactive system tailored appropriately for each user’s physical and cognitive characteristics is important for providing optimal user experience. In this chapter, we discuss how we could address such problems leveraging modern interactive machine learning techniques. As a case study, we introduce a method to individualize 3D spatial sound rendering with perceptual feedback. 3D spatial sound rendering traditionally required time-consuming measurement of individual user using an expensive device. By taking data-driven approach, one can replace such expensive measurement with simple calibration. We first describe how to train a generic deep learning model with an existing measured data set. We then describe how to adapt the model to a specific user with simple calibration process consisting of pairwise comparisons. Through this case study, the readers will get insight on how to adapt an interactive system for a specific user’s characteristics, taking advantage of the high expressiveness of modern machine learning techniques.
SP  - 571
EP  - 595
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_17
ER  - 

TY  - NA
AU  - Colley, Ashley; Inget, Virve; Rantala, Inka; Häkkilä, Jonna
TI  - MUM - Investigating interaction with a ring form factor
PY  - 2017
AB  - In this note, we study interaction with a finger worn ring, focusing on interaction enabled by the ring form factor. We report on 2 user studies, the first (n=13) investigating preferences for different interactions, whilst the second (n=7) explores usage contexts and applications. Twelve different ways of interacting with a ring were evaluated, including e.g. changing the placement of the ring on the fingers and moving the ring along or around a finger. Based the study results, the practical usability and concerns with each of the ring interactions is discussed. Whereas the concept was generally well received, the main concerns related to false positives, losing the ring e.g. when changing finger, and limitations from the rigid size of the form factor.
SP  - 107
EP  - 111
JF  - Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3152832.3152870
ER  - 

TY  - NA
AU  - Jeffries, Bryn; Baldwin, Timothy; Zalk, Marion; Taylor, Ben
TI  - ACE - Online Tutoring to Support Programming Exercises
PY  - 2020
AB  - Programming exercises lend themselves to learning environments that can provide automated testing and formative feedback to allow students to learn independently and at their own pace. Even so, there are still situations where there is no substitute for interaction with a human expert to provide guidance and encouragement to students. We describe how online tutoring is used within two very different contexts on the same platform. While both contexts use a specific learning environment, many of the experiences encountered in providing online tutoring support provide important lessons for educators that generalise to using any comparable environment.
SP  - 56
EP  - 65
JF  - Proceedings of the Twenty-Second Australasian Computing Education Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3373165.3373172
ER  - 

TY  - NA
AU  - Chen, Yan; Oney, Steve; Lasecki, Walter S.
TI  - CHI - Towards Providing On-Demand Expert Support for Software Developers
PY  - 2016
AB  - Software development is an expert task that requires complex reasoning and the ability to recall language or API-specific details. In practice, developers often seek support from IDE tools, Web resources, or other developers to help fill in gaps in their knowledge on-demand. In this paper, we present two studies that seek to inform the design of future systems that use remote experts to support developers on demand. The first explores what types of questions developers would ask a hypothetical assistant capable of answering any question they pose. The second study explores the interactions between developers and remote experts in supporting roles. Our results suggest eight key system features needed for on-demand remote developer assistants to be effective, which has implications for future human-powered development tools.
SP  - 3192
EP  - 3203
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858512
ER  - 

TY  - JOUR
AU  - Liu, Jen-Shuo; Tversky, Barbara; Feiner, Steven
TI  - Precueing Object Placement and Orientation for Manual Tasks in Augmented Reality.
PY  - 2022
AB  - When a user is performing a manual task, AR or VR can provide information about the current subtask (cueing) and upcoming subtasks (precueing) that makes them easier and faster to complete. Previous research on cueing and precueing in AR and VR has focused on path-following tasks requiring simple actions at each of a series of locations, such as pushing a button or just visiting. We consider a more complex task, whose subtasks involve moving to and picking up an item, moving that item to a designated place while rotating it to a specific angle, and depositing it. We conducted two user studies to examine how people accomplish this task while wearing an AR headset, guided by different visualizations that cue and precue movement and rotation. Participants performed best when given movement information for two successive subtasks and rotation information for a single subtask. In addition, participants performed best when the rotation visualization was split across the manipulated object and its destination.
SP  - 3799
EP  - 3809
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203111
ER  - 

TY  - NA
AU  - Voit, Alexandra; Schneegass, Stefan
TI  - MUM - FabricID: using smart textiles to access wearable devices
PY  - 2017
AB  - Wearable devices like smart watches or eye-wear computers are storing a myriad of personal information. Today, wearable devices support input techniques such as speech and gesture input. These user input methods however, are not well suited for authentication. With the development of smart textiles the design space for interaction for small smart devices can be increased. In this paper, we present the concept of FabricID a system which identifies users' hand-prints with a smart textile integrated into the sleeve of the user. To evaluate our concept, we recorded hand-prints of 16 users. We classified all recorded hand-prints and received an identification rate of 82.5% for all 16 users and on average 93.62% for groups of 4 users.
SP  - 379
EP  - 385
JF  - Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3152832.3156622
ER  - 

TY  - JOUR
AU  - Kim, Myoung Gon; Ryu, JiSeok; Son, Jaemin; Han, JungHyun
TI  - Virtual object sizes for efficient and convenient mid-air manipulation.
PY  - 2022
AB  - <AbstractText>It has been taken for granted that the sizes of virtual objects affect the efficiency and convenience of mid-air manipulation in immersive virtual environments. If a virtual object is too small or too large, for example, manipulating it becomes a difficult task. Nevertheless, the virtual object sizes that are optimal and convenient have rarely been studied. In this paper, we select a virtual object with many distinct geometric features and conduct user studies via docking tasks. Through the user studies, the optimal and convenient sizes for mid-air manipulation are estimated. In order to verify the results, a proxy-based manipulation method is designed and implemented, where the proxy is created with the estimated optimal size. The test based on the method shows that the optimal-size proxy enables users to manipulate efficiently virtual objects and the estimated range of convenient sizes is also preferred by the users.</AbstractText> <AbstractText Label="Supplementary Information" NlmCategory="UNASSIGNED">The online version contains supplementary material available at 10.1007/s00371-022-02555-6.</AbstractText> <CopyrightInformation>© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022.</CopyrightInformation>
SP  - 3463
EP  - 3474
JF  - The Visual computer
VL  - 38
IS  - 9-10
PB  - 
DO  - 10.1007/s00371-022-02555-6
ER  - 

TY  - JOUR
AU  - Villanueva, Ana; Zhu, Zhengzhe; Liu, Ziyi; Wang, Feiyang; Chidambaram, Subramanian; Ramani, Karthik
TI  - ColabAR: A Toolkit for Remote Collaboration in Tangible Augmented Reality Laboratories
PY  - 2022
AB  - <jats:p>Current times are accelerating new technologies to provide high-quality education for remote collaboration, as well as hands-on learning. This is particularly important in the case of laboratory-based classes, which play an essential role in STEM education. In this paper, we introduce ColabAR, a toolkit that uses physical proxies to manipulate virtual objects in Tangible Augmented Reality (TAR) laboratories. ColabAR introduces haptic-based customizable interaction techniques to promote remote collaboration between students. Our toolkit provides hardware and software that enable haptic feedback to improve user experience and promote collaboration during learning. Also, we present the architecture of our cloud platform for haptic interaction that supports information sharing between students in a TAR laboratory. We performed two user studies (N=40) to test the effect of our toolkit in enriching local and remote collaborative experiences. Finally, we demonstrated that our TAR laboratory enables students' performance (i.e., lab completion rate, lab scores) to be similar to their performance in an in-person laboratory.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512928
ER  - 

TY  - NA
AU  - Wu, Erwin; Yuan, Ye; Yeo, Hui-Shyong; Quigley, Aaron; Koike, Hideki; Kitani, Kris M.
TI  - UIST - Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-worn Camera via Dorsum Deformation Network
PY  - 2020
AB  - The automatic recognition of how people use their hands and fingers in natural settings -- without instrumenting the fingers -- can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fingers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress finger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the first vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81 degree for user-specific models and 9.77 degree for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer.
SP  - 1147
EP  - 1160
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415897
ER  - 

TY  - JOUR
AU  - Liu, Ziwei; Lin, Feng; Wang, Chao; Shen, Yijie; Ba, Zhongjie; Lu, Li; Xu, Wenyao; Ren, Kui
TI  - CamRadar
PY  - 2022
AB  - <jats:p>Hidden cameras in sensitive locations have become an increasing threat to personal privacy all over the world. Because the camera is small and camouflaged, it is difficult to detect the presence of the camera with naked eyes. Existing works on this subject have either only covered using wireless transmission to detect cameras, or using other methods which are cumbersome in practical use. In this paper, we introduce a new direction that leverages the unintentional electromagnetic (EM) emanations of the camera to detect it. We first find that the digital output of the camera's image sensor will be amplitude-modulated to the EM emanations of the camera's clock. Thus, changes in the scope of the camera will directly cause changes in the camera's EM emanations, which constitutes a unique characteristic for a hidden camera. Based on this, we propose a novel camera detection system named CamRadar, which can filter out potential camera EM emanations from numerous EM signals quickly and achieve accurate hidden camera detection. Benefitting from the camera's EM emanations, CamRadar will not be limited by the camera transmission types or the detection angle. Our extensive real-world experiments using CamRadar and 19 hidden cameras show that CamRadar achieves a fast detection (in 16.75s) with a detection rate of 93.23% as well as a low false positive rate of 3.95%.</jats:p>
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569505
ER  - 

TY  - JOUR
AU  - Gharaee, Zahra; Kowshik, Shreyas; Stromann, Oliver; Felsberg, Michael
TI  - Graph representation learning for road type classification
PY  - 2021
AB  - Abstract We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks. Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning. Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN 1 . Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.
SP  - 108174
EP  - NA
JF  - Pattern Recognition
VL  - 120
IS  - NA
PB  - 
DO  - 10.1016/j.patcog.2021.108174
ER  - 

TY  - NA
AU  - Palani, Srishti; Ledo, David; Fitzmaurice, George; Anderson, Fraser
TI  - "I don't want to feel like I'm working in a 1960s factory": The Practitioner Perspective on Creativity Support Tool Adoption
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501933
ER  - 

TY  - NA
AU  - Delabrida, Saul; Billinghurst, Mark; Thomas, Bruce H.; Rabelo, Ricardo Augusto; Ribeiro, Sérvio Pontes
TI  - SIGGRAPH ASIA Mobile Graphics and Interactive Applications - Design of a wearable system for 3D data acquisition and reconstruction for tree climbers
PY  - 2017
AB  - Ecologists often need to extract data from forests to determine the flora and fauna conditions. Tree climbing is one technique used for this. Climbers go to the top of the tree and use hand held equipment to collect and make notes while they are descending. This process demands expert climbing skills and biology expertise, which are two characteristics not commonly found. This paper describes the design of mobile system for 3D data collection and reconstruction of the field research environment for remote evaluation. The prototype can be used to reduce the time spent up trees and collected data that can be used for crowd-sourced evaluation.
SP  - 26
EP  - NA
JF  - SIGGRAPH Asia 2017 Mobile Graphics & Interactive Applications on - SA '17
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132787.3139198
ER  - 

TY  - NA
AU  - Gong, Jun; Yang, Xing-Dong; Irani, Pourang
TI  - UIST - WristWhirl: One-handed Continuous Smartwatch Input using Wrist Gestures
PY  - 2016
AB  - We propose and study a new input modality, WristWhirl, that uses the wrist as an always-available joystick to perform one-handed continuous input on smartwatches. We explore the influence of the wrist's bio-mechanical properties for performing gestures to interact with a smartwatch, both while standing still and walking. Through a user study, we examine the impact of performing 8 distinct gestures (4 directional marks, and 4 free-form shapes) on the stability of the watch surface. Participants were able to perform directional marks using the wrist as a joystick at an average rate of half a second and free-form shapes at an average rate of approximately 1.5secs. The free-form shapes could be recognized by a $1 gesture recognizer with an accuracy of 93.8% and by three human inspectors with an accuracy of 85%. From these results, we designed and implemented a proof-of-concept device by augmenting the watchband using an array of proximity sensors, which can be used to draw gestures with high quality. Finally, we demonstrate a number of scenarios that benefit from one-handed continuous input on smartwatches using WristWhirl.
SP  - 861
EP  - 872
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984563
ER  - 

TY  - JOUR
AU  - Shimizu, Evan; Paris, Sylvain; Fisher, Matthew; Yumer, Ersin; Fatahalian, Kayvon
TI  - Exploratory Stage Lighting Design using Visual Objectives
PY  - 2019
AB  - NA
SP  - 417
EP  - 429
JF  - Computer Graphics Forum
VL  - 38
IS  - 2
PB  - 
DO  - 10.1111/cgf.13648
ER  - 

TY  - NA
AU  - Le, Huy Viet; Mayer, Sven; Henze, Niels
TI  - IUI - Investigating the feasibility of finger identification on capacitive touchscreens using deep learning
PY  - 2019
AB  - Touchscreens enable intuitive mobile interaction. However, touch input is limited to 2D touch locations which makes it challenging to provide shortcuts and secondary actions similar to hardware keyboards and mice. Previous work presented a wide range of approaches to provide secondary actions by identifying which finger touched the display. While these approaches are based on external sensors which are inconvenient, we use capacitive images from mobile touchscreens to investigate the feasibility of finger identification. We collected a dataset of low-resolution fingerprints and trained convolutional neural networks that classify touches from eight combinations of fingers. We focused on combinations that involve the thumb and index finger as these are mainly used for interaction. As a result, we achieved an accuracy of over 92% for a position-invariant differentiation between left and right thumbs. We evaluated the model and two use cases that users find useful and intuitive. We publicly share our data set (CapFingerld) comprising 455,709 capacitive images of touches from each finger on a representative mutual capacitive touchscreen and our models to enable future work using and improving them.
SP  - 637
EP  - 649
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302295
ER  - 

TY  - NA
AU  - Parnami, Aman
TI  - Enabling In Situ & Context-Based Motion Gesture Design
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Matthies, Denys J.C.; Elvitigala, Don Samitha; Muthukumarana, Sachith; Huber, Jochen; Nanayakkara, Suranga
TI  - AH - CapMat: A Smart Foot Mat for User Authentication
PY  - 2019
AB  - We present CapMat, a smart foot mat that enables user identification, supporting applications such as multi-layer authentication. CapMat leverages a large form factor capacitive sensor to capture shoe sole images. These images vary based on shoe form factors, the individual wear, and the user's weight. In a preliminary evaluation, we distinguished 15 users with an accuracy of up to 100%.
SP  - 42
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311874
ER  - 

TY  - BOOK
AU  - Bezzateev, Sergey; Afanasyeva, Aleksandra; Voloshina, Natalia; Ometov, Aleksandr
TI  - AWICT - Multi-factor Authentication for Wearables: Configuring System Parameters with Risk Function
PY  - 2017
AB  - The users of today are already about to enter the era of highly integrated modern wearable devices -- the time when smart accessories will, in turn, push aside regular Smartphones and Tablets bringing a variety of new security challenges. The number of simultaneously used bio-sensors, both integrated into smart wearables and connected over wireless interfaces, allows novel opportunities for Multi-factor Authentication (MFA) of the user. This manuscript proposes a solution for configuring the MFA based on the average direct and indirect losses risk analysis. The example application of Bayesian function for MFA presents the applicability of the proposed framework for the utilization with wearables.
SP  - 1
EP  - 7
JF  - Proceedings of the Second International Conference on Advanced Wireless Information, Data, and Communication Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3231830.3231834
ER  - 

TY  - NA
AU  - Yang, Chouchang; Sample, Alanson P.
TI  - EM-ID: Tag-less identification of electrical devices via electromagnetic emissions
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - 2016 IEEE International Conference on RFID (RFID)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/rfid.2016.7488014
ER  - 

TY  - NA
AU  - Wang, Bingyue
TI  - Learning Device Usage in Context: A Continuous and Hierarchical Smartphone Authentication Scheme
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Matejka, Justin; Glueck, Michael; Bradner, Erin; Hashemi, Ali B.; Grossman, Tovi; Fitzmaurice, George
TI  - CHI - Dream Lens: Exploration and Visualization of Large-Scale Generative Design Datasets
PY  - 2018
AB  - This paper presents Dream Lens, an interactive visual analysis tool for exploring and visualizing large-scale generative design datasets. Unlike traditional computer aided design, where users create a single model, with generative design, users specify high-level goals and constraints, and the system automatically generates hundreds or thousands of candidates all meeting the design criteria. Once a large collection of design variations is created, the designer is left with the task of finding the design, or set of designs, which best meets their requirements. This is a complicated task which could require analyzing the structural characteristics and visual aesthetics of the designs. Two studies are conducted which demonstrate the usability and usefulness of the Dream Lens system, and a generatively designed dataset of 16,800 designs for a sample design problem is described and publicly released to encourage advancement in this area.
SP  - 369
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173943
ER  - 

TY  - NA
AU  - Chen, Lei; Liu, Yilin; Li, Yue; Yu, Lingyun; Gao, BoYu; Caon, Maurizio; Yue, Yong; Liang, Hai-Ning
TI  - Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality Collaboration.
PY  - 2021
AB  - Visual cues are essential in computer-mediated communication. It is especially important when communication happens in a collaboration scenario that requires focusing several users' attention on aspecific object among other similar ones. This paper explores the effect of visual cues on pointing tasks in co-located Augmented Reality (AR) collaboration. A user study (N = 32, 16 pairs) was conducted to compare two types of visual cues: Pointing Line (PL)and Moving Track (MT). Both are head-based visual techniques.Through a series of collaborative pointing tasks on objects with different states (static and dynamic) and density levels (low, mediumand high), the results showed that PL was better on task performance and usability, but MT was rated higher on social presenceand user preference. Based on our results, some design implicationsare provided for pointing tasks in co-located AR collaboration.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Matsumoto, Nagisa; Fujita, Koji; Sugiura, Yuta
TI  - Estimation of grip strength using monocular camera for home-based hand rehabilitation
PY  - 2021
AB  - Grip strength exercises are commonly used rehabilitation methods for recovery of hand function. They are easy to perform even without the direct support of a healthcare professional. However, witho...
SP  - 1
EP  - 11
JF  - SICE Journal of Control, Measurement, and System Integration
VL  - 14
IS  - 1
PB  - 
DO  - 10.1080/18824889.2020.1863612
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Soares, Gustavo; Glassman, Elena L.; Head, Andrew; D'Antoni, Loris; Hartmann, Björn
TI  - CHI Extended Abstracts - Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments
PY  - 2017
AB  - For massive programming classrooms, recent advances in program synthesis offer means to automatically grade and debug student submissions, and generate feedback at scale. A key challenge for synthesis-based autograders is how to design personalized feedback for students that is as effective as manual feedback given by teachers today. To understand the state of hint-giving practice, we analyzed 132 online Q&A posts and conducted a semi-structured interview with a teacher from a local massive programming class. We identified five types of teacher hints that can also be generated by program synthesis. These hints describe transformations, locations, data, behavior, and examples. We describe our implementation of three of these hint types. This work paves the way for future deployments of automatic, pedagogically-useful programming hints driven by program synthesis.
SP  - 2951
EP  - 2958
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053187
ER  - 

TY  - NA
AU  - Andersen, Daniel; Popescu, Voicu
TI  - ISMAR - AR Interfaces for Mid-Air 6-DoF Alignment: Ergonomics-Aware Design and Evaluation
PY  - 2020
AB  - Aligning hand-held objects into mid-air positions and orientations is important for many applications. Task performance depends on speed and accuracy, and also on minimizing the user’s physical exertion. Augmented reality head-mounted displays (AR HMDs) can guide users during mid-air alignments by tracking an object’s pose and delivering visual instruction directly into the user’s field of view (FoV). However, it is unclear which AR HMD interfaces are most effective for mid-air alignment guidance, and how the form factor of current AR HMD hardware (such as heaviness and low FoV) affects how users put themselves into tiring body poses during mid-air alignment. We defined a set of design requirements for mid-air alignment interfaces that target reduction of high-exertion body poses during alignment. We then designed, implemented, and tested several interfaces in a user study in which novice participants performed a sequence of mid-air alignments using each interface.Results show that interfaces that rely on visual guidance located near the hand-held object reduce acquisition times and translation errors, while interfaces that involve aiming at a faraway virtual object reduce rotation errors. Users tend to avoid focus shifts and to position the head and arms to maximize how much AR visualization is contained within a single FoV without moving the head. We found that changing the size of visual elements affected how far out the user extends the arm, which affects torque forces. We also found that dynamically adjusting where visual guidance is placed relative to the mid-air pose can help keep the head level during alignment, which is important for distributing the weight of the AR HMD.
SP  - 289
EP  - 300
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00055
ER  - 

TY  - JOUR
AU  - Maity, Shovan; Yang, David; Redford, Scott Stanton; Das, Debayan; Chatterjee, Baibhab; Sen, Shreyas
TI  - BodyWire-HCI: Enabling New Interaction Modalities by Communicating Strictly During Touch Using Electro-Quasistatic Human Body Communication
PY  - 2020
AB  - Communication during touch provides a seamless and natural way of interaction between humans and ambient intelligence. Current techniques that couple wireless transmission with touch detection suffer from the problem of selectivity and security, i.e., they cannot ensure communication only through direct touch and not through close proximity. We present BodyWire-HCI, which utilizes the human body as a wire-like communication channel, to enable human–computer interaction, that for the first time, demonstrates selective and physically secure communication strictly during touch. The signal leakage out of the body is minimized by utilizing a novel, low frequency Electro-QuasiStatic Human Body Communication (EQS-HBC) technique that enables interaction strictly when there is a conductive communication path between the transmitter and receiver through the human body. Design techniques such as capacitive termination and voltage mode operation are used to minimize the human body channel loss to operate at low frequencies and enable EQS-HBC. The demonstrations highlight the impact of BodyWire-HCI in enabling new human–machine interaction modalities for variety of application scenarios such as secure authentication (e.g., opening a door and pairing a smart device) and information exchange (e.g., payment, image, medical data, and personal profile transfer) through touch (https://www.youtube.com/watch?v=Uwrig2XQIH8).
SP  - 1
EP  - 25
JF  - ACM Transactions on Computer-Human Interaction
VL  - 27
IS  - 6
PB  - 
DO  - 10.1145/3406238
ER  - 

TY  - NA
AU  - Schor, Nadav; Katzir, Oren; Zhang, Hao; Cohen-Or, Daniel
TI  - CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition
PY  - 2018
AB  - Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to enable a model to generate a rich variety of samples from the entire target distribution, rather than only from a distribution confined to the training data. In other words, we would like the generative model to go beyond the observed samples and learn to generate ``unseen'', yet still plausible, data. In our work, we present CompoNet, a generative neural network for 2D or 3D shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions. Treating a shape not as an unstructured whole, but as a (re-)composable set of deformable parts, adds a combinatorial dimension to the generative process to enrich the diversity of the output, encouraging the generator to venture more into the ``unseen''. We show that our part-based model generates richer variety of plausible shapes compared with baseline generative models. To this end, we introduce two quantitative metrics to evaluate the diversity of a generative model and assess how well the generated data covers both the training data and unseen data from the same target distribution. Code is available at this https URL.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yoon, Sang Ho; Zhang, Yunbo; Huo, Ke; Ramani, Karthik
TI  - UIST - TRing: Instant and Customizable Interactions with Objects Using an Embedded Magnet and a Finger-Worn Device
PY  - 2016
AB  - We present TRing, a finger-worn input device which provides instant and customizable interactions. TRing offers a novel method for making plain objects interactive using an embedded magnet and a finger-worn device. With a particle filter integrated magnetic sensing technique, we compute the fingertip's position relative to the embedded magnet. We also offer a magnet placement algorithm that guides the magnet installation location based upon the user's interface customization. By simply inserting or attaching a small magnet, we bring interactivity to both fabricated and existing objects. In our evaluations, TRing shows an average tracking error of 8.6 mm in 3D space and a 2D targeting error of 4.96 mm, which are sufficient for implementing average-sized conventional controls such as buttons and sliders. A user study validates the input performance with TRing on a targeting task (92% accuracy within 45 mm distance) and a cursor control task (91% accuracy for a 10 mm target). Furthermore, we show examples that highlight the interaction capability of our approach.
SP  - 169
EP  - 181
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984529
ER  - 

TY  - JOUR
AU  - Liu, Jen-Shuo; Elvezio, Carmine; Tversky, Barbara; Feiner, Steven
TI  - Using Multi-Level Precueing to Improve Performance in Path-Following Tasks in Virtual Reality
PY  - 2021
AB  - Work on VR and AR task interaction and visualization paradigms has typically focused on providing information about the current step (a cue) immediately before or during its performance. Some research has also shown benefits to simultaneously providing information about the next step (a precue). We explore whether it would be possible to improve efficiency by precueing information about multiple upcoming steps before completing the current step. To accomplish this, we developed a remote VR user study comparing task completion time and subjective metrics for different levels and styles of precueing in a path-following task. Our visualizations vary the precueing level (number of steps precued in advance) and style (whether the path to a target is communicated through a line to the target, and whether the place of a target is communicated through graphics at the target). Participants in our study performed best when given two to three precues for visualizations using lines to show the path to targets. However, performance degraded when four precues were used. On the other hand, participants performed best with only one precue for visualizations without lines, showing only the places of targets, and performance degraded when a second precue was given. In addition, participants performed better using visualizations with lines than ones without lines.
SP  - 4311
EP  - 4320
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2021.3106476
ER  - 

TY  - JOUR
AU  - Chen, Wei; Fuge, Mark; Chazan, Jonah
TI  - Design Manifolds Capture the Intrinsic Complexity and Dimension of Design Spaces
PY  - 2017
AB  - <jats:p>This paper shows how to measure the intrinsic complexity and dimensionality of a design space. It assumes that high-dimensional design parameters actually lie in a much lower-dimensional space that represents semantic attributes—a design manifold. Past work has shown how to embed designs using techniques like autoencoders; in contrast, the method proposed in this paper first captures the inherent properties of a design space and then chooses appropriate embeddings based on the captured properties. We demonstrate this with both synthetic shapes of controllable complexity (using a generalization of the ellipse called the superformula) and real-world designs (glassware and airfoils). We evaluate multiple embeddings by measuring shape reconstruction error, pairwise distance preservation, and captured semantic attributes. By generating fundamental knowledge about the inherent complexity of a design space and how designs differ from one another, our approach allows us to improve design optimization, consumer preference learning, geometric modeling, and other design applications that rely on navigating complex design spaces. Ultimately, this deepens our understanding of design complexity in general.</jats:p>
SP  - 051102
EP  - NA
JF  - Journal of Mechanical Design
VL  - 139
IS  - 5
PB  - 
DO  - 10.1115/1.4036134
ER  - 

TY  - NA
AU  - Chidambaram, Subramanian; Reddy, Sai Swarup; Rumple, Matthew; Ipsita, Ananya; Villanueva, Ana; Redick, Thomas; Stuerzlinger, Wolfgang; Ramani, Karthik
TI  - EditAR: A Digital Twin Authoring Environment for Creation of AR/VR and Video Instructions from a Single Demonstration
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00048
ER  - 

TY  - JOUR
AU  - Zhang, Xiaotian; He, Weiping; Billinghurst, Mark; Liu, Daisong; Yang, Lingxiao; Feng, Shuo; Liu, Yizhe
TI  - Usability of Cross-Device Interaction Interfaces for Augmented Reality in Physical Tasks
PY  - 2022
AB  - NA
SP  - 1
EP  - 19
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2160537
ER  - 

TY  - NA
AU  - Qian, Xun; He, Fengming; Hu, Xiyun; Wang, Tianyi; Ramani, Karthik
TI  - ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545663
ER  - 

TY  - JOUR
AU  - Cang, Ruijin; Xu, Yaopengxiao; Chen, Shaohua; Liu, Yongming; Jiao, Yang; Ren, Max Yi
TI  - Microstructure Representation and Reconstruction of Heterogeneous Materials Via Deep Belief Network for Computational Material Design
PY  - 2017
AB  - Integrated Computational Materials Engineering (ICME) aims to accelerate optimal design of complex material systems by integrating material science and design automation. For tractable ICME, it is required that (1) a structural feature space be identified to allow reconstruction of new designs, and (2) the reconstruction process be property-preserving. The majority of existing structural presentation schemes rely on the designer's understanding of specific material systems to identify geometric and statistical features, which could be biased and insufficient for reconstructing physically meaningful microstructures of complex material systems. In this paper, we develop a feature learning mechanism based on convolutional deep belief network to automate a two-way conversion between microstructures and their lower-dimensional feature representations, and to achieves a 1000-fold dimension reduction from the microstructure space. The proposed model is applied to a wide spectrum of heterogeneous material systems with distinct microstructural features including Ti-6Al-4V alloy, Pb63-Sn37 alloy, Fontainebleau sandstone, and Spherical colloids, to produce material reconstructions that are close to the original samples with respect to 2-point correlation functions and mean critical fracture strength. This capability is not achieved by existing synthesis methods that rely on the Markovian assumption of material microstructures.
SP  - 071404
EP  - NA
JF  - Journal of Mechanical Design
VL  - 139
IS  - 7
PB  - 
DO  - 10.1115/1.4036649
ER  - 

TY  - CHAP
AU  - Palmarini, Riccardo; del Amo, Iñigo Fernández; Ariansyah, Dedy; Erkoyuncu, John Ahmet; Roy, Rajkumar
TI  - Augmented Reality for Remote Assistance (ARRA)
PY  - 2023
AB  - NA
SP  - 669
EP  - 685
JF  - Springer Handbooks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-67822-7_27
ER  - 

TY  - JOUR
AU  - Varga, Virag; Vakulya, Gergely; Sample, Alanson P.; Gross, Thomas R.
TI  - Enabling Interactive Infrastructure with Body Channel Communication
PY  - 2018
AB  - Body channel communication (BCC) uses the human body to carry signals, and therefore provides communication and localization that are directly tied to human presence and actions. Previous BCC systems were expensive, could operate only in a laboratory, or only focused on special use cases. We present here an end-to-end BCC system that is designed for ambient intelligence. We introduce the BCC infrastructure that consists of portable devices (e.g., a simple sphere), mobile devices (e.g., a smartwatch-like wristband), and stationary devices (e.g., floor/wall tiles). We also describe the core technology that is used in each of these units. The TouchCom hardware-software platform is a simple transceiver with software-centered processing. The focus on software (even the implementation of the physical layer is based on software) allows the adaptivity that is necessary to operate a BCC-based system in practice. The paper describes the design and a prototype implementation of the TouchCom-based interactive infrastructure and provides evidence that this BCC infrastructure works for different persons and different setups. The system provides moderate bandwidth (about 3.5 kb/s) that is suitable for several usage scenarios like games, localization, and identification. The implemented demonstrations illustrate the benefits these applications gain when touching an object is tied to communication.
SP  - 169
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161180
ER  - 

TY  - NA
AU  - Carter, Jason; Dewan, Prasun
TI  - Contextualizing Inferred Programming Difficulties
PY  - 2018
AB  - Communicating automatically inferred programming difficulty to appropriate observers can promote help if they have enough context to determine whether they should and can offer help. Buffered workspace awareness keeps a segment of the developer’s actions around an inferred difficulty as context, and allows the recording to be played a single or multiple times. Semi-structured interviews with mentor-intern pairs in a large company showed that they liked the general idea of buffered workspace awareness. We performed a two-phase user study where observers used both single- and multi-pass awareness to determine the problems developers’ had and the solution to those problems. Almost all solutions required a one-line fix. We could find no statistical difference in the correctness of their solutions in the two conditions, though the observers overwhelmingly preferred and were more confident using the multi-pass mechanism, and made use of its rewind and pause commands. Both kinds of mechanisms allowed the observers to solve the majority of problems by looking only at 5 minutes of the workers’ interaction before the difficulty. The time spent by them processing the context was a small fraction of the time spent by the developers on the difficulties. The time wasted on abandoned difficulties was a small fraction of the time spent on difficulties.
SP  - 32
EP  - 38
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Byun, Jeongmin; Park, Jungkook; Oh, Alice
TI  - Cocode: Providing Social Presence with Co-learner Screen Sharing in Online Programming Classes
PY  - 2021
AB  - Social presence is known to be important for distance education, and a common approach in online classes is to provide chat boxes and forums to provide the social presence. In such a class, however, learners must explicitly act beyond their normal learning activities, so often there is no social presence in the class even when there are several learners working on the same course material. In this paper, we develop an approach where learners can share the social presence without any explicit action; their normal learning activities would be used to provide visual cues for social presence. We present Cocode, a system designed for an online programming class that shows other learners' code editors and running output in the programming environment with minimum privacy issues. For evaluation, we ran two user studies with groups of participants who took an offline class and an online programming class from the university; results from the studies showed that learners felt less social presence in Cocode than in offline classes, but they felt significantly more social presence in Cocode than in online classes with live video lectures, forums, and chat sessions.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW2
PB  - 
DO  - 10.1145/3476041
ER  - 

TY  - NA
AU  - Wong, Pui Chung; Fu, Hongbo; Zhu, Kening
TI  - Back-Mirror
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2999508.2999522
ER  - 

TY  - JOUR
AU  - Oney, Steve; Brooks, Christopher; Resnick, Paul
TI  - Creating Guided Code Explanations with chat.codes
PY  - 2018
AB  - Effective communication is crucial for instructors and students in programming courses. However, communicating about code can be difficult --- particularly in asynchronous settings where an instructor authors an explanation meant to be read and understood by a student later on. Communicating about code is uniquely difficult for two reasons. First, because of the dichotomous nature of the explanation, which consists of fragments of code and natural language descriptions. Second, instructors' explanations of code often involve modifying code throughout their explanation. This paper introduces chat.codes, a new tool for creating guided explanations about code. chat.codes introduces two features that make it easier to communicate about code. First, it adds deictic code references that allows instructors to write messages that reference specific regions of code. Second, it tracks and summarizes code edits in-line with messages, allowing instructors to create explanations in stages. An evaluation showed that these features were beneficial for both instructors and students.
SP  - 131
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 2
IS  - CSCW
PB  - 
DO  - 10.1145/3274400
ER  - 

TY  - JOUR
AU  - Wang, Peng; Wang, Yue; Billinghurst, Mark; Yang, Huizhen; Xu, Peng; Li, Yanhong
TI  - BeHere: a VR/SAR remote collaboration system based on virtual replicas sharing gesture and avatar in a procedural task
PY  - 2023
AB  - NA
SP  - NA
EP  - NA
JF  - Virtual Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10055-023-00748-5
ER  - 

TY  - JOUR
AU  - Seidel, Stefan; Berente, Nicholas; Lindberg, Aron; Lyytinen, Kalle; Nickerson, Jeffrey V.
TI  - Autonomous tools and design: a triple-loop approach to human-machine learning
PY  - 2018
AB  - In addition to having a detailed understanding of the artifacts they intend to create, designers need to guide the software tools they use.
SP  - 50
EP  - 57
JF  - Communications of the ACM
VL  - 62
IS  - 1
PB  - 
DO  - 10.1145/3210753
ER  - 

TY  - NA
AU  - He, Zhenyi; Perlin, Ken
TI  - Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching with Chalktalk
PY  - 2019
AB  - Teaching that uses projected presentation media such as slide-shows lacks support for dynamic content whose form and behaviors require live changes during a lecture. Recent software alternatives such as the Chalktalk software platform allow the creation of interactive simulations in arbitrary sequences and combinations within presentations. These more dynamic solutions, however, do not optimize for face-to-face interactions: eye-contact, gaze direction, and concurrent awareness of another person's movements together with the presented content. To explore the extent to which these face-to-face interactions may improve learning and engagement during a lecture, we propose a Mixed Reality (MR) platform that places Chalktalk's behaviors and simulations within a mirrored virtual world environment designed for face-to-face, one-on-one interactions. We compare our system with projected Chalktalk to evaluate its relative effectiveness for learning, retention, and level of engagement.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Belkacem, Ilyasse; Pecci, Isabelle; Martin, Benoît
TI  - Pointing task on smart glasses: Comparison of four interaction techniques
PY  - 2019
AB  - Mobile devices such as smartphones, smartwatches or smart glasses have revolutionized how we interact. We are interested in smart glasses because they have the advantage of providing a simultaneous view of both physical and digital worlds. Despite this potential, pointing task on smart glasses is not really widespread. In this paper, we compared four interaction techniques for selecting targets : (a) the Absolute Head Movement and (b) the Relative Head Movement, where head movement controls the cursor on smart glasses in absolute or relative way, (c) the Absolute Free Hand interaction, where the forefinger control the cursor and (d) the Tactile Surface interaction, where the user controls the cursor via a small touchpad connected to smart glasses. We conducted an experiment with 18 participants. The Tactile Surface and Absolute Head Movement were the most efficient. The Relative Head Movement and Absolute Free Hand interactions were promising and require more exploration for other tasks.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yu, Chun; Xiaoying, Wei; Vachher, Shubh; Qin, Yue; Liang, Chen; Weng, Yueting; Gu, Yizheng; Shi, Yuanchun
TI  - CHI - HandSee: Enabling Full Hand Interaction on Smartphone with Front Camera-based Stereo Vision
PY  - 2019
AB  - We present HandSee, a novel sensing technique that can capture the state and movement of the user's hands touching or gripping a smartphone. We place a right angle prism mirror on the front camera to achieve a stereo vision of the scene above the touchscreen surface. We develop a pipeline to extract the depth image of hands from a monocular RGB image, which consists of three components: a stereo matching algorithm to estimate the pixel-wise depth of the scene, a CNN-based online calibration algorithm to detect hand skin, and a merging algorithm that outputs the depth image of the hands. Building on the output, a substantial set of valuable interaction information, such as fingers' 3D location, gripping posture, and finger identity can be recognized concurrently. Due to this unique sensing ability, HandSee enables a variety of novel interaction techniques and expands the design space for full hand interaction on smartphones.
SP  - 705
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300935
ER  - 

TY  - NA
AU  - Guo, Philip J.
TI  - UIST - Ten Million Users and Ten Years Later: Python Tutor’s Design Guidelines for Building Scalable and Sustainable Research Software in Academia
PY  - 2021
AB  - Research software is often built as prototypes that never get widespread usage and are left unmaintained after a few papers get published. To counteract this trend, we propose a method for building research software with scale and sustainability in mind so that it can organically grow a large userbase and enable longer-term research. To illustrate this method, we present the design and implementation of Python Tutor (pythontutor.com), a code visualization tool that is, to our knowledge, one of the most widely-used pieces of research software developed within a university lab. Over the past decade, it has been used by over ten million people in over 180 countries. It has also contributed to 55 publications from 35 research groups in 13 countries. We distilled lessons from working on Python Tutor into three sets of design guidelines: 1) user experience design for scale and sustainability, 2) software architecture design for long-term sustainability, and 3) designing a sustainable software development workflow within academia. These guidelines can enable a student to create long-lasting software that reaches many users and facilitates research from many independent groups.
SP  - 1235
EP  - 1251
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474819
ER  - 

TY  - NA
AU  - Yoon, Boram; Kim, Hyung-il; Lee, Gun A.; Billinqhurst, Mark; Woo, Woontack
TI  - VR - The Effect of Avatar Appearance on Social Presence in an Augmented Reality Remote Collaboration
PY  - 2019
AB  - This paper investigates the effect of avatar appearance on Social Presence and users' perception in an Augmented Reality (AR) telep-resence system. Despite the development of various commercial 3D telepresence systems, there has been little evaluation and discussions about the appearance of the collaborator's avatars. We conducted two user studies comparing the effect of avatar appearances with three levels of body part visibility (head & hands, upper body, and whole body) and two different character styles (realistic and cartoon-like) on Social Presence while performing two different remote collaboration tasks. We found that a realistic whole body avatar was perceived as being the best for remote collaboration, but an upper body or cartoon style could be considered as a substitute depending on the collaboration context. We discuss these results and suggest guidelines for designing future avatar-mediated AR remote collaboration systems.
SP  - 547
EP  - 556
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797719
ER  - 

TY  - JOUR
AU  - Feng, Tian; Fan, Feiyi; Bednarz, Tomasz
TI  - A review of computer graphics approaches to urban modeling from a machine learning perspective
PY  - 2021
AB  - Urban modeling facilitates the generation of virtual environments for various scenarios about cities. It requires expertise and consideration, and therefore consumes massive time and computation resources. Nevertheless, related tasks sometimes result in dissatisfaction or even failure. These challenges have received significant attention from researchers in the area of computer graphics. Meanwhile, the burgeoning development of artificial intelligence motivates people to exploit machine learning, and hence improves the conventional solutions. In this paper, we present a review of approaches to urban modeling in computer graphics using machine learning in the literature published between 2010 and 2019. This serves as an overview of the current state of research on urban modeling from a machine learning perspective.
SP  - 915
EP  - 925
JF  - Frontiers of Information Technology & Electronic Engineering
VL  - 22
IS  - 7
PB  - 
DO  - 10.1631/fitee.2000141
ER  - 

TY  - NA
AU  - Komiyama, Ryohei; Miyaki, Takashi; Rekimoto, Jun
TI  - AH - JackIn space: designing a seamless transition between first and third person view for effective telepresence collaborations
PY  - 2017
AB  - Traditional telepresence systems only supported first person view and users had difficulty in recognizing the surrounding situation of their remote workspace. JackIn Space is a telepresence system that solves this problem by seamlessly integrating first person view with third person view. With a head-mounted first person camera and multiple depth sensors installed in the environment, the surrogate user's first person view smoothly changes to the out-of-body third person view, and the user connected to the surrogate user can virtually look around the environment. The user can also dive into other surrogate users to gain different perspectives. Our evaluation supports that the concept and the function of JackIn Space was quite well accepted and our prototype system provides a more natural viewpoint selection. Overall, JackIn Space supports better remote collaborations.
SP  - 14
EP  - NA
JF  - Proceedings of the 8th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3041164.3041183
ER  - 

TY  - CHAP
AU  - Nyemkova, Elena; Chaplyha, Vyacheslav; Ministr, Jan; Chaplyha, Volodymyr
TI  - Advanced Methods and Means of Authentication of Devices for Processing Business Information
PY  - 2020
AB  - The article is devoted to the analysis of the current state of methods and means of authentication of electronic devices for tasks of processing business information. Transmission, transformation and storage of business information are carried out by a wide spectrum and increasing number of devices both in global networks and outside of them. This requires adequate mutual authentication between devices to enhance cybersecurity, as well as reliable authentication of a variety of electronic devices for digital media expertise, detection of counterfeit products, etc. The general approach to the authentication of electronic devices based on their individual differences allows solving various applications of business informatics. The classification of such tasks, as well as the analysis of methods and means of authentication of electronic devices for solving various classes of tasks showed the following. Forensic tasks are solved by methods of comparing two samples from two random processes based on Kolmogorov-Smirnov’s agreement with the use of specialized software such as “Fractal”. For geolocation tasks of an electronic device, it is expedient to use individual template for an error vector in determining the deviations in the signal constellation. The tasks of detecting counterfeit notebooks and mobile phones are solved by methods of comparing the spectra of their own electromagnetic radiation of these devices with the standard. Remote automated real-time authentication of various Supervisory Control and Data Acquisition system sensors can be solved by passive analysis of the sensor’s own noise based on its spectrum.
SP  - 205
EP  - 228
JF  - Data-Centric Business and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-35649-1_10
ER  - 

TY  - BOOK
AU  - Curran, Max T.; Merrill, Nick; Gandhi, Swapan; Chuang, John
TI  - PhyCS - Exploring the Feasibility and Performance of One-step Three-factor Authentication with Ear-EEG.
PY  - 2018
AB  - NA
SP  - 30
EP  - 41
JF  - Proceedings of the 5th International Conference on Physiological Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.5220/0006896300300041
ER  - 

TY  - NA
AU  - Li, Yuan; Lee, Sang Won; Bowman, Doug A.; Hicks, David; Lages, Wallace santos; Sharma, Akshay
TI  - ARCritique: Supporting Remote Design Critique of Physical Artifacts through Collaborative Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567700
ER  - 

TY  - JOUR
AU  - Ji, Xiaoyu; Cheng, Yushi; Zhang, Juchuan; Chi, Yuehan; Xu, Wenyuan; Chen, Yi-Chao
TI  - Device Fingerprinting with Magnetic Induction Signals Radiated by CPU Modules
PY  - 2021
AB  - <jats:p> With the widespread use of smart devices, device authentication has received much attention. One popular method for device authentication is to utilize internally measured device fingerprints, such as device ID, software or hardware-based characteristics. In this article, we propose <jats:monospace>DeMiCPU</jats:monospace> , a stimulation-response-based device fingerprinting technique that relies on externally measured information, i.e., magnetic induction (MI) signals emitted from the CPU module that consists of the CPU chip and its affiliated power-supply circuits. The key insight of <jats:monospace>DeMiCPU</jats:monospace> is that hardware discrepancies essentially exist among CPU modules and thus the corresponding MI signals make promising device fingerprints, which are difficult to be modified or mimicked. We design a stimulation and a discrepancy extraction scheme and evaluate them with 90 mobile devices, including 70 laptops (among which 30 are of totally identical CPU and operating system) and 20 smartphones. The results show that <jats:monospace>DeMiCPU</jats:monospace> can achieve 99.7% precision and recall on average, and 99.8% precision and recall for the 30 identical devices, with a fingerprinting time of 0.6~s. The performance can be further improved to 99.9% with multi-round fingerprinting. In addition, we implement a prototype of <jats:monospace>DeMiCPU</jats:monospace> docker, which can effectively reduce the requirement of test points and enlarge the fingerprinting area. </jats:p>
SP  - 1
EP  - 28
JF  - ACM Transactions on Sensor Networks
VL  - 18
IS  - 2
PB  - 
DO  - 10.1145/3495158
ER  - 

TY  - NA
AU  - Vatavu, Radu-Daniel; Bilius, Laura-Bianca
TI  - UIST - GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices
PY  - 2021
AB  - Despite an exciting area with many promises for innovations in wearable interactive systems, research on interaction techniques for smart rings lacks structured knowledge and readily-available resources for designers to systematically attain such innovations. In this work, we conduct a systematic literature review of ring-based gesture input, from which we extract key results and a large set of gesture commands for ring, ring-like, and ring-ready devices. We use these findings to deliver GestuRING, our web-based tool to support design of ring-based gesture input. GestuRING features a searchable gesture-to-function dictionary of 579 records with downloadable numerical data files and an associated YouTube video library. These resources are meant to assist the community in attaining further innovations in ring-based gesture input for interactive systems.
SP  - 710
EP  - 723
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474780
ER  - 

TY  - JOUR
AU  - Rhee, Taehyun; Thompson, Stephen; Medeiros, Daniel; Anjos, Rafael Kuffner dos; Chalmers, Andrew
TI  - Augmented Virtual Teleportation for High-Fidelity Telecollaboration
PY  - 2020
AB  - Telecollaboration involves the teleportation of a remote collaborator to another real-world environment where their partner is located. The fidelity of the environment plays an important role for allowing corresponding spatial references in remote collaboration. We present a novel asymmetric platform, Augmented Virtual Teleportation (AVT), which provides high-fidelity telepresence of a remote VR user ( VR-Traveler ) into a real-world collaboration space to interact with a local AR user ( AR-Host ). AVT uses a 360° video camera (360-camera) that captures and live-streams the omni-directional scenes over a network. The remote VR-Traveler watching the video in a VR headset experiences live presence and co-presence in the real-world collaboration space. The VR-Traveler's movements are captured and transmitted to a 3D avatar overlaid onto the 360-camera which can be seen in the AR-Host's display. The visual and audio cues for each collaborator are synchronized in the Mixed Reality Collaboration space (MRC-space), where they can interactively edit virtual objects and collaborate in the real environment using the real objects as a reference. High fidelity, real-time rendering of virtual objects and seamless blending into the real scene allows for unique mixed reality use-case scenarios. Our working prototype has been tested with a user study to evaluate spatial presence, co-presence, and user satisfaction during telecollaboration. Possible applications of AVT are identified and proposed to guide future usage.
SP  - 1923
EP  - 1933
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2020.2973065
ER  - 

TY  - NA
AU  - Jenal, Andrin; Savinov, Nikolay; Sattler, Torsten; Chaurasia, Gaurav
TI  - RNN-based Generative Model for Fine-Grained Sketching.
PY  - 2019
AB  - Deep generative models have shown great promise when it comes to synthesising novel images. While they can generate images that look convincing on a higher-level, generating fine-grained details is still a challenge. In order to foster research on more powerful generative approaches, this paper proposes a novel task: generative modelling of 2D tree skeletons. Trees are an interesting shape class because they exhibit complexity and variations that are well-suited to measure the ability of a generative model to generated detailed structures. We propose a new dataset for this task and demonstrate that state-of-the-art generative models fail to synthesise realistic images on our benchmark, even though they perform well on current datasets like MNIST digits. Motivated by these results, we propose a novel network architecture based on combining a variational autoencoder using Recurrent Neural Networks and a convolutional discriminator. The network, error metrics and training procedure are adapted to the task of fine-grained sketching. Through quantitative and perceptual experiments, we show that our model outperforms previous work and that our dataset is a valuable benchmark for generative models. We will make our dataset publicly available.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Hoppe, Adrian Heinrich; van de Camp, Florian; Stiefelhagen, Rainer
TI  - ShiSha: Enabling Shared Perspective With Face-to-Face Collaboration Using Redirected Avatars in Virtual Reality
PY  - 2021
AB  - The importance of remote collaboration grows in an interconnected world as the reasons to avoid travel increase. The spatial rendering and collaboration capabilities of virtual and augmented reality systems are well suited for tasks such as support or training. Users can take a shared perspective to build a common understanding. Also, users may engage in face-to-face cooperation to support interpersonal communication. However, a shared perspective and face-to-face collaboration are both desirable but naturally exclude each other. We place all users at the same location to provide a shared perspective. To avoid overlapping body parts, the avatars of the other connected users are shifted to the side. A redirected body pose modification corrects the resulting inconsistencies. The implemented system is compared to a baseline of two users standing in the same location and working with overlapping avatars. The results of a user study show that the proposed modifications provide an easy to use, efficient collaboration and yield higher co-presence and the feeling of teamwork. Applying redirection techniques to other users opens up novel ways to increase social presence for local or remote collaboration.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW3
PB  - 
DO  - 10.1145/3432950
ER  - 

TY  - JOUR
AU  - Benavides, M.; Miralles, David; Andújar, A.; Anguera, J.
TI  - Effects on human body and conductive body over a near field communication antenna
PY  - 2021
AB  - The paper presents an NFC antenna based on a rectangular spiral examining the effect of the geometry and the addition of a ferrite sheet. Several prototypes are fabricated and tested where the infl...
SP  - 1235
EP  - 1246
JF  - Journal of Electromagnetic Waves and Applications
VL  - 35
IS  - 9
PB  - 
DO  - 10.1080/09205071.2021.1875889
ER  - 

TY  - NA
AU  - Zhang, Yang; Zhou, Junhan; Laput, Gierad; Harrison, Chris
TI  - CHI - SkinTrack: Using the Body as an Electrical Waveguide for Continuous Finger Tracking on the Skin
PY  - 2016
AB  - SkinTrack is a wearable system that enables continuous touch tracking on the skin. It consists of a ring, which emits a continuous high frequency AC signal, and a sensing wristband with multiple electrodes. Due to the phase delay inherent in a high-frequency AC signal propagating through the body, a phase difference can be observed between pairs of electrodes. SkinTrack measures these phase differences to compute a 2D finger touch coordinate. Our approach can segment touch events at 99% accuracy, and resolve the 2D location of touches with a mean error of 7.6mm. As our approach is compact, non-invasive, low-cost and low-powered, we envision the technology being integrated into future smartwatches, supporting rich touch interactions beyond the confines of the small touchscreen.
SP  - 1491
EP  - 1503
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858082
ER  - 

TY  - NA
AU  - Kumaravel, Balasaravanan Thoravi; Nguyen, Cuong; DiVerdi, Stephen; Hartmann, Bjoern
TI  - UIST - TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators
PY  - 2020
AB  - Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.
SP  - 182
EP  - 195
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415827
ER  - 

TY  - JOUR
AU  - Mainetti, Luca; Panarese, Paolo; Vergallo, Roberto
TI  - WoX+: A Meta-Model-Driven Approach to Mine User Habits and Provide Continuous Authentication in the Smart City.
PY  - 2022
AB  - The literature is rich in techniques and methods to perform Continuous Authentication (CA) using biometric data, both physiological and behavioral. As a recent trend, less invasive methods such as the ones based on context-aware recognition allows the continuous identification of the user by retrieving device and app usage patterns. However, a still uncovered research topic is to extend the concepts of behavioral and context-aware biometric to take into account all the sensing data provided by the Internet of Things (IoT) and the smart city, in the shape of user habits. In this paper, we propose a meta-model-driven approach to mine user habits, by means of a combination of IoT data incoming from several sources such as smart mobility, smart metering, smart home, wearables and so on. Then, we use those habits to seamlessly authenticate users in real time all along the smart city when the same behavior occurs in different context and with different sensing technologies. Our model, which we called WoX+, allows the automatic extraction of user habits using a novel Artificial Intelligence (AI) technique focused on high-level concepts. The aim is to continuously authenticate the users using their habits as behavioral biometric, independently from the involved sensing hardware. To prove the effectiveness of WoX+ we organized a quantitative and qualitative evaluation in which 10 participants told us a spending habit they have involving the use of IoT. We chose the financial domain because it is ubiquitous, it is inherently multi-device, it is rich in time patterns, and most of all it requires a secure authentication. With the aim of extracting the requirement of such a system, we also asked the cohort how they expect WoX+ will use such habits to securely automatize payments and identify them in the smart city. We discovered that WoX+ satisfies most of the expected requirements, particularly in terms of unobtrusiveness of the solution, in contrast with the limitations observed in the existing studies. Finally, we used the responses given by the cohorts to generate synthetic data and train our novel AI block. Results show that the error in reconstructing the habits is acceptable: Mean Squared Error Percentage (MSEP) 0.04%.
SP  - 6980
EP  - 6980
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 18
PB  - 
DO  - 10.3390/s22186980
ER  - 

TY  - NA
AU  - Bianchi, Andrea; Je, Seungwoo
TI  - AH - Disambiguating touch with a smart-ring
PY  - 2017
AB  - Capacitive touchscreens have changed the way in which people interact with computational devices. In fact, direct touch input on screens is immediately understandable and appealing to both novice and advanced users and, more importantly, it leverages people's natural ability to use multiple fingers for input gestures. However, currently off-the-shelves touchscreens are unable to disambiguate among different fingers or to determine whether different touch-points belong to the same user, reducing the expressiveness of finger touches to that of multiple pointers. In this paper, we propose to augment human touch using a smart-ring. When the finger wearing the ring is in contact with the touchscreen, a unique ID is transmitted through vibration patterns from the ring to the touched device. It therefore becomes possible to distinguish touch between different users, or to associate different meanings to different touches for the same user. In this paper, we explore this design space and present a set of applications to demonstrate the feasibility of this technique.
SP  - 27
EP  - NA
JF  - Proceedings of the 8th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3041164.3041196
ER  - 

TY  - NA
AU  - Brock, Andrew; Lim, Theodore; Ritchie, J. M.; Weston, Nicholas John
TI  - Generative and Discriminative Voxel Modeling with Convolutional Neural Networks
PY  - 2016
AB  - When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Xiaotian; He, Weiping; Billinghurst, Mark; Yang, Lingxiao; Feng, Shuo; Liu, Daisong
TI  - Design and Evaluation of Bare-Hand Interaction for Precise Manipulation of Distant Objects in AR
PY  - 2022
AB  - NA
SP  - 1
EP  - 15
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2158527
ER  - 

TY  - NA
AU  - Petullo, W. Michael
TI  - Courses as Code: The Aquinas Learning System
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Cyber Security Experimentation and Test Workshop
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546096.3546099
ER  - 

TY  - JOUR
AU  - Jairath, Kapil; Singh, Navdeep; Shabaz, Mohammad; Jagota, Vishal; Singh, Bhupesh Kumar
TI  - Performance Analysis of Metamaterial-Inspired Structure Loaded Antennas for Narrow Range Wireless Communication
PY  - 2022
AB  - <jats:p>Nowadays, the demand for low-cost, compact, and interference rejected antennas with ultrawideband capability has been increased. Metamaterial-inspired loaded structures have capability of providing exceptional solutions for narrow range wireless communication and low consuming power while transmitting and receiving the signal. It is a difficult task to construct ideal metamaterial-inspired antennas with a variety of features such as extremely large bandwidth, notching out undesirable bands, and frequency. Metamaterial-inspired structures such as SRR and CSRR, and triangle-shaped TCSRR are most commonly used structures to achieve optimized characteristics in ultrawideband antennas. In this paper, an extensive literature survey is accomplished to get conception about metamaterial-inspired patch antennas. This review paper elucidates variants of metamaterial-inspired structures/resonators utilized in order to acquire sundry applications such as WiMAX, WLAN, satellite communication, and radar. Various researchers have used different methodology to design, stimulate, and analyze the metamaterial-inspired structure loaded antennas. Also, the results of different metamaterial-inspired antennas such as bandwidth, gain, return loss, and resonant frequency have been also represented in this paper. This manuscript also gives brief introduction about the metamaterial, its types, and then its application in microstrip patch antenna over the last decade. This manuscript throws light over the various studies conducted in the field of metamaterial-inspired antenna in the past. It has been seen that with the inclusion of metamaterial in conventional antenna, various characteristics such as impedance bandwidth, reflection coefficient, gain, and directivity have been improved. Also, frequency rejection of narrow bands which exits in ultrawideband frequency range can be done by embedding metamaterial-inspired structures such as SRR and CSRR.</jats:p>
SP  - 1
EP  - 17
JF  - Scientific Programming
VL  - 2022
IS  - NA
PB  - 
DO  - 10.1155/2022/7940319
ER  - 

TY  - NA
AU  - Hou, Xinying; Ericson, Barbara Jane; Wang, Xu
TI  - Using Adaptive Parsons Problems to Scaffold Write-Code Problems
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Conference on International Computing Education Research V.1
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3501385.3543977
ER  - 

TY  - JOUR
AU  - Ding, Dian; Yang, Lanqing; Chen, Yi-Chao; Xue, Guangtao
TI  - Leakage or Identification
PY  - 2021
AB  - <jats:p>The convenience of laptops brings with it the risk of information leakage, and conventional security systems based on the password or the explicit biometric do little to alleviate this problem. Biometric identification based on anatomical features provides far stronger security; however, a lack of suitable sensors on laptops limits the applicability of this technology. In this paper, we developed a behavior-irrelevant user identification system applicable to laptops with a metal casing. The proposed scheme, referred to as LeakPrint, is based on leakage current, wherein the system uses an earphone to capture current leaking through the body and then transmits the corresponding signal to a server for identification. The user identification is achieved via denoising, dimension reduction, and feature extraction. Compared to other biometric identification methods, the proposed system is less dependent on external hardware and more robust to environmental noise. The experiments in real-world environments demonstrated that LeakPrint can verify user identity with high accuracy (93.6%), while providing effective defense against replay attacks (96.5%) and mimicry attacks (90.9%).</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494984
ER  - 

TY  - NA
AU  - Huang, Hua; Lin, Shan
TI  - MobiCom - MET: a magneto-inductive sensing based electric toothbrushing monitoring system
PY  - 2020
AB  - Electric toothbrushes are widely used for home oral care, but many users do not achieve desired hygiene results due to insufficient brushing coverage or incorrect brushing techniques. Existing electric toothbrushing monitoring systems fail to detect these issues because they cannot achieve fine-grained position tracking. In this paper, we present a novel electric toothbrushing monitoring system called MET that tracks brushing coverage for all the 15 surfaces of teeth and detects different types of incorrect brushing techniques. This design is inspired by our observation that the motor inside an electric toothbrush generates a unique magnetic field, which can serve as a reliable signal for position and orientation tracking. MET is the first system that tracks both the position and orientation of an unmodified electric motor using magnetic inductive sensing. Experiments with fourteen users show that the average toothbrushing surface recognition accuracy of MET is 85.3%. Moreover, MET is robust to user location changes and posture variations and does not require any training from the users. Experimental results also demonstrate our significant advantages over existing commercial systems.
SP  - NA
EP  - NA
JF  - Proceedings of the 26th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3372224.3380896
ER  - 

TY  - NA
AU  - Ritchie, Daniel; Thomas, Anna E.; Hanrahan, Pat; Goodman, Noah D.
TI  - Neurally-Guided Procedural Models: Learning to Guide Procedural Models with Deep Neural Networks.
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Yang, Chouchang Jack; Sample, Alanson P.
TI  - EM-Comm: Touch-based Communication via Modulated Electromagnetic Emissions
PY  - 2017
AB  - Touch-based communication offers a direct and intuitive way for users to initiate and control data transfer when using tangible and ubiquitous interfaces. However, this requires that each interactive device be instrumented with a dedicated radio transmitter, which limits many applications. While not all devices have radio hardware, all devices do emit small amounts of electromagnetic noise in the form of EMI. We argue that if properly modulated these electromagnetic emissions can be used as an untapped communication channel capable of transmitting arbitrary data. To enable this, a spread spectrum frequency shift keying modulation scheme has been developed to encode data onto the device’s EMI. When the device is touched by a user, the data encoded EMI signal travels through their body and into our custom wrist band, consisting of a single op-amp and MCU. Our results show that we are able to turn electronic primitives such as LEDs, buttons, I/O lines, LCD screens, motors and power supplies into radio transmitters capable of touch communication. Effective data rates range from 5.8Kbps to 22 bit per image depending on the primitive used. To demonstrate this new communication technique, we develop several interactive experiences where users can retrieve complex information such as the function of buttons on a device, directions embedded into a LCD screen, and simplified device pairing. Ultimately, EM-Comm enables nearly any electronic device to be turned into a touch-based radio transmitter with only a software upgrade.
SP  - 118
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130984
ER  - 

TY  - NA
AU  - Henley, Austin Z.; Ball, Julian; Klein, Benjamin; Rutter, Aiden; Lee, Dylan
TI  - An Inquisitive Code Editor for Addressing Novice Programmers' Misconceptions of Program Behavior
PY  - 2021
AB  - Novice programmers face numerous barriers while attempting to learn how to code that may deter them from pursuing a computer science degree or career in software development. In this work, we propose a tool concept to address the particularly challenging barrier of novice programmers holding misconceptions about how their code behaves. Specifically, the concept involves an inquisitive code editor that: (1) identifies misconceptions by periodically prompting the novice programmer with questions about their program's behavior, (2) corrects the misconceptions by generating explanations based on the program's actual behavior, and (3) prevents further misconceptions by inserting test code and utilizing other educational resources. We have implemented portions of the concept as plugins for the Atom code editor and conducted informal surveys with students and instructors. Next steps include deploying the tool prototype to students enrolled in introductory programming courses.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chen, Yan
TI  - On-Demand Collaboration in Programming
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Piumsomboon, Thammathip; Dey, Arindam; Ens, Barrett; Lee, Gun A.; Billinghurst, Mark
TI  - ISMAR Adjunct - [POSTER] CoVAR: Mixed-Platform Remote Collaborative Augmented and Virtual Realities System with Shared Collaboration Cues
PY  - 2017
AB  - We present CoVAR, a novel Virtual Reality (VR) and Augmented Reality (AR) system for remote collaboration. It supports collaboration between AR and VR users by sharing a 3D reconstruction of the AR user's environment. To enhance this mixed platform collaboration, it provides natural inputs such as eye-gaze and hand gestures, remote embodiment through avatar's head and hands, and awareness cues of field-of-view and gaze cue. In this paper, we describe the system architecture, setup and calibration procedures, input methods and interaction, and collaboration enhancement features.
SP  - 218
EP  - 219
JF  - 2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2017.72
ER  - 

TY  - JOUR
AU  - Alharbi, Rawan; Sen, Sougata; Ng, Ada; Alshurafa, Nabil; Hester, Josiah
TI  - ActiSight: Wearer Foreground Extraction Using a Practical RGB-Thermal Wearable.
PY  - 2022
AB  - Wearable cameras provide an informative view of wearer activities, context, and interactions. Video obtained from wearable cameras is useful for life-logging, human activity recognition, visual confirmation, and other tasks widely utilized in mobile computing today. Extracting foreground information related to the wearer and separating irrelevant background pixels is the fundamental operation underlying these tasks. However, current wearer foreground extraction methods that depend on image data alone are slow, energy-<b>in</b>efficient, and even inaccurate in some cases, making many tasks-like activity recognition- challenging to implement in the absence of significant computational resources. To fill this gap, we built ActiSight, a wearable RGB-Thermal video camera that uses thermal information to make wearer segmentation practical for body-worn video. Using ActiSight, we collected a total of 59 hours of video from 6 participants, capturing a wide variety of activities in a natural setting. We show that wearer foreground extracted with ActiSight achieves a high dice similarity score while significantly lowering execution time and energy cost when compared with an RGB-only approach.
SP  - 237
EP  - NA
JF  - Proceedings of the ... IEEE International Conference on Pervasive Computing and Communications. IEEE International Conference on Pervasive Computing and Communications
VL  - 2022
IS  - NA
PB  - 
DO  - 10.1109/percom53586.2022.9762385
ER  - 

TY  - BOOK
AU  - Feld, Nico; Weyers, Benjamin
TI  - VR Workshops - Mixed Reality in Asymmetric Collaborative Environments: A Research Prototype for Virtual City Tours
PY  - 2021
AB  - Augmented and virtual reality have the potential to improve various aspects of everyday life such as tourism. An elementary part of tourism is the organization and execution of guided city tours. In case of tours for virtual cities, such as in the case of no longer existing ancient settlements, a combination of augmented and virtual reality may render high benefit for collaboration between guide and visitors. Immersive technology has been shown suitable for collaborative systems by previous research, as they can provide a better experience than other technologies. However, recent literature analyses showed that there is a lack of research regarding the role of augmented reality supporting the active/leading user role in such scenarios. Usually, augmented reality is used for the users who are passively experiencing the virtual environment and not for the one controlling it. However, we think that augmented reality has a large potential to improve the workflow of such a primary user. To investigate this research question, we present a collaborative system in this work, which implements a virtual city tour. In this system, the guide is using augmented reality and the visitor is immersed in the virtual city using virtual reality glasses. To implement a valid research prototype, the system has been developed in close feedback with experts in the field of guided city tours.
SP  - 250
EP  - 256
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00053
ER  - 

TY  - NA
AU  - Matthies, Denys J.C.; Parra, Laura Milena Daza; Urban, Bodo
TI  - Scaling notifications beyond alerts: from subtly drawing attention up to forcing the user to take action
PY  - 2018
AB  - New computational devices, in particular wearable devices, offer the unique property of always being available and thus to be able to constantly update the user with information, such as by notifications. While research has been done in sophisticated notifications, devices today mainly stick to a binary level of information, while they are either attention drawing or silent. In this paper, we want to go further and propose scalable notifications, which adjust the intensity reaching from subtle to obtrusive and even going beyond that level, while forcing the user to take action. To illustrate the technical feasibility and validity of this concept, we developed three prototypes providing mechano-pressure, thermal, and electrical feedback and evaluated them in different lab studies. Our first prototype provides subtle poking through to high and frequent pressure on the user's spine, which creates a significantly improved back posture. In a second scenario, the users are enabled to perceive the overuse of a drill by an increased temperature on the palm of a hand until the heat is intolerable and the users are forced to eventually put down the tool. The last project comprises a speed control in a driving simulation, while electric muscle stimulation on the users' legs conveys information on changing the car's speed by a perceived tingling until the system independently forces the foot to move. Although our selected scenarios are long way from being realistic, we see these lab studies as a means to validate our proof-of-concept. In conclusion, all studies' findings support the feasibility of our concept of a scalable notification system, including the system of forced intervention. While we envisage the implementation of our proof-of-concept into future wearables, more realistic application scenarios are worthy of exploration.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Hadar, Ethan; Shtok, Joseph; Cohen, Benjamin; Tzur, Yochay; Karlinsky, Leonid
TI  - CAiSE-Forum-DC - Hybrid Remote Expert - an Emerging Pattern of Industrial Remote Support.
PY  - 2017
AB  - NA
SP  - 33
EP  - 40
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Otsuki, Mai; Wang, Tzu-Yang; Kuzuoka, Hideaki
TI  - Assessment of Instructor's Capacity in One-to-Many AR Remote Instruction Giving
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565631
ER  - 

TY  - BOOK
AU  - Strak, Robin; Yu, Kevin; Pankratz, Frieder; Lazarovici, Marc; Sandmeyer, Benedikt; Reichling, Julia; Weidert, Simon; Kraetsch, Clemens; Roegele, Barbara; Navab, Nassir; Eck, Ulrich; Roth, Daniel
TI  - Mensch und Computer - Comparison Between Video-mediated and Asymmetric 3D Teleconsultation During a Preclinical Scenario
PY  - 2021
AB  - Current teleconsultation solutions for preclinical emergencies can transmit knowledge from a remote expert to a local paramedic using audio and 2D video channels. Such technology lacks precision and efficiency for medical diagnostic tasks, and visual feedback is often missing between participants. We investigate a mixed reality 3D teleconsultation solution for preclinical use, which provides a 3D reconstruction of the local scene to a remote expert, displayed in Virtual Reality. A remote expert can join the local scene virtually as an Augmented Reality avatar. The remote expert can annotate the local scene and guide the local paramedics through the procedure. We explored our system in a user study within a preclinical scenario on a collaborative task of attaching chest lead electrodes of a 12 channel electrocardiogram on a mannequin. We compared the 3D teleconsultation system to a 2D video-mediated teleconsultation via a top-mounted camera and report results from the consultee side in AR. Based on our empirical user study with 10 paramedics with an average of 17 years experience, we observe an improvement in the electrode placement quality using the 3D teleconsultation system. Results indicate no significant difference in the cognitive task-load between conditions. Participants perceived the video-mediated consultation as more usable due to their unfamiliarity with the 3D teleconsultation system. However, participants acknowledge the potential of 3D teleconsultation and believe such a system can significantly improve the preclinical treatment.
SP  - 227
EP  - 235
JF  - Mensch und Computer 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3473856.3473883
ER  - 

TY  - NA
AU  - Matulic, Fabrice; Arakawa, Riku; Vogel, Brian; Vogel, Daniel
TI  - CHI - PenSight: Enhanced Interaction with a Pen-Top Camera
PY  - 2020
AB  - We propose mounting a downward-facing camera above the top end of a digital tablet pen. This creates a unique and practical viewing angle for capturing the pen-holding hand and the immediate surroundings which can include the other hand. The fabrication of a prototype device is described and the enabled interaction design space is explored, including dominant and non-dominant hand pose recognition, tablet grip detection, hand gestures, capturing physical content in the environment, and detecting users and pens. A deep learning computer vision pipeline is developed for classification, regression, and keypoint detection to enable these interactions. Example applications demonstrate usage scenarios and a qualitative user evaluation confirms the potential of the approach.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376147
ER  - 

TY  - JOUR
AU  - Marques, Bernardo; Silva, Samuel; Alves, João; Rocha, António; Dias, Paulo; Santos, Beatriz Sousa
TI  - Remote collaboration in maintenance contexts using augmented reality: insights from a participatory process
PY  - 2022
AB  - NA
SP  - 419
EP  - 438
JF  - International Journal on Interactive Design and Manufacturing (IJIDeM)
VL  - 16
IS  - 1
PB  - 
DO  - 10.1007/s12008-021-00798-6
ER  - 

TY  - JOUR
AU  - Shi, Yilei; Zhang, Haimo; Zhao, Kaixing; Cao, Jiashuo; Sun, Mengmeng; Nanayakkara, Suranga
TI  - Ready, Steady, Touch!: Sensing Physical Contact with a Finger-Mounted IMU
PY  - 2020
AB  - A finger held in the air exhibits microvibrations, which are reduced when it touches a static object. When a finger moves along a surface, the friction between them produces vibrations, which can not be produced with a free-moving finger in the air. With an inertial measurement unit (IMU) capturing such motion characteristics, we demonstrate the feasibility to detect contact between the finger and static objects. We call our technique ActualTouch. Studies show that a single nail-mounted IMU on the index finger provides sufficient data to train a binary touch status classifier (i.e., touch vs. no-touch), with an accuracy above 95%, generalised across users. This model, trained on a rigid tabletop surface, was found to retain an average accuracy of 96% for 7 other types of everyday surfaces with varying rigidity, and in walking and sitting scenarios where no touch occurred. ActualTouch can be combined with other interaction techniques, such as in a uni-stroke gesture recogniser on arbitrary surfaces, where touch status from ActualTouch is used to delimit the motion gesture data that feed into the recogniser. We demonstrate the potential of ActualTouch in a range of scenarios, such as interaction for augmented reality applications, and leveraging daily surfaces and objects for ad-hoc interactions.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397309
ER  - 

TY  - JOUR
AU  - Ito, Hiroki; Shimakawa, Hiromitsu; Harada, Fumiko
TI  - Process-Oriented Understanding Estimation Using Code Puzzles
PY  - 2022
AB  - In the current programming education, in order to assess the true ability of learners, instructors still have no choice but to monitor their answering process, standing by them. However, this is impractical for freshman training in educational institutions and newcomer training in companies. Because of the practicality, a large number of learners are assessed at once using written tests or Web tests. They usually inquire of learners whether they know algorithms and grammar. If not, they assess only the behavior of source codes they submit, at best. Under the training based on such assessment, in reality, not a few learners fail to acquire the skill of writing source codes. It implies that the attainment of programming skills cannot be assessed only by tests on knowledge and submitted source codes. This paper proposes a method for analyzing learners’ understanding that focuses on their thinking process of programming. The proposed method focuses on a code puzzle in which learners arrange fragments of a program code to satisfy given requirements. It aims to estimate the learner’s perspective on how fragments are built up to achieve the requirements. Learners with low understanding are assumed to be different from those with high in terms of the consistency of arranging ways to compose code fragments for specific blocks in source codes. For the discrimination, the method builds a model using a hidden Markov model. The internal state obtained from this model would help instructors grasp the learner’s understanding level. The results of an experiment present that the hidden Markov model produces meaningful values, which enable instructors to interpret the understanding of individual learners.
SP  - 750
EP  - 767
JF  - Creative Education
VL  - 13
IS  - 3
PB  - 
DO  - 10.4236/ce.2022.133048
ER  - 

TY  - NA
AU  - Ahmed, Umair Z.; Srivastava, Nisheeth; Sindhgatta, Renuka; Karkare, Amey
TI  - ICSE (SEET) - Characterizing the pedagogical benefits of adaptive feedback for compilation errors by novice programmers
PY  - 2020
AB  - Can automated adaptive feedback for correcting erroneous programs help novice programmers learn to code better? In a large-scale experiment, we compare student performance when tutored by human tutors, and when receiving automated adaptive feedback. The automated feedback was designed using one of two well-known instructional principles: (i) presenting the correct solution for the immediate problem, or (ii) presenting generated examples or analogies that guide towards the correct solution. We report empirical results from a large-scale (N = 480, 10,000 + person hour) experiment assessing the efficacy of these automated compilation-error feedback tools. Using the survival analysis on error rates of students measured over seven weeks, we found that automated feedback allows students to resolve errors in their code more efficiently than students receiving manual feedback. However, we also found that this advantage is primarily logistical and not conceptual; the performance benefit seen during lab assignments disappeared during exams wherein feedback of any kind was withdrawn. We further found that the performance advantage of automated feedback over human tutors increases with problem complexity, and that feedback via example and specific repair have distinct, non-overlapping relative advantages for different categories of programming errors. Our results offer a clear and granular delimitation of the pedagogical benefits of automated feedback in teaching programming to novices.
SP  - 139
EP  - 150
JF  - Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377814.3381703
ER  - 

TY  - NA
AU  - Cang, Ruijin
TI  - Data-Augmented Structure-Property Mapping for Accelerating Computational Design of Advanced Material Systems
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kim, Daehwa; Harrison, Chris
TI  - EtherPose: Continuous Hand Pose Tracking with Wrist-Worn Antenna Impedance Characteristic Sensing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545665
ER  - 

TY  - JOUR
AU  - Yan, Zhenyu; Song, Qun; Tan, Rui
TI  - Touch-to-Access Device Authentication For Indoor Smart Objects
PY  - NA
AB  - NA
SP  - 1185
EP  - 1197
JF  - IEEE Transactions on Mobile Computing
VL  - 22
IS  - 2
PB  - 
DO  - 10.1109/tmc.2021.3089497
ER  - 

TY  - CHAP
AU  - Mitterberger, Daniela; Angelaki, Evgenia-Makrina; Salveridou, Foteini; Rust, Romana; Vasey, Lauren; Gramazio, Fabio; Kohler, Matthias
TI  - Extended Reality Collaboration: Virtual and Mixed Reality System for Collaborative Design and Holographic-Assisted On-site Fabrication
PY  - 2022
AB  - NA
SP  - 283
EP  - 295
JF  - Towards Radical Regeneration
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-13249-0_24
ER  - 

TY  - JOUR
AU  - Sharma, Adwait; Salchow-Hömmen, Christina; Mollyn, Vimal Suresh; Nittala, Aditya Shekhar; Hedderich, Michael A.; Koelle, Marion; Seel, Thomas; Steimle, Jürgen
TI  - SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures
PY  - 2022
AB  - <jats:p> Gestural interaction with freehands and while grasping an everyday object enables <jats:italic>always-available input</jats:italic> . To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present <jats:italic>SparseIMU</jats:italic> , a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios. </jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569894
ER  - 

TY  - JOUR
AU  - Yan, Zhenyu; Song, Qun; Tan, Rui
TI  - Touch-to-Access Device Authentication For Indoor Smart Objects
PY  - 2021
AB  - This paper presents TouchAuth, a new touch-to-access device authentication approach using induced body electric potentials (iBEPs) caused by the indoor ambient electric field that is mainly emitted from the building's electrical network. The design of TouchAuth is based on the electrostatics of iBEP generation and a resulting property, i.e., the iBEPs at two close locations on the same human body are similar, whereas those from different human bodies are distinct. Extensive experiments verify the above property and show that TouchAuth achieves high-profile receiver operating characteristics in implementing the touch-to-access policy. Our experiments also show that a range of possible interfering sources including appliances' electromagnetic emanations and noise injections into the power network do not affect the performance of TouchAuth. A key advantage of TouchAuth is that the iBEP sensing requires a simple analog-to-digital converter only, which is widely available on microcontrollers. Compared with the existing approaches including intra-body communication and physiological sensing, TouchAuth is a low-cost, lightweight, and convenient approach for authorized users to access the smart objects found in indoor environments.
SP  - 1
EP  - 1
JF  - IEEE Transactions on Mobile Computing
VL  - NA
IS  - 01
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Rudolph, Julius Cosmo Romeo; Holman, David; De Araujo, Bruno; Jota, Ricardo; Wigdor, Daniel; Savage, Valkyrie
TI  - Sensing Hand Interactions with Everyday Objects by Profiling Wrist Topography
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501320
ER  - 

TY  - JOUR
AU  - Shen, Cheng; Huang, Jun; Sun, Guangyu; Chen, Jingshu
TI  - Electromagnetic Fingerprinting of Memory Heartbeats
PY  - 2022
AB  - <jats:p>This paper presents MemScope, a system that fingerprints devices via electromagnetic sensing of their memory heartbeats, i.e., the clock signal that synchronizes memory and memory controller. MemScope leverages the enhanced resolution and security of memory heartbeat fingerprint, which has enriched spectral features thanks to the spread spectrum generation of memory clock, and cannot be concealed as long as the device accesses its memory during computing. MemScope employs signal processing algorithms that allow it to hear the memory heartbeats of devices from a distance, in the presence of noise, and in crowded environments where multiple devices coexist. It then fingerprints memory heartbeats using machine learning tools. Measurements on a set of 65 devices over a month validate the robustness of fingerprint against time variation, and show a high precision and recall. We then use the neural network to build a detector to defend against possible replay attacks. Finally, we further demonstrate the effectiveness of MemScope in two application scenarios, (i) detecting wireless identity spoofing and (ii) identifying and localizing unauthorized hidden cameras.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550295
ER  - 

TY  - BOOK
AU  - Carter, Jason; Pichiliani, Mauro Carlos; Dewan, Prasun
TI  - ECSCW Exploratory Papers - Exploring the Impact of Video on Inferred Difficulty Awareness
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.18420/ecscw2018_9
ER  - 

TY  - BOOK
AU  - Gulay, Emrecan
TI  - ISS Companion - Merging Physical and Digital in the Initial Design Stages of Architecture
PY  - 2020
AB  - With the widespread adoption of digital design and fabrication technologies, architects have been looking for ways to connect virtual and physical design platforms. Previous research offers a limited perspective on the feedback exchange between physical and digital modes of design. My doctoral research looks into the changing role of physical methods with the ongoing digitalization in architecture. The research has been progressing through research prototypes and interviews. Produced prototypes correlate tangible outcomes with the theoretical framework established during the first year of my studies. By introducing a feedback-oriented de-sign approach, linking physical and digital tools, this work aims to enhance initial design processes in architecture and facilitate creative programmatic solutions for architects.
SP  - 101
EP  - 105
JF  - Companion Proceedings of the 2020 Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3380867.3427409
ER  - 

TY  - NA
AU  - Yun, Gyeore; Lee, Hyoseung; Han, Sangyoon; Choi, Seungmoon
TI  - CHI - Improving Viewing Experiences of First-Person Shooter Gameplays with Automatically-Generated Motion Effects
PY  - 2021
AB  - In recent times, millions of people enjoy watching video gameplays at an eSports stadium or home. We seek a method that improves gameplay spectator or viewer experiences by presenting multisensory stimuli. Using a motion chair, we provide the motion effects automatically generated from the audiovisual stream to the viewers watching a first-person shooter (FPS) gameplay. The motion effects express the game character’s movement and gunfire action. We describe algorithms for the computation of such motion effects developed using computer vision techniques and deep learning. By a user study, we demonstrate that our method of providing motion effects significantly improves the viewing experiences of FPS gameplay. The contributions of this paper are with the motion synthesis algorithms integrated for FPS games and the empirical evidence for the benefits of experiencing multisensory gameplays.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445358
ER  - 

TY  - JOUR
AU  - Li, Zhenxing; Akkil, Deepak; Raisamo, Roope
TI  - Gaze Augmented Hand-Based Kinesthetic Interaction: What You See is What You Feel
PY  - 2019
AB  - Kinesthetic interaction between the user and the computer mainly utilizes the hand-based input with force-feedback devices. There are two major shortcomings in hand-based kinesthetic interaction: physical fatigue associated with continuous hand movements and the limited workspace of current force-feedback devices for accurately exploring a large environment. To address these shortcomings, we developed two interaction techniques that use eye gaze as an additional input modality: HandGazeTouch and GazeTouch. Hand GazeTouch combines eye gaze and hand motion as the input for kinesthetic interaction, i.e., it uses eye gaze to point and hand motion to touch. GazeTouch replaces all hand motions in touch behavior with eye gaze, i.e., it uses eye gaze to point and gaze dwell time to trigger the touch. In both interaction techniques, the user feels the haptic feedback through the force-feedback device. The gaze-based techniques were evaluated in a softness discrimination experiment by comparing them to the traditional kinesthetic interface, HandTouch, which only uses the hand-based input. The results indicate that the HandGazeTouch technique is not only as accurate, natural, and pleasant as the traditional interface but also more efficient.
SP  - 114
EP  - 127
JF  - IEEE transactions on haptics
VL  - 12
IS  - 2
PB  - 
DO  - 10.1109/toh.2019.2896027
ER  - 

TY  - NA
AU  - Fender, Andreas; Lindlbauer, David; Herholz, Philipp; Alexa, Marc; Müller, Jörg
TI  - UIST - HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior
PY  - 2017
AB  - We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.
SP  - 611
EP  - 621
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126621
ER  - 

TY  - JOUR
AU  - Li, Zhenxing; Akkil, Deepak; Raisamo, Roope
TI  - Gaze-based Kinaesthetic Interaction for Virtual Reality
PY  - 2020
AB  - Kinaesthetic interaction using force-feedback devices is promising in virtual reality. However, the devices are currently not suitable for interactions within large virtual spaces because of their limited workspace. We developed a novel gaze-based kinaesthetic interface that employs the user's gaze to relocate the device workspace. The workspace switches to a new location when the user pulls the mechanical arm of the device to its reset position and gazes at the new target. This design enables the robust relocating of device workspace, thus achieving an infinite interaction space, and simultaneously maintains a flexible hand-based kinaesthetic exploration. We compared the new interface with the scaling-based traditional interface in an experiment involving softness and smoothness discrimination. Our results showed that the gaze-based interface performs better than the traditional interface, in terms of efficiency and kinaesthetic perception. It improves the user experience for kinaesthetic interaction in virtual reality without increasing eye strain.
SP  - 17
EP  - 32
JF  - Interacting with Computers
VL  - 32
IS  - 1
PB  - 
DO  - 10.1093/iwcomp/iwaa002
ER  - 

TY  - BOOK
AU  - Kambayashi, Yasushi; Ueda, Keita; Kasahara, Masanari; Kusano, Tatsumi; Takimoto, Munehiro
TI  - IHSED - Development of a Prototype for Non-contact Keyboard
PY  - 2018
AB  - There are certain needs for non-contact input devices such as non-contact keyboard. Medical people such as doctors and nurses cannot use keyboards in the operation room because of the hygienic problem. Even ordinary people cannot use keyboards when they use head mounted display for full VR environment because they cannot see the keyboards. In order to solve such a problem, we propose employing a motion sensor device. A motion sensor device is a device for sensing motions. It detects human gestures such as handshakes and fingertip movements. Recently LeapMotion is gaining popularity as a non-contact input means, but we can observe few implementations of keyboard using LeapMotion. In this paper, we report our experiences of developing a non-contact keyboard system using LeapMotion.
SP  - 299
EP  - 304
JF  - Human Systems Engineering and Design
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-02053-8_46
ER  - 

TY  - NA
AU  - Peek, Nadya; Coleman, James; Moyer, Ilan; Gershenfeld, Neil
TI  - CHI - Cardboard Machine Kit: Modules for the Rapid Prototyping of Rapid Prototyping Machines
PY  - 2017
AB  - Digital fabrication machines (such as laser cutters or 3D printers) can be instructed to produce any part geometry within their application space. However, machines' application spaces are not easily modified or extended. How can we enable the production of application-specific computer-controlled machines by machine building novices? How can we facilitate rapid prototyping of rapid prototyping tools? We propose a novel set of modules, the Cardboard Machine Kit, for the construction of digital fabrication machines. These open-source modules are implemented using cardboard frames, stepper motors, and networked electronics controlled through a Python library. We evaluated the kit both through machine building workshops and by studying the usage of the kit in the wild. In the wild we observed more than 500 novice machine builders who built 125 different machines for 15 different application types. We argue that this breadth demonstrates the efficacy of this modular approach. Finally we discuss the limitations of the Cardboard Machine Kit and discuss how it could inform future machine building infrastructure.
SP  - 3657
EP  - 3668
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025491
ER  - 

TY  - BOOK
AU  - Hinckley, Ken; Buxton, Bill
TI  - Revolutionizing Education with Digital Ink - Inking Outside the Box: How Context Sensing Affords More Natural Pen (and Touch) Computing
PY  - 2016
AB  - The authors were invited to present a reprise of a recently-published paper on Sensing Techniques for Tablet \(+\) Stylus Interaction at the WIPTTE 2015 Workshop. The talk took the original contribution as a point of departure, because for the WIPTTE venue we felt that the most important role of the work was to illuminate and help the audience understand more deeply the interaction modalities of pen and touch—as well as their use in tandem. And in the process the authors felt like they came to understand the topic more deeply as well, hence the paper that follows.
SP  - 27
EP  - 47
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-31193-7_3
ER  - 

TY  - CHAP
AU  - Kaul, Oliver Beren; Domin, Andreas; Rohs, Michael; Simon, Benjamin; Schrapel, Maximilian
TI  - INTERACT (5) - VRTactileDraw: A Virtual Reality Tactile Pattern Designer for Complex Spatial Arrangements of Actuators
PY  - 2021
AB  - Creating tactile patterns on the body via a spatial arrangement of many tactile actuators offers many opportunities and presents a challenge, as the design space is enormous. This paper presents a VR interface that enables designers to rapidly prototype complex tactile interfaces. It allows for painting strokes on a modeled body part and translates these strokes into continuous tactile patterns using an interpolation algorithm. The presented VR approach avoids several problems of traditional 2D editors. It realizes spatial 3D input using VR controllers with natural mapping and intuitive spatial movements. To evaluate this approach in detail, we conducted a user study and iteratively improved the system. The study participants gave predominantly positive feedback on the presented VR interface (SUS score 79.7, AttrakDiff “desirable”). The final system is released alongside this paper as an open-source Unity project for various tactile hardware.
SP  - 212
EP  - 233
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85607-6_15
ER  - 

TY  - BOOK
AU  - Sidenmark, Ludwig; Mardanbegi, Diako; Gomez, Argenis Ramirez; Clarke, Christopher; Gellersen, Hans
TI  - ETRA - BimodalGaze: Seamlessly Refined Pointing with Gaze and Filtered Gestural Head Movement
PY  - 2020
AB  - Eye gaze is a fast and ergonomic modality for pointing but limited in precision and accuracy. In this work, we introduce BimodalGaze, a novel technique for seamless head-based refinement of a gaze cursor. The technique leverages eye-head coordination insights to separate natural from gestural head movement. This allows users to quickly shift their gaze to targets over larger fields of view with naturally combined eye-head movement, and to refine the cursor position with gestural head movement. In contrast to an existing baseline, head refinement is invoked automatically, and only if a target is not already acquired by the initial gaze shift. Study results show that users reliably achieve fine-grained target selection, but we observed a higher rate of initial selection errors affecting overall performance. An in-depth analysis of user performance provides insight into the classification of natural versus gestural head movement, for improvement of BimodalGaze and other potential applications.
SP  - 1
EP  - 9
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391312
ER  - 

TY  - NA
AU  - Richardson, Mark A.; Durasoff, Matt; Wang, Robert Y.
TI  - UIST - Decoding Surface Touch Typing from Hand-Tracking
PY  - 2020
AB  - We propose a novel text decoding method that enables touch typing on an uninstrumented flat surface. Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text. We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters. To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the fingers. Thus, we incorporate a language model as a text prior and use beam search to efficiently combine our motion and language models to decode text from erratic or ambiguous hand motion. We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contact-based text decoding and typing on a physical keyboard. Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an offline study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard. Our results show that hand-tracking has the potential to enable rapid text entry in mobile environments.
SP  - 686
EP  - 696
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415816
ER  - 

TY  - NA
AU  - Palleis, Henri; Wagner, Julie; Hussmann, Heinrich
TI  - AVI - Novel Indirect Touch Input Techniques Applied to Finger-Forming 3D Models
PY  - 2016
AB  - We address novel two-handed interaction techniques in dual display interactive workspaces combining direct and indirect touch input. In particular, we introduce the notion of a horizontal tool space with task-dependent graphical input areas. These input areas are designed as single purpose control elements for specific functions and allow users to manipulate objects displayed on a vertical screen using simple one- and two-finger touch gestures and both hands. For demonstrating this concept, we use 3D modeling tasks as a specific application area. Initial feedback of six expert users indicates that our techniques are easy to use and stimulate exploration rather than precise modeling. Further, we gathered qualitative feedback during a multi-session observational study with five novices who learned to use our tool and were interviewed several times. Preliminary results indicate that working with our setup is easy to learn and remember. Participants liked the partitioning character of the dual-surface setup and agreed on the benefiting quality of touch input, giving them a 'hands-on feeling'.
SP  - 228
EP  - 235
JF  - Proceedings of the International Working Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909132.2909257
ER  - 

TY  - NA
AU  - Yu, Chun; Gu, Yizheng; Zhican, Yang; Yi, Xin; Luo, Hengliang; Shi, Yuanchun
TI  - CHI - Tap, Dwell or Gesture?: Exploring Head-Based Text Entry Techniques for HMDs
PY  - 2017
AB  - Despite the increasing popularity of head mounted displays (HMDs), development of efficient text entry methods on these devices has remained under explored. In this paper, we investigate the feasibility of head-based text entry for HMDs, by which, the user controls a pointer on a virtual keyboard using head rotation. Specifically, we investigate three techniques: TapType, DwellType, and GestureType. Users of TapType select a letter by pointing to it and tapping a button. Users of DwellType select a letter by pointing to it and dwelling over it for a period of time. Users of GestureType perform word-level input using a gesture typing style. Two lab studies were conducted. In the first study, users typed 10.59 WPM, 15.58 WPM, and 19.04 WPM with DwellType, TapType, and GestureType, respectively. Users subjectively felt that all three of the techniques were easy to learn and considered the induced fatigue to be acceptable. In the second study, we further investigated GestureType. We improved its gesture-word recognition algorithm by incorporating the head movement pattern obtained from the first study. This resulted in users reaching 24.73 WPM after 60 minutes of training. Based on these results, we argue that head-based text entry is feasible and practical on HMDs, and deserves more attention.
SP  - 4479
EP  - 4488
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025964
ER  - 

TY  - NA
AU  - Lee, DoYoung; Kim, Jiwan; Oakley, Ian
TI  - CHI - FingerText: Exploring and Optimizing Performance for Wearable, Mobile and One-Handed Typing
PY  - 2021
AB  - Typing on wearables while situationally impaired, such as while walking, is challenging. However, while HCI research on wearable typing is diverse, existing work focuses on stationary scenarios and fine-grained input that will likely perform poorly when users are on-the-go. To address this issue we explore single-handed wearable typing using intra-hand touches between the thumb and fingers, a modality we argue will be robust to the physical disturbances inherent to input while mobile. We first examine the impact of walking on performance of these touches, noting no significant differences in accuracy or speed, then feed our study data into a multi-objective optimization process in order to design keyboard layouts (for both five and ten keys) capable of supporting rapid, accurate, comfortable, and unambiguous typing. A final study tests these layouts against QWERTY baselines and reports performance improvements of up to 10.45% WPM and 39.44% WER when users type while walking.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445106
ER  - 

TY  - CHAP
AU  - Alexander, Jason; Thanh, Chi
TI  - DSP Basics: Statistics, Digital Signal Processing, and Machine Learning in Practice
PY  - 2021
AB  - NA
SP  - 105
EP  - 139
JF  - Intelligent Computing for Interactive System Design
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447404.3447412
ER  - 

TY  - JOUR
AU  - Park, Kyeong-Beom; Choi, Sung Ho; Lee, Jae Yeol; Ghasemi, Yalda; Mohammed, Mustafa; Jeong, Heejin
TI  - Hands-Free Human–Robot Interaction Using Multimodal Gestures and Deep Learning in Wearable Mixed Reality
PY  - 2021
AB  - This study proposes a novel hands-free interaction method using multimodal gestures such as eye gazing and head gestures and deep learning for human-robot interaction (HRI) in mixed reality (MR) environments. Since human operators hold some objects for conducting tasks, there are many constrained situations where they cannot use their hands for HRI interactions. To provide more effective and intuitive task assistance, the proposed hands-free method supports coarse-to-fine interactions. Eye gazing-based interaction is used for coarse interactions such as searching and previewing of target objects, and head gesture interactions are used for fine interactions such as selection and 3D manipulation. In addition, deep learning-based object detection is applied to estimate the initial positioning of physical objects to be manipulated by the robot. The result of object detection is then combined with 3D spatial mapping in the MR environment for supporting accurate initial object positioning. Furthermore, virtual object-based indirect manipulation is proposed to support more intuitive and efficient control of the robot, compared with traditional direct manipulation (e.g., joint-based and end effector-based manipulations). In particular, a digital twin, the synchronized virtual robot of the real robot, is used to provide a preview and simulation of the real robot to manipulate it more effectively and accurately. Two case studies were conducted to confirm the originality and advantages of the proposed hands-free HRI: (1) performance evaluation of initial object positioning and (2) comparative analysis with traditional direct robot manipulations. The deep learning-based initial positioning reduces much effort for robot manipulation using eye gazing and head gestures. The object-based indirect manipulation also supports more effective HRI than previous direct interaction methods.
SP  - 55448
EP  - 55464
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3071364
ER  - 

TY  - JOUR
AU  - Lee, Lik Hang; Braud, Tristan; Lam, Kit Yung; Yau, Yui-Pan; Hui, Pan
TI  - From seen to unseen: Designing keyboard-less interfaces for text entry on the constrained screen real estate of Augmented Reality headsets
PY  - 2020
AB  - NA
SP  - 101148
EP  - NA
JF  - Pervasive and Mobile Computing
VL  - 64
IS  - NA
PB  - 
DO  - 10.1016/j.pmcj.2020.101148
ER  - 

TY  - JOUR
AU  - Chen, Huijie; Li, Fan; Hei, Xiaojun; Wang, Yu
TI  - CrowdX: Enhancing Automatic Construction of Indoor Floorplan with Opportunistic Encounters
PY  - 2018
AB  - The lack of floorplan limits the spread of pervasive indoor location-based services. Existing crowdsourcing based approaches mostly rely on identifying, locating landmarks in the environment and utilizing the spatial relationship between the landmarks and traces for efficiently constructing fine-grained floorplan. However, these methods are always restricted by the sparse landmark distribution or may cause privacy leakage. In this paper, we propose CrowdX, a crowdsourcing system for accurate, low-cost indoor floorplan construction enhanced with opportunistic encounters among mobile users. The key insight is that the spatial relation (i.e., the displacement of each user and the distance between each other during the encounter) will be extracted from the audio and inertia data, which are aligned by the proposed vibration event-based method. Such information can be used to calibrate the drift of encounter position. The calibrated encounter position is beneficial to most of the floorplan generation steps, such as trace drift elimination, landmark positioning, hallway assembling, and room area estimation. Our experiments in three shopping malls show that CrowdX achieves an average F-measure around 89.4%. In addition, the average estimated room area error within about 20%. The evaluation results demonstrate a significant improvement of accuracy enhanced with opportunistic encounters.
SP  - 159
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287037
ER  - 

TY  - JOUR
AU  - Terenti, Mihail; Vatavu, Radu-Daniel
TI  - VIREO: Web-based Graphical Authoring of Vibrotactile Feedback for Interactions with Mobile and Wearable Devices
PY  - 2022
AB  - NA
SP  - 1
EP  - 19
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2109584
ER  - 

TY  - NA
AU  - DeLong, Sean; Arif, Ahmed Sabbir; Mazalek, Ali
TI  - PerDis - Design and evaluation of graphical feedback on tangible interactions in a low-resolution edge display
PY  - 2019
AB  - We used the LED array around the edges of a custom tangible as a low-resolution display to provide users with real-time visual feedback on the current state of the system. We developed a guideline for mapping different types of edge feedback to different tangible interactions. We evaluated its effectiveness in two user studies. The first is an informal study, where experienced biologists worked with an existing tangible-tabletop biological system. The second is a formal study, where novice participants worked with a new tangible-tabletop biological system. Results of these studies suggest that edge feedback provides a better understanding of the system, increases user confidence, and can be useful in other interactive systems. Results also suggest that the proposed feedback mapping is intuitive and easy to remember.
SP  - NA
EP  - NA
JF  - Proceedings of the 8th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3321335.3324954
ER  - 

TY  - NA
AU  - Ablart, Damien
TI  - TVX (Adjunct) - Integration of Touch and Taste with Interactive Media
PY  - 2017
AB  - My PhD (started in Nov. 2015) focuses on the integration of touch and taste with interactive media. While technology, in particular haptic technology, is developing fast, there is a lack of understanding on what kind of experiences we can design when focusing on the sense of touch and taste. In particular, short overview on the research field, my work so far and an outlook on my future steps.
SP  - 109
EP  - 112
JF  - Adjunct Publication of the 2017 ACM International Conference on Interactive Experiences for TV and Online Video
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3084289.3084294
ER  - 

TY  - NA
AU  - Mahapatra, Chandan; Jensen, Jonas Kjeldmand; McQuaid, Michael; Ashbrook, Daniel
TI  - CHI - Barriers to End-User Designers of Augmented Fabrication
PY  - 2019
AB  - Augmented fabrication is the practice of designing and fabricating an artifact to work with existing objects. Although common both in the wild and as an area for research tools, little is known about how novices approach the task of designing under the constraints of interfacing with real-world objects. In this paper, we report the results of a study of fifteen novice end users in an augmented fabrication design task. We discuss obstacles encountered in four contexts: capturing information about physical objects, transferring information to 3D~modeling software, digitally modeling a new object, and evaluating whether the new object will work when fabricated. Based on our findings, we suggest how future tools can better support augmented fabrication in each of these contexts.
SP  - 383
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300613
ER  - 

TY  - JOUR
AU  - Lystbæk, Mathias N.; Rosenberg, Peter; Pfeuffer, Ken; Grønbæk, Jens Emil; Gellersen, Hans
TI  - Gaze-Hand Alignment
PY  - 2022
AB  - <jats:p>Gaze and freehand gestures suit Augmented Reality as users can interact with objects at a distance without need for a separate input device. We propose Gaze-Hand Alignment as a novel multimodal selection principle, defined by concurrent use of both gaze and hand for pointing and alignment of their input on an object as selection trigger. Gaze naturally precedes manual action and is leveraged for pre-selection, and manual crossing of a pre-selected target completes the selection. We demonstrate the principle in two novel techniques, Gaze&amp;Finger for input by direct alignment of hand and finger raised into the line of sight, and Gaze&amp;Hand for input by indirect alignment of a cursor with relative hand movement. In a menu selection experiment, we evaluate the techniques in comparison with Gaze&amp;Pinch and a hands-only baseline. The study showed the gaze-assisted techniques to outperform hands-only input, and gives insight into trade-offs in combining gaze with direct or indirect, and spatial or semantic freehand gestures.</jats:p>
SP  - 1
EP  - 18
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ETRA
PB  - 
DO  - 10.1145/3530886
ER  - 

TY  - JOUR
AU  - Sidenmark, Ludwig; Gellersen, Hans
TI  - Eye, Head and Torso Coordination During Gaze Shifts in Virtual Reality
PY  - 2019
AB  - Humans perform gaze shifts naturally through a combination of eye, head and body movements. Although gaze has been long studied as input modality for interaction, this has previously ignored the coordination of the eyes, head and body. This article reports a study of gaze shifts in virtual reality aimed to address the gap and inform design. We identify general eye, head and torso coordination patterns and provide an analysis of the relative movements’ contribution and temporal alignment. We quantify effects of target distance, direction and user posture, describe preferred eye-in-head motion ranges and identify a high variability in head movement tendency. Study insights lead us to propose gaze zones that reflect different levels of contribution from eye, head and body. We discuss design implications for HCI and VR, and in conclusion argue to treat gaze as multimodal input, and eye, head and body movement as synergetic in interaction design.
SP  - 1
EP  - 40
JF  - ACM Transactions on Computer-Human Interaction
VL  - 27
IS  - 1
PB  - 
DO  - 10.1145/3361218
ER  - 

TY  - NA
AU  - Gupta, Aakar; Samad, Majed; Kin, Kenrick; Kristensson, Per Ola; Benko, Hrvoje
TI  - ISMAR - Investigating Remote Tactile Feedback for Mid-Air Text-Entry in Virtual Reality
PY  - 2020
AB  - In this paper, we investigate the utility of remote tactile feedback for freehand text-entry on a mid-air Qwerty keyboard in VR. To that end, we use insights from prior work to design a virtual keyboard along with different forms of tactile feedback, both spatial and non-spatial, for fingers and for wrists. We report on a multi-session text-entry study with 24 participants where we investigated four vibrotactile feedback conditions: on-fingers, on-wrist spatialized, on-wrist non-spatialized, and audio-visual only. We use micro-metrics analyses and participant interviews to analyze the mechanisms underpinning the observed performance and user experience. The results show comparable performance across feedback types. However, participants overwhelmingly prefer the tactile feedback conditions and rate on-fingers feedback as significantly lower in mental demand, frustration, and effort. Results also show that spatialization of vibrotactile feedback on the wrist as a way to provide finger-specific feedback is comparable in performance and preference to a single vibration location. The micro-metrics analyses suggest that users compensated for the lack of tactile feedback with higher visual and cognitive attention, which ensured similar performance but higher user effort.
SP  - 350
EP  - 360
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00062
ER  - 

TY  - JOUR
AU  - Xu, Xuhai; Yu, Chun; Wang, Yuntao; Shi, Yuanchun
TI  - Recognizing Unintentional Touch on Interactive Tabletop
PY  - 2020
AB  - A multi-touch interactive tabletop is designed to embody the benefits of a digital computer within the familiar surface of a physical tabletop. However, the nature of current multi-touch tabletops to detect and react to all forms of touch, including unintentional touches, impedes users from acting naturally on them. In our research, we leverage gaze direction, head orientation and screen contact data to identify and filter out unintentional touches, so that users can take full advantage of the physical properties of an interactive tabletop, e.g., resting hands or leaning on the tabletop during the interaction. To achieve this, we first conducted a user study to identify behavioral pattern differences (gaze, head and touch) between completing usual tasks on digital versus physical tabletops. We then compiled our findings into five types of spatiotemporal features, and train a machine learning model to recognize unintentional touches with an F1 score of 91.3%, outperforming the state-of-the-art model by 4.3%. Finally we evaluated our algorithm in a real-time filtering system. A user study shows that our algorithm is stable and the improved tabletop effectively screens out unintentional touches, and provide more relaxing and natural user experience. By linking their gaze and head behavior to their touch behavior, our work sheds light on the possibility of future tabletop technology to improve the understanding of users' input intention.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3381011
ER  - 

TY  - NA
AU  - Shi, Yuanchun
TI  - UMAP - Interpreting User Input Intention in Natural Human Computer Interaction
PY  - 2018
AB  - Human Computer Interaction (HCI)1 is about information exchange between human and computers. Interaction between users and computers occurs at the User Interface (UI). Now, computers become pervasive, they are embedded in everyday things and UIs are the main value-added competitive advantages. UIs should be more natural for users. NUI (natural user interface) expands forms beyond formal input devices like the mouse and keyboard to more and more natural forms of interaction such as touch, speech, gestures, handwriting, and vision. Unlike speech, handwriting and vision, which have been researched for decades and put into practical use recently, touch and gestures are interaction tasks related, and yet lack of study. This talk will introduce methods of modelling user input action based on data with the random noise for fast touch input and natural gestures.
SP  - 277
EP  - 278
JF  - Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3209219.3209267
ER  - 

TY  - CHAP
AU  - Seifi, Hasti
TI  - Tuning Vibrations with Emotion Controls
PY  - 2019
AB  - When refining or personalizing a design, we count on being able to modify or move an element by changing its parameters rather than creating it anew in a different form or location—a standard utility in graphic and auditory authoring tools. Similarly, in our study of haptic personalization mechanisms in Chap. 3, users preferred the tuning mechanism the most. For tactile vibration display, however, we lack knowledge of the human perceptual mappings which must underlie such tools. Based on evidence that affective dimensions are a natural way to tune vibrations for practical purposes, we attempted to manipulate perception along three emotion dimensions (agitation, liveliness, and strangeness) using engineering parameters of hypothesized relevance. Results from two user studies show that an automatable algorithm can increase a vibration’s perceived agitation and liveliness to different degrees via signal energy, while increasing its discontinuity or randomness makes it more strange. These continuous mappings apply across diverse base vibrations; the extent of achievable emotion change varies. These results illustrate the potential for developing vibrotactile emotion controls as efficient tuning for designers and end-users.
SP  - 135
EP  - 161
JF  - Springer Series on Touch and Haptic Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-11379-7_7
ER  - 

TY  - NA
AU  - Grubert, Jens; Kranz, Matthias
TI  - VR - Towards ad hoc mobile multi-display environments on commodity mobile devices
PY  - 2017
AB  - We present a demonstration of HeadPhones (Headtracking + smart-Phones), a novel approach for the spatial registration of multiple mobile devices into an ad hoc multi-display environment. We propose to employ the user's head as external reference frame for the registration of multiple mobile devices into a common coordinate system. Our approach allows for dynamic repositioning of devices during runtime without the need for external infrastructure such as separate cameras or fiducials. Specifically, our only requirements are local network connections and mobile devices with built-in front facing cameras. This way, HeadPhones enables spatially-aware multi-display applications in mobile contexts.
SP  - 461
EP  - 462
JF  - 2017 IEEE Virtual Reality (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2017.7892379
ER  - 

TY  - NA
AU  - Carter, Scott; Avrahami, Daniel; Tokunaga, Nami
TI  - Using Inaudible Audio to Improve Indoor-Localization- and Proximity-Aware Intelligent Applications.
PY  - 2020
AB  - While it is often critical for indoor-location- and proximity-aware applications to know whether a user is in a space or not (e.g., a specific room or office), a key challenge is that the difference between standing on one side or another of a doorway or wall is well within the error range of most RF-based approaches. In this work, we address this challenge by augmenting RF-based localization and proximity detection with active ultrasonic sensing, taking advantage of the limited propagation of sound waves. This simple and cost-effective approach can allow, for example, a Bluetooth smart-lock to discern whether a user is inside or outside their home in order to lock or unlock doors automatically. We describe a configurable architecture for our solution and present experiments that validate this approach but also demonstrate that different user behavior and application needs can impact system configuration decisions. Finally, we describe applications that could benefit from our solution and address privacy concerns.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Lipari, Nicholas G.; Borst, Christoph W.
TI  - 3DUI - Toward vibrotactile rendering for irregular 2D tactor arrays
PY  - 2016
AB  - We motivate further study of vibrotactile rendering schemes for the sensation of arbitrary points in irregular grids or meshes, outline a conceptual approach, and propose a study for assessing and comparing approaches. A conceptual model presents the combination of vibrations from multiple elements (tactors) as a two-stage pairing of tactors into virtual tactors, considering the 2D dimensionality. To support irregular triangle meshes, we suggest parameters to characterize triangle shape and a future study to measure sensations for varying shape. Gathered data will be used to assess and compare perceptual combination models and to develop precise rendering functions for irregular triangle meshes.
SP  - 257
EP  - 258
JF  - 2016 IEEE Symposium on 3D User Interfaces (3DUI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/3dui.2016.7460068
ER  - 

TY  - NA
AU  - Xu, Han; Yang, Zheng; Zhou, Zimu; Yi, Ke; Peng, Chunyi
TI  - INFOCOM - TUM: Towards ubiquitous multi-device localization for cross-device interaction
PY  - 2017
AB  - Cross-device interaction is becoming an increasingly hot topic as we often have multiple devices at our immediate disposal in this era of mobile computing. Various cross-device applications such as file sharing, multi-screen display, and cross-device authentication have been proposed and investigated. However, one of the most fundamental enablers remains unsolved: How to achieve ubiquitous multi-device localization? Though pioneer efforts have resorted to gesture-assisted or sensing-assisted localization, they either require extensive user participation or impose some strong assumptions on device sensing abilities. This introduces extra costs and constraints, and thus degrades their practicality. To overcome these limitations, we propose TUM, an acoustic-assisted localization scheme Towards Ubiquitous Multi-device localization. The basic idea of TUM is to utilize the dual-microphones and speakers to obtain distance cues among devices. At the same time it resolves the location ambiguity with the help of MEMS sensors. We devise techniques for distance constraint extraction, static localization, continuous localization, and multi-device localization, and build a prototype that runs on commodity devices. Extensive experiments show that TUM provides a real-time 3D relative localization service under 10cm mean error for both static and continuous localization.
SP  - 1
EP  - 9
JF  - IEEE INFOCOM 2017 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom.2017.8057102
ER  - 

TY  - NA
AU  - Ushirobira, Rosane; Efimov, Denis; Casiez, Géry; Roussel, Nicolas; Perruquetti, Wilfrid
TI  - ECC - A forecasting algorithm for latency compensation in indirect human-computer interactions
PY  - 2016
AB  - Human-computer interactions are greatly affected by the latency between the human input and the system visual response. The compensation of this latency is an important problem for the HCI (human-computer interaction) community. In this work, a simple forecasting algorithm is developed for latency compensation in indirect interaction using a mouse, based on numerical differentiation. Several differentiators are compared, including a novel algebraic version. An optimized procedure is developed for tuning the parameters of the algorithm. The efficiency is demonstrated on real data, measured with a 1 ms sampling time.
SP  - 6
EP  - NA
JF  - 2016 European Control Conference (ECC)
VL  - 2016
IS  - NA
PB  - 
DO  - 10.1109/ecc.2016.7810433
ER  - 

TY  - CHAP
AU  - Adhikary, Jiban; Vertanen, Keith
TI  - INTERACT (4) - Typing on Midair Virtual Keyboards: Exploring Visual Designs and Interaction Styles
PY  - 2021
AB  - We investigate typing on a QWERTY keyboard rendered in virtual reality. Our system tracks users’ hands in the virtual environment via a Leap Motion mounted on the front of a head mounted display. This allows typing on an auto-correcting midair keyboard without the need for auxiliary input devices such as gloves or handheld controllers. It supports input via the index fingers of one or both hands. We compare two keyboard designs: a normal QWERTY layout and a split layout. We found users typed at around 16 words-per-minute using one or both index fingers on the normal layout, and about 15 words-per-minute using both index fingers on the split layout. Users had a corrected error rate below 2% in all cases. To explore midair typing with limited or no visual feedback, we had users type on an invisible keyboard. Users typed on this keyboard at 11 words-per-minute at an error rate of 3.3% despite the keyboard providing almost no visual feedback.
SP  - 132
EP  - 151
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85610-6_9
ER  - 

TY  - JOUR
AU  - Langner, Ricardo; Horak, Tom; Dachselt, Raimund
TI  - V is T iles : Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration
PY  - 2017
AB  - We present V is T iles , a conceptual framework that uses a set of mobile devices to distribute and coordinate visualization views for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devices offer the potential to provide a dynamic and user-defined interface supporting co-located collaborative data exploration with different individual workflows. As part of our framework, we contribute concepts that enable users to interact with coordinated & multiple views (CMV) that are distributed across several mobile devices. The major components of the framework are: (i) dynamic and flexible layouts for CMV focusing on the distribution of views and (ii) an interaction concept for smart adaptations and combinations of visualizations utilizing explicit side-by-side arrangements of devices. As a result, users can benefit from the possibility to combine devices and organize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a specific instance of our concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. We also illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts and the final prototype.
SP  - 626
EP  - 636
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2017.2744019
ER  - 

TY  - NA
AU  - Newell, Anthony; George, Abraham; Papakostas, Nikolaos; Lhachemi, Hugo; Malik, Ammar; Shorten, Robert
TI  - ICE/ITMC - On Design for Additive Manufacturing: Review of Challenges and Opportunities utilising Visualisation Technologies
PY  - 2019
AB  - Design for additive manufacturing poses new challenges and opportunities for manufacturers to produce highly customised parts while reducing cost, production time and improving quality. Manufacturing constraints of conventional manufacturing methods, such as geometric complexity limitations and workpiece handling, have shaped the landscape of computer-aided design tools, which are therefore not suitably adapted to design for additive manufacturing. Furthermore, computer-aided design tools require a high level of training to produce appropriate models. Augmented reality and feedback technologies pose an interesting opportunity for design for additive manufacturing, whereby the interaction with 3D models in an augmented or virtual design space can provide intuitive feedback to engineers and designers, providing fast validation of designs, parametric modelling and opportunities for training and use in both professional and amateur designer communities. This paper will explore and review the opportunities this exciting new technology provides.
SP  - 1
EP  - 7
JF  - 2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ice.2019.8792569
ER  - 

TY  - NA
AU  - Carter, Marcus; Newn, Joshua; Velloso, Eduardo; Vetere, Frank
TI  - OZCHI - Remote Gaze and Gesture Tracking on the Microsoft Kinect: Investigating the Role of Feedback
PY  - 2015
AB  - In this paper we present the results of a user experience and preference study into the combination of gaze and gesture in a lounge-style remote-interaction, using a novel system that tracks gaze and gesture using only the Kinect device at a distance of 2m from the user. Our results indicate exciting opportunities for gaze-tracking interfaces that use existing technologies, but suggest that findings from studies of highly-accurate gaze systems may not apply in these real-world simulations where the gaze-tracking is inherently less accurate. We contribute a series of design recommendations for gaze and gesture interfaces in this context, and based on these limitations.
SP  - 167
EP  - 176
JF  - Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2838739.2838778
ER  - 

TY  - JOUR
AU  - Dudley, John; Vertanen, Keith; Kristensson, Per Ola
TI  - Fast and Precise Touch-Based Text Entry for Head-Mounted Augmented Reality with Variable Occlusion
PY  - 2018
AB  - We present the VISAR keyboard: An augmented reality (AR) head-mounted display (HMD) system that supports text entry via a virtualised input surface. Users select keys on the virtual keyboard by imitating the process of single-hand typing on a physical touchscreen display. Our system uses a statistical decoder to infer users’ intended text and to provide error-tolerant predictions. There is also a high-precision fall-back mechanism to support users in indicating which keys should be unmodified by the auto-correction process. A unique advantage of leveraging the well-established touch input paradigm is that our system enables text entry with minimal visual clutter on the see-through display, thus preserving the user’s field-of-view. We iteratively designed and evaluated our system and show that the final iteration of the system supports a mean entry rate of 17.75wpm with a mean character error rate less than 1%. This performance represents a 19.6% improvement relative to the state-of-the-art baseline investigated: A gaze-then-gesture text entry technique derived from the system keyboard on the Microsoft HoloLens. Finally, we validate that the system is effective in supporting text entry in a fully mobile usage scenario likely to be encountered in industrial applications of AR HMDs.
SP  - 30
EP  - 40
JF  - ACM Transactions on Computer-Human Interaction
VL  - 25
IS  - 6
PB  - 
DO  - 10.1145/3232163
ER  - 

TY  - CHAP
AU  - Bu, Xingyuan; Pei, Mingtao; Jia, Yunde
TI  - ICONIP (4) - Attention Estimation for Input Switch in Scalable Multi-display Environments
PY  - 2016
AB  - Multi-Display Environments (MDEs) have become commonplace in office desks for editing and displaying different tasks, such as coding, searching, reading, and video-communicating. In this paper, we present a method of automatic switch for routing one input (including mouse/keyboard, touch pad, joystick, etc.) to different displays in scalable MDEs based on the user attention estimation. We set up an MDE in our office desk, in which each display is equipped with a webcam to capture the user’s face video for detecting if the user is looking at the display. We use Convolutional Neural Networks (CNNs) to learn the attention model from face videos with various poses, illuminations, and occlusions for achieving a high performance of attention estimation. Qualitative and quantitative experiments demonstrate the effectiveness and potential of the proposed approach. The results of the user study also shows that the participants deemed that the system is wonderful, useful, and friendly.
SP  - 329
EP  - 336
JF  - Neural Information Processing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-46681-1_40
ER  - 

TY  - NA
AU  - Millette, Alexandre; McGuffin, Michael J.
TI  - ISMAR Adjunct - DualCAD: Integrating Augmented Reality with a Desktop GUI and Smartphone Interaction
PY  - 2016
AB  - Head-Mounted Displays (HMDs) combined with 3-or-more Degree-of-Freedom (DoF) input enable rapid manipulation of stereoscopic 3D content. However, such input is typically performed with hands in midair and therefore lacks precision and stability. Also, recent consumer-grade HMDs suffer from limited angular resolution and/or limited field-of-view as compared to a desktop monitor. We present the DualCAD system that implements two solutions to these problems. First, the user may freely switch at runtime between an augmented reality HMD mode, and a traditional desktop mode with precise 2D mouse input and an external desktop monitor. Second, while in the augmented reality HMD mode, the user holds a smartphone in their non-dominant hand that is tracked with 6 DoF, allowing it to be used as a complementary high-resolution display as well as an alternative input device for stylus or multitouch input. Two novel bimanual interaction techniques that leverage the properties of the smartphone are presented. We also report initial user feedback.
SP  - 21
EP  - 26
JF  - 2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2016.0030
ER  - 

TY  - NA
AU  - Schüssel, Felix; Bäurle, Johannes; Kotzka, Simon; Weber, Michael; Pittino, Ferdinand; Huckauf, Anke
TI  - UbiComp Adjunct - Design and evaluation of a gaze tracking system for free-space interaction
PY  - 2016
AB  - Although using eye trackers for gaze tracking in human computer interaction gained a lot of attention, generic approaches that allow mobile and free-space interactions for numerous applications are still rare. Known approaches often focus on specific applications and provide only rough estimates for the achieved accuracies or impose sever restrictions on the user's movements. We present the design and detailed evaluation of a mobile gaze tracking system that can be generally applied in pervasive and ubiquitous applications that rely on free-space interactions. Using automated image processing, we also present a fast method for evaluating the achieved accuracies of the eye tracking device and the marker-based mapping onto a screen placed in the vicinity. Evaluation results not only provide details necessary for later applications in ubiquitous environments, but also reveal that manufacturer's specification of the eye tracker's accuracy is barely met in actual use.
SP  - 1676
EP  - 1685
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2968336
ER  - 

TY  - CHAP
AU  - Hoshi, Yuya; Lu, Chenghong; Jing, Lei
TI  - HCI (41) - Haptic Finger Glove for the VR Keyboard Input
PY  - 2021
AB  - With regards to use Virtual Reality headsets, people have many opportunities of key entry. However, current methods for key entry are suffering from low input speed and high error rate. In this paper, we suggest a new key entry system, Air keyboard with haptic finger glove. By applying haptic illusions such as phantom sensation and apparent movement, the finger glove could simulate the tactile feedback such as typing and unevenness sensation with low cost. Several controlled experiments were conducted to evaluate the efficiency and the accuracy of the air keyboard together with haptic finger glove. The result shows that the haptic finger glove could help user to better interact with the virtual keyboard. Moreover, a short-term training could increase the input efficiency and reduce the error rate, which is comparable or better than traditional pointing method with handheld controllers.
SP  - 31
EP  - 43
JF  - HCI International 2021 - Late Breaking Papers: Multimodality, eXtended Reality, and Artificial Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90963-5_3
ER  - 

TY  - CHAP
AU  - Yamato, Yuki; Suzuki, Yutaro; Takahashi, Shin
TI  - INTERACT (5) - FGFlick: Augmenting Single-Finger Input Vocabulary for Smartphones with Simultaneous Finger and Gaze Flicks.
PY  - 2021
AB  - FGFlick is an interactive technique featuring simultaneous single-finger operation and a gaze. The user flicks a smartphone and moves their gaze linearly. FGFlick thus augments the single-finger input vocabulary. As a result of the evaluation of the FGFlick gestures, we achieved success rates of 84.0%.
SP  - 421
EP  - 425
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85607-6_50
ER  - 

TY  - NA
AU  - Seo, Kyeongeun; Cho, Hyeonjoong; Choi, Daewoong; Lee, Sangyub; Lee, Jaekyu; Ko, Jae-jin
TI  - ICIP - TwohandsMusic: Multitask Learning-Based Egocentric Piano-Playing Gesture Recognition System for Two Hands
PY  - 2019
AB  - We present TwohandsMusic, a new real-time system for recognizing egocentric piano-playing gestures on planar objects by using a depth camera. Existing methods have usually recognized single tap gestures of one hand using a sensor installed in front of or under the user’s hand. In contrast, we consider recognizing multi-tap gestures of both hands using a depth camera installed near the user’s head. Our approach consists of two steps: hand detection and gesture recognition. At the hand detection step, we detect both hands using a 2DCNN (Convolutional Neural Network), called SegNet, and generate cropped hand images, which is to be used in the next step. In the gesture recognition step, we estimate 3D hand poses and classify multi-tap gestures simultaneously using a 3DCNN with multitask learning, called MusicNet. For training and validating of our system, we collect 85K dataset including tapping chords and show improved results over state-of-the-art methods.
SP  - 4614
EP  - 4618
JF  - 2019 IEEE International Conference on Image Processing (ICIP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icip.2019.8803568
ER  - 

TY  - NA
AU  - Peng, Huaishu; Wu, Rundong; Marschner, Steve; Guimbretière, François
TI  - CHI - On-The-Fly Print: Incremental Printing While Modelling
PY  - 2016
AB  - Current interactive fabrication tools offer tangible feedback by allowing users to work directly on the physical model, but they are slow because users need to participate in the physical instantiation of their designs. In contrast, CAD software offers powerful tools for 3D modeling but delays access to the physical workpiece until the end of the design process. In this paper we propose On-the-Fly Print: a 3D modeling approach that allows the user to design 3D models digitally while having a low-fidelity physical wireframe model printed in parallel. Our software starts printing features as soon as they are created and updates the physical model as needed. Users can quickly check the design in a real usage context by removing the partial physical print from the printer and replacing it afterwards to continue printing. Digital content modification can be updated with quick physical correction using a retractable cutting blade. We present the detailed description of On-the-Fly Print and showcase several examples designed and printed with our system.
SP  - 887
EP  - 896
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858106
ER  - 

TY  - NA
AU  - Ko, Donghyeon; Lee, Yujin; Bin Yim, Jee; Lee, Woohun
TI  - UIST (Adjunct Volume) - HeatMat: Designing Internal Structures for Supporting Hands-on Design Activity with Heated 3D Printed Objects
PY  - 2019
AB  - Recently, adopting a hands-on approach to conventional 3D fabrication has been attracting attention due to its advantages in design activity. In this context, we aim to support hands-on design activity in digital fabrication by designing internal structures for alleviating issues of external heating for shape deformation. As a first step, we simulate four simple structures with Computational Fluid Dynamic (CFD) simulation to investigate effective structural parameters such as cavity's ratio, its geometry, exposure to the heat source for influencing thermal properties, and deformation in a malleable state. Through the pilot experiment, we figured out that the simulation results of the basic structures are valid, the structure is stable in a malleable state, and the parameters are effective. In the future, we will design functional structures based on the explored parameters and embed them on various topologies.
SP  - 47
EP  - 49
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357111
ER  - 

TY  - NA
AU  - Marquardt, Nicolai; Riche, Nathalie Henry; Holz, Christian; Romat, Hugo; Pahud, Michel; Brudy, Frederik; Ledo, David; Park, Chunjong; Nicholas, Molly Jane; Seyed, Teddy; Ofek, Eyal; Lee, Bongshin; Buxton, William A. S.; Hinckley, Ken
TI  - UIST - AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures
PY  - 2021
AB  - AirConstellations supports a unique semi-fixed style of cross-device interactions via multiple self-spatially-aware armatures to which users can easily attach (or detach) tablets and other devices. In particular, AirConstellations affords highly flexible and dynamic device formations where the users can bring multiple devices together in-air — with 2–5 armatures poseable in 7DoF within the same workspace — to suit the demands of their current task, social situation, app scenario, or mobility needs. This affords an interaction metaphor where relative orientation, proximity, attaching (or detaching) devices, and continuous movement into and out of ad-hoc ensembles can drive context-sensitive interactions. Yet all devices remain self-stable in useful configurations even when released in mid-air. We explore flexible physical arrangement, feedforward of transition options, and layering of devices in-air across a variety of multi-device app scenarios. These include video conferencing with flexible arrangement of the person-space of multiple remote participants around a shared task-space, layered and tiled device formations with overview+detail and shared-to-personal transitions, and flexible composition of UI panels and tool palettes across devices for productivity applications. A preliminary interview study highlights user reactions to AirConstellations, such as for minimally disruptive device formations, easier physical transitions, and balancing ”seeing and being seen” in remote work.
SP  - 1252
EP  - 1268
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474820
ER  - 

TY  - JOUR
AU  - Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers
PY  - 2020
AB  - Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.
SP  - 3490
EP  - 3502
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - 26
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2020.3023567
ER  - 

TY  - JOUR
AU  - Yu, Difeng; Fan, Kaixuan; Zhang, Heng; Monteiro, Diego; Xu, Wenge; Liang, Hai-Ning
TI  - PizzaText: Text Entry for Virtual Reality Systems Using Dual Thumbsticks
PY  - 2018
AB  - We present PizzaText, a circular keyboard layout technique for text entry in virtual reality (VR) environments that uses the dual thumbsticks of a hand-held game controller. Text entry is a common activity in VR environments but remains challenging with existing techniques and keyboard layouts that is largely based on QWERTY. Our technique makes text entry simple, easy, and efficient, even for novice users. The technique uses a hand-held controller because it is still an important input device for users to interact with VR environments. To allow rapid search of characters, PizzaText divides a circle into slices and each slice contains 4 characters. To enable fast selection, the user uses the right thumbstick for traversing the slices, and the left thumbstick for choosing the letters. The design of PizzaText is based on three criteria: efficiency, learnability, and ease-of-use. In our first study, six potential layouts are considered and evaluated. The results lead to a design with 7 slices and 4 letters per slice. The final design is evaluated in a five-day study with 10 participants. The results show that novice users can achieve an average of 8.59 Words per Minute (WPM), while expert users are able to reach 15.85 WPM, with just two hours of training.
SP  - 2927
EP  - 2935
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2018.2868581
ER  - 

TY  - CHAP
AU  - Frutos-Pascual, Maite; Gale, Clara; Harrison, Jake Michael; Creed, Chris; Williams, Ian
TI  - INTERACT (1) - Character Input in Augmented Reality: An Evaluation of Keyboard Position and Interaction Visualisation for Head-Mounted Displays
PY  - 2021
AB  - Character input in immersive environments is a non trivial task that has attracted much attention in recent years. This paper presents an evaluation of keyboard position, orientation and interaction together with the influence of visual interaction feedback in a controlled character input task with 27 participants in Augmented Reality (AR). It presents 5 different keyboard locations (3 bounded to the headset and 2 bounded to the non-dominant hand of the user) and 3 visual interaction feedback methods (finger raycast, fingertip glow and both combined). Objective (completion time, accuracy, Key per Minute (KPM)) and subjective (After Scenario Questionnaire (ASQ)) metrics are presented. Results showed that keyboard placement had an effect on accuracy, KPM metrics and subjective preference, with keyboard visualisation parallel and bounded to the headset position and orientation outperforming other keyboard locations.
SP  - 480
EP  - 501
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85623-6_29
ER  - 

TY  - NA
AU  - Sugano, Yusuke; Zhang, Xucong; Bulling, Andreas
TI  - UIST - AggreGaze: Collective Estimation of Audience Attention on Public Displays
PY  - 2016
AB  - Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.
SP  - 821
EP  - 831
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984536
ER  - 

TY  - JOUR
AU  - Koutsabasis, Panayiotis; Vogiatzidakis, Panagiotis
TI  - Empirical Research in Mid-Air Interaction: A Systematic Review
PY  - 2019
AB  - ABSTRACTMid-air interaction is a distinct style of natural HCI (Human-Computer Interaction). In mid-air interaction, users make use of their whole body—with a strong focus on hands—and apply gestur...
SP  - 1747
EP  - 1768
JF  - International Journal of Human–Computer Interaction
VL  - 35
IS  - 18
PB  - 
DO  - 10.1080/10447318.2019.1572352
ER  - 

TY  - BOOK
AU  - Choi, Myungguen; Sakamoto, Daisuke; Ono, Tetsuo
TI  - ETRA - Bubble Gaze Cursor + Bubble Gaze Lens: Applying Area Cursor Technique to Eye-Gaze Interface
PY  - 2020
AB  - We conducted two studies exploring how an area cursor technique can improve the eye-gaze interface. We first examined the bubble cursor technique. We developed an eye-gaze-based cursor called the bubble gaze cursor and compared it to a standard eye-gaze interface and a bubble cursor with a mouse. The results revealed that the bubble gaze cursor interface was faster than the standard point cursor-based eye-gaze interface. In addition, the usability and mental workload were significantly better than those of the standard interface. Next, we extended the bubble gaze cursor technique and developed a bubble gaze lens. The results indicated that the bubble gaze lens technique was faster than the bubble gaze cursor method and the error rate was reduced by 54.0%. The usability and mental workload were also considerably better than those of the bubble gaze cursor.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391322
ER  - 

TY  - NA
AU  - Zhang, Yang; Pahud, Michel; Holz, Christian; Xia, Haijun; Laput, Gierad; McGuffin, Michael J.; Tu, Xiao; Mittereder, Andrew Pyon; Su, Fei; Buxton, William A. S.; Hinckley, Ken
TI  - CHI - Sensing Posture-Aware Pen+Touch Interaction on Tablets
PY  - 2019
AB  - Many status-quo interfaces for tablets with pen + touch input capabilities force users to reach for device-centric UI widgets at fixed locations, rather than sensing and adapting to the user-centric posture. To address this problem, we propose sensing techniques that transition between various nuances of mobile and stationary use via postural awareness. These postural nuances include shifting hand grips, varying screen angle and orientation, planting the palm while writing or sketching, and detecting what direction the hands approach from. To achieve this, our system combines three sensing modalities: 1) raw capacitance touchscreen images, 2) inertial motion, and 3) electric field sensors around the screen bezel for grasp and hand proximity detection. We show how these sensors enable posture-aware pen+touch techniques that adapt interaction and morph user interface elements to suit fine-grained contexts of body-, arm-, hand-, and grip-centric frames of reference.
SP  - 55
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300285
ER  - 

TY  - NA
AU  - Grubert, Jens; Witzani, Lukas; Ofek, Eyal; Pahud, Michel; Kranz, Matthias; Kristensson, Per Ola
TI  - VR - Text Entry in Immersive Head-Mounted Display-Based Virtual Reality Using Standard Keyboards
PY  - 2018
AB  - We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40–45% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing.
SP  - 159
EP  - 166
JF  - 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2018.8446059
ER  - 

TY  - NA
AU  - Antoine, Axel; Malacria, Sylvain; Casiez, Géry
TI  - CHI - Using High Frequency Accelerometer and Mouse to Compensate for End-to-end Latency in Indirect Interaction
PY  - 2018
AB  - End-to-end latency corresponds to the temporal difference between a user input and the corresponding output from a system. It has been shown to degrade user performance in both direct and indirect interaction. If it can be reduced to some extend, latency can also be compensated through software compensation by trying to predict the future position of the cursor based on previous positions, velocities and accelerations. In this paper, we propose a hybrid hardware and software prediction technique specifically designed for partially compensating end-to-end latency in indirect pointing. We combine a computer mouse with a high frequency accelerometer to predict the future location of the pointer using Euler based equations. Our prediction method results in more accurate prediction than previously introduced prediction algorithms for direct touch. A controlled experiment also revealed that it can improve target acquisition time in pointing tasks.
SP  - 609
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174183
ER  - 

TY  - NA
AU  - Aranovskiy, Stanislav; Ushirobira, Rosane; Efimov, Denis; Casiez, Géry
TI  - CDC - Frequency domain forecasting approach for latency reduction in direct human-computer interaction
PY  - 2017
AB  - The problem of latency reduction in direct humancomputer interaction is considered and formulated as a trajectory prediction problem. To solve the problem, the predictor is constructed as a frequency-domain approximation of the noncasual ideal predictor. This approximation can be computed analytically, or obtained as an optimization task. An adaptive modification of the forecasting algorithm is proposed taking into account possible variations in user behavior. Experimental results illustrate the applicability of the proposed solution.
SP  - 2623
EP  - 2628
JF  - 2017 IEEE 56th Annual Conference on Decision and Control (CDC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cdc.2017.8264040
ER  - 

TY  - NA
AU  - Sun, Ke; Wang, Wei; Liu, Alex X.; Dai, Haipeng
TI  - MobiSys - Depth Aware Finger Tapping on Virtual Displays
PY  - 2018
AB  - For AR/VR systems, tapping-in-the-air is a user-friendly solution for interactions. Most prior in-air tapping schemes use customized depth-cameras and therefore have the limitations of low accuracy and high latency. In this paper, we propose a fine-grained depth-aware tapping scheme that can provide high accuracy tapping detection. Our basic idea is to use light-weight ultrasound based sensing, along with one COTS mono-camera, to enable 3D tracking of user's fingers. The mono-camera is used to track user's fingers in the 2D space and ultrasound based sensing is used to get the depth information of user's fingers in the 3D space. Using speakers and microphones that already exist on most AR/VR devices, we emit ultrasound, which is inaudible to humans, and capture the signal reflected by the finger with the microphone. From the phase changes of the ultrasound signal, we accurately measure small finger movements in the depth direction. With fast and light-weight ultrasound signal processing algorithms, our scheme can accurately track finger movements and measure the bending angle of the finger between two video frames. In our experiments on eight users, our scheme achieves a 98.4% finger tapping detection accuracy with FPR of 1.6% and FNR of 1.4%, and a detection latency of 17.69ms, which is 57.7ms less than video-only schemes. The power consumption overhead of our scheme is 48.4% more than video-only schemes.
SP  - 283
EP  - 295
JF  - Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210240.3210315
ER  - 

TY  - NA
AU  - Elmadjian, Carlos; Morimoto, Carlos H.
TI  - CHI Extended Abstracts - GazeBar: Exploiting the Midas Touch in Gaze Interaction
PY  - 2021
AB  - Imagine an application that requires constant configuration changes, such as modifying the brush type in a drawing application. Typically, options are hierarchically organized in menu bars that the user must navigate, sometimes through several levels, to select the desired mode. An alternative to reduce hand motion is the use of multimodal techniques such as gaze-touch, that combines gaze pointing with mechanical selection. In this paper, we introduce GazeBar, a novel multimodal gaze interaction technique that uses gaze paths as a combined pointing and selection mechanism. The idea behind GazeBar is to maximize the interaction flow by reducing ”safety” mechanisms (such as clicking) under certain circumstances. We present GazeBar’s design and demonstrate it using a digital drawing application prototype. Advantages and disadvantages of GazeBar are discussed based on a user performance model.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451703
ER  - 

TY  - BOOK
AU  - Dobosz, Krzysztof; Popanda, Dominik; Sawko, Adrian
TI  - ICMMI - Head-Based Text Entry Methods for Motor-Impaired People
PY  - 2019
AB  - This article describes methods for text input using head mounted devices. The efficiency of six different virtual keyboards in a 3D environment were evaluated during experiments. Some of used techniques were already known earlier (dwell time, multimodality, swipes), while others are new (control of virtual cursor arrows, 8-button layout, half-star layout). Although the obtained results are not groundbreaking, the obtained results allow a good prognosis for head-based methods using VR goggles.
SP  - 3
EP  - 11
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-31964-9_1
ER  - 

TY  - JOUR
AU  - Le Goc, Mathieu; Perin, Charles; Follmer, Sean; Fekete, Jean-Daniel; Dragicevic, Pierre
TI  - Dynamic Composite Data Physicalization Using Wheeled Micro-Robots
PY  - 2018
AB  - This paper introduces dynamic composite physicalizations , a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.
SP  - 737
EP  - 747
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865159
ER  - 

TY  - NA
AU  - Wakimoto, Tomomasa; Takamori, Ryoma; Eguchi, Soya; Tanaka, Hiroya
TI  - CHI Extended Abstracts - Growable Robot with 'Additive-Additive-Manufacturing'
PY  - 2018
AB  - Additive manufacturing, the academic term of 3D-Printing, is the way of fabricating new objects by adding materials layer-by-layer. Our team extended this technology to the next level enabling users to add materials directly onto ready-made (3D-printed) objects. Therefore, we named our technology as 'Additive-Additive-Manufacturing'. While 'Additive-Additive-Manufacturing' is one of general-purpose technologies, we believe that one of the killer-applications using this technology is to customize and modify body parts of personal pet robots. In this paper, we report our concept, software and hardware technologies of additive-additive-manufacturing, and prototypes of a growable pet robot and its potentials.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188449
ER  - 

TY  - JOUR
AU  - Tokac, Iremnur; Philips, Johan; Bruyninckx, Herman; Moere, Andrew Vande
TI  - Fabrication grammars: bridging design and robotics to control emergent material expressions
PY  - 2021
AB  - Designers physically engage with a material to experience how certain characteristics allow the fabrication of unique expressions. In digital fabrication, however, this improvisational negotiation is typically replaced by a virtual simulation that predicts how a material expression can be fabricated, limiting the resulting design language to algorithmic forms. In contrast, we believe that digital fabrication can also produce ‘emergent’ material expressions that are so confounded that they appear slightly differently even when being produced by identical operations. This paper argues that such expressions can be executed by a domain-driven feedback paradigm, which integrates a human-in-the-loop to encode the tacit fabrication knowledge that is generated by reviewing intermediate outcomes. We encode this tacit knowledge by fabrication grammars, rule-based descriptions that causally relate fabrication parameters to qualitative descriptions of material expressions. By documenting a set of Single Point Incremental Forming experiments, this paper demonstrates how emergent material expressions can be controlled by semantically meaningful fabrication grammars, which even can be combined towards purposeful design goals. We believe our findings might allow the digital fabrication of material expressions that appear to have been produced manually or naturally; and support the future sharing of tacit fabrication knowledge.
SP  - 35
EP  - 48
JF  - Construction Robotics
VL  - 5
IS  - 1
PB  - 
DO  - 10.1007/s41693-021-00053-0
ER  - 

TY  - NA
AU  - Ko, Donghyeon; Shin, Yeeun; Shin, Junbeom; Hong, Jiwoo; Lee, Woohun
TI  - ChromoFilament: Designing a Thermochromic Filament for Displaying Malleable States
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533479
ER  - 

TY  - NA
AU  - Jiang, Haiyan; Weng, Dongdong; Zhang, Zhenliang; Bao, Yihua; Jia, Yufei; Nie, Mengman
TI  - ISMAR Adjunct - HiKeyb: High-Efficiency Mixed Reality System for Text Entry
PY  - 2018
AB  - Text entry is an imperative issue to be addressed in current entry systems for virtual environments (VEs). The entry method using a physical keyboard is still the most dominant choice for an efficient interaction regarding text entry. In this paper, we propose a typing system with a style of mixed reality, which is called HiKeyb, and it possesses a similar high-efficiency with the single physical keyboard in the real environment. The HiKeyb system consists of a depth camera, a pose tracking module, a head-mounted display (HMD), a QWERTY keyboard and a black table mat. This system can guarantee the entry efficiency and the amenity by not only introducing the force feedback from a movable physical keyboard, but also improving the immersion with the real hand image. In addition, the infrared absorption material helps improve the robustness of the system against different lighting environments. Experiments have proved that users wearing HMDs in Virtual Phrases session can achieve an entry rate of 23.1 words per minute and an error rate of 2.76%, and the rate ratio of virtual reality to real world is 78% when typing phrases. Besides, we find that the proposed system can provide a relatively close entry efficiency to that using a pure physical keyboard in the real environment.
SP  - 132
EP  - 137
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2018.00051
ER  - 

TY  - BOOK
AU  - Sidenmark, Ludwig; Lundström, Anders
TI  - ETRA - Gaze behaviour on interacted objects during hand interaction in virtual reality for eye tracking calibration
PY  - 2019
AB  - In this paper, we investigate the probability and timing of attaining gaze fixations on interacted objects during hand interaction in virtual reality, with the main purpose for implicit and continuous eye tracking re-calibration. We conducted an evaluation with 15 participants in which their gaze was recorded while interacting with virtual objects. The data was analysed to find factors influencing the probability of fixations at different phases of interaction for different object types. The results indicate that 1) interacting with stationary objects may be favourable in attaining fixations to moving objects, 2) prolonged and precision-demanding interactions positively influences the probability to attain fixations, 3) performing multiple interactions simultaneously can negatively impact the probability of fixations, and 4) feedback can initiate and end fixations on objects.
SP  - 6
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319815
ER  - 

TY  - NA
AU  - Le Goc, Mathieu; Kim, Lawrence H.; Parsaei, Ali; Fekete, Jean-Daniel; Dragicevic, Pierre; Follmer, Sean
TI  - UIST - Zooids: Building Blocks for Swarm User Interfaces
PY  - 2016
AB  - This paper introduces swarm user interfaces, a new class of human-computer interfaces comprised of many autonomous robots that handle both display and interaction. We describe the design of Zooids, an open-source open-hardware platform for developing tabletop swarm interfaces. The platform consists of a collection of custom-designed wheeled micro robots each 2.6 cm in diameter, a radio base-station, a high-speed DLP structured light projector for optical tracking, and a software framework for application development and control. We illustrate the potential of tabletop swarm user interfaces through a set of application scenarios developed with Zooids, and discuss general design considerations unique to swarm user interfaces.
SP  - 97
EP  - 109
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984547
ER  - 

TY  - CHAP
AU  - Gulay, Emrecan; Lucero, Andrés
TI  - INTERACT (2) - Understanding the Role of Physical and Digital Techniques in the Initial Design Processes of Architecture
PY  - 2021
AB  - Architecture has been taking new turns with rapidly developing digital design and fabrication technologies. Consequently, establishing a link between physical and virtual design methods remains an open area for investigation. This paper explores the contemporary idea generation methods and the role of physical and digital design techniques in the initial design processes of architecture. We report our findings from interviews conducted with 14 participants consisting of experts and practitioners from the architecture field. Then, we discuss potential application areas of the results in the context of HCI research.
SP  - 312
EP  - 329
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85616-8_19
ER  - 

TY  - JOUR
AU  - Yeo, Hui-Shyong; Minami, Ryosuke; Rodriguez, Kirill; Shaker, George; Quigley, Aaron
TI  - Exploring Tangible Interactions with Radar Sensing
PY  - 2018
AB  - Research has explored miniature radar as a promising sensing technique for the recognition of gestures, objects, users' presence and activity. However, within Human-Computer Interaction (HCI), its use remains underexplored, in particular in Tangible User Interface (TUI). In this paper, we explore two research questions with radar as a platform for sensing tangible interaction with the counting, ordering, identification of objects and tracking the orientation, movement and distance of these objects. We detail the design space and practical use-cases for such interaction which allows us to identify a series of design patterns, beyond static interaction, which are continuous and dynamic. With a focus on planar objects, we report on a series of studies which demonstrate the suitability of this approach. This exploration is grounded in both a characterization of the radar sensing and our rigorous experiments which show that such sensing is accurate with minimal training. With these techniques, we envision both realistic and future applications and scenarios. The motivation for what we refer to as Solinteraction, is to demonstrate the potential for radar-based interaction with objects in HCI and TUI.
SP  - 200
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287078
ER  - 

TY  - NA
AU  - Seo, Jongman; Mun, Sunung; Lee, Jaebong; Choi, Seungmoon
TI  - CHI - Substituting Motion Effects with Vibrotactile Effects for 4D Experiences
PY  - 2018
AB  - In this paper, we present two methods to substitute motion effects using vibrotactile effects in order to improve the 4D experiences of viewers. This work was motivated by the needs of more affordable 4D systems for individual users. Our sensory substitution algorithms convert motion commands to vibrotactile commands to a grid display that uses multiple actuators. While one method is based on the fundamental principle of vestibular feedback, the other method makes use of intuitive visually-based mapping from motion to vibrotactile stimulation. We carried out a user study and could confirm the effectiveness of our substitution methods in improving 4D experiences. To our knowledge, this is the first study that investigated the feasibility of replacing motion effects using much simpler and less expensive vibrotactile effects.
SP  - 428
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174002
ER  - 

TY  - JOUR
AU  - Seifi, Hasti; Chun, Mattew; MacLean, Karon E.
TI  - Toward Affective Handles for Tuning Vibrations
PY  - 2018
AB  - When refining or personalizing a design, we count on being able to modify or move an element by changing its parameters rather than creating it anew in a different form or location—a standard utility in graphic and auditory authoring tools. Similarly, we need to tune vibrotactile sensations to fit new use cases, distinguish members of communicative icon sets, and personalize items. For tactile vibration display, however, we lack knowledge of the human perceptual mappings that must underlie such tools. Based on evidence that affective dimensions are a natural way to tune vibrations for practical purposes, we attempted to manipulate perception along three emotion dimensions (agitation, liveliness, and strangeness) using engineering parameters of hypothesized relevance. Results from two user studies show that an automatable algorithm can increase a vibration’s perceived agitation and liveliness to different degrees via signal energy, while increasing its discontinuity or randomness makes it more strange. These continuous mappings apply across diverse base vibrations; the extent of achievable emotion change varies. These results illustrate the potential for developing vibrotactile emotion controls as efficient tuning for designers and end-users.
SP  - 22
EP  - 23
JF  - ACM Transactions on Applied Perception
VL  - 15
IS  - 3
PB  - 
DO  - 10.1145/3230645
ER  - 

TY  - NA
AU  - Yamanaka, Shota; Usuba, Hiroki; Takahashi, Haruki; Miyashita, Homei
TI  - UIST - Servo-Gaussian Model to Predict Success Rates in Manual Tracking: Path Steering and Pursuit of 1D Moving Target
PY  - 2020
AB  - We propose a Servo-Gaussian model to predict success rates in continuous manual tracking tasks. Two tasks were conducted to validate this model: path steering and pursuit of a 1D moving target. We hypothesized that (1) hand movements follow the servo-mechanism model, (2) submovement endpoints form a bivariate Gaussian distribution, thus enabling us to predict the success rate at which a submovement endpoint falls inside the tolerance, and (3) the success rate for a whole trial can be predicted if the number of submovements is known. The cross-validation showed R^2>0.92 and MAE0.95 and MAE
SP  - 844
EP  - 857
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415896
ER  - 

TY  - JOUR
AU  - Xin, Yizhong; Liu, Ruonan; Li, Yan
TI  - Strategy for Improving Target Selection Accuracy in Indirect Touch Input
PY  - 2020
AB  - NA
SP  - 1703
EP  - 1709
JF  - IEICE Transactions on Information and Systems
VL  - 103
IS  - 7
PB  - 
DO  - 10.1587/transinf.2019edp7218
ER  - 

TY  - NA
AU  - Kwon, Young D.; Shatilov, Kirill A.; Lee, Lik Hang; Kumyol, Serkan; Lam, Kit-Yung; Yau, Yui-Pan; Hui, Pan
TI  - PerCom Workshops - MyoKey: Surface Electromyography and Inertial Motion Sensing-based Text Entry in AR
PY  - 2020
AB  - The seamless textual input in Augmented Reality (AR) is very challenging and essential for enabling user-friendly AR applications. Existing approaches such as speech input and vision-based gesture recognition suffer from environmental obstacles and the large default keyboard size, sacrificing the majority of the screen's real estate in AR. In this paper, we propose MyoKey, a system that enables users to effectively and unobtrusively input text in a constrained environment of AR by jointly leveraging surface Electromyography (sEMG) and Inertial Motion Unit (IMU) signals transmitted by wearable sensors on a user's forearm. MyoKey adopts a deep learning-based classifier to infer hand gestures using sEMG. In order to show the feasibility of our approach, we implement a mobile AR application using the Unity application building framework. We present novel interaction and system designs to incorporate information of hand gestures from sEMG and arm motions from IMU to provide seamless text entry solution. We demonstrate the applicability of MyoKey by conducting a series of experiments achieving the accuracy of 0.91 on identifying five gestures in real-time (Inference time: 97.43 ms).
SP  - 1
EP  - 4
JF  - 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/percomworkshops48775.2020.9156084
ER  - 

TY  - NA
AU  - Abdelraouf, Mostafa
TI  - Virtual reality for the characterization of blood vessel to airway geometric relationships
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.17077/etd.vdslt404
ER  - 

TY  - JOUR
AU  - Schneider, Oliver; MacLean, Karon E.; Swindells, Colin; Booth, Kellogg S.
TI  - Haptic experience design
PY  - 2017
AB  - NA
SP  - 5
EP  - 21
JF  - International Journal of Human-Computer Studies
VL  - 107
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2017.04.004
ER  - 

TY  - JOUR
AU  - Pezent, Evan; Cambio, Brandon; OrMalley, Marcia K.
TI  - Syntacts: Open-Source Software and Hardware for Audio-Controlled Haptics
PY  - 2021
AB  - As vibrotactile feedback systems become increasingly commonplace, their application scenarios are becoming more complex. In this article, we present a method of vibrotactor control that accommodates emerging design requirements, namely large vibrotactor arrays that are capable of displaying complex waveforms. Our approach is based on control through digital audio interfaces. We describe a new open-source software and hardware package, Syntacts, that lowers the technical barrier to rendering vibrations with audio. We also present a tutorial on common control schemes with a discussion of their advantages and shortcomings. Our software is purpose-built to control arrays of vibrotactors with extremely low latency. In addition, Syntacts includes means to synthesize and sequence cues, and spatialize them on tactile arrays. The Syntacts Amplifier integrates with the audio hardware to provide high-quality analog signals to the tactors without adding excess noise to the system. Finally, we present results from a benchmarking study with Syntacts compared to commercially available systems.
SP  - 225
EP  - 233
JF  - IEEE transactions on haptics
VL  - 14
IS  - 1
PB  - 
DO  - 10.1109/toh.2020.3002696
ER  - 

TY  - JOUR
AU  - Ma, Zhao; Duenser, Simon; Schumacher, Christian; Rust, Romana; Bächer, Moritz; Gramazio, Fabio; Kohler, Matthias; Coros, Stelian
TI  - Stylized robotic clay sculpting
PY  - 2021
AB  - Abstract This paper presents an interactive design system that allows the user to create and fabricate stylized sculptures in water-based clay, using a standard 6-axis robot arm. This system facilitates the materialization of abstract design intentions into clay, through the algorithmic formulation of sculpting styles, the optimal path planning of the sculpting toolpaths, and a subtractive robotic fabrication process using customized tools. Unlike other precision-driven fabrication technologies, the authors embrace artistic uncertainty by conducting manual and robotic sculpting experiments and incorporating prominent parameters that affect the fabrication quality. The versatility of the described approach is demonstrated by designing a series of sculpting styles over a wide range of 3D models and robotically fabricating them in clay. Additionally, the paper explores various strategies for designing stylized robotic sculpting patterns by generating toolpaths informed by different techniques.
SP  - 150
EP  - 164
JF  - Computers & Graphics
VL  - 98
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.05.008
ER  - 

TY  - NA
AU  - Nisser, Martin; Liao, Christina Chen; Chai, Yuchen; Adhikari, Aradhana; Hodges, Steve; Mueller, Stefanie
TI  - CHI - LaserFactory: A Laser Cutter-based Electromechanical Assembly and Fabrication Platform to Make Functional Devices & Robots
PY  - 2021
AB  - LaserFactory is an integrated fabrication process that augments a commercially available fabrication machine to support the manufacture of fully functioning devices without human intervention. In addition to creating 2D and 3D mechanical structures, LaserFactory creates conductive circuit traces with arbitrary geometries, picks-and-places electronic and electromechanical components, and solders them in place. To enable this functionality, we make four contributions. First, we build a hardware add-on to the laser cutter head that can deposit silver circuit traces and assemble components. Second, we develop a new method to cure dispensed silver using a CO2 laser. Third, we build a motion-based signaling method that allows our system to be readily integrated with commercial laser cutters. Finally, we provide a design and visualization tool for making functional devices with LaserFactory. Having described the LaserFactory system, we demonstrate how it is used to fabricate devices such as a fully functioning quadcopter and a sensor-equipped wristband. Our evaluation shows that LaserFactory can assemble a variety of differently sized components (up to 65g), that these can be connected by narrow traces (down to 0.75mm) that become highly conductive after laser soldering (3.2Ω/m), and that our acceleration-based sensing scheme works reliably (to 99.5% accuracy).
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445692
ER  - 

TY  - NA
AU  - Zheng, Clement; Yong, Zhen Zhou; Lin, Hongnan; Oh, HyunJoo; Yen, Ching Chiuan
TI  - Shape-Haptics: Planar & Passive Force Feedback Mechanisms for Physical Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501829
ER  - 

TY  - NA
AU  - Mueller, Stefanie; Seufert, Anna; Peng, Huaishu; Kovacs, Robert; Reuss, Kevin; Guimbretière, François; Baudisch, Patrick
TI  - Tangible and Embedded Interaction - FormFab: Continuous Interactive Fabrication
PY  - 2019
AB  - Several systems have illustrated the concept of interactive fabrication, i.e. rather than working through a digital editor, users make edits directly on the physical workpiece. However, so far the interaction has been limited to turn-taking, i.e., users first perform a command and then the system responds with physical feedback. In this paper, we present a first step towards interactive fabrication that changes the workpiece continuously while the user is manipulating it.To achieve this, our system FormFab does not add or subtract material but instead reshapes it (formative fabrication). A heat gun attached to a robotic arm warms up a thermoplastic sheet until it becomes compliant; users then control a pneumatic system that applies either pressure or vacuum thereby pushing the material outwards or pulling it inwards. Since FormFab reshapes the workpiece continuously while users are moving their hands, users can interactively explore different sizes of a shape with a single interaction.
SP  - 315
EP  - 323
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295620
ER  - 

TY  - NA
AU  - Yi, Xin; Wang, Chen; Bi, Xiaojun; Shi, Yuanchun
TI  - CHI - PalmBoard: Leveraging Implicit Touch Pressure in Statistical Decoding for Indirect Text Entry
PY  - 2020
AB  - We investigated how to incorporate implicit touch pressure, finger pressure applied to a touch surface during typing, to improve text entry performance via statistical decoding. We focused on one-handed touch-typing on indirect interface as an example scenario. We first collected typing data on a pressure-sensitive touchpad, and analyzed users' typing behavior such as touch point distribution, key-to-finger mappings, and pressure images. Our investigation revealed distinct pressure patterns for different keys. Based on the findings, we performed a series of simulations to iteratively optimize the statistical decoding algorithm. Our investigation led to a Markov-Bayesian decoder incorporating pressure image data into decoding. It improved the top-1 accuracy from 53% to 74% over a naive Bayesian decoder. We then implemented PalmBoard, a text entry method that implemented the Markov-Bayesian decoder and effectively supported one-handed touch-typing on indirect interfaces. A user study showed participants achieved an average speed of 32.8 WPM with 0.6% error rate. Expert typists could achieve 40.2 WPM with 30 minutes of practice. Overall, our investigation showed that incorporating implicit touch pressure is effective in improving text entry decoding.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376441
ER  - 

TY  - NA
AU  - Schneider, Oliver; Shigeyama, Jotaro; Kovacs, Robert; Roumen, Thijs; Marwecki, Sebastian; Boeckhoff, Nico; Gloeckner, Daniel Amadeus; Bounama, Jonas; Baudisch, Patrick
TI  - UIST - DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds
PY  - 2018
AB  - We present a new haptic device that enables blind users to continuously interact with spatial virtual environments that contain moving objects, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand. Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand. This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.5/7).
SP  - 877
EP  - 887
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242604
ER  - 

TY  - JOUR
AU  - Yang, Zhen; Chen, Cheng; Lin, Yuqing; Wang, Duming; Li, Hongting; Xu, Weidan
TI  - Effect of spatial enhancement technology on input through the keyboard in virtual reality environment.
PY  - 2019
AB  - NA
SP  - 164
EP  - 175
JF  - Applied ergonomics
VL  - 78
IS  - NA
PB  - 
DO  - 10.1016/j.apergo.2019.03.006
ER  - 

TY  - NA
AU  - Schneider, Oliver; MacLean, Karon E.
TI  - HAPTICS - Studying design process and example use with Macaron, a web-based vibrotactile effect editor
PY  - 2016
AB  - Examples are a critical part of any design process, but supporting their use for a haptic medium is nontrivial. Current libraries for vibrotactile (VT) effects provide neither insight into examples' construction nor capability for deconstruction and re-composition. To investigate the special requirements of example use for VT design, we studied designers as they used a web-based effect editor, Macaron, which we created as both an evaluation platform and a practical tool. We qualitatively characterized participants' design processes and observed two basic example uses: as a starting point or template for a design task, and as a learning method. We discuss how features supporting internal visibility and composition influenced these example uses, and articulate several implications for VT editing tools and libraries of VT examples. We conclude with future work, including plans to deploy Macaron online to examine examples and other aspects of VT design in situ.
SP  - 52
EP  - 58
JF  - 2016 IEEE Haptics Symposium (HAPTICS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/haptics.2016.7463155
ER  - 

TY  - NA
AU  - Fashimpaur, Jacqui; Kin, Kenrick; Longest, Matt
TI  - CHI Extended Abstracts - PinchType: Text Entry for Virtual and Augmented Reality Using Comfortable Thumb to Fingertip Pinches
PY  - 2020
AB  - Text entry is an integral component to many use cases in virtual and augmented reality. We present PinchType: A new method of virtual text entry that combines users' existing knowledge of the QWERTY keyboard layout with simple thumb and finger interactions. Users pinch with the thumb and fingertip to select from the same group of letters the finger would press on a QWERTY keyboard; a language model disambiguates. In a preliminary study with 14 participants, we investigated PinchType's speed and accuracy on initial use, as well as its physical comfort relative to a mid-air keyboard. After entering 40 phrases, most people reported that PinchType was more comfortable than the mid-air keyboard. Most participants reached a mean speed of 12.54 WPM, or 20.07 WPM without the time spent correcting errors. This compares favorably to other thumb-to-finger virtual text entry methods.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382888
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Lam, Kit Yung; Yau, Yui-Pan; Braud, Tristan; Hui, Pan
TI  - PerCom - HIBEY: Hide the Keyboard in Augmented Reality
PY  - 2019
AB  - Text input is a very challenging task in Augmented Reality (AR). On non-touch AR headsets, virtual keyboards are counter-intuitive and character keys are hard to locate inside the constrained screen real estate. In this paper, we present the design, implementation and evaluation of HIBEY, a text input system for smartglasses. HIBEY provides a fast, reliable, affordable, and easy-to-use text entry solution through vision-based freehand interactions. Supported by a probabilistic spatial model and a language model, a three-level holographic environment enables users to apply fast and continuous hand gesture to pick characters and predictive words in a keyboardless interface. Through the pilot study and a thorough evaluations lasting 8 days, we show that HIBEY leads to a mean text entry rate of 9.95 word per minute (WPM) with 96.06% accuracy, which is comparable to other state-of-the-art approaches. After 8 days, participants can achieve an average of 13.19 WPM. In addition, HIBEY only occupies 13.14% of the screen real estate at the edge region, which is 62.80% smaller than the default keyboard layout on Microsoft Hololens.
SP  - 1
EP  - 10
JF  - 2019 IEEE International Conference on Pervasive Computing and Communications (PerCom
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/percom.2019.8767420
ER  - 

TY  - NA
AU  - Antoine, Axel; Nancel, Mathieu; Ge, Ella; Zheng, Jingjie; Zolghadr, Navid; Casiez, Géry
TI  - UIST - Modeling and Reducing Spatial Jitter caused by Asynchronous Input and Output Rates
PY  - 2020
AB  - Jitter in interactive systems occurs when visual feedback is perceived as unstable or trembling even though the input signal is smooth or stationary. It can have multiple causes such as sensing noise, or feedback calculations introducing or exacerbating sensing imprecisions. Jitter can however occur even when each individual component of the pipeline works perfectly, as a result of the differences between the input frequency and the display refresh rate. This asynchronicity can introduce rapidly-shifting latencies between the rendered feedbacks and their display on screen, which can result in trembling cursors or viewports. % This paper contributes a better understanding of this particular type of jitter. We first detail the problem from a mathematical standpoint, from which we develop a predictive model of jitter amplitude as a function of input and output frequencies, and a new metric to measure this spatial jitter. Using touch input data gathered in a study, we developed a simulator to validate this model and to assess the effects of different techniques and settings with any output frequency. The most promising approach, when the time of the next display refresh is known, is to estimate (interpolate or extrapolate) the user's position at a fixed time interval before that refresh. % When input events occur at 125~Hz, as is common in touch screens, we show that an interval of 4 to 6~ms works well for a wide range of display refresh rates. This method effectively cancels most of the jitter introduced by input/output asynchronicity, while introducing minimal imprecision or latency.
SP  - 869
EP  - 881
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415833
ER  - 

TY  - BOOK
AU  - Kim, Jeeeun; Zheng, Clement; Takahashi, Haruki; Gross, Mark D.; Ashbrook, Daniel; Yeh, Tom
TI  - SCF - Compositional 3D printing: expanding & supporting workflows towards continuous fabrication
PY  - 2018
AB  - We present Compositional 3D Printing, recasting the 3D printer as a tool for expression that responds to real-time design decisions, analogous to composing a piece of music using a mixer. Our paradigm supports a wide range of inputs and interactions for designers, to be used in the moment; not only before printing, but anytime during production. We propose the design space of this digital fabrication paradigm, and outline methods and technical details with which researchers and practitioners can expand this space.
SP  - NA
EP  - NA
JF  - Proceedings of the 2nd ACM Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3213512.3213518
ER  - 

TY  - NA
AU  - Bulling, Andreas
TI  - UbiComp/ISWC Adjunct - Human visual behaviour for collaborative human-machine interaction
PY  - 2015
AB  - Non-verbal behavioural cues are fundamental to human communication and interaction. Despite significant advances in recent years, state-of-the-art human-machine systems still fall short in sensing, analysing, and fully "understanding" cues naturally expressed in everyday settings. Two of the most important non-verbal cues, as evidenced by a large body of work in experimental psychology and behavioural sciences, are visual (gaze) behaviour and body language. We envision a new class of collaborative human-machine systems that fully exploit the information content available in non-verbal human behaviour in everyday settings through joint analysis of human gaze and physical behaviour.
SP  - 901
EP  - 905
JF  - Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers - UbiComp '15
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2800835.2815378
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Huang, Da-Yuan; Wang, Chi; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong; Chen, Bing-Yu
TI  - CHI - Aarnio: Passive Kinesthetic Force Output for Foreground Interactions on an Interactive Chair
PY  - 2019
AB  - We propose a new type of haptic output for foreground interactions on an interactive chair, where input is carried out explicitly in the foreground of the user's consciousness. This type of force output restricts a user's motion by modulating the resistive force when rotating a seat, tilting the backrest, or rolling the chair. These interactions are useful for many applications in a ubiquitous computing environment, ranging from immersive VR games to rapid and private query of information for people who are occupied with other tasks (e.g. in a meeting). We carefully designed and implemented our proposed haptic force output on a standard office chair and determined the recognizability of five force profiles for rotating, tilting, and rolling the chair. We present the result of our studies, as well as a set of novel interaction techniques enabled by this new force output for chairs.
SP  - 672
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300902
ER  - 

TY  - NA
AU  - Konishi, Yukari; Hanamitsu, Nobuhisa; Outram, Benjamin I.; Minamizawa, Kouta; Sato, Ayahiko; Mizuguchi, Tetsuya
TI  - Synesthesia Suit
PY  - 2016
AB  - NA
SP  - 149
EP  - 149
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985739
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Kim, Jeeeun
TI  - CHI - 3D Pen + 3D Printer: Exploring the Role of Humans and Fabrication Machines in Creative Making
PY  - 2019
AB  - The emergence of a 3D pen brings 3D modeling from a screen-based computer-aided design (CAD) system and 3D printing to direct and rapid crafting by 3D doodling. However, 3D doodling remains challenging, requiring craft skills to rapidly express an idea, which is critical in creative making. We explore a new process of 3D modeling using 3D pen + 3D printer. Our pilot study shows that users need support to reduce the number of non-creative tasks to explore a wide design strategy. With the opportunity to invent a new 3D modeling process that needs to incorporate both a pen and printer, we propose techniques and a system that empower users to print while doodling to focus on creative exploration. Our user study shows that users can create diverse 3D models using a pen and printer. We discuss the roles of the human and fabrication machine for the future of fabrication.
SP  - 295
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300525
ER  - 

TY  - JOUR
AU  - Maher, Mary Lou; Lee, Lina
TI  - Designing for Gesture and Tangible Interaction
PY  - 2017
AB  - NA
SP  - i
EP  - 111
JF  - Synthesis Lectures on Human-Centered Informatics
VL  - 10
IS  - 2
PB  - 
DO  - 10.2200/s00758ed1v01y201702hci036
ER  - 

TY  - NA
AU  - Leen, Danny; Ramakers, Raf; Luyten, Kris
TI  - UIST - StrutModeling: A Low-Fidelity Construction Kit to Iteratively Model, Test, and Adapt 3D Objects
PY  - 2017
AB  - We present StrutModeling, a computationally enhanced construction kit that enables users without a 3D modeling background to prototype 3D models by assembling struts and hub primitives in physical space. Physical 3D models are immediately captured in software and result in readily available models for 3D printing. Given the concrete physical format of StrutModels, modeled objects can be tested and fine tuned in the presence of existing objects and specific needs of users. StrutModeling avoids puzzling with pieces by contributing an adjustable strut and universal hub design. Struts can be adjusted in length and snap to magnetic hubs in any configuration. As such, arbitrarily complex models can be modeled, tested, and adjusted during the design phase. In addition, the embedded sensing capabilities allow struts to be used as measuring devices for lengths and angles, and tune physical mesh models according to existing physical objects.
SP  - 471
EP  - 479
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126643
ER  - 

TY  - NA
AU  - Li, Yaxuan; Yoo, Yongjae; Weill-Duflos, Antoine; Cooperstock, Jeremy R.
TI  - Towards Context-aware Automatic Haptic Effect Generation for Home Theatre Environments
PY  - 2021
AB  - The application of haptic technology in entertainment systems, such as Virtual Reality and 4D cinema, enables novel experiences for users and drives the demand for efficient haptic authoring systems. Here, we propose an automatic multimodal vibrotactile content creation pipeline that substantially improves the overall hapto-audiovisual (HAV) experience based on contextual audio and visual content from movies. Our algorithm is implemented on a low-cost system with nine actuators attached to a viewing chair and extracts significant features from video files to generate corresponding haptic stimuli. We implemented this pipeline and used the resulting system in a user study (n = 16), quantifying user experience according to the sense of immersion, preference, harmony, and discomfort. The results indicate that the haptic patterns generated by our algorithm complement the movie content and provide an immersive and enjoyable HAV user experience. This further suggests that the pipeline can facilitate the efficient creation of 4D effects and could therefore be applied to improve the viewing experience in home theatre environments.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489887
ER  - 

TY  - NA
AU  - Lander, Christian
TI  - MobileHCI Adjunct - Methods for calibration free and multi-user eye tracking
PY  - 2016
AB  - Human beings sense and perceive most of the world through their eyes. The point of gaze clearly reflects our visual attention indicating our interests. Hence gaze can be used as a powerful tool in different research areas (e.g., marketing, psychology). The progress made over the years in eye tracking enables the creation of gaze-based interactive interfaces. However, these interfaces lack of generic usability outside a controlled environment in a spontaneous pervasive way. The main objective of this research is to investigate eye-tracking technologies by means of calibration. Since calibration is user, location, orientation and target dependent, it prevents from Multi-User interaction and gaze estimation on multiple various objects (e.g., multiple screens of different sizes). Tackling these issues, new mobile as well as remote interfaces are explored and new design spaces are opened.
SP  - 899
EP  - 900
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2957265.2963116
ER  - 

TY  - NA
AU  - Dassen, Wendy; Alonso, Miguel Bruns
TI  - Conference on Designing Interactive Systems (Companion Volume) - Aesthetics of Haptics: An Experience Approach to Haptic Interaction Design
PY  - 2017
AB  - Technological developments are expanding the interaction possibilities for the haptic domain, allowing designers to program dynamic haptic feedback. While haptic sensations are emerging in design practice, the relation between programmable haptics and the user experience remains underexplored. This study presents a research-through-design approach on the design of a push button and a knob with multiple haptic sensations. Interaction design of these haptic sensations started from an experience perspective building on material qualities. We reflect on the design process and present future directions for research.
SP  - 254
EP  - 259
JF  - Proceedings of the 2017 ACM Conference Companion Publication on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064857.3079156
ER  - 

TY  - JOUR
AU  - Lander, Christian; Löchtefeld, Markus; Krüger, Antonio
TI  - hEYEbrid: A hybrid approach for mobile calibration-free gaze estimation
PY  - 2018
AB  - We introduce hEYEbrid, a calibration-free method for spontaneous and long-term eye gaze tracking, with competitive gaze estimation. It is based on a hybrid concept that combines infrared eye images with corneal imaging. For this, two eye cameras are mounted on a glasses frame. In this way, the pupil can be tracked quickly with high precision. This information is translated into the corneal image, which is used to create a connection to the environment, acting like a scene camera. In a user study with 20 participants, we evaluated our approach against an extended version of the system, called 3C-hEYEbrid, and a state-of-the-art head-mounted Pupil Labs eye tracker. We show that hEYEbrid provides accurate gaze estimation in unconstrained environments and is robust against calibration drift (e.g. caused by taking off and putting on the device). In addition, we present a mobile and wearable implementation of hEYEbrid and 3C-hEYEbrid, that is also usable with a monocular Pupil Labs eye tracker. It connects the head-mounted device to a mobile phone, enabling gaze estimation in real time. hEYEbrid represents a significant step towards pervasive gaze-based interfaces.
SP  - 149
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161166
ER  - 

TY  - NA
AU  - Park, Gunhyuk; Choi, Seungmoon
TI  - CHI - Tactile Information Transmission by 2D Stationary Phantom Sensations
PY  - 2018
AB  - A phantom sensation refers to an illusory tactile sensation perceived midway between multiple distant stimulations on the skin. Phantom sensations have been used intensively in tactile interfaces owing to their simplicity and effectiveness. Despite that, the perceptual performance of phantom sensations is not completely understood, especially for 2D cases. This work is concerned with 2D stationary phantom sensations and their fundamental value as a means for information display. In User Study 1, we quantified the information transmission capacity using an absolute identification task of 2D phantom sensations. In User Study 2, we probed the distributions of the actual perceived positions of 2D phantom sensations. The investigations included both types of phantom sensations-within and out of the body. Our results provide general guidelines as to leveraging 2D phantom sensations in the design of spatial tactile display.
SP  - 258
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173832
ER  - 

TY  - BOOK
AU  - Müller, Philipp; Buschek, Daniel; Huang, Michael Xuelin; Bulling, Andreas
TI  - ETRA - Reducing calibration drift in mobile eye trackers by exploiting mobile phone usage
PY  - 2019
AB  - Automatic saliency-based recalibration is promising for addressing calibration drift in mobile eye trackers but existing bottom-up saliency methods neglect user's goal-directed visual attention in natural behaviour. By inspecting real-life recordings of egocentric eye tracker cameras, we reveal that users are likely to look at their phones once these appear in view. We propose two novel automatic recalibration methods that exploit mobile phone usage: The first builds saliency maps using the phone location in the egocentric view to identify likely gaze locations. The second uses the occurrence of touch events to recalibrate the eye tracker, thereby enabling privacy-preserving recalibration. Through in-depth evaluations on a recent mobile eye tracking dataset (N=17, 65 hours) we show that our approaches outperform a state-of-the-art saliency approach for automatic recalibration. As such, our approach improves mobile eye tracking and gaze-based interaction, particularly for long-term use.
SP  - 9
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319918
ER  - 

TY  - BOOK
AU  - Ahn, Sunggeun; Heo, Seongkook; Lee, Geehyuk
TI  - ISS - Typing on a Smartwatch for Smart Glasses
PY  - 2017
AB  - While smart glasses make information more accessible in mobile scenarios, entering text on these devices is still difficult. In this paper, we suggest using a smartwatch as an indirect input device for smart glasses text entry. With the watch-glasses combination, users do not need to lift the arm to touch the glasses nor need to carry a special external input device. To prove the feasibility of the suggested combination, we implemented two text entry methods: a modified version of SwipeBoard, which we adapted for the suggested combination, and HoldBoard, which we newly designed and implemented specifically for the suggested combination. We evaluated the performances of the two text entry methods through two user studies, and could show that they are faster than prior art for smart glasses text entry in a seated condition. A further study showed that they are competitive with the prior art also in a walking condition.
SP  - 201
EP  - 209
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134136
ER  - 

TY  - BOOK
AU  - Chioino, Jamil; Contreras, Ivan; Barrientos, Alfredo; Vives, Luis
TI  - ICISDM - Designing a decision tree for Cross-device communication technology aimed at iOS and Android developers
PY  - 2018
AB  - This analysis proposes a decision tree for selecting cross-device communication technologies for iOS and Android mobile devices. This tree accelerates the selection of cross-device technologies by taking into account known use cases of interaction. Five different communication technologies were tested (Real-time Multiplayer, Nearby Messages, PeerJS, iBeacon and Eddystone) by means of 13 proof of concept applications distributed between both operating systems (Android-iOS, iOS-iOS, Android-Android) and the design of 20 architecture diagrams of three types: sequence (connection to services and message sending), deployment and component. The decision tree was validated by mobile development experts resulting in a maximum reduction of up to 30 days of technology selection research. The effectiveness of the tree as a tool is 60%, its usefulness 80% and its ease of comprehension 90%, according to the results obtained from the experts.
SP  - 81
EP  - 87
JF  - Proceedings of the 2nd International Conference on Information System and Data Mining
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206098.3206103
ER  - 

TY  - NA
AU  - Kaul, Oliver Beren; Rohs, Michael
TI  - CHI - HapticHead: A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality
PY  - 2017
AB  - Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4% vs. 54.2% success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is as expected more precise (99.7% success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a head-mounted display.
SP  - 3729
EP  - 3740
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025684
ER  - 

TY  - NA
AU  - Yokogawa, Takashi; Zhu, Wei; Matsumoto, Masaaki; Zang, Jun; Tanaka, Ayumu; Ano, Masaaki; Tobita, Hiroaki
TI  - ICT-DM - SitusCall: Location-Based Mobile Communication for Group Calls and Human Augmentation
PY  - 2019
AB  - We introduce SitusCall, a new system that enhances group communication in emergencies quickly. The sytem allows users to communicate face-to-face with location information through a smartphone. To detect a user's position, the system employs GPS and several beacons, and any nearby user can simply select certain other users to initiate communication. Thus, it does not require personal authentication information, such as an email address or a telephone number. Also, SitusCall affords communication with a flexible streaming layout. The system provides several predefined layouts for face-to-face communication from which users can select to communicate with other connected users. Yet our approach is so simple and direct that it is useful for emergency cases. For example, if a person wants to announce evacuation information (e.g., about dangerous places or conditions) to other people nearby, he or she can simply select icons and connect with those people instantly and share information with them. In this paper, we describe SitusCall in terms of its design, implementation, evaluation, and applications.
SP  - 1
EP  - 8
JF  - 2019 International Conference on Information and Communication Technologies for Disaster Management (ICT-DM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ict-dm47966.2019.9032965
ER  - 

TY  - NA
AU  - Casiez, Géry; Pietrzak, Thomas; Marchal, Damien; Poulmane, Sebastien; Falce, Matthieu; Roussel, Nicolas
TI  - UIST - Characterizing Latency in Touch and Button-Equipped Interactive Systems
PY  - 2017
AB  - We present a low cost method to measure and characterize the end-to-end latency when using a touch system (tap latency) or an input device equipped with a physical button. Our method relies on a vibration sensor attached to a finger and a photo-diode to detect the screen response. Both are connected to a micro-controller connected to a host computer using a low-latency USB communication protocol in order to combine software and hardware probes to help determine where the latency comes from. We present the operating principle of our method before investigating the main sources of latency in several systems. We show that most of the latency originates from the display side. Our method can help application designers characterize and troubleshoot latency on a wide range of interactive systems.
SP  - 29
EP  - 39
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126606
ER  - 

TY  - NA
AU  - Berger, Christopher C.; Gonzalez-Franco, Mar
TI  - SAP - Expanding the sense of touch outside the body
PY  - 2018
AB  - Under normal circumstances, our sense of touch is limited to our body. Recent evidence suggests, however, that our perception of touch can also be expanded to objects we are holding when certain tactile illusions are elicited by delivering vibrotactile stimuli in a particular manner. Here, we examined whether an extra-corporeal illusory sense of touch could be elicited using vibrotactile stimuli delivered via two independent handheld controllers while in virtual reality. Our results suggest that under the right conditions, one's sense of touch in space can be extended outside the body, and even into the empty space that surrounds us. Specifically, we show, in virtual reality, that one's sense of touch can be extended to a virtual stick one is holding, and also into the empty space between one's hands. These findings provide a means with which to expand the sense of touch beyond the hands in VR systems using two independent controllers, and also have important implications for our understanding of the human representation of touch.
SP  - 10
EP  - NA
JF  - Proceedings of the 15th ACM Symposium on Applied Perception
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3225153.3225172
ER  - 

TY  - NA
AU  - Zhuang, Yuzhou; Wang, Yuntao; Yan, Yukang; Xu, Xuhai; Shi, Yuanchun
TI  - UIST - ReflecTrack: Enabling 3D Acoustic Position Tracking Using Commodity Dual-Microphone Smartphones
PY  - 2021
AB  - 3D position tracking on smartphones has the potential to unlock a variety of novel applications, but has not been made widely available due to limitations in smartphone sensors. In this paper, we propose ReflecTrack, a novel 3D acoustic position tracking method for commodity dual-microphone smartphones. A ubiquitous speaker (e.g., smartwatch or earbud) generates inaudible Frequency Modulated Continuous Wave (FMCW) acoustic signals that are picked up by both smartphone microphones. To enable 3D tracking with two microphones, we introduce a reflective surface that can be easily found in everyday objects near the smartphone. Thus, the microphones can receive sound from the speaker and echoes from the surface for FMCW-based acoustic ranging. To simultaneously estimate the distances from the direct and reflective paths, we propose the echo-aware FMCW technique with a new signal pattern and target detection process. Our user study shows that ReflecTrack achieves a median error of 28.4 mm in the 60cm × 60cm × 60cm space and 22.1 mm in the 30cm × 30cm × 30cm space for 3D positioning. We demonstrate the easy accessibility of ReflecTrack using everyday surfaces and objects with several typical applications of 3D position tracking, including 3D input for smartphones, fine-grained gesture recognition, and motion tracking in smartphone-based VR systems.
SP  - 1050
EP  - 1062
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474805
ER  - 

TY  - CHAP
AU  - Zhang, Xinyong
TI  - HCI (7) - An Evaluation of Eye-Foot Input for Target Acquisitions
PY  - 2021
AB  - The multimodal interaction technique that combines eye and foot input not only provides a great opportunity for the user with busy hands or hand disabilities to interaction with computer, but can also overcome the drawbacks when using dwell time to solve the “Midas Problem” of gaze input. However, the user’s capability of eye-foot coordination was still unclear. At the same time, the human performance in the basic task of target acquisitions by eye-foot input was also uncertain, and especially a proper performance model was lacking. Motivated by this situation, an eye pointing and foot tapping task experiment had been carried out to fill these gaps. A low-cost eye tracker and a USB foot pedal switcher were used as the input devices from different modalities. The experimental results indicated that the user was soon able to coordinate her/his foot with the eyes for target acquisitions, and that the user could respond fast to tap the foot pedal to finish a task trial in the level of 600 ms. The main performance measures of eye movement time (EMT) and eye pointing time (EPT) under the eye-foot multimodal input condition were significantly increased with the increase of saccadic amplitude A and/or the decrease of target width (size) W, and vice versa. Regression analysis shown that the \(ID_{eye}\) model was more suitable than the standard Fitts’ law to model the human performance in this multimodal interaction context.
SP  - 499
EP  - 517
JF  - Universal Access in Human-Computer Interaction. Design Methods and User Experience
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78092-0_34
ER  - 

TY  - JOUR
AU  - Xu, Wenge; Liang, Hai-Ning; Zhao, Yuxuan; Zhang, Tianyu; Yu, Difeng; Monteiro, Diego
TI  - Errata to “RingText: Dwell-Free and Hands-Free Text Entry for Mobile Head-Mounted Displays Using Head Motions”
PY  - 2019
AB  - In this paper, we present a case for text entry using a circular keyboard layout for mobile head-mounted displays (HMDs) that is dwell-free and does not require users to hold a dedicated input device for letter selection. To support the case, we have implemented RingText whose design is based on a circular layout with two concentric circles. The outer circle is subdivided into regions containing letters. Selection is made by using a virtual cursor controlled by the user's head movements—entering a letter region triggers a selection and moving back into the inner circle resets the selection. The design of RingText follows an iterative process, where we initially conduct one first study to investigate the optimal number of letters per region, inner circle size, and alphabet starting location. We then optimize its design by selecting the most suitable features from the first study: one letter per region, narrowing the trigger area to lower error rates, and creating candidate regions that incorporate two suggested words to appear next to the current letter region (close to the cursor) using a dynamic (rather than fixed) approach. Our second study compares text entry performance of RingText with four other hands-free techniques and the results show that RingText outperforms them. Finally, we run a third study lasting four consecutive days with 10 participants (5 novice users and 5 expert users) doing two daily sessions and the results show that RingText is quite efficient and yields a low error rate. At the end of the eighth session, the novice users can achieve a text entry speed of 11.30 WPM after 60 minutes of training while the expert (more experienced) users can reach an average text entry speed of 13.24 WPM after 90 minutes of training.
SP  - 1991
EP  - 2001
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2019.2898736
ER  - 

TY  - JOUR
AU  - Ryu, Kunhee; Lee, Joong-Jae; Park, Jung-Min
TI  - GG Interaction: a gaze–grasp pose interaction for 3D virtual object selection
PY  - 2019
AB  - During the last two decades, development of 3D object selection techniques has been widely studied because it is critical for providing an interactive virtual environment to users. Previous techniques encounter difficulties with selecting small or distant objects, as well as naturalness and physical fatigue. Although eye-hand based interaction techniques have been promoted as the ideal solution to these problems, research on eye-hand based spatial interaction techniques in 3D virtual spaces has progressed very slowly. We propose a natural and efficient spatial interaction technique for object selection, which is motivated by understanding the human grasp. The proposed technique, gaze–grasp pose interaction (GG Interaction), has many advantages, such as quick and easy selection of small or distant objects, less physical fatigue, and elimination of eye-hand visibility mismatch. Additionally, even if an object is partially overlapped by other objects, GG Interaction enables a user to select the target object easily. We compare GG Interaction with a standard ray-casting technique through a formal user study (participants $$=$$ 20) across two scenarios. The results of the study confirm that GG Interaction provides natural, quick and easy selection for users.
SP  - 383
EP  - 393
JF  - Journal on Multimodal User Interfaces
VL  - 13
IS  - 4
PB  - 
DO  - 10.1007/s12193-019-00305-y
ER  - 

TY  - NA
AU  - Kubo, Yuki; Takada, Ryosuke; Shizuki, Buntarou; Takahashi, Shin
TI  - CHI Extended Abstracts - SynCro: Context-Aware User Interface System for Smartphone-Smartwatch Cross-Device Interaction
PY  - 2017
AB  - We present a context-aware user interface system, called SynCro, comprising a smartphone and a smartwatch. SynCro provides the user with context-dependent user interfaces (UIs), and will synthesize layouts, feedback, or input methods of these devices during use, depending on a identified usage context. To develop SynCro, we implemented a context recognizer that uses the smartphone and smartwatch accelerometers. The recognizer can identify 24 contexts relating to the smartphone grip, user arm posture, and user activity. Further, we implemented example applications that change the UI depending on the context identified.
SP  - 1794
EP  - 1801
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053088
ER  - 

TY  - JOUR
AU  - Stoppel, Sergej; Bruckner, Stefan
TI  - Vol 2 velle: Printable Interactive Volume Visualization
PY  - 2017
AB  - Interaction is an indispensable aspect of data visualization. The presentation of volumetric data, in particular, often significantly benefits from interactive manipulation of parameters such as transfer functions, rendering styles, or clipping planes. However, when we want to create hardcopies of such visualizations, this essential aspect is lost. In this paper, we present a novel approach for creating hardcopies of volume visualizations which preserves a certain degree of interactivity. We present a method for automatically generating Volvelles, printable tangible wheel charts that can be manipulated to explore different parameter settings. Our interactive system allows the flexible mapping of arbitrary visualization parameters and supports advanced features such as linked views. The resulting designs can be easily reproduced using a standard printer and assembled within a few minutes.
SP  - 861
EP  - 870
JF  - IEEE transactions on visualization and computer graphics
VL  - 23
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2016.2599211
ER  - 

TY  - JOUR
AU  - Aranovskiy, Stanislav; Ushirobira, Rosane; Efimov, Denis; Casiez, Géry
TI  - An adaptive FIR filter for trajectory prediction and latency reduction in direct Human–Computer interactions
PY  - 2019
AB  - The problem of latency reduction in direct Human-Computer interactions is considered. The proposed method is based on a frequency-domain approximation of a non-causal ideal predictor with a finite impulse response filter. Given a sufficiently rich dataset, the parameters of the filter can be either optimized off-line or tuned on-line with the proposed adaptive algorithm. The performance of the proposed solution is evaluated in an experimental study consisting of drawings on a touchscreen.
SP  - 104093
EP  - NA
JF  - Control Engineering Practice
VL  - 91
IS  - NA
PB  - 
DO  - 10.1016/j.conengprac.2019.07.011
ER  - 

TY  - NA
AU  - Ogitani, Taihei; Arahori, Yoshitaka; Shinyama, Yusuke; Gondow, Katsuhiko
TI  - AINA - Space Saving Text Input Method for Head Mounted Display with Virtual 12-key Keyboard
PY  - 2018
AB  - Head-Mounted Displays, or HMDs, are rapidly spreading in recent years. However, existing text input methods for HMDs have several problems, hampering their further popularization. In this paper, we focus on the two problems with existing text input methods: (1)the difficulty of its setup and (2) the space needed for its operation. To address these problems, we propose a novel text input system for HMDs. Our system (1) builds on top of a commodity camera device, Leap Motion, and (2) enables effective input in a small physical/virtual space, with a virtual 12-key keyboard. Our experimental results show the advantage of our approach over existing hand-tracking text input systems; especially our method indicates the effectiveness for Japanese text input, with a small virtual keyboard. Based on the experimental results, we believe that our text-input system is effective for any language in HMD environments, if we have a 12-key keyboard tuned for each language (as well as Japanese).
SP  - 342
EP  - 349
JF  - 2018 IEEE 32nd International Conference on Advanced Information Networking and Applications (AINA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aina.2018.00059
ER  - 

TY  - CHAP
AU  - Shao, Yitian
TI  - Spatiotemporal Haptic Effects via Control of Cutaneous Wave Propagation
PY  - 2022
AB  - AbstractA perspective informing the research in this book is that new haptic technologies should be designed to account for the mechanisms of human touch sensing, including biomechanical processes. The results from the preceding chapters provide a view of tactile sensing as being mediated via the transmission of viscoelastic waves in the skin. To illustrate how these findings can inform haptic technology engineering, this chapter presents a new method for rendering evocative haptic effects by exploiting a dominant property of this wave process: frequency-dependent damping. It uses full-field optical vibrometry to show that vibrations introduced at the fingertip elicit waves in the finger that propagate proximally toward the hand, with travel distances decreasing rapidly with frequency. Based on the results, this chapter presents a new design of haptic effects producing wave fields that expand or contract in size and can be delivered via a single actuator. In a perception experiment, subjects accurately (median >95%) identified these stimuli as expanding or contracting without prior exposure or training. These findings demonstrate how the physics of waves in the skin can be exploited for the design of spatiotemporal tactile effects that are practical and effective.
SP  - 105
EP  - 118
JF  - Springer Series on Touch and Haptic Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90839-3_6
ER  - 

TY  - NA
AU  - Grubert, Jens; Kranz, Matthias
TI  - CHI - HeadPhones: Ad Hoc Mobile Multi-Display Environments through Head Tracking
PY  - 2017
AB  - We present HeadPhones (Headtracking + smartPhones), a novel approach for the spatial registration of multiple mobile devices into an ad hoc multi-display environment. We propose to employ the user's head as external reference frame for the registration of multiple mobile devices into a common coordinate system. Our approach allows for dynamic repositioning of devices during runtime without the need for external infrastructure such as separate cameras or fiducials. Specifically, our only requirements are local network connections and mobile devices with built-in front facing cameras. This way, HeadPhones enables spatially-aware multi-display applications in mobile contexts. A user study and accuracy evaluation indicate the feasibility of our approach.
SP  - 3966
EP  - 3971
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025533
ER  - 

TY  - NA
AU  - Cruz, Ramon; Coffey, Mela C.; Sawaya, Ambert Yang; Khurshid, Rebecca P.
TI  - WHC - Modular Haptic Feedback for Rapid Prototyping of Tactile Displays
PY  - 2021
AB  - This work presents the foundations of a novel haptic toolkit, consisting of a set of modules that seeks to enable designers to easily and quickly produce new tactile prototypes. The modules are re-attachable, wearable, wireless, customizable, and can be placed on different parts of the body. This paper first discusses a series of design decisions that were made when producing these modules. The paper then presents a set of five modules that were created using the decided upon design techniques. The five haptic modules presented produce the three most common tactile feedback modalities: vibrotactile, skin-stretch, and probing. Each module haptic cue parameters can be customized and controlled wirelessly through an off- board computer. The modules can be used either in isolation or in groups for haptic sketching to rapidly iterate through tactile displays.
SP  - 703
EP  - 708
JF  - 2021 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc49131.2021.9517256
ER  - 

TY  - JOUR
AU  - Lhachemi, Hugo; Malik, Ammar; Shorten, Robert
TI  - Augmented Reality, Cyber-Physical Systems, and Feedback Control for Additive Manufacturing: A Review
PY  - 2019
AB  - Our objective in this paper is to review the application of feedback ideas in the area of additive manufacturing. Both the application of feedback control to the 3D printing process and the application of feedback theory to enable users to interact better with machines are reviewed. Where appropriate, opportunities for future work are highlighted.
SP  - 50119
EP  - 50135
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2907287
ER  - 

TY  - NA
AU  - Zhou, Qian; Sykes, Sarah; Fels, Sidney; Kin, Kenrick
TI  - CHI - Gripmarks: Using Hand Grips to Transform In-Hand Objects into Mixed Reality Input
PY  - 2020
AB  - We introduce Gripmarks, a system that enables users to opportunistically use objects they are already holding as input surfaces for mixed reality head-mounted displays (HMD). Leveraging handheld objects reduces the need for users to free up their hands or acquire a controller to interact with their HMD. Gripmarks associate a particular hand grip with the shape primitive of the physical object without the need of object recognition or instrumenting the object. From the grip pose and shape primitive we can infer the surface of the object. With an activation gesture, we can enable the object for use as input to the HMD. With five gripmarks we demonstrate a recognition rate of 94.2%; we show that our grip detection benefits from the physical constraints of holding an object. We explore two categories of input objects 1) tangible surfaces and 2) tangible tools and present two representative applications. We discuss the design and technical challenges for expanding the concept.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376313
ER  - 

TY  - NA
AU  - Huang, Da-Yuan; Chan, Liwei; Jian, Xiao-Feng; Chang, Chiun-Yao; Chen, Mu-Hsuan; Yang, De-Nian; Hung, Yi-Ping; Chen, Bing-Yu
TI  - SIGGRAPH ASIA Emerging Technologies - Vibroplay: authoring three-dimensional spatial-temporal tactile effects with direct manipulation
PY  - 2016
AB  - Vibrotactile feedback provides immersive media experience. To facilitate the design of vibrotactile effects, an authoring system is essential. Soundtrack streamlined interfaces [Ryu and Choi 2008; Lee et al. 2009] were initially initially introduced to create temporal vibrotactile effects for individual actuators. Unfortunately, temporal effects designing with sound waves in temporal domain cannot well transfer to tactile sensations for designers without sufficient sound or haptic skills. More recent research [Schneider et al. 2015] designed spatial vibrotactile effects that manipulating control points that specify spatial tactile paths. However, spatial effects designing with control points might be difficult to capture complex spatial behaviors.
SP  - 3
EP  - NA
JF  - SIGGRAPH ASIA 2016 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2988240.2988250
ER  - 

TY  - NA
AU  - Zhu, Kening; Dancu, Alexandru; Zhao, Shengdong
TI  - Conference on Designing Interactive Systems - FusePrint: A DIY 2.5D Printing Technique Embracing Everyday Artifacts
PY  - 2016
AB  - FusePrint is a Stereolithography-based 2.5D rapid prototyping technique that allows high-precision fabrication without high-end modeling tools, enabling the mixing of everyday physical artifacts and liquid conductive gels with photo-reactive resin during the printing process, facilitating the creation of 2.5D objects that perfectly fit the existing objects. Based on our polynomial model on 2.5D resin printing, we developed the design interface of FusePrint, which allows users to design the printed shapes using physical objects as references, generates projection patterns, and notifies users when to place the objects in the resin during the printing process. Our workshops suggested that FusePrint is easy to learn and use, provides a greater level of interactivity, and could be useful for a wide range of applications domains including: mechanical fabrication, wearable accessory, toys, interactive systems, etc.
SP  - 146
EP  - 157
JF  - Proceedings of the 2016 ACM Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2901790.2901792
ER  - 

TY  - NA
AU  - Ion, Alexandra; Lindlbauer, David; Herholz, Philipp; Alexa, Marc; Baudisch, Patrick
TI  - CHI - Understanding Metamaterial Mechanisms
PY  - 2019
AB  - In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.
SP  - 647
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300877
ER  - 

TY  - BOOK
AU  - Bonada, Santiago; Veras, Rafael; Collins, Christopher
TI  - ISS Companion - Personalized Views for Immersive Analytics
PY  - 2016
AB  - In this paper we present work-in-progress toward a vision of personalized views of visual analytics interfaces in the context of collaborative analytics in immersive spaces. In particular, we are interested in the sense of immersion, responsiveness, and personalization afforded by gaze-based input. Through combining large screen visual analytics tools with eye-tracking, a collaborative visual analytics system can become egocentric while not disrupting the collaborative nature of the experience. We present a prototype system and several ideas for real-time personalization of views in visual analytics.
SP  - 83
EP  - 89
JF  - Proceedings of the 2016 ACM Companion on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3009939.3009953
ER  - 

TY  - BOOK
AU  - Wang, Feiyang; Bradley, Adam James; Collins, Christopher
TI  - ETRA Short Papers - Eye Tracking for Target Acquisition in Sparse Visualizations
PY  - 2020
AB  - In this paper, we present a novel marker-free method for identifying screens of interest when using head-mounted eye tracking for visualization in cluttered and multi-screen environments. We offer a solution to discerning visualization entities from sparse backgrounds by incorporating edge-detection into the existing pipeline. Our system allows for both more efficient screen identification and improved accuracy over the state-of-the-art ORB algorithm.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379156.3391834
ER  - 

TY  - NA
AU  - Serim, Baris; Jacucci, Giulio
TI  - CHI - Pointing while Looking Elsewhere: Designing for Varying Degrees of Visual Guidance during Manual Input
PY  - 2016
AB  - We propose using eye tracking to support interface use with decreased reliance on visual guidance. While the design of most graphical user interfaces take visual guidance during manual input for granted, eye tracking allows distinguishing between the cases when the manual input is conducted with or without guidance. We conceptualize the latter cases as input with uncertainty that require separate handling. We describe the design space of input handling by utilizing input resources available to the system, possible actions the system can realize and various feedback techniques for informing the user. We demonstrate the particular action mechanisms and feedback techniques through three applications we developed for touch interaction on a large screen. We conducted a two stage study of positional accuracy during target acquisition with varying visual guidance, to determine the selection range around a touch point due to positional uncertainty. We also conducted a qualitative evaluation of example applications with participants to identify perceived utility and hand eye coordination challenges while using interfaces with decreased visual guidance.
SP  - 5789
EP  - 5800
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858480
ER  - 

TY  - JOUR
AU  - Chen, Huijie; Li, Fan; Wang, Yu
TI  - SoundMark: Accurate Indoor Localization via Peer-Assisted Dead Reckoning
PY  - 2018
AB  - Pedestrian dead reckoning enables pervasive indoor localization without a site survey on fingerprints or an intensive deployment of infrastructures. But accumulated errors in dead reckoning limit the spread of pervasive indoor location-based services. Existing landmark-based approaches mostly rely on resetting the user’s position with the landmark position only when the user is detected while on arrival at a landmark. However, such methods are still restricted by the specific movement patterns and sparse landmark distributions so that the opportunity for position calibration is limited. In this paper, an accurate peer-assisted localization system (called SoundMark ) on a smartphone with no prior infrastructure or fingerprinting is proposed. It calibrates mobile user’s dead reckoning position by leveraging the location constraints between another stationary user who arrives at a landmark. To detect whether a user arrives at a landmark, motion pattern is extracted by fusing the multiple sensors. Then, user activity in the landmark is decomposed to determine whether the user is stationary for performing audio ranging. Besides, SoundMark also applies a mobility-induced time-difference-of-arrival-based audio ranging to extract the location constraints between the peers for localization. SoundMark is implemented on the Android platform for evaluations. The results show that the accuracy of proposed peer-assisted localization is within 2.1 m at the percentage of 80%.
SP  - 4803
EP  - 4815
JF  - IEEE Internet of Things Journal
VL  - 5
IS  - 6
PB  - 
DO  - 10.1109/jiot.2018.2821364
ER  - 

TY  - NA
AU  - Zhang, Mingrui Ray; Wobbrock, Jacob O.
TI  - UIST - Beyond the Input Stream: Making Text Entry Evaluations More Flexible with Transcription Sequences
PY  - 2019
AB  - Method-independent text entry evaluation tools are often used to conduct text entry experiments and compute performance metrics, like words per minute and error rates. The input stream paradigm of Soukoreff & MacKenzie (2001, 2003) still remains prevalent, which presents a string for transcription and uses a strictly serial character representation for encoding the text entry process. Although an advance over prior paradigms, the input stream paradigm is unable to support many modern text entry features. To address these limitations, we present transcription sequences: for each new input, a snapshot of the entire transcribed string unto that point is captured. By comparing adjacent strings within a transcription sequence, we can compute all prior metrics, reduce artificial constraints on text entry evaluations, and introduce new metrics. We conducted a study with 18 participants who typed 1620 phrases using a laptop keyboard, on-screen keyboard, and smartphone keyboard using features such as auto-correction, word prediction, and copy/paste. We also evaluated non-keyboard methods Dasher, gesture typing, and T9. Our results show that modern text entry methods and features can be accommodated, prior metrics can be correctly computed, and new metrics can reveal insights. We validated our algorithms using ground truth based on cursor positioning, confirming 100% accuracy. We also provide a new tool, TextTest++, to facilitate web-based evaluations.
SP  - 831
EP  - 842
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347922
ER  - 

TY  - NA
AU  - Gulay, Emrecan; Kotnik, Toni; Lucero, Andrés
TI  - CHI - Exploring a Feedback-Oriented Design Process Through Curved Folding
PY  - 2021
AB  - The advancement of computational design and fabrication technologies has allowed combining physical and digital processes in architecture. Existing methods for physical-digital integration offer limited support for explorations with folded non-linear surfaces. This paper introduces a feedback-oriented design approach linking physical models with digital tools to enhance ideation processes in architecture. We employ paper as a medium for translating simple mock-up ideas to more elaborate digital design models. We explain the physical exploration, 3D scanning, digital simulation, and fabrication processes. Then, we discuss the results, observations, and limitations of this design approach.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445639
ER  - 

TY  - NA
AU  - Liang, Hui; Wang, Jin; Sun, Qian; Liu, Yong-Jin; Yuan, Junsong; Luo, Jun; He, Ying
TI  - I3D - Barehanded music: real-time hand interaction for virtual piano
PY  - 2016
AB  - This paper presents an efficient data-driven approach to track fingertip and detect finger tapping for virtual piano using an RGB-D camera. We collect 7200 depth images covering the most common finger articulation for playing piano, and train a random regression forest using depth context features of randomly sampled pixels in training images. In the online tracking stage, we firstly segment the hand from the plane in contact by fusing the information from both color and depth images. Then we use the trained random forest to estimate the 3D position of fingertips and wrist in each frame, and predict finger tapping based on the estimated fingertip motion. Finally, we build a kinematic chain and recover the articulation parameters for each finger. In contrast to the existing hand tracking algorithms that often require hands are in the air and cannot interact with physical objects, our method is designed for hand interaction with planar objects, which is desired for the virtual piano application. Using our prototype system, users can put their hands on a desk, move them sideways and then tap fingers on the desk, like playing a real piano. Preliminary results show that our method can recognize most of the beginner's piano-playing gestures in realtime for soothing rhythms.
SP  - 87
EP  - 94
JF  - Proceedings of the 20th ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2856400.2856411
ER  - 

TY  - NA
AU  - Wu, Hao; Feng, Jinghao; Tian, Xuejin; Sun, Edward; Liu, Yunxin; Dong, Bo; Xu, Fengyuan; Zhong, Sheng
TI  - MobiSys - EMO: real-time emotion recognition from single-eye images for resource-constrained eyewear devices
PY  - 2020
AB  - Real-time user emotion recognition is highly desirable for many applications on eyewear devices like smart glasses. However, it is very challenging to enable this capability on such devices due to tightly constrained image contents (only eye-area images available from the on-device eye-tracking camera) and computing resources of the embedded system. In this paper, we propose and develop a novel system called EMO that can recognize, on top of a resource-limited eyewear device, real-time emotions of the user who wears it. Unlike most existing solutions that require whole-face images to recognize emotions, EMO only utilizes the single-eye-area images captured by the eye-tracking camera of the eyewear. To achieve this, we design a customized deep-learning network to effectively extract emotional features from input single-eye images and a personalized feature classifier to accurately identify a user's emotions. EMO also exploits the temporal locality and feature similarity among consecutive video frames of the eye-tracking camera to further reduce the recognition latency and system resource usage. We implement EMO on two hardware platforms and conduct comprehensive experimental evaluations. Our results demonstrate that EMO can continuously recognize seven-type emotions at 12.8 frames per second with a mean accuracy of 72.2%, significantly outperforming the state-of-the-art approach, and consume much fewer system resources.
SP  - 448
EP  - 461
JF  - Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3386901.3388917
ER  - 

TY  - JOUR
AU  - Serim, Baris; Pfeuffer, Ken; Gellersen, Hans; Jacucci, Giulio
TI  - Visual Attention-Based Access: Granting Access Based on Users' Joint Attention on Shared Workspaces
PY  - 2018
AB  - During collaboration, individual users' capacity to maintain awareness, avoid duplicate work and prevent conflicts depends on the extent to which they are able to monitor the workspace. Existing access control models disregard this contextual information by managing access strictly based on who performs the action. As an alternative approach, we propose managing access by taking the visual attention of collaborators into account. For example, actions that require consensus can be limited to collaborators' joint attention, editing another user's personal document can require her visual supervision and private information can become unavailable when another user is looking. We prototyped visual attention-based access for 3 collaboration scenarios on a large vertical display using head orientation input as a proxy for attention. The prototype was deployed for an exploratory user study, where participants in pairs were tasked to assign visual attention-based access to various actions. The results reveal distinct motivations for their use such as preventing accidents, maintaining individual control and facilitating group awareness. Visual attention-based access has been perceived as more convenient but also less certain when compared to traditional access control. We conclude that visual attention-based access can be a useful addition to groupware to flexibly facilitate awareness and prevent conflicts.
SP  - 133
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 3
PB  - 
DO  - 10.1145/3264943
ER  - 

TY  - NA
AU  - Ko, Donghyeon; Bin Yim, Jee; Lee, Yujin; Pyun, Jaehoon; Lee, Woohun
TI  - CHI - Designing Metamaterial Cells to Enrich Thermoforming 3D Printed Object for Post-Print Modification
PY  - 2021
AB  - In this paper, we present a metamaterial structure called thermoformable cells, TF-Cells, to enrich thermoforming for post-print modification. So far, thermoforming is limitedly applied for modifying a 3D printed object due to its low thermal conductivity. TF-Cells consists of beam arrays that affluently pass hot air and have high heat transference. Through heating the embedded TF-Cells of the printed object, users can modify not only the deeper area of the object surface but also its form factor. With a series of technical experiments, we investigated TF-Cells’ thermoformability, depending on their structure’s parameters, orientations, and heating conditions. Next, we present a series of compound cells consisting of TF-Cells and solid structure to adjust stiffness or reduce undesirable shape deformation. Adapting the results from the experiments, we built a simple tool for embedding TF-Cells into a 3D model. Using the tool, we implemented examples under contexts of mechanical fitting, ergonomic fitting, and aesthetic tuning.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445229
ER  - 

TY  - NA
AU  - Voelker, Simon; Hueber, Sebastian; Holz, Christian; Remy, Christian; Marquardt, Nicolai
TI  - CHI - GazeConduits: Calibration-Free Cross-Device Collaboration through Gaze and Touch
PY  - 2020
AB  - We present GazeConduits, a calibration-free ad-hoc mobile interaction concept that enables users to collaboratively interact with tablets, other users, and content in a cross-device setting using gaze and touch input. GazeConduits leverages recently introduced smartphone capabilities to detect facial features and estimate users' gaze directions. To join a collaborative setting, users place one or more tablets onto a shared table and position their phone in the center, which then tracks users present as well as their gaze direction to determine the tablets they look at. We present a series of techniques using GazeConduits for collaborative interaction across mobile devices for content selection and manipulation. Our evaluation with 20 simultaneous tablets on a table shows that GazeConduits can reliably identify which tablet or collaborator a user is looking at.
SP  - 451
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376578
ER  - 

TY  - JOUR
AU  - Wang, Zhengjie; Hou, Yushan; Jiang, Kangkang; Dou, Wenwen; Zhang, Chengming; Huang, Zehua; Guo, Yinjing
TI  - Hand Gesture Recognition Based on Active Ultrasonic Sensing of Smartphone: A Survey
PY  - 2019
AB  - With the rapid development of Internet of Things, hand gesture recognition has drawn wide attention in the field of ubiquitous computing because it provides us with simple and natural human-computer interaction mode. Among these various implementations, hand gesture recognition using ultrasonic signals of smartphone has become a hot research topic due to its various advantages. In this paper, we consider the smartphone as an active sonar sensing system to identify hand movements. Specifically, the speakers emit ultrasonic signal and the microphone on the same phone receives the changed echo affected by hand movements. This paper investigates the state-of-the-art hand gesture applications and presents a comprehensive survey on the characteristics of studies using the active sonar sensing system. Firstly, we review the existing research of hand gesture recognition based on acoustic signals. After that, we introduce the characteristics of ultrasonic signal and describe the fundamental principle of hand gesture recognition. Then, we focus on the typical methods used in these studies and present a detailed analysis on signal generation, feature extraction, preprocessing, and recognition methods. Next, we investigate the state-of-the-art ultrasonic-based applications of hand gesture recognition using smartphone and analyze them in detail from dynamic gesture recognition and hand tracking. Afterwards, we make a discussion about these systems from signal acquisition, signal processing, and performance evaluation to obtain some insight into development of the ultrasonic hand gesture recognition system. Finally, we conclude by discussing the challenges, insight, and open issues involved in hand gesture recognition based on ultrasonic signal of the smartphone.
SP  - 111897
EP  - 111922
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2933987
ER  - 

TY  - CHAP
AU  - D'Eusanio, Andrea; Pini, Stefano; Borghi, Guido; Simoni, Alessandro; Vezzani, Roberto
TI  - Unsupervised Detection of Dynamic Hand Gestures from Leap Motion Data
PY  - 2022
AB  - AbstractThe effective and reliable detection and classification of dynamic hand gestures is a key element for building Natural User Interfaces, systems that allow the users to interact using free movements of their body instead of traditional mechanical tools. However, methods that temporally segment and classify dynamic gestures usually rely on a great amount of labeled data, including annotations regarding the class and the temporal segmentation of each gesture. In this paper, we propose an unsupervised approach to train a Transformer-based architecture that learns to detect dynamic hand gestures in a continuous temporal sequence. The input data is represented by the 3D position of the hand joints, along with their speed and acceleration, collected through a Leap Motion device. Experimental results show a promising accuracy on both the detection and the classification task and that only limited computational power is required, confirming that the proposed method can be applied in real-world applications.KeywordsDynamic hand gestures recognitionUnsupervised gesture detectionTransformer-based architectureLeap motion
SP  - 414
EP  - 424
JF  - Image Analysis and Processing – ICIAP 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06427-2_35
ER  - 

TY  - JOUR
AU  - Vatavu, Radu-Daniel
TI  - Visual Impairments and Mobile Touchscreen Interaction: State-of-the-Art, Causes of Visual Impairment, and Design Guidelines
PY  - 2017
AB  - ABSTRACTThis article identifies, catalogues, and discusses factors that are responsible for causing visual impairment of either a pathological or situational nature for touch and gesture input on smart mobile devices. Because the vast majority of interactions with touchscreen devices are highly visual in nature, any factor that prevents a clear, direct view of the mobile device’s screen can have potential negative implications on the effectiveness and efficiency of the interaction. This work presents the first overview of such factors, which are grouped in a catalogue of users, devices, and environments. The elements of the catalogue (e.g., psychological factors that relate to the user, or the social acceptability of mobile device use in public that relates to the social environment) are discussed in the context of current eye pathology classification from medicine and the recent literature in human–computer interaction on mobile touch and gesture input for people with visual impairments, for which a stat...
SP  - 486
EP  - 509
JF  - International Journal of Human–Computer Interaction
VL  - 33
IS  - 6
PB  - 
DO  - 10.1080/10447318.2017.1279827
ER  - 

TY  - NA
AU  - Clark, Ben; Schneider, Oliver; MacLean, Karon E.; Tan, Hong Z.
TI  - WHC - Predictable and distinguishable morphing of vibrotactile rhythm
PY  - 2017
AB  - Vibrotactile (VT) icons are a ubiquitous, increasingly expressive and expected communicative element of many user interfaces, routinely deployed with technology from rumble motors to novel expressive tactile actuators. However, it is still difficult to design, customize, and experiment with VT feedback. Here we consider manipulation of one of the most salient VT design elements, rhythm, through perceptual morphing. The ability to create perceptual morphs between pairs of VT signal parents expands editorial scope and precision for designers, end-users, and hapticians. To assess perceptual morphs, we propose criteria of predictability — a morph has similarities to one or both parents; and distinguishability — a morph is different from its parents. We developed a new algorithm for perceptual rhythm morphing based on dynamic-time warping (DTW), implemented in an open-source online tool, MacaronMix. Two studies revealed limits and conditions under which DTW-produced VT rhythm morphs are predictable and distinguishable.
SP  - 84
EP  - 89
JF  - 2017 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2017.7989881
ER  - 

TY  - NA
AU  - Ablart, Damien; Velasco, Carlos; Obrist, Marianna
TI  - TVX - Integrating Mid-Air Haptics into Movie Experiences
PY  - 2017
AB  - "Seeing is believing, but feeling is the truth". This idiom from the seventieth century English clergyman Thomas Fuller gains new momentum in light of an increased proliferation of haptic technologies that allow people to have various kinds of `touch' and `touchless' interactions. Here, we report on the process of creating and integrating touchless feedback (i.e. mid-air haptic stimuli) into short movie experiences (i.e. one-minute movie format). Based on a systematic evaluation of user's experiences of those haptically enhanced movies, we show evidence for the positive effect of haptic feedback during the first viewing experience, but also for a repeated viewing after two weeks. This opens up a promising design space for content creators and researchers interested in sensory augmentation of audiovisual content. We discuss our findings and the use of mid-air haptics technologies with respect to its effect on users' emotions, changes in the viewing experience over time, and the effects of synchronisation.
SP  - 77
EP  - 84
JF  - Proceedings of the 2017 ACM International Conference on Interactive Experiences for TV and Online Video
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3077548.3077551
ER  - 

TY  - NA
AU  - Zhu, Kening; Dancu, Alexandru; Zhao, Shengdong
TI  - CHI Extended Abstracts - FusePrint: A DIY 2.5D Printing Technique for Good-fit Fabrication with Daily Objects
PY  - 2017
AB  - FusePrint is a Stereolithography-based 2.5D rapid prototyping technique that allows high-precision fabrication without high-end modeling tools, enabling the mixing of everyday physical artifacts and liquid conductive gels with photo-reactive resin during the printing process, facilitating the creation of 2.5D objects that perfectly fit the existing objects. Based on our polynomial model on 2.5D resin printing, we developed the design interface of FusePrint, which allows users to design the printed shapes using physical objects as references, generates projection patterns, and notifies users when to place the objects in the resin during the printing process. FusePrint could be useful for a wide range of application domains including: mechanical fabrication, wearable accessory, toys, interactive systems, etc.
SP  - 413
EP  - 416
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3050430
ER  - 

TY  - NA
AU  - Simeone, Adalberto L.; Bulling, Andreas; Alexander, Jason; Gellersen, Hans
TI  - AVI - Three-Point Interaction: Combining Bi-manual Direct Touch with Gaze
PY  - 2016
AB  - The benefits of two-point interaction for tasks that require users to simultaneously manipulate multiple entities or dimensions are widely known. Two-point interaction has become common, e.g., when zooming or pinching using two fingers on a smartphone. We propose a novel interaction technique that implements three-point interaction by augmenting two-finger direct touch with gaze as a third input channel. We evaluate two key characteristics of our technique in two multi-participant user studies. In the first, we used the technique for object selection. In the second, we evaluate it in a 3D matching task that requires simultaneous continuous input from fingers and the eyes. Our results show that in both cases participants learned to interact with three input channels without cognitive or mental overload. Participants' performance tended towards fast selection times in the first study and exhibited parallel interaction in the second. These results are promising and show that there is scope for additional input channels beyond two-point interaction.
SP  - 168
EP  - 175
JF  - Proceedings of the International Working Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909132.2909251
ER  - 

TY  - NA
AU  - Peek, Nadya; Moyer, Ilan
TI  - Tangible and Embedded Interaction - Popfab: A Case for Portable Digital Fabrication
PY  - 2017
AB  - We present a case study of Popfab, a portable multi-purpose digital fabrication tool. It is uses interchangeable heads (3D printer, CNC mill, and CNC knife) on a general-purpose motion platform that folds into a briefcase. Popfab contributed to the discussion of the future of digital fabrication tools by demonstrating the feasibility of both portability and both additive and subtractive manufacturing on a single platform. Portability is not yet a widely considered option for digital fabrication tools, but with Popfab we demonstrate that general site-specific personal fabrication is possible.
SP  - 325
EP  - 329
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3025009
ER  - 

TY  - JOUR
AU  - Liu, Zongjian; He, Jieling; Feng, Jianjiang; Zhou, Jie
TI  - PrinType
PY  - 2022
AB  - <jats:p>We present PrinType, a fingerprint recognition based typing technique for virtual reality. Different regions of fingers covered by friction ridges are assigned to different keys (i.e. letters, numbers, punctuation, or functions). Once the thumb-worn fingerprint sensor touches a finger, the contact region (and its key) is recognized by matching the current image with registered templates. Using only a small sensor, PrinType turns each segment of all fingers into a touchable key. We designed keyboard layouts corresponding to three interaction sub-spaces: whole finger keyboard, fingertip keyboard, and single-handed keyboard. A 12-person user study was conducted to evaluate the performance of different strategies. Our user evaluation showed that participants achieved an average of 29.56, 32.38, and 34.22 WPM with 0.79%, 0.20%, and 0.21% not corrected error rate in the three strategies. In addition, we provided a detailed analysis of various micro metrics to further understand user performance and technical characteristics. Overall, PrinType is favored by users for its usability, efficiency, and novelty.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569491
ER  - 

TY  - CHAP
AU  - Park, Jaeyoung; Kim, Jaeha; Oh, Yonghwan; Tan, Hong Z.
TI  - EuroHaptics (1) - Rendering Moving Tactile Stroke on the Palm Using a Sparse 2D Array
PY  - 2016
AB  - The present study presents a new rendering algorithm for a moving tactile stroke on the palm of the hand placed on a sparse 2D tactor array. Our algorithm utilizes the relation between signal duration and signal onset asynchrony previously proposed for "tactile brush" [1], but extends it by applying 3-actuator phantom sensations and adjusting the sampling rate. We compare our proposed algorithm to the tactile brush algorithm for their similarity in target trajectories and uniformity of tactile stroke motions. The results show that the participants judge the tactile strokes with our algorithm to move significantly closer to target motions and with more uniform velocity than the "tactile brush." The effect of our algorithm is more significant for experimental stimuli with longer travel time and length.
SP  - 47
EP  - 56
JF  - Haptics: Perception, Devices, Control, and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-42321-0_5
ER  - 

TY  - NA
AU  - Jiang, Haiyan; Weng, Dongdong
TI  - VR - HiPad: Text entry for Head-Mounted Displays Using Circular Touchpad
PY  - 2020
AB  - Text entry in virtual reality (VR) is currently a common activity and a challenging problem. In this paper, we introduce HiPad, leveraging a circular touchpad with a circular virtual keyboard, to support the one-hand text entry in mobile head-mounted displays (HMDs). The design of HiPad’s layout is based on a circle and a square with rounded corners, where the outer circle is subdivided into six keys’ regions containing letters. This technique input text by a common hand-held controller with a circular touchpad for HMDs and disambiguates the word based on the sequence of keys pressed by the user. In our first study, three potential layouts are considered and evaluated, leading to the design containing six keys. By analyzing the touch behavior of users, we optimize the 6-keys layout and conduct the second study, showing that the optimized layout has better performance. Then the third study is conducted to evaluate the performance of 6-keys HiPad with VE-layout and TP-layout and to study the learning curves. The results show that novices can achieve 13.57 Words per Minute (WPM) with VE-layout and 11.60 WPM with TP-layout and the speeds increase by 74.42% for VE-layout users and by 81.53% for TP-layout users through a short 60-phrase training.
SP  - 692
EP  - 703
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581236395562
ER  - 

TY  - JOUR
AU  - Al-Sada, Mohammed; Jiang, Keren; Ranade, Shubhankar; Kalkattawi, Mohammed; Nakajima, Tatsuo
TI  - HapticSnakes: multi-haptic feedback wearable robots for immersive virtual reality
PY  - 2019
AB  - Haptic feedback plays a large role in enhancing immersion and presence in VR. However, previous research and commercial products have limitations in terms of variety and locations of delivered feedbacks. To address these challenges, we present HapticSnakes, which are snake-like waist-worn robots that can deliver multiple types of feedback in various body locations, including taps-, gestures-, airflow-, brushing- and gripper-based feedbacks. We developed two robots, one is lightweight and suitable for taps and gestures, while the other is capable of multiple types of feedback. We presented a design space based on our implementations and conducted two evaluations. Since taps are versatile, easy to deliver and largely unexplored, our first evaluation focused on distinguishability of tap strengths and locations on the front and back torso. Participants had highest accuracy in distinguishing feedback on the uppermost regions and had superior overall accuracy in distinguishing feedback strengths over locations. Our second user study investigated HapticSnakes’ ability to deliver multiple feedback types within VR experiences, as well as users’ impressions of wearing our robots and receiving novel feedback in VR. The results indicate that participants had distinct preferences for feedbacks and were in favor of using our robots throughout. Based on the results of our evaluations, we extract design considerations and discuss research challenges and opportunities for developing multi-haptic feedback robots.
SP  - 191
EP  - 209
JF  - Virtual Reality
VL  - 24
IS  - 2
PB  - 
DO  - 10.1007/s10055-019-00404-x
ER  - 

TY  - CHAP
AU  - Didehkhorshid, Seyed Amir Ahmad; Philip, Siju; Samimi, Elaheh; Teather, Robert J.
TI  - HCI (46) - Text Input in Virtual Reality Using a Tracked Drawing Tablet
PY  - 2020
AB  - We present an experiment evaluating the effectiveness of a tracked drawing tablet for use in virtual reality (VR) text input. Participants first completed a text input pre-test, entering several phrases using a physical keyboard. Participants then entered text in VR using an HTC Vive, with a tracker mounted on a drawing tablet with a QWERTY soft keyboard overlaid on the virtual tablet. This was similar to text input using stylus-supported mobile devices. Our results indicate that not only did participants prefer the Vive controller, it also offered superior entry speed (16.31 wpm vs. 12.79 wpm with the tablet and stylus) and error rates (4.1% vs. 6.4%). Pre-test scores were also correlated to measured entry speeds, and reveal that user typing speed on physical keyboards provides a modest predictor of VR text input speed (R2 of 0.6 for the Vive controller, 0.45 for the tablet).
SP  - 314
EP  - 329
JF  - HCI International 2020 – Late Breaking Papers: Virtual and Augmented Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-59990-4_24
ER  - 

TY  - NA
AU  - Arif, Ahmed Sabbir; East, Brien; DeLong, Sean; Manshaei, Roozbeh; Gupta, Apurva; Lalwani, Manasvi; Mazalek, Ali
TI  - Tangible and Embedded Interaction - Extending the Design Space of Tangible Objects via Low-Resolution Edge Displays
PY  - 2017
AB  - We developed a custom tangible that uses LED arrays around the edges as a low-resolution display to provide real-time visual feedback on the current state of the system. We developed a guideline for mapping different types of edge feedback to different tangible interactions. We evaluated its effectiveness in an informal user study where users interacted with a tabletop and tangible system with the edge feedback enabled. Results suggest that edge feedback provides a better understanding of the system.
SP  - 481
EP  - 488
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3025078
ER  - 

TY  - NA
AU  - Tian, Rundong; Saran, Vedant; Kritzler, Mareike; Michahelles, Florian; Paulos, Eric
TI  - UIST - Turn-by-Wire: Computationally Mediated Physical Fabrication
PY  - 2019
AB  - Advances in digital fabrication have simultaneously created new capabilities while reinforcing outdated workflows that constrain how, and by whom, these fabrication tools are used. In this paper, we investigate how a new class of hybrid-controlled machines can collaborate with novice and expert users alike to yield a more lucid making experience. We demonstrate these ideas through our system, Turn-by-Wire. By combining the capabilities of a traditional lathe with haptic input controllers that modulate both position and force, we detail a series of novel interaction metaphors that invite a more fluid making process spanning digital, model-centric, computer control, and embodied, adaptive, human control. We evaluate our system through a user study and discuss how these concepts generalize to other fabrication tools.
SP  - 713
EP  - 725
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347918
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Chen, Yu; Yang, Yue; Yu, Zhi; Luo, Danli; Gu, Jianzhe; Yao, Lining; Tao, Ye; Wang, Guanyun
TI  - CHI - FlexTruss: A Computational Threading Method for Multi-material, Multi-form and Multi-use Prototyping
PY  - 2021
AB  - 3D printing, as a rapid prototyping technique, usually fabricates objects that are difficult to modify physically. This paper presents FlexTruss, a design and construction pipeline based on the assembly of modularized truss-shaped objects fabricated with conventional 3D printers and assembled by threading. To create an end-to-end system, a parametric design tool with an optimal Euler path calculation method is developed, which can support both inverse and forward design workflow and multi-material construction of modular parts. In addition, the assembly of truss modules by threading is evaluated with a series of application cases to demonstrate the affordance of FlexTruss. We believe that FlexTruss extends the design space of 3D printing beyond typically hard and fixed forms, and it will provide new capabilities for designers and researchers to explore the use of such flexible truss structures in human-object interaction.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445311
ER  - 

TY  - JOUR
AU  - Deng, Shujie; Chang, Jian; Kirkby, Julie A.; Zhang, Jian J.
TI  - Gaze–mouse coordinated movements and dependency with coordination demands in tracing
PY  - 2016
AB  - Eye movements have been shown to lead hand movements in tracing tasks where subjects have to move their fingers along a predefined trace. The question remained, whether the leading relationship was similar when tracing with a pointing device, such as a mouse; more importantly, whether tasks that required more or less gaze–mouse coordination would introduce variation in this pattern of behaviour, in terms of both spatial and temporal leading of gaze position to mouse movement. A three-level gaze–mouse coordination demand paradigm was developed to address these questions. A substantial dataset of 1350 trials was collected and analysed. The linear correlation of gaze–mouse movements, the statistical distribution of the lead time, as well as the lead distance between gaze and mouse cursor positions were all considered, and we proposed a new method to quantify lead time in gaze–mouse coordination. The results supported and extended previous empirical findings that gaze often led mouse movements. We found that the gaze–mouse coordination demands of the task were positively correlated to the gaze lead, both spatially and temporally. However, the mouse movements were synchronised with or led gaze in the simple straight line condition, which demanded the least gaze–mouse coordination.
SP  - 665
EP  - 679
JF  - Behaviour & Information Technology
VL  - 35
IS  - 8
PB  - 
DO  - 10.1080/0144929x.2016.1181209
ER  - 

TY  - JOUR
AU  - Alharbi, Ohoud; Stuerzlinger, Wolfgang; Putze, Felix
TI  - The Effects of Predictive Features of Mobile Keyboards on Text Entry Speed and Errors
PY  - 2020
AB  - Mobile users rely on typing assistant mechanisms such as prediction and autocorrect. Previous studies on mobile keyboards showed decreased performance for heavy use of word prediction, which identifies a need for more research to better understand the effectiveness of predictive features for different users. Our work aims at such a better understanding of user interaction with autocorrections and the prediction panel while entering text, in particular when these approaches fail. We present a crowd-sourced mobile text entry study with 170 participants. Our mobile web application simulates autocorrection and word prediction to capture user behaviours around these features. We found that using word prediction saves an average of 3.43 characters per phrase but also adds an average of two seconds compared to actually typing the word, resulting in a negative effect on text entry speed. We also identified that the time to fix wrong autocorrections is on average 5.5 seconds but that autocorrection does not have a significant effect on typing speed.
SP  - 1
EP  - 16
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427311
ER  - 

TY  - CONF
AU  - Li, Chengzhao; Gutwin, Carl; Stanley, Kevin G.; Nacenta, Miguel A.
TI  - Graphics Interface - All Across the Circle: Using Auto-Ordering to Improve Object Transfer between Mobile Devices
PY  - 2016
AB  - People frequently form small groups in many social and professional situations: from conference attendees meeting at a coffee break, to siblings gathering at a family barbecue. These ad-hoc gatherings typically form into predictable geometries based on circles or circular arcs (called F-Formations). Because our lives are increasingly stored and represented by data on handheld devices, the desire to be able to share digital objects while in these groupings has increased. Using the relative position in these groups to facilitate file sharing can enable intuitive techniques such as passing or flicking. However, there is no reliable, lightweight, ad-hoc technology for detecting and representing relative locations around a circle. In this paper, we present two systems that can auto-order locations about a circle based on sensors that are standard on commodity smartphones. We tested these systems using an object-passing task in a laboratory environment against unordered and proximity-based systems, and show that our techniques are faster, are more accurate, and are preferred by users.
SP  - 49
EP  - 56
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Tokac, Iremnur; Gursoy, Benay; Bruyninckx, Herman; Vande Moere, Andrew
TI  - Craft-Inspired Digital Fabrication: A Study of Interactive Robotic Clay Carving
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3562003
ER  - 

TY  - JOUR
AU  - Cao, Siyuan; Wang, He
TI  - Enabling Public Cameras to Talk to the Public
PY  - 2018
AB  - This paper asks: Is it possible for cameras in public areas, say ceiling cameras in a museum, to send personalized messages to people without knowing any addresses of their phones? We define this kind of problem as Private Human Addressing and develop a real-time end-to-end system called PHADE to solve it. Unlike traditional data transmission protocols that need to first learn the destination's address, our cameras rely on viewing user's motion patterns, and use the uniqueness of these patterns as the address for communication. Once receiving the wireless broadcast from the cameras, the user's phone can locally compare the "motion address" of the packet against its own motion sensor data, and accept the packet upon a "good" match. In addition to requiring no data from users, our system transforms the motion patterns into low-dimensional codes to prevent leakage of user's walking behaviors. Thus, a hacker who collects all the broadcast messages would still not be able to infer the motion patterns of users. Real-world experiments show that PHADE discriminates 2, 4, 6, 8, 10 people with 98%, 95%, 90%, 90%, 87% correctness and about 3 seconds constant delay. Since abundant and accurate information can be extracted from videos, PHADE would find applications in various contexts. Extended to localization system and audio guide, PHADE achieves a median error of 0.19m and 99.7% matching correctness, respectively. PHADE can also deliver messages based on human gestures. There is no need to deploy any extra infrastructures or to require users to rent any dedicated device.
SP  - 63
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 2
PB  - 
DO  - 10.1145/3214266
ER  - 

TY  - JOUR
AU  - Dandu, Bharat; Shao, Yitian; Visell, Yon
TI  - Rendering Spatiotemporal Haptic Effects Via the Physics of Waves in the Skin
PY  - 2021
AB  - A major challenge in haptic engineering has been to design practical methods to efficiently stimulate distributed areas of skin. Here, we show how to use a single actuator to generate vibrotactile stimuli which cause sensations of temporally varying spatial extent. Through optical vibrometry methods, we show that vibrational stimuli applied at the fingertip elicit waves in the finger that propagate proximally toward the hand and show how the frequency-dependent damping behavior of skin causes propagation distances to decrease rapidly with increasing frequency of stimulation. Utilizing these results, we design haptic stimuli applied through a single actuator that produces wavefields that expand or contract in size. In a perception experiment, participants accurately (median $>$95%) identified these stimuli as expanding or contracting without prior exposure or training. As a potential application, we used these effects as haptic cues for interactions in virtual reality. We show through a second experiment that the spatiotemporal haptic stimuli were rated as significantly more engaging than conventional vibrotactile stimuli. These findings demonstrate how the physics of waves in skin can be utilized to excite spatiotemporal tactile effects over large surface areas with a single actuator, and inform methods to utilize the effects in practical applications.
SP  - 347
EP  - 358
JF  - IEEE transactions on haptics
VL  - 14
IS  - 2
PB  - 
DO  - 10.1109/toh.2020.3029768
ER  - 

TY  - NA
AU  - Egeberg, Mie; Lind, Stine; Nilsson, Niels Christian; Serafin, Stefania
TI  - SAP - Exploring the Effects of Actuator Configuration and Visual Stimuli on Cutaneous Rabbit Illusions in Virtual Reality
PY  - 2021
AB  - This paper details preliminary explorations of how cutaneous rabbit illusions and virtual reality can be combined to produce experiences of cutaneous saltation along paths that do not match the actuator configuration, and to reduce the number of needed actuators. We present two studies exploring the effects of actuator configuration, vibration properties, and varying visual presentations of the virtual object touching the user’s forearm. The results indicate that the illusion can occur along a curvilinear path, and congruent visual movement can enhance the illusion and potentially reduce the number of needed actuators. Incongruent visual movement does not break the illusion but appears to modulate the perceived path of the virtual object.
SP  - 1
EP  - 9
JF  - ACM Symposium on Applied Perception 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474451.3476230
ER  - 

TY  - BOOK
AU  - Maher, Mary Lou; Lee, Lina; Carroll, John M.
TI  - Designing for Gesture and Tangible Interaction
PY  - 2017
AB  - Interactive technology is increasingly integrated with physical objects that do not have a traditional keyboard and mouse style of interaction, and many do not even have a display. These objects require new approaches to interaction design, referred to as post-WIMP (Windows, Icons, Menus, and Pointer) or as embodied interaction design. This book provides an overview of the design opportunities and issues associated with two embodied interaction modalities that allow us to leave the traditional keyboard behind: tangible and gesture interaction. We explore the issues in designing for this new age of interaction by highlighting the significance and contexts for these modalities. We explore the design of tangible interaction with a reconceptualization of the traditional keyboard as a Tangible Keyboard, and the design of interactive three-dimensional (3D) models as Tangible Models. We explore the design of gesture interaction through the design of gesture-base commands for a walk-up-and-use information display, and through the design of a gesture-based dialogue for the willful marionette. We conclude with design principles for tangible and gesture interaction and a call for research on the cognitive effects of these modalities.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Covaci, Alexandra; Zou, Longhao; Tal, Irina; Muntean, Gabriel-Miro; Ghinea, Gheorghita
TI  - Is Multimedia Multisensorial? - A Review of Mulsemedia Systems
PY  - 2018
AB  - Mulsemedia—multiple sensorial media—makes possible the inclusion of layered sensory stimulation and interaction through multiple sensory channels. The recent upsurge in technology and wearables provides mulsemedia researchers a vehicle for potentially boundless choice. However, in order to build systems that integrate various senses, there are still some issues that need to be addressed. This review deals with mulsemedia topics that remain insufficiently explored by previous work, with a focus on the multi-multi (multiple media-multiple senses) perspective, where multiple types of media engage multiple senses. Moreover, it addresses the evolution of previously identified challenges in this area and formulates new exploration directions.
SP  - 91
EP  - 35
JF  - ACM Computing Surveys
VL  - 51
IS  - 5
PB  - 
DO  - 10.1145/3233774
ER  - 

TY  - NA
AU  - Park, Eunji; Lee, Byungjoo
TI  - An Intermittent Click Planning Model
PY  - 2020
AB  - Pointing is the task of tracking a target with a pointer and confirming the target selection through a click action when the pointer is positioned within the target. Little is known about the mechanism by which users plan and execute the click action in the middle of the target tracking process. The Intermittent Click Planning model proposed in this study describes the process by which users plan and execute optimal click actions, from which the model predicts the pointing error rates. In two studies in which users pointed to a stationary target and a moving target, the model proved to accurately predict the pointing error rates (R2 = 0.992 and 0.985, respectively). The model has also successfully identified differences in cognitive characteristics among first-person shooter game players.
SP  - 3376725
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376725
ER  - 

TY  - JOUR
AU  - Willett, Wesley; Jansen, Yvonne; Dragicevic, Pierre
TI  - Embedded Data Representations
PY  - 2017
AB  - We introduce embedded data representations , the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents - the real-world entities and spaces to which data corresponds - and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations , which display data in proximity to data referents, and embedded representations , which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.
SP  - 461
EP  - 470
JF  - IEEE transactions on visualization and computer graphics
VL  - 23
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2016.2598608
ER  - 

TY  - NA
AU  - Horak, Tom; Mathisen, Andreas; Klokmose, Clemens Nylandsted; Dachselt, Raimund; Elmqvist, Niklas
TI  - CHI - Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups
PY  - 2019
AB  - We present Vistribute, a framework for the automatic distribution of visualizations and UI components across multiple heterogeneous devices. Our framework consists of three parts: (i) a design space considering properties and relationships of interactive visualizations, devices, and user preferences in multi-display environments; (ii) specific heuristics incorporating these dimensions for guiding the distribution for a given interface and device ensemble; and (iii) a web-based implementation instantiating these heuristics to automatically generate a distribution as well as providing interaction mechanisms for user-defined adaptations. In contrast to existing UI distribution systems, we are able to infer all required information by analyzing the visualizations and devices without relying on additional input provided by users or programmers. In a qualitative study, we let experts create their own distributions and rate both other manual distributions and our automatic ones. We found that all distributions provided comparable quality, hence validating our framework.
SP  - 616
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300846
ER  - 

TY  - JOUR
AU  - Tawa, Syunsuke; Nagano, Hikaru; Tazaki, Yuichi; Yokokohji, Yasuyoshi
TI  - Extended phantom sensation: vibrotactile-based movement sensation in the area outside the inter-stimulus
PY  - 2020
AB  - Vibrotactile phantom sensation is an intuitive methodology for perceiving the localization of a moving object. However, the presentation area is limited to the inter-stimulus. To extend the range o...
SP  - 268
EP  - 280
JF  - Advanced Robotics
VL  - 35
IS  - 5
PB  - 
DO  - 10.1080/01691864.2020.1854114
ER  - 

TY  - NA
AU  - Zhang, Hao; Yin, Yafeng; Xie, Lei; Lu, Sanglu
TI  - UbiComp/ISWC Adjunct - AirTyping: a mid-air typing scheme based on leap motion
PY  - 2020
AB  - In Human-Computer Interactions (HCI), to reduce the dependency of bulky devices like physical keyboards and joysticks, many gesture-based HCI schemes are adopted. As a typical HCI technology, text input has aroused much concern and many virtual or wearable keyboards have been proposed. To further remove the keyboard and allow people to type in a device-free way, we propose AirTyping, i.e., a mid-air typing scheme based on Leap Motion. During the typing process, the Leap Motion Controller captures the typing gestures with cameras and provides the coordinates of finger joints. Then, AirTyping detects the possible keystrokes, infers the typed words based on Bayesian method, and outputs the inputted word sequence. The experiment results show that our system can detect the keystrokes and infer the typed text efficiently, i.e., the true positive rate of keystroke detection is 92.2%, while the accuracy that the top-1 inferred word is the typed word achieves 90.2%.
SP  - 168
EP  - 171
JF  - Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410530.3414387
ER  - 

TY  - NA
AU  - Dandu, Bharat; Shao, Yitian; Stanley, Andrew A.; Visell, Yon
TI  - WHC - Spatiotemporal Haptic Effects from a Single Actuator via Spectral Control of Cutaneous Wave Propagation
PY  - 2019
AB  - A key challenge in haptic engineering is to design methods for stimulating the skin – a continuous medium with infinitely many degrees of freedom – via practical devices with few degrees of freedom. Here, we show how to use a single actuator to generate tactile stimuli with dynamically controlled spatial extent. The method is based on the frequency-dependent damping of propagating waves in the skin. We use full-field optical vibrometry to show that vibrations introduced at the fingertip elicit waves in the finger that propagate proximally toward the hand. We show that these waves travel distances that decrease rapidly with frequency. We demonstrate the utility of these results by designing haptic effects that produce wave fields that expand or contract in size, and that can be delivered via a single actuator. In a perception experiment, subjects accurately (median >95%) identified these stimuli as expanding or contracting without prior exposure or training. These findings demonstrate how the physics of waves in the skin can be exploited for the design of spatiotemporal tactile effects that are practical and effective.
SP  - 425
EP  - 430
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816149
ER  - 

TY  - NA
AU  - Saniee-Monfared, Gazelle; Fan, Kevin; Xu, Qianq; Mizobuchi, Sachi; Zhou, Lewis; Irani, Pourang; Li, Wei
TI  - MobileHCI - Tent Mode Interactions: Exploring Collocated Multi-User Interaction on a Foldable Device
PY  - 2020
AB  - Foldable handheld displays have the potential to offer a rich interaction space, particularly as they fold into a convex form factor, for collocated multi-user interactions. In this paper, we explore Tent mode, a convex configuration of a foldable device partitioned into a primary and a secondary display, as well as a tertiary, Edge display that sits at the intersection of the two. We specifically explore the design space for a wide range of scenarios, such as co-browsing a gallery or co-planning a trip. Through a first collection of interviews, end-users identified a suite of apps that could leverage Tent mode for multi-user interactions. Based on these results we propose an interaction design space that builds on unique Tent mode properties, such as folding, flattening or tilting the device, and the interplay between the three sub-displays. We examine how end-users exploit this rich interaction space when presented with a set of collaborative tasks through a user study, and elicit potential interaction techniques. We implemented these interaction techniques and report on the preliminary user feedback we collected. Finally, we discuss the design implications for collocated interaction in Tent mode configurations.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403566
ER  - 

TY  - NA
AU  - Grubert, Jens; Witzani, Lukas; Ofek, Eyal; Pahud, Michel; Kranz, Matthias; Kristensson, Per Ola
TI  - VR - Effects of Hand Representations for Typing in Virtual Reality
PY  - 2018
AB  - Alphanumeric text entry is a challenge for Virtual Reality (VR) applications. VR enables new capabilities, impossible in the real world, such as an unobstructed view of the keyboard, without occlusion by the user's physical hands. Several hand representations have been proposed for typing in VR on standard physical keyboards. However, to date, these hand representations have not been compared regarding their performance and effects on presence for VR text entry. Our work addresses this gap by comparing existing hand representations with minimalistic fingertip visualization. We study the effects of four hand representations (no hand representation, inverse kinematic model, fingertip visualization using spheres and video inlay) on typing in VR using a standard physical keyboard with 24 participants. We found that the fingertip visualization and video inlay both resulted in statistically significant lower text entry error rates compared to no hand or inverse kinematic model representations. We found no statistical differences in text entry speed.
SP  - 151
EP  - 158
JF  - 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2018.8446250
ER  - 

TY  - NA
AU  - Danieau, Fabien; Guillotel, Philippe; Dumas, Olivier; Lopez, Thomas; Leroy, Bertrand; Mollet, Nicolas
TI  - VRST - HFX studio: haptic editor for full-body immersive experiences
PY  - 2018
AB  - Current virtual reality systems enable users to explore virtual worlds, fully embodied in avatars. This new type of immersive experience requires specific authoring tools. The traditional ones used in the movie and the video games industries were modified to support immersive visual and audio content. However, few solutions exist to edit haptic content, especially when the whole user's body is involved. To tackle this issue we propose HFX Studio, a haptic editor based on haptic perceptual models. Three models of pressure, vibration and temperature were defined to allow the spatialization of haptic effects on the user's body. These effects can be designed directly on the body (egocentric approach), or specified as objects of the scene (allocentric approach). The perceptual models are also used to describe capabilities of haptic devices. This way the created content is generic, and haptic feedback is rendered on the available devices. The concept has been implemented with the Unity®game engine, a tool already used in VR production. A qualitative pilot user study was conducted to analyze the usability of our tool with expert users. Results shows that the edition of haptic feedback is intuitive for these users.
SP  - 37
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281518
ER  - 

TY  - NA
AU  - Israr, Ali; Schwemler, Zachary; Mars, John; Krainer, Brian
TI  - VRST - VR360HD: a VR360° player with enhanced haptic feedback
PY  - 2016
AB  - We present a VR360° video player with haptic feedback playback. The VR360HD application enhances VR viewing experience by triggering customized haptic effects associated with user's activities, biofeedback, network messages and customizable timeline triggers incorporated in the VR media. The app is developed in the Unity3D game engine and tested using a GearVR headset, therefore allowing users to add animations to VR gameplay and to the VR360° streams. A custom haptic plugin allows users to author and associate animated haptic effects to the triggers, and playback these effects on a custom haptic hardware, the Haptic Chair. We show that the VR360HD app creates rich tactile effects and can be easily adapted to other media types.
SP  - 183
EP  - 186
JF  - Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2993369.2993404
ER  - 

TY  - NA
AU  - Wang, Anran; Gollakota, Shyamnath
TI  - MilliSonic: Pushing the Limits of Acoustic Motion Tracking.
PY  - 2019
AB  - Recent years have seen interest in device tracking and localization using acoustic signals. State-of-the-art acoustic motion tracking systems however do not achieve millimeter accuracy and require large separation between microphones and speakers, and as a result, do not meet the requirements for many VR/AR applications. Further, tracking multiple concurrent acoustic transmissions from VR devices today requires sacrificing accuracy or frame rate. We present MilliSonic, a novel system that pushes the limits of acoustic based motion tracking. Our core contribution is a novel localization algorithm that can provably achieve sub-millimeter 1D tracking accuracy in the presence of multipath, while using only a single beacon with a small 4-microphone array.Further, MilliSonic enables concurrent tracking of up to four smartphones without reducing frame rate or accuracy. Our evaluation shows that MilliSonic achieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for smartphones, which is 5x more accurate than state-of-the-art systems. MilliSonic enables two previously infeasible interaction applications: a) 3D tracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D tracking for the Google Cardboard VR system using a small microphone array.
SP  - 18
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300248
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Mayer, Benedikt; Mardanbegi, Diako; Gellersen, Hans
TI  - SUI - Gaze + pinch interaction in virtual reality
PY  - 2017
AB  - Virtual reality affords experimentation with human abilities beyond what's possible in the real world, toward novel senses of interaction. In many interactions, the eyes naturally point at objects of interest while the hands skilfully manipulate in 3D space. We explore a particular combination for virtual reality, the Gaze + Pinch interaction technique. It integrates eye gaze to select targets, and indirect freehand gestures to manipulate them. This keeps the gesture use intuitive like direct physical manipulation, but the gesture's effect can be applied to any object the user looks at --- whether located near or far. In this paper, we describe novel interaction concepts and an experimental system prototype that bring together interaction technique variants, menu interfaces, and applications into one unified virtual experience. Proof-of-concept application examples were developed and informally tested, such as 3D manipulation, scene navigation, and image zooming, illustrating a range of advanced interaction capabilities on targets at any distance, without relying on extra controller devices.
SP  - 99
EP  - 108
JF  - Proceedings of the 5th Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131277.3132180
ER  - 

TY  - NA
AU  - Yi, HyeonBeom; Hong, Jiwoo; Kim, Hwan; Lee, Woohun
TI  - VRST - DexController : Designing a VR Controller with Grasp-Recognition for Enriching Natural Game Experience
PY  - 2019
AB  - We present DexController, which is a hand-held controller leveraging grasp as an additional modality for virtual reality (VR) game. The pressure-sensitive surface of DexController was designed to recognize two different grasp-poses (i.e. precision grip and power grip) and detect grasp-force. Based on the results of two feasibility tests, a VR defense game was designed in which players could attack each enemy using the proper weapon with a proper level of force. A within-subject comparative study is conducted with a button-based controller which has the same physical form of DexController. The results indicated that DexController enhanced the perceived naturalness of the controller and game enjoyment, with having acceptable physical demand. This study clarifies the empirical effect of utilizing grasp-recognition on VR game controller to enhance interactivity. Also, we provide insight for the integration of VR game elements with the grasping modality of a controller.
SP  - NA
EP  - NA
JF  - 25th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359996.3364263
ER  - 

TY  - JOUR
AU  - Zhican, Yang; Yu, Chun; Yi, Xin; Shi, Yuanchun
TI  - Investigating Gesture Typing for Indirect Touch
PY  - 2019
AB  - With the development of ubiquitous computing, entering text on HMDs and smart TVs using handheld touchscreen devices (e.g., smartphone and controller) is becoming more and more attractive. In these indirect touch scenarios, the touch input surface is decoupled from the visual display. Compared with direct touch input, entering text using a keyboard in indirect touch is more challenging because before the finger touch, no visual feedback is available for locating the touch finger. Aiming at this problem, in this paper, we investigate the feasibility of gesture typing for indirect touch since keeping the finger in touch with the screen during typing makes it possible to provide continuous visual feedback, which is beneficial for increasing the input performance. We first examine users' gesture typing ability in terms of the appropriate keyboard size and location in motor space and then compare the typing performance in direct and indirect touch mode. We then propose an improved design to address the uncertainty and inaccuracy of the first touch. Our evaluation result shows that users can quickly acquire indirect gesture typing, and type 22.3 words per minute after 30 phases, which significantly outperforms previous numbers in literature. Our work provides the empirical support for leveraging gesture typing for indirect touch.
SP  - 117
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3351275
ER  - 

TY  - CHAP
AU  - Seifi, Hasti
TI  - Deriving Semantics and Interlinkages of Facets
PY  - 2019
AB  - Haptic facets (categories of attributes that characterize collection items in different ways) are a way to describe, navigate and analyze the cognitive frameworks by which users make sense of qualitative and affective characteristics of haptic sensations. Embedded in tools, they will provide designers and end-users interested in customization with a road-mapped perceptual and cognitive design space. In the previous chapter, we compiled five haptic facets based on how people describe vibrations: physical, sensory, emotional, metaphoric, and usage examples. Here, we report a study in which we deployed these facets to identify underlying dimensions and cross-linkages in participants’ perception of a 120-item vibration library. We found that the facets are crosslinked in people’s minds, and discuss three scenarios where the facet-based organizational schemes, their linkages and consequent redundancies can support design, evaluation and personalization of expressive vibrotactile effects. Furthermore, we report between-subject variation (individual differences) and within-subject consistency (reliability) in participants’ rating and tagging patterns to inform future progress on haptic evaluation. This facet-based approach is also applicable to other kinds of haptic sensations. Finally, we detail our novel methodology for collecting user annotations for a large haptic collection in the lab.
SP  - 71
EP  - 109
JF  - Springer Series on Touch and Haptic Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-11379-7_5
ER  - 

TY  - JOUR
AU  - Kim, Seungwon; Billinghurst, Mark; Lee, Gun A.; Huang, Weidong
TI  - Gaze window: A new gaze interface showing relevant content close to the gaze point
PY  - 2020
AB  - NA
SP  - 979
EP  - 996
JF  - Journal of the Society for Information Display
VL  - 28
IS  - 12
PB  - 
DO  - 10.1002/jsid.954
ER  - 

TY  - NA
AU  - Chen, Huijie; Li, Fan; Wang, Yu
TI  - INFOCOM - EchoTrack: Acoustic device-free hand tracking on smart phones
PY  - 2017
AB  - This paper explores the limits of acoustic ranging on smart phone in the scenario of device-free hand tracking. Tracking the hand is challenging since it requires continuously locating the moving hand in the air with fine resolution. Existing work on hand tracking relies on special hardware or requires users hold the mobile device. This paper presents EchoTrack, which continuously locates the hand by leveraging mobile audio hardware advances without special infrastructure supported. EchoTrack measures the distance from the hand to the speaker array embedded in smart phone via the chirp's Time of Flight (TOF). The speaker array and hand yield a unique triangle. The hand can be located with this triangular geometry. The trajectory accuracy can be improved with the method of Doppler shift compensation and trajectory correction (i.e., roughness penalty smoothing method). We implement a prototype on smart phone and the evaluation shows that EchoTrack can achieve tracking accuracy within about three centimeters of 76% and two centimeters of 48%.
SP  - 1
EP  - 9
JF  - IEEE INFOCOM 2017 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom.2017.8057101
ER  - 

TY  - JOUR
AU  - Liu, Manni; Cheng, Linsong; Qian, Kun; Wang, Jiliang; Wang, Jin; Liu, Yunhao
TI  - Indoor acoustic localization: a survey
PY  - 2020
AB  - Applications of localization range from body tracking, gesture capturing, indoor plan construction to mobile health sensing. Technologies such as inertial sensors, radio frequency signals and cameras have been deeply excavated to locate targets. Among all the technologies, the acoustic signal gains enormous favor considering its comparatively high accuracy with common infrastructure and low time latency. Range-based localization falls into two categories: absolute range and relative range. Different mechanisms, such as Time of Flight, Doppler effect and phase shift, are widely studied to achieve the two genres of localization. The subcategories show distinguishing features but also face diverse challenges. In this survey, we present a comprehensive overview on various indoor localization systems derived from the various mechanisms. We also discuss the remaining issues and the future work.
SP  - 1
EP  - 24
JF  - Human-centric Computing and Information Sciences
VL  - 10
IS  - 1
PB  - 
DO  - 10.1186/s13673-019-0207-4
ER  - 

TY  - NA
AU  - Kytö, Mikko; Ens, Barrett; Piumsomboon, Thammathip; Lee, Gun A.; Billinghurst, Mark
TI  - CHI - Pinpointing: Precise Head- and Eye-Based Target Selection for Augmented Reality
PY  - 2018
AB  - Head and eye movement can be leveraged to improve the user's interaction repertoire for wearable displays. Head movements are deliberate and accurate, and provide the current state-of-the-art pointing technique. Eye gaze can potentially be faster and more ergonomic, but suffers from low accuracy due to calibration errors and drift of wearable eye-tracking sensors. This work investigates precise, multimodal selection techniques using head motion and eye gaze. A comparison of speed and pointing accuracy reveals the relative merits of each method, including the achievable target size for robust selection. We demonstrate and discuss example applications for augmented reality, including compact menus with deep structure, and a proof-of-concept method for on-line correction of calibration drift.
SP  - 81
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173655
ER  - 

TY  - JOUR
AU  - Usman, Muhammad; Asghar, Muhammad Rizwan; Ansari, Imran Shafique; Granelli, Fabrizio; Qaraqe, Khalid A.
TI  - Technologies and Solutions for Location-Based Services in Smart Cities: Past, Present, and Future
PY  - 2018
AB  - Location-based services (LBS) in smart cities have drastically altered the way cities operate, giving a new dimension to the life of citizens. LBS rely on location of a device, where proximity estimation remains at its core. The applications of LBS range from social networking and marketing to vehicle-to-everything communications. In many of these applications, there is an increasing need and trend to learn the physical distance between nearby devices. This paper elaborates upon the current needs of proximity estimation in LBS and compares them against the available Localization and Proximity (LP) finding technologies (LP technologies in short). These technologies are compared for their accuracies and performance based on various different parameters, including latency, energy consumption, security, complexity, and throughput. Hereafter, a classification of these technologies, based on various different smart city applications, is presented. Finally, we discuss some emerging LP technologies that enable proximity estimation in LBS and present some future research areas.
SP  - 22240
EP  - 22248
JF  - IEEE Access
VL  - 6
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2826041
ER  - 

TY  - JOUR
AU  - Kubo, Yuki; Takada, Ryosuke; Shizuki, Buntarou; Takahashi, Shin
TI  - Exploring Context-Aware User Interfaces for Smartphone-Smartwatch Cross-Device Interaction
PY  - 2017
AB  - In this study, we explore context-aware cross-device interactions between a smartphone and smartwatch. We present 24 contexts, and then examine and prioritize suitable user interfaces (UIs) for each. In addition, we present example applications, including a map, notification management system, multitasking application, music player, and video chat application, each of which has its own context-aware UIs. To support these context-aware UIs, we investigate the performance of our context recognizer in which recognition is based on machine-learning using the accelerometers in a smartphone and smartwatch. We conduct seven different evaluations using four machine-learning algorithms: J48 decision tree, sequential minimal optimization (SMO)-based support vector machine (SVM), random forest, and multilayer perceptron. With each algorithm, we conduct a long-interval experiment to examine the level of accuracy at which each context is recognized using data previously collected for training. The results show that SMO-based SVM is suitable for recognizing the 24 contexts considered in this study.
SP  - 69
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 3
PB  - 
DO  - 10.1145/3130934
ER  - 

TY  - NA
AU  - Yamanaka, Shota
TI  - CHI - Steering Performance with Error-accepting Delays
PY  - 2019
AB  - In steering law tasks, deviating from the path is immediately considered an error operation. However, in navigating a hierarchical menu item, which is a representative application of the law, a deviation within a short duration is sometimes permitted. We tested the validity of the steering law model with various durations of such error-accepting delays and found that it showed high fits for each delay condition (R2 > 0.96) but poor fits if the delay values were not separated (R2 = 0.58). Because the average movement speed linearly increased as the delay increased, we refined the model by taking the delay into account, and the fitness was significantly improved (R2 = 0.97). Our model will help GUI designers estimate the average operational time on the basis of the menu item length, width, and error-accepting delay.
SP  - 570
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300800
ER  - 

TY  - NA
AU  - Zhang, Mingrui Ray; Zhai, Shumin; Wobbrock, Jacob O.
TI  - CHI - Text Entry Throughput: Towards Unifying Speed and Accuracy in a Single Performance Metric
PY  - 2019
AB  - Human-computer input performance inherently involves speed-accuracy tradeoffs---the faster users act, the more inaccurate those actions are. Therefore, comparing speeds and accuracies separately can result in ambiguous outcomes: Does a fast but inaccurate technique perform better or worse overall than a slow but accurate one? For pointing, speed and accuracy has been unified for over 60 years as throughput (bits/s) (Crossman 1957, Welford 1968), but to date, no similar metric has been established for text entry. In this paper, we introduce a text entry method-independent throughput metric based on Shannon information theory (1948). To explore the practical usability of the metric, we conducted an experiment in which 16 participants typed with a laptop keyboard using different cognitive sets, i.e., speed-accuracy biases. Our results show that as a performance metric, text entry throughput remains relatively stable under different speed-accuracy conditions. We also evaluated a smartphone keyboard with 12 participants, finding that throughput varied least compared to other text entry metrics. This work allows researchers to characterize text entry performance with a single unified measure of input efficiency.
SP  - 636
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300866
ER  - 

TY  - CHAP
AU  - Wölfel, Kim; Müller, Jörg; Henrich, Dominik
TI  - INTERACT (3) - ToolBot: Robotically Reproducing Handicraft
PY  - 2021
AB  - We present ToolBot, a robotic system for creating handicraft with hand-held tools. Through using our process users are able to work with their favorite tools when producing handicraft. We observe the production process using motion capture. We provide a trajectory editor, which allows non-robotics-experts to semi-automatically edit the tool motion. Finally, ToolBot executes the movements using a lightweight robot arm. This is non-trivial for three reasons. First, the original tool trajectory might be non-replicable due the robots limited workspace, collisions, or singularities. Second, the robots maximum velocities and accelerations can be exceeded. Third, the user might want to let the robot use a different tool or replicate downscaled tool motion. We resolve these issues, such that the hand-held tool can be mounted on a robot arm, which can then mass-produce the handicraft. We evaluate the usability of ToolBot through a user study.
SP  - 470
EP  - 489
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85613-7_32
ER  - 

TY  - NA
AU  - Tokac, Iremnur; Bruyninckx, Herman; Cannaerts, Corneel; Moere, Andrew Vande
TI  - CHI Extended Abstracts - Material Sketching: Towards the Digital Fabrication of Emergent Material Effects
PY  - 2019
AB  - Designing for digital or robotic fabrication typically involves a virtual model in order to determine and coordinate the required operations of its construction. As a result, its creative design space becomes constrained to material expressions that can be predicted through digital modeling. This paper describes our preliminary thinking and first empirical results when this digital modeling phase is skipped, and the designing occurs interactively 'with' the fabrication operations themselves. By analyzing the material responses of corrugated cardboard to simple linear cutting operations that are executed by a robotic arm, we demonstrate how emergent material effects can be discovered improvisationally. Such material effects cannot be virtually modeled, however, they can be recreated and controlled by the robotic manipulations. We believe this form of 'material sketching' broadens the advances in 'human-fabrication interaction' towards a novel and unforeseeable expressions of physical form that require a much more direct, yet still digital, relationship with materiality.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3313036
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Phang, Xiao-Shen; Ha, Taejin; Woo, Woontack; Quigley, Aaron
TI  - CHI Extended Abstracts - TiTAN: Exploring Midair Text Entry using Freehand Input
PY  - 2017
AB  - TiTAN is a spatial user interface that enables freehand, midair text entry with a distant display while only requiring a low-cost depth sensor. Our system aims to leverage one's familiarity with the QWERTY layout. It allows users to input text, in midair, by mimicking the typing action they typically perform on a physical keyboard or touchscreen. Here, both hands and ten fingers are individually tracked, along with click action detection which enables a wide variety of interactions. We propose three midair text entry techniques and evaluate the TiTAN system with two different sensors.
SP  - 3041
EP  - 3049
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053228
ER  - 

TY  - NA
AU  - Tian, Rundong; Sterman, Sarah; Chiou, Ethan; Warner, Jeremy; Paulos, Eric
TI  - CHI - MatchSticks: Woodworking through Improvisational Digital Fabrication
PY  - 2018
AB  - Digital fabrication tools have broadened participation in making and enabled new methods of rapid physical prototyping across diverse materials. We present a novel smart tool designed to complement one of the first materials employed by humans - wood - and celebrate the fabrication practice of joinery. Our tool, MatchSticks, is a digital fabrication system tailored for joinery. Combining a portable CNC machine, touchscreen user interface, and parametric joint library, MatchSticks enables makers of varying skill to rapidly explore and create artifacts from wood. Our system embodies tacit woodworking knowledge and distills the distributed workflow of CNC tools into a hand tool; it operates on materials existing machines find difficult, produces assemblies much larger than its workspace, and supports the parallel creation of geometries. We describe the workflow and technical details of our system, present example artifacts produced by our tool, and report results from our user study.
SP  - 149
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173723
ER  - 

TY  - BOOK
AU  - Barz, Michael; Daiber, Florian; Bulling, Andreas
TI  - ETRA - Prediction of gaze estimation error for error-aware gaze-based interfaces
PY  - 2016
AB  - Gaze estimation error is inherent in head-mounted eye trackers and seriously impacts performance, usability, and user experience of gaze-based interfaces. Particularly in mobile settings, this error varies constantly as users move in front and look at different parts of a display. We envision a new class of gaze-based interfaces that are aware of the gaze estimation error and adapt to it in real time. As a first step towards this vision we introduce an error model that is able to predict the gaze estimation error. Our method covers major building blocks of mobile gaze estimation, specifically mapping of pupil positions to scene camera coordinates, marker-based display detection, and mapping of gaze from scene camera to on-screen coordinates. We develop our model through a series of principled measurements of a state-of-the-art head-mounted eye tracker.
SP  - 275
EP  - 278
JF  - Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2857491.2857493
ER  - 

TY  - NA
AU  - Jung, Jingun; Son, Sunmin; Lee, Sangyoon; Kim, Yeonsu; Lee, Geehyuk
TI  - CHI - ThroughHand: 2D Tactile Interaction to Simultaneously Recognize and Touch Multiple Objects
PY  - 2021
AB  - Users with visual impairments find it difficult to enjoy real-time 2D interactive applications on the touchscreen. Touchscreen applications such as sports games often require simultaneous recognition of and interaction with multiple moving targets through vision. To mitigate this issue, we propose ThroughHand, a novel tactile interaction that enables users with visual impairments to interact with multiple dynamic objects in real time. We designed the ThroughHand interaction to utilize the potential of the human tactile sense that spatially registers both sides of the hand with respect to each other. ThroughHand allows interaction with multiple objects by enabling users to perceive the objects using the palm while providing a touch input space on the back of the same hand. A user study verified that ThroughHand enables users to locate stimuli on the palm with a margin of error of approximately 13 mm and effectively provides a real-time 2D interaction experience for users with visual impairments.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445530
ER  - 

TY  - NA
AU  - Kaviani, Samira; Rahimi, Amir; Hartley, Richard
TI  - Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation.
PY  - 2021
AB  - To obtain 3D annotations, we are restricted to controlled environments or synthetic datasets, leading us to 3D datasets with less generalizability to real-world scenarios. To tackle this issue in the context of semi-supervised 3D hand shape and pose estimation, we propose the Pose Alignment network to propagate 3D annotations from labelled frames to nearby unlabelled frames in sparsely annotated videos. We show that incorporating the alignment supervision on pairs of labelled-unlabelled frames allows us to improve the pose estimation accuracy. Besides, we show that the proposed Pose Alignment network can effectively propagate annotations on unseen sparsely labelled videos without fine-tuning.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Homaeian, Leila; Wallace, James R.; Scott, Stacey D.
TI  - Handoff and Deposit: Designing Temporal Coordination in Cross-Device Transfer Techniques for Mixed-Focus Collaboration
PY  - 2022
AB  - <jats:p>When working together, people frequently share information with each other to enable division of labour, assistance, and delegation of responsibility. The literature has explored both synchronous and asynchronous transfer techniques, known as Handoff and Deposit, respectively. However, current cross-device environments tend to only provide a single mechanism. Moreover, we have little understanding of the impact of different techniques on collaborative process. To understand how Handoff and Deposit may be designed to support complex sensemaking tasks, we followed a Research through Design process to iteratively design Handoff and Deposit techniques using paper and digital sketches and high-fidelity prototypes. We consulted the HCI literature to corroborate our findings with studies and descriptions of existing cross-device transfer designs and to understand the potential impact of those designs on mixed-focus collaboration. We learned that as we move away from a restricted physical workspace and leverage the flexibility of digital personal devices, there is a large design space for realizing cross-device transfer. To inform these designs, we provide five design considerations for cross-device transfer techniques: Transfer Acceptance, Action Dependencies, Immediate Usability, Interruption Potential, and Connection Actions.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555192
ER  - 

TY  - NA
AU  - Gil, Hyunjae; Shin, Yonghwan; Son, Hyungki; Hwang, Inwook; Oakley, Ian; Kim, Jin Ryong
TI  - VRST - Characterizing In-Air Eyes-Free Typing Movements in VR
PY  - 2020
AB  - We empirically explore fundamental requirements for achieving VR in-air typing by observing the unconstrained eyes-free in-air typing of touch typists. We show that unconstrained typing movements differ substantively from previously observed constrained in-air typing movements and introduce a novel binary categorization of typing strategies: typists who use finger movements alone (FINGER) and those who combine finger movement with gross hand movement (HAND). We examine properties of finger kinematics, correlated movement of fingers, interrelation in consecutive key-strokes, and 3D distribution of key-stroke movements. We report that, compared to constrained typing, unconstrained typing generates shorter (49 mm) and faster (764 mm/s) key-strokes with a high correlation of finger movement and that the HAND strategy group exhibits more dynamic key-strokes. We discuss how these findings can inform the design of future in-air typing systems.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418963
ER  - 

TY  - NA
AU  - Pham, Duc-Minh; Stuerzlinger, Wolfgang
TI  - VRST - HawKEY: Efficient and Versatile Text Entry for Virtual Reality
PY  - 2019
AB  - Text entry is still a challenging task in modern Virtual Reality (VR) systems. The lack of efficient text entry methods limits the applications that can be used productively in VR. Previous work has addressed this issue through virtual keyboards or showing the physical keyboard in VR. While physical keyboards afford faster text entry, they usually require a seated user and an instrumented environment. We introduce a new keyboard, worn on a hawker’s tray in front of the user, which affords a compact, simple, flexible, and efficient text entry solution for VR, without restricting physical movement. In our new video condition, we also show the keyboard only when the user is looking down at it. To evaluate our novel solution and to identify good keyboard visualizations, we ran a user study where we asked participants to enter both lowercase sentences as well as complex text while standing. The results show that text entry rates are affected negatively by simplistic keyboard visualization conditions and that our solution affords desktop text entry rates, even when standing.
SP  - 21
EP  - NA
JF  - 25th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359996.3364265
ER  - 

TY  - JOUR
AU  - Adhikary, Jiban; Vertanen, Keith
TI  - Text Entry in Virtual Environments using Speech and a Midair Keyboard
PY  - 2021
AB  - Entering text in virtual environments can be challenging, especially without auxiliary input devices. We investigate text input in virtual reality using hand-tracking and speech. Our system visualizes users' hands in the virtual environment, allowing typing on an auto-correcting midair keyboard. It also supports speaking a sentence and then correcting errors by selecting alternative words proposed by a speech recognizer. We conducted a user study in which participants wrote sentences with and without speech. Using only the keyboard, users wrote at 11 words-per-minute at a 1.2% error rate. Speaking and correcting sentences was faster and more accurate at 28 words-per-minute and a 0.5% error rate. Participants achieved this performance despite half of sentences containing an uncommon out-of-vocabulary word (e.g. proper name). For sentences with only in-vocabulary words, performance using speech and midair keyboard corrections was faster at 36 words-per-minute with a low 0.3% error rate.
SP  - 2648
EP  - 2658
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067776
ER  - 

TY  - JOUR
AU  - Metzger, Florian; Geisler, Stefan; Grigorjew, Alexej; Loh, Frank; Moldovan, Christian; Seufert, Michael; Hosfeld, Tobias
TI  - An Introduction to Online Video Game QoS and QoE Influencing Factors
PY  - 2022
AB  - Online video games and cloud gaming are rapidly growing in pervasiveness. Their resource demands can put significant stress on the global communication infrastructure. And network conditions are amongst the chief factors that influence one&#x2019;s enjoyment while playing games. This makes it imperative for video games to be considered for network dimensioning, server placement or protocol development. For that reason, in this work we provide an introduction to the technical aspects of video games in general and of their network aspects in particular. This understanding forms the basis for a rich taxonomy of factors that influence and provide context to a video game&#x2019;s Quality of Service (QoS) and Quality of Experience (QoE). The taxonomy covers influence factors from all aspects involved in a video game, from the subjective player and game influence factors to the system and networking influence factors. Finally, this work gives an overview of conducted and ongoing research as well as future research opportunities while taking into account lessons learned from past approaches.
SP  - 1894
EP  - 1925
JF  - IEEE Communications Surveys & Tutorials
VL  - 24
IS  - 3
PB  - 
DO  - 10.1109/comst.2022.3177251
ER  - 

TY  - NA
AU  - Lander, Christian; Gehring, Sven; Löchtefeld, Markus; Bulling, Andreas; Krüger, Antonio
TI  - MUM - Eyemirror: mobile calibration-free gaze approximation using corneal imaging
PY  - 2017
AB  - Gaze is a powerful measure of people's attracted attention and reveals where we are looking within our current FOV. Hence gaze-based interfaces are gaining in importance. However, gaze estimation usually requires extensive hardware and depends on a calibration that has to be renewed regularly. We present EyeMirror, a mobile device for calibration-free gaze approximation on surfaces (e.g., displays). It consists of a head-mounted camera, connected to a wearable mini-computer, capturing the environment reflected on the human cornea. The corneal images are analyzed using natural feature tracking for gaze estimation on surfaces. In two lab studies we compared variations of EyeMirror against established methods for gaze estimation in a display scenario, and investigated the effect of display content (i.e. number of features). EyeMirror achieved 4.03° gaze estimation error, while we found no significant effect of display content.
SP  - 279
EP  - 291
JF  - Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3152832.3152839
ER  - 

TY  - NA
AU  - Streli, Paul; Jiang, Jiaxi; Fender, Andreas Rene; Meier, Manuel; Romat, Hugo; Holz, Christian
TI  - TapType: Ten-finger text entry on everyday surfaces via Bayesian inference
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501878
ER  - 

TY  - BOOK
AU  - Zhang, Zigang; Sun, Minghui; Gao, BoYu; Wang, Limin
TI  - VR Workshops - 2-Thumbs Typing: A Novel Bimanual Text Entry Method in Virtual Reality Environments
PY  - 2021
AB  - We propose a new technique named 2-Thumbs Typing (2TT) enabling text entry with a touchpad in HTC VIVE controller using two thumbs. 2TT method works similarly with bimanual handwriting input but using new designed uni-stroke gestures considering only strokes’ direction. We first design a set of gestures and improve them to finish the final design by a preliminary study through memory, performance efficiency and ease of use. The initial results show that the 2TT technique is easy and comfortable to use, no additional equipment required and supporting eyes-free entry. 2TT can reach 8.5 words per minute with extensive training.
SP  - 530
EP  - 531
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00147
ER  - 

TY  - NA
AU  - Lander, Christian; Wiehr, Frederik; Herbig, Nico; Krüger, Antonio; Löchtefeld, Markus
TI  - CHI Extended Abstracts - Inferring Landmarks for Pedestrian Navigation from Mobile Eye-Tracking Data and Google Street View
PY  - 2017
AB  - While it has been well established that incorporating landmarks into route descriptions enhances understanding and performance of wayfinding, only a very few available systems make use of them. This is primarily due to the fact that landmark data is often not available, and the creation of the data is connected to tedious manual labor. Prior work explored crowd-sourced approaches to collect landmark data, but most of that work focused on explicit user input to gather the data. In this paper, we presented our work towards a system to automatically infer suitable landmarks for pedestrian navigation instructions from mobile eye-tracking data. By matching the video feed of the scene camera of a head-mounted eye tracker to Google Street View imagery, our system is able to cluster the visual attention of the users and extract suitable landmarks from it. We present early results of a field study conducted with six participants to highlight the potential of our approach.
SP  - 2721
EP  - 2729
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053201
ER  - 

TY  - NA
AU  - Bockes, Florian; Wimmer, Raphael; Schmid, Andreas
TI  - CHI Extended Abstracts - LagBox -- Measuring the Latency of USB-Connected Input Devices
PY  - 2018
AB  - High latency in an interactive system liMassachusetts Institute of Technologys its usability. In order to reduce end-to-end latency of such systems, it is necessary to analyze and optimize the latency of individual contributors, such as input devices, applica-tions, or displays. We present a simple tool for measur-ing the latency of USB-connected input devices with sub-millisecond accuracy. The tool, based on a Rasp-berry Pi 2 microcomputer, repeatedly toggles a button of a game controller, mouse, or keyboard via an opto-coupler soldered to the button and measures the time until the input event arrives. This helps researchers, developers and users to identify and characterize sources of input lag. An initial comparison of multiple input devices shows differences not only in average latency but also in its variance.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188632
ER  - 

TY  - NA
AU  - Riche, Yann; Riche, Nathalie Henry; Hinckley, Ken; Panabaker, Sheri; Fuelling, Sarah; Williams, Sarah Graham
TI  - CHI - As We May Ink?: Learning from Everyday Analog Pen Use to Improve Digital Ink Experiences
PY  - 2017
AB  - This paper sheds light on gaps and discrepancies between the experiences afforded by analog pens and their digital counterparts. Despite the long history (and recent renaissance) of digital pens, the literature still lacks a comprehensive survey of what types of marks people make and what motivates them to use ink-both analog and digital in daily life. To capture the diversity of inking behaviors and tease out the unique affordances of pen-and ink, we conducted a diary study with 26 participants from diverse backgrounds. From analysis of 493 diary entries we identified 8 analog pen-and-ink activities, and 9 affordances of pens. We contextualized and contrasted these findings using a survey with 1,633 respondents and a follow-up diary study with 30 participants, observing digital pens. Our analysis reveals gaps and research opportunities based on pen affordances not yet fully explored in the literature.
SP  - 3241
EP  - 3253
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025716
ER  - 

TY  - NA
AU  - Luzhnica, Granit; Stein, Sebastian; Veas, Eduardo; Pammer, Viktoria; Williamson, John; Smith, Roderick Murray
TI  - ISWC - Personalising vibrotactile displays through perceptual sensitivity adjustment
PY  - 2017
AB  - Haptic displays are commonly limited to transmitting a discrete set of tactile motives. In this paper, we explore the transmission of real-valued information through vibrotactile displays. We simulate spatial continuity with three perceptual models commonly used to create phantom sensations: the linear, logarithmic and power model. We show that these generic models lead to limited decoding precision, and propose a method for model personalization adjusting to idiosyncratic and spatial variations in perceptual sensitivity. We evaluate this approach using two haptic display layouts: circular, worn around the wrist and the upper arm, and straight, worn along the forearm. Results of a user study measuring continuous value decoding precision show that users were able to decode continuous values with relatively high accuracy (4.4% mean error), circular layouts performed particularly well, and personalisation through sensitivity adjustment increased decoding precision.
SP  - 66
EP  - 73
JF  - Proceedings of the 2017 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123021.3123029
ER  - 

TY  - NA
AU  - Peng, Huaishu; Briggs, Jimmy; Wang, Cheng Yao; Guo, Kevin; Kider, Joseph T.; Mueller, Stefanie; Baudisch, Patrick; Guimbretière, François
TI  - CHI - RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer
PY  - 2018
AB  - We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience. As a designer creates a new model using RoMA AR CAD editor, features are constructed concurrently by a 3D printing robotic arm sharing the same design volume. The partially printed physical model then serves as a tangible reference for the designer as she adds new elements to her design. RoMA's proxemics-inspired handshake mechanism between the designer and the 3D printing robotic arm allows the designer to quickly interrupt printing to access a printed area or to indicate that the robot can take full control of the model to finish printing. RoMA lets users integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts or to extend existing objects. We conclude by presenting the strengths and limitations of our current design.
SP  - 579
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174153
ER  - 

TY  - BOOK
AU  - Hinckley, Ken
TI  - The Handbook of Multimodal-Multisensor Interfaces, Volume 1 (1) - A background perspective on touch as a multimodal (and multisensor) construct
PY  - 2017
AB  - This chapter will illustrate, through a series of examples, seven different perspectives of how touch input can be re-framed and re-conceived as a multimodal, multisensor construct.These perspectives often can particularly benefit from considering the background of interaction [Buxton 1995]---that is, interaction that takes place "behind" the foreground of the user's conscious attention, in response to sensed contextual information. For example, with touch-screen input, the user's intentional contact with the screen would comprise the foreground act, but the resulting vibrational forces imparted to the device can be sensed and leveraged "in the background" to infer additional contextual details of the touch.Over the years, I've found this background perspective extremely useful as a tool-for-thought to devise novel interactions, especially when multiple modalities and multiple sensors can be used simultaneously, in complementary and mutually reenforcing ways. This approach can be especially helpful to break out of whatever preconceptions one might have regarding an input modality, even something as 144 Chapter 4 A Background Perspective on Touch as a Multimodal (and Multisensor) Construct seemingly well studied and well understood as touch, which just for that reason will provide us with the bulk of the examples that I draw from in this chapter.These perspectives of touch range from its traditional view as a modality that affords direct and intentional touchscreen input, to the inadvertent (yet still potentially valuable) phenomenon of unintentional (or "Midas") touch. We will consider various combinations of touch with other sensor signals such as tilt, inertial motion, grip sensing, and above-screen pre-touch sensing. We'll also discuss interesting ways to use pen and touch as complementary modalities for bimanual interaction. The focus of the chapter is design-centric, with contributions that focus on invention and innovation [Hudson and Mankoff 2014], rather than contributions, say, of formal experimental analysis or recognition methodologies. Likewise, the focus here is on the input side of touch, rather than the output side---as afforded by haptic and tactile feedback technologies (cf. Chapter 3).The key point of this chapter is that low-level "sensing" channels such as grip and proximity and motion can be conceived of as new modalities that afford natural interaction with devices. And, likewise, it is possible to subtly shift our perspective of "modalities" to consider novel ways that they may yield insights of sensing, particularly if considered from the oft-neglected perspective that the background of interaction can support the user's task focus in the foreground of attention most effectively.To encourage reflection on the material, the chapter concludes with eight openended Focus Questions that can also serve as starting-points for independent research projects, as well as a Glossary of key terms.
SP  - 143
EP  - 199
JF  - The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3015783.3015789
ER  - 

TY  - NA
AU  - Rädle, Roman; Jetter, Hans-Christian; Fischer, Jonathan; Gabriel, Inti; Klokmose, Clemens Nylandsted; Reiterer, Harald; Holz, Christian
TI  - CHI - PolarTrack: Optical Outside-In Device Tracking that Exploits Display Polarization
PY  - 2018
AB  - PolarTrack is a novel camera-based approach to detecting and tracking mobile devices inside the capture volume. In PolarTrack, a polarization filter continuously rotates in front of an off-the-shelf color camera, which causes the displays of observed devices to periodically blink in the camera feed. The periodic blinking results from the physical characteristics of current displays, which shine polarized light either through an LC overlay to produce images or through a polarizer to reduce light reflections on OLED displays. PolarTrack runs a simple detection algorithm on the camera feed to segment displays and track their locations and orientations, which makes PolarTrack particularly suitable as a tracking system for cross-device interaction with mobile devices. Our evaluation of PolarTrack's tracking quality and comparison with state-of-the-art camera-based multi-device tracking showed a better tracking accuracy and precision with similar tracking reliability. PolarTrack works as standalone multi-device tracking but is also compatible with existing camera-based tracking systems and can complement them to compensate for their limitations.
SP  - 497
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174071
ER  - 

TY  - NA
AU  - Zhang, Xiang; Ikematsu, Kaori; Kato, Kunihiro; Sugiura, Yuta
TI  - ReflecTouch: Detecting Grasp Posture of Smartphone Using Corneal Reflection Images
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517440
ER  - 

TY  - JOUR
AU  - Seifi, Hasti; MacLean, Karon E.
TI  - Exploiting haptic facets
PY  - 2017
AB  - NA
SP  - 38
EP  - 61
JF  - International Journal of Human-Computer Studies
VL  - 107
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2017.04.003
ER  - 

TY  - NA
AU  - Lee, Sun-Kyung; Kim, Jong-Hwan
TI  - ACM Multimedia - Air-Text: Air-Writing and Recognition System
PY  - 2021
AB  - Text entry takes an important role of effectively delivering the intention of users to computers, where physical and soft keyboards have been widely used. However, with the recent trends of developing technologies like augmented reality and increasing contactless services due to COVID-19, a more advanced type of text entry is required. To tackle this issue, we propose Air-Text which is an intuitive system to write in the air using fingertips as a pen. Unlike previously suggested air-writing systems, Air-Text provides various functionalities by the seamless integration of air-writing and text-recognition modules. Specifically, the air-writing module takes a sequence of RGB images as input and tracks both the location of fingertips (5.33 pixel error in 640x480 image) and current hand gesture class (98.29% classification accuracy) frame by frame. Users can easily perform writing operations such as writing or deleting a text by changing hand gestures, and tracked fingertip locations can be stored as a binary image. Then the text-recognition module, which is compatible with any pre-trained recognition models, predicts a written text in the binary image. In this paper, examples of single digit recognition with MNIST classifier (96.0% accuracy) and word-level recognition with text recognition model (79.36% character recognition rate) are provided.
SP  - 1267
EP  - 1274
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3475694
ER  - 

TY  - JOUR
AU  - Elsayed, Hesham; Weigel, Martin; Müller, Florian; Schmitz, Martin; Marky, Karola; Günther, Sebastian; Riemann, Jan; Mühlhäuser, Max
TI  - VibroMap: Understanding the Spacing of Vibrotactile Actuators across the Body
PY  - 2020
AB  - In spite of the great potential of on-body vibrotactile displays for a variety of applications, research lacks an understanding of the spacing between vibrotactile actuators. Through two experiments, we systematically investigate vibrotactile perception on the wrist, forearm, upper arm, back, torso, thigh, and leg, each in transverse and longitudinal body orientation. In the first experiment, we address the maximum distance between vibration motors that still preserves the ability to generate phantom sensations. In the second experiment, we investigate the perceptual accuracy of localizing vibrations in order to establish the minimum distance between vibration motors. Based on the results, we derive VibroMap, a spatial map of the functional range of inter-motor distances across the body. VibroMap supports hardware and interaction designers with design guidelines for constructing body-worn vibrotactile displays.
SP  - 1
EP  - 16
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432189
ER  - 

TY  - NA
AU  - Marwecki, Sebastian; Wilson, Andrew D.; Ofek, Eyal; Franco, Mar Gonzalez; Holz, Christian
TI  - UIST - Mise-Unseen: Using Eye Tracking to Hide Virtual Reality Scene Changes in Plain Sight
PY  - 2019
AB  - Creating or arranging objects at runtime is needed in many virtual reality applications, but such changes are noticed when they occur inside the user's field of view. We present Mise-Unseen, a software system that applies such scene changes covertly inside the user's field of view. Mise-Unseen leverages gaze tracking to create models of user attention, intention, and spatial memory to determine if and when to inject a change. We present seven applications of Mise-Unseen to unnoticeably modify the scene within view (i) to hide that task difficulty is adapted to the user, (ii) to adapt the experience to the user's preferences, (iii) to time the use of low fidelity effects, (iv) to detect user choice for passive haptics even when lacking physical props, (v) to sustain physical locomotion despite a lack of physical space, (vi) to reduce motion sickness during virtual locomotion, and (vii) to verify user understanding during story progression. We evaluated Mise-Unseen and our applications in a user study with 15 participants and find that while gaze data indeed supports obfuscating changes inside the field of view, a change is rendered unnoticeably by using gaze in combination with common masking techniques.
SP  - 777
EP  - 789
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347919
ER  - 

TY  - NA
AU  - Jetter, Hans-Christian; Rädle, Roman; Feuchtner, Tiare; Anthes, Christoph; Friedl, Judith; Klokmose, Clemens Nylandsted
TI  - CHI - "In VR, everything is possible!": Sketching and Simulating Spatially-Aware Interactive Spaces in Virtual Reality
PY  - 2020
AB  - We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with finger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identified the designers' experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based "World Editor".
SP  - 1
EP  - 16
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376652
ER  - 

TY  - NA
AU  - Grønbæk, Jens Emil; Knudsen, Mille Skovhus; O'Hara, Kenton; Krogh, Peter Gall; Vermeulen, Jo; Petersen, Marianne Graves
TI  - CHI - Proxemics Beyond Proximity: Designing for Flexible Social Interaction Through Cross-Device Interaction
PY  - 2020
AB  - Cross-device interactions enable ad hoc sharing of content and control in co-located collaboration. Cross-device research often draws from proxemics theory for designing interactions based on detection of spatial relations such as distance and orientation between people and devices. However, detection of human-human or human-device proximity also constrains flexibility in co-located social interaction. We suggest a proxemics-based approach to designing flexible cross-device interactions. From observations in a field study, we articulate how co-located sharing practices are shaped by the interplay between everyday mobile devices and the physical environment. Based on these insights, we present three cross-device prototypes as proofs-of-concept, demonstrating three design sensitivities for considering proxemics beyond proximity; incorporating features in the environment, enabling flexibility in interpersonal distance and orientation, and providing multiple alternative action possibilities. Drawing from characteristics of our prototypes, we discuss concrete proposals for designing cross-device interactions to enable flexible social interaction.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376379
ER  - 

TY  - NA
AU  - Creed, Chris; Frutos-Pascual, Maite; Williams, Ian
TI  - CHI - Multimodal Gaze Interaction for Creative Design
PY  - 2020
AB  - We present a new application ("Sakura") that enables people with physical impairments to produce creative visual design work using a multimodal gaze approach. The system integrates multiple features tailored for gaze interaction including the selection of design artefacts via a novel grid approach, control methods for manipulating canvas objects, creative typography, a new color selection approach, and a customizable guide technique facilitating the alignment of design elements. A user evaluation (N=24) found that non-disabled users were able to utilize the application to complete common design activities and that they rated the system positively in terms of usability. A follow-up study with physically impaired participants (N=6) demonstrated they were able to control the system when working towards a website design, rating the application as having a good level of usability. Our research highlights new directions in making creative activities more accessible for people with physical impairments.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376196
ER  - 

TY  - NA
AU  - Nancel, Mathieu; Vogel, Daniel; Araujo, Bruno; Jota, Ricardo; Casiez, Géry
TI  - UIST - Next-Point Prediction Metrics for Perceived Spatial Errors
PY  - 2016
AB  - Touch screens have a delay between user input and corresponding visual interface feedback, called input 'latency' (or 'lag'). Visual latency is more noticeable during continuous input actions like dragging, so methods to display feedback based on the most likely path for the next few input points have been described in research papers and patents. Designing these 'next-point prediction' methods is challenging, and there have been no standard metrics to compare different approaches. We introduce metrics to quantify the probability of 7 spatial error 'side-effects' caused by next-point prediction methods. Types of side-effects are derived using a thematic analysis of comments gathered in a 12 participants study covering drawing, dragging, and panning tasks using 5 state-of-the-art next-point predictors. Using experiment logs of actual and predicted input points, we develop quantitative metrics that correlate positively with the frequency of perceived side-effects. These metrics enable practitioners to compare next-point predictors using only input logs.
SP  - 271
EP  - 285
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984590
ER  - 

TY  - NA
AU  - Saxena, Ankit
TI  - VIANO-the virtual piano
PY  - 2017
AB  - Image processing is a comprehensive term for delineating and scrutinizing data in its very visual form. Virtual reality right since its embryonic stage holds big potential to become the next big computing platform. Images can be wielded to create visual experiences and employ it to solve existing problems. This paper flaunts the fusion of image processing and computer vision by proposing a method to create a prototype of virtual piano that would allow user to play actual piano by just touching its image or projection, showcasing virtual reality. This in turn would help to eradicate the problem of portability of bulky pianos by replacing them with just an ‘image’, thus making it cost efficient. This whole process is subsumed in stages of extracting piano from a real world image, finding the particular key user touch and then generating corresponding key's sound. This paper also explains the future possibilities of making touch or pressure sensitive piano by using depth sensing techniques. Thus, this paper would open the gates for exploring other possibilities for blending these two techniques to solve other prevailing problems and making life of people easier.
SP  - 1
EP  - 4
JF  - 2017 3rd International Conference on Computational Intelligence & Communication Technology (CICT)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ciact.2017.7977332
ER  - 

TY  - NA
AU  - Ko, Donghyeon; Kim, Myeongseong; Lee, Woohun
TI  - Thermoformable Shell for Repeatable Thermoforming
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558632
ER  - 

TY  - NA
AU  - Gruen, Robert; Ofek, Eyal; Steed, Anthony; Gal, Ran; Sinclair, Mike; Gonzalez-Franco, Mar
TI  - VR - Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices
PY  - 2020
AB  - Measuring Visual Latency in VR and AR devices has become increasingly complicated as many of the components will influence others in multiple loops and ultimately affect the human cognitive and sensory perception. In this paper we present a new method based on the idea that the performance of humans on a rapid motor task will remain constant, and that any added delay will correspond to the system latency. We ask users to perform a task inside different video see-through devices and also in front of a computer. We also calculate the latency of the systems using a hardware instrumentation-based measurement technique for bench-marking. Results show that this new form of latency measurement through human cognitive performance can be reliable and comparable to hardware instrumentation-based measurement. Our method is adaptable to many forms of user interaction. It is particularly suitable for systems, such as AR and VR, where externalizing signals is difficult, or where it is important to measure latency while the system is in use by a user.
SP  - 791
EP  - 799
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1580498468656
ER  - 

TY  - BOOK
AU  - Pfeuffer, Ken; Alexander, Jason; Gellersen, Hans
TI  - ETRA Short Papers - Multi-user Gaze-based Interaction Techniques on Collaborative Touchscreens
PY  - 2021
AB  - Eye-gaze is a technology for implicit, fast, and hands-free input for a variety of use cases, with the majority of techniques focusing on single-user contexts. In this work, we present an exploration into gaze techniques of users interacting together on the same surface. We explore interaction concepts that exploit two states in an interactive system: 1) users visually attending to the same object in the UI, or 2) users focusing on separate targets. Interfaces can exploit these states with increasing availability of eye-tracking. For example, to dynamically personalise content on the UI to each user, and to provide a merged or compromised view on an object when both users’ gaze are falling upon it. These concepts are explored with a prototype horizontal interface that tracks gaze of two users facing each other. We build three applications that illustrate different mappings of gaze to multi-user support: an indoor map with gaze-highlighted information, an interactive tree-of-life visualisation that dynamically expands on users’ gaze, and a worldmap application with gaze-aware fisheye zooming. We conclude with insights from a public deployment of this system, pointing toward the engaging and seamless ways how eye based input integrates into collaborative interaction.
SP  - 1
EP  - 7
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448018.3458016
ER  - 

TY  - NA
AU  - Sidenmark, Ludwig; Gellersen, Hans
TI  - UIST - Eye&Head: Synergetic Eye and Head Movement for Gaze Pointing and Selection
PY  - 2019
AB  - Eye gaze involves the coordination of eye and head movement to acquire gaze targets, but existing approaches to gaze pointing are based on eye-tracking in abstraction from head motion. We propose to leverage the synergetic movement of eye and head, and identify design principles for Eye&Head gaze interaction. We introduce three novel techniques that build on the distinction of head-supported versus eyes-only gaze, to enable dynamic coupling of gaze and pointer, hover interaction, visual exploration around pre-selections, and iterative and fast confirmation of targets. We demonstrate Eye&Head interaction on applications in virtual reality, and evaluate our techniques against baselines in pointing and confirmation studies. Our results show that Eye&Head techniques enable novel gaze behaviours that provide users with more control and flexibility in fast gaze pointing and selection.
SP  - 1161
EP  - 1174
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347921
ER  - 

TY  - NA
AU  - Zhang, Mingrui Ray; Zhai, Shumin; Wobbrock, Jacob O.
TI  - TypeAnywhere: A QWERTY-Based Text Entry Solution for Ubiquitous Computing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517686
ER  - 

TY  - JOUR
AU  - Durnez, Wouter; Zheleva, Aleksandra; Claypool, Mark; Maes, Mathias; Bombeke, Klaas; Van Looy, Jan; De Marez, Lieven
TI  - <i>Spaz!</i> The Effects of Local Latency on Player Actions in a Desktop-Based Exergame
PY  - 2022
AB  - NA
SP  - 623
EP  - 631
JF  - IEEE Transactions on Games
VL  - 14
IS  - 4
PB  - 
DO  - 10.1109/tg.2021.3128714
ER  - 

TY  - NA
AU  - Dudley, John; Benko, Hrvoje; Wigdor, Daniel; Kristensson, Per Ola
TI  - ISMAR - Performance Envelopes of Virtual Keyboard Text Input Strategies in Virtual Reality
PY  - 2019
AB  - Virtual and Augmented Reality deliver engaging interaction experiences that can transport and extend the capabilities of the user. To ensure these paradigms are more broadly usable and effective, however, it is necessary to also deliver many of the conventional functions of a smartphone or personal computer. It remains unclear how conventional input tasks, such as text entry, can best be translated into virtual and augmented reality. In this paper we examine the performance potential of four alternative text entry strategies in virtual reality (VR). These four strategies are selected to provide full coverage of two fundamental design dimensions: i) physical surface association; and ii) number of engaged fingers. Specifically, we examine typing with index fingers on a surface and in mid-air and typing using all ten fingers on a surface and in mid-air. The central objective is to evaluate the human performance potential of these four typing strategies without being constrained by current tracking and statistical text decoding limitations. To this end we introduce an auto-correction simulator that uses knowledge of the stimulus to emulate statistical text decoding within constrained experimental parameters and use high-precision motion tracking hardware to visualise and detect fingertip interactions. We find that alignment of the virtual keyboard with a physical surface delivers significantly faster entry rates over a mid-air keyboard. Also, users overwhelmingly fail to effectively engage all ten fingers in mid-air typing, resulting in slower entry rates and higher error rates compared to just using two index fingers. In addition to identifying the envelopes of human performance for the four strategies investigated, we also provide a detailed analysis of the underlying features that distinguish each strategy in terms of its performance and behaviour.
SP  - 289
EP  - 300
JF  - 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar.2019.00027
ER  - 

TY  - NA
AU  - Maman, Ben; Bermano, Amit
TI  - TypeNet: Towards Camera Enabled Touch Typing on Flat Surfaces through Self-Refinement
PY  - 2022
AB  - Text entry for mobile devices nowadays is an equally crucial and time-consuming task, with no practical solution available for natural typing speeds without extra hardware. In this paper, we introduce a real-time method that is a significant step towards enabling touch typing on arbitrary flat surfaces (e.g., tables). The method employs only a simple video camera, placed in front of the user on the flat surface — at an angle practical for mobile usage. To achieve this, we adopt a classification framework, based on the observation that, in touch typing, similar hand configurations imply the same typed character across users. Importantly, this approach allows the convenience of un-calibrated typing, where the hand positions, with respect to the camera and each other, are not dictated.To improve accuracy, we propose a Language Processing scheme, which corrects the typed text and is specifically designed for real-time performance and integration with the vision-based signal. To enable feasible data collection and training, we propose a self-refinement approach that allows training on unlabeled flat-surface-typing footage; A network trained on (labeled) keyboard footage labels flat-surface videos using dynamic time warping, and is trained on them, in an Expectation Maximization (EM) manner.Using these techniques, we introduce the TypingHands26 Dataset, comprising videos of 26 different users typing on a keyboard, and 10 users typing on a flat surface, labeled at the frame level. We validate our approach and present a single camera-based system with character-level accuracy of 93.5% on average for known users, and 85.7% for unknown ones, outperforming pose-estimation-based methods by a large margin, despite performing at natural typing speeds of up to 80 Words Per Minute. Our method is the first to rely on a simple camera alone, and runs in interactive speeds, while still maintaining accuracy comparable to systems employing non-commodity equipment.
SP  - NA
EP  - NA
JF  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wacv51458.2022.00064
ER  - 

TY  - NA
AU  - Esakia, Andrey
TI  - Development and Exploratory Findings of a Smartwatch Interface to Facilitate Group Cohesion in a Statewide Health Promotion Program
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhu, Fengyuan; Grossman, Tovi
TI  - CHI - BISHARE: Exploring Bidirectional Interactions Between Smartphones and Head-Mounted Augmented Reality
PY  - 2020
AB  - In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376233
ER  - 

TY  - NA
AU  - Hu, Jinghui; Dudley, John J.; Kristensson, Per Ola
TI  - An Evaluation of Caret Navigation Methods for Text Editing in Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00132
ER  - 

TY  - NA
AU  - Chao, Beibei; Zhao, Xiaoyan; Shi, Dapeng; Feng, Guihuan; Luo, Bin
TI  - IUI - Eyes Understand the Sketch!: Gaze-Aided Stroke Grouping of Hand-Drawn Flowcharts
PY  - 2017
AB  - Stroke grouping in sketch recognition is both difficult and time-consuming. Our preliminary experiment indicates that, when people drawing flowcharts, their gaze focused on non-arrow areas, which providing a spatial cue for stroke grouping. Therefore, we present a novel stroke grouping method aided by gaze information. Based on gaze data that is collected simultaneously during natural drawing process, we generate hotspot areas serving as the position reference of semantic symbols. Strokes are first roughly grouped by the hotspot areas, so as to efficiently decrease the searching space. Experiment on a dataset of 54 flowcharts shows that time efficiency of stroke grouping can be greatly improved in our method and there is much potential for introducing eye-gaze data in sketch recognition.
SP  - 79
EP  - 83
JF  - Proceedings of the 22nd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025171.3025220
ER  - 

TY  - NA
AU  - Muthukumarana, Sachith; Nassani, Alaeddin; Park, Noel; Steimle, Jurgen; Billinghurst, Mark; Nanayakkara, Suranga
TI  - XRtic: A Prototyping Toolkit for XR Applications using Cloth Deformation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00071
ER  - 

TY  - JOUR
AU  - Morimoto, Carlos H.; Coutinho, Flávio Luiz; Hansen, Dan Witzner
TI  - Screen-Light Decomposition Framework for Point-of-Gaze Estimation Using a Single Uncalibrated Camera and Multiple Light Sources
PY  - 2020
AB  - The use of a single uncalibrated camera is desirable for eye tracking to reduce the overall complexity and cost of the system. Quite often, at least one external light source is used to enhance image quality and generate a corneal reflection used as a reference point to estimate the point-of-gaze (PoG). Though the use of more than one light source has shown to enhance accuracy and robustness to head motion, it is unlikely that all corneal reflections appear in the eye images during natural eye movements. In this paper, we introduce the Screen-Light Decomposition (SLD) framework as a generalized model for PoG estimation using a single uncalibrated camera and a variable number of light sources. SLD synthesizes existing uncalibrated video-based eye trackers and can be used as a modeling tool to compare and design eye trackers. We have used the framework to design a novel eye-tracking technique, called SAGE, for single normalized space adaptive gaze estimation, that can gracefully degrade the gaze tracker performance when one or more corneal reflections are not detected, even during the calibration procedure. Results from an user experiment are presented to demonstrate its improved performance over other designs.
SP  - 585
EP  - 605
JF  - Journal of Mathematical Imaging and Vision
VL  - 62
IS  - 4
PB  - 
DO  - 10.1007/s10851-020-00947-8
ER  - 

TY  - NA
AU  - Kaviani, Samira; Rahimi, Amir; Hartley, Richard
TI  - Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 2021 Digital Image Computing: Techniques and Applications (DICTA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/dicta52665.2021.9647255
ER  - 

TY  - NA
AU  - Yaghoubisharif, Negin; Getschmann, Christopher; Echtler, Florian
TI  - HeadsUp: Mobile Collision Warnings through Ultrasound Doppler Sensing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3568458
ER  - 

TY  - NA
AU  - Langner, Ricardo; Satkowski, Marc; Büschel, Wolfgang; Dachselt, Raimund
TI  - CHI - MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis
PY  - 2021
AB  - We present Marvis, a conceptual framework that combines mobile devices and head-mounted Augmented Reality (AR) for visual data analysis. We propose novel concepts and techniques addressing visualization-specific challenges. By showing additional 2D and 3D information around and above displays, we extend their limited screen space. AR views between displays as well as linking and brushing are also supported, making relationships between separated visualizations plausible. We introduce the design process and rationale for our techniques. To validate Marvis’ concepts and show their versatility and widespread applicability, we describe six implemented example use cases. Finally, we discuss insights from expert hands-on reviews. As a result, we contribute to a better understanding of how the combination of one or more mobile devices with AR can benefit visual data analysis. By exploring this new type of visualization environment, we hope to provide a foundation and inspiration for future mobile data visualizations.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445593
ER  - 

TY  - NA
AU  - Li, Nianlong; Han, Teng; Tian, Feng; Huang, Jin; Sun, Minghui; Irani, Pourang; Alexander, Jason
TI  - CHI - Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen
PY  - 2020
AB  - The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376698
ER  - 

TY  - JOUR
AU  - Messerschmidt, Moritz Alexander; Muthukumarana, Sachith; Hamdan, Nur Al-Huda; Wagner, Adrian; Zhang, Haimo; Borchers, Jan; Nanayakkara, Suranga Chandima
TI  - ANISMA: A Prototyping Toolkit to Explore Haptic Skin Deformation Applications Using Shape-Memory Alloys
PY  - 2022
AB  - <jats:p>We present ANISMA, a software and hardware toolkit to prototype on-skin haptic devices that generate skin deformation stimuli like pressure, stretch, and motion using shape-memory alloys (SMAs). Our toolkit embeds expert knowledge that makes SMA spring actuators more accessible to human–computer interaction (HCI) researchers. Using our software tool, users can design different actuator layouts, program their spatio-temporal actuation and preview the resulting deformation behavior to verify a design at an early stage. Our toolkit allows exporting the actuator layout and 3D printing it directly on skin adhesive. To test different actuation sequences on the skin, a user can connect the SMA actuators to our customized driver board and reprogram them using our visual programming interface. We report a technical analysis, verify the perceptibility of essential ANISMA skin deformation devices with 8 participants, and evaluate ANISMA regarding its usability and supported creativity with 12 HCI researchers in a creative design task.</jats:p>
SP  - 1
EP  - 34
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3490497
ER  - 

TY  - JOUR
AU  - Leng, Jiaye; Wang, Lili; Liu, Xiaolong; Shi, Xuehuai; Wang, Miao
TI  - Efficient Flower Text Entry in Virtual Reality.
PY  - 2022
AB  - Text entry is a frequently used task in virtual reality (VR) applications, and controller is the most common interactive device in current VR systems. However, in terms of typing speed, there is still a gap between the existing controller-based text entry techniques and using a physical keyboard in reality, so it is important to improve the efficiency of the controller-based text entry. In this paper, we introduce Flower Text Entry, a single-controller text entry method based on a newly designed flower-shaped keyboard using hand 3D translation interaction for letters selection. We conduct user studies to optimize the keyboard design and the mapping between the interaction and selection, so as to evaluate our method. The results show that our method has high typing speed, lower error rate, and is very friendly to novices compared with the state-of-the-art controller-based text entry methods. After a short training, the novice group can type at 17.65 words per minute (WPM), and the potential expert group can type at 22.97 WPM. The highest typing speed is up to 30.80 WPM achieved by a potential expert participant.
SP  - 3662
EP  - 3672
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203101
ER  - 

TY  - NA
AU  - Marquardt, Nicolai; Brudy, Frederik; Liu, Can; Bengler, Ben; Holz, Christian
TI  - CHI - SurfaceConstellations: A Modular Hardware Platform for Ad-Hoc Reconfigurable Cross-Device Workspaces
PY  - 2018
AB  - We contribute SurfaceConstellations, a modular hardware platform for linking multiple mobile devices to easily create novel cross-device workspace environments. Our platform combines the advantages of multi-monitor workspaces and multi-surface environments with the flexibility and extensibility of more recent cross-device setups. The SurfaceConstellations platform includes a comprehensive library of 3D-printed link modules to connect and arrange tablets into new workspaces, several strategies for designing setups, and a visual configuration tool for automatically generating link modules. We contribute a detailed design space of cross-device workspaces, a technique for capacitive links between tablets for automatic recognition of connected devices, designs of flexible joint connections, detailed explanations of the physical design of 3D printed brackets and support structures, and the design of a web-based tool for creating new SurfaceConstellation setups.
SP  - 354
EP  - 366
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173928
ER  - 

TY  - NA
AU  - Gulay, Emrecan; Lucero, Andrés
TI  - CHI - Integrated Workflows: Generating Feedback Between Digital and Physical Realms
PY  - 2019
AB  - As design thinking shifted away from conventional methods with the rapid adoption of computer-aided design and fabrication technologies, architects have been seeking ways to initiate a comprehensive dialogue between the virtual and the material realms. Current methodologies do not offer embodied workflows that utilize the feedback obtained through a subsequent transition process between physical and digital design. Therefore, narrowing the separation between these two platforms remains as a research problem. This literature review elaborates the divide between physical and digital design, testing and manufacturing techniques in the morphological process of architectural form. We first review the digital transformation in the architectural design discourse. Then, we proceed by introducing a variety of methods that are integrating digital and physical workflows and suggesting an alternative approach. Our work unveils that there is a need for empirical research with a focus on integrated approaches to create intuitively embodied experiences for architectural designers.
SP  - 60
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300290
ER  - 

TY  - CHAP
AU  - Norton, Wil J.; Pitura, Philip; Gerhard, David
TI  - Design and Implementation of a Chorded-Keyboard Mapping for Existing VR Hand Controllers
PY  - 2021
AB  - This paper presents a novel controller-centric text input system that allows for chording key selection using existing consumer VR hardware. The design for a layout is discussed, consisting of a one-handed alphabetic keyboard. We argue that the use of existing handheld controllers and the inclusion of a functional one-handed layout would allow for text entry in roomscale or movement-based VR experiences where other keyboard implementations are typically impractical. Mappings of the layout to Oculus and Valve controllers are also presented. We argue that tactile button feedback already present in controllers allows users of the layout, once learned, to touch type without the need for additional hardware, and that virtual key labels affords ease of learning and allows for non-touch typists to use the keyboard for text entry in an efficient manner. Testing and further evaluation of the layout is discussed.
SP  - 443
EP  - 454
JF  - Proceedings of the Future Technologies Conference (FTC) 2021, Volume 3
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-89912-7_34
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Botner, Alexander; Rukzio, Enrico; Gugenheimer, Jan
TI  - Ephemeral Fabrication: Exploring a Ubiquitous Fabrication Scenario of Low-Effort, In-Situ Creation of Short-Lived Physical Artifacts
PY  - 2022
AB  - Personal fabrication empowers users to create objects increasingly easier and faster. This continuous decrease in effort evokes a speculative scenario of Ephemeral Fabrication (EF), enabled and amplified by emerging paradigms of mobile, wearable, or even body-integrated fabrication. EF yields fast, temporary, in-situ solutions for everyday problems (e.g., creating a protective skin, affixing a phone). Users solely create those, since the required effort is negligible. We present and critically reflect on the EF scenario, by exploring current trends in research and building a body-worn fabrication device. EF is a plausible extrapolation of current developments, entailing both positive (e.g., accessibility) and negative implications (e.g., unsustainability). Using speculative design methodology to question the trajectory of personal fabrication, we argue that to avert the aftermath of such futures, topics like sustainability can not remain an afterthought, but rather be situated in interactions themselves: through embedded constraints, conscious material choice, and constructive embedding of ephemerality.
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501331
ER  - 

TY  - JOUR
AU  - Wang, Kang; Feng, Zhiquan; Li, Jian; Han, Rui
TI  - A Structural Design and Interaction Algorithm of Smart Microscope Embedded on Virtual and Real Fusion Technologies
PY  - 2019
AB  - The microscope is an important teaching tool in secondary school biology. This important equipment needs to be understood and mastered through biological experiments. However, in actual experimental teaching in China, microscope samples are difficult to produce, preserve and share, and the implementation status is not ideal. These conditions seriously affect student enthusiasm and require additional participant labor. Moreover, they hinder teachers' evaluation of experimental learning. Thus, this paper constructs a smart microscope physical interaction kit for secondary school biology experiments. First, the main components of the traditional microscope are replaced by a variety of different sensing elements, which are 3D printed. Then, a novel multichannel information integration strategy based on visual, auditory and tactile information is proposed to manage different channels of information, to understand the user's operational intent and to organize reasonable interactions (including the display of real-time adjustment effects). Then, we propose a navigational interaction paradigm based on multimodal intent understanding, aiming at reminding users of invalid behavior and providing necessary operational guidance for users, thus achieving the purpose of intelligent interaction and intelligent experimental teaching. The experimental results show that the proposed microscope kit, multichannel integration strategy and navigation interaction algorithm not only imbue microscopy experiments with the characteristics of intelligent interaction but also stimulate student enthusiasm and help the evaluation of experimental learning. We find that these capabilities are well received by users.
SP  - 152088
EP  - 152102
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2945330
ER  - 

TY  - NA
AU  - Gongora, Daniel; Nagano, Hikaru; Konyo, Masashi; Tadokoro, Satoshi
TI  - WHC - Vibrotactile rendering of camera motion for bimanual experience of first-person view videos
PY  - 2017
AB  - We propose a vibrotactile rendering method for the motion of a camera in first-person view videos that enables people to feel the movement of the camera with both hands. Concretely, we consider an arrangement of two vibrotactile actuators to render panning movements on the horizontal axis as vibrations that move from hand to hand, and to represent sudden vertical displacements of the camera as transient vibrations on both hands. We investigate three representation methods for the panning motion based on the estimated velocity and acceleration of the camera and a combination of both. In a preliminary user experiment, we observed favorable effects of applying our proposed rendering method on the perceived realism and satisfaction associated with the experience of watching a video.
SP  - 454
EP  - 459
JF  - 2017 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2017.7989944
ER  - 

TY  - JOUR
AU  - Wang, Zhisheng; Liu, Liang; Cui, Junning; Zhao, Dongfang; Li, Muhang
TI  - MilliLoc: Human computer interaction oriented acoustic millimeter-level real-time locating system
PY  - 2023
AB  - <jats:p> In recent years, people have become more interested in using acoustics to locate equipment in intelligent human–computer interfaces (HCI). However, existing acoustic locating systems have poor compatibility with smart devices. To achieve millimeter-level accuracy, most systems require large microphone spacing, complex algorithms, and a high cost. In this paper, we propose a new HCI system, MilliLoc, to achieve millimeter-level real-time acoustic locating. Our first contribution is to design a high-performance and convenient microphone array. It has a high sampling rate of 96 kHz, real-time complete dataflow, and is compatible with any smart device. The second contribution is to propose an improved generalized cross-correlation phase transform method with simplicity and real-time capability. Even if the signal-to-noise ratio is not high enough, it can still estimate the time delay accurately. We combine the microphone array with the algorithm to illustrate the implementation of MilliLoc. Experiments show that MilliLoc has a median 2D accuracy of 7.2 mm in a 60 cm sector area and achieves millimeter-level locating. </jats:p>
SP  - 15113
EP  - NA
JF  - Review of Scientific Instruments
VL  - 94
IS  - 1
PB  - 
DO  - 10.1063/5.0098220
ER  - 

TY  - NA
AU  - Degraen, Donald; Fruchard, Bruno; Smolders, Frederik; Potetsianakis, Emmanouil; Güngör, Seref; Krüger, Antonio; Steimle, Jürgen
TI  - UIST - Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization
PY  - 2021
AB  - Effective haptic feedback in virtual reality (VR) is an essential element for creating convincing immersive experiences. To design such feedback, state-of-the-art VR setups provide APIs for programmatically generating controller vibration patterns. While tools for designing vibrotactile feedback keep evolving, they often require expert knowledge and rarely support direct manipulation methods for mapping feedback to user interactions within the VR environment. To address these challenges, we contribute a novel concept called Weirding Haptics, that supports fast-prototyping by leveraging the user’s voice to design such feedback while manipulating virtual objects in-situ. Through a pilot study (N = 9) focusing on how tactile experiences are vocalized during object manipulation, we identify spatio-temporal mappings and supporting features needed to produce intended vocalizations. To study our concept, we built a VR design tool informed by the results of the pilot study. This tool enables users to design tactile experiences using their voice while manipulating objects, provides a set of modifiers for fine-tuning the created experiences in VR, and allows to rapidly compare various experiences by feeling them. Results from a validation study (N = 8) show that novice hapticians can vocalize experiences and refine their designs with the fine-tuning modifiers to match their intentions. We conclude our work by discussing uncovered design implications for direct manipulation and vocalization of vibrotactile feedback in immersive virtual environments.
SP  - 936
EP  - 953
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474797
ER  - 

TY  - NA
AU  - Shen, Vivian; Harrison, Chris
TI  - Pull Gestures with Coordinated Graphics on Dual-Screen Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556620
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Alexander, Jason; Gellersen, Hans
TI  - CHI - Partially-indirect Bimanual Input with Gaze, Pen, and Touch for Pan, Zoom, and Ink Interaction
PY  - 2016
AB  - Bimanual pen and touch UIs are mainly based on the direct manipulation paradigm. Alternatively we propose partially-indirect bimanual input, where direct pen input is used with the dominant hand, and indirect-touch input with the non-dominant hand. As direct and indirect inputs do not overlap, users can interact in the same space without interference. We investigate two indirect-touch techniques combined with direct pen input: the first redirects touches to the user's gaze position, and the second redirects touches to the pen position. In this paper, we present an empirical user study where we compare both partially-indirect techniques to direct pen and touch input in bimanual pan, zoom, and ink tasks. Our experimental results show that users are comparatively fast with the indirect techniques, but more accurate as users can dynamically change the zoom-target during indirect zoom gestures. Further our studies reveal that direct and indirect zoom gestures have distinct characteristics regarding spatial use, gestural use, and bimanual parallelism.
SP  - 2845
EP  - 2856
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858201
ER  - 

TY  - NA
AU  - He, Zhenyi; Lutteroth, Christof; Perlin, Ken
TI  - TapGazer: Text Entry with Finger Tapping and Gaze-directed Word Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501838
ER  - 

TY  - JOUR
AU  - Lystbæk, Mathias N.; Pfeuffer, Ken; Grønbæk, Jens Emil Sloth; Gellersen, Hans
TI  - Exploring Gaze for Assisting Freehand Selection-based Text Entry in AR
PY  - 2022
AB  - <jats:p>With eye-tracking increasingly available in Augmented Reality, we explore how gaze can be used to assist freehand gestural text entry. Here the eyes are often coordinated with manual input across the spatial positions of the keys. Inspired by this, we investigate gaze-assisted selection-based text entry through the concept of spatial alignment of both modalities. Users can enter text by aligning both gaze and manual pointer at each key, as a novel alternative to existing dwell-time or explicit manual triggers. We present a text entry user study comparing two of such alignment techniques to a gaze-only and a manual-only baseline. The results show that one alignment technique reduces physical finger movement by more than half compared to standard in-air finger typing, and is faster and exhibits less perceived eye fatigue than an eyes-only dwell-time technique. We discuss trade-offs between uni and multimodal text entry techniques, pointing to novel ways to integrate eye movements to facilitate virtual text entry.</jats:p>
SP  - 1
EP  - 16
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ETRA
PB  - 
DO  - 10.1145/3530882
ER  - 

TY  - JOUR
AU  - Wang, Gang; Ren, Gang; Hong, Xinye; Peng, Xun; Li, Wenbin; O'Neill, Eamonn
TI  - Freehand Gestural Selection with Haptic Feedback in Wearable Optical See-Through Augmented Reality
PY  - 2022
AB  - <jats:p>Augmented reality (AR) technologies can blend digital and physical space and serve a variety of applications intuitively and effectively. Specifically, wearable AR enabled by optical see-through (OST) AR head-mounted displays (HMDs) might provide users with a direct view of the physical environment containing digital objects. Besides, users could directly interact with three-dimensional (3D) digital artefacts using freehand gestures captured by OST HMD sensors. However, as an emerging user interaction paradigm, freehand interaction with OST AR still requires further investigation to improve user performance and satisfaction. Thus, we conducted two studies to investigate various freehand selection design aspects in OST AR, including target placement, size, distance, position, and haptic feedback on the hand and body. The user evaluation results indicated that 40 cm might be an appropriate target distance for freehand gestural selection. A large target size might lower the selection time and error rate, and a small target size could minimise selection effort. The targets positioned in the centre are the easiest to select, while those in the corners require extra time and effort. Furthermore, we discovered that haptic feedback on the body could lead to high user preference and satisfaction. Based on the research findings, we conclude with design recommendations for effective and comfortable freehand gestural interaction in OST AR.</jats:p>
SP  - 566
EP  - NA
JF  - Information
VL  - 13
IS  - 12
PB  - 
DO  - 10.3390/info13120566
ER  - 

TY  - NA
AU  - Sturdee, Miriam; Alexander, Jason
TI  - Toward a Practice of User-Centred Design for Shape-Changing Interfaces
PY  - 2016
AB  - Shape-changing User Interfaces (UIs) are maturing in application and complexity, and therefore are becoming a tangible reality with regards to commercialization and design in a wider context. Current prototypes support a variety of bespoke interactions and are extensively tested, but applications for these constructs are often limited to the research scenario, and within the limitations of academic institutions. Engaging with a wider audience to develop novel UIs and applications is a valuable addition to the early design process, and can elicit new directions for research. Additionally, focusing on the user fulfils a requirement for developing a User-Centred Design methodology for shape-change, as it presents novel challenges for interaction design. This position paper calls for early adoption of such processes to support the emerging technology of shape-change, and for the formation of a collaborative UCD working group in this field.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Seifi, Hasti; Chun, Matthew; Gallacher, Colin; Schneider, Oliver; MacLean, Karon E.
TI  - How Do Novice Hapticians Design? A Case Study in Creating Haptic Learning Environments
PY  - 2020
AB  - Access to haptic technology is on the rise, in smartphones, virtual reality gear, and open-source education kits. However, engineers and interaction designers are often inexperienced in designing with haptics, and rarely have tools and guidelines for creating multisensory experiences. To examine the impact of this deficit, we supplied a haptic design kit, custom software, and technical support to nine teams (25 students) for an innovation challenge at a major haptics conference. Teams (predominantly undergraduate engineers with little haptics, interaction design, or education training) designed and built haptic environments to support learning of science topics. Qualitative analysis of surveys, interviews, team blogs, and expert assessments of teams’ final demonstrations exposed three themes in these design efforts. 1) Novice teams tended to ignore many of ten design choices that experts navigate, such as explicitly choosing whether haptic and graphic feedback should reinforce versus complement one other. 2) Their design activities differed in timing and inclusion from the ten activities observed in expert process. 3) We identified three success strategies in how teams devised useful and engaging interactions and interpretable multimodal experiences, and communicated about their designs. We compare novice and expert design needs and highlight where future haptic design tools and theory need to support novice practice and training.
SP  - 791
EP  - 805
JF  - IEEE transactions on haptics
VL  - 13
IS  - 4
PB  - 
DO  - 10.1109/toh.2020.2968903
ER  - 

TY  - CONF
AU  - Yamanaka, Shota
TI  - Graphics Interface - Evaluating Temporal Delays and Spatial Gaps in Overshoot-avoiding Mouse-pointing Operations
PY  - 2020
AB  - NA
SP  - 440
EP  - 451
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Antoine, Axel; Malacria, Sylvain; Casiez, Géry
TI  - CHI Extended Abstracts - TurboMouse: End-to-end Latency Compensation in Indirect Interaction
PY  - 2018
AB  - End-to-end latency corresponds to the temporal difference between a user input and the corresponding output from a system. It has been shown to degrade user performance in both direct and indirect interaction. If it can be reduced to some extend, latency can also be compensated through software compensation by trying to predict the future position of the cursor based on previous positions, velocities and accelerations. In this paper, we propose a hybrid hardware and software prediction technique specifically designed for partially compensating end-to-end latency in indirect pointing. We combine a computer mouse with a high frequency accelerometer to predict the future location of the pointer using Euler based equations.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186542
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Chen, Yu; Yang, Yue; Gu, Jianzhe; Tao, Ye; Yao, Lining; Wang, Guanyun
TI  - UIST (Adjunct Volume) - WireTruss: A Fast-Modifiable Prototyping Method Through 3D Printing
PY  - 2020
AB  - Digital fabrication allows users to produce physical objects from digital models. However, conventional fabrication processes are mostly irreversible: once an object is fabricated, it is detached from its original virtual model and cannot be physically changed. In this work, we propose WireTruss, a novel approach for rapid prototyping truss mesh structures, which can be rapidly fabricated, easily assembled and manually modified to support an intuitive design iteration. We developed a parametric design tool that first simplifies the object into a truss graph composed of multi-way joints, and then calculates the route of the wire which is inherently a Euler path. Furthermore, WireTruss can be demonstrated its practical usability through a series of application cases.
SP  - 93
EP  - 95
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416196
ER  - 

TY  - JOUR
AU  - Biener, Verena; Gesslein, Travis; Schneider, Daniel; Kawala, Felix; Otte, Alexander; Kristensson, Per Ola; Pahud, Michel; Ofek, Eyal; Campos, Cuauhtli; Kljun, Matjaz; Pucihar, Klen Copic; Grubert, Jens
TI  - PoVRPoint: Authoring Presentations in Mobile Virtual Reality.
PY  - 2022
AB  - Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint-a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded shapes or objects. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.
SP  - 2069
EP  - 2079
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2022.3150474
ER  - 

TY  - JOUR
AU  - Andoh, Yutaka; Shibata, Hirohito
TI  - Design Suggestions for Reading Support Method Based on Action Analysis
PY  - 2020
AB  - NA
SP  - 831
EP  - 840
JF  - Journal of Japan Society for Fuzzy Theory and Intelligent Informatics
VL  - 32
IS  - 5
PB  - 
DO  - 10.3156/jsoft.32.5_831
ER  - 

TY  - NA
AU  - Torres, César I.; Nicholas, Molly Jane; Lee, Sangyeon; Paulos, Eric
TI  - Tangible and Embedded Interaction - A Conversation with Actuators: An Exploratory Design Environment for Hybrid Materials
PY  - 2019
AB  - An exciting, expanding palette of hybrid materials is emerging that can be programmed to actuate by a range of external and internal stimuli. However, there exists a dichotomy between the physicality of the actuators and the intangible computational signal that is used to program them. For material practitioners, this lack of physical cues limits their ability to engage in a "conversation with materials" (CwM). This paper presents a creative workstation for supporting this epistemological style by bringing a stronger physicality to the computational signal and balance the conversation between physical and digital actors. The station utilizes a streaming architecture to distribute control across multiple devices and leverage the rich spatial cognition that a physical space affords. Through a formal user study, we characterize the actuation design practice supported by the CwM workstation and discuss opportunities for tangible interfaces to hybrid materials.
SP  - 657
EP  - 667
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295643
ER  - 

TY  - NA
AU  - Foy, Conor R.; Dudley, John; Gupta, Aakar; Benko, Hrvoje; Kristensson, Per Ola
TI  - CHI - Understanding, Detecting and Mitigating the Effects of Coactivations in Ten-Finger Mid-Air Typing in Virtual Reality
PY  - 2021
AB  - Typing with ten fingers on a virtual keyboard in virtual or augmented reality exposes a challenging input interpretation problem. There are many sources of noise in this interaction context and these exacerbate the challenge of accurately translating human actions into text. A particularly challenging input noise source arises from the physiology of the hand. Intentional finger movements can produce unintentional coactivations in other fingers. On a physical keyboard, the resistance of the keys alleviates this issue. On a virtual keyboard, coactivations are likely to introduce spurious input events under a naive solution to input detection. In this paper we examine the features that discriminate intentional activations from coactivations. Based on this analysis, we demonstrate three alternative coactivation detection strategies with high discrimination power. Finally, we integrate coactivation detection into a probabilistic decoder and demonstrate its ability to further reduce uncorrected character error rates by approximately 10% relative and 0.9% absolute.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445671
ER  - 

TY  - NA
AU  - Ozgur, Ayberk; Johal, Wafa; Mondada, Francesco; Dillenbourg, Pierre
TI  - CHI - Haptic-Enabled Handheld Mobile Robots: Design and Analysis
PY  - 2017
AB  - The Cellulo robots are small tangible robots that are designed to represent virtual interactive point-like objects that reside on a plane within carefully designed learning activities. In the context of these activities, our robots not only display autonomous motion and act as tangible interfaces, but are also usable as haptic devices in order to exploit, for instance, kinesthetic learning. In this article, we present the design and analysis of the haptic interaction module of the Cellulo robots. We first detail our hardware and controller design that is low-cost and versatile. Then, we describe the task-based experimental procedure to evaluate the robot's haptic abilities. We show that our robot is usable in most of the tested tasks and extract perceptive and manipulative guidelines for the design of haptic elements to be integrated in future learning activities. We conclude with limitations of the system and future work.
SP  - 2449
EP  - 2461
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025994
ER  - 

TY  - NA
AU  - Stylianidis, Alexandros; Vermeulen, Jo; Houben, Steven; MacDonald, Lindsay; Beale, Russell
TI  - CHI Extended Abstracts - SenseBelt: A Belt-Worn Sensor to Support Cross-Device Interaction
PY  - 2017
AB  - Mobile interaction is shifting from a single device to simultaneous interaction with ensembles of devices such as phones, tablets, or watches. Spatially-aware cross-device interaction between mobile devices typically requires a fixed tracking infrastructure, which limits mobility. In this paper, we present SenseBelt -- a sensing belt that enhances existing mobile interactions and enables low-cost, ad hoc sensing of cross-device gestures and interactions. SenseBelt enables proxemic interactions between people and their personal devices. SenseBelt also supports cross-device interaction between personal devices and stationary devices, such as public displays. We discuss the design and implementation of SenseBelt together with possible applications. With an initial evaluation, we provide insights into the benefits and drawbacks of a belt-worn mediating sensor to support cross-device interactions.
SP  - 2123
EP  - 2131
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053135
ER  - 

TY  - NA
AU  - Wimmer, Raphael; Schmid, Andreas; Bockes, Florian
TI  - CHI - On the Latency of USB-Connected Input Devices
PY  - 2019
AB  - We propose a method for accurately and precisely measuring the intrinsic latency of input devices and document measurements for 36 keyboards, mice and gamepads connected via USB. Our research shows that devices differ not only in average latency, but also in the distribution of their latencies, and that forced polling at 1000 Hz decreases latency for some but not all devices. Existing practices - measuring end-to-end latency as a proxy of input latency and reporting only mean values and standard deviations - hide these characteristic latency distributions caused by device intrinsics and polling rates. A probabilistic model of input device latency demonstrates these issues and matches our measurements. Thus, our work offers guidance for researchers, engineers, and hobbyists who want to measure the latency of input devices or select devices with low latency.
SP  - 420
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300650
ER  - 

TY  - NA
AU  - Wittchen, Dennis; Spiel, Katta; Fruchard, Bruno; Degraen, Donald; Schneider, Oliver; Freitag, Georg; Strohmeier, Paul
TI  - TactJam: An End-to-End Prototyping Suite for Collaborative Design of On-Body Vibrotactile Feedback
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501307
ER  - 

TY  - JOUR
AU  - Xohua-Chacón, Antonio; Benítez-Guerrero, Edgard; Muñoz-Arteaga, Jaime; Mezura-Godoy, Carmen
TI  - Using a tangible system to promote inclusive, collaborative activities to learn relational algebra for students with hearing impairment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Universal Access in the Information Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10209-022-00891-x
ER  - 

TY  - NA
AU  - Das, Sauvik; Lu, David; Lee, Taehoon; Lo, Joanne; Hong, Jason
TI  - UIST - The Memory Palace: Exploring Visual-Spatial Paths for Strong, Memorable, Infrequent Authentication
PY  - 2019
AB  - Many accounts and devices require only infrequent authentication by an individual, and thus authentication secrets should be both secure and memorable without much reinforcement. Inspired by people's strong visual-spatial memory, we introduce a novel system to help address this problem: the Memory Palace. The Memory Palace encodes authentication secrets as paths through a 3D virtual labyrinth navigated in the first-person perspective. We ran two experiments to iteratively design and evaluate the Memory Palace. In the first, we found that visual-spatial secrets are most memorable if navigated in a 3D first-person perspective. In the second, we comparatively evaluated the Memory Palace against Android's 9-dot pattern lock along three dimensions: memorability after one week, resilience to shoulder surfing, and speed. We found that relative to 9-dot, complexity-controlled secrets in the Memory Palace were significantly more memorable after one week, were much harder to break through shoulder surfing, and were not significantly slower to enter.
SP  - 1109
EP  - 1121
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347917
ER  - 

TY  - BOOK
AU  - Fang, Xinrui; Xia, Chengshuo; Sugiura, Yuta
TI  - AsianCHI@CHI - FacialPen: Using Facial Detection to Augment Pen-Based Interaction
PY  - 2021
AB  - Pen-based interactions have been ubiquitously adopted on mobile and stationary devices, but the usability can be further augmented through the use of advanced techniques. In this work, we propose FacialPen, a prototype that uses facial gestures to trigger commands for pen-based manipulation. In our prototype, a fisheye camera is mounted to the end of a stylus that provides a broad view from which to capture the human face. We facilitated an elicitation study to identify natural and user-defined gestures for interactions with facial expressions. Different gestures can be further discerned via face detection and a classification pipeline. We designed a sketching demonstration application to explore usage scenarios and evaluated the effectiveness of FacialPen through a qualitative study. The user study posits that FacialPen supports efficiency by reducing screen widgets, enabling the continuity of creation work and liberating the user’s stylus holding postures when switching sketch functions.
SP  - 1
EP  - 8
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3467672
ER  - 

TY  - NA
AU  - Deber, Jonathan; Araujo, Bruno; Jota, Ricardo; Forlines, Clifton; Leigh, Darren; Sanders, Steven Leonard; Wigdor, Daniel
TI  - CHI - Hammer Time!: A Low-Cost, High Precision, High Accuracy Tool to Measure the Latency of Touchscreen Devices
PY  - 2016
AB  - We report on the Latency Hammer, a low-cost yet highaccuracy and high-precision automated tool that measures the interface latency of touchscreen devices. The Hammer directly measures latency by triggering a capacitive touch event on a device using an electrically actuated touch simulator, and a photo sensor to monitor the screen for a visual response. This allows us to measure the full end-toend latency of a touchscreen system exactly as it would be experienced by a user. The Hammer does not require human interaction to perform a measurement, enabling the acquisition of large datasets. We present the operating principles of the Hammer, and discuss its design and construction; full design documents are available online. We also present a series of tools and equipment that were built to assess and validate the performance of the Hammer, and demonstrate that it provides reliable latency measurements.
SP  - 2857
EP  - 2868
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858394
ER  - 

TY  - NA
AU  - Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers
PY  - 2020
AB  - Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Streli, Paul; Jiang, Jiaxi; Fender, Andreas Rene; Meier, Manuel; Romat, Hugo; Holz, Christian
TI  - Demonstrating TapType for mobile ten-finger text entry anywhere
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519900
ER  - 

TY  - JOUR
AU  - Martens, Judith; Franke, Thomas; Rauh, Nadine; Krems, Josef F.
TI  - Effects of low-range latency on performance and perception in a virtual, unstable second-order control task
PY  - 2018
AB  - System latency (i.e., time between user action and system response) has known detrimental effects, particularly in the increasingly prevalent complex human–computer interaction types (e.g., higher control order like in games or teleoperation). The objective of the present research was to examine the impact of low-range latencies on behaviour (performance, control) and perception (perceived control difficulty, latency) in an unstable system with second-order control. Furthermore, the influence of the controller gain (affecting the system’s sensitivity to user input) on latency effects was investigated. The study extends existing research through examining multiple parameters of human–computer interaction in relation to differing levels of low-range latencies (14–198 ms). For this aim, participants across two experiments performed a second-order control task consisting of balancing a ball on a beam using a low-latency computer system with varying levels of added latency. Latency affected performance even with the smallest added latency of 14 ms (d = 0.61). A larger controller gain increased latency effects on control indicators (all d ≥ 0.68) and perceived control difficulty (d = 0.85). Latency impacted performance, control parameters and perceived control difficulty prior to participants’ awareness of the latency. Thus, optimizing latency can be important regardless of whether the user perceives the latency. The diverse effects regarding the different parameters emphasize the usefulness of a comprehensive latency effect assessment and indicate human adaptation to latency.
SP  - 1
EP  - 17
JF  - Quality and User Experience
VL  - 3
IS  - 1
PB  - 
DO  - 10.1007/s41233-018-0023-z
ER  - 

TY  - NA
AU  - Letter, Maximilian; Wolf, Katrin
TI  - Tangible Version Control: Exploring a Physical Object's Alternative Versions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519686
ER  - 

TY  - NA
AU  - Chen, Chen; Yarmand, Matin; Xu, Zhuoqun; Singh, Varun; Zhang, Yang; Weibel, Nadir
TI  - Investigating Input Modality and Task Geometry on Precision-first 3D Drawing in Virtual Reality
PY  - 2022
AB  - Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is indispensable for many precise 3D tasks. However, due to lack of physical support, limited depth perception, and the non-planar nature of 3D curves, it is challenging to adjust mid-air strokes to achieve high precision. Instead of creating new interaction techniques, we investigated how task geometric shapes and input modalities affect precision-first drawing performance in a within-subject study (n = 12) focusing on 3D target tracing in commercially available VR headsets. We found that compared to using bare hands, VR controllers and pens yield nearly 30% of precision gain, and that the tasks with large curvature, forward-backward or left-right orientations perform best. We finally discuss opportunities for designing novel interaction techniques for precise 3D drawing. We believe that our work will benefit future research aiming to create usable toolboxes for precise 3D drawing.
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00054
ER  - 

TY  - NA
AU  - Wang, Guanyun; Yang, Humphrey; Yan, Zeyu; Ulu, Nurcan Gecer; Tao, Ye; Gu, Jianzhe; Kara, Levent Burak; Yao, Lining
TI  - UIST - 4DMesh: 4D Printing Morphing Non-Developable Mesh Surfaces
PY  - 2018
AB  - We present 4DMesh, a method of combining shrinking and bending thermoplastic actuators with customized geometric algorithms to 4D print and morph centimeter- to meter-sized functional non-developable surfaces. We will share two end-to-end inverse design algorithms. With our tools, users can input CAD models of target surfaces and produce respective printable files. The flat sheet printed can morph into target surfaces when triggered by heat. This system saves shipping and packaging costs, in addition to enabling customizability for the design of relatively large non-developable structures. We designed a few functional artifacts to leverage the advantage of non-developable surfaces for their unique functionalities in aesthetics, mechanical strength, geometric ergonomics and other functionalities. In addition, we demonstrated how this technique can potentially be adapted to customize molds for industrial parts (e.g., car, boat, etc.) in the future.
SP  - 623
EP  - 635
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242625
ER  - 

TY  - NA
AU  - Yan, Yihui; Huang, Zezhe; Xudu, Feiyang; Yang, Zhice
TI  - Enabling Tangible Interaction on Non-touch Displays with Optical Mouse Sensor and Visible Light Communication
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517666
ER  - 

TY  - BOOK
AU  - Elmadjian, Carlos; Shukla, Pushkar; Tula, Antonio Diaz; Morimoto, Carlos H.
TI  - COGAIN@ETRA - 3D gaze estimation in the scene volume with a head-mounted eye tracker
PY  - 2018
AB  - Most applications involving gaze-based interaction are supported by estimation techniques that find a mapping between gaze data and corresponding targets on a 2D surface. However, in Virtual and Augmented Reality (AR) environments, interaction occurs mostly in a volumetric space, which poses a challenge to such techniques. Accurate point-of-regard (PoR) estimation, in particular, is of great importance to AR applications, since most known setups are prone to parallax error and target ambiguity. In this work, we expose the limitations of widely used techniques for PoR estimation in 3D and propose a new calibration procedure using an uncalibrated head-mounted binocular eye tracker coupled with an RGB-D camera to track 3D gaze within the scene volume. We conducted a study to evaluate our setup with real-world data using a geometric and an appearance-based method. Our results show that accurate estimation in this setting still is a challenge, though some gaze-based interaction techniques in 3D should be possible.
SP  - 3
EP  - NA
JF  - Proceedings of the Workshop on Communication by Gaze Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206343.3206351
ER  - 

TY  - NA
AU  - Foley, Margaret Jean; Roy, Quentin; Huang, Da-Yuan; Li, Wei; Vogel, Daniel
TI  - Switching Between Standard Pointing Methods with Current and Emerging Computer Form Factors
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517433
ER  - 

TY  - NA
AU  - Singhal, Yatharth; Noeske, Richard Huynh; Bhardwaj, Ayush; Kim, Jin Ryong
TI  - Improving Finger Stroke Recognition Rate for Eyes-Free Mid-Air Typing in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502100
ER  - 

TY  - JOUR
AU  - Van Laerhoven, Kristof
TI  - The Three A’s of Wearable and Ubiquitous Computing: Activity, Affect, and Attention
PY  - 2021
AB  - NA
SP  - 691622
EP  - NA
JF  - Frontiers in Computer Science
VL  - 3
IS  - NA
PB  - 
DO  - 10.3389/fcomp.2021.691622
ER  - 

TY  - JOUR
AU  - Gil, Hyunjae; Oakley, Ian
TI  - ThumbAir
PY  - 2022
AB  - <jats:p>Typing while wearing a standalone Head Mounted Display (HMD)---systems without external input devices or sensors to support text entry---is hard. To address this issue, prior work has used external trackers to monitor finger movements to support in-air typing on virtual keyboards. While performance has been promising, current systems are practically infeasible: finger movements may be visually occluded from inside-out HMD based tracking systems or, otherwise, awkward and uncomfortable to perform. To address these issues, this paper explores an alternative approach. Taking inspiration from the prevalence of thumb-typing on mobile phones, we describe four studies exploring, defining and validating the performance of ThumbAir, an in-air thumb-typing system implemented on a commercial HMD. The first study explores viable target locations, ultimately recommending eight targets sites. The second study collects performance data for taps on pairs of these targets to both inform the design of a target selection procedure and also support a computational design process to select a keyboard layout. The final two studies validate the selected keyboard layout in word repetition and phrase entry tasks, ultimately achieving final WPMs of 27.1 and 13.73. Qualitative data captured in the final study indicate that the discreet movements required to operate ThumbAir, in comparison to the larger scale finger and hand motions used in a baseline design from prior work, lead to reduced levels of perceived exertion and physical demand and are rated as acceptable for use in a wider range of social situations.</jats:p>
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569474
ER  - 

TY  - JOUR
AU  - Xia, Haijun; Glueck, Michael; Annett, Michelle; Wang, Michael; Wigdor, Daniel
TI  - Iteratively Designing Gesture Vocabularies: A Survey and Analysis of Best Practices in the HCI Literature
PY  - 2022
AB  - <jats:p>Gestural interaction has evolved from a set of novel interaction techniques developed in research labs, to a dominant interaction modality used by millions of users everyday. Despite its widespread adoption, the design of appropriate gesture vocabularies remains a challenging task for developers and designers. Existing research has largely used Expert-Led, User-Led, or Computationally-Based methodologies to design gesture vocabularies. These methodologies leverage the expertise, experience, and capabilities of experts, users, and systems to fulfill different requirements. In practice, however, none of these methodologies provide designers with a complete, multi-faceted perspective of the many factors that influence the design of gesture vocabularies, largely because a singular set of factors has yet to be established. Additionally, these methodologies do not identify or emphasize the subset of factors that are crucial to consider when designing for a given use case. Therefore, this work reports on the findings from an exhaustive literature review that identified 13 factors crucial to gesture vocabulary design and examines the evaluation methods and interaction techniques commonly associated with each factor. The identified factors also enable a holistic examination of existing gesture design methodologies from a factor-oriented viewpoint and highlighting the strengths and weaknesses of each methodology. This work closes with proposals of future research directions of developing an iterative user-centered and factor-centric gesture design approach as well as establishing an evolving ecosystem of factors that are crucial to gesture design.</jats:p>
SP  - 1
EP  - 54
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 4
PB  - 
DO  - 10.1145/3503537
ER  - 

TY  - NA
AU  - Walker, James Faure; Li, Bochao; Vertanen, Keith; Kuhl, Scott A.
TI  - CHI - Efficient Typing on a Visually Occluded Physical Keyboard
PY  - 2017
AB  - The rise of affordable head-mounted displays (HMDs) has raised questions about how to best design user interfaces for this technology. This paper focuses on the use of HMDs for home and office applications that require substantial text input. A physical keyboard is a familiar and effective text input device in normal desktop computing. But without additional camera technology, an HMD occludes all visual feedback about a user's hand position over the keyboard. We describe a system that assists HMD users in typing on a physical keyboard. Our system has a virtual keyboard assistant that provides visual feedback inside the HMD about a user's actions on the physical keyboard. It also provides powerful automatic correction of typing errors by extending a state-of-the-art touchscreen decoder. In a study with 24 participants, we found our virtual keyboard assistant enabled users to type more accurately on a visually-occluded keyboard. We found users wearing an HMD could type at over 40 words-per-minute while obtaining an error rate of less than 5%.
SP  - 5457
EP  - 5461
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025783
ER  - 

TY  - NA
AU  - Tran O'Leary, Jasper; Jun, Eunice; Peek, Nadya
TI  - Improving Programming for Exploratory Digital Fabrication with Inline Machine Control and Styled Toolpath Visualizations
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3561998
ER  - 

TY  - NA
AU  - Yamanaka, Shota; Usuba, Hiroki; Miyashita, Homei
TI  - Bivariate Effective Width Method to Improve the Normalization Capability for Subjective Speed-accuracy Biases in Rectangular-target Pointing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517466
ER  - 

TY  - NA
AU  - Mitterberger, Daniela; Ercan Jenny, Selen; Vasey, Lauren; Lloret-Fritschi, Ena; Aejmelaeus-Lindström, Petrus; Gramazio, Fabio; Kohler, Matthias
TI  - Interactive Robotic Plastering: Augmented Interactive Design and Fabrication for On-site Robotic Plastering
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501842
ER  - 

TY  - NA
AU  - Gomez, Argenis Ramirez; Gellersen, Hans
TI  - CHI - SuperVision: Playing with Gaze Aversion and Peripheral Vision
PY  - 2019
AB  - In this work, we challenge the Gaze interaction paradigm "What you see is what you get" to introduce "playing with peripheral vision". We developed the conceptual framework to introduce this novel gaze-aware game dynamic. We illustrated the concept with SuperVision, a collection of three games that play with peripheral vision. We propose perceptual and interaction challenges that require players not to look and rely on their periphery. To validate the game dynamic and experience, we conducted a user study with twenty-four participants. Results show how the game concept created an engaging and playful experience playing with peripheral vision. Participants showed proficiency in overcoming the game challenges, developing clear strategies to succeed. Moreover, we found evidence that playing the game can affect our visual skills, with greater peripheral awareness.
SP  - 473
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300703
ER  - 

TY  - NA
AU  - Copeland, Brian W
TI  - Establishing a relative positioning system to achieve mobile localization
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Kita, Naoki; Saito, Takafumi
TI  - Computational design of generalized centrifugal puzzles
PY  - 2020
AB  - NA
SP  - 21
EP  - 28
JF  - Computers & Graphics
VL  - 90
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2020.05.005
ER  - 

TY  - NA
AU  - Kato, Kunihiro; Ikematsu, Kaori; Igarashi, Yuki; Kawahara, Yoshihiro
TI  - Paper-Woven Circuits: Fabrication Approach for Papercraft-based Electronic Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3502253
ER  - 

TY  - JOUR
AU  - Park, Jaehyun; Bhat, Ganapati; Nk, Anish; Geyik, Cemil S.; Ogras, Umit Y.; Lee, Hyung Gyu
TI  - Energy per Operation Optimization for Energy-Harvesting Wearable IoT Devices
PY  - 2020
AB  - Wearable internet of things (IoT) devices can enable a variety of biomedical applications, such as gesture recognition, health monitoring, and human activity tracking. Size and weight constraints limit the battery capacity, which leads to frequent charging requirements and user dissatisfaction. Minimizing the energy consumption not only alleviates this problem, but also paves the way for self-powered devices that operate on harvested energy. This paper considers an energy-optimal gesture recognition application that runs on energy-harvesting devices. We first formulate an optimization problem for maximizing the number of recognized gestures when energy budget and accuracy constraints are given. Next, we derive an analytical energy model from the power consumption measurements using a wearable IoT device prototype. Then, we prove that maximizing the number of recognized gestures is equivalent to minimizing the duration of gesture recognition. Finally, we utilize this result to construct an optimization technique that maximizes the number of gestures recognized under the energy budget constraints while satisfying the recognition accuracy requirements. Our extensive evaluations demonstrate that the proposed analytical model is valid for wearable IoT applications, and the optimization approach increases the number of recognized gestures by up to 2.4× compared to a manual optimization.
SP  - 764
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 3
PB  - 
DO  - 10.3390/s20030764
ER  - 

TY  - NA
AU  - Albaugh, Lea; Hudson, Scott E.; Yao, Lining
TI  - CHI Extended Abstracts - Digital Fabrication of Soft Actuated Objects by Machine Knitting
PY  - 2019
AB  - With recent interest in shape-changing interfaces, material-driven design, wearable technologies, and soft robotics, digital fabrication of soft actuatable material is increasingly in demand. Much of this research focuses on elastomers or non-stretchy air bladders. Computationally-controlled machine knitting offers an alternative fabrication technology which can rapidly produce soft textile objects that have a very different character: breathable, lightweight, and pleasant to the touch. These machines are well established and optimized for the mass production of garments, but compared to other digital fabrication techniques such as CNC machining or 3D printing, they have received much less attention as general purpose fabrication devices. In this work, we explore new ways to employ machine knitting for the creation of actuated soft objects. We describe the basic operation of this type of machine, then show new techniques for knitting tendon-based actuation into objects. We explore a series of design strategies for integrating tendons with shaping and anisotropic texture design. Finally, we investigate different knit material properties, including considerations for motor control and sensing.
SP  - 184
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300414
ER  - 

TY  - NA
AU  - Lambrichts, Mannu; Tijerina, Jose Maria; De Weyer, Tom; Ramakers, Raf
TI  - Tangible and Embedded Interaction - DIY Fabrication of High Performance Multi-Layered Flexible PCBs
PY  - 2020
AB  - We present a novel DIY fabrication workflow for prototyping highly flexible circuit boards using a laser cutter. As our circuits consist of Kapton and copper, they are highly conductive and thus support high-frequency signals, such as I2C. Key to our approach is a laser machine that supports both a CO2 laser as well as a fiber laser to precisely process respectively Kapton and copper. We also show how the laser cutter can cure soldering paste to realize VIAs (Vertical Interconnect Access) and solder components. In contrast, previous approaches for prototyping flexible PCBs through laser cutting only considered CO2 lasers which can not process metals. Therefore these approaches mainly used ink-based conductors that have a significantly higher electrical resistance than copper.
SP  - 565
EP  - 571
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374988
ER  - 

TY  - NA
AU  - Schneegass, Stefan; Voit, Alexandra
TI  - ISWC - GestureSleeve: using touch sensitive fabrics for gestural input on the forearm for controlling smartwatches
PY  - 2016
AB  - Smartwatches provide quick and easy access to information. Due to their wearable nature, users can perceive the information while being stationary or on the go. The main drawback of smartwatches, however, is the limited input possibility. They use similar input methods as smartphones but thereby suffer from a smaller form factor. To extend the input space of smartwatches, we present GestureSleeve, a sleeve made out of touch enabled textile. It is capable of detecting different gestures such as stroke based gestures or taps. With these gestures, the user can control various smartwatch applications. Exploring the performance of the GestureSleeve approach, we conducted a user study with a running application as use case. In this study, we show that input using the GestureSleeve outperforms touch input on the smartwatch. In the future the GestureSleeve can be integrated into regular clothing and be used for controlling various smart devices.
SP  - 108
EP  - 115
JF  - Proceedings of the 2016 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971763.2971797
ER  - 

TY  - JOUR
AU  - Wu, Yu; Jiang, Dai; Liu, Xiao; Bayford, Richard; Demosthenous, Andreas
TI  - A Human–Machine Interface Using Electrical Impedance Tomography for Hand Prosthesis Control
PY  - 2018
AB  - This paper presents a human–machine interface that establishes a link between the user and a hand prosthesis. It successfully uses electrical impedance tomography, a conventional bio-impedance imaging technique, using an array of electrodes contained in a wristband on the user's forearm. Using a high-performance analog front-end application specific integrated circuit (ASIC), the user's forearm inner bio-impedance redistribution is accurately assessed. These bio-signatures are strongly related to hand motions and using artificial neural networks, they can be learned so as to recognize the user's intention in real time for prosthesis operation. In this work, eleven hand motions are designed for prosthesis operation with a gesture switching enabled sub-grouping method. Experiments with five subjects show that the system can achieve 98.5% accuracy with a grouping of three gestures and an accuracy of 94.4% with two sets of five gestures. The ASIC comprises a current driver with common-mode reduction capability and a current feedback instrumentation amplifier (that occupy an area of 0.07 mm2). The ASIC operates from ±1.65 V power supplies and has a minimum bio-impedance sensitivity of 12.7 mΩp-p.
SP  - 1322
EP  - 1333
JF  - IEEE transactions on biomedical circuits and systems
VL  - 12
IS  - 6
PB  - 
DO  - 10.1109/tbcas.2018.2878395
ER  - 

TY  - CHAP
AU  - Mitropoulou, Ioanna; Ariza, Inés; Bernhard, Mathias; Dillenburger, Benjamin; Gramazio, Fabio; Kohler, Matthias
TI  - Numerical Sculpting - Volumetric Modelling Tools for in place Spatial Additive Manufacturing
PY  - 2019
AB  - This paper presents a novel application of volumetric modelling (VM) for the design of fabrication-informed three-dimensional deposition paths for in place spatial additive manufacturing (AM). VM offers modelling techniques for designing with great geometric flexibility using numeric data, as well as for managing fabrication constraints. To address the challenges and new design possibilities presented by in place spatial AM, we propose a set of tools to design deposition paths with embedded fabrication constraints, as well as methods to combine VM with curve and surface geometry to generate production data. As a case study, we implement this toolset to create wire arc additive manufacturing (WAAM) connections of standard elements. A comparison of virtual and physical results is presented to validate the approach. Finally, we discuss potentials and limitations of using VM tools for fabrication-aware design of in place spatial AM.
SP  - 132
EP  - 145
JF  - Impact: Design With All Senses
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29829-6_11
ER  - 

TY  - NA
AU  - Sinha, Swapnil; Meisel, Nicholas A.
TI  - Quantifying the effect of embedded component orientation on flexural properties in additively manufactured structures
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Salemi Parizi, Farshid; Whitmire, Eric; Patel, Shwetak N.
TI  - AuraRing
PY  - 2022
AB  - <jats:p>Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. It has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.</jats:p>
SP  - 34
EP  - 37
JF  - GetMobile: Mobile Computing and Communications
VL  - 25
IS  - 3
PB  - 
DO  - 10.1145/3511285.3511295
ER  - 

TY  - JOUR
AU  - Mosenia, Arsalan; Sur-Kolay, Susmita; Raghunathan, Anand; Jha, Niraj K.
TI  - Wearable Medical Sensor-Based System Design: A Survey
PY  - 2017
AB  - Wearable medical sensors (WMSs) are garnering ever-increasing attention from both the scientific community and the industry. Driven by technological advances in sensing, wireless communication, and machine learning, WMS-based systems have begun transforming our daily lives. Although WMSs were initially developed to enable low-cost solutions for continuous health monitoring, the applications of WMS-based systems now range far beyond health care. Several research efforts have proposed the use of such systems in diverse application domains, e.g., education, human-computer interaction, and security. Even though the number of such research studies has grown drastically in the last few years, the potential challenges associated with their design, development, and implementation are neither well-studied nor well-recognized. This article discusses various services, applications, and systems that have been developed based on WMSs and sheds light on their design goals and challenges. We first provide a brief history of WMSs and discuss how their market is growing. We then discuss the scope of applications of WMS-based systems. Next, we describe the architecture of a typical WMS-based system and the components that constitute such a system, and their limitations. Thereafter, we suggest a list of desirable design goals that WMS-based systems should satisfy. Finally, we discuss various research directions related to WMSs and how previous research studies have attempted to address the limitations of the components used in WMS-based systems and satisfy the desirable design goals.
SP  - 124
EP  - 138
JF  - IEEE Transactions on Multi-Scale Computing Systems
VL  - 3
IS  - 2
PB  - 
DO  - 10.1109/tmscs.2017.2675888
ER  - 

TY  - NA
AU  - Gil, Hyunjae; Lee, DoYoung; Im, Seunggyu; Oakley, Ian
TI  - CHI - TriTap: Identifying Finger Touches on Smartwatches
PY  - 2017
AB  - The small screens of smartwatches provide limited space for input tasks. Finger identification is a promising technique to address this problem by associating different functions with different fingers. However, current technologies for finger identification are unavailable or unsuitable for smartwatches. To address this problem, this paper observes that normal smartwatch use takes places with a relatively static pose between the two hands. In this situation, we argue that the touch and angle profiles generated by different fingers on a standard smartwatch touch screen will differ sufficiently to support reliable identification. The viability of this idea is explored in two studies that capture touches in natural and exaggerated poses during tapping and swiping tasks. Machine learning models report accuracies of up to 93% and 98% respectively, figures that are sufficient for many common interaction tasks. Furthermore, the exaggerated poses show modest costs (in terms of time/errors) compared to the natural touches. We conclude by presenting examples and discussing how interaction designs using finger identification can be adapted to the smartwatch form factor.
SP  - 3879
EP  - 3890
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025561
ER  - 

TY  - NA
AU  - Jones, Michael; Walker, Casey; Anderson, Zann; Lusk, Candice; Bryce, Andrew
TI  - Tangible and Embedded Interaction - Exploring a Modular Approach to Redesigning Interfaces for Physical Interactive Devices
PY  - 2017
AB  - Tools for creating graphical user interfaces (GUIs) hide implementation details associated with changing the position, size or shape of a widget. For example, a GUI designer can change the position of a button without explicitly reimplementing the code that determines if the button has been clicked. We seek to provide a similar experience for moving tangible widgets on physical interactive devices (PIDs). One reason that GUI design tools can hide implementation details is that widgets are modules. In this work in progress, we describe a system for leveraging modularity to hide implementation details associated with changing the position, size and shape of widgets in pid interfaces. We have used the system to design, redesign and fabricate 10 interfaces for 3 example applications. Fundamentaly, working with atoms to make pid interfaces is different than working with pixels to make GUI interfaces--but modularity in pid widgets appears to be a promising way to hide implementation details in pid interface redesign and fabrication.
SP  - 465
EP  - 471
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3025075
ER  - 

TY  - NA
AU  - Zhang, Yang; Harrison, Chris
TI  - CHI - Pulp Nonfiction: Low-Cost Touch Tracking for Paper
PY  - 2018
AB  - Paper continues to be a versatile and indispensable material in the 21st century. Of course, paper is a passive medium with no inherent interactivity, precluding us from computationally-enhancing a wide variety of paper-based activities. In this work, we present a new technical approach for bringing the digital and paper worlds closer together, by enabling paper to track finger input and also drawn input with writing implements. Importantly, for paper to still be considered paper, our method had to be very low cost. This necessitated research into materials, fabrication methods and sensing techniques. We describe the outcome of our investigations and show that our method can be sufficiently low-cost and accurate to enable new interactive opportunities with this pervasive and venerable material.
SP  - 117
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173691
ER  - 

TY  - JOUR
AU  - Covi, Erika; Donati, Elisa; Liang, Xiangpeng; Kappel, David; Heidari, Hadi; Payvand, Melika; Wang, Wei
TI  - Adaptive Extreme Edge Computing for Wearable Devices.
PY  - 2021
AB  - Wearable devices are a fast-growing technology with impact on personal healthcare for both society and economy. Due to the widespread of sensors in pervasive and distributed networks, power consumption, processing speed, and system adaptation are vital in future smart wearable devices. The visioning and forecasting of how to bring computation to the edge in smart sensors have already begun, with an aspiration to provide adaptive extreme edge computing. Here, we provide a holistic view of hardware and theoretical solutions toward smart wearable devices that can provide guidance to research in this pervasive computing era. We propose various solutions for biologically plausible models for continual learning in neuromorphic computing technologies for wearable sensors. To envision this concept, we provide a systematic outline in which prospective low power and low latency scenarios of wearable sensors in neuromorphic platforms are expected. We successively describe vital potential landscapes of neuromorphic processors exploiting complementary metal-oxide semiconductors (CMOS) and emerging memory technologies (e.g., memristive devices). Furthermore, we evaluate the requirements for edge computing within wearable devices in terms of footprint, power consumption, latency, and data size. We additionally investigate the challenges beyond neuromorphic computing hardware, algorithms and devices that could impede enhancement of adaptive edge computing in smart wearable devices.
SP  - 611300
EP  - 611300
JF  - Frontiers in neuroscience
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnins.2021.611300
ER  - 

TY  - JOUR
AU  - Scherer, Moritz; Magno, Michele; Erb, Jonas; Mayer, Philipp; Eggimann, Manuel; Benini, Luca
TI  - TinyRadarNN: Combining Spatial and Temporal Convolutional Neural Networks for Embedded Gesture Recognition With Short Range Radars
PY  - 2021
AB  - This work proposes a low-power high-accuracy embedded hand-gesture recognition algorithm targeting battery-operated wearable devices using low-power short-range RADAR sensors. A 2-D convolutional neural network (CNN) using range-frequency Doppler features is combined with a temporal convolutional neural network (TCN) for time sequence prediction. The final algorithm has a model size of only 46 thousand parameters, yielding a memory footprint of only 92 KB. Two data sets containing 11 challenging hand gestures performed by 26 different people have been recorded containing a total of 20’210 gesture instances. On the 11 hand gesture data set, accuracies of 86.6% (26 users) and 92.4% (single user) have been achieved, which are comparable to the state of the art, which achieves 87% (10 users) and 94% (single user), while using a TCN-based network that is $7500\times $ smaller than the state of the art. Furthermore, the gesture recognition classifier has been implemented on a parallel ultralow power processor, demonstrating that real-time prediction is feasible with only 21 mW of power consumption for the full TCN sequence prediction network, while a system-level power consumption of less than 120 mW is achieved. We provide open-source access to example code and all data collected and used in this work on tinyradar.ethz.ch.
SP  - 10336
EP  - 10346
JF  - IEEE Internet of Things Journal
VL  - 8
IS  - 13
PB  - 
DO  - 10.1109/jiot.2021.3067382
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Steimle, Jürgen
TI  - UbiComp Adjunct - Digital fabrication pipeline for on-body sensors: design goals and challenges
PY  - 2016
AB  - In this paper, we propose a digital fabrication pipeline for designing on-body sensors. We discuss the various steps in our envisioned pipeline and present the technical and research challenges that exist at each of the steps. The research and technical challenges span across various areas, including human computer interaction, computer graphics and computer vision. Our fabrication pipeline and the research challenges can inform the community about the various issues that one might encounter when realizing such a fabrication pipeline.
SP  - 950
EP  - 953
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2979140
ER  - 

TY  - JOUR
AU  - Huh, Jun Ho; Shin, Hyejin; Kim, HongMin; Cheon, Eunyong; Song, Youngeun; Lee, Choong-Hoon; Oakley, Ian
TI  - WristAcoustic
PY  - 2022
AB  - <jats:p>PIN and pattern lock are difficult to accurately enter on small watch screens, and are vulnerable against guessing attacks. To address these problems, this paper proposes a novel implicit biometric scheme based on through-wrist acoustic responses. A cue signal is played on a surface transducer mounted on the dorsal wrist and the acoustic response recorded by a contact microphone on the volar wrist. We build classifiers using these recordings for each of three simple hand poses (relax, fist and open), and use an ensemble approach to make final authentication decisions. In an initial single session study (N=25), we achieve an Equal Error Rate (EER) of 0.01%, substantially outperforming prior on-wrist biometric solutions. A subsequent five recall-session study (N=20) shows reduced performance with 5.06% EER. We attribute this to increased variability in how participants perform hand poses over time. However, after retraining classifiers performance improved substantially, ultimately achieving 0.79% EER. We observed most variability with the relax pose. Consequently, we achieve the most reliable multi-session performance by combining the fist and open poses: 0.51% EER. Further studies elaborate on these basic results. A usability evaluation reveals users experience low workload as well as reporting high SUS scores and fluctuating levels of perceived exertion: moderate during initial enrollment dropping to slight during authentication. A final study examining performance in various poses and in the presence of noise demonstrates the system is robust to such disturbances and likely to work well in wide range of real-world contexts.</jats:p>
SP  - 1
EP  - 34
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569473
ER  - 

TY  - JOUR
AU  - Jiang, Shuo; Li, Ling; Xu, Haipeng; Xu, Junkai; Gu, Guoying; Shull, Peter B.
TI  - Stretchable e-Skin Patch for Gesture Recognition on the Back of the Hand
PY  - 2020
AB  - Gesture recognition is important for human–computer interaction and a variety of emerging research and commercial areas including virtual and augmented reality. Current approaches typically require sensors to be placed on the forearm, wrist, or directly across finger joints; however, they can be cumbersome or hinder human movement and sensation. In this paper, we introduce a novel approach to recognize hand gestures by estimating skin strain with multiple soft sensors optimally placed across the back of the hand. A pilot study was first conducted by covering the back of the hand with 40 small 2.5 mm reflective markers and using a high-precision camera system to measure skin strain patterns for individual finger movements. Optimal strain locations are then determined and used for sensor placement in a stretchable e-skin patch prototype. Experimental testing is performed to evaluate the stretchable e-skin patch performance in classifying individual finger gestures and American Sign Language 0–9 number gestures. Results showed classification accuracies of 95.3% and 94.4% for finger gestures and American Sign Language 0–9 gestures, respectively. These results demonstrate the feasibility of a stretchable e-skin patch on the back of the hand for hand gesture recognition and their potential to significantly enhance human–computer interaction.
SP  - 647
EP  - 657
JF  - IEEE Transactions on Industrial Electronics
VL  - 67
IS  - 1
PB  - 
DO  - 10.1109/tie.2019.2914621
ER  - 

TY  - JOUR
AU  - Zeng, Qinglin; Kuang, Zheng; Wu, Shuaibing; Yang, Jun
TI  - A Method of Ultrasonic Finger Gesture Recognition Based on the Micro-Doppler Effect
PY  - 2019
AB  - With the popularity of small-screen smart mobile devices, gestures as a new type of human–computer interaction are highly demanded. Furthermore, finger gestures are more familiar to people in controlling devices. In this paper, a new method for recognizing finger gestures is proposed. Ultrasound was actively emitted to measure the micro-Doppler effect caused by finger motions and was obtained at high resolution. By micro-Doppler processing, micro-Doppler feature maps of finger gestures were generated. Since the feature map has a similar structure to the single channel color image, a recognition model based on a convolutional neural network was constructed for classification. The optimized recognition model achieved an average accuracy of 96.51% in the experiment.
SP  - 2314
EP  - NA
JF  - Applied Sciences
VL  - 9
IS  - 11
PB  - 
DO  - 10.3390/app9112314
ER  - 

TY  - NA
AU  - Choi, Frederick; Mayer, Sven; Harrison, Chris
TI  - MobileHCI - 3D Hand Pose Estimation on Conventional Capacitive Touchscreens
PY  - 2021
AB  - Contemporary mobile devices with touchscreens capture the X/Y position of finger tips on the screen and pass these coordinates to applications as though the input were points in space. Of course, human hands are much more sophisticated, able to form rich 3D poses capable of far more complex interactions than poking at a screen. In this paper, we describe how conventional capacitive touchscreens can be used to estimate 3D hand pose, enabling richer interaction opportunities. Importantly, our software-only approach requires no special or new sensors, either internal or external. As a proof of concept, we use an off-the-shelf Samsung Tablet flashed with a custom kernel. After describing our software pipeline, we report findings from our user study, we conclude with several example applications we built to illustrate the potential of our approach.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472045
ER  - 

TY  - NA
AU  - Fossdal, Frikk H.; Dyvik, Jens; Nilsson, Jakob Anders; Nordby, Jon; Helgesen, Torbjørn Nordvik; Heldal, Rogardt; Peek, Nadya
TI  - Tangible and Embedded Interaction - Fabricatable Machines: A Toolkit for Building Digital Fabrication Machines
PY  - 2020
AB  - Digital fabrication is changing the way we design and manufacture the objects around us. Digital fabrication machines enable mass-customisation. However, customising the machines themselves requires a high amount of expertise, which prevents even advanced users from taking part in the creation of bespoke fabrication tools. We present Fabricatable Machines, an open-source toolkit for designing custom fabrication machines. We designed a linear motion module, The Fabricatable Axis, that provides robust automated linear motion. The Fabricatable Axis can be resized, adjusted, and fabricated from different materials. Users can build machines by combining multiple axes. We optimised the design of the axis to be manufactured using a CNC mill, with few externally sourced parts. We observed users creating machines including portable milling machines, 3D printers, and pipe inspection robots using the Fabricatable Machines Toolkit.
SP  - 411
EP  - 422
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374929
ER  - 

TY  - JOUR
AU  - Kuno, Wakaba; Sugimoto, Maki; Sugiura, Yuta
TI  - Finger posture estimation by measuring skin deformation on back of hand
PY  - 2019
AB  - NA
SP  - 595
EP  - 601
JF  - The Journal of The Institute of Image Information and Television Engineers
VL  - 73
IS  - 3
PB  - 
DO  - 10.3169/itej.73.595
ER  - 

TY  - JOUR
AU  - Xie, Yue; Chen, Xiang
TI  - Support-free interior carving for 3D printing
PY  - 2017
AB  - Abstract Recent interior carving methods for functional design necessitate a cumbersome cut-and-glue process in fabrication. We propose a method to generate interior voids which not only satisfy the functional purposes but are also support-free during the 3D printing process. We introduce a support-free unit structure for voxelization and derive the wall thicknesses parametrization for continuous optimization. We also design a discrete dithering algorithm to ensure the printability of ghost voxels. The interior voids are iteratively carved by alternating the optimization and dithering. We apply our method to optimize the static and rotational stability, and print various results to evaluate the efficacy.
SP  - 9
EP  - 15
JF  - Visual Informatics
VL  - 1
IS  - 1
PB  - 
DO  - 10.1016/j.visinf.2017.01.002
ER  - 

TY  - NA
AU  - Zhang, Xiaoting; Fang, Guoxin; Dai, Chengkai; Verlinden, Jouke; Wu, Jun; Whiting, Emily; Wang, Charlie C. L.
TI  - UIST - Thermal-Comfort Design of Personalized Casts
PY  - 2017
AB  - This paper introduces a novel method for designing personalized orthopedic casts which are aware of thermal-comfort while satisfying mechanical requirements. Our pipeline starts from thermal images taken by an infrared camera, by which the distribution of thermal-comfort sensitivity is generated on the surface of a 3D scanned model. We formulate a hollowed Voronoi tessellation pattern to represent the covered region for a web-like cast design. The pattern is further optimized according to the thermal-comfort sensitivity calculated from thermal images. Working together with a thickness variation method, we generate a solid model for a personalized cast maximizing both thermal comfort and mechanical stiffness. To demonstrate the effectiveness of our approach, 3D printed models of personalized casts are tested on body parts of different individuals.
SP  - 243
EP  - 254
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126600
ER  - 

TY  - NA
AU  - Ledo, David; Anderson, Fraser; Schmidt, Ryan; Oehlberg, Lora; Greenberg, Saul; Grossman, Tovi
TI  - CHI - Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices
PY  - 2017
AB  - Interactive, smart objects-customized to individuals and uses-are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects' "smarts". Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.
SP  - 2583
EP  - 2593
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025652
ER  - 

TY  - NA
AU  - Hanton, Ollie; Wessely, Michael; Mueller, Stefanie; Fraser, Mike; Roudaut, Anne
TI  - CHI Extended Abstracts - ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes
PY  - 2020
AB  - ProtoSpray is a fabrication method that combines 3D printing and spray coating, to create interactive displays of arbitrary shapes. Our approach makes novel use of 3D printed conductive channels to create base electrodes on 3D shapes. This is then combined with spraying active materials to produce illumination. We demonstrate the feasibility and benefits of this combined approach in 6 evaluations exploring different shaped topologies. We analyze factors such as spray orientations, surface topologies and printer resolutions, to discuss how spray nozzles can be integrated into traditional 3D printers. We present a series of ProtoSprayed objects demonstrating how our technique goes beyond existing fabrication techniques by allowing creation of displays on objects with curvatures as complex as a Mobius strip. Our work provides a platform to empower makers to use displays as a fabrication material.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376543
ER  - 

TY  - NA
AU  - Zhang, Yunbo; Gao, Wei; Paredes, Luis; Ramani, Karthik
TI  - CHI - CardBoardiZer: Creatively Customize, Articulate and Fold 3D Mesh Models
PY  - 2016
AB  - Computer-aided design of flat patterns allows designers to prototype foldable 3D objects made of heterogeneous sheets of material. We found origami designs are often characterized by pre-synthesized patterns and automated algorithms. Furthermore, augmenting articulated features to a desired model requires time-consuming synthesis of interconnected joints. This paper presents CardBoardiZer, a rapid cardboard based prototyping platform that allows everyday sculptural 3D models to be easily customized, articulated and folded. We develop a building platform to allow the designer to 1) import a desired 3D shape, 2) customize articulated partitions into planar or volumetric foldable patterns, and 3) define rotational movements between partitions. The system unfolds the model into 2D crease-cut-slot patterns ready for die-cutting and folding. In this paper, we developed interactive algorithms and validated the usability of CardBoardiZer using various 3D models. Furthermore, comparisons between CardBoardiZer and methods of Autodesk® 123D Make, demonstrated significantly shorter time-to-prototype and ease of fabrication.
SP  - 897
EP  - 907
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858362
ER  - 

TY  - JOUR
AU  - Zhao, Tianming; Liu, Jian; Wang, Yan; Liu, Hongbo; Chen, Yingying
TI  - Towards Low-Cost Sign Language Gesture Recognition Leveraging Wearables
PY  - 2021
AB  - Different from traditional gestures, sign language gestures involve a lot of finger-level gestures without wrist or arm movements. They are hard to detect using existing motion sensors-based approaches. We introduce the first low-cost sign language gesture recognition system that can differentiate fine-grained finger movements using the Photoplethysmography (PPG) and motion sensors in commodity wearables. By leveraging the motion artifacts in PPG, our system can accurately recognize sign language gestures when there are large body movements, which cannot be handled by the traditional motion sensor-based approaches. We further explore the feasibility of using both PPG and motion sensors in wearables to improve the sign language gesture recognition accuracy when there are limited body movements. We develop a gradient boost tree (GBT) model and deep neural network-based model (i.e., ResNet) for classification. The transfer learning technique is applied to ResNet-based model to reduce the training effort. We develop a prototype using low-cost PPG and motions sensors and conduct extensive experiments and collect over 7000 gestures from 10 adults in the static and body-motion scenarios. Results demonstrate that our system can differentiate nine finger-level gestures from the American Sign Language with an average recognition accuracy over 98 percent.
SP  - 1685
EP  - 1701
JF  - IEEE Transactions on Mobile Computing
VL  - 20
IS  - 4
PB  - 
DO  - 10.1109/tmc.2019.2962760
ER  - 

TY  - NA
AU  - Wang, Tianyi; Huo, Ke; Chawla, Pratik; Chen, Guiming; Banerjee, Siddharth; Ramani, Karthik
TI  - Conference on Designing Interactive Systems - Plain2Fun: Augmenting Ordinary Objects with Interactive Functions by Auto-Fabricating Surface Painted Circuits
PY  - 2018
AB  - The growing makers' community demands better supports for designing and fabricating interactive functional objects. Most of the current approaches focus on embedding desired functions within new objects. Instead, we advocate repurposing the existing objects and rapidly authoring interactive functions onto them. We present Plain2Fun, a design and fabrication pipeline enabling users to quickly transform ordinary objects into interactive and functional ones. Plain2Fun allows users to directly design the circuit layouts onto the surfaces of the scanned 3D model of existing objects. Our design tool automatically generates as short as possible circuit paths between any two points while avoiding intersections. Further, we build a digital machine to construct the conductive paths accurately. With a specially designed housing base, users can simply snap the electronic components onto the surfaces and obtain working physical prototypes. Moreover, we evaluate the usability of our system with multiple use cases and a preliminary user study.
SP  - 1095
EP  - 1106
JF  - Proceedings of the 2018 Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3196709.3196791
ER  - 

TY  - NA
AU  - Li, Jiahao; Cui, Meilin; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - UIST - Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities
PY  - 2020
AB  - Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.
SP  - 897
EP  - 911
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415826
ER  - 

TY  - JOUR
AU  - Klum, Michael; Schmidt, Malte; Klaproth, Joel; Pielmus, Alexandru-Gabriel; Tigges, Timo; Orglmeister, Reinhold
TI  - EMBC - Balanced Adjustable Mirrored Current Source with Common Mode Feedback and Output Measurement for Bioimpedance Applications
PY  - 2019
AB  - Bioimpedance methods are used in a variety of applications such as impedance tomography, electrodermal activity detection and vascular disease assessment. Recent developments in portable and unobtrusive biosignal acquisition systems facilitate the integration of wearable bioimpedance applications including sleep monitoring, respiration estimation and fluid monitoring. However, the less stable measurement situation in a wearable scenario increases the requirements for the system’s accuracy and adaptability. The current source of a bioimpedance system needs to drive large complex loads subject to vast variations over time while maintaining a high level of accuracy. The widely used improved Howland current source suffers from multiple disadvantages when considered for an adaptive bioimpedance system. We propose an optimized mirrored architecture which allows for a simple output current adjustment and current measurement without an additional shunt resistor in the load path. The system implements a common mode feedback system which includes balancing of the mirrored sources. Our design is validated by calculation, SPICE simulation and complex load measurements. We achieved output impedances in excess of 3 MΩ and derived a simplified transconductance function valid for frequencies up to 1 MHz. We conclude that the presented architecture is an important step forward towards accurate wearable bioimpedance acquisition. Employing generalized impedance converters, the output impedance could be further optimized.
SP  - 1278
EP  - 1281
JF  - Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference
VL  - 2019
IS  - NA
PB  - 
DO  - 10.1109/embc.2019.8856325
ER  - 

TY  - JOUR
AU  - Crea, Simona; Manca, Silvia; Parri, Andrea; Zheng, Enhao; Mai, Jingeng; Lova, Raffaele Molino; Vitiello, Nicola; Wang, Qining
TI  - Controlling a Robotic Hip Exoskeleton With Noncontact Capacitive Sensors
PY  - 2019
AB  - For partial lower-limb exoskeletons, an accurate real-time estimation of the gait phase is paramount to provide timely and well-tailored assistance during gait. To this end, dedicated wearable sensors separated from the exoskeletons mechanical structure may be preferable because they are typically isolated from movement artifacts that often result from the transient dynamics of the physical human–robot interaction. Moreover, wearable sensors that do not require time-consuming calibration procedures are more easily acceptable by users. In this paper, a robotic hip orthosis was controlled using capacitive sensors placed in orthopedic cuffs on the shanks. The capacitive signals are zeroed after donning the cuffs and do not require any further calibration. The capacitive-sensing-based controller was designed to perform online estimation of the gait cycle phase via adaptive oscillators, and to provide a phase-locked assistive torque. Two experimental activities were carried out to validate the effectiveness of the proposed control strategy. Experiments conducted with seven healthy subjects walking on a treadmill at different speeds demonstrated that the controller can estimate the gait phase with an average error of 4%, while also providing hip flexion assistance. Moreover, experiments carried out with four healthy subjects showed that the capacitive-sensing-based controller could reduce the metabolic expenditure of subjects compared to the unassisted condition (mean $\pm$ SEM, $-$ 3.2% $\pm$ 1.1).
SP  - 2227
EP  - 2235
JF  - IEEE/ASME Transactions on Mechatronics
VL  - 24
IS  - 5
PB  - 
DO  - 10.1109/tmech.2019.2929826
ER  - 

TY  - JOUR
AU  - Bellacicca, Andrea; Santaniello, Tommaso; Milani, Paolo
TI  - Embedding electronics in 3D printed structures by combining fused filament fabrication and supersonic cluster beam deposition
PY  - 2018
AB  - NA
SP  - 60
EP  - 66
JF  - Additive Manufacturing
VL  - 24
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2018.09.010
ER  - 

TY  - NA
AU  - Tan, Yanke; Yoon, Sang Ho; Ramani, Karthik
TI  - CHI Extended Abstracts - BikeGesture: User Elicitation and Performance of Micro Hand Gesture as Input for Cycling
PY  - 2017
AB  - The use of hand gestures has a potential as an promising input metaphor. Wearables like smart textile and data gloves can provide hand gesture recognition to potentially replace, augment or improve existing input methods. Although recent bikes provide advanced functions with electro mechanical components, the input metaphor still relies on mechanical switches or levers. In this paper, we investigate the acceptance and performance of using hand gesture during cycling. Through an observational study with 16 users, we devised a taxonomy of hand gestures. Users prefer subtle micro hand gestures to ensure safe cycling while maintaining a flexible controllability. We also implemented a wearable prototype that recognizes these gestures. In our evaluation, the prototype shows an average of 92% accuracy while showing similar response time to existing mechanical inputs.
SP  - 2147
EP  - 2154
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053075
ER  - 

TY  - NA
AU  - Jones, Lee; Girouard, Audrey
TI  - Creativity &amp; Cognition - Patching Textiles: Insights from Visible Mending Educators on Wearability, Extending the Life of Our Clothes, and Teaching Tangible Crafts
PY  - 2021
AB  - Textiles have several characteristics that make them well suited for updates, sometimes called patching or mending, but textile repair is underexplored in the context of personal fabrication. This exploration is an urgent sustainability issue so we can extend the life of textiles and avoid producing more materials. In this paper we take a craft ethnography approach by interviewing 15 visible mending educators for insights into how they teach the techniques of repair and re-use so individuals can upcycle the textiles they already own. We discuss the values that menders bring to the practice, the teaching strategies they employ, the tangible teaching materials and tools of the practice, and introduce three types of teaching samplers: wearable samplers, sampler swatches, and practice samplers. Overall, these interviews provide insights for textile maker toolkits, textile personal fabrication, and how we can teach tangible hybrid crafts and sustainable making practices.
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450741.3465265
ER  - 

TY  - NA
AU  - Ferrane, Andrea; Jiang, Xianta; Maiolo, Luca; Pecora, A.; Colace, Lorenzo; Menon, Carlo
TI  - A fabric-based wearable band for hand gesture recognition based on filament strain sensors: A preliminary investigation
PY  - 2016
AB  - A wearable system based on a breathable cloth wristband equipped with stretchable strain gauge sensors were assembled and tested to detect a set of 16 different hand gestures. The sensors embedded on the wristband prototype do not require a direct contact with the skin, thus maximizing comfort. To evaluate the performance of the developed band, different gestures were labelled by using grasping information detected in real-time by commercial Force-Sensing Resistor (FSR) sensors. Signals recorded by the wristband were processed through two machine-learning algorithms, i.e. Linear Discriminant Analysis (LDA) and Support Vector Machine (SVM), reaching accuracies of 87% and 95% respectively.
SP  - 113
EP  - 116
JF  - 2016 IEEE Healthcare Innovation Point-Of-Care Technologies Conference (HI-POCT)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/hic.2016.7797710
ER  - 

TY  - JOUR
AU  - Bhatt, Prahar M.; Malhan, Rishi K.; Rajendran, Pradeep; Gupta, Satyandra K.
TI  - Building free-form thin shell parts using supportless extrusion-based additive manufacturing
PY  - 2020
AB  - NA
SP  - 101003
EP  - NA
JF  - Additive Manufacturing
VL  - 32
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2019.101003
ER  - 

TY  - NA
AU  - Bhaduri, Srinjita; Tovar, Jesús G. Ortiz; Kane, Shaun K.
TI  - Creativity &amp; Cognition - Fabrication Games: Using 3D Printers to Explore New Interactions for Tabletop Games
PY  - 2017
AB  - Personal fabrication technologies such as 3D printers are becoming increasingly affordable, enabling many to own and use 3D printers in their own homes. Yet we have little understanding of how fabrication tools and technologies can be used and appropriated within the home. In this paper, we explore the opportunities and challenges related to using personal fabrication technologies as part of play, specifically in the context of board and tabletop games. We present an overview of existing uses of 3D printers in the context of gaming, which has largely focused on creating and replacing pieces for existing games. Drawing on existing uses of 3D printing in games, and on prior research in interacting with fabrication tools, we then introduce a set of gameplay elements that use the affordances of the 3D printer to enhance and extend gameplay. We evaluated these gameplay elements through a focus group with 9 gaming hobbyists, who provided feedback on these elements and designed new games that used these elements. Our contributions include an extended set of gameplay elements that leverage fabrication tools, a set of reference games, and guidelines for augmenting existing fabrication tools to support playful interactions.
SP  - 51
EP  - 62
JF  - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3059454.3059463
ER  - 

TY  - NA
AU  - Suzuki, Yutaro; Sekimori, Kodai; Shizuki, Buntarou; Takahashi, Shin
TI  - PerCom Workshops - Touch Sensing on the Forearm Using the Electrical Impedance Method
PY  - 2019
AB  - We present a novel on-skin touch sensing approach based on the electrical impedance method (EIM). Our approach enables the user to detect touch across the surface of the forearm by wearing two bands, one each on the lower and upper forearm. EIM uses a conductive substance to identify the area being touched. We focused on the electrical conductivity of the skin and applied EIM to the forearm. The two bands have electrodes on the inside. Signals are applied to these electrodes, and the resulting voltage on the surface of the skin is then measured. The advantages of our approach are that it works over a large area of the forearm, and can recognize both hand gestures and touch.
SP  - 255
EP  - 260
JF  - 2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/percomw.2019.8730739
ER  - 

TY  - JOUR
AU  - Jiang, Shuo; Kang, Peiqi; Song, Xinyu; Lo, Benny; Shull, Peter B.
TI  - Emerging Wearable Interfaces and Algorithms for Hand Gesture Recognition: A Survey.
PY  - 2022
AB  - Hands are vital in a wide range of fundamental daily activities, and neurological diseases that impede hand function can significantly affect quality of life. Wearable hand gesture interfaces hold promise to restore and assist hand function and to enhance human-human and human-computer communication. The purpose of this review is to synthesize current novel sensing interfaces and algorithms for hand gesture recognition, and the scope of applications covers rehabilitation, prosthesis control, sign language recognition, and human-computer interaction. Results showed that electrical, dynamic, acoustical/vibratory, and optical sensing were the primary input modalities in gesture recognition interfaces. Two categories of algorithms were identified: 1) classification algorithms for predefined, fixed hand poses and 2) regression algorithms for continuous finger and wrist joint angles. Conventional machine learning algorithms, including linear discriminant analysis, support vector machines, random forests, and non-negative matrix factorization, have been widely used for a variety of gesture recognition applications, and deep learning algorithms have more recently been applied to further facilitate the complex relationship between sensor signals and multi-articulated hand postures. Future research should focus on increasing recognition accuracy with larger hand gesture datasets, improving reliability and robustness for daily use outside of the laboratory, and developing softer, less obtrusive interfaces.
SP  - 1
EP  - 1
JF  - IEEE reviews in biomedical engineering
VL  - 15
IS  - NA
PB  - 
DO  - 10.1109/rbme.2021.3078190
ER  - 

TY  - NA
AU  - Gong, Hebo; Cui, Zhitong; Wang, Yanan; Shen, Chengyi; Zhang, Deyin; Luo, Shijian
TI  - CHI Extended Abstracts - eGlove: Designing Interactive Fabric Sensor for Enhancing Contact-Based Interactions
PY  - 2021
AB  - We present eGlove, a wearable and low-cost fabric sensor for recognizing a rich context of objects by touching them, including daily necessities, fruits, plants, as well as different body parts. Our sensing approach utilizes Swept frequency Capacitive Sensing (SFCS) to provide consistent sensor readings even when the fabric electrode is under varying deformation and stretching degrees. Our work proposes an easy fabrication method and hardware configuration for prototyping the interactive fabric sensor. We evaluated our system’s classification accuracy through per-user training and found a real-time classification of 96.3%. We also demonstrated novel contextual interactions enabled by our technical approach with several applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451824
ER  - 

TY  - JOUR
AU  - Hesenius, Marc; Gruhn, Volker
TI  - GestureCards: A Hybrid Gesture Notation
PY  - 2019
AB  - Describing gestures in detail has various advantages for project teams: communication is simplified, interaction concepts are documented, and technical decisions are supported. Common gesture notations focus on textual or graphical elements only, but we argue that hybrid approaches have various advantages, especially because some gesture traits are easier to describe with text and others with arrows or icons. We present GestureCards, a hybrid gesture notation mixing graphical and textual elements we developed to describe multi-touch gestures. To evaluate our approach, we compared how users perceive and are affected by different notations. First, we compared GestureCards with a textual notation and observed advantages in terms of speed, correctness, and confidence. Second, we asked participants to compare GestureCards, a textual, and a graphical notation and rate them. The results indicate that the participants' perception depends on the gesture, but GestureCards received consistently good ratings. Third, we monitored several participants working with GestureCards solving practical development tasks for gesture-based applications and they felt well supported by GestureCards.
SP  - 1
EP  - 35
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - EICS
PB  - 
DO  - 10.1145/3331164
ER  - 

TY  - JOUR
AU  - Rzecki, Krzysztof
TI  - Classification Algorithm for Person Identification and Gesture Recognition Based on Hand Gestures with Small Training Sets.
PY  - 2020
AB  - Classification algorithms require training data initially labelled by classes to build a model and then to be able to classify the new data. The amount and diversity of training data affect the classification quality and usually the larger the training set, the better the accuracy of classification. In many applications only small amounts of training data are available. This article presents a new time series classification algorithm for problems with small training sets. The algorithm was tested on hand gesture recordings in tasks of person identification and gesture recognition. The algorithm provides significantly better classification accuracy than other machine learning algorithms. For 22 different hand gestures performed by 10 people and the training set size equal to 5 gesture execution records per class, the error rate for the newly proposed algorithm is from 37% to 75% lower than for the other compared algorithms. When the training set consists of only one sample per class the new algorithm reaches from 45% to 95% lower error rate. Conducted experiments indicate that the algorithm outperforms state-of-the-art methods in terms of classification accuracy in the problem of person identification and gesture recognition.
SP  - 7279
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 24
PB  - 
DO  - 10.3390/s20247279
ER  - 

TY  - JOUR
AU  - Hattab, Ammar; Gonsher, Ian; Moreno, Daniel; Taubin, Gabriel
TI  - Differential 3D Scanning.
PY  - 2017
AB  - During the creative process, designers use various techniques and strategies to move from the abstract to the concrete, utilizing different physical and virtual means to represent form. The changes between virtual and physical models are not always fluent, however. Differential 3D scanning can detect the differences between a scanned model (point cloud) and a reference model (polygon mesh or CAD model) and then reflect those changes in the reference model. This can save designers time by reconstructing only the small changed regions rather than the entire object.
SP  - 43
EP  - 51
JF  - IEEE computer graphics and applications
VL  - 37
IS  - 3
PB  - 
DO  - 10.1109/mcg.2017.39
ER  - 

TY  - JOUR
AU  - Jiang, Jingchao; Newman, Stephen T.; Zhong, Ray Y.
TI  - A review of multiple degrees of freedom for additive manufacturing machines
PY  - 2020
AB  - NA
SP  - 195
EP  - 211
JF  - International Journal of Computer Integrated Manufacturing
VL  - 34
IS  - 2
PB  - 
DO  - 10.1080/0951192x.2020.1858510
ER  - 

TY  - BOOK
AU  - Dew, Kristin; Shorey, Samantha; Rosner, Daniela K.
TI  - LIMITS - Making within limits: towards salvage fabrication
PY  - 2018
AB  - Cultures of making have received broad attention within HCI studies of design and material production, surfacing the uneven social and political consequences of maker visions. Less explored but equally important in this scholarship is what it means to make within limits: what makers' tools would look like if they took seriously concerns for resource scarcity, provenance, and disposal. Deploying a mix of ethnographic and design methods, this paper identifies salvage practices as a core but under-recognized resource within an academic makerspace. To help open the conversation around making within limits, we propose a series of tools for salvage fabrication, an alternative design concept that emphasizes the interconnected material flows into and beyond local processes of material production.
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 Workshop on Computing within Limits
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3232617.3232626
ER  - 

TY  - JOUR
AU  - Zifei, Xu; Yao, Jiafeng; Wang, Zheng; Liu, Yanli; Wang, Hao; Chen, Bai; Wu, Hongtao
TI  - Development of a Portable Electrical Impedance Tomography System for Biomedical Applications
PY  - 2018
AB  - A portable electrical impedance tomography (EIT) system has been developed with Red Pitaya STEMlab for biomedical applications. The Red Pitaya STEMlab is a portable device to realize voltage generation and data acquisition for the EIT system. The EIT system includes a modified howland circuit as a voltage-controlled current source, a high-speed analogy multiplexer module, an 8-electrode array, and a personal computer. The generalized vector sampled pattern matching algorithm and the Tikhonov regularization algorithm are used to reconstruct the image generated by the EIT system. The reconstructed images by using Red Pitaya STEMlab are compared with a commercial impedance analyzer IM3570 within the frequencies of $f = 100$ KHz. The results show that the maximum difference of the image correlation between Red Pitaya STEMlab and IM3570 is 5.36%. Finally, the EIT system is used to image the conductivity of eggs during heating process. These results verified that the developed portable EIT system with Red Pitaya STEMlab could measure the biological tissue in a high accuracy at low cost.
SP  - 8117
EP  - 8124
JF  - IEEE Sensors Journal
VL  - 18
IS  - 19
PB  - 
DO  - 10.1109/jsen.2018.2864539
ER  - 

TY  - NA
AU  - Borrego-Carazo, Juan; Castells-Rufas, David; Carrabina, Jordi; Biempica, Ernesto
TI  - DDECS - Capacitive-sensing module with dynamic gesture recognition for automotive applications
PY  - 2020
AB  - Capacitive sensing offers new possibilities for HMI product development. Its short range of interaction entails robustness against environmental noise and its flexibility for integration makes it a genuine technology for embedded systems. In the automotive context, capacitive sensing is explicitly devoted to driver interaction with car functionalities. However, the increasing complexity of captured signals and related interaction procedures impose severe difficulties for a classic modelling approach. Neural networks have demonstrated unbeatable performance in tasks with abundant data. Specifically, recurrent neural networks (RNNs) show excellent performance for tasks with inherent temporal structure. In this article, we develop a capacitive-sensing module that includes RNN-based dynamic gesture recognition, which has a suitable implementation size for embedded automotive applications.
SP  - 1
EP  - 6
JF  - 2020 23rd International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ddecs50862.2020.9095748
ER  - 

TY  - BOOK
AU  - Ververidis, Dimitrios; Karavarsamis, Sotirios; Nikolopoulos, Spiros; Kompatsiaris, Ioannis
TI  - MOCO - Pottery gestures style comparison by exploiting Myo sensor and forearm anatomy
PY  - 2016
AB  - In this paper we propose a set of Electromyogram (EMG) based features such as muscles total pressure, flexors pressure, tensors pressure, and gesture stiffness, for the purpose of identifying differences in performing the same gesture across three pottery constructions namely bowl, cylindrical vase, and spherical vase. In identifying these EMG-based features we have developed a tool for visualizing in real-time the signals generated from a Myo sensor along with the muscle activation level in 3D space. In order to do this, we have introduced an algorithm for estimating the activation level of each muscle based on the weighted sum of the 8 EMG signals captured by Myo. In particular, the weights are calculated as the distance of the muscle cross-sectional volumes at Myo plane level from each of the 8 Myo pods, multiplied by the muscle cross-section volume. Statistics estimated on an experimental dataset for the proposed features such as mean, variance, and percentiles, indicate that gestures such as "Raise clay" and "Form down cyclic clay" exhibit differences across the three vase types (i.e. bowl, cylinder, and sphere), although perceived as identical.
SP  - 3
EP  - NA
JF  - Proceedings of the 3rd International Symposium on Movement and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2948910.2948924
ER  - 

TY  - JOUR
AU  - Katsuragawa, Keiko; Kamal, Ankit; Liu, Qi Feng; Negulescu, Matei; Lank, Edward
TI  - Bi-Level Thresholding: Analyzing the Effect of Repeated Errors in Gesture Input
PY  - 2019
AB  - In gesture recognition, one challenge that researchers and developers face is the need for recognition strategies that mediate between false positives and false negatives. In this article, we examine bi-level thresholding, a recognition strategy that uses two thresholds: a tighter threshold limits false positives and recognition errors, and a looser threshold prevents repeated errors (false negatives) by analyzing movements in sequence. We first describe early observations that led to the development of the bi-level thresholding algorithm. Next, using a Wizard-of-Oz recognizer, we hold recognition rates constant and adjust for fixed versus bi-level thresholding; we show that systems using bi-level thresholding result in significantly lower workload scores on the NASA-TLX and significantly lower accelerometer variance when performing gesture input. Finally, we examine the effect that bi-level thresholding has on a real-world dataset of wrist and finger gestures, showing an ability to significantly improve measures of precision and recall. Overall, these results argue for the viability of bi-level thresholding as an effective technique for balancing between false positives, recognition errors, and false negatives.
SP  - 15
EP  - 30
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 9
IS  - 2
PB  - 
DO  - 10.1145/3181672
ER  - 

TY  - JOUR
AU  - Huppert, Felix; Holzl, Gerold; Kranz, Matthias
TI  - Design Different: Pen and Paper for Laser Cutting
PY  - 2019
AB  - Interdisciplinary teams and studies need new approaches to design prototypes using tools indistinguishable from the ones they are used to. We utilize a digital pen and physical paper, as the everyday life tools, to build a smart interface for laser-cutters, giving non-technical experienced people the possibility to rapidly, seamlessly, and collaboratively fabricate creative prototypes.
SP  - 29
EP  - 37
JF  - IEEE Pervasive Computing
VL  - 18
IS  - 4
PB  - 
DO  - 10.1109/mprv.2019.2938714
ER  - 

TY  - JOUR
AU  - Bhatt, Prahar M.; Malhan, Rishi K.; Shembekar, Aniruddha V.; Yoon, Yeo Jung; Gupta, Satyandra K.
TI  - Expanding capabilities of additive manufacturing through use of robotics technologies: A survey
PY  - 2020
AB  - Abstract Robots are versatile machines that can perform complex manipulation operations. Recent advances in industrial robotics make robots useful in a wide variety of manufacturing processes. Several recent efforts have demonstrated how robots can be used in additive manufacturing (AM) processes. This paper surveys the work focused on expanding the functional capabilities of AM processes using robots. We identify the following main capabilities realized by performing AM using robots: (1) multi-directional fabrication, (2) conformal deposition, (3) assembling prefabricated components in AM, (4) supportless AM, and (5) large-scale AM. We classify the recent literature in this area in terms of mechanisms, kinematic degrees of freedom (DOF) of the system, types of AM process, and materials. Finally, we discuss the limitations of the current work and the opportunities for future research in this area.
SP  - 100933
EP  - NA
JF  - Additive Manufacturing
VL  - 31
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2019.100933
ER  - 

TY  - NA
AU  - Hong, Freddie; Myant, Connor; Boyle, David
TI  - Thermoformed Circuit Boards: Fabrication of highly conductive freeform 3D printed circuit boards with heat bending
PY  - 2021
AB  - Fabricating 3D printed electronics using desktop printers has become more accessible with recent developments in conductive thermoplastic filaments. Because of their high resistance and difficulties in printing traces in vertical directions, most applications are restricted to capacitive sensing. In this paper, we introduce Thermoformed Circuit Board (TCB), a novel approach that employs the thermoformability of the 3D printed plastics to construct various double-sided, rigid and highly conductive freeform circuit boards that can withstand high current applications through copper electroplating. To illustrate the capability of the TCB, we showcase a range of examples with various shapes, electrical characteristics and interaction mechanisms. We also demonstrate a new design tool extension to an existing CAD environment that allows users to parametrically draw the substrate and conductive trace, and export 3D printable files. TCB is an inexpensive and highly accessible fabrication technique intended to broaden HCI researcher participation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445469
ER  - 

TY  - JOUR
AU  - Hedayatipour, Ava; Aslanzadeh, Shaghayegh; Hesari, Shahram Hatefi; Haque, Mohammad Aminul; McFarlane, Nicole
TI  - A Wearable CMOS Impedance to Frequency Sensing System for Non-Invasive Impedance Measurements
PY  - 2020
AB  - In this paper, we demonstrate a novel non-invasive, wearable impedance sensor. The impedance sensor, using an impedance to frequency measurement, with two modes of resistance and capacitance measurement is implemented in CMOS 130 nm technology. The sensor consisting of current and voltage comparators for different mode of measurement, has a low power consumption of 30 $ \mu$ W per channel. The sensor is demonstrated in two applications, thoracic impedance and hand gesture recognition. Thoracic impedance is based on impedance modulation through fluid accumulation. Hand gestures are detected through tissue impedance sensing. The full thoracic impedance sensing system is smaller than a credit card, low cost, and consumes 3 mW which includes the sensor, transmitter, and power control unit. Data received by this sensor can be easily transferred for further processing and, eventually, detection of heart failure. The electrodes were implemented using conductive paint, and the system was validated using passive loads to represent human tissue models and test subjects. The hand gesture system operates on 600 $\mu$ W with the maximum number of electrodes, and uses adhesive copper with electrical paint as electrodes.
SP  - 1108
EP  - 1121
JF  - IEEE transactions on biomedical circuits and systems
VL  - 14
IS  - 5
PB  - 
DO  - 10.1109/tbcas.2020.3025218
ER  - 

TY  - JOUR
AU  - Wang, Huxi; Zuo, Siming; Cerezo-Sánchez, María; Arekhloo, Negin Ghahremani; Nazarpour, Kianoush; Heidari, Hadi
TI  - Wearable super-resolution muscle-machine interfacing.
PY  - 2022
AB  - Muscles are the actuators of all human actions, from daily work and life to communication and expression of emotions. Myography records the signals from muscle activities as an interface between machine hardware and human wetware, granting direct and natural control of our electronic peripherals. Regardless of the significant progression as of late, the conventional myographic sensors are still incapable of achieving the desired high-resolution and non-invasive recording. This paper presents a critical review of state-of-the-art wearable sensing technologies that measure deeper muscle activity with high spatial resolution, so-called super-resolution. This paper classifies these myographic sensors according to the different signal types (i.e., biomechanical, biochemical, and bioelectrical) they record during measuring muscle activity. By describing the characteristics and current developments with advantages and limitations of each myographic sensor, their capabilities are investigated as a super-resolution myography technique, including: (i) non-invasive and high-density designs of the sensing units and their vulnerability to interferences, (ii) limit-of-detection to register the activity of deep muscles. Finally, this paper concludes with new opportunities in this fast-growing super-resolution myography field and proposes promising future research directions. These advances will enable next-generation muscle-machine interfaces to meet the practical design needs in real-life for healthcare technologies, assistive/rehabilitation robotics, and human augmentation with extended reality.
SP  - 1020546
EP  - NA
JF  - Frontiers in neuroscience
VL  - 16
IS  - NA
PB  - 
DO  - 10.3389/fnins.2022.1020546
ER  - 

TY  - NA
AU  - Yamaoka, Junichi; Niiyama, Ryuma; Kakehi, Yasuaki
TI  - UIST - BlowFab: Rapid Prototyping for Rigid and Reusable Objects using Inflation of Laser-cut Surfaces
PY  - 2017
AB  - This study proposes BlowFab, a prototyping method used to create a 2.5-dimensional prototype in a short time by combining laser cutting and blow molding techniques. The user creates adhesive areas and inflatable areas by engraving and cutting multilayered plastic sheets using a laser cutter. These adhesive areas are fused automatically by overlapping two crafted sheets and softening them with a heater. The user can then create hard prototypes by injecting air into the sheets. Objects can be bent in any direction by cutting incisions or engraving a resistant resin. The user can create uneven textures by engraving a pattern with a heat-resistant film. These techniques can be used for prototyping various strong inflatable objects. The finished prototype is strong and can be collapsed readily for storage when not required. In this study, the design process is described using the proposed method. The study also evaluates possible bending mechanisms and texture expression methods along with various usage scenarios and discusses the resolution, strength, and reusability of the prototype developed.
SP  - 461
EP  - 469
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126624
ER  - 

TY  - JOUR
AU  - Xu, Ke; Li, Yingguang; Chen, Lufeng; Tang, Kai
TI  - Curved Layer Based Process Planning for Multi-axis Volume Printing of Freeform Parts
PY  - 2019
AB  - NA
SP  - 51
EP  - 63
JF  - Computer-Aided Design
VL  - 114
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2019.05.007
ER  - 

TY  - JOUR
AU  - Muntoni, Alessandro; Livesu, Marco; Scateni, Riccardo; Sheffer, Alla; Panozzo, Daniele
TI  - Axis-Aligned Height-Field Block Decomposition of 3D Shapes
PY  - 2018
AB  - We propose a novel algorithm for decomposing general three-dimensional geometries into a small set of overlap-free height-field blocks, volumes enclosed by a flat base and a height-field surface defined with respect to this base. This decomposition is useful for fabrication methodologies such as 3-axis CNC milling, where a single milling pass can only carve a single height-field surface defined with respect to the machine tray but can also benefit other fabrication settings. Computing our desired decomposition requires solving a highly constrained discrete optimization problem, variants of which are known to be NP-hard. We effectively compute a high-quality decomposition by using a two-step process that leverages the unique characteristics of our setup. Specifically, we notice that if the height-field directions are constrained to the major axes, then we can always produce a valid decomposition starting from a suitable surface segmentation. Our method first produces a compact set of large, possibly overlapping, height-field blocks that jointly cover the model surface by recasting this discrete constrained optimization problem as an unconstrained optimization of a continuous function, which allows for an efficient solution. We then cast the computation of an overlap-free, final decomposition as an ordering problem on a graph and solve it via a combination of cycle elimination and topological sorting. The combined algorithm produces a compact set of height-field blocks that jointly describe the input model within a user given tolerance. We demonstrate our method on a range of inputs and showcase a number of real life models manufactured using our technique.
SP  - 169
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 5
PB  - 
DO  - 10.1145/3204458
ER  - 

TY  - JOUR
AU  - BAHÇE, Erkan; BOYACI, Sarkis; GÜLER, Mehmet Sami
TI  - The Effects of the Production Direction on the Part in Additive Manufacturing
PY  - 2022
AB  - <jats:p xml:lang="en">Additive manufacturing has become increasingly common in the last ten years for the production of designs that cannot be produced with traditional manufacturing methods or are very difficult and costly. Additive manufacturing is done by adding layer by layer in line with the production direction of the part. Support structures used in the manufacture of parts are a component of additive manufacturing. These structures form the base plate of the part, reduce thermal deformations and provide support for sagging that may occur on the surfaces. Therefore, the production direction of a part affects the quality, cost and other properties of the object. In this study, the effects of production direction on part integrity, geometric precision and surface roughness were investigated on a part designed to consist of flat, curved and angular surfaces. In this direction, roughness measurements were made from the upper surface of the samples produced and the effects of precision balance and support structures on the weight were examined. In addition, the images of the produced samples were transferred to the CAD environment and their geometric accuracy was investigated. The stability of the base plate and the sufficient cooling of the layers ensured the surface quality and geometric accuracy of the part. Inhomogeneous thermal stress on the base plate increases the deviation of the part from the nominal size.</jats:p>
SP  - 193
EP  - 203
JF  - Karadeniz Fen Bilimleri Dergisi
VL  - 12
IS  - 1
PB  - 
DO  - 10.31466/kfbd.1011338
ER  - 

TY  - JOUR
AU  - Leem, Seong Kyu; Khan, Faheem; Cho, Sung Ho
TI  - Detecting Mid-Air Gestures for Digit Writing With Radio Sensors and a CNN
PY  - 2020
AB  - In this paper, we classify digits written in mid-air using hand gestures. Impulse radio ultrawideband (IR-UWB) radar sensors are used for data acquisition, with three radar sensors placed in a triangular geometry. Conventional radar-based gesture recognition methods use whole raw data matrices or a group of features for gesture classification using convolutional neural networks (CNNs) or other machine learning algorithms. However, if the training and testing data differ in distance, orientation, hand shape, hand size, or even gesture speed or the radar setup environment, these methods become less accurate. To develop a more robust gesture recognition method, we propose not using raw data for the CNN classifier, but instead employing the hand’s mid-air trajectory for classification. The hand trajectory has a stereotypical shape for a given digit, regardless of the hand’s orientation or speed, making its classification easy and robust. Our proposed method consists of three stages: signal preprocessing, hand motion localization, and tracking and transforming the trajectory data into an image to classify it using a CNN. Our proposed method outperforms conventional approaches because it is robust to changes in orientation, distance, and hand shape and size. Moreover, this method does not require building a huge training database of digits drawn by different users in different orientations; rather, we can use training databases already available in the image processing field. Overall, the proposed mid-air handwritten digit recognition system provides a user-friendly and accurate mid-air handwriting modality that does not place restrictions on users.
SP  - 1066
EP  - 1081
JF  - IEEE Transactions on Instrumentation and Measurement
VL  - 69
IS  - 4
PB  - 
DO  - 10.1109/tim.2019.2909249
ER  - 

TY  - NA
AU  - Kim, Jeeeun; Guo, Anhong; Yeh, Tom; Hudson, Scott E.; Mankoff, Jennifer
TI  - Conference on Designing Interactive Systems - Understanding Uncertainty in Measurement and Accommodating its Impact in 3D Modeling and Printing
PY  - 2017
AB  - The growing accessibility of 3D printing to everyday users has led to the rapid adoption, sharing of 3D models on sites such as Thingiverse.com, and visions of a future in which customization is a norm and 3D printing can solve a variety of real-world problems. However, in practice, creating models is difficult and many end users simply print models created by others. In this paper, we explore a specific area of model design that is a challenge for end users' measurement. When a model must conform to a specific real world goal once printed, it is important that that goal is precisely specified. We demonstrate that measurement errors are a significant (yet often overlooked) challenge for end users through a systematic study of the sources and types of measurement errors. We argue for a new design principle--accommodating measurement error--that designers, as well as novice modelers, should to use at design time. We offer two strategies--buffer insertion and replacement of minimal parts--to help designers, as well as novice modelers, to build models that are robust to measurement error. We argue that these strategies can reduce the need for and costs of iteration and demonstrate their use in a series of printed objects.
SP  - 1067
EP  - 1078
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064690
ER  - 

TY  - NA
AU  - Dew, Kristin; Rosner, Daniela K.
TI  - Conference on Designing Interactive Systems - Designing with Waste: A Situated Inquiry into the Material Excess of Making
PY  - 2019
AB  - This paper describes a series of collaborative studio explorations in examining waste. We assembled a design team to explore how designers might conceive, handle, and rework the material left behind as waste within an on-campus makerspace and adjacent design labs. We turned discarded 3D printing filament into a reparative glue for broken prints and dissolved cardboard boxes into a medium for pollinator habitats. We describe how attending to waste involves understanding the relationships that define it, like how a material comes to be categorized as biodegradable but is impossible to break down in practice. Bringing this insight to the design context, we introduce the tactic of ecological inversions, experiments in reversing material flows to expose the wider infrastructure on which they depend. We discuss how ecological inversions could invite design researchers to notice the infrastructural relationships that exceed the physical limitations of the makerspace, revealing challenges around complicity and legibility.
SP  - 1307
EP  - 1319
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322320
ER  - 

TY  - NA
AU  - Nisser, Martin; Zhu, Junyi; Chen, Tianye; Bulovic, Katarina; Punpongsanon, Parinya; Mueller, Stefanie
TI  - Tangible and Embedded Interaction - Sequential Support: 3D Printing Dissolvable Support Material for Time-Dependent Mechanisms
PY  - 2019
AB  - In this paper, we propose a different perspective on the use of support material: rather than printing support structures for overhangs, our idea is to make use of its transient nature, i.e. the fact that it can be dissolved when placed in a solvent, such as water. This enables a range of new use cases, such as quickly dissolving and replacing parts of a prototype during design iteration, printing temporary assembly labels directly on the object that leave no marks when dissolved, and creating time-dependent mechanisms, such as fading in parts of an image in a shadow art piece or releasing relaxing scents from a 3D printed structure sequentially overnight. Since we use regular support material (PVA), our approach works on consumer 3D printers without any modifications. To facilitate the design of objects that leverage dissolvable support, we built a custom 3D editor plugin that includes a simulation showing how support material dissolves over time. In our evaluation, our simulation predicted geometries that are statistically similar to the example shapes within 10% error across all samples.
SP  - 669
EP  - 676
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295630
ER  - 

TY  - JOUR
AU  - Yoon, Hyoseok; Park, Ho; Lee, Kyung-Taek
TI  - Lightful user interaction on smart wearables
PY  - 2016
AB  - Smart wearables are body-worn small devices that require novel user interaction due to its compactness and wearability. Current UI/UX of smart wearables is rooted in a smartphone-like UI/UX that is inadequate in many cases constrained by such small form factors. To overcome these limitations, research efforts are invested for augmenting wearable devices with various sensors and improving efficiency of existing input modalities through careful orchestration. In this paper, we propose a new concept called lightful user interaction exploiting a readily available ambient light sensor as a novel and alternative user interface for smart wearables. We design and model lightful user interaction based on typical usages of representative smart wearables. Then, we demonstrate the proposed lightful user interaction through three implemented applications such as PIN entry, morse code, and control indicator, respectively. At the end, we evaluate the concept and applications in terms of occluded display area, input expressivity and lightweight implementation aspects to make a case for a promising novel and alternative UI for smart wearables.
SP  - 973
EP  - 984
JF  - Personal and Ubiquitous Computing
VL  - 20
IS  - 6
PB  - 
DO  - 10.1007/s00779-016-0959-z
ER  - 

TY  - NA
AU  - Kono, Michinari; Nakamura, Hiromi; Rekimoto, Jun
TI  - AH - Intentio: power distribution through a potentialized human body
PY  - 2017
AB  - We present Intentio, which allows a potentialized human body to activate low-power electronic devices by touching them. A potentialized human body, i.e., one carrying our device, acts as a common power source and distributes power to electronic devices through their body. An electronic device generally requires its own power source that needs to be managed individually. The diversity of battery sources, which must also be frequently charged or exchanged, has introduced complications for users who manage them. Providing a common power source may help users more easily manage electric devices because the devices are passive, with no power source of their own. This paper presents Intentio's design and implementation, followed by several proof-of-concept applications based on a potentialized human body. We also discuss the safety concerns of our method. Intentio is a concept and framework for augmenting humans so that they may interact with and handle electronic devices with power distribution, where the electric devices can passively exist without complex wiring or power sources.
SP  - 6
EP  - NA
JF  - Proceedings of the 8th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3041164.3041175
ER  - 

TY  - NA
AU  - Sun, Wei; Chen, Yanjun; Zhan, Simon; Han, Teng; Tian, Feng; Wang, Hongan; Yang, Xing-Dong
TI  - CHI - RElectrode: A Reconfigurable Electrode For Multi-Purpose Sensing Based on Microfluidics
PY  - 2021
AB  - In this paper, we propose a reconfigurable electrode, RElectrode, using a microfluidic technique that can change the geometry and material properties of the electrode to satisfy the needs for sensing a variety of different types of user input through touch/touchless gestures, pressure, temperature, and distinguish between different types of objects or liquids. Unlike the existing approaches, which depend on the specific-shaped electrode for particular sensing (e.g., coil for inductive sensing), RElectrode enables capacity, inductance, resistance/pressure, temperature, pH sensings all in a single package. We demonstrate the design and fabrication of the microfluidic structure of our RElectrode, evaluate its sensing performance through several studies, and provide some unique applications. RElectrode demonstrates technical feasibility and application values of integrating physical and biochemical properties of microfluidics into novel sensing interfaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445652
ER  - 

TY  - NA
AU  - Zhu, Junyi; Snowden, Jackson C.; Verdejo, Joshua; Chen, Emily; Zhang, Paul; Ghaednia, Hamid; Schwab, Joseph H.; Mueller, Stefanie
TI  - UIST (Adjunct Volume) - EIT-kit Demo: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing
PY  - 2021
AB  - In this paper, we propose EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1) an extension to a 3D editor for personalizing the form factor of electrode arrays and electrode distribution, (2) a customized EIT sensing motherboard for performing the measurements, (3) a microcontroller library that automates signal calibration and facilitates data collection, and (4) an image reconstruction library for mobile devices for interpolating and visualizing the measured data. Together, these EIT-kit components allow for applications that require 2- or 4-terminal setups, up to 64 electrodes, and single or multiple (up to four) electrode arrays simultaneously. We motivate the design of each component of EIT-kit with a formative study, and conduct a technical evaluation of the data fidelity of our EIT measurements. We demonstrate the design space that EIT-kit enables by showing various applications in health as well as motion sensing and control.
SP  - 100
EP  - 102
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474758
ER  - 

TY  - NA
AU  - Goudswaard, Maas; Abraham, Abel; da Rocha, Bruna Goveia; Andersen, Kristina; Liang, Rong-Hao
TI  - Conference on Designing Interactive Systems - FabriClick: Interweaving Pushbuttons into Fabrics Using 3D Printing and Digital Embroidery
PY  - 2020
AB  - Mechanical pushbuttons, which provide physical landmarks and clear tactile feedback, are easily accessible and highly reliable in eyes-free use. Potentially, their merits can improve the experiences of on-body or wearable HCI. However, they are not commonly adopted as a user interface of smart textiles because the physical mechanism of conventional pushbutton hardware requires further integration, which should be seamless enough to be comfortably worn. In this pictorial, we present a design exploration of the methodologies for interweaving mechanical pushbuttons into fabrics. The exploration used a frame system, which unifies the workflow of digital embroidery and 3D printing and enables the exploration of the physical design. Through the process, we investigated methods of integration and fabrication through making and presented our findings with proof-of-concept implementations. We also discussed the alternative designs and interaction methods as well as their implications to enlighten future research directions and opportunities.
SP  - 379
EP  - 393
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395569
ER  - 

TY  - JOUR
AU  - Byun, Sung-Woo; Lee, Seok-Pil
TI  - Implementation of Hand Gesture Recognition Device Applicable to Smart Watch Based on Flexible Epidermal Tactile Sensor Array.
PY  - 2019
AB  - Ever since the development of digital devices, the recognition of human gestures has played an important role in many Human-Computer interface applications. Various wearable devices have been developed, and inertial sensors, magnetic sensors, gyro sensors, electromyography, force-sensitive resistors, and other types of sensors have been used to identify gestures. However, there are different drawbacks for each sensor, which affect the detection of gestures. In this paper, we present a new gesture recognition method using a Flexible Epidermal Tactile Sensor based on strain gauges to sense deformation. Such deformations are transduced to electric signals. By measuring the electric signals, the sensor can estimate the degree of deformation, including compression, tension, and twist, caused by movements of the wrist. The proposed sensor array was demonstrated to be capable of analyzing the eight motions of the wrist, and showed robustness, stability, and repeatability throughout a range of experiments aimed at testing the sensor array. We compared the performance of the prototype device with those of previous studies, under the same experimental conditions. The result shows our recognition method significantly outperformed existing methods.
SP  - 692
EP  - NA
JF  - Micromachines
VL  - 10
IS  - 10
PB  - 
DO  - 10.3390/mi10100692
ER  - 

TY  - NA
AU  - Mutlu, Mehmet; Hauser, Simon; Bernardino, Alexandre; Ijspeert, Auke Jan
TI  - ICRA - Playdough to Roombots: Towards a Novel Tangible User Interface for Self-reconfigurable Modular Robots
PY  - 2018
AB  - One of the main strengths of self-reconfigurable modular robots (SRMR) is their ability to shape-shift and dynamically change their morphology. In the case of our SRMR system “Roombots”, these shapes can be quite arbitrary for a great variety of tasks while the major utility is envisioned to be self-reconfigurable furniture. As such, the ideas and inspirations from users quickly need to be translated into the final Roombots shape. This involves a multitude of separate processes and - most importantly - requires an intuitive user interface. Our current approach led to the development of a tangible user interface (TUI) which involves 3D-scanning of a shape formed by modeling clay and the necessary steps to prepare the digitized model to be formed by Roombots. The system is able to generate a solution in less than two minutes for our target use as demonstrated with various examples.
SP  - 1970
EP  - 1977
JF  - 2018 IEEE International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra.2018.8461248
ER  - 

TY  - NA
AU  - Sridhar, Srinath; Markussen, Anders; Oulasvirta, Antti; Theobalt, Christian; Boring, Sebastian
TI  - CHI - WatchSense: On- and Above-Skin Input Sensing through a Wearable Depth Sensor
PY  - 2017
AB  - This paper contributes a novel sensing approach to support on- and above-skin finger input for interaction on the move. WatchSense uses a depth sensor embedded in a wearable device to expand the input space to neighboring areas of skin and the space above it. Our approach addresses challenging camera-based tracking conditions, such as oblique viewing angles and occlusions. It can accurately detect fingertips, their locations, and whether they are touching the skin or hovering above it. It extends previous work that supported either mid-air or multitouch input by simultaneously supporting both. We demonstrate feasibility with a compact, wearable prototype attached to a user's forearm (simulating an integrated depth sensor). Our prototype---which runs in real-time on consumer mobile devices---enables a 3D input space on the back of the hand. We evaluated the accuracy and robustness of the approach in a user study. We also show how WatchSense increases the expressiveness of input by interweaving mid-air and multitouch for several interactive applications.
SP  - 3891
EP  - 3902
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - 271
IS  - 20
PB  - 
DO  - 10.1145/3025453.3026005
ER  - 

TY  - NA
AU  - Eggimann, Manuel; Erb, Jonas; Mayer, Philipp; Magno, Michele; Benini, Luca
TI  - Low Power Embedded Gesture Recognition Using Novel Short-Range Radar Sensors
PY  - 2019
AB  - This work proposes a low-power high-accuracy embedded hand-gesture recognition using low power short-range radar sensors. The hardware and software match the requirements for battery-operated wearable devices. A 2D Convolutional Neural Network (CNN) using range frequency Doppler features is combined with a Temporal Convolutional Neural Network (TCN) for time sequence prediction. The final algorithm has a model size of only 45723 parameters, yielding a memory footprint of only 91kB. Two datasets containing 11 challenging hand gestures performed by 26 different people have been recorded containing a total of 20210 gesture instances. On the 11 hands, gestures and an accuracy of 87% (26 users) and 92% (single user) have been achieved. Furthermore, the prediction algorithm has been implemented in the GAP8 Parallel Ultra-Low-Power processor by GreenWaves Technologies, showing that live-prediction is feasible with only 21mW of power consumption for the full gesture prediction neural network.
SP  - 8956617
EP  - NA
JF  - 2019 IEEE SENSORS
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sensors43011.2019.8956617
ER  - 

TY  - NA
AU  - Masson, Damien; Vermeulen, Jo; Fitzmaurice, George; Matejka, Justin
TI  - Supercharging Trial-and-Error for Learning Complex Software Applications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501895
ER  - 

TY  - NA
AU  - Wessely, Michael; Morenko, Nadiya; Steimle, Jürgen; Schmitz, Michael
TI  - UIST (Adjunct Volume) - Interactive Tangrami: Rapid Prototyping with Modular Paper-folded Electronics
PY  - 2018
AB  - Prototyping interactive objects with personal fabrication tools like 3D printers requires the maker to create subsequent design artifacts from scratch which produces unnecessary waste and does not allow to reuse functional components. We present Interactive Tangrami, paper-folded and reusable building blocks (Tangramis) that can contain various sensor input and visual output capabilities. We propose a digital design toolkit that lets the user plan the shape and functionality of a design piece. The software manages the communication to the physical artifact and streams the interaction data via the Open Sound protocol (OSC) to an application prototyping system (e.g. MaxMSP). The building blocks are fabricated digitally with a rapid and inexpensive ink-jet printing method. Our systems allows to prototype physical user interfaces within minutes and without knowledge of the underlying technologies. We demo its usefulness with two application examples.
SP  - 143
EP  - 145
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3271630
ER  - 

TY  - JOUR
AU  - Liu, Xin; Xian, Chuhua; Jin, Shuo; Li, Guiqing
TI  - Surface attributes driven volume segmentation for 3D-printing
PY  - 2021
AB  - NA
SP  - 43
EP  - 53
JF  - Computers & Graphics
VL  - 100
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.07.020
ER  - 

TY  - JOUR
AU  - Attene, Marco; Livesu, Marco; Lefebvre, Sylvain; Funkhouser, Thomas; Rusinkiewicz, Szymon; Ellero, Stefano; Martínez, Jonás; Bermano, Amit Haim
TI  - Design, Representations, and Processing for Additive Manufacturing
PY  - 2018
AB  - NA
SP  - 1
EP  - 146
JF  - Synthesis Lectures on Visual Computing
VL  - 10
IS  - 2
PB  - 
DO  - 10.2200/s00847ed1v01y201804vcp031
ER  - 

TY  - NA
AU  - Zhang, Ting; Hu, Zhenhong; Gupta, Aakar; Wu, Chi-Hao; Benko, Hrvoje; Jonker, Tanya R.
TI  - RIDS: Implicit Detection of a Selection Gesture Using Hand Motion Dynamics During Freehand Pointing in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545701
ER  - 

TY  - JOUR
AU  - Jensen, Walther; Craft, Brock; Löchtefeld, Markus; Bjørn, Pernille
TI  - Learning through interactive artifacts: Personal fabrication using electrochromic displays to remember Atari women programmers
PY  - 2022
AB  - Abstract In recent years makerspaces have gained traction as an environment where makers and tinkerers can freely create artefacts with digital fabrication tools. They are particularly suited for introducing new fabrication techniques because these spaces support hands-on experiences. Electrochromic displays are one such technology that has become possible to fabricate using new techniques and off-the-shelf tools which lends itself to be used in a workshop setting. Leveraging this development, we facilitated a makerspace workshop that introduced participants to this new technology. To limit the scope of the workshop outcome we used the little known history of female developers of video games (Atari) from the 1970s and 1980s as a design framing. The participants (undergraduates, 16 female, 2 male, aged 19–21 years) explored the Atari women’s role in development and through this exploration they created artifacts using novel electrochromic displays as designed responses. Throughout the workshop participants answered daily questionnaires and kept records of their progress. Our analysis of the questionnaires and the resulting projects suggests that having a relatable and meaningful context increases both motivation and engagement of the participants. We discuss the extrinsic motivations that enhance engagement, and provide suggestions for introducing new technologies in the makerspace context.
SP  - 100464
EP  - NA
JF  - Entertainment Computing
VL  - 40
IS  - NA
PB  - 
DO  - 10.1016/j.entcom.2021.100464
ER  - 

TY  - NA
AU  - Wu, Chenming; Dai, Chengkai; Fang, Guoxin; Liu, Yong-Jin; Wang, Charlie C. L.
TI  - ICRA - RoboFDM: A robotic system for support-free fabrication using FDM
PY  - 2017
AB  - This paper presents a robotic system — RoboFDM that targets at printing 3D models without support-structures, which is considered as the major restriction to the flexibility of 3D printing. The hardware of RoboFDM consists of a robotic arm providing 6-DOF motion to the platform of material accumulation and an extruder forming molten filaments of polylactic acid (PLA). The fabrication of 3D models in this system follows the principle of fused decomposition modeling (FDM). Different from conventional FDM, an input model fabricated by RoboFDM is printed along different directions at different places. A new algorithm is developed to decompose models into support-free parts that can be printed one by one in a collision-free sequence. The printing directions of all parts are also determined during the computation of model decomposition. Experiments have been successfully taken on our RoboFDM system to print general freeform objects in a support-free manner.
SP  - 1175
EP  - 1180
JF  - 2017 IEEE International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra.2017.7989140
ER  - 

TY  - NA
AU  - Wang, Yuntao; Sun, Ke; Sun, Lu; Yu, Chun; Shi, Yuanchun
TI  - UbiComp Adjunct - SkinMotion: what does skin movement tell us?
PY  - 2016
AB  - With the increasing popularity of wearable computing, emerging techniques allow novel interaction modalities to be transferred from portable devices to the human body itself. One promising approach is to appropriate the skin for input interface. While researches explore the potential of using the skin as an input surface, we show an alternative interaction modality - SkinMotion. SkinMotion reconstructs human motions from skin-stretching movements. In this workshop, we discuss the potential applications of SkinMotion. In addition, we explore one specific instance -- finger motion detection using the skin movement on the dorsum of the hand. Results show that SkinMotion can achieve 5.84° estimate error for proximal phalanx flexion on average. We expect SkinMotion to open new possibilities for skin-based interactions and to extend new boundaries of on-body technologies.
SP  - 914
EP  - 917
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2979132
ER  - 

TY  - NA
AU  - Scherer, Moritz; Magno, Michele; Erb, Jonas; Mayer, Philipp; Eggimann, Manuel; Benini, Luca
TI  - TinyRadarNN: Combining Spatial and Temporal Convolutional Neural Networks for Embedded Gesture Recognition with Short Range Radars
PY  - 2020
AB  - This work proposes a low-power high-accuracy embedded hand-gesture recognition algorithm targeting battery-operated wearable devices using low power short-range RADAR sensors. A 2D Convolutional Neural Network (CNN) using range frequency Doppler features is combined with a Temporal Convolutional Neural Network (TCN) for time sequence prediction. The final algorithm has a model size of only 46 thousand parameters, yielding a memory footprint of only 92 KB. Two datasets containing 11 challenging hand gestures performed by 26 different people have been recorded containing a total of 20,210 gesture instances. On the 11 hand gesture dataset, accuracies of 86.6% (26 users) and 92.4% (single user) have been achieved, which are comparable to the state-of-the-art, which achieves 87% (10 users) and 94% (single user), while using a TCN-based network that is 7500x smaller than the state-of-the-art. Furthermore, the gesture recognition classifier has been implemented on a Parallel Ultra-Low Power Processor, demonstrating that real-time prediction is feasible with only 21 mW of power consumption for the full TCN sequence prediction network, while a system-level power consumption of less than 100 mW is achieved. We provide open-source access to all the code and data collected and used in this work on this http URL.
SP  - NA
EP  - NA
JF  - arXiv: Signal Processing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhao, Donghua; Guo, Weizhong; Han, Youcheng
TI  - General Function sets theory-based type synthesis of the collaborative 3D printer for planar and non-planar printing
PY  - 2022
AB  - <jats:p> 3-dimension printing (3DP) or Additive Manufacturing promises rapid prototyping with 3D printers, the precondition for manufacturing complex components and surfaces. Usually, three-axis 3D printers are widely applied with planar slicing, limiting solid freeform fabrication with finite printing direction. Non-planar slicing (or curved layer slicing) has been put forward to break through the layerwise framework with infinite directions, requiring multi-axis 3D printers with more than four DOFs. However, as far as the authors know, the specific design method of 3D printers’ structure has seldom been studied for non-planar printing. Furthermore, there is no specified systematical research on the type synthesis of the 3D printer, which has two collaborative modules functioning together. One module serves as the build platform and the other as the print head attached moving platform. Hence, to obtain topological structures of the collaborative 3D printer for planar and non-planar printing, this paper investigates the type synthesis by introducing a research background at first. Then, the motion characteristics of the collaborative 3D printer are determined based on the planned paths’ geometric characteristics and motion requirements of the 3D printer. After that, General Function sets theory and the distribution principles are introduced, and the case study of 3D printers with 3T, 3T1R, and 3T2R is given. Finally, this research paper proposes a class of rotary 3D printers having 2T1R, 2T2R, and 2T3R innovatively. Furthermore, one prototype of the above case study is designed, and a preliminary experiment for planar and non-planar printing is carried out to verify the feasibility of the type synthesis’s result. In general, aiming at planar and non-planar printing, this paper presents a theory for type synthesis of collaborative 3D printers to offer a much richer set of kinematic structures than the most commonly used three Degree of Freedom printers. </jats:p>
SP  - 8965
EP  - 8979
JF  - Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science
VL  - 236
IS  - 16
PB  - 
DO  - 10.1177/09544062221094159
ER  - 

TY  - NA
AU  - Lim, Chin Guan; Tsai, Chin Yi; Chen, Mike Y.
TI  - Tangible and Embedded Interaction - MuscleSense: Exploring Weight Sensing using Wearable Surface Electromyography (sEMG)
PY  - 2020
AB  - Strength training improves overall health, well-being, physical appearance, and sports performance.There are four major factors that affect training efficacy in a training session: exercise type, number of repetitions, movement velocity, and workload. Prior research has used wearable sensors to detect exercise type, number of repetitions, and movement velocity while training. However, detecting workload remains constrained to instrumented exercise equipment, such as smart exercise machines or RFID-tagged free weights.This paper presents MuscleSense, an approach that estimates exercise workload by using wearable Surface Electromyography (sEMG) sensors and regression analysis. We evaluated the accuracy of several regression models and the effects of sensor placement through a 20-person user study. Results showed that MuscleSense achieved an accuracy of 0.68kg (root mean square error, RMSE) in sensing workload using both forearm and arm sensors and support vector regression (SVR).
SP  - 255
EP  - 263
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374943
ER  - 

TY  - NA
AU  - Han, Teng; Hasan, Khalad; Nakamura, Keisuke; Gomez, Randy; Irani, Pourang
TI  - UIST - SoundCraft: Enabling Spatial Interactions on Smartwatches using Hand Generated Acoustics
PY  - 2017
AB  - We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft's localization and classification capabilities in non-noisy and noisy environments.
SP  - 579
EP  - 591
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126612
ER  - 

TY  - JOUR
AU  - Eldakroury, Mohamed; Chen, Niechen; Frank, Matthew C.
TI  - A new method for locating candidate substrates for multi axis hybrid manufacturing systems
PY  - 2018
AB  - Purpose This study aims to introduce a new method for locating candidate substrates in part models and evaluating their feasibility. Design/methodology/approach Slices of an STL model along candidate directions are evaluated for the fitting of regular cylindrical and rectangular stock. Next, the part model is skeletonized and tested for collision assuming deposition growth of features from the candidate substrate. Findings The method is successfully able to find feasible substrates and conduct collision simulation for a variety of part models. Research limitations/implications The algorithm is limited to cylindrical and rectangular substrates and only considers collision between the substrate and the deposition head. Originality/value This method represents a new approach to solving a portion of the hybrid manufacturing process planning problem.
SP  - 237
EP  - 248
JF  - Rapid Prototyping Journal
VL  - 24
IS  - 2
PB  - 
DO  - 10.1108/rpj-12-2016-0213
ER  - 

TY  - NA
AU  - Schmitz, Martin; Steimle, Jürgen; Huber, Jochen; Dezfuli, Niloofar; Mühlhäuser, Max
TI  - CHI - Flexibles: Deformation-Aware 3D-Printed Tangibles for Capacitive Touchscreens
PY  - 2017
AB  - We introduce Flexibles: 3D-printed flexible tangibles that are deformation-aware and operate on capacitive touchscreens. Flexibles add expressive deformation input to interaction with on-screen tangibles. Based on different types of deformation mapping, we contribute a set of 3D-printable mechanisms that capture pressing, squeezing, and bending input with multiple levels of intensities. They can be integrated into 3D printed objects with custom geometries and on different locations. A Flexible is printed in a single pass on a consumer-level 3D printer without requiring further assembly. Through a series of interactive prototypes, example applications and a technical evaluation, we show the technical feasibility and the wide applicability of Flexibles.
SP  - 1001
EP  - 1014
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025663
ER  - 

TY  - NA
AU  - Katakura, Shohei; Kuroki, Yuto; Watanabe, Keita
TI  - UIST - A 3D Printer Head as a Robotic Manipulator
PY  - 2019
AB  - We introduce new ways of using a 3D printer head as a 3-axis robotic manipulator to enable advanced fabrication and usage such as assembling separately printed parts, breaking support materials and actuating printed objects on a build-plate. To achieve these manipulations, we customize a low-cost fused deposition modeling (FDM) 3D printer that can attach/detach printed end-effectors which change the function of the 3D printer head (e.g. grab, break, and rotate printed objects). These techniques afford the 3D printer to fabricate and assemble complete kinetic objects such as automatons without manual processing (i.e. removing support materials and assembling objects). We conclude that a small modification to a standard 3D printer, allows us to fabricate and assemble objects without human intervention.
SP  - 535
EP  - 548
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347885
ER  - 

TY  - NA
AU  - Parizi, Farshid Salemi; Whitmire, Eric; Cao, Alvin; Li, Tianke; Patel, Shwetak N.
TI  - UIST (Adjunct Volume) - Demo of AuraRing: Precise Electromagnetic Finger Tracking
PY  - 2019
AB  - We present AuraRing, a wearable electromagnetic tracking system for fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the finger. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.
SP  - 122
EP  - 124
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356893
ER  - 

TY  - NA
AU  - Ichinco, Michelle; Hnin, Wint; Kelleher, Caitlin
TI  - VL/HCC - Suggesting examples to novice programmers in an open-ended context with the example guru
PY  - 2016
AB  - Many novice programmers use blocks-based programming environments outside of classrooms, due to a lack of computer science education in schools. Many solutions for supporting learning in these environments are out of the context of the programming environment and the user's project. In this poster, we present a way of expanding novice users' knowledge of a blocks-based programming environment by suggesting examples during open-ended programming.
SP  - 230
EP  - 231
JF  - 2016 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2016.7739691
ER  - 

TY  - JOUR
AU  - Sánchez, Juan David Muñoz; Leyton, Víctor Hugo Mosquera
TI  - Tomography for electrical impedance
PY  - 2020
AB  - <jats:p>Introduction:This review article shows the state of the art of different techniques for monitoring joint injuries. This work is the product of the research project "Viability of electrical impedance tomography for the monitoring of joint injuries", which took place at the University of Cauca during the period 2018-2019.&#x0D; Aim:Identify non-invasive bio-image methods that are used in the evaluation of joint diseases.&#x0D; Methodology: Selection and review of papers related to the evaluation of joint injuries using non-invasive bio-image technologies using systematic mapping.&#x0D; Results: Magnetic resonance and computed tomography systems make up the non-invasive methods of greater reliability and application in the evaluation of joint injuries. Similarly, some studies show good results from other methods such as systems based on bio-impedance when monitoring the deterioration of joint cartilage. However, electrical impedance tomography (EIT) devices have not yet been widely studied in the joint injuries evaluation.&#x0D; Conclusion: Electronic prototypes of low-cost electrical impedance tomography have been developed that have allowed for the detection and recognition of gestures made by hand from the analysis of the distribution of conductivity in the wrist joint, which allows us to infer that EIT could be a good alternative for the monitoring of joint injuries.&#x0D; Originality: The literature does not show studies focused on the development and implementation of EIT systems in medical applications related to joint injuries.&#x0D; Limitations: This review paper only mentions those studies that describe the non-invasive bio-image methods used to evaluate joint diseases, including the medical applications of EIT systems.</jats:p>
SP  - NA
EP  - NA
JF  - Ingeniería Solidaria
VL  - 16
IS  - 1
PB  - 
DO  - 10.16925/2357-6014.2020.01.05
ER  - 

TY  - NA
AU  - Iravantchi, Yasha; Zhang, Yang; Bernitsas, Evi; Goel, Mayank; Harrison, Chris
TI  - CHI - Interferi: Gesture Sensing using On-Body Acoustic Interferometry
PY  - 2019
AB  - Interferi is an on-body gesture sensing technique using acoustic interferometry. We use ultrasonic transducers resting on the skin to create acoustic interference patterns inside the wearer's body, which interact with anatomical features in complex, yet characteristic ways. We focus on two areas of the body with great expressive power: the hands and face. For each, we built and tested a series of worn sensor configurations, which we used to identify useful transducer arrangements and machine learning fea-tures. We created final prototypes for the hand and face, which our study results show can support eleven- and nine-class gestures sets at 93.4% and 89.0% accuracy, re-spectively. We also evaluated our system in four continu-ous tracking tasks, including smile intensity and weight estimation, which never exceed 9.5% error. We believe these results show great promise and illuminate an inter-esting sensing technique for HCI applications.
SP  - 276
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300506
ER  - 

TY  - NA
AU  - Liu, Xin; Vega, Katia; Maes, Pattie; Paradiso, Joseph A.
TI  - AH - Wearability Factors for Skin Interfaces
PY  - 2016
AB  - As interfaces progress beyond wearables and into intrinsic human augmentation, the human body has become an increasingly important topic in the field of HCI. Wearables already act as a new layer of functionality located on the body that leads us to rethink the convergence between technology and fashion, not just in terms of the ability to wear, but also in how devices interact with us. Already, several options for wearable technology have emerged in the form of clothing and accessories. However, by applying sensors and other computing devices directly onto the body surface, wearables could also be designed as skin interfaces. In this paper, we review the wearability factors impacting wearables as clothes and accessories in order to discuss them in the context of skin interfaces. We classify these wearability factors in terms of body aspects (location, body movements and body characteristics) and device aspects (weight, attachment methods, accessibility, interaction, aesthetics, conductors, insulation, device care, connection, communication, battery life). We discuss these factors in the context of two different example skin interfaces: a rigid board embedded into special effects makeup and skin-mounted soft materials connected to devices.
SP  - 21
EP  - NA
JF  - Proceedings of the 7th Augmented Human International Conference 2016
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2875194.2875248
ER  - 

TY  - JOUR
AU  - Abasi, Sara; Bhat, Ankita; Guiseppi-Elie, Anthony
TI  - Electrode Selection for Electrostimulation and TEER Using ECSARA
PY  - 2020
AB  - NA
SP  - 2882
EP  - 2892
JF  - Electroanalysis
VL  - 32
IS  - 12
PB  - 
DO  - 10.1002/elan.202060313
ER  - 

TY  - NA
AU  - Noma, Yuta; Narumi, Koya; Okuya, Fuminori; Kawahara, Yoshihiro
TI  - UIST - Pop-up Print: Rapidly 3D Printing Mechanically Reversible Objects in the Folded State
PY  - 2020
AB  - Despite recent advancements in 3D printing technology, which allows users to rapidly produce 3D objects, printing tall and/or large objects still consumes more time and large amount of support material. In order to address these problems, we propose Pop-up Print, a method to 3D print an object in a compact "folded" state and then unfold it after printing to achieve the final artifact. Using this method, we can reduce the object's print height and volume, which directly affects the printing time and support material consumption. In addition, thanks to the reversibility of folding/unfolding, we can reversibly minimize the printed object's volume when unused for storage or transportation, and expand it only in use. To achieve Pop-up Print, we first conducted an experiment using selected printed sample objects with several parameters, in order to determine suitable crease patterns that make both the unfolded and folded state mechanically stable. Based on this result, we developed an interactive design tool to convert 3D models - such as a Stanford Bunny or a Huffman's cone - to the folded shape. Our design tool allows users to decide non-intuitive parameters that may affect the form's mechanical stability, while maintaining both functional crease patterns and the object's original form factor. Finally, we demonstrate the feasibility of our method through several examples of folded objects.
SP  - 58
EP  - 70
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415853
ER  - 

TY  - JOUR
AU  - Zhou, Yuqing; Nomura, Tsuyoshi; Saitou, Kazuhiro
TI  - Anisotropic Multicomponent Topology Optimization for Additive Manufacturing With Build Orientation Design and Stress-Constrained Interfaces
PY  - 2020
AB  - This paper presents a multicomponent topology optimization method for designing structures assembled from additively-manufactured components, considering anisotropic material behavior for each component due to its build orientation, distinct material behavior and stress constraint at component interfaces (i.e., joints). Based upon the multicomponent topology optimization (MTO) framework, the simultaneous optimization of structural topology, its partitioning, and the build orientations of each component is achieved, which maximizes an assembly-level structural stiffness performance subject to maximum stress constraints at component interfaces. The build orientations of each component are modeled by its orientation tensor that avoids numerical instability experienced by the conventional angular representation. A new joint model is introduced at component interfaces, which enables the identification of the interface location, the specification of a distinct material tensor, and imposing maximum stress constraints during optimization. Both 2D and 3D numerical examples are presented to illustrate the effect of the build orientation anisotropy and the component interface behavior on the resulting multicomponent assemblies.
SP  - NA
EP  - NA
JF  - Journal of Computing and Information Science in Engineering
VL  - 21
IS  - 1
PB  - 
DO  - 10.1115/1.4047487
ER  - 

TY  - JOUR
AU  - Mueller, Stefanie
TI  - 3D printing for human-computer interaction
PY  - 2017
AB  - <jats:p> Envisioning, designing, and implementing the user interface require a comprehensive understanding of interaction technologies. In this forum we scout trends and discuss new technologies with the potential to influence interaction design. <jats:bold>--- Albrecht Schmidt, Editor</jats:bold> </jats:p>
SP  - 76
EP  - 79
JF  - Interactions
VL  - 24
IS  - 5
PB  - 
DO  - 10.1145/3125399
ER  - 

TY  - NA
AU  - Chen, Xiang 'Anthony'
TI  - UIST (Adjunct Volume) - Making Fabrication Real
PY  - 2016
AB  - Low-cost, easy-to-use 3D printers have promised to empower everyday users with the ability to fabricate physical objects of their own design. While these printers specialize in building objects from scratch, they are innately oblivious to the real world in which the printed objects will be situated and in use. In my thesis research, I develop fabrication techniques with tool integration to enable users to expressively specify how a design can be attached to, augment, adapt, support, or otherwise function with existing real world objects. In this paper, I describe projects to date as well as ongoing work that explores this space of research.
SP  - 17
EP  - 20
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2984785
ER  - 

TY  - NA
AU  - Zhou, Yuqing; Nomura, Tsuyoshi; Saitou, Kazuhiro
TI  - Anisotropic multicomponent topology optimization for additive manufacturing with build orientation design and stress-constrained interfaces.
PY  - 2019
AB  - This paper presents a multicomponent topology optimization method for designing structures assembled from additively-manufactured components, considering anisotropic material behavior for each component due to its build orientation, distinct material behavior and stress constraint at component interfaces (i.e., joints). Based upon the multicomponent topology optimization (MTO) framework, the simultaneous optimization of structural topology, its partitioning, and the build orientations of each component is achieved, which maximizes an assembly-level structural stiffness performance subject to maximum stress constraints at component interfaces. The build orientations of each component are modeled by its orientation tensor that avoids numerical instability experienced by the conventional angular representation. A new joint model is introduced at component interfaces, which enables the identification of the interface location, the specification of a distinct material tensor, and imposing maximum stress constraints during optimization. Both 2D and 3D numerical examples are presented to illustrate the effect of the build orientation anisotropy and the component interface behavior on the resulting multicomponent assemblies.
SP  - NA
EP  - NA
JF  - arXiv: Computational Engineering, Finance, and Science
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Castelli, Kevin; Giberti, Hermes
TI  - A Preliminary 6 Dofs Robot Based Setup for Fused Deposition Modeling
PY  - 2018
AB  - This paper is aimed at describing a preliminary setup based on a robotic arm with 6 degrees of freedom that could be exploited to build 3D printed objects that better suit their specification. Standard Fused Deposition Modeling (FDM) 3D printers obtain the final object by superimposing a layer on top of another, along a unique direction. A robot arm has been used to experiment with deposition on differently oriented planes in order to build features that can act as reinforcement, customize existing builds, reconstruct damaged parts and remove the necessity for wasteful extrusions to generate supports. The initial steps from the design of the system to the preliminary experimental results will be addressed.
SP  - 249
EP  - 257
JF  - Mechanisms and Machine Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-03320-0_27
ER  - 

TY  - BOOK
AU  - Truong, Hoang; Nguyen, Phuc; Nguyen, Anh; Bui, Nam; Vu, Tam
TI  - WearSys@MobiSys - Capacitive Sensing 3D-printed Wristband for Enriched Hand Gesture Recognition
PY  - 2017
AB  - In this work, we design a wearable-form hand gesture recognition system using capacitive sensing technique. Our proposed system includes a 3D printed wristband, capacitive sensors arrays in a flexible circuit board, a low-cost micro-controller unit and wireless communication module (BLE). In particular, the wristband manipulates the changes in capacitance from multiple capacitive sensors to recognize and detect users' hand gestures. The software stack translates the detected gestures into control command for application layer, together with an user-friendly web interface that supports both data communication and training between the wristband and the host PC. We also release an open API of our designed system for future applications. Lastly, we envision our system open API will be available for developers to customize vast range of hand gesture and integrate the wristband into various applications, from command on remote computer to video game controller.
SP  - 11
EP  - 15
JF  - Proceedings of the 2017 Workshop on Wearable Systems and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3089351.3089359
ER  - 

TY  - NA
AU  - Kato, Kunihiro; Ishizuka, Hiroki; Kajimoto, Hiroyuki; Miyashita, Homei
TI  - CHI - Double-sided Printed Tactile Display with Electro Stimuli and Electrostatic Forces and its Assessment
PY  - 2018
AB  - Humans can perceive tactile sensation through multimodal stimuli. To demonstrate realistic pseudo tactile sensation for the users, a tactile display is needed that can provide multiple tactile stimuli. In this paper, we have explicated a novel printed tactile display that can provide both the electrical stimulus and the electrostatic force. The circuit patterns for each stimulus were fabricated by employing the technique of double-sided conductive ink printing. Requirements for the fabrication process were analyzed and the durability of the tactile display was evaluated. Users' perceptions of a single tactile stimulus and multiple tactile stimuli were also investigated. The obtained experimental results indicate that the proposed tactile display is capable of exhibiting realistic tactile sensation and can be incorporated by various applications such as tactile sensation printing of pictorial illustrations and paintings. Furthermore, the proposed hybrid tactile display can contribute to accelerated prototyping and development of new tactile devices.
SP  - 450
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174024
ER  - 

TY  - NA
AU  - Sun, Jiangtao; Lu, Xupeng; Sun, Shijie; Wang, Rui; Xie, Yuedong
TI  - Application of Multi-channel Impedance Measurement Device in Teaching of "Signal Analysis and Processing"
PY  - 2021
AB  - This article develops a low-cost multi-channel impedance measurement device as a teaching tool based on the electrical impedance tomography (EIT) technique. The device is applied for the teaching of "signal analysis and processing" related courses in the subject of instrumentation science and technology, which can reconstruct images of conductivity distributions within a domain of interest and recognize a variety of human gestures based on machine learning. This auxiliary teaching tool could help students deepen the understanding of core knowledge in the course, such as analog signal processing by operational amplification, signal sampling and discretization, discrete Fourier transform, and data classification through machine learning. The proposed teaching tool is easy to demonstrate during classes, which can help improve teaching efficiency and enhance students’ interests towards related knowledge.
SP  - 616
EP  - 620
JF  - 2021 2nd International Conference on Artificial Intelligence and Education (ICAIE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icaie53562.2021.00136
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Kruttwig, Klaus; Lee, Jaeyeon; Bennewitz, Roland; Arzt, Eduard; Steimle, Jürgen
TI  - CHI - Like A Second Skin: Understanding How Epidermal Devices Affect Human Tactile Perception
PY  - 2019
AB  - The emerging class of epidermal devices opens up new opportunities for skin-based sensing, computing, and interaction. Future design of these devices requires an understanding of how skin-worn devices affect the natural tactile perception. In this study, we approach this research challenge by proposing a novel classification system for epidermal devices based on flexural rigidity and by testing advanced adhesive materials, including tattoo paper and thin films of poly (dimethylsiloxane) (PDMS). We report on the results of three psychophysical experiments that investigated the effect of epidermal devices of different rigidity on passive and active tactile perception. We analyzed human tactile sensitivity thresholds, two-point discrimination thresholds, and roughness discrimination abilities on three different body locations (fingertip, hand, forearm). Generally, a correlation was found between device rigidity and tactile sensitivity thresholds as well as roughness discrimination ability. Surprisingly, thin epidermal devices based on PDMS with a hundred times the rigidity of commonly used tattoo paper resulted in comparable levels of tactile acuity. The material offers the benefit of increased robustness against wear and the option to re-use the device. Based on our findings, we derive design recommendations for epidermal devices that combine tactile perception with device robustness.
SP  - 380
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300610
ER  - 

TY  - JOUR
AU  - Ho, Jeffrey
TI  - ClothSurface: Exploring a Low-Cost Prototyping Tool to Support Ideation for Shape Displays
PY  - 2019
AB  - A shape-changing user interface is a type of interface that interacts with users by changing its physical form. Although researchers have been extensively studying shape-changing user interfaces, relevant research on its various design aspects—including tools and methods—remains limited. Prototyping shape-changing interfaces often requires sophisticated equipment and knowledge, which makes this sphere of design unwelcoming for designers with limited resources and technical knowledge (e.g., design students). In this study, we propose ClothSurface—a simple and low-cost prototyping tool to design for shape displays—and explore its use through a series of design sessions. The results reveal that ClothSurface can allow inexperienced designers to illustrate their ideas and to explore the design space of shape displays.
SP  - 64
EP  - NA
JF  - Multimodal Technologies and Interaction
VL  - 3
IS  - 3
PB  - 
DO  - 10.3390/mti3030064
ER  - 

TY  - NA
AU  - McIntosh, Jess; Marzo, Asier; Fraser, Mike
TI  - UIST - SensIR: Detecting Hand Gestures with a Wearable Bracelet using Infrared Transmission and Reflection
PY  - 2017
AB  - Gestures have become an important tool for natural interaction with computers and thus several wearables have been developed to detect hand gestures. However, many existing solutions are unsuitable for practical use due to low accuracy, high cost or poor ergonomics. We present SensIR, a bracelet that uses near-infrared sensing to infer hand gestures. The bracelet is composed of pairs of infrared emitters and receivers that are used to measure both the transmission and reflection of light through/off the wrist. SensIR improves the accuracy of existing infrared gesture sensing systems through the key idea of taking measurements with all possible combinations of emitters and receivers. Our study shows that SensIR is capable of detecting 12 discrete gestures with 93.3% accuracy. SensIR has several advantages compared to other systems such as high accuracy, low cost, robustness against bad skin coupling and thin form-factor.
SP  - 593
EP  - 597
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126604
ER  - 

TY  - JOUR
AU  - Tan, Baosen; Kuang, Shaolong; Li, Xiaoming; Cheng, Xiao; Duan, Wei; Zhang, Jinming; Liu, Wenyong; Fan, Yubo
TI  - Stereotactic technology for 3D bioprinting: from the perspective of robot mechanism.
PY  - 2021
AB  - Three-dimensional (3D) bioprinting has been widely applied in the field of biomedical engineering because of its rapidly individualized fabrication and precisely geometric designability. The emerging demand for bioprinted tissues/organs with bio-inspired anisotropic property is stimulating new bioprinting strategies. Stereotactic bioprinting is regarded as a preferable strategy for this purpose, which can perform bioprinting at the target position from any desired orientation in 3D space. In this work, based on the motion characteristics analysis of the stacked bioprinting technologies, mechanism configurations and path planning methods for robotic stereotactic bioprinting were investigated and a prototype system based on the double parallelogram mechanism was introduced in detail. Moreover, the influence of the time dimension on stereotactic bioprinting was discussed. Finally, technical challenges and future trends of stereotactic bioprinting within the field of biomedical engineering were summarized.
SP  - 043001
EP  - NA
JF  - Biofabrication
VL  - 13
IS  - 4
PB  - 
DO  - 10.1088/1758-5090/ac1846
ER  - 

TY  - JOUR
AU  - Dai, Chengkai; Wang, Charlie C. L.; Wu, Chenming; Lefebvre, Sylvain; Fang, Guoxin; Liu, Yong-Jin
TI  - Support-free volume printing by multi-axis motion
PY  - 2018
AB  - This paper presents a new method to fabricate 3D models on a robotic printing system equipped with multi-axis motion. Materials are accumulated inside the volume along curved tool-paths so that the need of supporting structures can be tremendously reduced - if not completely abandoned - on all models. Our strategy to tackle the challenge of tool-path planning for multi-axis 3D printing is to perform two successive decompositions, first volume-to-surfaces and then surfaces-to-curves. The volume-to-surfaces decomposition is achieved by optimizing a scalar field within the volume that represents the fabrication sequence. The field is constrained such that its iso-values represent curved layers that are supported from below, and present a convex surface affording for collision-free navigation of the printer head. After extracting all curved layers, the surfaces-to-curves decomposition covers them with tool-paths while taking into account constraints from the robotic printing system. Our method successfully generates tool-paths for 3D printing models with large overhangs and high-genus topology. We fabricated several challenging cases on our robotic platform to verify and demonstrate its capabilities.
SP  - 134
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201342
ER  - 

TY  - NA
AU  - Ran, Mengyuan; Wang, Shanshan; Liao, Jun; Zhang, Yuhao; Liu, Li
TI  - SMC - STGauntlet: Recognizing Hand Gestures over Multiple Hand-Worn Motion Sensors
PY  - 2020
AB  - Hand gesture recognition with wearables typically focuses on the characteristics of a single point on hand, but ignores the diversity of motion information over hand skeleton. As a result, current methods suffer from two key challenges to manage multiple hand joints: displacement detection and motion representation. This leads us to define a spatio-temporal framework, named STGauntlet, that explicitly characterizes the hand motion context of spatio-temporal relations among multiple joints and detects hand gestures in real-time. The framework introduces the Lie algebra to capture the inherent structural varieties of hand motions with spatio-temporal dependencies among multiple joints. In addition, we developed a hand-worn prototype with multiple motion sensors respectively attached to various joints on hand and collected 7000 samples of seven gestures from nine subjects. Our in-lab study shows that STGauntlet is capable of detecting gesture types together with their 3D tracking trajectory with 97.35% and 95.17% accuracies for subject dependent and independent recognition, respectively.
SP  - 3340
EP  - 3345
JF  - 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smc42975.2020.9282843
ER  - 

TY  - JOUR
AU  - Atitallah, Bilel Ben; Hu, Zheng; Bouchaala, Dhouha; Hussain, Mohammed Abrar; Ismail, Amir; Derbel, Nabil; Kanoun, Olfa
TI  - Hand Sign Recognition System based on EIT Imaging and Robust CNN Classification
PY  - 2022
AB  - Hand sign recognition is gaining importance in human-human and in human-machine communication and interaction. Electrical Impedance Tomography (EIT) is thereby very interesting as it provides information on impedance changes in depth of the section of the arm, which infers muscle contractions. This paper introduces an EIT imaging system for hand sign recognition and monitoring having a low complexity and including an electronic interface with 8 electrodes placed on the forearm, a Gauss-Newton image reconstruction algorithm, a robust CNN based hand sign classification and a virtual hand model for visualization. A database has been collected for EIT measurements in pole mode taken by eight subjects performing the American sign language numbers from 0 to 9. The overall imaging system is validated using a water tank system, where conductive objects can be changed in properties and positions. The correspondence between the reconstructed images and the expected muscle behavior for the hand signs is investigated. A robust Convolutional Neural Network (CNN) classification algorithm was implemented and optimized by implementing an Adam optimizer and conducting a dedicated study to avoid overfitting. The results obtained by CNN are compared to the results by a Support Vector Machine (SVM), and a Softmax classifier. They show a classification accuracy of 95.94%, 75.61%, and 62.9% respectively. In term of subject dependency, the system using the CNN model shows a higher performance, as the accuracy decreases only by 0.72% while increasing the number of subjects from one to eight. Finally, for visualization, a 3D virtual hand model is designed and controlled based on detected hand signs.
SP  - 1729
EP  - 1737
JF  - IEEE Sensors Journal
VL  - 22
IS  - 2
PB  - 
DO  - 10.1109/jsen.2021.3130982
ER  - 

TY  - NA
AU  - Tao, Ye; Do, Youngwook; Yang, Humphrey; Lee, Yi-Chin; Wang, Guanyun; Catherine, Mondoa; Cui, Jianxun; Wen, Wang; Yao, Lining
TI  - UIST - Morphlour: Personalized Flour-based Morphing Food Induced by Dehydration or Hydration Method
PY  - 2019
AB  - In this paper, we explore personalized morphing food that enhances traditional food with new HCI capabilities, rather than replacing the chef and authentic ingredients (e.g. flour) with an autonomous machine and heterogeneous mixtures (e.g. gel). Thus, we contribute a unique transformation mechanism of kneaded and sheeted flour-based dough, with an integrated design strategy for morphing food during two general cooking methods: dehydration (e.g. baking) or hydration (e.g. water boiling). We also enrich the design space of morphing food by demonstrating several applications. We end by discussing hybrid cooking between human and a design tool that we developed to ensure accuracy while preserving customizability for morphing food.
SP  - 329
EP  - 340
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347949
ER  - 

TY  - JOUR
AU  - Jiang, Jingchao; Fu, Yun-Fei
TI  - A short survey of sustainable material extrusion additive manufacturing
PY  - 2020
AB  - Additive manufacturing (AM) technology has been developed for more than thirty years and has become a mainstream manufacturing process. Sustainability is becoming increasingly significant for human...
SP  - 1
EP  - 10
JF  - Australian Journal of Mechanical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/14484846.2020.1825045
ER  - 

TY  - NA
AU  - Wu, Chenming; Liu, Yong-Jin; Wang, Charlie C. L.
TI  - Learning to Accelerate Decomposition for Multi-Directional 3D Printing
PY  - 2020
AB  - Multi-directional 3D printing has the capability of decreasing or eliminating the need for support structures. Recent work proposed a beam-guided search algorithm to find an optimized sequence of plane-clipping, which gives volume decomposition of a given 3D model. Different printing directions are employed in different regions to fabricate a model with tremendously less support (or even no support in many cases).To obtain optimized decomposition, a large beam width needs to be used in the search algorithm, leading to a very time-consuming computation. In this paper, we propose a learning framework that can accelerate the beam-guided search by using a smaller number of the original beam width to obtain results with similar quality. Specifically, we use the results of beam-guided search with large beam width to train a scoring function for candidate clipping planes based on six newly proposed feature metrics. With the help of these feature metrics, both the current and the sequence-dependent information are captured by the neural network to score candidates of clipping. As a result, we can achieve around 3x computational speed. We test and demonstrate our accelerated decomposition on a large dataset of models for 3D printing.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Hamdan, Nur Al-huda; Heller, Florian; Wacharamanotham, Chat; Thar, Jan; Borchers, Jan
TI  - CHI Extended Abstracts - Grabrics: A Foldable Two-Dimensional Textile Input Controller
PY  - 2016
AB  - Textile interfaces can be ubiquitously integrated into the fabrics that already surround us. So far, existing interfaces transfer concepts, such as buttons and sliders, to the textile domain without leveraging the affordances and qualities of fabric. This paper presents Grabrics, a two-dimensional textile sensor that is manipulated by grabbing a fold and moving it between your fingers. Grabrics can be integrated invisibly into everyday clothing or into textile objects, like a living room sofa, while minimizing accidental activation. We describe the construction and the fold-based interaction technique of our Grabrics sensor. A preliminary study shows that Grabrics can be folded and manipulated from any arbitrary position, and it can detect 2D stroke gestures.
SP  - 2497
EP  - 2503
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892529
ER  - 

TY  - JOUR
AU  - Wu, Chenming; Dai, Chengkai; Fang, Guoxin; Liu, Yong-Jin; Wang, Charlie C. L.
TI  - General Support-Effective Decomposition for Multi-Directional 3-D Printing
PY  - 2020
AB  - We present a method for fabricating general models with multi-directional 3-D printing systems by printing different model regions along with different directions. The core of our method is a support-effective volume decomposition algorithm that minimizes the area of the regions with large overhangs. A beam-guided searching algorithm with manufacturing constraints determines the optimal volume decomposition, which is represented by a sequence of clipping planes. While current approaches require manually assembling separate components into a final model, our algorithm allows for directly printing the final model in a single pass. It can also be applied to models with loops and handles. A supplementary algorithm generates special supporting structures for models where supporting structures for large overhangs cannot be eliminated. We verify the effectiveness of our method using two hardware systems: a Cartesian-motion-based system and an angular-motion-based system. A variety of 3-D models have been successfully fabricated on these systems. Note to Practitioners —In conventional planar-layer-based 3-D printing systems, supporting structures need to be added at the bottom of large overhanging regions to prevent material collapse. Supporting structures used in single-material 3-D printing technologies have three major problems: being difficult to remove, introducing surface damage, and wasting material. This article introduces a method to improve 3-D printing by adding rotation during the manufacturing process. To keep the hardware system relatively inexpensive, the hardware, called a multi-directional 3-D printing system , only needs to provide unsynchronized rotations. In this system, models are subdivided into different regions, and then, the regions are printed in different directions. We develop a general volume decomposition algorithm for effectively reducing the area that needs supporting structures. When supporting structures cannot be eliminated, we provide a supplementary algorithm for generating supports compatible with multi-directional 3-D printing. Our method can speed up the process of 3-D printing by saving time in producing and removing supports.
SP  - 599
EP  - 610
JF  - IEEE Transactions on Automation Science and Engineering
VL  - 17
IS  - 2
PB  - 
DO  - 10.1109/tase.2019.2938219
ER  - 

TY  - JOUR
AU  - Livesu, Marco; Ellero, Stefano; Martínez, Jonàs; Lefebvre, Sylvain; Attene, Marco
TI  - From 3D Models to 3D Prints: an Overview of the Processing Pipeline
PY  - 2017
AB  - Due to the wide diffusion of 3D printing technologies, geometric algorithms for Additive Manufacturing are being invented at an impressive speed. Each single step, in particular along the Process Planning pipeline, can now count on dozens of methods that prepare the 3D model for fabrication, while analysing and optimizing geometry and machine instructions for various objectives. This report provides a classification of this huge state of the art, and elicits the relation between each single algorithm and a list of desirable objectives during Process Planning. The objectives themselves are listed and discussed, along with possible needs for tradeoffs. Additive Manufacturing technologies are broadly categorized to explicitly relate classes of devices and supported features. Finally, this report offers an analysis of the state of the art while discussing open and challenging problems from both an academic and an industrial perspective.
SP  - 537
EP  - 564
JF  - Computer Graphics Forum
VL  - 36
IS  - 2
PB  - 
DO  - 10.1111/cgf.13147
ER  - 

TY  - JOUR
AU  - Leung, Ho-Man Colman; Fu, Chi-Wing; Heng, Pheng-Ann
TI  - TwistIn: Tangible Authentication of Smart Devices via Motion Co-analysis with a Smartwatch
PY  - 2018
AB  - Smart devices contain sensitive information that has to be guarded against unauthorized access through authentication. Existing authentication methods become obsolete as they are designed either for logging-in one device at a time or are ineffective in a multi-user multi-device environment. This paper presents TwistIn, a simple gesture that takes a smartwatch as an authentication token for fast access and control of other smart devices. Our mechanism is particularly useful for devices such as smartphones, smart glasses, and small IoT objects. To log in a device, one simply need to pick it up and twist it a few times. Then, by co-analyzing the motion data from the device and the watch, our method can extend the user authentication on the watch to the device. This is a simple and tangible interaction that takes only one to two seconds to perform. Furthermore, to account for user variation in wrist bending, we decompose wrist and forearm rotations via an optimization to improve the method accuracy. We implemented TwistIn, collected thousands of gesture samples, and conducted various experiments to evaluate our prototype system and show that it achieved over 95% detection accuracy.
SP  - 72
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 2
PB  - 
DO  - 10.1145/3214275
ER  - 

TY  - NA
AU  - Fossdal, Frikk H.; Heldal, Rogardt; Dyvik, Jens; Rutle, Adrian
TI  - MoDELS - A parametric model for creating customized fabrication machines
PY  - 2020
AB  - Digital fabrication tools such as 3D printers, computer-numerically controlled (CNC) milling machines, and laser cutters are becoming increasingly available, ranging from consumer to industrial versions. Although these tools have enabled a completely new set of users to take part in manufacturing, they are still limited in their use and the workflows they provide. As an answer to this, users are modifying and customizing their machines by changing the work envelope and adding different end-effectors. However, customizing, modifying and creating digital fabrication machines require extensive knowledge within multiple different engineering domains, and is non-trivial. In this paper, we present The Fabricatable Axis, a high-level parametric model that aims to simplify the process of experimenting, customizing and implementing digital fabrication machines. This model encapsulates the knowledge of an experienced machine designer into a model that less experienced machine builders can use to design and customize linear and rotary motion axes which can be combined into different machines. The model receives high-level input parameters such as axis type, length and speed-parameters, and outputs a CAD model of a motion axis consisting of fabricatable parts (parts that are readily available or parts that can be fabricated using accessible tools such as a CNC milling machine). To validate our contribution, we first present a constructed scenario were we use the model to implement a specific machine. Secondly, we present an evaluation of our tool through a series of interviews with users who have been using the model to create different types of machines.
SP  - 143
EP  - 153
JF  - Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365438.3410960
ER  - 

TY  - JOUR
AU  - Gbouna, Zakka Vincent; Pang, Gaoyang; Yang, Geng; Hou, Zeyang; Lv, Honghao; Yu, Zhangwei; Pang, Zhibo
TI  - User-Interactive Robot Skin with Large-Area Scalability for Safer and Natural Human-Robot Collaboration in Future Telehealthcare.
PY  - 2021
AB  - With the fourth revolution of healthcare, i.e., Healthcare 4.0, collaborative robotics is spilling out from traditional manufacturing and will blend into human living or working environments to deliver care services, especially telehealthcare. Because of the frequent and seamless interaction between robots and care recipients, it poses several challenges that require careful consideration: 1) the ability of the human to collaborate with the robots in a natural manner; and 2) the safety of the human collaborating with the robot. In this regard, we have proposed a proximity sensing solution based on the self-capacitive technology to provide an extended sense of touch for collaborative robots, allowing approach and contact measurement to enhance safe and natural human-robot collaboration. The modular design of our solution enables it to scale up to form a large-area sensing system. The sensing solution is proposed to work in two operation modes: the interaction mode and the safety mode. In the interaction mode, utilizing the ability of the sensor to localize the point of action, gesture command is used for robot manipulation. In the safety mode, the sensor enables the robot to actively avoid obstacles.
SP  - 1
EP  - 1
JF  - IEEE journal of biomedical and health informatics
VL  - 25
IS  - 12
PB  - 
DO  - 10.1109/jbhi.2021.3082563
ER  - 

TY  - NA
AU  - Ojuroye, Olivia; Torah, Russel; Beeby, Steve; Wilde, Adriana
TI  - UbiComp Adjunct - Autonomy is the key: from smart towards intelligent textiles
PY  - 2016
AB  - Electronic textiles become smart by embedding circuits and sensors which offer some passive or active capabilities. Smart textiles become intelligent due to their computational abilities allowing awareness of their environment, extract input data from it, and consequently demonstrate untaught behaviours. Intelligent systems require machine intelligence through artificial intelligence algorithms to complete these input data manipulations. However, producing intelligent electronic textiles is a current research challenge. Hypothesising their eventuality and ubiquity, challenges such as remote communication, power generation, data processing, security, and ethics arise. In what remains we focus on the ethical implications and approaches to risk mitigation.
SP  - 678
EP  - 681
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2968558
ER  - 

TY  - NA
AU  - Xiuyan, Li; Jianrui, Sun; Qi, Wang; Xiaojie, Duan; Jianming, Wang
TI  - EIT Gesture Recognition Method With Stronger Robustness
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 2nd International Conference on Frontiers of Electronics, Information and Computation Technologies (ICFEICT)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icfeict57213.2022.00015
ER  - 

TY  - JOUR
AU  - Ma, Gang; Hao, Zhiliang; Wu, Xuan; Wang, Xiaojie
TI  - An Optimal Electrical Impedance Tomography Drive Pattern for Human-Computer Interaction Applications
PY  - 2020
AB  - In this article, we presented an optimal Electrical Impedance Tomography (EIT) drive pattern based on feature selection and model explanation, and proposed a portable EIT system for applications in human-computer interaction for gesture recognition and contact detection, which can reduce the measurement time and realize a performance trade-off between the accuracy and the time response. In our experiment, eleven hand gestures were designed to verify the proposed approach and EIT system. Compared to the traditional eight-electrode method, the optimal electrode drive pattern achieved a recognition accuracy of 97.5% with seven electrodes and the measurement time was reduced by 60%. To illustrate the universality of this method, we performed a contact detection experiment. By setting seven labels on the conductive panel and using optimal electrode drive pattern, the detection accuracy reached 100% with seven electrodes and the measurement time was reduced by 85%.
SP  - 402
EP  - 411
JF  - IEEE transactions on biomedical circuits and systems
VL  - 14
IS  - 3
PB  - 
DO  - 10.1109/tbcas.2020.2967785
ER  - 

TY  - NA
AU  - Jensen, Walther; Löchtefeld, Markus
TI  - ECPlotter: A Toolkit for Rapid Prototyping of Electrochromic Displays
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3568466
ER  - 

TY  - NA
AU  - Abdullah, Muhammad; Sommerfeld, Romeo; Sievers, Bjarne; Geier, Leonard; Noack, Jonas; Ding, Marcus; Thieme, Christoph; Seidel, Laurenz; Fritzsche, Lukas; Langenhan, Erik; Adameck, Oliver; Dzingel, Moritz; Kern, Thomas; Taraz, Martin; Lempert, Conrad; Katakura, Shohei; Elhassany, Hany Mohsen; Roumen, Thijs; Baudisch, Patrick
TI  - HingeCore: Laser-Cut Foamcore for Fast Assembly
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545618
ER  - 

TY  - CHAP
AU  - Liu, Xiaodong; Zheng, Enhao
TI  - ICIRA (2) - Gesture Recognition and Conductivity Reconstruction Parameters Analysis with an Electrical-Impedance-Tomography (EIT) Based Interface: Preliminary Results.
PY  - 2021
AB  - With the development of Human-machine interface (HMI), the requirements of perceiving the human intention are much higher. Electrical Impedance Tomography (EIT) is a promising alternative to existing HMIs because of its portability, non-invasiveness and inexpensiveness. In this study, we designed an EIT-based gesture recognition method achieving the recognition of 9 forearm motion patterns. We analysed the parameters, including current level and contact impedance, which are relevant for practical applications in robotic control. The gesture recognition method produced an average accuracy of 99.845% over nine gestures with PCA and QDA model on one subject. The preliminary results of parameter analysis suggested that the resolution increased with the current amplitude less than a threshold (5.5 mA) but decreased when the current amplitude was over 5.5 mA. The mean value of Region of Interest (ROI) nodes didn’t change obviously when the contact impedance increased. In future works, extensive studies will be conducted on the priori information of forearm and biological-model-based methods to further improve recognition performances in more complicated tasks.
SP  - 25
EP  - 35
JF  - Intelligent Robotics and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-89098-8_3
ER  - 

TY  - CHAP
AU  - Zhao, Yan; Sugiura, Yuta; Tada, Mitsunori; Mitani, Jun
TI  - ICEC - InsTangible: A Tangible User Interface Combining Pop-up Cards with Conductive Ink Printing
PY  - 2017
AB  - We propose a tangible user interface combining pop-up cards with conductive ink printing technique. To fulfill our purpose, we developed software that can automatically calculate the circuit layouts on the pop-up card. The circuit layout is made to have the shortest path by considering the thickness of the layout, the space between circuit lines, and that the layout should not crossover cut lines. The crease pattern and layouts are printed separately when the layouts are calculated. By attaching electronic elements, we can make an interactive pop-up card. In a use trial, we confirmed that even novice could create original interactive pop-up card with our software and use such card to build an application. Furthermore, we demonstrated the effectiveness of our method by making and showing example applications using the interactive pop-up card interface.
SP  - 72
EP  - 80
JF  - Entertainment Computing – ICEC 2017
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-66715-7_8
ER  - 

TY  - NA
AU  - Zhao, Tianming; Liu, Jian; Wang, Yan; Liu, Hongbo; Chen, Yingying
TI  - INFOCOM - PPG-based Finger-level Gesture Recognition Leveraging Wearables
PY  - 2018
AB  - This paper subverts the traditional understanding of Photoplethysmography (PPG) and opens up a new direction of the utility of PPG in commodity wearable devices, especially in the domain of human computer interaction of fine-grained gesture recognition. We demonstrate that it is possible to leverage the widely deployed PPG sensors in wrist-worn wearable devices to enable finger-level gesture recognition, which could facilitate many emerging human-computer interactions (e.g., sign-language interpretation and virtual reality). While prior solutions in gesture recognition require dedicated devices (e.g., video cameras or IR sensors) or leverage various signals in the environments (e.g., sound, RF or ambient light), this paper introduces the first PPG-based gesture recognition system that can differentiate fine-grained hand gestures at finger level using commodity wearables. Our innovative system harnesses the unique blood flow changes in a user's wrist area to distinguish the user's finger and hand movements. The insight is that hand gestures involve a series of muscle and tendon movements that compress the arterial geometry with different degrees, resulting in significant motion artifacts to the blood flow with different intensity and time duration. By leveraging the unique characteristics of the motion artifacts to PPG, our system can accurately extract the gesture-related signals from the significant background noise (i.e., pulses), and identify different minute finger-level gestures. Extensive experiments are conducted with over 3600 gestures collected from 10 adults. Our prototype study using two commodity PPG sensors can differentiate nine finger-level gestures from American Sign Language with an average recognition accuracy over 88%, suggesting that our PPG-based finger-level gesture recognition system is promising to be one of the most critical components in sign language translation using wearables.
SP  - 1457
EP  - 1465
JF  - IEEE INFOCOM 2018 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom.2018.8486006
ER  - 

TY  - JOUR
AU  - Navarre, David; Palanque, Philippe; Coppers, Sven; Luyten, Kris; Vanacken, Davy
TI  - Model-based Engineering of Feedforward Usability Function for GUI Widgets
PY  - 2021
AB  - Abstract Feedback and feedforward are two fundamental mechanisms that support users’ activities while interacting with computing devices. While feedback can be easily solved by providing information to the users following the triggering of an action, feedforward is much more complex as it must provide information before an action is performed. For interactive applications where making a mistake has more impact than just reduced user comfort, correct feedforward is an essential step toward correctly informed, and thus safe, usage. Our approach, Fortunettes, is a generic mechanism providing a systematic way of designing feedforward addressing both action and presentation problems. Including a feedforward mechanism significantly increases the complexity of the interactive application hardening developers’ tasks to detect and correct defects. We build upon an existing formal notation based on Petri Nets for describing the behavior of interactive applications and present an approach that allows for adding correct and consistent feedforward.
SP  - 73
EP  - 91
JF  - Interacting with Computers
VL  - 33
IS  - 1
PB  - 
DO  - 10.1093/iwcomp/iwab014
ER  - 

TY  - JOUR
AU  - Kao, Hsin-Liu Cindy; Bedri, Abdelkareem; Lyons, Kent
TI  - SkinWire: Fabricating a Self-Contained On-Skin PCB for the Hand
PY  - 2018
AB  - Current wearable form factors often house electronics using an enclosure that is attached to the body. This form factor, while wearable, tends to protrude from the body and therefore can limit wearability. While emerging research in on-skin interfaces from the HCI and wearable communities have generated form factors with lower profiles, they often still require support by conventional electronics and associated form factors for the microprocessor, wireless communication, and battery units. In this work, we introduce SkinWire, a fabrication approach that extends the early work in on-skin interfaces to shift wearable devices from their traditional box-like forms to a fully self-contained on-skin form factor. The Skin Wire approach starts with the placement of electronic components into individual PCB islands, which are then distributed over the body surface. The islands are connected through a novel skin-wiring approach that deposits conformal multi-stranded metallic wires on thin silicon substrates through a sewing-based technique. The process affords on-skin interfaces with the needed wiring in limited surface areas. We exemplify the capacity of this approach by shifting an IMU-based hand gesture system - which traditionally come in bulky glove-based form factors - directly onto the skin. Inspired by the emerging body art trend of body wiring, the Skin Wire approach uses readily accessible materials and affords aesthetic customization. We evaluate fabrication parameters, and conduct a user study to uncover wearability concerns.
SP  - 116
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 3
PB  - 
DO  - 10.1145/3264926
ER  - 

TY  - JOUR
AU  - Muñoz, Juan D; Mosquera, Víctor H; Rengifo, Carlos F
TI  - A low-cost, portable, two-dimensional bioimpedance distribution estimation system based on the AD5933 impedance converter.
PY  - 2022
AB  - This study proposes a low-cost, portable, eight-channel electrical impedance tomograph based on the AD5933 impedance converter. The patterns for current injection and voltage measurement are managed by an Arduino Mega 2560 board and four 74HC4067 Texas Instruments multiplexers. Regarding the experimental results, the errors in the impedance estimates of an electrical circuit that represents a Cole model were less than 1.14% for the magnitude and 4.15% for the phase. Furthermore, the signal-to-noise ratio measured in a resistive phantom was 55.23 dB. Additional experiments consisted of placing five spheres of different size and conductivity in a saline tank, measuring their impedance through eight electrodes, and then generating impedance maps using the Electrical Impedance Tomography and Diffuse Optical Tomography Reconstruction Software (EIDORS). These maps were different for each sphere, suggesting the proposed prototype as a promising alternative for medical applications.
SP  - e00274
EP  - e00274
JF  - HardwareX
VL  - 11
IS  - NA
PB  - 
DO  - 10.1016/j.ohx.2022.e00274
ER  - 

TY  - JOUR
AU  - Gao, Yisong; Wu, Lifang; Yan, Dong-Ming; Nan, Liangliang
TI  - Near support-free multi-directional 3D printing via global-optimal decomposition
PY  - 2019
AB  - NA
SP  - 101034
EP  - NA
JF  - Graphical Models
VL  - 104
IS  - NA
PB  - 
DO  - 10.1016/j.gmod.2019.101034
ER  - 

TY  - JOUR
AU  - Gibas, Christian; Grünewald, Armin; Wunderlich, Hans-Werner; Marx, Philipp; Brück, Rainer
TI  - EMBC - A wearable EIT system for detection of muscular activity in the extremities
PY  - 2019
AB  - Electrical Impedance Tomography (EIT) is a method for measuring physiological states and processes that can be used as an imaging method for muscular activities. In addition to the medical evaluation of the EIT data of the lung, this technology can be used to make a statement about muscular activity in the extremities. This paper presents a developed, mobile EIT system that can be used with an electrode bracelet on the arm. In a rst study, the EIT data for different hand gestures were evaluated.
SP  - 2496
EP  - 2499
JF  - Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference
VL  - 2019
IS  - NA
PB  - 
DO  - 10.1109/embc.2019.8856792
ER  - 

TY  - CONF
AU  - Hong, Freddie; Myant, Connor; Boyle, David
TI  - CHI - Thermoformed Circuit Boards: Fabrication of highly conductive freeform 3D printed circuit boards with heat bending
PY  - 2021
AB  - Fabricating 3D printed electronics using desktop printers has become more accessible with recent developments in conductive thermoplastic filaments. Because of their high resistance and difficulties in printing traces in vertical directions, most applications are restricted to capacitive sensing. In this paper, we introduce Thermoformed Circuit Board (TCB), a novel approach that employs the thermoformability of the 3D printed plastics to construct various double-sided, rigid and highly conductive freeform circuit boards that can withstand high current applications through copper electroplating. To illustrate the capability of the TCB, we showcase a range of examples with various shapes, electrical characteristics and interaction mechanisms. We also demonstrate a new design tool extension to an existing CAD environment that allows users to parametrically draw the substrate and conductive trace, and export 3D printable files. TCB is an inexpensive and highly accessible fabrication technique intended to broaden HCI researcher participation.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jiang, Weiwei; Wang, Chaofan; Sarsenbayeva, Zhanna; Irlitti, Andrew; Knibbe, Jarrod; Dingler, Tilman; Goncalves, Jorge; Kostakos, Vassilis
TI  - InfoPrint: Embedding Information into 3D Printed Objects.
PY  - 2021
AB  - We present a technique to embed information invisible to the eye inside 3D printed objects. The information is integrated in the object model, and then fabricated using off-the-shelf dual-head FDM (Fused Deposition Modeling) 3D printers. Our process does not require human intervention during or after printing with the integrated model. The information can be arbitrary symbols, such as icons, text,binary, or handwriting. To retrieve the information, we evaluate two different infrared-based imaging devices that are readily available-thermal cameras and near-infrared scanners. Based on our results, we propose design guidelines for a range of use cases to embed and extract hidden information. We demonstrate how our method can be used for different applications, such as interactive thermal displays, hidden board game tokens, tagging functional printed objects, and autographing non-fungible fabrication work.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Choi, Seunghwan; Ryu, Joonghyun; Lee, Mo-Kwon; Cha, Jehyun; Kim, Hyunwoo; Song, Chanyoung; Kim, Deok-Soo
TI  - Support-free hollowing with spheroids and efficient 3D printing utilizing circular printing motions based on Voronoi diagrams
PY  - 2020
AB  - Abstract Extrusion-based printing frequently requires a hollowing step to remove material from inside of artifacts and subsequently reduce the amount of material, printing time, product weight, energy consumption, and ultimately, the cost. Here, we introduce a novel support-free hollowing method that uses an arrangement of vertically-aligned prolate spheroids. In addition to reducing stress concentration through their inherently smooth boundaries, these spheroids require no additional support structure, when properly designed. Additionally, the resulting spheroidal hollows facilitate the circular printing motion of extruders using G2/G3-codes, which provide three critical advantages compared to the currently popular G1-code-based linear motion: shorter printing time, better printing quality, and smaller tool path file. Here, spheroids are arranged by the Voronoi diagram of 3D ellipsoids and the tool path, including circular printing motions, is produced using the Voronoi diagram of circular 2D disks. The proposed algorithms are implemented as the HollowTron webserver and are freely available from Voronoi Diagram Research Center.
SP  - 101254
EP  - NA
JF  - Additive Manufacturing
VL  - 35
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2020.101254
ER  - 

TY  - NA
AU  - Sethapakdi, Ticha; Anderson, Daniel; Sy, Adrian Reginald Chua; Mueller, Stefanie
TI  - CHI - Fabricaide: Fabrication-Aware Design for 2D Cutting Machines
PY  - 2021
AB  - Designers of machine-cut objects must often consider whether and how their design can be fabricated with their available materials. In contrast to tools that support preparing finished designs for fabrication, we investigate shortening the feedback loop between design creation and fabrication preparation. To this end, we present Fabricaide, a fabrication-aware tool that interleaves the processes of creating and preparing designs for fabrication. By providing live feedback on how parts should be placed onto material sheets, analyzing how much material is consumed, and alerting users when designs are infeasible, Fabricaide enables users to proactively tailor their design to their available material. Fabricaide achieves this with a custom packing algorithm that arranges parts onto material sheets at interactive speeds. Our qualitative user study shows how Fabricaide can support different workflows, encourage material-conscious design practices, and provide insights on how to further improve similar interfaces in the future.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445345
ER  - 

TY  - JOUR
AU  - Nandi, Chandrakana; Wilcox, James R.; Panchekha, Pavel; Blau, Taylor; Grossman, Dan; Tatlock, Zachary
TI  - Functional programming for compiling and decompiling computer-aided design
PY  - 2018
AB  - Desktop-manufacturing techniques like 3D printing are increasingly popular because they reduce the cost and complexity of producing customized objects on demand. Unfortunately, the vibrant communities of early adopters, often referred to as "makers," are not well-served by currently available software pipelines. Users today must compose idiosyncratic sequences of tools which are typically repurposed variants of proprietary software originally designed for expert specialists. This paper proposes fundamental programming-languages techniques to bring improved rigor, reduced complexity, and new functionality to the computer-aided design (CAD) software pipeline for applications like 3D-printing. Compositionality, denotational semantics, compiler correctness, and program synthesis all play key roles in our approach, starting from the perspective that solid geometry is a programming language. Specifically, we define a purely functional language for CAD called LambdaCAD and a polygon surface-mesh intermediate representation. We then define denotational semantics of both languages to 3D solids and a compiler from CAD to mesh accompanied by a proof of semantics preservation. We illustrate the utility of this foundation by developing a novel synthesis algorithm based on evaluation contexts to "reverse compile" difficult-to-edit meshes downloaded from online maker communities back to more-editable CAD programs. All our prototypes have been implemented in OCaml to enable further exploration of functional programming for desktop manufacturing.
SP  - 99
EP  - 31
JF  - Proceedings of the ACM on Programming Languages
VL  - 2
IS  - ICFP
PB  - 
DO  - 10.1145/3236794
ER  - 

TY  - NA
AU  - McDonald, Joselyn; Zhao, Siyan; Liu, Jen; Rivera, Michael L.
TI  - Conference on Designing Interactive Systems (Companion Volume) - MaxiFab: Applied Fabrication to Advance Period Technologies
PY  - 2018
AB  - MaxiFab is a multifaceted collaborative effort aimed to address current shortcomings of menstrual technologies through digital fabrication techniques. We explore using 3D printing to produce customizable frames for sanitary napkins and laser cutting to fabricate fused washable sanitary napkins. Our preliminary explorations create menstrual products that address some of the most pressing problems with current period technologies---namely, access and cost barriers, waste, and lack of customization. Our work aims to reduce stigma regarding the discussion of menstruation while contextualizing the topic as an under-examined design research space.
SP  - 13
EP  - 19
JF  - Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3197391.3205405
ER  - 

TY  - NA
AU  - Gray, Colin M.; Kou, Yubo; Battles, Bryan; Hoggatt, Joseph; Toombs, Austin L.
TI  - CHI - The Dark (Patterns) Side of UX Design
PY  - 2018
AB  - Interest in critical scholarship that engages with the complexity of user experience (UX) practice is rapidly expanding, yet the vocabulary for describing and assessing criticality in practice is currently lacking. In this paper, we outline and explore the limits of a specific ethical phenomenon known as "dark patterns," where user value is supplanted in favor of shareholder value. We assembled a corpus of examples of practitioner-identified dark patterns and performed a content analysis to determine the ethical concerns contained in these examples. This analysis revealed a wide range of ethical issues raised by practitioners that were frequently conflated under the umbrella term of dark patterns, while also underscoring a shared concern that UX designers could easily become complicit in manipulative or unreasonably persuasive practices. We conclude with implications for the education and practice of UX designers, and a proposal for broadening research on the ethics of user experience.
SP  - 534
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174108
ER  - 

TY  - NA
AU  - Ichinco, Michelle; Hnin, Wint; Kelleher, Caitlin
TI  - CHI - Suggesting API Usage to Novice Programmers with the Example Guru
PY  - 2017
AB  - Programmers, especially novices, often have difficulty learning new APIs (Application Programming Interfaces). Existing research has not fully addressed novice programmers' unawareness of all available API methods. To help novices discover new and appropriate uses for API methods, we designed a system called the Example Guru. The Example Guru suggests context-relevant API methods based on each programmer's code. The suggestions provide contrasting examples to demonstrate how to use the API methods. To evaluate the effectiveness of the Example Guru, we ran a study comparing novice programmers' use of the Example Guru and documentation-inspired API information. We found that twice as many participants accessed the Example Guru suggestions compared to documentation and that participants used more than twice as many new API methods after accessing suggestions than documentation.
SP  - 1105
EP  - 1117
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025827
ER  - 

TY  - JOUR
AU  - Abasi, Sara; Aggas, John R.; Garayar-Leyva, Guillermo G.; Walther, Brandon K.; Guiseppi-Elie, Anthony
TI  - Bioelectrical Impedance Spectroscopy for Monitoring Mammalian Cells and Tissues under Different Frequency Domains: A Review
PY  - 2022
AB  - NA
SP  - 495
EP  - 516
JF  - ACS Measurement Science Au
VL  - 2
IS  - 6
PB  - 
DO  - 10.1021/acsmeasuresciau.2c00033
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Koelle, Marion; Mjaku, Fjolla; Strohmeier, Paul; Steimle, Jürgen
TI  - Print-A-Sketch: A Handheld Printer for Physical Sketching of Circuits and Sensors on Everyday Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502074
ER  - 

TY  - NA
AU  - Wang, Tianyi; Huo, Ke; Chawla, Pratik; Chen, Guiming; Banerjee, Siddharth; Ramani, Karthik
TI  - CHI Extended Abstracts - Plain2Fun: Augmenting Ordinary Objects with Surface Painted Circuits
PY  - 2018
AB  - The growing makers' community demands better supports for designing and fabricating interactive functional objects. Most of the current approaches focus on embedding desired functions within new objects. Instead, we advocate re-purposing existing objects and authoring interactive functions onto them. We present Plain2Fun, a design and fabrication pipeline enabling users to quickly transform ordinary objects into interactive and functional ones. Plain2Fun allows users to directly design the circuit layouts onto the surfaces of the scanned 3D model of existing objects. Our design tool automatically generates as short as possible circuit paths between any two points while avoiding intersections. Further, we build a digital machine to construct the conductive paths accurately. With a specially designed housing base, users can simply snap the electronic components onto the surfaces and obtain working physical prototypes. Moreover, we evaluate the usability of our system with multiple use cases.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188655
ER  - 

TY  - BOOK
AU  - Withana, Anusha; Steimle, Jürgen
TI  - ISS - Personalized Interactive Surfaces with Printed Electronics
PY  - 2017
AB  - Recent advances in printed electronics have enabled the design and fabrication of thin, flexible and customizable interactive surfaces. These interfaces create opportunities for a variety of novel interactions leveraging on the unique form factor, flexibility and customization options. Previous research has demonstrated the possibility of customizable multi-touch sensors, conformal on-skin interfaces and printable shape changing displays. The aim of this tutorial is to acquire basic conceptual and practical skills in developing interactive surfaces with printed electronics.
SP  - 473
EP  - 476
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3135088
ER  - 

TY  - JOUR
AU  - Nawaz, Mehmood; Chan, Russell W.; Malik, Anju; Khan, Tariq; Cao, Peng
TI  - Hand Gestures Classification Using Electrical Impedance Tomography Images
PY  - 2022
AB  - Human–computer communication using hand gestures has always been difficult. More than half a century ago, people used differentways of interactionwith computers from the early mediums such as perforated game cards. Nowadays, if a richer lexicon of gestures is given, people can communicate more effectively with computers. Machine learning is now used to recognize and classify hand gestures in amore preciseway. In order to increase the communication between computers and humans, we proposed a technique, which uses a wearable low-cost device to generate the electrical impedance tomography (EIT) images to recover the inner impedance structure of a user’s wrist. This is done by measuring the transverse impedance between all the 16 pairs of electrodesofwrist band that lie on the skin of the user hand. The proposedtechnique is enough to integrate the technology into the prototypewrist band tomonitor and classify gestures in real time. We have conducted a study of 16 gestures with a focus on gross hand and pinch finger gestures. The results evaluation shows that the gross hand gestures achieved 90% accuracy inwrist position, while pinch gestures achieved 93% accuracy.
SP  - 18922
EP  - 18932
JF  - IEEE Sensors Journal
VL  - 22
IS  - 19
PB  - 
DO  - 10.1109/jsen.2022.3193718
ER  - 

TY  - NA
AU  - Guo, Kaiwen; Zhou, Hao; Tian, Ye; Zhou, Wangqiu; Ji, Yusheng; Li, Xiang-Yang
TI  - Mudra: A Multi-Modal Smartwatch Interactive System with Hand Gesture Recognition and User Identification
PY  - 2022
AB  - The great popularity of smartwatches leads to a growing demand for smarter interactive systems. Hand gesture is suitable for interaction due to its unique features. However, the existing single-modal gesture interactive systems have different biases in diverse scenarios, which makes it intractable to be applied in real life. In this paper, we propose a multi-modal smartwatch interactive system named Mudra, which fuses vision and Inertial Measurement Unit (IMU) signals to recognize and identify hand gestures for convenient and robust interaction. We carefully design a parallel attention multi-task model for different modals, and fuse classification results at the decision level with an adaptive weight adjustment algorithm. We implement a prototype of Mudra and collect data from 25 volunteers to evaluate its effectiveness. Extensive experiments demonstrate that Mudra can achieve 95.4% and 92.3% F1-scores on recognition and identification tasks, respectively. Meanwhile, Mudra can maintain stability and robustness under different experimental settings.
SP  - NA
EP  - NA
JF  - IEEE INFOCOM 2022 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom48880.2022.9796879
ER  - 

TY  - NA
AU  - Bhatt, Prahar M.; Kulkarni, Ashish; Malhan, Rishi K.; Gupta, Satyandra K.
TI  - ICRA - Optimizing Part Placement for Improving Accuracy of Robot-Based Additive Manufacturing
PY  - 2021
AB  - Robotic manipulators are increasingly being used to perform additive manufacturing. The accuracy of a built part is dependent on the trajectory execution error of the manipulator. For articulated manipulators, the trajectory execution error and achievable build accuracy vary considerably over the workspace. Therefore, the build accuracy depends on where the part is placed in the manipulator workspace. If the part is small compared to the manipulator workspace, its placement can be optimized to improve the accuracy. This paper provides experimental evidence that the placement of the parts changes its build accuracy. We model the trajectory execution error of the manipulator for additive manufacturing. We validate these errors by comparing the predicted errors with the experimental errors. Finally, we present an algorithm to optimize the part placement for improving built part accuracy during robot-based additive manufacturing.
SP  - 859
EP  - 865
JF  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra48506.2021.9561494
ER  - 

TY  - NA
AU  - Zheng, Clement; Gyory, Peter; Yi-Luen, Ellen
TI  - Conference on Designing Interactive Systems - Tangible Interfaces with Printed Paper Markers
PY  - 2020
AB  - This pictorial presents a design investigation at the intersection of paper and computer vision for tangible interfaces. Through this exploration, we uncovered various characteristics of paper that connect tangible interactions with concealing and revealing printed fiducial markers for detection-particularly through the affordances of paper craft and fiber. We illustrate a variety of paper structures that construct and deconstruct fiducial markers. We also demonstrate how these structures enable untethered functional physical inputs, such as push buttons and sliders. We showcase four proposals that extend these material insights into tangible interface applications, including interactive data physicalizations and functional paper prototypes. Furthermore, we continue the legacy of pictorials by exposing fabrication drawings for others to engage with this work at a more practical level.
SP  - 909
EP  - 923
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395578
ER  - 

TY  - NA
AU  - Li, Jiahao; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - UIST - Robiot: A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms
PY  - 2019
AB  - Users can now easily communicate digital information with an Internet of Things; in contrast, there remains a lack of support to automate physical tasks that involve legacy static objects, e.g. adjusting a desk lamp's angle for optimal brightness, turning on/off a manual faucet when washing dishes, sliding a window to maintain a preferred indoor temperature. Automating these simple physical tasks has the potential to improve people's quality of life, which is particularly important for people with a disability or in situational impairment. We present Robiot -- a design tool for generating mechanisms that can be attached to, motorized, and actuating legacy static objects to perform simple physical tasks. Users only need to take a short video manipulating an object to demonstrate an intended physical behavior. Robiot then extracts requisite parameters and automatically generates 3D models of the enabling actuation mechanisms by performing a scene and motion analysis of the 2D video in alignment with the object's 3D model. In an hour-long design session, six participants used Robiot to actuate seven everyday objects, imbuing them with the robotic capability to automate various physical tasks.
SP  - 673
EP  - 685
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347894
ER  - 

TY  - JOUR
AU  - Hu, Fang; He, Peng; Xu, Songlin; Li, Yin; Zhang, Cheng
TI  - FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist
PY  - 2020
AB  - In this paper, we present FingerTrak, a minimal-obtrusive wristband that enables continuous 3D finger tracking and hand pose estimation with four miniature thermal cameras mounted closely on a form-fitting wristband. FingerTrak explores the feasibility of continuously reconstructing the entire hand postures (20 finger joints positions) without the needs of seeing all fingers. We demonstrate that our system is able to estimate the entire hand posture by observing only the outline of the hand, i.e., hand silhouettes from the wrist using low-resolution (32 x 24) thermal cameras. A customized deep neural network is developed to learn to "stitch" these multi-view images and estimate 20 joints positions in 3D space. Our user study with 11 participants shows that the system can achieve an average angular error of 6.46° when tested under the same background, and 8.06° when tested under a different background. FingerTrak also shows encouraging results with the re-mounting of the device and has the potential to reconstruct some of the complicated poses. We conclude this paper with further discussions of the opportunities and challenges of this technology.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397306
ER  - 

TY  - NA
AU  - Vanderdonckt, Jean; Khaddam, Iyad; Vatavu, Radu-Daniel
TI  - EICS - The foldinterface editor: a visual tool for designing user interfaces for foldable displays
PY  - 2020
AB  - Designing foldable user interfaces ("foldinterfaces") is complex and time-consuming because of the multiple technologies involved in the process, from the hardware details for folding pixels to design requirements regarding efficiency and ease of use. To assist this process, we introduce a visual editor for designers to specify foldinterfaces by implementing an extension of the Yoshizawa-Randlett diagramming system for origami and Event-Condition-Action rules from event-driven software architecture. The outcome is rendered as a 3-D foldable surface in a virtual environment.
SP  - NA
EP  - NA
JF  - Companion Proceedings of the 12th ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3393672.3398490
ER  - 

TY  - JOUR
AU  - Russo, Stefania; Assaf, Roy; Carbonaro, Nicola; Tognetti, Alessandro
TI  - Touch Position Detection in Electrical Tomography Tactile Sensors Through Quadratic Classifier
PY  - 2019
AB  - Traditional electrical tomography tactile sensors consider the usage of the system’s finite element model. This approach brings disadvantages that jeopardize their applicability aspect and wide use. To address this limitation, the main thrust of this paper is to present a method for touch position identification for an electrical tomography flexible tactile sensor. This is done by using a supervised machine learning algorithm for performing classification, namely quadratic discriminant analysis. This approach provides accurate contact location identification, increasing the detection speed and the sensor versatility when compared with traditional electrical tomography approaches. Results obtained show classification accuracy rates of up to 91.6% on unseen test data and an average Euclidean error ranging from 1 to 10 mm depending on the contact location over the sensor. The sensor is then applied in real case scenarios to show its efficiency. These outcomes are encouraging since they promote the future practical usage of flexible and low-cost sensors.
SP  - 474
EP  - 483
JF  - IEEE Sensors Journal
VL  - 19
IS  - 2
PB  - 
DO  - 10.1109/jsen.2018.2878774
ER  - 

TY  - JOUR
AU  - Iyer, Vikram; Chan, Justin; Gollakota, Shyamnath
TI  - 3D printing wireless connected objects
PY  - 2017
AB  - Our goal is to 3D print wireless sensors, input widgets and objects that can communicate with smartphones and other Wi-Fi devices, without the need for batteries or electronics. To this end, we present a novel toolkit for wireless connectivity that can be integrated with 3D digital models and fabricated using commodity desktop 3D printers and commercially available plastic filament materials. Specifically, we introduce the first computational designs that 1) send data to commercial RF receivers including Wi-Fi, enabling 3D printed wireless sensors and input widgets, and 2) embed data within objects using magnetic fields and decode the data using magnetometers on commodity smartphones. To demonstrate the potential of our techniques, we design the first fully 3D printed wireless sensors including a weight scale, flow sensor and anemometer that can transmit sensor data. Furthermore, we 3D print eyeglass frames, armbands as well as artistic models with embedded magnetic data. Finally, we present various 3D printed application prototypes including buttons, smart sliders and physical knobs that wirelessly control music volume and lights as well as smart bottles that can sense liquid flow and send data to nearby RF devices, without batteries or electronics.
SP  - 242
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 6
PB  - 
DO  - 10.1145/3130800.3130822
ER  - 

TY  - NA
AU  - Ahuja, Karan; Goel, Mayank; Harrison, Chris
TI  - SUI - BodySLAM: Opportunistic User Digitization in Multi-User AR/VR Experiences
PY  - 2020
AB  - Today’s augmented and virtual reality (AR/VR) systems do not provide body, hand or mouth tracking without special worn sensors or external infrastructure. Simultaneously, AR/VR systems are increasingly being used in co-located, multi-user experiences, opening the possibility for opportunistic capture of other users. This is the core idea behind BodySLAM, which uses disparate camera views from users to digitize the body, hands and mouth of other people, and then relay that information back to the respective users. If a user is seen by two or more people, 3D pose can be estimated via stereo reconstruction. Our system also maps the arrangement of users in real world coordinates. Our approach requires no additional hardware or sensors beyond what is already found in commercial AR/VR devices, such as Microsoft HoloLens or Oculus Quest.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418452
ER  - 

TY  - NA
AU  - Segawa, Norihisa; Kato, Kunihiro; Manabe, Hiroyuki
TI  - UIST (Adjunct Volume) - Rapid Prototyping of Paper Electronics Using a Metal Leaf and Laser Printer
PY  - 2019
AB  - We introduce a novel prototyping method using a metal leaf and laser printer. Proposed metal leaf circuit is capable of producing circuits on normal paper that can be printed by a laser printer. The metal leaf is adhered to paper using a toner used in a laser printer. When the metal leaf is placed on the printed circuit pattern and pressed with an iron, the metal leaf adheres to the circuit pattern. Removal of the excess metal leaf completes the metal leaf circuit diagram on the paper. In addition, by using the change of temperature of toner, we can easily cut and repair the circuit. We made a metal leaf circuit on the cloth with the masu sake cup, and evaluated it.
SP  - 99
EP  - 101
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356885
ER  - 

TY  - JOUR
AU  - Bachmann, Adam L; Hanrahan, Brendan; Dickey, Michael D; Lazarus, Nathan
TI  - Self-Folding PCB Kirigami: Rapid Prototyping of 3D Electronics via Laser Cutting and Forming.
PY  - 2022
AB  - This paper demonstrates laser forming, localized heating with a laser to induce plastic deformation, can self-fold 2D printed circuit boards (PCBs) into 3D structures with electronic function. There are many methods for self-folding but few are compatible with electronic materials. We use a low-cost commercial laser writer to both cut and fold a commercial flexible PCB. Laser settings are tuned to select between cutting and folding with higher power resulting in cutting and lower power resulting in localized heating for folding into 3D shapes. Since the thin copper traces used in commercial PCBs are highly reflective and difficult to directly fold, two approaches are explored for enabling folding: plating with a nickel/gold coating or using a single, high-power laser exposure to oxidize the surface and improve laser absorption. We characterized the physical effect of the exposure on the sample as well as the fold angle as a function of laser passes and demonstrate the ability to lift weights comparable with circuit packages and passive components. This technique can form complex, multifold structures with integrated electronics; as a demonstrator, we fold a commercial board with a common timing circuit. Laser forming to add a third dimension to printed circuit boards is an important technology to enable the rapid prototyping of complex 3D electronics.
SP  - 14774
EP  - 14782
JF  - ACS applied materials & interfaces
VL  - 14
IS  - 12
PB  - 
DO  - 10.1021/acsami.2c01027
ER  - 

TY  - NA
AU  - Han, Changyo; Takahashi, Ryo; Yahagi, Yuchi; Naemura, Takeshi
TI  - CHI Extended Abstracts - PneuModule: Using Inflatable Pin Arrays for Reconfigurable Physical Controls on Pressure-Sensitive Touch Surfaces
PY  - 2020
AB  - We present PneuModule, a tangible interface platform that enables users to reconfigure physical controls on pressure-sensitive touch surfaces using pneumatically-actuated inflatable pin arrays. PneuModule consists of a main module and extension modules. The main module is tracked on the touch surface and forwards continuous inputs from attached multiple extension modules to the touch surface. Extension modules have distinct mechanisms for user input, which pneumatically actuates the inflatable pins at the bottom of the main module through internal air pipes. The main module accepts multi-dimensional inputs since each pin is individually inflated by the corresponding air chamber. Also, since the extension modules are swappable and identifiable owing to the marker design, users can quickly customize the interface layout. We contribute to design details of inflatable pins and diverse pneumatic input control design examples for PneuModule. We also showcase the feasibility of PneuModule through a series of evaluations and interactive prototypes.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376838
ER  - 

TY  - BOOK
AU  - Manabe, Hiroyuki; Iwai, Yuki
TI  - ISS Companion - Backhand Display: A Wearable Device for the Back of the Hand
PY  - 2020
AB  - Smartwatches and smartphones have their own unique advantages and their combination allows the user to quickly access information. Our proposal, Backhand Display, is a wearable device for the back of the hand. It is a single device that can replace both the smartwatch and the smartphone. Its fundamental performances are evaluated, including text input, vibration notification, and response time between notification and reply. It is confirmed that Backhand Display offers faster text input than the smartwatch, greater notification capability than the smartphone, and quicker response to notifications than either of them. These benefits are supported by the results of a user questionnaire. Though there are several limitations and issues to be resolved, these results show the potential of Backhand Display, a wearable device on the back of the hand.
SP  - 41
EP  - 45
JF  - Companion Proceedings of the 2020 Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3380867.3426200
ER  - 

TY  - NA
AU  - Kim, Jeeeun; Takahashi, Haruki; Miyashita, Homei; Annett, Michelle; Yeh, Tom
TI  - CHI Extended Abstracts - Machines as Co-Designers: A Fiction on the Future of Human-Fabrication Machine Interaction
PY  - 2017
AB  - While current fabrication technologies have led to a wealth of techniques to create physical artifacts of virtual designs, they require unidirectional and constraining interaction workflows. Instead of acting as intelligent agents that support human's natural tendencies to iteratively refine ideas and experiment, today's fabrication machines function as output devices. In this work, we argue that fabrication machines and tools should be thought of as live collaborators to aid in-situ creativity, adapting physical dynamics come from unique materiality and/or machine specific parameters. Through a series of design narratives, we explore Human-FabMachine Interaction (HFI), a novel viewpoint from which to reflect on the importance of (i) interleaved design thinking and refinement during fabrication, (ii) enriched methods of interaction with fabrication machines regardless of skill level, and (iii) concurrent human and machine interaction.
SP  - 790
EP  - 805
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3052763
ER  - 

TY  - BOOK
AU  - Matulic, Fabrice; Vogel, Brian; Kimura, Naoki; Vogel, Daniel
TI  - ISS - Eliciting Pen-Holding Postures for General Input with Suitability for EMG Armband Detection
PY  - 2019
AB  - We conduct a two-part study to better understand pen grip postures for general input like mode switching and com-mand invocation. The first part of the study asks participants what variations of their normal pen grip posture they might use, without any specific consideration for sensing capabilities. The second part evaluates three of their sug-gested postures with an additional set of six postures designed for the sensing capabilities of a consumer EMG armband. Results show that grips considered normal and mature, such as the dynamic tripod and the dynamic quadrupod, are the best candidates for pen-grip based interaction, followed by finger-on-pen postures and grips using pen tilt. A convolutional neural network trained on EMG data gathered during the study yields above 70% within-participant recognition accuracy for common sets of five postures and above 80% for three-posture subsets. Based on the results, we propose design guidelines for pen interaction using variations of grip postures.
SP  - 89
EP  - 100
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359720
ER  - 

TY  - NA
AU  - Savage, Valkyrie; Tejada, Carlos; Zhong, Mengyu; Ramakers, Raf; Ashbrook, Daniel; Kim, Hyunyoung
TI  - AirLogic: Embedding Pneumatic Computation and I/O in 3D Models to Fabricate Electronics-Free Interactive Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545642
ER  - 

TY  - NA
AU  - Roquet, Claudia Daudén; Kim, Jeeeun; Yeh, Tom
TI  - Conference on Designing Interactive Systems - 3D Folded PrintGami: Transforming Passive 3D Printed Objects to Interactive by Inserted Paper Origami Circuits
PY  - 2016
AB  - Recent advances in 3D printing allowed end users to easily utilize desktop 3D printers. However, these printers mainly consume molten plastic, due to limited materiality. While high interests on creating interactive 3D objects with electronics are ubiquitous, commercial 3D printers have its own boundaries of not integrating electronics. On the other hand, paper as flexible medium has shown its possibility to be a channel to host interactive objects, enabling interaction triggered by user. In this paper, we introduce a process of integrating paper circuit into a 3D printed object, combining two universal fabrication techniques, 3D printing and paper crafting: 1) design a hollow 3D model, 2) construct circuits on the 2D planar cutout of this figure, 3) pause 3D printing to insert folded circuits in, 4) resume to continue printing. We empower makers and tinkerers to employ pervasive technology to build interactive 3D objects without hacking machine, obtaining professional utilities, or buying expensive materials.
SP  - 187
EP  - 191
JF  - Proceedings of the 2016 ACM Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2901790.2901891
ER  - 

TY  - JOUR
AU  - Chiu, Che-Min; Chen, Shuo-Wen; Pao, Yu-Ping; Huang, Ming-Zheng; Chan, Shuen-Wen; Lin, Zong-Hong
TI  - A smart glove with integrated triboelectric nanogenerator for self-powered gesture recognition and language expression
PY  - 2019
AB  - Flexible electronics with great functional characteristics have proved to be a stepping stone in the field of wearable devices. Amongst all, gesture-sensing techniques have been widely studied for ...
SP  - 964
EP  - 971
JF  - Science and technology of advanced materials
VL  - 20
IS  - 1
PB  - 
DO  - 10.1080/14686996.2019.1665458
ER  - 

TY  - JOUR
AU  - Jiang, Xianta; Merhi, Lukas-Karim; Xiao, Zhen Gang; Menon, Carlo
TI  - Exploration of Force Myography and surface Electromyography in hand gesture classification
PY  - 2017
AB  - NA
SP  - 63
EP  - 73
JF  - Medical engineering & physics
VL  - 41
IS  - NA
PB  - 
DO  - 10.1016/j.medengphy.2017.01.015
ER  - 

TY  - JOUR
AU  - Chen, Haofeng; Yang, Xuanxuan; Wang, Peng; Geng, Jialu; Ma, Gang; Wang, Xiaojie
TI  - A Large-Area Flexible Tactile Sensor for Multi-Touch and Force Detection Using Electrical Impedance Tomography
PY  - 2022
AB  - For large area robot skin design, the distribution of rigid components and wires in traditional array sensors leads to the decrease of the flexibility and extensibility of sensors. The flexible sensors based on non-invasive electrical impedance tomography (EIT) can avoid these shortcomings. However, design of a large-area flexible tactile sensors using EIT technique is still challenging due to the tradeoff between manufacturability and spatial resolution. In this study, we proposed a novel non-array EIT-based tactile sensor which is made of a porous elastic polymer and ionic liquid. The sensor free from internal array electrodes is straightforward to manufacture and can cover a large area with low cost. To improve the spatial resolution and touch sensitivity, we adopted a deep learning scheme Pyramid Scene Parsing Network (PSPNet) to postprocess the originally reconstructed images to enhance the sensor tactile perception. With this data-driven method, we achieved a single-point position detection error of 7.5± 4.5 mm without using internal electrodes. To overcome the location dependency of EIT sensing problems, we proposed a method of sub-regional fitting to calibrate the distributed forces for the large-area flexible tactile sensor and obtained a quantitative relationship between the touch force and EIT measurement for the entire sensing area for continuous sensing. The real-time performance of the proposed prototype sensor system demonstrated that we can achieve more accuracy of multi-touch and distributed force detection on the non-array sensors with a temporal resolution of about 1.3 frames/sec for potential applications in human-robot interfaces.
SP  - 7119
EP  - 7129
JF  - IEEE Sensors Journal
VL  - 22
IS  - 7
PB  - 
DO  - 10.1109/jsen.2022.3155125
ER  - 

TY  - JOUR
AU  - Chen, Haofeng; Ma, Gang; Wang, Peng; Xiaojie, Wang
TI  - A Bio-Impedance Analysis Method Based on Human Hand Anatomy for Hand Gesture Recognition
PY  - 2021
AB  - In this article, we presented a bio-impedance analysis method (BIAM) based on the human hand anatomy and propose a feasible and flexible BIAM system to obtain the bio-impedance signals for different gestures to achieve a high classification accuracy in hand gesture recognition with flexibility in electrode arrangement and fewer electrodes. To verify the proposed method, 11 gestures, including two sets: hand gestures and pinch gestures, were selected for the experiment. Based on the functional structure of the human hand, we identified appropriate electrode positions and placed five electrodes on the hand surface for bio-impedance signal measurement. Compared with the electrical impedance tomography (EIT) method, which uses a band with the same number of electrodes wrapped around the wrist, the proposed method achieved 98.7% recognition accuracies on the hand gesture set and 97.8% on the pinch gesture set, while the EIT achieved only 97.1% and 86.3%, respectively. In particular, the proposed method demonstrated the advantage of distinguishing gestures with similar muscle contractions.
SP  - 1
EP  - 10
JF  - IEEE Transactions on Instrumentation and Measurement
VL  - 70
IS  - NA
PB  - 
DO  - 10.1109/tim.2021.3112775
ER  - 

TY  - JOUR
AU  - Jiang, Dai; Wu, Yu; Demosthenous, Andreas
TI  - Hand Gesture Recognition Using Three-Dimensional Electrical Impedance Tomography
PY  - 2020
AB  - This brief presents a 16-electrode electrical impedance tomography (EIT) system for hand gesture recognition. The hardware of the system is based on integrated circuits including a 12-bit high spectral purity current-steering DAC implemented in $0.18~\mu \text{m}$ CMOS technology, a current driver and an instrumentation amplifier in $0.35~\mu \text{m}$ CMOS technology. Both 2D and 3D EIT electrode arrangements were tested for hand gesture recognition. It is shown that using machine learning algorithms, eight hand gestures can be distinguished from the measured bio-impedance data with an accuracy of 97.9% when the electrodes are placed on a single wristband, and an accuracy of 99.5% with the same number of electrodes distributed on two wristbands for 3D EIT measurement. In particular 3D EIT demonstrated significant superiority in its ability to discriminate between gestures with similar muscle contractions.
SP  - 1554
EP  - 1558
JF  - IEEE Transactions on Circuits and Systems II: Express Briefs
VL  - 67
IS  - 9
PB  - 
DO  - 10.1109/tcsii.2020.3006430
ER  - 

TY  - JOUR
AU  - Jiang, Jingchao; Newman, Stephen T.; Zhong, Ray Y.
TI  - A review of multiple degrees of freedom for additive manufacturing machines
PY  - 2020
AB  - Currently, additive manufacturing (AM) technology has received significant attention from both academia and industry. AM is characterized by fabricating geometrically complex components in a layer-...
SP  - 195
EP  - 211
JF  - International Journal of Computer Integrated Manufacturing
VL  - 34
IS  - 1
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhang, Yang; Laput, Gierad; Harrison, Chris
TI  - CHI - Electrick: Low-Cost Touch Sensing Using Electric Field Tomography
PY  - 2017
AB  - Current touch input technologies are best suited for small and flat applications, such as smartphones, tablets and kiosks. In general, they are too expensive to scale to large surfaces, such as walls and furniture, and cannot provide input on objects having irregular and complex geometries, such as tools and toys. We introduce Electrick, a low-cost and versatile sensing technique that enables touch input on a wide variety of objects and surfaces, whether small or large, flat or irregular. This is achieved by using electric field tomography in concert with an electrically conductive material, which can be easily and cheaply added to objects and surfaces. We show that our technique is compatible with commonplace manufacturing methods, such as spray/brush coating, vacuum forming, and casting/molding enabling a wide range of possible uses and outputs. Our technique can also bring touch interactivity to rapidly fabricated objects, including those that are laser cut or 3D printed. Through a series of studies and illustrative example uses, we show that Electrick can enable new interactive opportunities on a diverse set of objects and surfaces that were previously static.
SP  - 1
EP  - 14
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025842
ER  - 

TY  - JOUR
AU  - Lu, Xupeng; Sun, Shijie; Liu, Kangqi; Sun, Jiangtao; Xu, Lijun
TI  - Development of a Wearable Gesture Recognition System Based on Two-terminal Electrical Impedance Tomography.
PY  - 2022
AB  - This paper proposes a low-cost, wearable gesture recognition system based on the two-terminal electrical impedance tomography (EIT) technique. The system includes a wearable EIT sensor of eight electrodes, a hardware device, and gesture recognition software running on a PC. Nine different gestures can be stably identified from the measured impedance changes through machine learning algorithms. Experimental results show that the Quadric Discriminator algorithm has the highest recognition rate of 98.49% for the filtered validation set. Besides, the recognition results in the two-terminal mode and transformed four-terminal mode are compared by applying a two-to-four-terminal mapping to the two-terminal EIT system, and the recognition rate decreases with the most classification models in the latter mode. Thus, it is supposed that contact impedance plays an important role in gesture recognition. By analyzing the data characteristics with variance inflation factor (VIF) test and principal component analysis (PCA), the supposition is explained and verified, proving the merit of a two-terminal EIT system in gesture recognition.
SP  - 1
EP  - 1
JF  - IEEE journal of biomedical and health informatics
VL  - 26
IS  - 6
PB  - 
DO  - 10.1109/jbhi.2021.3130374
ER  - 

TY  - NA
AU  - Schmitz, Martin; Stitz, Martin; Müller, Florian; Funk, Markus; Mühlhäuser, Max
TI  - CHI - ./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects
PY  - 2019
AB  - Hover, touch, and force are promising input modalities that get increasingly integrated into screens and everyday objects. However, these interactions are often limited to flat surfaces and the integration of suitable sensors is time-consuming and costly. To alleviate these limitations, we contribute Trilaterate: A fabrication pipeline to 3D print custom objects that detect the 3D position of a finger hovering, touching, or forcing them by combining multiple capacitance measurements via capacitive trilateration. Trilaterate places and routes actively-shielded sensors inside the object and operates on consumer-level 3D printers. We present technical evaluations and example applications that validate and demonstrate the wide applicability of Trilaterate.
SP  - 454
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300684
ER  - 

TY  - JOUR
AU  - Sel, Kaan; Osman, Deen; Jafari, Roozbeh
TI  - Non-Invasive Cardiac and Respiratory Activity Assessment From Various Human Body Locations Using Bioimpedance
PY  - 2021
AB  - Objective: Bioimpedance sensing is a powerful technique that measures the tissue impedance and captures important physiological parameters including blood flow, lung movements, muscle contractions, body fluid shifts, and other cardiovascular parameters. This paper presents a comprehensive analysis of the modality at different arterial (ulnar, radial, tibial, and carotid arteries) and thoracic (side-rib cage and top thoracolumbar fascia) body regions and offers insights into the effectiveness of capturing various cardiac and respiratory activities. Methods: We assess the bioimpedance performance in estimating inter-beat (IBI) and inter -breath intervals (IBrI) on six-hours of data acquired in a pilot-study from five healthy participants at rest. Results: Overall, we achieve mean errors as low as 0.003 ± 0.002 and 0.67 ± 0.28 seconds for IBI and IBrI estimations, respectively. Conclusions: The results show that bioimpedance can be effectively used to monitor cardiac and respiratory activities both at limbs and upper body and demonstrate a strong potential to be adopted by wearables that aim to provide high-fidelity physiological sensing to address precision medicine needs.
SP  - 210
EP  - 217
JF  - IEEE open journal of engineering in medicine and biology
VL  - 2
IS  - NA
PB  - 
DO  - 10.1109/ojemb.2021.3085482
ER  - 

TY  - JOUR
AU  - Zhu, Zijie; Wang, Xuewei; Kapoor, Aakaash; Zhang, Zhichao; Pan, Tingrui; Yu, Zhou
TI  - EIS: A Wearable Device for Epidermal American Sign Language Recognition
PY  - 2018
AB  - American Sign Language (ASL) is widely used among hearing impaired individuals in English-speaking countries. Various technologies have been developed to perform ASL recognition, including optical signal sensing, electrical signal sensing, and mechanical signal sensing. However, wearable devices using those methods have bulky and complex sensing modules that lead to long-term discomfort as well as poor accuracy. In this paper, we present an epidermal-iontronic sensing (EIS)-based wearable device that wears on finger joints for 35 fingerspelling ASL recognitions (i.e., 26 alphabets from A to Z and 9 digits from one to nine). Compared to current on-market devices, such design is lighter, comfortable to wear and has better appearance according to user comments. When bending the finger, a physical contact forms between the ionic material and the epidermis of skin, leading to an electric double layer (EDL) established at the interface. Therefore, a significant capacitive change can be achieved with various finger gestures. By using Nafion as the ionic sensing material, we developed a sensing device to provide excellent flexibility and optical transparency. We used machine learning methods, such as neural networks to track and perform ASL recognition using the signals obtained from the designed device. The algorithm achieved a within-user accuracy of 99.6% and a cross-user accuracy of 76.1% when adapted the model to different users. This wearable device is low-cost and has broad potential to be integrated in future application of human-machine interactions (HMI), smart home controls, and nonverbal communications.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287080
ER  - 

TY  - JOUR
AU  - Alam, Shahinur; Kwon, Ki-Chul; Kim, Nam
TI  - Implementation of a Character Recognition System Based on Finger-Joint Tracking Using a Depth Camera
PY  - 2021
AB  - The joint tracking-based writing system refers to writing characters by changing the position of the finger joint. It is a new research field in interaction-based input systems. However, joint tracking is a very challenging task. In this article, we present a new method for a finger-joint tracking-based character recognition system using a 3-D camera. The proposed method tracks the finger-joint from 3-D information to identify a numerical digit, alphabet, character, special key, or symbol using the distance between the thumb tip and another finger-joint. The recognition is based on Euclidean distance thresholding and geometric slope techniques. Joint data are stored in a 3-D matrix to assign the 3-D coordinate values. The exact character is identified according to the specified definitions. First, a single hand-based digit recognition method is introduced, in which the left or right hand is used. Second, a double hand-based writing system is presented in which both hands are used simultaneously; this system features a full keyboard with 124 different characters. Our results show an overall accuracy of 91.95% and 91.85% for single-hand and double-hand recognition, respectively; with a recognition time of less than 60 ms for each character. An important contribution of this article is that the proposed system can work in both light and dark environments, requires only a small computation area and has a large number of character sets (124 characters). In addition, a region-based user study has been conducted to verify the approach.
SP  - 229
EP  - 241
JF  - IEEE Transactions on Human-Machine Systems
VL  - 51
IS  - 3
PB  - 
DO  - 10.1109/thms.2021.3066854
ER  - 

TY  - NA
AU  - Chen, Chen; Sun, Ke; Zhang, Xinyu
TI  - IPSN - ExGSense: Toward Facial Gesture Sensing with a Sparse Near-Eye Sensor Array
PY  - 2021
AB  - Immersive face-to-virtual-face telecommunication is one unique use case for virtual reality (VR) technologies. Existing camera-based telephony systems cannot be used for such immersive VR video chat, due to the physical occlusions of head-mounted displays (HMDs) and/or unwieldy positioning of cameras. To address these, we present ExGSense, a new VR input modality that can sense and reconstruct both upper and lower facial gestures, by only using lightweight biopotential sensors embedded within the HMDs. We optimize the sensor arrangement based on facial anatomy and employ a multiview classification pipeline to exploit the multiple dimensions of signal features. We thus enable ExGSense to detect whole facial gestures by using a sparse set of biopotential transducers. We prototyped ExGSense and evaluated its performance with 42 facial gestures and across different users. We showed a 93% accuracy for user-specific evaluation, and 77% accuracy for user-independent evaluation with low calibration overhead. We believe ExGSense constitutes a promising input modality for immersive VR interactions.
SP  - 222
EP  - 237
JF  - Proceedings of the 20th International Conference on Information Processing in Sensor Networks (co-located with CPS-IoT Week 2021)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3412382.3458268
ER  - 

TY  - NA
AU  - Watanabe, Keisuke; Yamamura, Ryosuke; Kakehi, Yasuaki
TI  - CHI Extended Abstracts - foamin: A Deformable Sensor for Multimodal Inputs Based on Conductive Foam with a Single Wire
PY  - 2021
AB  - Soft sensors made of deformable materials, that are capable of sensing touches or gestures, have attracted considerable attention for use in tangible interfaces or soft robotics. However, to achieve multimodal gesture detection with soft sensors, prior studies have combined multiple sensors or utilized complex configurations with multiple wires. To achieve multimodal gesture sensing with a simpler configuration, a novel soft sensor consisting of a conductive foam with a single wire, was proposed in this study. This sensor was named as foamin and utilizes an impedance measurement technique at multiple frequencies called foamin. Additionally, a surface-shielding method was designed for improving the detection performance of the sensor. Several patterns of foamin were implemented to investigate the detection accuracy and three application scenarios based on the sensor were proposed.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451547
ER  - 

TY  - JOUR
AU  - Avdeev, Artem; Shvets, Andrey; Gushchin, Ilya; Torubarov, Ivan; Drobotov, Aleksey; Makarov, Aleksey; Plotnikov, Aleksander; Serdobintsev, Yuri
TI  - Strength Increasing Additive Manufacturing Fused Filament Fabrication Technology, Based on Spiral Toolpath Material Deposition
PY  - 2019
AB  - The paper provides an overview of ways to increase the strength of polymer products obtained by fused filament fabrication (FFF) technology. An algorithm for calculating the spiral toolpaths for the material deposition using multi-axis printing is proposed. The design of the five-axis device for spiral-shaped deposition of the material is shown. The description of the proposed printing method is given. The results of comparative three-point bend and compression tests are presented. The standard samples obtained in the usual way by FFF technology, as well as samples with 2, 4, 6, 8 and 10 reinforcing layers obtained by spiral deposition of the material were investigated. The description of the tests is given, the dependences of the strength of the products on the number of reinforcing layers are obtained. Conclusions about the influence of the layer deposition method on the strength of the products are formulated.
SP  - 57
EP  - NA
JF  - Machines
VL  - 7
IS  - 3
PB  - 
DO  - 10.3390/machines7030057
ER  - 

TY  - NA
AU  - Lin, Stephen Shiao-ru; Gamage, Nisal Menuka; Herath, Kithmini; Withana, Anusha
TI  - MyoSpring: 3D Printing Mechanomyographic Sensors for Subtle Finger Gesture Recognition
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501321
ER  - 

TY  - NA
AU  - Banerjee, Siddhartha; Gombolay, Matthew C.; Chernova, Sonia
TI  - RO-MAN - A Tale of Two Suggestions: Action and Diagnosis Recommendations for Responding to Robot Failure
PY  - 2020
AB  - Robots operating without close human supervision might need to rely on a remote call center of operators for assistance in the event of a failure. In this work, we investigate the effects of providing decision support through diagnosis suggestions, as feedback, and action recommendations, as feedforward, to the human operators. We conduct a 10-condition user study involving 200 participants on Amazon Mechanical Turk to evaluate the effects of providing noisy and noise-free diagnosis suggestions and/or action recommendations to operators. We find that although action recommendations (feedforward) have a greater effect on successful error resolution than diagnosis information (feedback), the feedback likely helps ameliorate the deleterious effects of noise. Therefore, we find that error recovery interfaces should display both diagnosis and action recommendations for maximum effectiveness.
SP  - 398
EP  - 405
JF  - 2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ro-man47096.2020.9223545
ER  - 

TY  - NA
AU  - Sinha, Swapnil; Meisel, Nicholas A.
TI  - Influence of Embedding Process on Mechanical Properties of Material Extrusion Parts
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Clarke, Charisma; Steel, Kyle; Romero-Ramirez, Edwar
TI  - Additive Manufacturing of Flexible Sensors for Human-Computer Interaction
PY  - 2022
AB  - AbstractHigh-performance wearable strain gauges have always been of interest for human-computer interaction. Many sensors have been developed using traditional manufacturing techniques or processes that are costly to replicate. This work showcases the use of commercially available filaments using additive manufacturing technology to create a sensor that can be used to monitor human motion with little user discomfort. A sensor was designed and tested, and it was found to have good sensitivity and reliability. The sensor uses the piezoresistive effect with changes up to 50% when conforming to small-sized objects. The sensor was tested on a glove and found to detect finger motion while grabbing objects or typing on a keyboard.KeywordsMonitoringFlexible sensorAdditive manufacturing
SP  - 267
EP  - 273
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06388-6_35
ER  - 

TY  - NA
AU  - Valkeneers, Tom; Leen, Danny; Ashbrook, Daniel; Ramakers, Raf
TI  - UIST - StackMold: Rapid Prototyping of Functional Multi-Material Objects with Selective Levels of Surface Details
PY  - 2019
AB  - We present StackMold, a DIY molding technique to prototype multi-material and multi-colored objects with embedded electronics. The key concept of our approach is a novel multi-stage mold buildup in which casting operations are interleaved with the assembly of the mold to form independent compartments for casting different materials. To build multi-stage molds, we contribute novel algorithms that computationally design and optimize the mold and casting procedure. By default, the multi-stage mold is fabricated in slices using a laser cutter. For regions that require more surface detail, a high-fidelity 3D-printed mold subsection can be incorporated. StackMold is an integrated end-to-end system, supporting all stages of the process: it provides a UI to specify material and detail regions of a 3D~object; it generates fabrication files for the molds; and it produces a step-by-step casting instruction manual.
SP  - 687
EP  - 699
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347915
ER  - 

TY  - NA
AU  - Leong, Joanne; Martinez, Jose; Perteneder, Florian; Nakagaki, Ken; Ishii, Hiroshi
TI  - Tangible and Embedded Interaction - WraPr: Spool-Based Fabrication for Object Creation and Modification
PY  - 2020
AB  - We propose a novel fabrication method for 3D objects based on the principle of spooling. By wrapping off-the-shelf materials such as thread, ribbon, tape or wire onto a core structure, new objects can be created and existing objects can be augmented with desired aesthetic and functional qualities. Our system, WraPr, enables gesture-based modelling and controlled thread deposition. We outline and explore the design space for this approach. Various examples are fabricated to demonstrate the possibility to attain a range of physical and functional properties. The simplicity of the proposed method opens the grounds for a light-weight fabrication approach for the generation of new structures and the customization of existing objects using soft materials.
SP  - 581
EP  - 588
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374990
ER  - 

TY  - NA
AU  - Booth, Riley
TI  - Design and Testing of a Wristband with Piezoelectric Sensors for Finger Gesture Recognition
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.11575/prism/25577
ER  - 

TY  - JOUR
AU  - Youn, Eunhye; Kim, Taejun; Lee, Geehyuk
TI  - WristMenu with Tactons: An Eyes- and Ears-Free Menu with Tactons Describing Menu Items in the Wrist Rotation Space
PY  - 2022
AB  - NA
SP  - 1
EP  - 12
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2159780
ER  - 

TY  - JOUR
AU  - Liu, Yilin; Zhang, Shijia; Gowda, Mahanth; Nelakuditi, Srihari
TI  - Leveraging the Properties of mmWave Signals for 3D Finger Motion Tracking for Interactive IoT Applications
PY  - 2022
AB  - <jats:p> mmWave signals form a critical component of 5G and next-generation wireless networks, which are also being increasingly considered for sensing the environment around us to enable ubiquitous IoT applications. In this context, this paper leverages the properties of mmWave signals for tracking 3D finger motion for interactive IoT applications. While conventional vision-based solutions break down under poor lighting, occlusions, and also suffer from privacy concerns, mmWave signals work under typical occlusions and non-line-of-sight conditions, while being privacy-preserving. In contrast to prior works on mmWave sensing that focus on predefined gesture classification, this work performs continuous 3D finger motion tracking. Towards this end, we first observe via simulations and experiments that the small size of fingers coupled with specular reflections do not yield stable mmWave reflections. However, we make an interesting observation that focusing on the forearm instead of the fingers can provide stable reflections for 3D finger motion tracking. Muscles that activate the fingers extend through the forearm, whose motion manifests as vibrations on the forearm. By analyzing the variation in phases of reflected mmWave signals from the forearm, this paper designs <jats:italic>mm4Arm,</jats:italic> a system that tracks 3D finger motion. Nontrivial challenges arise due to the high dimensional search space, complex vibration patterns, diversity across users, hardware noise, etc. mm4Arm exploits anatomical constraints in finger motions and fuses them with machine learning architectures based on encoder-decoder and ResNets in enabling accurate tracking. A systematic performance evaluation with 10 users demonstrates a median error of 5.73° (location error of 4.07 mm) with robustness to multipath and natural variation in hand position/orientation. The accuracy is also consistent under non-line-of-sight conditions and clothing that might occlude the forearm. <jats:italic>mm4Arm</jats:italic> runs on smartphones with a latency of 19 <jats:italic>ms</jats:italic> and low energy overhead. </jats:p>
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Measurement and Analysis of Computing Systems
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3570613
ER  - 

TY  - JOUR
AU  - Yang, Yuan; Zhou, Wenbo; Chen, Xuyang; Ye, Jinhua; Wu, Haibin
TI  - A flexible touching sensor with the variation of electrical impedance distribution
PY  - 2021
AB  - NA
SP  - 109778
EP  - NA
JF  - Measurement
VL  - 183
IS  - NA
PB  - 
DO  - 10.1016/j.measurement.2021.109778
ER  - 

TY  - JOUR
AU  - Maia, Henrique Teles; Li, Dingzeyu; Yang, Yuan; Zheng, Changxi
TI  - LayerCode: optical barcodes for 3D printed shapes
PY  - 2019
AB  - With the advance of personal and customized fabrication techniques, the capability to embed information in physical objects becomes evermore crucial. We present LayerCode, a tagging scheme that embeds a carefully designed barcode pattern in 3D printed objects as a deliberate byproduct of the 3D printing process. The LayerCode concept is inspired by the structural resemblance between the parallel black and white bars of the standard barcode and the universal layer-by-layer approach of 3D printing. We introduce an encoding algorithm that enables the 3D printing layers to carry information without altering the object geometry. We also introduce a decoding algorithm that reads the LayerCode tag of a physical object by just taking a photo. The physical deployment of LayerCode tags is realized on various types of 3D printers, including Fused Deposition Modeling printers as well as Stereolithography based printers. Each offers its own advantages and tradeoffs. We show that LayerCode tags can work on complex, nontrivial shapes, on which all previous tagging mechanisms may fail. To evaluate LayerCode thoroughly, we further stress test it with a large dataset of complex shapes using virtual rendering. Among 4,835 tested shapes, we successfully encode and decode on more than 99% of the shapes.
SP  - 112
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 4
PB  - 
DO  - 10.1145/3306346.3322960
ER  - 

TY  - JOUR
AU  - Kubalak, Joseph R.; Wicks, Alfred L.; Williams, Christopher B.
TI  - Exploring multi-axis material extrusion additive manufacturing for improving mechanical properties of printed parts
PY  - 2019
AB  - Material extrusion (ME) suffers from anisotropic mechanical properties that stem from the three degree of freedom (DoF) toolpaths used for deposition. The formation of each layer is restricted to the XY-plane, which produces poorly bonded layer interfaces along the build direction. Multi-axis ME affords the opportunity to change the layering and deposition directions locally throughout a part, which could improve a part’s overall mechanical performance. The purpose of this paper is to evaluate the effects of changing the layering and deposition directions on the tensile mechanical properties of parts printed via multi-axis ME.,A multi-axis toolpath generation algorithm is presented and implemented on a 6-DoF robotic arm ME system to fabricate tensile specimens at different global orientations. Specifically, acrylonitrile butadiene styrene (ABS) tensile specimens are printed at various inclination angles using the multi-axis technique; the resulting tensile strengths of the multi-axis specimens are compared to similarly oriented specimens printed using a traditional 3-DoF method.,The multi-axis specimens had similar performances regardless of orientation and were equivalent to the 3-DoF specimens printed in the XYZ orientation (i.e. flat on the bed with roads aligned to the loading condition). This similarity is attributed to those sets of specimens having the same degree of road alignment.,Parts with out-of-plane loads currently require design compromises (e.g. additional material in critical areas). Multi-axis deposition strategies could enable local changes in layering and deposition directions to more optimally orient roads in critical areas of the part.,Though multi-axis ME systems have been demonstrated in literature, no prior work has been done to determine the effects of the deposition angle on the resulting mechanical properties. This work demonstrates that identical mechanical properties can be obtained irrespective of the build direction through multi-axis deposition. For ABS, the yield tensile strength of vertically oriented tensile bars was improved by 153 per cent using multi-axis deposition as compared to geometrically similar samples fabricated via 3-DoF deposition.
SP  - 356
EP  - 362
JF  - Rapid Prototyping Journal
VL  - 25
IS  - 2
PB  - 
DO  - 10.1108/rpj-02-2018-0035
ER  - 

TY  - JOUR
AU  - Wang, Ziqi; Song, Peng; Pauly, Mark
TI  - State of the Art on Computational Design of Assemblies with Rigid Parts
PY  - 2021
AB  - NA
SP  - 633
EP  - 657
JF  - Computer Graphics Forum
VL  - 40
IS  - 2
PB  - 
DO  - 10.1111/cgf.142660
ER  - 

TY  - NA
AU  - Yamaoka, Junichi; Dogan, Mustafa Doga; Bulovic, Katarina; Saito, Kazuya; Kawahara, Yoshihiro; Kakehi, Yasuaki; Mueller, Stefanie
TI  - FoldTronics Demo
PY  - 2019
AB  - We present FoldTronics, a 2D-cutting based fabrication technique to integrate electronics into 3D folded objects. The key idea is to cut and perforate a 2D sheet to make it foldable into a honeycomb structure using a cutting plotter; before folding the sheet into a 3D structure, users place the electronic components and circuitry onto the sheet. The fabrication process only takes a few minutes enabling users to rapidly prototype functional interactive devices. The resulting objects are lightweight and rigid, thus allowing for weight-sensitive and force-sensitive applications. Finally, due to the nature of the honeycomb structure, the objects can be folded flat along one axis and thus can be efficiently transported in this compact form factor. We describe the structure of the foldable sheet, and present a design tool that enables users to quickly prototype the desired objects. We showcase a range of examples made with our design tool, including objects with integrated sensors and display elements.Japan Science and Technology Corporation. Exploratory Research for Advanced Technology research funding program (Grant no. JPMJER1501
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3313242
ER  - 

TY  - JOUR
AU  - Wu, Chenming; Liu, Yong-Jin; Wang, Charlie C. L.
TI  - Learning to Accelerate Decomposition for Multi-Directional 3D Printing
PY  - 2020
AB  - Multi-directional 3D printing has the capability of decreasing or eliminating the need for support structures. Recent work proposed a beam-guided search algorithm to find an optimized sequence of plane-clipping, which gives volume decomposition of a given 3D model. Different printing directions are employed in different regions to fabricate a model with tremendously less support (or even no support in many cases). To obtain optimized decomposition, a large beam width needs to be used in the search algorithm, leading to a very time-consuming computation. In this letter, we propose a learning framework that can accelerate the beam-guided search by using a smaller number of the original beam width to obtain results with similar quality. Specifically, we use the results of beam-guided search with large beam width to train a scoring function for candidate clipping planes based on six newly proposed feature metrics. With the help of these feature metrics, both the current and the sequence-dependent information are captured by the neural network to score candidates of clipping. As a result, we can achieve around 3× computational speed. We test and demonstrate our accelerated decomposition on a large dataset of models for 3D printing.
SP  - 5897
EP  - 5904
JF  - IEEE Robotics and Automation Letters
VL  - 5
IS  - 4
PB  - 
DO  - 10.1109/lra.2020.3011369
ER  - 

TY  - JOUR
AU  - Leins, David P.; Gibas, Christian; Brück, Rainer; Haschke, Robert
TI  - Toward More Robust Hand Gesture Recognition on EIT Data.
PY  - 2021
AB  - Striving for more robust and natural control of multi-fingered hand prostheses, we are studying electrical impedance tomography (EIT) as a method to monitor residual muscle activations. Previous work has shown promising results for hand gesture recognition, but also lacks generalization across multiple sessions and users. Thus, the present paper aims for a detailed analysis of an existing EIT dataset acquired with a 16-electrode wrist band as a prerequisite for further improvement of machine learning results on this type of signals. The performed t-SNE analysis confirms a much stronger inter-session and inter-user variance compared to the expected in-class variance. Additionally, we observe a strong drift of signals within a session. To handle these challenging problems, we propose new machine learning architectures based on deep learning, which allow to separate undesired from desired variation and thus significantly improve the classification accuracy. With these new architectures we increased cross-session classification accuracy on 12 gestures from 19.55% to 30.45%. Based on a fundamental data analysis we developed three calibration methods and thus were able to further increase cross-session classification accuracy to 39.01%, 55.37%, and 56.34%, respectively.
SP  - 659311
EP  - NA
JF  - Frontiers in neurorobotics
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnbot.2021.659311
ER  - 

TY  - NA
AU  - Truong, Hoang; Zhang, Shuo; Muncuk, Ufuk; Nguyen, Phuc; Bui, Nam; Nguyen, Anh; Lv, Qin; Chowdhury, Kaushik R.; Dinh, Thang N.; Vu, Tam
TI  - SenSys - CapBand: Battery-free Successive Capacitance Sensing Wristband for Hand Gesture Recognition
PY  - 2018
AB  - We present CapBand, a battery-free hand gesture recognition wearable in the form of a wristband. The key challenges in creating such a system are (1) to sense useful hand gestures at ultra-low power so that the device can be powered by the limited energy harvestable from the surrounding environment and (2) to make the system work reliably without requiring training every time a user puts on the wristband. We present successive capacitance sensing, an ultra-low power sensing technique, to capture small skin deformations due to muscle and tendon movements on the user's wrist, which corresponds to specific groups of wrist muscles representing the gestures being performed. We build a wrist muscles-to-gesture model, based on which we develop a hand gesture classification method using both motion and static features. To eliminate the need for per-usage training, we propose a kernel-based on-wrist localization technique to detect the CapBand's position on the user's wrist. We prototype CapBand with a custom-designed capacitance sensor array on two flexible circuits driven by a custom-built electronic board, a heterogeneous material-made, deformable silicone band, and a custom-built energy harvesting and management module. Evaluations on 20 subjects show 95.0% accuracy of gesture recognition when recognizing 15 different hand gestures and 95.3% accuracy of on-wrist localization.
SP  - 54
EP  - 67
JF  - Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3274783.3274854
ER  - 

TY  - NA
AU  - Babaei, NA; Al-Jemeli, Marwan; Avazpour, Iman
TI  - OZCHI - UGI: a multi-dimensional ultrasonic-based interaction approach
PY  - 2016
AB  - We are currently witnessing an era where interaction with computers is no longer limited to conventional methods (i.e. keyboard and mouse). Human Computer Interaction (HCI) as a progressive field of research, has opened up alternatives to the traditional interaction techniques. Embedded Infrared (IR) sensors, Accelerometers and RGBD cameras have become common inputs for devices to recognize gestures and body movements. These sensors are vision based and as a result the devices that incorporate them will be reliant on presence of light. Ultrasonic sensors on the other hand do not suffer this limitation as they utilize properties of sound waves. These sensors however, have been mainly used for distance detection and not with HCI devices. This paper presents our approach in developing a multi-dimensional interaction input method and tool Ultrasonic Gesture-based Interaction (UGI) that utilizes ultrasonic sensors. We demonstrate how these sensors can detect object movements and recognize gestures. We present our approach in building the device and demonstrate sample interactions with it. We have also conducted a user study to evaluate our tool and its distance and micro gesture detection accuracy. This paper reports these results and outlines our future work in the area.
SP  - 363
EP  - 370
JF  - Proceedings of the 28th Australian Conference on Computer-Human Interaction - OzCHI '16
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3010915.3010949
ER  - 

TY  - JOUR
AU  - Wang, Yuntao; Zhou, Jianyu; Li, Hanchuan; Zhang, Tengxiang; Gao, Minxuan; Cheng, Zhuolin; Yu, Chun; Patel, Shwetak N.; Shi, Yuanchun
TI  - FlexTouch: Enabling Large-Scale Interaction Sensing Beyond Touchscreens Using Flexible and Conductive Materials
PY  - 2019
AB  - In this paper, we present FlexTouch, a technique that enables large-scale interaction sensing beyond the spatial constraints of capacitive touchscreens using passive low-cost conductive materials. This is achieved by customizing 2D circuit-like patterns with an array of conductive strips that can be easily attached to the sensing nodes on the edge of the touchscreen. FlexTouch requires no hardware modification, and is compatible with various conductive materials (copper foil tape, silver nanoparticle ink, ITO frames, and carbon paint), as well as fabrication methods (cutting, coating, and ink-jet printing). Through a series of studies and illustrative examples, we demonstrate that FlexTouch can support long-range touch sensing for up to 4 meters and everyday object presence detection for up to 2 meters. Finally, we show the versatility and feasibility of FlexTouch through applications such as body posture recognition, human-object interaction as well as enhanced fitness training experiences.
SP  - 109
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3351267
ER  - 

TY  - NA
AU  - Kiani, Kimia; Cui, George; Bunt, Andrea; McGrenere, Joanna; Chilana, Parmit K.
TI  - CHI - Beyond "One-Size-Fits-All": Understanding the Diversity in How Software Newcomers Discover and Make Use of Help Resources
PY  - 2019
AB  - For most modern feature-rich software, considerable external help and learning resources are available on the web (e.g., documentation, tutorials, videos, Q&A forums). But, how do users new to an application discover and make use of such resources? We conducted in-lab and diary studies with 26 software newcomers from a variety of different backgrounds who were all using Fusion 360, a 3D modeling application, for the first time. Our results illustrate newcomers' diverse needs, perceptions, and help-seeking behaviors. We found a number of distinctions in how technical and non-technical users approached help-seeking, including: when and how they initiated the help-seeking process, their struggles in recognizing relevant help, the degree to which they made coordinated use of the application and different resources, and in how they perceived the utility of different help formats. We discuss implications for moving beyond "one-size-fits-all" help resources towards more structured, personalized, and curated help and learning materials.
SP  - 340
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300570
ER  - 

TY  - JOUR
AU  - Kawaguchi, Junki; Yoshimoto, Shunsuke; Kuroda, Yoshihiro; Oshiro, Osamu
TI  - Estimation of Finger Joint Angles Based on Electromechanical Sensing of Wrist Shape
PY  - 2016
AB  - An approach to finger motion capture that places fewer restrictions on the usage environment and actions of the user is an important research topic in biomechanics and human-computer interaction. We proposed a system that electrically detects finger motion from the associated deformation of the wrist and estimates the finger joint angles using multiple regression models. A wrist-mounted sensing device with 16 electrodes detects deformation of the wrist from changes in electrical contact resistance at the skin. In this study, we experimentally investigated the accuracy of finger joint angle estimation, the adequacy of two multiple regression models, and the resolution of the estimation of total finger joint angles. In experiments, both the finger joint angles and the system output voltage were recorded as subjects performed flexion/extension of the fingers. These data were used for calibration using the least-squares method. The system was found to be capable of estimating the total finger joint angle with a root-mean-square error of 29–34 degrees. A multiple regression model with a second-order polynomial basis function was shown to be suitable for the estimation of all total finger joint angles, but not those of the thumb.
SP  - 1409
EP  - 1418
JF  - IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society
VL  - 25
IS  - 9
PB  - 
DO  - 10.1109/tnsre.2016.2626800
ER  - 

TY  - NA
AU  - Gannon, Madeline; Grossman, Tovi; Fitzmaurice, George
TI  - CHI - ExoSkin: On-Body Fabrication
PY  - 2016
AB  - There is a long tradition for crafting wearable objects directly on the body, such as garments, casts, and orthotics. However, these high-skill, analog practices have yet to be augmented by digital fabrication techniques. In this paper, we explore the use of hybrid fabrication workflows for on-body printing. We outline design considerations for creating on-body fabrication systems, and identify several human, machine, and material challenges unique to this endeavor. Based on our explorations, we present ExoSkin, a hybrid fabrication system for designing and printing digital artifacts directly on the body. ExoSkin utilizes a custom built fabrication machine designed specifically for on-body printing. We demonstrate the potential of on-body fabrication with a set of sample workflows, and share feedback from initial observation sessions.
SP  - 5996
EP  - 6007
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858576
ER  - 

TY  - NA
AU  - Qamar, Isabel P. S.; Groh, Rainer; Holman, David; Roudaut, Anne
TI  - CHI - HCI meets Material Science: A Literature Review of Morphing Materials for the Design of Shape-Changing Interfaces
PY  - 2018
AB  - With the proliferation of flexible displays and the advances in smart materials, it is now possible to create interactive devices that are not only flexible but can reconfigure into any shape on demand. Several Human Computer Interaction (HCI) and robotics researchers have started designing, prototyping and evaluating shape-changing devices, realising, however, that this vision still requires many engineering challenges to be addressed. On the material science front, we need breakthroughs in stable and accessible materials to create novel, proof-of-concept devices. On the interactive devices side, we require a deeper appreciation for the material properties and an understanding of how exploiting material properties can provide affordances that unleash the human interactive potential. While these challenges are interesting for the respective research fields, we believe that the true power of shape-changing devices can be magnified by bringing together these communities. In this paper we therefore present a review of advances made in shape-changing materials and discuss their applications within an HCI context.
SP  - 374
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173948
ER  - 

TY  - NA
AU  - Kerber, Frederic; Puhl, Michael; Krüger, Antonio
TI  - MobileHCI - User-independent real-time hand gesture recognition based on surface electromyography
PY  - 2017
AB  - In this paper, we present a novel real-time hand gesture recognition system based on surface electromyography. We employ a user-independent approach based on a support vector machine utilizing ten features extracted from the raw electromyographic data obtained from the Myo armband by Thalmic Labs. Through an improved synchronization approach, we simplified the application process of the sensing armband. We report the results of a user study with 14 participants using an extended set consisting of 40 gestures. Considering the set of five hand gestures currently supported off-the-shelf by the Myo armband, we outperform their approach with an overall accuracy of 95% compared to 68% with the original algorithm on the same dataset.
SP  - 36
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3098553
ER  - 

TY  - NA
AU  - Chang, Zekun; Kim, Heeju; Kato, Kunihiro; Saito, Kazuya; Ta, Tung D.; Jiang, Weiwei; Narumi, Koya; Miyamoto, Yoshinobu; Kawahara, Yoshihiro
TI  - CHI Extended Abstracts - Kirigami Keyboard: Inkjet Printable Paper Interface with Kirigami Structure Presenting Kinesthetic Feedback
PY  - 2019
AB  - We propose a DIY process to produce customized paper keyboards with kinesthetic feedback that interact with touchscreens. The process is built using two techniques: kirigami and printable double-layered circuits. Our goal is to improve the extensibility and usability of various interfaces made with 2D paper substrates. First, Our kirigami structures provide kinesthetic sensations whose z-directional key stroke is comparable to that of traditional keyboards. In order to design keys with appropriate stroke and reaction force, we adopted the Rotational Erection System (RES). Second, printable double-layered circuits allow users to easily adjust input layouts. This easy-to-customize keyboard can be especially useful for those who have specific requirements for input devices.
SP  - 3312757
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312757
ER  - 

TY  - JOUR
AU  - Ezair, Ben; Fuhrmann, Saul; Elber, Gershon
TI  - Volumetric covering print-paths for additive manufacturing of 3D models
PY  - 2018
AB  - NA
SP  - 1
EP  - 13
JF  - Computer-Aided Design
VL  - 100
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2018.02.006
ER  - 

TY  - JOUR
AU  - Suto, Kai; Noma, Yuta; Tanimichi, Kotaro; Narumi, Koya; Tachi, Tomohiro
TI  - Crane: An Integrated Computational Design Platformfor Functional, Foldable, and Fabricable Origami Products
PY  - 2022
AB  - <jats:p>Despite the recent trend of computational origami for human-computer interaction (HCI) and digital fabrication, it is still difficult for designers to complete a series of design, simulation, and fabrication of objects leveraging computational origami theory. In this paper, we propose Crane, an integrated origami design platform implemented with Grasshopper. With this platform, users can seamlessly (1) design the 2D and 3D crease pattern, (2) simulate 3D folding transformation from the given crease pattern, (3) inversely find a new pattern under design constraints, (4) thicken the 2D pattern into a 3D volume along with the appropriate hinge structures for different fabrication methods, and (5) optionally connect the resulting design to other Rhinoceros or Grasshopper plugins for post-processes. To help understand how to use our system and demonstrate its feasibility, we showed three examples of origami products designed using our system. We also reported user feedback from the workshop as an evaluation.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3576856
ER  - 

TY  - JOUR
AU  - Mariello, Massimo; Fachechi, Luca; Guido, Francesco; De Vittorio, Massimo
TI  - Conformal, Ultra-thin Skin-Contact-Actuated Hybrid Piezo/Triboelectric Wearable Sensor Based on AlN and Parylene-Encapsulated Elastomeric Blend
PY  - 2021
AB  - NA
SP  - 2101047
EP  - NA
JF  - Advanced Functional Materials
VL  - 31
IS  - 27
PB  - 
DO  - 10.1002/adfm.202101047
ER  - 

TY  - NA
AU  - Fang, Chiao; Chan, Vivian Hsinyueh; Cheng, Lung-Pan
TI  - Flaticulation: Laser Cutting Joints with Articulated Angles
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545695
ER  - 

TY  - NA
AU  - Peng, Huaishu; Guimbretière, François; McCann, James; Hudson, Scott E.
TI  - UIST - A 3D Printer for Interactive Electromagnetic Devices
PY  - 2016
AB  - We introduce a new form of low-cost 3D printer to print interactive electromechanical objects with wound in place coils. At the heart of this printer is a mechanism for depositing wire within a five degree of freedom (5DOF) fused deposition modeling (FDM) 3D printer. Copper wire can be used with this mechanism to form coils which induce magnetic fields as a current is passed through them. Soft iron wire can additionally be used to form components with high magnetic permeability which are thus able to shape and direct these magnetic fields to where they are needed. When fabricated with structural plastic elements, this allows simple but complete custom electromagnetic devices to be 3D printed. As examples, we demonstrate the fabrication of a solenoid actuator for the arm of a Lucky Cat figurine, a 6-pole motor stepper stator, a reluctance motor rotor and a Ferrofluid display. In addition, we show how printed coils which generate small currents in response to user actions can be used as input sensors in interactive devices.
SP  - 553
EP  - 562
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984523
ER  - 

TY  - NA
AU  - Leen, Danny; Peek, Nadya; Ramakers, Raf
TI  - UIST - LamiFold: Fabricating Objects with Integrated Mechanisms Using a Laser cutter Lamination Workflow
PY  - 2020
AB  - We present LamiFold, a novel design and fabrication workflow for making functional mechanical objects using a laser cutter. Objects fabricated with LamiFold embed advanced rotary, linear, and chained mechanisms, including linkages that support fine-tuning and locking position. Laser cutting such mechanisms without LamiFold requires designing for and embedding off-the-shelf parts such as springs, bolts, and axles for gears. The key to laser cutting our functional mechanisms is the selective cutting and gluing of stacks of sheet material. Designing mechanisms for this workflow is non-trivial, therefore we contribute a set of mechanical primitives that are compatible with our lamination workflow and can be combined to realize advanced mechanical systems. Our software design environment facilitates the process of inserting and composing our mechanical primitives and realizing functional laser-cut objects.
SP  - 304
EP  - 316
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415885
ER  - 

TY  - NA
AU  - Ulu, Erva; Ulu, Nurcan Gecer; Li, Jiahao; Hsiao, Walter
TI  - Curvy: An Interactive Design Tool for Varying Density Support Structures.
PY  - 2021
AB  - We introduce Curvy-an interactive design tool to generate varying density support structures for 3D printing. Support structures are essential for printing models with extreme overhangs. Yet, they often cause defects on contact areas, resulting in poor surface quality. Low-level design of support structures may alleviate such negative effects. However, it is tedious and unintuitive for novice users as it is hard to predict the impact of changes to the support structure on the final printed part. Curvy allows users to define their high-level preferences on the surface quality directly on the target object rather than explicitly designing the supports. These preferences are then automatically translated into low-level design parameters to generate the support structure. Underlying novel curvy zigzag toolpathing algorithm uses these instructions to generate varying density supports by altering the spacing between individual paths in order to achieve prescribed quality. Combined with the build orientation optimization, Curvy provides a practical solution to the design of support structures with minimal perceptual or functional impact on the target part to be printed.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wessely, Michael; Sethapakdi, Ticha; Castillo, Carlos; Snowden, Jackson C.; Hanton, Ollie; Qamar, Isabel P. S.; Fraser, Mike; Roudaut, Anne; Mueller, Stefanie
TI  - CHI - Sprayable User Interfaces: Prototyping Large-Scale Interactive Surfaces with Sensors and Displays
PY  - 2020
AB  - We present Sprayable User Interfaces: room-sized interactive surfaces that contain sensor and display elements created by airbrushing functional inks. Since airbrushing is inherently mobile, designers can create large-scale user interfaces on complex 3D geometries where existing stationary fabrication methods fail. To enable Sprayable User Interfaces, we developed a novel design and fabrication pipeline that takes a desired user interface layout as input and automatically generates stencils for airbrushing the layout onto a physical surface. After fabricating stencils from cardboard or projecting stencils digitally, designers spray each layer with an airbrush, attach a microcontroller to the user interface, and the interface is ready to be used. Our technical evaluation shows that Sprayable User Interfaces work on various geometries and surface materials, such as porous stone and rough wood. We demonstrate our system with several application examples including interactive smart home applications on a wall and a soft leather sofa, an interactive smart city application, and interactive architecture in public office spaces.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376249
ER  - 

TY  - JOUR
AU  - Xu, Ke; Chen, Lufeng; Tang, Kai
TI  - Support-Free Layered Process Planning Toward 3 + 2-Axis Additive Manufacturing
PY  - 2019
AB  - For the traditional three-axis configuration of additive manufacturing (AM) platform, it is imperative to construct adequate support structures prior to the fabrication of overhanging features on the part geometry. To completely eliminate the use of support by taking advantage of the newly emerged five-axis AM platform, a novel multidirectional process planning algorithm for 3 + 2-axis AM is proposed in this paper. The core of the strategy is to decompose the model into support-free parts directly pertaining to the cusp-height constraint, each with its own build direction. The nozzle will follow the staged sequence to fabricate each individual part along its build direction, as facilitated by the adjustable orientation of the nozzle head on a five-axis platform. For model decomposition, a recursive downward flooding expansion algorithm is introduced to identify a surface patch in accordance with the support-free criteria. After being stitched as a watertight geometry, the decomposed part together with the remaining part will be archived into a prescribed hierarchy based on which the build sequence is readily established. Preliminary testing results have verified the effectiveness of the algorithm to handle the geometries of different types. Note to Practitioners —This paper was motivated by the deficiencies encountered in traditional three-axis additive manufacturing (AM). In practice, most commercialized 3-D printers are of three-axis configuration, on which it is required to construct support structures for those overhanging features on the part geometry. These support structures not only cost extra time and materials but also leave noticeable artifacts on the part surface even after the clean-up process. As an ultimate solution to this issue, the newly emerged five-axis AM platform is able to physically eliminate the need for support by properly and continuously adjusting the build direction. This, however, demands a delicate algorithm for determining the changing build direction for an arbitrary part that is currently unavailable. To simplify this challenging task, we took advantage of the 3 + 2 motion capability of the five-axis AM platform to progressively plan the three-axis printing process for a given freeform model. The core idea of our solution is to decompose the model by a flooding-like algorithm into individual support-free parts, each of which is printable along a fixed build direction without any support. The nozzle will then follow a prescribed sequence to print each part one on the other. The proposed algorithm is purely geometric, without taking into account the material property and the mass distribution. Preliminary testing results show that our approach is feasible and robust when dealing with limited geometries that abide a tree-like structure and features a clear flat base. Besides these geometric limitations, our current preliminary scheme for collision avoidance may become invalid upon an arbitrary complex geometry. We will further extend this method to cater to more sophisticated and general geometries.
SP  - 838
EP  - 850
JF  - IEEE Transactions on Automation Science and Engineering
VL  - 16
IS  - 2
PB  - 
DO  - 10.1109/tase.2018.2867230
ER  - 

TY  - NA
AU  - Devrio, Nathan; Harrison, Chris
TI  - DiscoBand: Multiview Depth-Sensing Smartwatch Strap for Hand, Body and Environment Tracking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545634
ER  - 

TY  - NA
AU  - Shi, Lei; Zhao, Yuhang; Azenkot, Shiri
TI  - ASSETS - Designing Interactions for 3D Printed Models with Blind People
PY  - 2017
AB  - Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.
SP  - 200
EP  - 209
JF  - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132525.3132549
ER  - 

TY  - CHAP
AU  - Yao, Cheng; Tao, Ye; Zhang, Ting; Wang, Guanyun; Ying, Fangtian
TI  - HCI (21) - A Programming Cutting System to Enhance Productivity with Individualities
PY  - 2017
AB  - As an alternative to traditional laser cutting, we present LaserLeast, a fabrication-oriented design system that produce 3D objects using a laser cutter to bridge the gaps in prototyping from software to hardware and further evoke energy- and resource-saving consciousness in rapid manufacturing context. The key idea behind our system is that it employs a line-based strategy to improve the original workflow at the 2D output arrangement stage by using two shapes sharing one cutting line, and create a line-based workflow without a 3D modeling stage using three customizable components (line shape, 2.5D technique, and assemble plug). We compare LaserLeast with the traditional strategy with two use cases to address the advantages and limitations and show that LaserLeast has the potential to stimulate the creativity of product designers and the human computer interaction community to enhance productivity in both design and laser cutting through the Internet.
SP  - 561
EP  - 571
JF  - Distributed, Ambient and Pervasive Interactions
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-58697-7_42
ER  - 

TY  - JOUR
AU  - Grushko, Stefan; Spurný, Tomáš; Černý, Martin
TI  - Control methods for transradial prostheses based on remnant muscle activity and its relationship with proprioceptive feedback
PY  - 2020
AB  - The loss of a hand can significantly affect one's work and social life. For many patients, an artificial limb can improve their mobility and ability to manage everyday activities, as well as provide the means to remain independent. This paper provides an extensive review of available biosensing methods to implement the control system for transradial prostheses based on the measured activity in remnant muscles. Covered techniques include electromyography, magnetomyography, electrical impedance tomography, capacitance sensing, near-infrared spectroscopy, sonomyography, optical myography, force myography, phonomyography, myokinetic control, and modern approaches to cineplasty. The paper also covers combinations of these approaches, which, in many cases, achieve better accuracy while mitigating the weaknesses of individual methods. The work is focused on the practical applicability of the approaches, and analyses present challenges associated with each technique along with their relationship with proprioceptive feedback, which is an important factor for intuitive control over the prosthetic device, especially for high dexterity prosthetic hands.
SP  - 4883
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 17
PB  - 
DO  - 10.3390/s20174883
ER  - 

TY  - NA
AU  - Weigel, Martin; Nittala, Aditya Shekhar; Olwal, Alex; Steimle, Jürgen
TI  - CHI - SkinMarks: Enabling Interactions on Body Landmarks Using Conformal Skin Electronics
PY  - 2017
AB  - The body provides many recognizable landmarks due to the underlying skeletal structure and variations in skin texture, elasticity, and color. The visual and spatial cues of such body landmarks can help in localizing on-body interfaces, guide input on the body, and allow for easy recall of mappings. Our main contribution are SkinMarks, novel skin-worn I/O devices for precisely localized input and output on fine body landmarks. SkinMarks comprise skin electronics on temporary rub-on tattoos. They conform to fine wrinkles and are compatible with strongly curved and elastic body locations. We identify five types of body landmarks and demonstrate novel interaction techniques that leverage SkinMarks' unique touch, squeeze and bend sensing with integrated visual output. Finally, we detail on the conformality and evaluate sub-millimeter electrodes for touch sensing. Taken together, SkinMarks expands the on-body interaction space to more detailed, highly curved and challenging areas on the body.
SP  - 3095
EP  - 3105
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025704
ER  - 

TY  - NA
AU  - Rawn, Eric; Li, Jingyi
TI  - CHI Extended Abstracts - Laser Cut Layered Gels for Lighting Design
PY  - 2020
AB  - Recent advancements in lighting design have focused on the visualization and simulation of programmable LED lighting fixtures. However, single-bulb conventional fixtures alongside subtractive color filter gels are still widely used in many art galleries and installations, photography studios, and experimental theatres due to their low cost and existing prevalence in industry. We introduce a novel approach to creating lighting effects for single-bulb fixtures with gels, which enables designers to quickly and inexpensively produce complex, multi-colored effects approximating a target digital image. Our system uses a grid-based approach which cuts small openings in different colored gels and layers them together, forming color combinations when lit. Our work expands the design space of lighting gels with a precise and expressive method, enabling designers to experiment with novel lighting effects through an iterative personal fabrication process.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382861
ER  - 

TY  - JOUR
AU  - Son, Choonghyun; Kim, SeulGee; Kim, Seung Jong; Choi, Junho; Kim, Dae Eun
TI  - Detection of muscle activation through multi-electrode sensing using electrical stimulation
PY  - 2018
AB  - NA
SP  - 19
EP  - 28
JF  - Sensors and Actuators A: Physical
VL  - 275
IS  - NA
PB  - 
DO  - 10.1016/j.sna.2018.03.030
ER  - 

TY  - CHAP
AU  - Ng, Him Wai; Jiang, Xianta; Merhi, Lukas-Karim; Menon, Carlo
TI  - IWBBIO (1) - Investigation of the Feasibility of Strain Gages as Pressure Sensors for Force Myography
PY  - 2017
AB  - Hand gesture recognition is a popular topic of many research studies, and force myography (FMG) has recently emerged for this application. This work investigates a novel sensor system based on electrical resistance strain gages that is fully wearable and easy-to-use. This system consists of eight strain gages embedded in a transparent flexible plastic band, covering the entire wrist. The system was tested with 8 subjects by performing 14 different hand gestures, with an accuracy of 99.2% using support vector machine. The impressive accuracy of the wearable band confirms the capability of strain gages as pressure sensors in force myography in hand gesture recognition applications.
SP  - 261
EP  - 270
JF  - Bioinformatics and Biomedical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-56148-6_22
ER  - 

TY  - CHAP
AU  - Wang, Lihui; Wang, Xi Vincent
TI  - Context-Aware Human-Robot Collaborative Assembly
PY  - 2017
AB  - In human-robot collaborative manufacturing, industrial robots would work alongside the human workers who jointly perform the assigned tasks. Recent research work revealed that recognised human motions could be used as input for industrial robots control. However, the human-robot collaboration team still cannot work symbiotically. In response to the requirement, this chapter explores the potential of establishing context awareness between a human worker and an industrial robot for human-robot collaborative assembly. The context awareness between the human worker and the industrial robot is established by applying gesture recognition, human motion recognition and Augmented Reality (AR) based worker instruction technologies. Such a system works in a cyber-physical environment and is demonstrated by case studies.
SP  - 261
EP  - 294
JF  - Cloud-Based Cyber-Physical Systems in Manufacturing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-67693-7_11
ER  - 

TY  - JOUR
AU  - Lu, Duo; Xu, Kai; Huang, Dijiang
TI  - IJCB - A data driven in-air-handwriting biometric authentication system
PY  - 2017
AB  - The gesture-based human-computer interface requires new user authentication technique because it does not have traditional input devices like keyboard and mouse. In this paper, we propose a new finger-gesture-based authentication method, where the in-air-handwriting of each user is captured by wearable inertial sensors. Our approach is featured with the utilization of both the content and the writing convention, which are proven to be essential for the user identification problem by the experiments. A support vector machine (SVM) classifier is built based on the features extracted from the hand motion signals. To quantitatively benchmark the proposed framework, we build a prototype system with a custom data glove device. The experiment result shows our system achieve a 0.1% equal error rate (EER) on a dataset containing 200 accounts that are created by 116 users. Compared to the existing gesture-based biometric authentication systems, the proposed method delivers a significant performance improvement.
SP  - 531
EP  - 537
JF  - 2017 IEEE International Joint Conference on Biometrics (IJCB)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/btas.2017.8272739
ER  - 

TY  - NA
AU  - Wu, Yu; Jiang, Dai; Duan, Jifang; Liu, Xiao; Bayford, Richard; Demosthenous, Andreas
TI  - ISCAS - Towards a High Accuracy Wearable Hand Gesture Recognition System Using EIT
PY  - 2018
AB  - This paper presents a high accuracy hand gesture recognition system based on electrical impedance tomography (EIT). The system interfaces the forearm using a wrist wrap with embedded electrodes. It measures the inner conductivity distributions caused by bone and muscle movement of the forearm in real-time and passes the data to a deep learning neural network for gesture recognition. The system has an EIT bandwidth of 500 kHz and a measured sensitivity in excess of 6.4 Ω per frame. Nineteen hand gestures are designed for recognition, and with the proposed round robin sub-grouping method, an accuracy of over 98% is achieved.
SP  - 1
EP  - 4
JF  - 2018 IEEE International Symposium on Circuits and Systems (ISCAS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iscas.2018.8351296
ER  - 

TY  - NA
AU  - Zhu, Junyi; Snowden, Jackson C; Verdejo, Joshua; Chen, Emily; Zhang, Paul; Ghaednia, Hamid; Schwab, Joseph H; Mueller, Stefanie
TI  - EIT-kit Demo: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480185
ER  - 

TY  - JOUR
AU  - Rich, Steven; Jiang, Zhi; Fukuda, Kenjiro; Someya, Takao
TI  - Well-rounded devices: the fabrication of electronics on curved surfaces – a review
PY  - 2021
AB  - With the arrival of the internet of things and the rise of wearable computing, electronics are playing an increasingly important role in our everyday lives. Until recently, however, the rigid angular nature of traditional electronics has prevented them from being integrated into many of the organic, curved shapes that interface with our bodies (such as ergonomic equipment or medical devices) or the natural world (such as aerodynamic or optical components). In the past few years, many groups working in advanced manufacturing and soft robotics have endeavored to develop strategies for fabricating electronics on these curved surfaces. This is their story. In this work, we describe the motivations, challenges, methodologies, and applications of curved electronics, and provide a outlook for this promising field.
SP  - 1926
EP  - 1958
JF  - Materials horizons
VL  - 8
IS  - 7
PB  - 
DO  - 10.1039/d1mh00143d
ER  - 

TY  - NA
AU  - Zheng, Clement; Yi-Luen, Ellen; Budd, Jim
TI  - Creativity &amp; Cognition - Joinery: Parametric Joint Generation for Laser Cut Assemblies
PY  - 2017
AB  - Laser cutting is widely used by industrial designers and mechanical engineers as a rapid modeling tool. However, designing and fabricating laser cut assemblies can be a complex and tedious process, especially for novice designers. Through our research, we developed Joinery, a parametric joint generation tool for laser cut assemblies. Through Joinery, designers simply define connections between parts of an assembly, while the system generates the joints. Joinery supports fabrication-aware design through six different joint profiles that cater to different material and design needs. In this paper, we illustrate the use of Joinery as a creativity support tool in an industrial design process, and present several artifacts resulting from the tool. In addition, we discuss our findings from deploying this system in a college-level industrial design class.
SP  - 63
EP  - 74
JF  - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3059454.3059459
ER  - 

TY  - CHAP
AU  - Butt, Abdul Haleem; Moschetti, Alessandra; Fiorini, Laura; Dario, Paolo; Cavallo, Francesca Romana
TI  - Wearable sensors for gesture analysis in smart healthcare applications
PY  - 2017
AB  - Technological solutions represent new opportunities to help elderly people and their caregivers in daily life. Understanding human behavior becomes thus essential in Ambient Assisted Living field especially for prevention and monitoring applications. In particular, recognition of human gestures is important to deliver personalized service to keep elderly people independent, while being monitored by caregivers. This chapter aims to underline the importance of recognizing behavior and in particular gesture in order to monitor older persons. An overview of the gesture recognition applications in AAL is therefore presented with a focus on the existing technologies used to capture hand gestures. Algorithms for data processing and classification are also described. Finally, an example of daily gesture recognition in AAL is presented where different gestures are recognized by mean of SensHand.
SP  - 79
EP  - 102
JF  - Human Monitoring, Smart Health and Assisted Living: Techniques and technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1049/pbhe009e_ch4
ER  - 

TY  - JOUR
AU  - Fossdal, Frikk H.; Heldal, Rogardt; Dyvik, Jens; Rutle, Adrian
TI  - Fabricatable axis: an approach for modelling customized fabrication machines
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Digital fabrication tools such as 3D printers, computer-numerically controlled (CNC) milling machines, and laser cutters are becoming increasingly available, ranging from consumer to industrial versions. Recent studies have shown that users, ranging from researchers, to industry professionals, to hobbyists, are interested in modifying and changing the inherit workflows these tools provide. As an answer to this, these users are increasingly modifying and customizing their machines by changing the work envelope, adding different end-effectors, and creating their own fabrication workflows in software. However, customizing, modifying and creating digital fabrication machines and the workflows they provide require extensive knowledge within multiple different engineering domains and is non-trivial. In this article we present a model-driven approach that enables users to expand their digital fabrication scope by providing a high-level tool that facilitates the customization of fabrication tools. We present The Farbicatable Axis, a model that enables users to create customized linear actuators. The model takes high-level input parameters such as length and gearing-parameters, and outputs a CAD model of a linear motion axis consisting of <jats:italic>fabricatable</jats:italic> parts. We then present how instances of the Fabricatable Axis can be combined and used to design and implement Fabricatable Machines.</jats:p>
SP  - 1907
EP  - 1929
JF  - Software and Systems Modeling
VL  - 21
IS  - 5
PB  - 
DO  - 10.1007/s10270-022-01007-y
ER  - 

TY  - BOOK
AU  - Kim, Hongmin; Kim, MyungSung; Oakley, Ian
TI  - WearSys@MobiSys - A Mobility Evaluation of Tilt Panning and Offset Sensing Smart Watch Input
PY  - 2019
AB  - Small smartwatch touchscreens restrict input and researchers have long explored how alternative modalities and techniques can enable new possibilities and boost expressiveness. However, these techniques are generally designed in and for static, stable settings; mobility issues are rarely considered. To address this omission, we conducted a mobility study that compares two recently proposed alternative smartwatch input modalities: physical movements of a watch on the wrist, and an offset touch sensor on the edge of the device. We observed high selection errors for tilt input (23.89% to 34.22%) and prolonged times for offset sensing (>1000ms). We propose a combined input technique designed to fit the constraints of mobile watch use: touches to the device edge stabilize and constrain input, while tilt and touch control and trigger it. A second study shows this design can improve target selection time while mobile to less than 800ms with error rates of 10.2%.
SP  - 5
EP  - 10
JF  - The 5th ACM Workshop on Wearable Systems and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3325424.3329665
ER  - 

TY  - JOUR
AU  - Nam, Hyoungsik; Seol, Ki-Hyuk; Lee, Junhee; Cho, Hyeonseong; Jung, Sang Won
TI  - Review of Capacitive Touchscreen Technologies: Overview, Research Trends, and Machine Learning Approaches.
PY  - 2021
AB  - Touchscreens have been studied and developed for a long time to provide user-friendly and intuitive interfaces on displays. This paper describes the touchscreen technologies in four categories of resistive, capacitive, acoustic wave, and optical methods. Then, it addresses the main studies of SNR improvement and stylus support on the capacitive touchscreens that have been widely adopted in most consumer electronics such as smartphones, tablet PCs, and notebook PCs. In addition, the machine learning approaches for capacitive touchscreens are explained in four applications of user identification/authentication, gesture detection, accuracy improvement, and input discrimination.
SP  - 4776
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 21
IS  - 14
PB  - 
DO  - 10.3390/s21144776
ER  - 

TY  - NA
AU  - Zhou, Junhan; Zhang, Yang; Laput, Gierad; Harrison, Chris
TI  - UIST - AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing
PY  - 2016
AB  - Existing smartwatches rely on touchscreens for display and input, which inevitably leads to finger occlusion and confines interactivity to a small area. In this work, we introduce AuraSense, which enables rich, around-device, smartwatch interactions using electric field sensing as an adapted device. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities. We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above or in close proximity to watches, and touchscreen-like finger tracking on the skin. We quantify the feasibility of these input modalities, suggesting that AuraSense can be low latency and robust across users and environments.
SP  - 81
EP  - 86
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984568
ER  - 

TY  - NA
AU  - Mazursky, Alex; Teng, Shan-Yuan; Nith, Romain; Lopes, Pedro
TI  - CHI - MagnetIO: Passive yet Interactive Soft Haptic Patches Anywhere
PY  - 2021
AB  - We propose a new type of haptic actuator, which we call MagnetIO, that is comprised of two parts: one battery-powered voice-coil worn on the user's fingernail and any number of interactive soft patches that can be attached onto any surface (everyday objects, user's body, appliances, etc.). When the user's finger wearing our voice-coil contacts any of the interactive patches it detects its magnetic signature via magnetometer and vibrates the patch, adding haptic feedback to otherwise input-only interactions. To allow these passive patches to vibrate, we make them from silicone with regions doped with polarized neodymium powder, resulting in soft and stretchable magnets. This stretchable form-factor allows them to be wrapped to the user's body or everyday objects of various shapes. We demonstrate how these add haptic output to many situations, such as adding haptic buttons to the walls of one's home. In our technical evaluation, we demonstrate that our interactive patches can be excited across a wide range of frequencies (0-500 Hz) and can be tuned to resonate at specific frequencies based on the patch's geometry. Furthermore, we demonstrate that MagnetIO's vibration intensity is as powerful as a typical linear resonant actuator (LRA); yet, unlike these rigid actuators, our passive patches operate as springs with multiple modes of vibration, which enables a wider band around its resonant frequency than an equivalent LRA.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445543
ER  - 

TY  - JOUR
AU  - Oh, Yosep; Zhou, Chi; Behdad, Sara
TI  - Part decomposition and assembly-based (Re) design for additive manufacturing: A review
PY  - 2018
AB  - Abstract Additive Manufacturing (AM), also known as 3D printing, has been highlighted as a complementary method to the traditional (subtractive and formative) manufacturing. This mainly results from its distinctive characteristics to directly produce complex shapes and assemblies without an assembly process. With these aspects, AM has affected the way products are designed and formed, which leads to an exclusive research area, known as Design for AM (DfAM). As a step towards addressing DfAM, this paper reviews the literature on re-designing an original model into assemblies produced in AM, named as Part Decomposition (PD). Although PD has received less attention in DfAM compared with Part Consolidation (PC) that is re-designing assemblies into a consolidated single part, PD has been studied with various motives and challenges for AM. To investigate the research trend in PD, 37 main publications are categorized under five motives including printability, productivity, functionality, artistry and flexibility. Additionally, from technical and methodological aspects, relevant studies are organized into decomposition issues (automatic, semi-automatic and manual decompositions), buildup issues (orientation decision for single- and multi-part and packing problem), and assembly issues (connection design and assembly process planning). As witnessed in this comprehensive review, the concept of PD leaves further research challenges spanning several disciplines. Along this line, we further elaborate future research directions of PD under three main categories: (1) enhancing the AM productivity for mass customization; (2) developing novel decomposition methods and guidelines; and (3) applying conventional design methodologies to PD.
SP  - 230
EP  - 242
JF  - Additive Manufacturing
VL  - 22
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2018.04.018
ER  - 

TY  - JOUR
AU  - Mita, Yoshio; Kawahara, Yoshihiro
TI  - 15-year educational experience on autonomous electronic information devices by flipped classroom and try-by-yourself methods
PY  - 2017
AB  - Since 2001, the departments of Electrical and Electronics and Information and Communication Electronics Engineering of Faculty of Engineering, the University of Tokyo (UTokyo) have jointly given a lecture on autonomous electronic information devices for undergraduate students. According to the on-line questionnaire, 80% of students in 2010–2012 replied that the lecture was useful for their future career. The task given to students was to design and realise an autonomous electronic information devices (so-called ‘the IoT-gadgets’) by themselves, and conduct a live demonstration in front of their colleagues. The device must have an ‘input’, ‘output’, and some ‘information processing’. Being aware of the speed of technology evolution as well as the short hours of the lecture, the professor tried not to give direct answers to students’ questions on how-to instantly build and program an information device. Instead, the students were told to beware of their ‘methods’. This refers to the deductive thinking introduced by Rene Descartes, and in the lecture's context how students should behave in order to realise the device. In this study, the backgrounds of various associated topics are discussed, such as the UTokyo's educational system, the world's rapid prototyping movement, open hardware, course design, students’ reactions, and future directions.
SP  - 321
EP  - 329
JF  - IET Circuits, Devices & Systems
VL  - 11
IS  - 4
PB  - 
DO  - 10.1049/iet-cds.2016.0406
ER  - 

TY  - JOUR
AU  - Hattab, Ammar; Gonsher, Ian; Moreno, Daniel; Taubin, Gabriel
TI  - Differential 3D Scanning
PY  - 2017
AB  - During the creative process, designers use various techniques and strategies to move from the abstract to the concrete, utilizing different physical and virtual means to represent form. The changes between virtual and physical models are not always fluent, however. Differential 3D scanning can detect the differences between a scanned model (point cloud) and a reference model (polygon mesh or CAD model) and then reflect those changes in the reference model. This can save designers time by reconstructing only the small changed regions rather than the entire object.
SP  - 43
EP  - 51
JF  - IEEE Computer Graphics and Applications
VL  - 38
IS  - 3
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Agarwal, Sonu; Ghosh, Sanjay
TI  - ICMLA - Evaluation of Microgesture Recognition Using a Smartwatch
PY  - 2017
AB  - Gesture based interaction and its recognition has been an area of active research with the growing popularity of wearables. We here propose an approach to detect fine-grained finger and palm motions using inertial sensors in a commercial smartwatch. A user specific SVM based classifier is developed for 7 microgestures with a classification accuracy of 94.4%. We extend this to a user adaptive model by including a few representative instances of a new user and achieve a classification accuracy of 91.7%. Further, we are able to differentiate between variations of a microgesture using three fundamental building blocks - distance, speed and orientation. A novel regression based approach is presented to predict the distance parameter. The idea is demonstrated on a swipe gesture with an error of 14%.
SP  - 986
EP  - 991
JF  - 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icmla.2017.00-24
ER  - 

TY  - NA
AU  - Liu, Yilin; Zhang, Shijia; Gowda, Mahanth
TI  - WWW - NeuroPose: 3D Hand Pose Tracking using EMG Wearables
PY  - 2021
AB  - Ubiquitous finger motion tracking enables a number of exciting applications in augmented reality, sports analytics, rehabilitation-healthcare, haptics etc. This paper presents NeuroPose, a system that shows the feasibility of 3D finger motion tracking using a platform of wearable ElectroMyoGraphy (EMG) sensors. EMG sensors can sense electrical potential from muscles due to finger activation, thus offering rich information for fine-grained finger motion sensing. However converting the sensor information to 3D finger poses is non trivial since signals from multiple fingers superimpose at the sensor in complex patterns. Towards solving this problem, NeuroPose fuses information from anatomical constraints of finger motion with machine learning architectures on Recurrent Neural Networks (RNN), Encoder-Decoder Networks, and ResNets to extract 3D finger motion from noisy EMG data. The generated motion pattern is temporally smooth as well as anatomically consistent. Furthermore, a transfer learning algorithm is leveraged to adapt a pretrained model on one user to a new user with minimal training overhead. A systematic study with 12 users demonstrates a median error of 6.24° and a 90%-ile error of 18.33° in tracking 3D finger joint angles. The accuracy is robust to natural variation in sensor mounting positions as well as changes in wrist positions of the user. NeuroPose is implemented on a smartphone with a processing latency of 0.101s, and a low energy overhead.
SP  - 1471
EP  - 1482
JF  - Proceedings of the Web Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3442381.3449890
ER  - 

TY  - NA
AU  - Lin, Hongnan; He, Liang; Song, Fangli; Li, Yifan; Cheng, Tingyu; Zheng, Clement; Wang, Wei; Oh, HyunJoo
TI  - FlexHaptics: A Design Method for Passive Haptic Inputs Using Planar Compliant Structures
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502113
ER  - 

TY  - JOUR
AU  - Deng, Xiaoke; Li, Zhaoyu; Wang, Xiangyu; Shi, Fan; Tang, Kai
TI  - A new visual-guided and partition-based multi-setup 3D printing system
PY  - NA
AB  - NA
SP  - 35
EP  - 56
JF  - Journal of Manufacturing Systems
VL  - 67
IS  - NA
PB  - 
DO  - 10.1016/j.jmsy.2022.12.013
ER  - 

TY  - NA
AU  - Liu, Min; Zhang, Yunbo; Bai, Jing; Cao, Yuanzhi; Alperovich, Jeffrey; Ramani, Karthik
TI  - CHI - WireFab: Mix-Dimensional Modeling and Fabrication for 3D Mesh Models
PY  - 2017
AB  - Many rapid fabrication technologies are directed towards layer wise printing or laser based prototyping. We propose WireFab, a rapid modeling and prototyping system that uses bent metal wires as the structure framework. WireFab approximates both the skeletal articulation and the skin appearance of the corresponding virtual skin meshes, and it allows users to personalize the designs by (1) specifying joint positions and part segmentations, (2) defining joint types and motion ranges to build a wire-based skeletal model, and (3) abstracting the segmented meshes into mixed-dimensional appearance patterns or attachments. The WireFab is designed to allow the user to choose how to best preserve the fidelity of the topological structure and articulation motion while selectively maintaining the fidelity of the geometric appearance. Compared to 3D-printing based high-fidelity fabrication systems, WireFab increases prototyping speed by ignoring unnecessary geometric details while preserving structural integrity and articulation motion. In addition, other rapid or low-fidelity fabrication systems produce only static models, while WireFab produces posable articulated models and has the potential to enable personalized functional products larger than the machines that produce them.
SP  - 965
EP  - 976
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025619
ER  - 

TY  - BOOK
AU  - Acharya, Sraddhanjali; Matovu, Richard; Serwadda, Abdul; Griswold-Steiner, Isaac
TI  - ICCDA - Gamification of Wearable Data Collection: A Tool for both Friend and Foe
PY  - 2019
AB  - Games are designed to build on certain inherently reward centered aspects of our psychology. This encourages the user to keep playing and engaging in the entertainment for just a bit longer. In this paper we explore the way in which gamification can be used to benefit researchers aiming to collect large amounts of data from sometimes less than enthusiastically motivated participants.We also investigate the risks associated with such mechanisms for data collection and how malicious entities could use these same methods to trick users into exposing private information. Across several experiments used to measure the cross-applicable nature of the data we collected, we demonstrate that a gamified version of a data collection tool could be used to predict the pattern used to unlock a phone.
SP  - 68
EP  - 77
JF  - Proceedings of the 2019 3rd International Conference on Compute and Data Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314545.3314572
ER  - 

TY  - JOUR
AU  - Li, Ling; Jiang, Shuo; Shull, Peter B.; Gu, Guoying
TI  - SkinGest: artificial skin for gesture recognition via filmy stretchable strain sensors*
PY  - 2018
AB  - Stretchable sensors are promising in the field of wearable robotics. To date, it is still a challenge to design an artificial skin with thin and sensitive stretchable sensors. In this paper, we pre...
SP  - 1112
EP  - 1121
JF  - Advanced Robotics
VL  - 32
IS  - 21
PB  - 
DO  - 10.1080/01691864.2018.1490666
ER  - 

TY  - JOUR
AU  - Hu, Chao; Qin, Qing-Hua
TI  - Advances in fused deposition modeling of discontinuous fiber/polymer composites
PY  - 2020
AB  - NA
SP  - 100867
EP  - NA
JF  - Current Opinion in Solid State and Materials Science
VL  - 24
IS  - 5
PB  - 
DO  - 10.1016/j.cossms.2020.100867
ER  - 

TY  - JOUR
AU  - Liu, Hongyi; Wang, Lihui
TI  - Gesture recognition for human-robot collaboration: A review
PY  - 2018
AB  - NA
SP  - 355
EP  - 367
JF  - International Journal of Industrial Ergonomics
VL  - 68
IS  - NA
PB  - 
DO  - 10.1016/j.ergon.2017.02.004
ER  - 

TY  - NA
AU  - Amorim, Vicente J. P.; Oliveira, Ricardo A. O.; da Silva, Maurício José
TI  - Recent Trends in Wearable Computing Research: A Systematic Review.
PY  - 2020
AB  - Wearable devices are a trending topic in both commercial and academic areas. Increasing demand for innovation has led to increased research and new products, addressing new challenges and creating profitable opportunities. However, despite a number of reviews and surveys on wearable computing, a study outlining how this area has recently evolved, which provides a broad and objective view of the main topics addressed by scientists, is lacking. The systematic review of literature presented in this paper investigates recent trends in wearable computing studies, taking into account a set of constraints applied to relevant studies over a window of ten years. The extracted articles were considered as a means to extract valuable information, creating a useful data set to represent the current status. Results of this study faithfully portray evolving interests in wearable devices. The analysis conducted here involving studies made over the past ten years allows evaluation of the areas, research focus, and technologies that are currently at the forefront of wearable device development. Conclusions presented in this review aim to assist scientists to better perceive recent demand trends and how wearable technology can further evolve. Finally, this study should assist in outlining the next steps in current and future development.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Moghanizadeh, Abbas; Ashrafizadeh, Fakhreddin
TI  - Studying sacrificial ice structure, as soluble support layers, in 3D printing of polymers (FDM)
PY  - 2021
AB  - NA
SP  - 757
EP  - 763
JF  - Progress in Additive Manufacturing
VL  - 6
IS  - 4
PB  - 
DO  - 10.1007/s40964-021-00195-1
ER  - 

TY  - JOUR
AU  - Zhao, Donghua; Guo, Weizhong
TI  - Shape and Performance Controlled Advanced Design for Additive Manufacturing: A Review of Slicing and Path Planning
PY  - 2019
AB  - <jats:title>Abstract</jats:title><jats:p>Additive manufacturing (AM) brings out a revolution of how the products are designed and manufactured. To obtain desired components, advanced design for additive manufacturing (ADfAM) is widely emphasized in geometry, material, and function design. 3D slicing and path planning, which are the critical steps of ADfAM, directly determine manufacturing process variables, shape, and performance of printed parts. For widely used planar slicing, the contradiction between accuracy and build time has attracted considerable attention and efforts, leading to various novel and optimization methods. Nevertheless, curved surfaces and slopes along the build direction constrain the surfaces to be smooth due to the inherent staircase effect of AM. Meanwhile, there is significant anisotropy of the printed piece making it sensitive to any shear (or bending) stress. Moreover, support structures for the overhang part are necessary when building along one direction, resulting in time-consuming and cost-expensive process. Due to the rapid development of 3D slicing and path planning, and various newly proposed methods, there is a lack of comprehensive knowledge. Notwithstanding, there are fewer literature reviews concerning planar slicing and filling strategy. Less attention has been paid to non-planar slicing, path planning on curved surfaces, and multi-degree of freedom (DOF) AM equipment, as well as printing under pressure. Hence, it is significant to get a comprehensive understanding of current status and challenges. Then, with suitable technologies, the printed parts with improved surface quality, minimum support structures, and better isotropy could be acquired. Finally, the recommendation for the future development of slicing and path planning is also provided.</jats:p>
SP  - NA
EP  - NA
JF  - Journal of Manufacturing Science and Engineering
VL  - 142
IS  - 1
PB  - 
DO  - 10.1115/1.4045055
ER  - 

TY  - NA
AU  - Bhatt, Prahar M.; Malhan, Rishi K.; Gupta, Satyandra K.
TI  - Computational Foundations for Using Three Degrees of Freedom Build Platforms to Enable Supportless Extrusion-Based Additive Manufacturing
PY  - 2019
AB  - <jats:title>Abstract</jats:title> <jats:p>Extrusion-based additive manufacturing systems usually use three degrees of freedom extrusion tools to perform the deposition operation. This requires the use of support structures to deposit structures with overhang features. The use of support structures can be avoided by adding degrees of freedom to the build platform. The elimination of build structures can offer benefits in terms of reduction of build time and elimination of postprocessing costs. This paper demonstrates that the use of three degrees of freedom build platform enables printing of complex shapes without support structures. We present computational foundations for generating paths and trajectories for synchronizing the motion of three degrees of freedom build platforms and three degrees of freedom extrusion tools. We report results on six different test parts in terms of reduction in build time, accuracy, and surface roughness.</jats:p>
SP  - NA
EP  - NA
JF  - Volume 1: Additive Manufacturing; Manufacturing Equipment and Systems; Bio and Sustainable Manufacturing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1115/msec2019-3024
ER  - 

TY  - JOUR
AU  - Shi, Yuanyuan; Li, Yunan; Fu, Xiaolong; Kaibin, M.I.A.O.; Qiguang, M.I.A.O.
TI  - Review of dynamic gesture recognition
PY  - 2021
AB  - Abstract In recent years, gesture recognition has been widely used in the fields of intelligent driving, virtual reality, and human-computer interaction. With the development of artificial intelligence, deep learning has achieved remarkable success in computer vision. To help researchers better understanding the development status of gesture recognition in video, this article provides a detailed survey of the latest developments in gesture recognition technology for videos based on deep learning. The reviewed methods are broadly categorized into three groups based on the type of neural networks used for recognition: twostream convolutional neural networks, 3D convolutional neural networks, and Long-short Term Memory (LSTM) networks. In this review, we discuss the advantages and limitations of existing technologies, focusing on the feature extraction method of the spatiotemporal structure information in a video sequence, and consider future research directions.
SP  - 183
EP  - 206
JF  - Virtual Reality & Intelligent Hardware
VL  - 3
IS  - 3
PB  - 
DO  - 10.1016/j.vrih.2021.05.001
ER  - 

TY  - JOUR
AU  - He, Liang; Wittkopf, Jarrid A.; Jun, Ji Won; Erickson, Kris; Ballagas, Rafael Tico
TI  - ModElec
PY  - 2021
AB  - <jats:p>Integrating electronics with highly custom 3D designs for the physical fabrication of interactive prototypes is traditionally cumbersome and requires numerous iterations of manual assembly and debugging. With the new capabilities of 3D printers, combining electronic design and 3D modeling workflows can lower the barrier for achieving interactive functionality or iterating on the overall design. We present ModElec---an interactive design tool that enables the coordinated expression of electronic and physical design intent by allowing designers to integrate 3D-printable circuits with 3D forms. With ModElec, the user can arrange electronic parts in a 3D body, modify the model design with embedded circuits updated, and preview the auto-generated 3D traces that can be directly printed with a multi-material-based 3D printer. We demonstrate the potential of ModElec with four example applications, from a set of game controls to reconfigurable devices. Further, the tool was reported as easy to use through a preliminary evaluation with eight designers.</jats:p>
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3495000
ER  - 

TY  - NA
AU  - Ramakers, Raf; Anderson, Fraser; Grossman, Tovi; Fitzmaurice, George
TI  - CHI - RetroFab: A Design Tool for Retrofitting Physical Interfaces using Actuators, Sensors and 3D Printing
PY  - 2016
AB  - We present RetroFab, an end-to-end design and fabrication environment that allows non-experts to retrofit physical interfaces. Our approach allows for changing the layout and behavior of physical interfaces. Unlike customizing software interfaces, physical interfaces are often challenging to adapt because of their rigidity. With RetroFab, a new physical interface is designed that serves as a proxy interface for the legacy controls that are now operated by actuators. RetroFab makes this concept of retrofitting devices available to non-experts by automatically generating an enclosure structure from an annotated 3D scan. This enclosure structure holds together actuators, sensors as well as components for the redesigned interface. To allow retrofitting a wide variety of legacy devices, the RetroFab design tool comes with a toolkit of 12 components. We demonstrate the versatility and novel opportunities of our approach by retrofitting five domestic objects and exploring their use cases. Preliminary user feedback reports on the experience of retrofitting devices with RetroFab.
SP  - 409
EP  - 419
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858485
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Wagner, Tobias; Gugenheimer, Jan; Rukzio, Enrico
TI  - CHI - Mix&Match: Towards Omitting Modelling Through In-situ Remixing of Model Repository Artifacts in Mixed Reality
PY  - 2020
AB  - The accessibility of tools to model artifacts is one of the core driving factors for the adoption of Personal Fabrication. Subsequently, model repositories like Thingiverse became important tools in (novice) makers' processes. They allow them to shorten or even omit the design process, offloading a majority of the effort to other parties. However, steps like measurement of surrounding constraints (e.g., clearance) which exist only inside the users' environment, can not be similarly outsourced. We propose Mix&Match a mixed-reality-based system which allows users to browse model repositories, preview the models in-situ, and adapt them to their environment in a simple and immediate fashion. Mix&Match aims to provide users with CSG operations which can be based on both virtual and real geometry. We present interaction patterns and scenarios for Mix&Match, arguing for the combination of mixed reality and model repositories. This enables almost modelling-free personal fabrication for both novices and expert makers.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376839
ER  - 

TY  - JOUR
AU  - Corchia, Laura; Monti, Giuseppina; Raheli, Federica; Candelieri, Giulia; Tarricone, Luciano
TI  - Dry Textile Electrodes for Wearable Bio-Impedance Analyzers
PY  - 2020
AB  - In this paper the results of the study performed on dry textile electrodes for bio-impedance measurements are reported. The use of two different conductive fabrics is investigated: a silver-plated bi-elastic knitted fabric and a bi-layer structure comprising a layer of a silver-plated knitted fabric and a layer of conductive silicone. In this latter, the presence of the silicone layer guarantees a good contact with the skin, which is a fundamental condition for accurate bio-impedance measurements, and preserves the silver by oxidation due to sweat. The fabricated textile electrodes have been sewn on elastic cuffs and two different relative positions of the current and voltage electrodes have been investigated. The proposed cuffs are suitable to be embedded into a t-shirt for continuous and real-time bio-impedance measurements. The reliability of the developed prototypes is validated by comparison with data obtained from Ag/AgCl commercial electrodes. Promising results have been obtained for both the investigated conductive fabrics.
SP  - 6139
EP  - 6147
JF  - IEEE Sensors Journal
VL  - 20
IS  - 11
PB  - 
DO  - 10.1109/jsen.2020.2972603
ER  - 

TY  - JOUR
AU  - Wenqiang, Chen; Lin, Chen; Ma, Meiyi; Parizi, Farshid Salemi; Patel, Shwetak N.; Stankovic, John A.
TI  - ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch
PY  - 2021
AB  - Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 1
PB  - 
DO  - 10.1145/3448119
ER  - 

TY  - NA
AU  - Salkim, Enver; Wu, Yu
TI  - Anatomical 3D Modeling of Upper Limb for Bio-impedance based Hand Motion Interpretation
PY  - 2021
AB  - Bio-impedance analysis (BIA) is a non-invasive way of assessing body compositions and has been recently used for hand motion interpretation using ‘brute force’ pattern recognition. To better promote BIA applications in human-machine interface, this paper develops an anatomically accurate 3D model towards a sound BIA recording strategy. The model is developed based on transient finite element analysis. It can be used for precise location of transcutaneous electrical stimulation to provide 3D current and potential distributions within the skin, fat, muscle, and bone layers of the upper arm, each defined by their dielectric properties. With the model, it is possible to investigate the impact of the electrode placement on the muscle when using, e.g., textile and flexible electrodes. As proof of concept for guiding the electrode placement, the electrical potential was simulated for two different electrode stimulation arrangements. The results showed that when the electrodes were shifted towards the upper arm, the electrical potential was reduced. This may be related to the anatomical layers' electric features and the distance of the electrode to the targeted muscle.
SP  - NA
EP  - NA
JF  - 2021 IEEE International Conference on Flexible and Printable Sensors and Systems (FLEPS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/fleps51544.2021.9469842
ER  - 

TY  - JOUR
AU  - Paredes, Luis; Reddy, Sai Swarup; Chidambaram, Subramanian; Vagholkar, Devashri; Zhang, Yunbo; Benes, Bedrich; Ramani, Karthik
TI  - FabHandWear: An End-to-End Pipeline from Design to Fabrication of Customized Functional Hand Wearables
PY  - 2021
AB  - Current hand wearables have limited customizability, they are loose-fit to an individual's hand and lack comfort. The main barrier in customizing hand wearables is the geometric complexity and size variation in hands. Moreover, there are different functions that the users can be looking for; some may only want to detect hand's motion or orientation; others may be interested in tracking their vital signs. Current wearables usually fit multiple functions and are designed for a universal user with none or limited customization. There are no specialized tools that facilitate the creation of customized hand wearables for varying hand sizes and provide different functionalities. We envision an emerging generation of customizable hand wearables that supports hand differences and promotes hand exploration with additional functionality. We introduce FabHandWear, a novel system that allows end-to-end design and fabrication of customized functional self-contained hand wearables. FabHandWear is designed to work with off-the-shelf electronics, with the ability to connect them automatically and generate a printable pattern for fabrication. We validate our system by using illustrative applications, a durability test, and an empirical user evaluation. Overall, FabHandWear offers the freedom to create customized, functional, and manufacturable hand wearables.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463518
ER  - 

TY  - NA
AU  - Dimas, Christos; Sotiriadis, Paul P.
TI  - Conductivity distribution measurement at different low frequencies using a modular 64 electrode electrical impedance tomography system
PY  - 2017
AB  - Electrical Impedance Tomography (EIT) is a technique that is widely spread in applications in biomedical engineering. The purpose of EIT is to use a sinusoidal current rather than the traditional methods of tomography where radiation or ultrasound waves are used. In this work, a system architecture using 64-electrodes applied to a circular liquid setup is implemented as well as a test circuitry to verify the system operation. Moreover, measurements are taken at a low-frequency range and their results are presented and compared.
SP  - NA
EP  - NA
JF  - 2017 Panhellenic Conference on Electronics and Telecommunications (PACET)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/pacet.2017.8259973
ER  - 

TY  - NA
AU  - Ahuja, Karan; Harrison, Chris; Goel, Mayank; Xiao, Robert
TI  - UIST - MeCap: Whole-Body Digitization for Low-Cost VR/AR Headsets
PY  - 2019
AB  - Low-cost, smartphone-powered VR/AR headsets are becoming more popular. These basic devices - little more than plastic or cardboard shells - lack advanced features, such as controllers for the hands, limiting their interactive capability. Moreover, even high-end consumer headsets lack the ability to track the body and face. For this reason, interactive experiences like social VR are underdeveloped. We introduce MeCap, which enables commodity VR headsets to be augmented with powerful motion capture ("MoCap") and user-sensing capabilities at very low cost (under $5). Using only a pair of hemi-spherical mirrors and the existing rear-facing camera of a smartphone, MeCap provides real-time estimates of a wearer's 3D body pose, hand pose, facial expression, physical appearance and surrounding environment - capabilities which are either absent in contemporary VR/AR systems or which require specialized hardware and controllers. We evaluate the accuracy of each of our tracking features, the results of which show imminent feasibility.
SP  - 453
EP  - 462
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347889
ER  - 

TY  - NA
AU  - Kim, Hyunyoung; Coutrix, Céline; Roudaut, Anne
TI  - IHM - KnobSlider: design of a shape-changing device grounded in users' needs
PY  - 2016
AB  - In this paper, we explore how to combine the advantages of physical knobs and sliders by using shape-change in order to transform one into another on-demand. By doing so we merge the benefits of both input devices: knobs enable relative angular input with dynamic gain and require little space; sliders require more space but enable absolute linear input and can be grouped to be simultaneously adjusted and monitored. Our initial contextual interviews unveil situations where such dynamic device could be particularly beneficial for professionals such as sound and light engineers as they require tangible devices and high flexibility. We then present the 9 alternative designs of KnobSlider, a shape changing input device that can switch from a knob to slider and vice-versa. We propose a set of 11 requirements for such a shape-changing device and conducted a systematic analysis of our 9 designs against our 11 requirements. This work is a first step toward building a high-fidelity KnobSlider.
SP  - 91
EP  - 102
JF  - Actes de la 28ième conférence francophone sur l'Interaction Homme-Machine on - IHM '16
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3004107.3004125
ER  - 

TY  - NA
AU  - O'Leary, Jasper Tran; Nandi, Chandrakana; Lee, Khang; Peek, Nadya
TI  - UIST - Taxon: a Language for Formal Reasoning with Digital Fabrication Machines
PY  - 2021
AB  - Digital fabrication machines for makers have expanded access to manufacturing processes such as 3D printing, laser cutting, and milling. While digital models encode the data necessary for a machine to manufacture an object, understanding the trade-offs and limitations of the machines themselves is crucial for successful production. Yet, this knowledge is not codified and must be gained through experience, which limits both adoption of and creative exploration with digital fabrication tools. To formally represent machines, we present Taxon, a language that encodes a machine’s high-level characteristics, physical composition, and performable actions. With this programmatic foundation, makers can develop rules of thumb that filter for appropriate machines for a given job and verify that actions are feasible and safe. We integrate the language with a browser-based system for simulating and experimenting with machine workflows. The system lets makers engage with rules of thumb and enrich their understanding of machines. We evaluate Taxon by representing several machines from both common practice and digital fabrication research. We find that while Taxon does not exhaustively describe all machines, it provides a starting point for makers and HCI researchers to develop tools for reasoning about and making decisions with machines.
SP  - 691
EP  - 709
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474779
ER  - 

TY  - NA
AU  - Weiler, Jennifer; Kuznetsov, Stacey
TI  - CHI Extended Abstracts - Crafting Colorful Objects: a DIY Method for Adding Surface Detail to 3D Prints
PY  - 2017
AB  - Low-cost 3D printing methods have revolutionized DIY fabrication, and it is only a matter of time before high-fidelity color 3D printing technologies reach the average consumer market. In parallel to this trajectory, our paper describes an ultra low-cost, DIY method for augmenting 3D objects with surface detail. We developed a software tool that maps 2D images onto 3D surfaces and an easy workflow for layering the resulting, warped images onto 3D printed artifacts. We assessed our approach in a workshop where local makers created 3D-printed nested dolls overlaid with personally designed images. Our workshop demonstrates the feasibility of our ultra low-cost method for adding rich surface detail to 3D objects, and our ongoing work will streamline this process and support more complex shapes.
SP  - 2217
EP  - 2223
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053144
ER  - 

TY  - NA
AU  - Heller, Florian; Thar, Jan; Lewandowski, Dennis; Hartmann, Mirko; Schoonbrood, Pierre; Stönner, Sophy; Voelker, Simon; Borchers, Jan
TI  - Conference on Designing Interactive Systems - CutCAD - An Open-source Tool to Design 3D Objects in 2D
PY  - 2018
AB  - Laser cutters are 2D tools, but their speed and compatibility with a variety of affordable materials also makes them a frequent choice to create 3D objects. We propose CutCAD, a tool to easily construct simple 3D objects from 2D faces, inspired by the process of paper modeling and magnetic construction kits. The user creates her 3D model by drawing or loading existing 2D shapes, and connecting their edges in the software. CutCAD then automatically resolves the resulting constraints, and folds the faces up into a 3D model that is previewed live. CutCAD also automatically creates the required finger joints based on thickness of the material and dihedral angles, for smooth assembly. Cutouts are easy to add by importing their outlines as vector drawings, and placing them onto faces. After the faces have been cut, CutCAD provides assembly instructions. Observations and feedback from using CutCAD show the resulting process to be easier to understand than traditional 3D modelling. CutCAD is open-source, and has been downloaded over 2,000 times.
SP  - 1135
EP  - 1139
JF  - Proceedings of the 2018 Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3196709.3196800
ER  - 

TY  - JOUR
AU  - Jiang, Jingchao; Xu, Xun; Stringer, Jonathan
TI  - Support Structures for Additive Manufacturing: A Review
PY  - 2018
AB  - Additive manufacturing (AM) has developed rapidly since its inception in the 1980s. AM is perceived as an environmentally friendly and sustainable technology and has already gained a lot of attention globally. The potential freedom of design offered by AM is, however, often limited when printing complex geometries due to an inability to support the stresses inherent within the manufacturing process. Additional support structures are often needed, which leads to material, time and energy waste. Research in support structures is, therefore, of great importance for the future and further improvement of additive manufacturing. This paper aims to review the varied research that has been performed in the area of support structures. Fifty-seven publications regarding support structure optimization are selected and categorized into six groups for discussion. A framework is established in which future research into support structures can be pursued and standardized. By providing a comprehensive review and discussion on support structures, AM can be further improved and developed in terms of support waste in the future, thus, making AM a more sustainable technology.
SP  - 64
EP  - NA
JF  - Journal of Manufacturing and Materials Processing
VL  - 2
IS  - 4
PB  - 
DO  - 10.3390/jmmp2040064
ER  - 

TY  - JOUR
AU  - Chakraborty, Piyali; Eqbal, Md. Danish; Ahmed, Jasim
TI  - Three‐dimensional printing and its application to legume proteins: A review
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Legume Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1002/leg3.172
ER  - 

TY  - JOUR
AU  - Nakashima, Kazutaka; Auzinger, Thomas; Iarussi, Emmanuel; Zhang, Ran; Igarashi, Takeo; Bickel, Bernd
TI  - CoreCavity: interactive shell decomposition for fabrication with two-piece rigid molds
PY  - 2018
AB  - Molding is a popular mass production method, in which the initial expenses for the mold are offset by the low per-unit production cost. However, the physical fabrication constraints of the molding technique commonly restrict the shape of moldable objects. For a complex shape, a decomposition of the object into moldable parts is a common strategy to address these constraints, with plastic model kits being a popular and illustrative example. However, conducting such a decomposition requires considerable expertise, and it depends on the technical aspects of the fabrication technique, as well as aesthetic considerations. We present an interactive technique to create such decompositions for two-piece molding, in which each part of the object is cast between two rigid mold pieces. Given the surface description of an object, we decompose its thin-shell equivalent into moldable parts by first performing a coarse decomposition and then utilizing an active contour model for the boundaries between individual parts. Formulated as an optimization problem, the movement of the contours is guided by an energy reflecting fabrication constraints to ensure the moldability of each part. Simultaneously the user is provided with editing capabilities to enforce aesthetic guidelines. Our interactive interface provides control of the contour positions by allowing, for example, the alignment of part boundaries with object features. Our technique enables a novel workflow, as it empowers novice users to explore the design space, and it generates fabrication-ready two-piece molds that can be used either for casting or industrial injection molding of free-form objects.
SP  - 135
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201341
ER  - 

TY  - JOUR
AU  - Wang, Zhujiang; Wang, Zimo; Ko, Woo-Hyun; Iquebal, Ashif Sikandar; Nguyen, Vu Huy; Kazerooni, Nazanin Afsar; Ma, Qiyang; Srinivasa, Arun R.; Kumar, P. R.; Bukkapatnam, Satish T. S.
TI  - An autonomous laser kirigami method with low-cost real-time vision-based surface deformation feedback system
PY  - 2021
AB  - We introduce an autonomous laser kirigami technique, a novel custom manufacturing machine system which functions somewhat similar to a photocopier. This technique is capable of creating functional freeform shell structures using cutting and folding (kirigami) operations on sheet precursors. Conventional laser kirigami techniques are operated manually and rely heavily on precise calibrations. However, it is unrealistic to design and plan out the process (open loop) to realize arbitrary geometric features from a wide variety of materials. In our work, we develop and demonstrate a completely autonomous system, which is composed of a laser system, a 4-axis robotic arm, a real-time vision-based surface deformation monitoring system, and an associated control system. The laser system is based on the Lasersaur, which is a 120-Watt CO2 open-source laser cutter. The robotic arm is employed to precisely adjust the distance between a workpiece and the laser lens so that a focused and defocused laser beam can be used to cut and fold the workpiece respectively. The four-axis robotic arm provides flexibility for expanding the limits of possible shapes, compared to conventional laser machine setups where the workpiece is fixed on rigid holders. The real-time vision-based surface deformation monitoring system is composed of four low-cost cameras, an integrated AI-assisted algorithm, and the sensors (detachable planar markers) mounted on the polymer-based sheet precursors, and allows real-time monitoring of the sheet forming process with a geometric feature estimation error less than 5% and delay time around 100 ms. The developed control system manages the laser power, the laser scanning speed, the motion of the robotic arm based on the designed plan, and the close-loop feedback provided by the vision-based surface deformation monitoring system. This cyber-physical kirigami platform can operate a sequence of cutting and folding processes in order to create kirigami objects. Hence, complicated kirigami design products with various different polygonal structures can be realized by undergoing sequential designed laser cuts and bends (at any folding angles within designed geometric tolerance) using this autonomous kirigami platform.
SP  - 1
EP  - 11
JF  - The International Journal of Advanced Manufacturing Technology
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ferrone, A.; Maita, Francesco; Maiolo, Luca; Arquilla, M.; Castiello, Andrea; Pecora, Alessandro; Jiang, Xianta; Menon, Carlo; Colace, Lorenzo
TI  - BioRob - Wearable band for hand gesture recognition based on strain sensors
PY  - 2016
AB  - A novel fully wearable system based on a smart wristband equipped with stretchable strain gauge sensors and readout electronics have been assembled and tested to detect a set of movements of a hand crucial in rehabilitation procedures. The high sensitivity of the active devices embedded on the wristband do not need a direct contact with the skin, thus maximizing the comfort on the arm of the tester. The gestures done with the device have been auto-labeled by comparing the signals detected in real-time by the sensors with a commercial infrared device (Leap motion). Finally, the system has been evaluated with two machine-learning algorithms Linear Discriminant Analysis (LDA) and Support Vector Machine (SVM), reaching a reproducibility of 98% and 94%, respectively.
SP  - 1319
EP  - 1322
JF  - 2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)
VL  - 2016
IS  - NA
PB  - 
DO  - 10.1109/biorob.2016.7523814
ER  - 

TY  - NA
AU  - Ahsan, Amm Nazmul
TI  - Form and Functionality of Additively Manufactured Parts with Internal Structure
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kim, Hyunyoung; Coutrix, Céline; Roudaut, Anne
TI  - CHI - Morphees+: Studying Everyday Reconfigurable Objects for the Design and Taxonomy of Reconfigurable UIs
PY  - 2018
AB  - Users interact with many reconfigurable objects in daily life. These objects embed reconfigurations and shape- changing features that users are familiar with. For this reason, everyday reconfigurable objects have informed the design and taxonomy of shape changing UI. However, they have never been explored systematically. In this paper, we present a data set of 82 everyday reconfigurable objects that we collected in a workshop. We discuss how they can inspire the design of reconfigurable interfaces. We particularly focus on taxonomies of reconfigurable interfaces. Taxonomies have been suggested to help design and communication among researchers, however despite their extensive use, taxonomies are rarely evaluated. This paper analyses two established taxonomies - Rasmussen's and Roudaut's - using daily reconfigurable objects. We show relationships between the taxonomies and area for improvements. We propose Morphees+, a refined taxonomy based on Roudaut's Shape Resolution Taxonomy.
SP  - 619
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174193
ER  - 

TY  - NA
AU  - Schmitz, Martin; Herbers, Martin; Dezfuli, Niloofar; Günther, Sebastian; Mühlhäuser, Max
TI  - CHI - Off-Line Sensing: Memorizing Interactions in Passive 3D-Printed Objects
PY  - 2018
AB  - Embedding sensors into objects allow them to recognize various interactions. However, sensing usually requires active electronics that are often costly, need time to be assembled, and constantly draw power. Thus, we propose off-line sensing: passive 3D-printed sensors that detect one-time interactions, such as accelerating or flipping, but neither require active electronics nor power at the time of the interaction. They memorize a pre-defined interaction via an embedded structure filled with a conductive medium (e.g., a liquid). Whether a sensor was exposed to the interaction can be read-out via a capacitive touchscreen. Sensors are printed in a single pass on a consumer-level 3D printer. Through a series of experiments, we show the feasibility of off-line sensing.
SP  - 182
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173756
ER  - 

TY  - NA
AU  - Bidoky, Fazel Zare
TI  - Enhancement of the Dynamic Performance of Electrolyte-Gated Transistors: Toward Fast-Switching, Low-Operating Voltage Printed Electronics
PY  - 2019
AB  - University of Minnesota Ph.D. dissertation. June 2019. Major: Chemistry. Advisor: Daniel Frisbie. 1 computer file (PDF); ix, 148 pages.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yan, Zeyu; Peng, Huaishu
TI  - UIST - FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer
PY  - 2021
AB  - We introduce FabHydro, a set of rapid and low-cost methods to prototype interactive hydraulic devices based on an off-the-shelf 3D printer and flexible photosensitive resin. We first present printer settings and custom support structures to warrant the successful print of flexible and deformable objects. We then demonstrate two printing methods to seal the transmission fluid inside these deformable structures: the Submerged Printing process that seals the liquid resin without manual assembly, and the Printing with Plugs method that allows the use of different transmission fluids without modification to the printer. Following the printing methods, we report a design space with a range of 3D printable primitives, including the hydraulic generator, transmitter, and actuator. To demonstrate the feasibility of our approaches and the breadth of new designs that they enable, we showcase a set of examples from a printed robotic gripper that can be operated at a distance to a mobile phone stand that serves as a status reminder by repositioning the user’s phone. We conclude with a discussion of our approach’s limitations and possible future improvements.
SP  - 298
EP  - 311
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474751
ER  - 

TY  - JOUR
AU  - Alammar, Amirah; Kois, John C; Revilla-León, Marta; Att, Wael
TI  - Additive Manufacturing Technologies: Current Status and Future Perspectives.
PY  - 2022
AB  - A review of the main additive manufacturing technologies including vat-polymerization, material extrusion, material jetting, binder jetting, powder-based fusion, sheet lamination, and direct energy deposition is provided. Additionally, the dental applications of polymer, metal, and ceramic printing technologies are discussed.
SP  - 4
EP  - 12
JF  - Journal of prosthodontics : official journal of the American College of Prosthodontists
VL  - 31
IS  - S1
PB  - 
DO  - 10.1111/jopr.13477
ER  - 

TY  - NA
AU  - Maereg, Andualem Tadesse; Lou, Yang; Secco, Emanuele Lindo; King, Raymond
TI  - VISIGRAPP (2: HUCAPP) - Hand Gesture Recognition based on Near-infrared Sensing Wristband.
PY  - 2020
AB  - Wrist-worn gesture sensing systems can be used as a seamless interface for AR/VR interactions and control of various devices. In this paper, we present a low-cost gesture sensing system that utilizes near Infrared Emitters (600 - 1100 nm) and Photo-Receivers encompassing the wrist to infer hand gestures. The proposed system consists of a wristband comprising Infrared emitters and receivers, data acquisition hardware, data post-processing software, and gesture classification algorithms. During the data acquisition process, 24 near Infrared Emitters are sequentially switched on around the wrist, and twelve Photo-diodes measure the light reflected, refracted, and scattered by the tissues inside the wrist. The acquired data corresponding to different gestures are labeled and input into a machine learning algorithm for gesture classification. To demonstrated the accuracy and speed of the proposed system, real-time gesture sensing user studies were conducted. As a result of this comparison, we obtained an average accuracy of 98.06% with standard deviation of 1.82%. In addition, we evaluated that the system can perform six-eight gestures per second in real time using a desktop computer operating with Core i7-7800X CPU at 3.5GHz and 32 GB RAM.
SP  - 110
EP  - 117
JF  - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.5220/0008909401100117
ER  - 

TY  - JOUR
AU  - Moghanizadeh, Abbas
TI  - Development of 3D printing scaffolds by sacrificial ice support layers
PY  - 2022
AB  - NA
SP  - 116
EP  - 118
JF  - Manufacturing Letters
VL  - 31
IS  - NA
PB  - 
DO  - 10.1016/j.mfglet.2021.09.002
ER  - 

TY  - NA
AU  - Wang, Guanyun; Tao, Ye; Capunaman, Ozguc Bertug; Yang, Humphrey; Yao, Lining
TI  - CHI - A-line: 4D Printing Morphing Linear Composite Structures
PY  - 2019
AB  - This paper presents A-line, a 4D printing system for designing and fabricating morphing three-dimensional shapes out of simple linear elements. In addition to the commonly known benefit of 4D printing to save printing time, printing materials, and packaging space, A-line also takes advantage of the unique properties of thin lines, including their suitability for compliant mechanisms and ability to travel through narrow spaces and self-deploy or self-lock on site. A-line integrates a method of bending angle control in up to eight directions for one printed line segment, using a single type of thermoplastic material. A software platform to support the design, simulation and tool path generation is developed to support the design and manufacturing of various A-line structures. Finally, the design space of A-line is explored through four application areas, including line sculpting, compliant mechanisms, self-deploying, and self-locking structures.
SP  - 426
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300656
ER  - 

TY  - JOUR
AU  - Zhu, Lin; Feng, Ruiliang; Li, Xianda; Xi, Juntong; Wei, Xiangzhi
TI  - A Tree-Shaped Support Structure for Additive Manufacturing Generated by Using a Hybrid of Particle Swarm Optimization and Greedy Algorithm
PY  - 2019
AB  - <jats:title>Abstract</jats:title> <jats:p>Reducing the volume of support structures is a critical means for saving materials and budgets of additive manufacturing, and tree structure is an effective topology for this purpose. Although a few articles in literature and commercial software have been devoted to developing tree-supports, those tree-supports are generated based on geometry optimization or user-defined parameters, which cannot guarantee a minimum volume with robust fabrication guarantee. To address this issue, we propose a set of formulas for stably growing the tree-supports with physical constraints based on 3D printing experiments using fused decomposition modelling (FDM) machines, and a volume minimization mechanism using a hybrid of particle swarm optimization (PSO) method and a greedy algorithm. We show that this combination is effective in reducing the volume of tree-supports and the simulations reveal that the volume curves monotonically descent to a constant within a short time, and our experimental results show that the models with the tree-supports can be manufactured stably.</jats:p>
SP  - 1
EP  - 19
JF  - Journal of Computing and Information Science in Engineering
VL  - 19
IS  - 4
PB  - 
DO  - 10.1115/1.4043530
ER  - 

TY  - JOUR
AU  - Dew, Kristin; Landwehr-Sydow, Sophie-Bettina; Rosner, Daniela K.; Thayer, Alexander; Jonsson, Martin
TI  - Producing Printability: Articulation Work and Alignment in 3D Printing
PY  - 2019
AB  - Three-dimensional printing is widely celebrated as enabling open design and manufacturing practice. With easy-to-use techniques such as automated modeling, fabrication machines ostensibly help desi...
SP  - 433
EP  - 469
JF  - Human–Computer Interaction
VL  - 34
IS  - 5-6
PB  - 
DO  - 10.1080/07370024.2019.1566001
ER  - 

TY  - JOUR
AU  - Cai, Ruifan; Lin, Yingying; Li, Honglin; Zhu, Yuzhen; Tang, Xiangjun; Weng, Yanjun; You, Lihua; Jin, Xiaogang
TI  - Wowtao: A personalized pottery-making system
PY  - 2021
AB  - NA
SP  - 103325
EP  - NA
JF  - Computers in Industry
VL  - 124
IS  - NA
PB  - 
DO  - 10.1016/j.compind.2020.103325
ER  - 

TY  - JOUR
AU  - Wang, Zhujiang; Wang, Zimo; Ko, Woo-Hyun; Iquebal, Ashif Sikandar; Nguyen, Vu; Kazerooni, Nazanin Afsar; Ma, Qiyang; Srinivasa, Arun; Kumar, Panganamala Ramana; Bukkapatnam, Satish
TI  - An autonomous laser kirigami method with low-cost real-time vision-based surface deformation feedback system
PY  - 2021
AB  - NA
SP  - 1873
EP  - 1883
JF  - The International Journal of Advanced Manufacturing Technology
VL  - 118
IS  - 5-6
PB  - 
DO  - 10.1007/s00170-021-07661-8
ER  - 

TY  - NA
AU  - Kato, Kunihiro; Saito, Kazuya; Kawahara, Yoshihiro
TI  - CHI Extended Abstracts - OrigamiSpeaker: Handcrafted Paper Speaker with Silver Nano-Particle Ink
PY  - 2019
AB  - In this study, we present an OrigamiSpeaker which can be handcrafted with silver nano-particle ink on a paper substrate. The OrigamiSpeaker is based on the "electrostatic loudspeaker" technique. The audio signal is amplified to a high voltage and applied to an electrode that vibrates to generate sound. By using Origami techniques, users are able to design various shapes of OrigamiSpeaker.
SP  - 3312872
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312872
ER  - 

TY  - NA
AU  - Ku, Pin-Sung; Shao, Qijia; Wu, Te-Yen; Gong, Jun; Zhu, Ziyan; Zhou, Xia; Yang, Xing-Dong
TI  - CHI - ThreadSense: Locating Touch on an Extremely Thin Interactive Thread
PY  - 2020
AB  - We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods. Our approach is also unique in that it locates a touch based on a mathematical model describing the change in thread impedance in relation to the touch locations. This allows the system to be easily calibrated by the user touching a known location(s) on the thread. The system can thus quickly adapt to various environmental settings and users. A system evaluation showed that our system could track the slide motion of a finger with an average error distance of 6.13 mm and 4.16 mm using one and five touches for calibration, respectively. The system could also distinguish between single touch and two concurrent touches with an accuracy of 99% and could track two concurrent touches with an average error distance of 8.55 mm. We demonstrate new interactions enabled by our sensing approach in several unique applications.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376779
ER  - 

TY  - NA
AU  - Ashbrook, Daniel; Guo, Shitao Stan; Lambie, Alan J.
TI  - CHI Extended Abstracts - Towards Augmented Fabrication: Combining Fabricated and Existing Objects
PY  - 2016
AB  - One of the main uses for digital fabrication systems is fabrication for use with existing objects. We call this paradigm "augmented fabrication." In this paper, we discuss the types of augmented fabrication activities that can take place, situate previous work into this context, and introduce Printy, an augmented fabrication system that allows novice users to fabricate fully-functional Internet-connected objects.
SP  - 1510
EP  - 1518
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892509
ER  - 

TY  - NA
AU  - Nguyen, Viet; Rupavatharam, Siddharth; Liu, Luyang; Howard, Richard; Gruteser, Marco
TI  - SenSys - HandSense: capacitive coupling-based dynamic, micro finger gesture recognition
PY  - 2019
AB  - Head-mounted devices (HMD) for Augmented Reality (AR) are gaining traction thanks to a growing number of applications in the areas of image guided therapy, computer aided design, cargo packing, manufacturing and digital field service. However, providing an always available, intuitive and user friendly input for these devices remains a challenging problem. This paper explores recognizing dynamic, micro finger gestures using capacitive coupling for interacting with a head-mounted device. Electrodes are attached to fingertips of users gloves and capacitive coupling among all pairs of electrodes is measured quickly to infer the real-time spatial relationship between fingers. The system is able to recognize fine, low-effort finger gestures, such as swiping, sliding, tap, double-tap. We evaluated our prototype with 14 gestures executed by 10 subjects and found a 97% accuracy of gesture recognition.
SP  - 285
EP  - 297
JF  - Proceedings of the 17th Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3356250.3360040
ER  - 

TY  - JOUR
AU  - Liang, Xiangpeng; Ghannam, Rami; Heidari, Hadi
TI  - Wrist-Worn Gesture Sensing With Wearable Intelligence
PY  - 2019
AB  - This paper presents an innovative wrist-worn device with machine learning capabilities and a wearable pressure sensor array. The device is used for monitoring different hand gestures by tracking tendon movements around the wrist. Thus, an array of PDMS-encapsulated capacitive pressure sensors is attached to the user to capture wrist movement. The sensors are embedded on a flexible substrate and their readout requires a reliable approach for measuring small changes in capacitance. This challenge was addressed by measuring the capacitance via the switched capacitor method. The values were processed using a programme on LabVIEW to visually reconstruct the gestures on a computer. In addition, to overcome limitations of tendo’s uncertainty when the wristband is re-worn, or the user is changed, a calibration step based on the support vector machine (SVM) learning technique is implemented. Sequential minimal optimization algorithm is also applied in the system to generate SVM classifiers efficiently in real-time. The working principle and the performance of the SVM algorithms demonstrate through experiments. Three discriminated gestures have been clearly separated by SVM hyperplane and correctly classified with high accuracy (>90%) during real-time gesture recognition.
SP  - 1082
EP  - 1090
JF  - IEEE Sensors Journal
VL  - 19
IS  - 3
PB  - 
DO  - 10.1109/jsen.2018.2880194
ER  - 

TY  - BOOK
AU  - Faltaous, Sarah; Gruenefeld, Uwe; Schneegass, Stefan
TI  - Mensch und Computer - Towards a Universal Human-Computer Interaction Model for Multimodal Interactions
PY  - 2021
AB  - Models in HCI describe and provide insights into how humans use interactive technology. They are used by engineers, designers, and developers to understand and formalize the interaction process. At the same time, novel interaction paradigms arise constantly introducing new ways of how interactive technology can support humans. In this work, we look into how these paradigms can be described using the classical HCI model introduced by Schomaker in 1995. We extend this model by presenting new relations that would provide a better understanding of them. For this, we revisit the existing interaction paradigms and try to describe their interaction using this model. The goal of this work is to highlight the need to adapt the models to new interaction paradigms and spark discussion in the HCI community on this topic.
SP  - 59
EP  - 63
JF  - Mensch und Computer 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3473856.3474008
ER  - 

TY  - NA
AU  - Chen, Xiang 'Anthony'; Kim, Jeeeun; Mankoff, Jennifer; Grossman, Tovi; Coros, Stelian; Hudson, Scott E.
TI  - UIST - Reprise: A Design Tool for Specifying, Generating, and Customizing 3D Printable Adaptations on Everyday Objects
PY  - 2016
AB  - Everyday tools and objects often need to be customized for an unplanned use or adapted for specific user, such as adding a bigger pull to a zipper or a larger grip for a pen. The advent of low-cost 3D printing offers the possibility to rapidly construct a wide range of such adaptations. However, while 3D printers are now affordable enough for even home use, the tools needed to design custom adaptations normally require skills that are beyond users with limited 3D modeling experience.In this paper, we describe Reprise--a design tool for specifying, generating, customizing and fitting adaptations onto existing household objects. Reprise allows users to express at a high level what type of action is applied to an object. Based on this high level specification, Reprise automatically generates adaptations. Users can use simple sliders to customize the adaptations to better suit their particular needs and preferences, such as increasing the tightness for gripping, enhancing torque for rotation, or making a larger base for stability. Finally, Reprise provides a toolkit of fastening methods and support structures for fitting the adaptations onto existing objects.To validate our approach, we used Reprise to replicate 15 existing adaptation examples, each of which represents a specific category in a design space distilled from an analysis of over 3000 cases found in the literature and online communities. We believe this work would benefit makers and designers for prototyping lifehacking solutions and assistive technologies.
SP  - 29
EP  - 39
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984512
ER  - 

TY  - NA
AU  - Zhang, Yang; Xiao, Robert; Harrison, Chris
TI  - UIST - Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography
PY  - 2016
AB  - Electrical Impedance Tomography (EIT) was recently employed in the HCI domain to detect hand gestures using an instrumented smartwatch. This prior work demonstrated great promise for non-invasive, high accuracy recognition of gestures for interactive control. We introduce a new system that offers improved sampling speed and resolution. In turn, this enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation ' we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.
SP  - 843
EP  - 850
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984574
ER  - 

TY  - NA
AU  - Subbaraman, Blair; Peek, Nadya
TI  - p5.fab: Direct Control of Digital Fabrication Machines from a Creative Coding Environment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533496
ER  - 

TY  - JOUR
AU  - Gao, Qinghua; Jiang, Shuo; Shull, Peter B.
TI  - Simultaneous Hand Gesture Classification and Finger Angle Estimation via a Novel Dual-Output Deep Learning Model.
PY  - 2020
AB  - Hand gesture classification and finger angle estimation are both critical for intuitive human-computer interaction. However, most approaches study them in isolation. We thus propose a dual-output deep learning model to enable simultaneous hand gesture classification and finger angle estimation. Data augmentation and deep learning were used to detect spatial-temporal features via a wristband with ten modified barometric sensors. Ten subjects performed experimental testing by flexing/extending each finger at the metacarpophalangeal joint while the proposed model was used to classify each hand gesture and estimate continuous finger angles simultaneously. A data glove was worn to record ground-truth finger angles. Overall hand gesture classification accuracy was 97.5% and finger angle estimation R 2 was 0.922, both of which were significantly higher than shallow existing learning approaches used in isolation. The proposed method could be used in applications related to the human-computer interaction and in control environments with both discrete and continuous variables.
SP  - 2972
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 10
PB  - 
DO  - 10.3390/s20102972
ER  - 

TY  - NA
AU  - Khan, Arshad; Roo, Joan Sol; Kraus, Tobias; Steimle, Jürgen
TI  - UIST - Soft Inkjet Circuits: Rapid Multi-Material Fabrication of Soft Circuits using a Commodity Inkjet Printer
PY  - 2019
AB  - Despite the increasing popularity of soft interactive devices, their fabrication remains complex and time consuming. We contribute a process for rapid do-it-yourself fabrication of soft circuits using a conventional desktop inkjet printer. It supports inkjet printing of circuits that are stretchable, ultrathin, high resolution, and integrated with a wide variety of materials used for prototyping. We introduce multi-ink functional printing on a desktop printer for realizing multi-material devices, including conductive and isolating inks. We further present DIY techniques to enhance compatibility between inks and substrates and the circuits' elasticity. This enables circuits on a wide set of materials including temporary tattoo paper, textiles, and thermoplastic. Four application cases demonstrate versatile uses for realizing stretchable devices, e-textiles, body-based and re-shapeable interfaces.
SP  - 341
EP  - 354
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347892
ER  - 

TY  - NA
AU  - Chen, Jay; Abouzied, Azza; Hutchful, David; Ming, Joy; Ghosh, Ishita
TI  - ICTD - printr: Exploring the Potential of Paper-based Tools in Low-resource Settings
PY  - 2016
AB  - Despite the recent push toward using information and communication technologies (ICTs) to replace paper-based workflows, there remain many barriers to designing appropriate and deployable ICT solutions that replace paper. As a result, paper tools such as forms, charts, and graphs continue to be widely used, especially in developing regions. While paper is not without its drawbacks, its advantages are especially relevant to low-resource settings as paper tools require only a fraction of the development, deployment, and operational costs of software apps. In this paper, we investigate how paper tools can be improved and combined with ICTs so that low-resource organizations working in developing regions can benefit from the advantages of both types of tools. We perform an exploration of existing tools to design, printr, a system that integrates into existing paper-based workflows by allowing an organization to rapidly generate paper tools that can perform some functions typically associated with computation---addition, subtraction, lookup, visual feedback, and visualization --- without requiring the introduction of an ICT at the point of use. We compare two paper tools that printr produces with two mobile phone apps developed by a large NGO in Ghana and find comparable user performance between apps and generated paper tools.
SP  - 23
EP  - NA
JF  - Proceedings of the Eighth International Conference on Information and Communication Technologies and Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909609.2909649
ER  - 

TY  - BOOK
AU  - Tsuruta, Masaya; Nakamae, Shuta; Shizuki, Buntarou
TI  - ISS - RootCap: Touch Detection on Multi-electrodes using Single-line Connected Capacitive Sensing
PY  - 2016
AB  - In designing interactive products, it is important for designers to test the product's usability by manufacturing its shape and interface iteratively through rapid prototyping. The goal of our research is to provide the designers with an additional touch sensing method for rapid prototyping interactive products with flat, curved, or flexible surface. In this paper, we present RootCap, a capacitive touch sensing method that can detect a touch on a multi-electrode input surface while maintaining the characteristics of a single-line connection. The key concept behind realizing this goal is the imposition of unique capacitance on each electrode (including the capacitor connected to the touch electrode) branching from the single-line connection. Moreover, we developed a technique for creating a capacitor by printing silver nanoparticle ink on both sides of a sheet of paper, supporting designers in the creation of a multi-electrode input surface, on which each electrode has a unique capacitance.
SP  - 23
EP  - 32
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992180
ER  - 

TY  - NA
AU  - Kim, Raphael
TI  - CHI Extended Abstracts - Virus as Quasi-Living Bio-Material for Interaction Design: Practical, Ethical, and Philosophical Implications
PY  - 2021
AB  - The interaction design research community continues to benefit from material-focused approaches, and from the diversity of materials under investigation. One category of such material is bio-materials of microbial origin, such as bacteria, mycelium, moulds, and Euglena. However, despite the increasing momentum towards bio-material based research, one type that is yet to be investigated in HCI, is viruses; an infectious, sub-microscopic, quasi-living, computational bio-agent. This paper initiates exploration of Human-Virus Interaction (HVI), through a material lens. This was achieved first by generating a literature-based material profile sketch of viruses, highlighting some of their distinct and/or unique material properties, characteristics, composition, and meaning. The components of the profile were then used as anchor points, to unpack the practical, ethical, and philosophical implications that are associated with viruses, and those that could be considered by researchers to help in their preparation of working with viruses in interaction design.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451770
ER  - 

TY  - NA
AU  - Roumen, Thijs; Kruck, Bastian; Dürschmid, Tobias; Nack, Tobias; Baudisch, Patrick
TI  - Mobile Fabrication
PY  - 2016
AB  - We present an exploration into the future of fabrication, in particular the vision of mobile fabrication, which we define as "personal fabrication on the go". We explore this vision with two surveys, two simple hardware prototypes, matching custom apps that provide users with access to a solution database, custom fabrication processes we designed specifically for these devices, and a user study conducted in situ on metro trains. Our findings suggest that mobile fabrication is a compelling next direction for personal fabrication. From our experience with the prototypes we derive hardware requirements to make mobile fabrication also technically feasible.
SP  - 3
EP  - 14
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984586
ER  - 

TY  - JOUR
AU  - Chacon, D. Antony; Shinoda, Kazuhiro; Yokota, Tomoyuki; Yatani, Koji
TI  - Demonstrating the Feasibility of Subepidermal Image Sensing for Hand Posture and Gesture Recognition
PY  - 2022
AB  - Light-based wearable sensing methods for human body motion often rely on a few single light emitters and receivers, which leads to limited sensing capabilities. While increasing the number of light sources and sensors can help detect more complex motions, this increase in hardware often degrades wearability and mobility. In this letter, we employ a flexible organic photosensor matrix surrounded by an LED array as the light source to detect subepidermal images on the back of the hand. We then use computer vision and deep learning techniques to detect patterns based on blood-related changes under the skin. Our sensor system can accurately distinguish 32 hand postures and 17 gestures in user-dependent training, showing promise for ultralight wearable systems in natural user interface applications.
SP  - 1
EP  - 4
JF  - IEEE Sensors Letters
VL  - 6
IS  - 10
PB  - 
DO  - 10.1109/lsens.2022.3209074
ER  - 

TY  - NA
AU  - Wu, Chenming; Dai, Chengkai; Fang, Guoxin; Liu, Yong-Jin; Wang, Charlie C. L.
TI  - General Support-Effective Decomposition for Multi-Directional 3D Printing
PY  - 2018
AB  - We present a method for fabricating general models with multi-directional 3D printing systems by printing different model regions along with different directions. The core of our method is a support-effective volume decomposition algorithm that minimizes the area of the regions with large overhangs. A beam-guided searching algorithm with manufacturing constraints determines the optimal volume decomposition, which is represented by a sequence of clipping planes. While current approaches require manually assembling separate components into a final model, our algorithm allows for directly printing the final model in a single pass. It can also be applied to models with loops and handles. A supplementary algorithm generates special supporting structures for models where supporting structures for large overhangs cannot be eliminated. We verify the effectiveness of our method using two hardware systems: a Cartesian-motion based system and an angular-motion based system. A variety of 3D models have been successfully fabricated on these systems.
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Peng, Yi-Hao; Lin, Muh-Tarng; Chen, Yi; Chen, Tzu-chuan; Ku, Pin Sung; Taele, Paul; Lim, Chin Guan; Chen, Mike Y.
TI  - CHI - PersonalTouch: Improving Touchscreen Usability by Personalizing Accessibility Settings based on Individual User's Touchscreen Interaction
PY  - 2019
AB  - Modern touchscreen devices have recently introduced customizable touchscreen settings to improve accessibility for users with motor impairments. For example, iOS 10 introduced the following four Touch Accommodation settings: 1) Hold Duration, 2) Ignore Repeat, 3) Tap Assistance, and 4) Tap Assistance Gesture Delay. These four independent settings lead to a total of more than 1 million possible configurations, making it impractical to manually determine the optimal settings. We present PersonalTouch, which collects and analyzes touchscreen gestures performed by individual users, and recommends personalized, optimal touchscreen accessibility settings. Results from our user study show that PersonalTouch significantly improves touch input success rate for users with motor impairments (20.2%, N=12, p=.00054) and for users without motor impairments (1.28%, N=12, p=.032).
SP  - 683
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300913
ER  - 

TY  - JOUR
AU  - Hafid, Abdelakram; Benouar, Sara; Kedir-Talha, Malika; Abtahi, Farhad; Attari, Mokhtar; Seoane, Fernando
TI  - Full Impedance Cardiography Measurement Device Using Raspberry PI3 and System-on-Chip Biomedical Instrumentation Solutions
PY  - 2017
AB  - Impedance cardiography (ICG) is a noninvasive method for monitoring cardiac dynamics using electrical bioimpedance (EBI) measurements. Since its appearance more than 40 years ago, ICG has been used for assessing hemodynamic parameters. This paper presents a measurement system based on two System on Chip (SoC) solutions and Raspberry PI, implementing both a full three-lead ECG recorder and an impedance cardiographer, for educational and research development purposes. Raspberry PI is a platform supporting Do-It-Yourself project and education applications across the world. The development is part of Biosignal PI, an open hardware platform focusing in quick prototyping of physiological measurement instrumentation. The SoC used for sensing cardiac biopotential is the ADAS1000, and for the EBI measurement is the AD5933. The recordings were wirelessly transmitted through Bluetooth to a PC, where the waveforms were displayed, and hemodynamic parameters such as heart rate, stroke volume, ejection time and cardiac output were extracted from the ICG and ECG recordings. These results show how Raspberry PI can be used for quick prototyping using relatively widely available and affordable components, for supporting developers in research and engineering education. The design and development documents will be available on www.BiosignalPI.com , for open access under a Non Commercial-Share A like 4.0 International License.
SP  - 1883
EP  - 1894
JF  - IEEE journal of biomedical and health informatics
VL  - 22
IS  - 6
PB  - 
DO  - 10.1109/jbhi.2017.2783949
ER  - 

TY  - JOUR
AU  - Ku, Pin-Sung; Molla, Md. Tahmidul Islam; Huang, Kunpeng; Kattappurath, Priya; Ranjan, Krithik; Kao, Hsin-Liu Cindy
TI  - SkinKit
PY  - 2021
AB  - <jats:p>The emergence of on-skin interfaces has created an opportunity for seamless, always-available on-body interactions. However, developing a new fabrication process for on-skin interfaces can be time-consuming, challenging to incorporate new features, and not available for quick form-factor preview through prototyping. We introduce SkinKit, the first construction toolkit for on-skin interfaces, which enables fast, low-fidelity prototyping with a slim form factor directly applicable to the skin. SkinKit comprises modules consisting of skin-conformable base substrates and reusable Flexible Printed Circuits Board (FPCB) blocks. They are easy to attach and remove under tangible plug-and-play construction but still offer robust conductive connections in a slim form. Further, SkinKit aims to lower the barrier to entry in building on-skin interfaces without demanding technical expertise. It leverages a variety of preprogrammed modules connected in unique sequences to achieve various function customizations. We describe our iterative design and development process of SkinKit, comparing materials, connection mechanisms, and modules reflecting on its capability. We report results from single- and multi- session workshops with 34 maker participants spanning STEM and design backgrounds. Our findings reveal how diverse maker populations engage in on-skin interface design, what types of applications they choose to build, and what challenges they faced.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494989
ER  - 

TY  - NA
AU  - Kaimoto, Hiroki; Yamaoka, Junichi; Nakamaru, Satoshi; Kawahara, Yoshihiro; Kakehi, Yasuaki
TI  - Tangible and Embedded Interaction - ExpandFab: Fabricating Objects Expanding and Changing Shape with Heat
PY  - 2020
AB  - ExpandFab is a fabrication method for creating expanding objects using foam materials. The printed objects change their shape and volume, which is advantageous for reducing the printing time and transportation costs. For the fabrication of expanding objects, we investigated a basic principle of the expansion rate and developed materials by mixing a foam powder and elastic adhesive. Furthermore, we developed a fabrication method using the foam materials. A user can design expanded objects using our design software and sets the expansion areas on the surface. The software simulates and exports the 3d model into a three-dimensional (3D) printer. The 3D printer prints the expandable object by curing with ultraviolet light. Finally, the user heats the printed objects, and the objects expand to maximum approximately 2.7 times of their original size. ExpandFab allows users to prototype products that expand and morph into various shapes, such as objects changing from one shape to various shapes, and functional prototype with electronic components. In this paper, we describe the basic principle of this technique, implementation of the software and hardware, application examples, limitations and discussions, and future works.
SP  - 153
EP  - 164
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374949
ER  - 

TY  - JOUR
AU  - Booth, Riley; Goldsmith, Peter
TI  - A Wrist-Worn Piezoelectric Sensor Array for Gesture Input
PY  - 2017
AB  - Accurately estimating hand and finger poses for recognizing gestures helps solve many technical challenges, such as controlling prosthetics, robotic manipulation, and computer input, e.g. for virtual reality. A major challenge is accurately identifying gestures under different conditions—such as mobile or low-light environments—without hindering hand function. This paper describes a low-cost wrist-mounted device that uses piezoelectric sensors to estimate finger gestures. The signals that are recorded are vibrations and shape changes that occur at the wrist due to muscle and tendon motion. An array of six piezoelectric sensors was affixed to the inside of an adjustable wrist strap. A user study was completed. To identify when a subject made a finger tap gesture, a touch graphics tablet recorded when a fingertip contacted the tablet surface. Piezoelectric signal features were computed over timing windows coinciding with a gesture. The features were used in the training of a support vector machine classification model. The results indicate the viability of using piezoelectric sensors to classify finger tap gestures, with a mean classification accuracy of 97% for tap gestures made with each of the five fingers.
SP  - 284
EP  - 295
JF  - Journal of Medical and Biological Engineering
VL  - 38
IS  - 2
PB  - 
DO  - 10.1007/s40846-017-0303-8
ER  - 

TY  - NA
AU  - Sun, Lingyun; Yang, Yue; Chen, Yu; Li, Jiaji; Luo, Danli; Liu, Haolin; Yao, Lining; Tao, Ye; Wang, Guanyun
TI  - CHI - ShrinCage: 4D Printing Accessories that Self-Adapt
PY  - 2021
AB  - 3D printing technology makes Do-It-Yourself and reforming everyday objects a reality. However, designing and fabricating attachments that can seamlessly adapt existing objects to extended functionality is a laborious process, which requires accurate measuring, modeling, manufacturing, and assembly. This paper presents ShrinCage, a 4D printing system that allows novices to easily create shrinkable adaptations to fit and fasten existing objects. Specifically, the design tool presented in this work aid in the design of attachment that adapts to irregular morphologies, which accommodates the variations in measurements and fabrication, subsequently simplifying the modeling and assembly processes. We further conduct mechanical tests and user studies to evaluate the availability and feasibility of this method. Numerous application examples created by ShrinCage prove that it can be adopted by aesthetic modification, assistive technology, repair, upcycling, and augmented 3D printing.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445220
ER  - 

TY  - JOUR
AU  - Mat-Shayuti, Muhammad Shafiq; Zulkifli, H.; Yahya, Effah; Othman, Nur Hidayati; Hassan, Zulkafli
TI  - Development of Low-Cost, Non-Obtrusive Electrical Impedance Tomography Device for Liquid-Gas Flow Visualization
PY  - 2019
AB  - The high cost of flow visualization equipment on the market hinders more active exploration in flow science. This paper presents a development of low-cost, non-obtrusive electrical impedance tomography (EIT) device for two-phase (liquid-gas) flow visualization. The device consists of three main hardware; an Arduino microcontroller as processor and communicators between components, a customized printed circuit board to control current injection and voltage measurement, and sensors band which consists of sixteen copper electrodes. During testing, the sensors band was enclosed around a cylindrical phantom tank filled with water. The sensing field then was stimulated by injecting current to the sensors of copper electrodes. Next, the measured voltage values were imported to MATLAB and interfaced with electrical impedance tomography and diffuse optical tomography reconstruction software (EIDORS) software for reconstruction of test images. The system capitalized on the different conductivity values of water and air, with the reconstructed images showed excellent agreement with the test images. © 2019 Int. J. Elec. & Elecn. Eng. & Telcomm
SP  - 119
EP  - 126
JF  - International Journal of Electrical and Electronic Engineering & Telecommunications
VL  - NA
IS  - NA
PB  - 
DO  - 10.18178/ijeetc.8.2.119-126
ER  - 

TY  - NA
AU  - Iravantchi, Yasha; Goel, Mayank; Harrison, Chris
TI  - CHI - BeamBand: Hand Gesture Sensing with Ultrasonic Beamforming
PY  - 2019
AB  - BeamBand is a wrist-worn system that uses ultrasonic beamforming for hand gesture sensing. Using an array of small transducers, arranged on the wrist, we can ensem-ble acoustic wavefronts to project acoustic energy at spec-ified angles and focal lengths. This allows us to interro-gate the surface geometry of the hand with inaudible sound in a raster-scan-like manner, from multiple view-points. We use the resulting, characteristic reflections to recognize hand pose at 8 FPS. In our user study, we found that BeamBand supports a six-class hand gesture set at 94.6% accuracy. Even across sessions, when the sensor is removed and reworn later, accuracy remains high: 89.4%. We describe our software and hardware, and future ave-nues for integration into devices such as smartwatches and VR controllers.
SP  - 15
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300245
ER  - 

TY  - BOOK
AU  - Attene, Marco; Livesu, Marco; Lefebvre, Sylvain; Funkhouser, Thomas; Rusinkiewicz, Szymon; Ellero, Stefano; Martínez, Jonàs; Bermano, Amit
TI  - Design, Representations, and Processing for Additive Manufacturing
PY  - 2018
AB  - Abstract The wide diffusion of 3D printing technologies continuously calls for effective solutions for designing and fabricating objects of increasing complexity. The so called "computational fabri...
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yamazaki, Hiroshi; Watanabe, Kazuhiro
TI  - GCCE - Fiber Optic Folding Angle Sensors for Origami-Inspired Devices
PY  - 2019
AB  - Origami-inspired devices have been widely applied in engineering for converting two-dimensional materials into innovative and functional three-dimensional objects. This paper proposed a novel sensing method for identifying folding angles, which was able to be employed to foldable materials like a sheet of paper with no constraint. A fundamental examination was conducted in which a proposed fiber wiring pattern enabled to convert folding angle into moderate curvature of an optical fiber, and therefore the optical fiber detected the folding as an optical loss changes. Furthermore, the sensitivity to folding angle could be enhanced by introducing a hetero-core fiber optic macro-bending sensor.
SP  - 648
EP  - 650
JF  - 2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/gcce46687.2019.9014647
ER  - 

TY  - NA
AU  - Alasasfeh, Hayat; Hafsa, Mariem; Bader, Oumaima; Ibbini, Mohammed; Hasan, Sameer; Kanoun, Olfa
TI  - Electrical Impedance Tomography Image Reconstruction with apriori Knowledge for Gesture Recognition
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 19th International Multi-Conference on Systems, Signals & Devices (SSD)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ssd54932.2022.9955849
ER  - 

TY  - NA
AU  - Moghanizadeh, Abbas; Ashrafizadeh, Fakhreddin
TI  - Studying sacrificial ice structure, as soluble support layers, in 3D printing of polymers (FDM)
PY  - 2021
AB  - Additive manufacturing has the potential to fabricate complex structures and shapes using a simple layer-to-layer technique. In additive manufacturing processes, support layers have been used extensively for overhanging surfaces, however, a difficult procedure for design and a long-lasting process for eliminating support layers are required. Eliminating support layers often involve time-consuming post-print processes and adding costs. One viable solution to make the 3D printing process more streamlined is to use soluble sacrificial support materials. In this study, a novel technique based on employing ice structures as sacrificial support layers in the FDM process is evaluated. In the proposed technique, the water layers are frozen by a cryogenic tank and the ice layers play the role of supporting layers. Then, polymer layers are created on the ice support layers based on the CAD model. Lastly, the support layers made of ice are removed by freeze-drying or melting process. The results confirm that objects requiring supporting layers can be printed on ice layers which are removed easily and quickly by melting. However, the heat of the molten polymer laying down on the ice support layers affects the geometric tolerance and surface roughness of the printed objects. In addition, unwanted placement of ice layers between the polymer layers also reduces the tensile strength of the objects compared to the classical FDM. The main merit of the presented technique is that it drastically reduces the post-processing required to remove support layers.
SP  - 1
EP  - 7
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Pierson, Harry A.; Chivukula, Bharat
TI  - Process–Property Relationships for Fused Filament Fabrication on Preexisting Polymer Substrates
PY  - 2018
AB  - <jats:p>Recent advances in fused filament fabrication (FFF), such as five-axis printing, patching existing parts, and certain hybrid manufacturing processes, involve printing atop a previously manufactured polymer substrate. The success of these technologies depends upon the bond strength between the substrate and the newly added geometry. ANOVA and response surface methods were used to determine the effect of three process parameters on bond tensile strength: surface roughness, layer thickness, and raster angle. Experimental results indicate that the process–property relationships are not identical to those found in single, continuous FFF operations, and that the physical bonding mechanisms may also be different. Bond strength was found to be highly sensitive to surface roughness and layer thickness, and distinct optimal parameter settings exist. These results represent a first step toward understanding bond strength in such circumstances, allowing manufacturers to intelligently select process parameters for the production of both the substrate and the secondary geometry.</jats:p>
SP  - 084501
EP  - NA
JF  - Journal of Manufacturing Science and Engineering
VL  - 140
IS  - 8
PB  - 
DO  - 10.1115/1.4039766
ER  - 

TY  - NA
AU  - Schmitz, Martin; Günther, Sebastian; Schön, Dominik; Müller, Florian
TI  - Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501981
ER  - 

TY  - JOUR
AU  - Bin Ishak, Ismayuzri; Larochelle, Pierre M.
TI  - MotoMaker: a robot FDM platform for multi-plane and 3D lattice structure printing
PY  - 2019
AB  - Additive manufacturing is a process to fabricate three-dimensional (3D) objects usually by joining a material layer by layer. The layer by layer joining process simplifies the fabrication method by slicing the 3D object into stacks of 2D contours. The layers are combined in a single build direction to form the 3D object. In this research, a six degrees of freedom robot arm is integrated with a fused deposition modeling system for multi-plane and 3D lattice structure printing applications. The integration processes in developing the platform are discussed including the robot arm platform, extruder system, software architecture, toolpath generation, 3D lattice generator, and extruder calibration. Use of the system offers unique advantages over a conventional Cartesian 3D printer platform which is limited to single-plane layering for the printing of 3D objects. Test cases are performed to demonstrate the capability of the robot arm fused deposition modeling platform for multi-plane object printing and 3D lattice structure printing.
SP  - 703
EP  - 720
JF  - Mechanics Based Design of Structures and Machines
VL  - 47
IS  - 6
PB  - 
DO  - 10.1080/15397734.2019.1615943
ER  - 

TY  - JOUR
AU  - Murtezaoglu, Yavuz; Plakhotnik, Denys; Stautner, Marc; Vaneker, Thomas H.J.; van Houten, Fred J. A. M.
TI  - Geometry-Based Process Planning For Multi-Axis Support-Free Additive Manufacturing
PY  - 2018
AB  - Abstract In contrast to standard layer based additive manufacturing methodologies, multi-axis material deposition can print structures without the need for support material. However, this method is jeopardized by potential collisions between a depositing unit (nozzle, wire, power and powder sources, etc.) and the already deposited material. The goal of this research is to initiate development of a methodology to check manufacturing feasibility of geometries and generate subsequent process planning strategies. The paper describes a geometry-based concept to decompose the product geometry into discrete volumes by using space partitioning with infinite planes and considering advantages and constraints of multi-axis additive manufacturing. The discrete volumes are used to generate process planning variants and to compute and generate boundary conditions for such process planning strategies. The algorithm generates multi-axis slices that require no support structures because of relative nozzle/workpiece orientation. In addition, the planning tackles more complex scenarios, in which overhangs, nozzle orientation, and gravity can be considered.
SP  - 73
EP  - 78
JF  - Procedia CIRP
VL  - 78
IS  - NA
PB  - 
DO  - 10.1016/j.procir.2018.08.175
ER  - 

TY  - NA
AU  - Faisandaz, Gauthier Robert Jean; Goguey, Alix; Jouffrais, Christophe; Nigay, Laurence
TI  - Keep in Touch: Combining Touch Interaction with Thumb-to-Finger µGestures for People with Visual Impairment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556589
ER  - 

TY  - NA
AU  - Mosenia, Arsalan
TI  - Addressing Security and Privacy Challenges in Internet of Things
PY  - 2017
AB  - Internet of Things (IoT), also referred to as the Internet of Objects, is envisioned as a holistic and transformative approach for providing numerous services. The rapid development of various communication protocols and miniaturization of transceivers along with recent advances in sensing technologies offer the opportunity to transform isolated devices into communicating smart things. Smart things, that can sense, store, and even process electrical, thermal, optical, chemical, and other signals to extract user-/environment-related information, have enabled services only limited by human imagination. Despite picturesque promises of IoT-enabled systems, the integration of smart things into the standard Internet introduces several security challenges because the majority of Internet technologies, communication protocols, and sensors were not designed to support IoT. Several recent research studies have demonstrated that launching security/privacy attacks against IoT-enabled systems, in particular wearable medical sensor (WMS)-based systems, may lead to catastrophic situations and life-threatening conditions. Therefore, security threats and privacy concerns in the IoT domain need to be proactively studied and aggressively addressed. In this thesis, we tackle several domain-specific security/privacy challenges associated with IoT-enabled systems.
SP  - NA
EP  - NA
JF  - arXiv: Cryptography and Security
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Norman, James J.; Madurawe, Rapti D.; Moore, Christine M. V.; Khan, Mansoor A.; Khairuzzaman, Akm
TI  - A new chapter in pharmaceutical manufacturing: 3D-printed drug products.
PY  - 2016
AB  - NA
SP  - 39
EP  - 50
JF  - Advanced drug delivery reviews
VL  - 108
IS  - 108
PB  - 
DO  - 10.1016/j.addr.2016.03.001
ER  - 

TY  - NA
AU  - Ulu, Erva; McCann, James; Kara, Levent Burak
TI  - Structural Design Using Laplacian Shells
PY  - 2019
AB  - We introduce a method to design lightweight shell objects that are structurally robust under the external forces they may experience during use. Given an input 3D model and a general description of the external forces, our algorithm generates a structurally-sound minimum weight shell object. Our approach works by altering the local shell thickness repeatedly based on the stresses that develop inside the object. A key issue in shell design is that large thickness values might result in self-intersections on the inner boundary creating a significant computational challenge during optimization. To address this, we propose a shape parametrization based on the solution to the Laplace's equation that guarantees smooth and intersection-free shell boundaries. Combined with our gradient-free optimization algorithm, our method provides a practical solution to the structural design of hollow objects with a single inner cavity. We demonstrate our method on a variety of problems with arbitrary 3D models under complex force configurations and validate its performance with physical experiments.
SP  - NA
EP  - NA
JF  - arXiv: Computational Geometry
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Matulic, Fabrice; Vogel, Daniel
TI  - Deep Learning-Based Hand Posture Recognition for Pen Interaction Enhancement
PY  - 2021
AB  - This chapter examines how digital pen interaction can be expanded by detecting different hand postures formed primarily by the hand while it grips the pen. Three systems using different types of sensors are considered: an EMG armband, the raw capacitive image of the touchscreen, and a pen-top fisheye camera. In each case, deep neural networks are used to perform classification or regression to detect hand postures and gestures. Additional analyses are provided to demonstrate the benefit of deep learning over conventional machine-learning methods, as well as explore the impact on model accuracy resulting from the number of postures to be recognised, user-dependent versus user-independent models, and the amount of training data. Examples of posture-based pen interaction in applications are discussed and a number of usability aspects resulting from user evaluations are identified. The chapter concludes with perspectives on the recognition and design of posture-based pen interaction for future systems.
SP  - 193
EP  - 225
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_7
ER  - 

TY  - JOUR
AU  - Liu, Hao; Liu, Lei; Li, Dawei; Huang, Renkai; Dai, Ning
TI  - An approach to partition workpiece CAD model towards 5-axis support-free 3D printing
PY  - 2019
AB  - This paper presents a new method to fabricate workpieces using a 5-axis printing equipment with similar movement way of 5-axis milling machine tools. The method includes two main steps: (i) CAD model is partitioned into several sub-parts using a gravity effect partition method simulating the material-falling process when the model is stacked along the Z direction. In our processing plan, every sub-part has a slice direction. Before printing a sub-part, we rotate A axis and C axis so that its slice direction exactly coincides with the Z axis positive direction and then materials are stacked along the slice direction; (ii) these sub-parts are sorted with printing-base constraint and interference-free constraint. The two constraints, respectively, mean that the previously printed sub-parts are used as the printing bases of subsequently printed sub-parts; there is no interference between printed sub-parts and the printer head. These partition and sort principles have been generalized as an optimization model to satisfy printing-base constraint, interference-free constraint, and shortest empty printing path constraint. Our printing processing-plan can be regarded as a process to solve the optimization model. We have successfully generated sub-part sequences for some CAD models with large overhangs and complex structures to verify the printing processing-plan method.
SP  - 683
EP  - 699
JF  - The International Journal of Advanced Manufacturing Technology
VL  - 106
IS  - 1
PB  - 
DO  - 10.1007/s00170-019-04495-3
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Koelle, Marion; Fruchard, Bruno; Mavali, Sahar; Klamka, Konstantin; Groeger, Daniel; Strohmeier, Paul; Steimle, Jürgen
TI  - CHI - BodyStylus: Freehand On-Body Design and Fabrication of Epidermal Interfaces
PY  - 2021
AB  - In traditional body-art, designs are adjusted to the body as they are applied, enabling creative improvisation and exploration. Conventional design and fabrication methods of epidermal interfaces, however, separate these steps. With BodyStylus we present the first computer-assisted approach for on-body design and fabrication of epidermal interfaces. Inspired by traditional techniques, we propose a hand-held tool that augments freehand inking with digital support: projected in-situ guidance assists creating valid on-body circuits and aesthetic ornaments that align with the human bodyscape, while pro-active switching between inking and non-inking creates error preventing constraints. We contribute BodyStylus’s design rationale and interaction concept along with an interactive prototype that uses self-sintering conductive ink. Results of two focus group explorations showed that guidance was more appreciated by artists, while constraints appeared more useful to engineers, and that working on the body inspired critical reflection on the relationship between bodyscape, interaction, and designs.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445475
ER  - 

TY  - NA
AU  - Xu, Xuhai; Gong, Jun; Brum, Carolina; Liang, Lilian; Suh, Bongsoo; Gupta, Shivam Kumar; Agarwal, Yash; Lindsey, Laurence; Kang, Runchang; Shahsavari, Behrooz; Nguyen, Tu; Nieto, Heriberto; Hudson, Scott E; Maalouf, Charlie; Mousavi, Jax Seyed; Laput, Gierad
TI  - Enabling Hand Gesture Customization on Wrist-Worn Devices
PY  - 2022
AB  - We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we first deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7% and a false-positive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3%, 83.1%, and 87.2% on using one, three, or five shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the effectiveness, learnability, and usability of our customization framework. Our approach paves the way for a future where users are no longer bound to pre-existing gestures, freeing them to creatively introduce new gestures tailored to their preferences and abilities.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501904
ER  - 

TY  - NA
AU  - Kim, Doheon; Shin, Joongi; Saakes, Daniel
TI  - Conference on Designing Interactive Systems (Companion Volume) - Foam Sheets as Material for Fabricating Large and Functional Soft Objects
PY  - 2020
AB  - We report on our experiments to fabricate soft, functional and furniture size objects from PE foam sheets. This material is desirable because it is flexible, lightweight, widely available in a variety of thicknesses, and could be made out of sugar cane. We present a set of unique joining mechanisms that exploit the flexibility and do not use adhesive or fasteners. We show how to program softness in meta structures to avoid the use of multiple materials. Finally, we demonstrate a number of examples and discuss the applicability for prototyping as well as customization in deploying the design.
SP  - 239
EP  - 244
JF  - Companion Publication of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3393914.3395866
ER  - 

TY  - NA
AU  - Vasquez, Joshua; Twigg-Smith, Hannah; O'Leary, Jasper Tran; Peek, Nadya
TI  - CHI Extended Abstracts - Jubilee Demo: An Extensible Machine for Multi-Tool Fabrication
PY  - 2020
AB  - We present Jubilee, an open-source hardware machine with automatic tool-changing and interchangeable bed plates. As digital fabrication tools have become more broadly accessible, tailoring those machines to new users and novel workflows has become central to HCI research. However, the lack of hardware infrastructure makes custom application development cumbersome. We identify a need for an extensible platform to allow HCI researchers to develop workflows for fabrication, material exploration, and other applications. Jubilee addresses this need. It can automatically and repeatably change tools in the same operation. It can be built with a combination of simple 3D-printed and readily available parts. It has several standard head designs for a variety of applications including 3D printing, syringe-based liquid handling, imaging, and plotting. We present Jubilee with a comprehensive set of assembly instructions and kinematic mount templates for user-designed tools and bed plates. Finally we demonstrate Jubilee's multi-tool workflow functionality with a series of example applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376425
ER  - 

TY  - CONF
AU  - Kuno, Wakaba; Sugiura, Yuta; Asano, Nao; Kawai, Wataru; Sugimoto, Maki
TI  - ICAT-EGVE (Posters and Demos) - Estimation of 3D Finger Postures with wearable device measuring Skin Deformation on Back of Hand
PY  - NA
AB  - NA
SP  - 41
EP  - 42
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.2312/egve.20171384
ER  - 

TY  - JOUR
AU  - Liu, Haoyan; Panahi, Atiyehsadat; Andrews, David; Nelson, Alexander
TI  - An FPGA-Based Upper-Limb Rehabilitation Device for Gesture Recognition and Motion Evaluation Using Multi-Task Recurrent Neural Networks
PY  - 2022
AB  - Upper-Extremity motor impairment affects millions of Americans due to cerebrovascular incidents, spinal cord injuries, or brain trauma. Current therapy practices used to assist these individuals in regaining motor functionality often require extensive time at rehabilitation facilities with potentially prohibitive travel or financial costs. This work presents a mobile low-cost field programmable gate array (FPGA)-smart rehabilitation system that can be used in home environments. The prototype is a rehabilitation table instrumented with a capacitive sensor array (CSA) to track upper-extremity motions of the user through proximity or touch. In addition, inertial measurement units (IMUs) are placed on the affected upper limb and combined with the CSA data with our sensor fusion signal processing architecture. Motions are classified and evaluated using multi-task convolutional recurrent neural networks with three additional motion quality output classes to personalize recognition based on the particular motor skills of each patient. The prototype achieves above 99&#x0025; accuracy with 32-bit fixed-point format implementation for recognizing dynamic motions and identifying unnatural characteristics (i.e., tremor or limited flexion and extension) in upper limb motions based on sensor values. The convolutional recurrent neural network (C-RNN) fusion classification network is implemented on a 200 MHz Zynq ZCU104 FPGA using an HLS-based design optimized with pipelining and parallelism techniques and achieves 5.4x speedup compared to ARM&#x00AE; Cortex-A53 implementation running at an operating frequency of 1.3 GHz. The prototype is also demonstrated to perform the machine learning classification in real-time.
SP  - 3605
EP  - 3615
JF  - IEEE Sensors Journal
VL  - 22
IS  - 4
PB  - 
DO  - 10.1109/jsen.2022.3141659
ER  - 

TY  - NA
AU  - Lu, Duo; Huang, Dijiang
TI  - FMCode: A 3D In-the-Air Finger Motion Based User Login Framework for Gesture Interface.
PY  - 2018
AB  - Applications using gesture-based human-computer interface require a new user login method with gestures because it does not have a traditional input method to type a password. However, due to various challenges, existing gesture-based authentication systems are generally considered too weak to be useful in practice. In this paper, we propose a unified user login framework using 3D in-air-handwriting, called FMCode. We define new types of features critical to distinguish legitimate users from attackers and utilize Support Vector Machine (SVM) for user authentication. The features and data-driven models are specially designed to accommodate minor behavior variations that existing gesture authentication methods neglect. In addition, we use deep neural network approaches to efficiently identify the user based on his or her in-air-handwriting, which avoids expansive account database search methods employed by existing work. On a dataset collected by us with over 100 users, our prototype system achieves 0.1% and 0.5% best Equal Error Rate (EER) for user authentication, as well as 96.7% and 94.3% accuracy for user identification, using two types of gesture input devices. Compared to existing behavioral biometric systems using gesture and in-air-handwriting, our framework achieves the state-of-the-art performance. In addition, our experimental results show that FMCode is capable to defend against client-side spoofing attacks, and it performs persistently in the long run. These results and discoveries pave the way to practical usage of gesture-based user login over the gesture interface.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Liu, Hongyi; Wang, Lihui
TI  - Latest Developments of Gesture Recognition for Human–Robot Collaboration
PY  - 2021
AB  - Recently, the concept of human–robot collaboration has raised many research interests. Instead of robots replacing human operators in workplaces, human–robot collaboration is the direction that allows human operators and robots to work together. Although the communication channels between human operators and robots are still limited, gesture recognition has been effectively applied as the interface between humans and computers for a long time. Covering some of the most important technologies and algorithms of gesture recognition, this chapter is intended to provide an overview of the gesture recognition research and explore the possibility to apply gesture recognition in human–robot collaboration. In this chapter, an overall model of gesture recognition for human–robot collaboration is also proposed. There are four essential technical components in the model of gesture recognition for human–robot collaboration: sensor technologies, gesture identification, gesture tracking and gesture classification. Reviewed approaches are classified according to the four essential technical components. After the reviewed technical components, an example of gesture recognition for human–robot collaboration is provided. In the last part of the chapter, future research trends are outlined.
SP  - 43
EP  - 68
JF  - Advanced Human-Robot Collaboration in Manufacturing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-69178-3_2
ER  - 

TY  - NA
AU  - Parizi, Farshid Salemi; Kienzle, Wolf; Whitmire, Eric; Gupta, Aakar; Benko, Hrvoje
TI  - RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband
PY  - 2021
AB  - We introduce RotoWrist, an infrared (IR) light based solution for continuously and reliably tracking 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489886
ER  - 

TY  - JOUR
AU  - Li, Xiuyan; Sun, Jianrui; Wang, Qi; Zhang, Ronghua; Duan, Xiaojie; Sun, Yukuan; Wang, Jianming
TI  - Dynamic Hand Gesture Recognition Using Electrical Impedance Tomography.
PY  - 2022
AB  - Electrical impedance tomography (EIT) has been applied in the field of human-computer interaction due to its advantages including the fact that it is non-invasive and has both low power consumption and a low cost. Previous work has focused on static gesture recognition based on EIT. Compared with static gestures, dynamic gestures are more informative and can achieve more functions in human-machine collaboration. In order to verify the feasibility of dynamic gesture recognition based on EIT, a traditional excitation drive pattern is optimized in this paper. The drive pattern of the fixed excitation electrode is tested for the first time to simplify the measurement process of the dynamic gesture. To improve the recognition accuracy of the dynamic gestures, a dual-channel feature extraction network combining a convolutional neural network (CNN) and gated recurrent unit (GRU), namely CG-SVM, is proposed. The new center distance loss is designed in order to simultaneously supervise the intra-class distance and inter-class distance. As a result, the discriminability of the confusing data is improved. With the new excitation drive pattern and classification network, the recognition accuracy of different interference data has increased by 2.7~14.2%. The new method has stronger robustness, and realizes the dynamic gesture recognition based on EIT for the first time.
SP  - 7185
EP  - 7185
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 19
PB  - 
DO  - 10.3390/s22197185
ER  - 

TY  - JOUR
AU  - Vaquero-Gallardo, Noelia; Martínez-García, Herminio
TI  - Electrical Impedance Tomography for Hand Gesture Recognition for HMI Interaction Applications
PY  - 2022
AB  - <jats:p>Electrical impedance tomography (EIT) is based on the physical principle of bioimpedance defined as the opposition that biological tissues exhibit to the flow of a rotating alternating electrical current. Consequently, here, we propose studying the characterization and classification of bioimpedance patterns based on EIT by measuring, on the forearm with eight electrodes in a non-invasive way, the potential drops resulting from the execution of six hand gestures. The starting point was the acquisition of bioimpedance patterns studied by means of principal component analysis (PCA), validated through the cross-validation technique, and classified using the k-nearest neighbor (kNN) classification algorithm. As a result, it is concluded that reduction and classification is feasible, with a sensitivity of 0.89 in the worst case, for each of the reduced bioimpedance patterns, leading to the following direct advantage: a reduction in the numbers of electrodes and electronics required. In this work, bioimpedance patterns were investigated for monitoring subjects’ mobility, where, generally, these solutions are based on a sensor system with moving parts that suffer from significant problems of wear, lack of adaptability to the patient, and lack of resolution. Whereas, the proposal implemented in this prototype, based on the so-called electrical impedance tomography, does not have these problems.</jats:p>
SP  - 41
EP  - 41
JF  - Journal of Low Power Electronics and Applications
VL  - 12
IS  - 3
PB  - 
DO  - 10.3390/jlpea12030041
ER  - 

TY  - NA
AU  - Salami, Dariush; Hasibi, Ramin; Palipana, Sameera; Popovski, Petar; Michoel, Tom; Sigg, Stephan
TI  - Tesla-Rapture: A Lightweight Gesture Recognition System from mmWave Radar Point Clouds.
PY  - 2021
AB  - We present Tesla-Rapture, a gesture recognition interface for point clouds generated by mmWave Radars. State of the art gesture recognition models are either too resource consuming or not sufficiently accurate for integration into real-life scenarios using wearable or constrained equipment such as IoT devices (e.g. Raspberry PI), XR hardware (e.g. HoloLens), or smart-phones. To tackle this issue, we developed Tesla, a Message Passing Neural Network (MPNN) graph convolution approach for mmWave radar point clouds. The model outperforms the state of the art on two datasets in terms of accuracy while reducing the computational complexity and, hence, the execution time. In particular, the approach, is able to predict a gesture almost 8 times faster than the most accurate competitor. Our performance evaluation in different scenarios (environments, angles, distances) shows that Tesla generalizes well and improves the accuracy up to 20% in challenging scenarios like a through-wall setting and sensing at extreme angles. Utilizing Tesla, we develop Tesla-Rapture, a real-time implementation using a mmWave Radar on a Raspberry PI 4 and evaluate its accuracy and time-complexity. We also publish the source code, the trained models, and the implementation of the model for embedded devices.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Moon, Kongpyung (Justin); Lee, Haeun; Kim, Jeeeun; Bianchi, Andrea
TI  - ShrinkCells: Localized and Sequential Shape-Changing Actuation of 3D-Printed Objects via Selective Heating
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545670
ER  - 

TY  - NA
AU  - Landwehr Sydow, Sophie; Jonsson, Martin; Tholander, Jakob
TI  - Modding the Pliable Machine: Unpacking the Creative and Social Practice of Upkeep at the Makerspace
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3532804
ER  - 

TY  - JOUR
AU  - Song, Peng; Deng, Bailin; Wang, Ziqi; Dong, Zhi-Chao; Li, Wei; Fu, Chi-Wing; Liu, Ligang
TI  - CofiFab: coarse-to-fine fabrication of large 3D objects
PY  - 2016
AB  - This paper presents CofiFab, a coarse-to-fine 3D fabrication solution, combining 3D printing and 2D laser cutting for cost-effective fabrication of large objects at lower cost and higher speed. Our key approach is to first build coarse internal base structures within the given 3D object using laser cutting, and then attach thin 3D-printed parts, as an external shell, onto the base to recover the fine surface details. CofiFab achieves this with three novel algorithmic components. First, we formulate an optimization model to compute fabricatable polyhedrons of maximized volume, as the geometry of the internal base. Second, we devise a new interlocking scheme to tightly connect the laser-cut parts into a strong internal base, by iteratively building a network of nonorthogonal joints and interlocking parts around polyhedral corners. Lastly, we optimize the partitioning of the external object shell into 3D-printable parts, while saving support material and avoiding overhangs. Besides cost saving, these components also consider aesthetics, stability and balancing. Hence, CofiFab can efficiently produce large objects by assembly. To evaluate CofiFab, we fabricate objects of varying shapes and sizes, and show that CofiFab can significantly outperform previous methods.
SP  - 45
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 35
IS  - 4
PB  - 
DO  - 10.1145/2897824.2925876
ER  - 

TY  - NA
AU  - Roumen, Thijs; Apel, Ingo; Kern, Thomas; Taraz, Martin; Sharma, Ritesh; Schlueter, Ole; Johnson, Jeffrey; Meier, Dominik; Lempert, Conrad; Baudisch, Patrick
TI  - Structure-Preserving Editing of Plates and Volumes for Laser Cutting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3561996
ER  - 

TY  - JOUR
AU  - Cortes, Guillaume; Marchand, Eric; Brincin, Guillaume; Lécuyer, Anatole
TI  - MoSART: Mobile Spatial Augmented Reality for 3D Interaction With Tangible Objects.
PY  - 2018
AB  - In this paper we introduce MoSART, a novel approach for Mobile Spatial Augmented Reality on Tangible objects. MoSART is dedicated to mobile interaction with tangible objects in single or collaborative situations. It is based on a novel " all-in-one " Head-Mounted Display (AMD) including a projector (for the SAR display) and cameras (for the scene registration). Equipped with the HMD the user is able to move freely around tangible objects and manipulate them at will. The system tracks the position and orientation of the tangible 3D objects and projects virtual content over them. The tracking is a feature-based stereo optical tracking providing high accuracy and low latency. A projection mapping technique is used for the projection on the tangible objects which can have a complex 3D geometry. Several interaction tools have also been designed to interact with the tangible and augmented content, such as a control panel and a pointer metaphor, which can benefit as well from the MoSART projection mapping and tracking features. The possibilities offered by our novel approach are illustrated in several use cases, in single or collaborative situations, such as for virtual prototyping, training or medical visualization.
SP  - 93
EP  - NA
JF  - Frontiers in robotics and AI
VL  - 5
IS  - 93
PB  - 
DO  - 10.3389/frobt.2018.00093
ER  - 

TY  - NA
AU  - Chen, Xiang 'Anthony'; Coros, Stelian; Hudson, Scott E.
TI  - CHI - Medley: A Library of Embeddables to Explore Rich Material Properties for 3D Printed Objects
PY  - 2018
AB  - In our everyday life, we interact with and benefit from objects with a wide range of material properties. In contrast, personal fabrication machines (e.g., desktop 3D printers) currently only support a much smaller set of materials. Our goal is to close the gap between current limitations and the future of multi-material printing by enabling people to explore the reuse of material from everyday objects into their custom designs. To achieve this, we develop a library of embeddables--everyday objects that can be cut, worked and embedded into 3D printable designs. We describe a design space that characterizes the geometric and material properties of embeddables. We then develop Medley---a design tool whereby users can import a 3D model, search for embeddables with desired material properties, and interactively edit and integrate their geometry to fit into the original design. Medley also supports the final fabrication and embedding process, including instructions for carving or cutting the objects, and generating optimal paths for inserting embeddables. To validate the expressiveness of our library, we showcase numerous examples augmented by embeddables that go beyond the objects' original printed materials.
SP  - 162
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173736
ER  - 

TY  - JOUR
AU  - Anandkumar, R.; Babu, S. Ramesh
TI  - FDM filaments with unique segmentation since evolution: a critical review
PY  - 2018
AB  - The urge towards faster and sophisticated manufacturing is a nurturing factor of human life. Researchers envisage in developing complex products in shorter time duration. The conventional subtractive manufacturing undergoes pre-processing, processing, and post-processing stages. Additive manufacturing (AM) undergoes the same set of stages, where as raw material is added gradually to get the final product in contradiction to the subtractive manufacturing. There are many variants in AM technology, amongst that fused deposition modeling (FDM) is less sophisticated and incurs lesser manufacturing cost. The filament is the deciding factor for the final quality and cost of the product in FDM. Filament invariants have evolved slowly since the 1980s and had shown imperative growth in the last decade. Hence, the significance of filament in FDM insists to undergo a critical review on filament segmentation, growth, and its future. The absence of specific article or publication presenting a review is the prime objective of the critical review.
SP  - 185
EP  - 193
JF  - Progress in Additive Manufacturing
VL  - 4
IS  - 2
PB  - 
DO  - 10.1007/s40964-018-0069-8
ER  - 

TY  - NA
AU  - Frich, Jonas; Nouwens, Midas; Halskov, Kim; Dalsgaard, Peter
TI  - CHI - How Digital Tools Impact Convergent and Divergent Thinking in Design Ideation
PY  - 2021
AB  - Digital tools that support creative activities are ubiquitous in the design industry, yet practitioners appear to prefer pen and paper for design ideation. To better understand this exception, we conducted a comparative study between analog and digital tools and their impact on the divergent and convergent thinking patterns of groups of designers. We analysed how 24 participants solved comparable design ideation tasks in two conditions using linkographic protocol analysis – a notation method that focuses on identifying and linking small steps in the design process called moves. Our findings suggest that digital ideation tools yield more convergent thinking compared to analog tools, with no discernible impact on general productivity or divergent thinking.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445062
ER  - 

TY  - JOUR
AU  - Ballesteros, Joaquin; Ayala, Inmaculada; Caro-Romero, Juan Rafael; Amor, Mercedes; Fuentes, Lidia
TI  - Evolving dynamic self-adaptation policies of mHealth systems for long-term monitoring.
PY  - 2020
AB  - Tele-rehabilitation can complement traditional rehabilitation therapies by providing valuable information that can help in the evaluation, monitoring, and treatment of patients. Many patient tele-monitoring systems that integrate wearable technology are emerging as an effective tool for the long-term surveillance of rehabilitation progression, enabling continuous sampling of patient real-time movement in a non-invasive way, without affecting the normal daily activity of the outpatient, who, therefore, will not need to make frequent clinic visits. One of the main challenges of tele-rehabilitation systems is to pay special attention to the diversity of dysfunctions in patients by offering devices with customized behaviours adaptable to the physical conditions of each patient at the different stages of the rehabilitation therapy. Long-term monitoring systems need an adaptation policy to autonomously reconfigure their behaviour according to vital signs read during the physical activity of the patient, the remaining battery level, or the required accuracy of collected data. However, it would alsobe desirable to adjust such adaptation policies over time, according to the patient's evolution. This work presents a wearable patient-monitoring system for tele-rehabilitation that is able to dynamically self-configure its internal behaviour to the current context of the outpatient according to a set of adaptation policies that optimize battery consumption, taking into account other QoS parameters at the same time. Our system is also able to self-adapt its internal adaptation policies as a patient's condition improves, while maintaining the system's efficiency. We illustrate our proposal with a real mHealth case study. The results of the experiments show that the system updates the adaptation policies, taking into account specific indicators of the disease. The validation results show that the evolution of the self-adaptation policies correlates with the progression of different patients.
SP  - 103494
EP  - NA
JF  - Journal of biomedical informatics
VL  - 108
IS  - NA
PB  - 
DO  - 10.1016/j.jbi.2020.103494
ER  - 

TY  - NA
AU  - Remy, Christian; Harboe, Gunnar; Frich, Jonas; Biskjaer, Michael Mose; Dalsgaard, Peter
TI  - ECCE - Challenges and Opportunities in the Design of Digital Distributed Affinity Diagramming Tools
PY  - 2021
AB  - Affinity diagramming is an oft-used sense-making technique in design research and practice to analyze qualitative data, utilizing a large number of sticky notes on walls. Over the past two decades, several digital tools have been tried and tested to augment or even replace the physical affinity diagramming process. Even so, the analog process usually prevails. We developed an online collaboration tool specifically tailored toward affinity diagramming to explore the challenges and opportunities of such a system in the particular case where the distributed teams do not have access to co-located settings. Here, we present a user experience study of five groups (dyads) of students based on a one-hour diagramming task under remote observation, followed by semi-structured interviews. Our study contributes three distinct insights to inform future work, namely that digital affinity diagrams 1) reduce the awareness of co-participants’ actions, 2) provide fewer cues about ownership and use than physical diagrams, and 3) save time, improve manipulation, and overview. We end with a discussion of the challenges and opportunities for the design of digital tools for distributed teams involved in sense-making tasks.
SP  - NA
EP  - NA
JF  - European Conference on Cognitive Ergonomics 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3452853.3452871
ER  - 

TY  - JOUR
AU  - Okamoto, Masahiro; Murao, Kazuya
TI  - Multi-Touch Interaction Generation Device by Spatiotemporally Switching Electrodes
PY  - 2021
AB  - With the spread of devices equipped with touch panels, such as smartphones, tablets, and laptops, the opportunity for users to perform touch interaction has increased. In this paper, we constructed a device that generates multi-touch interactions to realize high-speed, continuous, or hands-free touch input on a touch panel. The proposed device consists of an electrode sheet printed with multiple electrodes using conductive ink and a voltage control board, and generates eight multi-touch interactions: tap, double-tap, long-press, press-and-tap, swipe, pinch-in, pinch-out, and rotation, by changing the capacitance of the touch panel in time and space. In preliminary experiments, we investigated the appropriate electrode size and spacing for generating multi-touch interactions, and then implemented the device. From the evaluation experiments, it was confirmed that the proposed device can generate multi-touch interactions with high accuracy. As a result, tap, press-and-tap, swipe, pinch-in, pinch-out, and rotation can be generated with a success rate of 100%. It was confirmed that all the multi-touch interactions evaluated by the proposed device could be generated with high accuracy and acceptable speed.
SP  - 1475
EP  - NA
JF  - Electronics
VL  - 10
IS  - 12
PB  - 
DO  - 10.3390/electronics10121475
ER  - 

TY  - NA
AU  - Xiao, Robert; Benko, Hrvoje
TI  - CHI - Augmenting the Field-of-View of Head-Mounted Displays with Sparse Peripheral Displays
PY  - 2016
AB  - In this paper, we explore the concept of a sparse peripheral display, which augments the field-of-view of a head-mounted display with a lightweight, low-resolution, inexpensively produced array of LEDs surrounding the central high-resolution display. We show that sparse peripheral displays expand the available field-of-view up to 190o horizontal, nearly filling the human field-of-view. We prototyped two proof-of-concept implementations of sparse peripheral displays: a virtual reality headset, dubbed SparseLightVR, and an augmented reality headset, called SparseLightAR. Using SparseLightVR, we conducted a user study to evaluate the utility of our implementation, and a second user study to assess different visualization schemes in the periphery and their effect on simulator sickness. Our findings show that sparse peripheral displays are useful in conveying peripheral information and improving situational awareness, are generally preferred, and can help reduce motion sickness in nausea-susceptible people.
SP  - 1221
EP  - 1232
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858212
ER  - 

TY  - NA
AU  - Masson, Damien; Malacria, Sylvain; Lank, Edward; Casiez, Géry
TI  - CHI - Chameleon: Bringing Interactivity to Static Digital Documents
PY  - 2020
AB  - Documents such as presentations, instruction manuals, and research papers are disseminated using various file formats, many of which barely support the incorporation of interactive content. To address this lack of interactivity, we present Chameleon, a system-wide tool that combines computer vision algorithms used for image identification with an open database format to allow for the layering of dynamic content. Using Chameleon, static documents can be easily upgraded by layering user-generated interactive content on top of static images, all while preserving the original static document format and without modifying existing applications. We describe the development of Chameleon, including the design and evaluation of vision-based image replacement algorithms, the new document-creation pipeline as well as a user study evaluating Chameleon.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376559
ER  - 

TY  - NA
AU  - Han, Changyo; Matsui, Katsufumi; Naemura, Takeshi
TI  - Tangible and Embedded Interaction - ForceStamps: Fiducial Markers for Pressure-sensitive Touch Surfaces to Support Rapid Prototyping of Physical Control Interfaces
PY  - 2020
AB  - We present ForceStamps, fiducial markers for supporting rapid prototyping of physical control interfaces on pressure-sensitive touch surfaces. We investigate marker design options for supporting various physical controls, with focusing on creating dedicated footprints and maintaining the structural stability. ForceStamps can be persistently tracked on surfaces along with the force information and other attributes. Designers without knowledge of electronics can rapidly prototype physical controls by attaching mechanisms to ForceStamps, while manipulating the haptic feedback with buffer materials. The created control widgets can be spatially configured on the touch surface to make an interface layout. We showcase a variety of example controls created with ForceStamps. In addition, we report on our analysis of a two-day musical instrument design workshop to explore the affordances of ForceStamps for making novel instruments with diverse interaction designs.
SP  - 273
EP  - 285
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374924
ER  - 

TY  - NA
AU  - Clergeaud, Damien; Roo, Joan Sol; Hachet, Martin; Guitton, Pascal
TI  - VRST - Towards seamless interaction between physical and virtual locations for asymmetric collaboration
PY  - 2017
AB  - Virtual Reality allows rapid prototyping and simulation of physical artefacts, which would be difficult and expensive to perform otherwise. On the other hand, when the design process is complex and involves multiple stakeholders, decisions are taken in meetings hosted in the physical world. In the case of aerospace industrial designs, the process is accelerated by having asymmetric collaboration between the two locations: experts discuss the possibilities in a meeting room while a technician immersed in VR tests the selected alternatives. According to experts, the current approach is not without limitations, and in this work, we present prototypes designed to tackle them. The described artefacts were created to address the main issues: awareness of the remote location, remote interaction and manipulation, and navigation between locations. First feedback from experts regarding the prototypes is also presented. The resulting design considerations can be used in other asymmetric collaborative scenarios.
SP  - 17
EP  - NA
JF  - Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3139131.3139165
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Kim, Jeeeun
TI  - UIST - 3D Printed Fabric: Techniques for Design and 3D Weaving Programmable Textiles
PY  - 2019
AB  - We present a technique for fabricating soft and flexible textiles using a consumer grade fused deposition modeling (FDM) 3D printer. By controlling the movement of the print header, the FDM alternately weaves the stringing fibers across a row of pillars. Owing to the structure of the fibers, which supports and strengthens the pillars from each side, a 3D printer can print a thin sheet of fabric in an upright position while the fibers are being woven. In addition, this technique enables users to employ materials with various colors and/or properties when designing a pattern, and to prototype an interactive object using a variety of off-the-shelf materials such as a conductive filament. We also describe a technique for weaving textiles and introduce a list of parameters that enable users to design their own textile variations. Finally, we demonstrate examples showing the feasibility of our approach as well as numerous applications integrating printed textiles in a custom object design.
SP  - 43
EP  - 51
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347896
ER  - 

TY  - JOUR
AU  - Chang, Joonho; Moon, Seung Ki; Jung, Kihyo; Kim, Wonmo; Parkinson, Matthew B.; Freivalds, Andris; Simpson, Timothy W.; Baik, Seon Pill
TI  - Glasses-type wearable computer displays: usability considerations examined with a 3D glasses case study.
PY  - 2017
AB  - This study presents usability considerations and solutions for the design of glasses-type wearable computer displays and examines their effectiveness in a case study. Design countermeasures were in...
SP  - 670
EP  - 681
JF  - Ergonomics
VL  - 61
IS  - 5
PB  - 
DO  - 10.1080/00140139.2017.1401670
ER  - 

TY  - NA
AU  - Yasu, Kentaro
TI  - CHI - Magnetic Plotter: A Macrotexture Design Method Using Magnetic Rubber Sheets
PY  - 2017
AB  - This paper presents a method for designing tactile macrotextures with magnetic rubber sheets. In the method, named "Magnetic Plotter", a desktop digital plotting machine combined with a tiny neodymium magnet writes fine magnetic patterns on the surface of the magnetic rubber sheets. This method enables users to design magnetic fields freely with inexpensive commercially available materials as if they are drawing pictures. Moreover, when the magnetic sheets are rubbed together, unique haptic stimuli are displayed on the fingers. The haptic stimuli can be changed by the magnetic patterns designed on the rubber sheets. We developed a prototype of the Magnetic Plotter and investigated the range of the generated haptic stimuli and the texture design possibilities.
SP  - 4983
EP  - 4993
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025702
ER  - 

TY  - JOUR
AU  - Wu, Aoyu; Qu, Huamin
TI  - Multimodal Analysis of Video Collections: Visual Exploration of Presentation Techniques in TED Talks
PY  - 2018
AB  - While much research in the educational field has revealed many presentation techniques, they often overlap and are even occasionally contradictory. Exploring presentation techniques used in TED Talks could provide evidence for a practical guideline. This study aims to explore the verbal and non-verbal presentation techniques from a collection of TED Talks. However, such analysis is challenging due to the difficulties of analyzing multimodal video collections consisted of frame images, text, and metadata. This paper proposes a visual analytic system to analyze multimodal content in video collections. The system features three views at different levels: the Projection view with novel glyphs to facilitate cluster analysis regarding presentation styles; the Comparison View to present temporal distribution and concurrences of presentation techniques and support intra-cluster analysis; and the Video View to enable contextualized exploration of a video. We conduct a case study with language education experts and university students to provide anecdotal evidence about the effectiveness of our approach, and report new findings about presentation techniques in TED Talks. Quantitative feedback from a user study confirms the usefulness of our visual system for multimodal analysis of video collections.
SP  - 2429
EP  - 2442
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2018.2889081
ER  - 

TY  - NA
AU  - Narazani, Marla; Eghtebas, Chloe; Jenney, Sarah Louise; Mühlhaus, Michael
TI  - UbiComp/ISWC Adjunct - Tangible urban models: two-way interaction through 3D printed conductive tangibles and AR for urban planning
PY  - 2019
AB  - Physical models are a key component in the architectural process and play an important role in understanding material and space relationships. We present Tangible Urban Models, an approach for leveraging the use of conductive material for 3D printed architectural prototypes. This enables non-interactive objects, such as buildings, to become tangible without the need to attach additional components. We combine this capability with an augmented reality (AR) app and explore the use of gestures for interacting with digital and physical content. The multi-material 3D printed buildings consist of thin layers of white plastic filament and a conductive wireframe to enable touch gestures. In this way, we enable a two-way interaction either with the physical model or with the mobile AR interface.
SP  - 320
EP  - 323
JF  - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341162.3343810
ER  - 

TY  - NA
AU  - Li, Jingyi; Hashim, Sonia; Jacobs, Jennifer
TI  - CHI - What We Can Learn From Visual Artists About Software Development
PY  - 2021
AB  - This paper explores software’s role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists’ experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445682
ER  - 

TY  - NA
AU  - Sun, Zhida; Sun, Mingfei; Cao, Nan; Ma, Xiaojuan
TI  - SIGGRAPH Asia Symposium on Visualization - VideoForest: interactive visual summarization of video streams based on danmu data
PY  - 2016
AB  - Emerging online video-sharing websites such as YouTube allow users to access a huge number of videos and generate feedback via reviews and/or live comments (a.k.a. danmu), making it possible to summarize videos based on media and user responses collectively. A video summary produced by existing techniques may not fully capture the audience's perception of and reaction to the source video, and thus may be less reflective. In this paper, we introduce VideoForest, a visualization system designed to convert an input video augmented with danmu commentary data into a tree-like visual summary of content highlights under users' supervision. The proposed visualization design employs a forest metaphor. The overall summary of different video sessions is illustrated as scene trees on top of the session timeline ground, with the roots depicting the corresponding danmu messages. VideoForest can also generate a detailed synopsis of the user-selected video segment(s) as a compact storyline in the form of circle packing. We evaluate our system via case studies with real video data based on experts' feedback. The results suggest the power and potential of the system.
SP  - 10
EP  - NA
JF  - SIGGRAPH ASIA 2016 Symposium on Visualization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3002151.3002159
ER  - 

TY  - NA
AU  - Hatley, Jonathan; Posner, Noah; Oh, Hyunjoo; Leigh, Sang-won
TI  - Mixed Dimensions: Exploring Novel Material and Extrusion Tool for 2.5D Shape Painting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501318
ER  - 

TY  - JOUR
AU  - Hládek, Ľuboš; Porr, Bernd; Brimijoin, W. Owen
TI  - Real-time estimation of horizontal gaze angle by saccade integration using in-ear electrooculography.
PY  - 2018
AB  - The manuscript proposes and evaluates a real-time algorithm for estimating eye gaze angle based solely on single-channel electrooculography (EOG), which can be obtained directly from the ear canal using conductive ear moulds. In contrast to conventional high-pass filtering, we used an algorithm that calculates absolute eye gaze angle via statistical analysis of detected saccades. The estimated eye positions of the new algorithm were still noisy. However, the performance in terms of Pearson product-moment correlation coefficients was significantly better than the conventional approach in some instances. The results suggest that in-ear EOG signals captured with conductive ear moulds could serve as a basis for light-weight and portable horizontal eye gaze angle estimation suitable for a broad range of applications. For instance, for hearing aids to steer the directivity of microphones in the direction of the user’s eye gaze.
SP  - e0190420
EP  - NA
JF  - PloS one
VL  - 13
IS  - 1
PB  - 
DO  - 10.1371/journal.pone.0190420
ER  - 

TY  - JOUR
AU  - Neate, Timothy; Jones, Matt; Evans, Michael
TI  - Cross-device media: a review of second screening and multi-device television
PY  - 2017
AB  - Television viewers interacting with second screens has become a common sight in the modern living room. Such activities are a mixture of related, semi-related, and non-related browsing of content. This growing trend is revolutionising the way that broadcasters think about their content. Through the envisioned connected home, driven by end-to-end IP connected networks, television content creators and app developers are now considering the design space for multi-device, interactive experiences. In this review paper, we consider the pre-digital beginnings of such scenarios and progress to discuss how the introduction of mobile devices has affected the TV viewing experience. We discuss dual-screen usage over a variety of contexts in the connected home, with a focus on `designed' dual-screen experiences such as companion applications. We conclude with reflections on the future of this area so that app developers, broadcasters, and academics may push further the space and improve future dual- and multi-screen experiences.
SP  - 391
EP  - 405
JF  - Personal and Ubiquitous Computing
VL  - 21
IS  - 2
PB  - 
DO  - 10.1007/s00779-017-1016-2
ER  - 

TY  - JOUR
AU  - Bruno, Alessandro; Gugliuzza, Francesco; Pirrone, Roberto; Ardizzone, Edoardo
TI  - A Multi-Scale Colour and Keypoint Density-Based Approach for Visual Saliency Detection
PY  - 2020
AB  - In the first seconds of observation of an image, several visual attention processes are involved in the identification of the visual targets that pop-out from the scene to our eyes. Saliency is the quality that makes certain regions of an image stand out from the visual field and grab our attention. Saliency detection models, inspired by visual cortex mechanisms, employ both colour and luminance features. Furthermore, both locations of pixels and presence of objects influence the Visual Attention processes. In this paper, we propose a new saliency method based on the combination of the distribution of interest points in the image with multiscale analysis, a centre bias module and a machine learning approach. We use perceptually uniform colour spaces to study how colour impacts on the extraction of saliency. To investigate eye-movements and assess the performances of saliency methods over object-based images, we conduct experimental sessions on our dataset ETTO (Eye Tracking Through Objects). Experiments show our approach to be accurate in the detection of saliency concerning state-of-the-art methods and accessible eye-movement datasets. The performances over object-based images are excellent and consistent on generic pictures. Besides, our work reveals interesting findings on some relationships between saliency and perceptually uniform colour spaces.
SP  - 121330
EP  - 121343
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.3006700
ER  - 

TY  - NA
AU  - Saidi, Houssem; Dubois, Emmanuel; Serrano, Marcos
TI  - CHI - HoloBar: Rapid Command Execution for Head-Worn AR Exploiting Around the Field-of-View Interaction
PY  - 2021
AB  - Inefficient menu interfaces lead to system and application commands being tedious to execute in Immersive Environments. HoloBar is a novel approach to ease the interaction with multi-level menus in immersive environments: with HoloBar, the hierarchical menu splits between the field of view (FoV) of the Head Mounted Display and the smartphone (SP). Command execution is based on around-the-FoV interaction with the SP, and touch input on the SP display. The HoloBar offers a unique combination of features, namely rapid mid-air activation, implicit selection of top-level items and preview of second-level items on the SP, ensuring rapid access to commands. In a first study we validate its activation method, which consists in bringing the SP within an activation distance from the FoV. In a second study, we compare the HoloBar to two alternatives, including the standard HoloLens menu. Results show that the HoloBar shortens each step of a multi-level menu interaction (menu activation, top-level item selection, second-level item selection and validation), with a high success rate. A follow-up study confirms that these results remain valid when compared with the two validation mechanisms of HoloLens (Air-Tap and clicker).
SP  - 1
EP  - 17
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445255
ER  - 

TY  - JOUR
AU  - Liu, Meng; Li, Youfu; Liu, Hai
TI  - 3D Gaze Estimation for Head-Mounted Eye Tracking System with Auto-Calibration Method
PY  - 2020
AB  - The general challenges of 3D gaze estimation for head-mounted eye tracking systems are inflexible marker-based calibration procedure and significant errors of depth estimation. In this paper, we propose a 3D gaze estimation with an auto-calibration method. To acquire the accurate 3D structure of the environment, an RGBD camera is applied as the scene camera of our system. By adopting the saliency detection method, saliency maps can be acquired through scene images, and 3D salient pixels in the scene are considered potential 3D calibration targets. The 3D eye model is built on the basis of eye images to determine gaze vectors. By combining 3D salient pixels and gaze vectors, the auto-calibration can be achieved with our calibration method. Finally, the 3D gaze point is obtained through the calibrated gaze vectors, and the point cloud is generated from the RGBD camera. The experimental result shows that the proposed system can achieve an average accuracy of 3.7° in the range of 1 m to 4 m indoors and 4.0° outdoors. The proposed system also presents a great improvement in depth measurement, which is sufficient for tracking users' visual attention in real scenes.
SP  - 104207
EP  - 104215
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.2999633
ER  - 

TY  - NA
AU  - Li, Jingyi; Hashim, Sonia; Jacobs, Jennifer
TI  - What We Can Learn From Visual Artists About Software Development
PY  - 2021
AB  - This paper explores software's role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists' experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Borowski, Marcel; Murray, Luke; Bagge, Rolf; Kristensen, Janus Bager; Satyanarayan, Arvind; Klokmose, Clemens Nylandsted
TI  - Varv: Reprogrammable Interactive Software as a Declarative Data Structure
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502064
ER  - 

TY  - JOUR
AU  - Xiang, Siyuan; Wang, Ruoyu; Feng, Chen
TI  - Mobile projective augmented reality for collaborative robots in construction
PY  - 2021
AB  - NA
SP  - 103704
EP  - NA
JF  - Automation in Construction
VL  - 127
IS  - NA
PB  - 
DO  - 10.1016/j.autcon.2021.103704
ER  - 

TY  - NA
AU  - Zagermann, Johannes; Pfeil, Ulrike; von Bauer, Philipp; Fink, Daniel; Reiterer, Harald
TI  - CHI - "It's in my other hand!" Studying the Interplay of Interaction Techniques and Multi-Tablet Activities
PY  - 2020
AB  - Cross-device interaction with tablets is a popular topic in HCI research. Recent work has shown the benefits of including multiple devices into users' workflows while various interaction techniques allow transferring content across devices. However, users are only reluctantly using multiple devices in combination. At the same time, research on cross-device interaction struggles to find a frame of reference to compare techniques or systems. In this paper, we try to address these challenges by studying the interplay of interaction techniques, device utilization, and task-specific activities in a user study with 24 participants from different but complementary angles of evaluation using an abstract task, a sensemaking task, and three interaction techniques. We found that different interaction techniques have a lower influence than expected, that work behaviors and device utilization depend on the task at hand, and that participants value specific aspects of cross-device interaction.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376540
ER  - 

TY  - NA
AU  - Roo, Joan Sol; Hachet, Martin
TI  - 3DUI - Towards a hybrid space combining Spatial Augmented Reality and virtual reality
PY  - 2017
AB  - Spatial Augmented Reality (SAR) allows a user, or a group of users, to benefit from digital augmentations embedded directly into the physical world. This enables co-located information and unobstructed interaction. On the other hand, SAR suffers from limitations that are inherently linked to its physical dependency, which is not the case for see-through or immersive displays. In this work, we explore how to facilitate the transition from SAR to VR, and vice versa, integrating both into a unified experience. We developed a set of interaction techniques and obtained first feedback from informal interviews.
SP  - 195
EP  - 198
JF  - 2017 IEEE Symposium on 3D User Interfaces (3DUI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/3dui.2017.7893339
ER  - 

TY  - NA
AU  - Sydow, Sophie Landwehr; Jonsson, Martin; Tholander, Jakob
TI  - NordiCHI - Machine Sensibility: Unpacking the Embodied and Situated Dimensions of 3D Printing
PY  - 2020
AB  - This paper offers a conceptual contribution to understand 3D printing practice. We have studied conversations between 3D printing practitioners who discuss failed and discarded printed artifacts and analyzed how they make sense of the printing process. Based on findings of interactions with the machine itself, materials used, and designs applied, this study contributes to the field of HCI by highlighting the embodied and situated dimensions of 3D printing. Introducing the concept of machine sensibility, we characterize our findings around: i) assessing printability, ii) monitoring and intervening and iii) reading the prints. We use the term machine to highlight the importance of understanding the materiality of the 3D printer, and sensibility, to address critical interactions and abilities that surfaced in studying this practice. The concept allows researchers to put 3D printing practice in the context of contemporary interaction design research and helps to understand challenges of material-machine-design interdependencies.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3419249.3420166
ER  - 

TY  - BOOK
AU  - Dierkes, Kai; Kassner, Moritz; Bulling, Andreas
TI  - ETRA - A novel approach to single camera, glint-free 3D eye model fitting including corneal refraction
PY  - 2018
AB  - Model-based methods for glint-free gaze estimation typically infer eye pose using pupil contours extracted from eye images. Existing methods, however, either ignore or require complex hardware setups to deal with refraction effects occurring at the corneal interfaces. In this work we provide a detailed analysis of the effects of refraction in glint-free gaze estimation using a single near-eye camera, based on the method presented by [Swirski and Dodgson 2013]. We demonstrate systematic deviations in inferred eyeball positions and gaze directions with respect to synthetic ground-truth data and show that ignoring corneal refraction can result in angular errors of several degrees. Furthermore, we quantify gaze direction dependent errors in pupil radius estimates. We propose a novel approach to account for corneal refraction in 3D eye model fitting and by analyzing synthetic and real images show that our new method successfully captures refraction effects and helps to overcome the shortcomings of the state of the art approach.
SP  - 9
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204525
ER  - 

TY  - NA
AU  - Günther, Sebastian; Müller, Florian; Schmitz, Martin; Riemann, Jan; Dezfuli, Niloofar; Funk, Markus; Schön, Dominik; Mühlhäuser, Max
TI  - CHI Extended Abstracts - CheckMate: Exploring a Tangible Augmented Reality Interface for Remote Interaction
PY  - 2018
AB  - The digitalized world comes with increasing Internet capabilities, allowing to connect persons over distance easier than ever before. Video conferencing and similar online applications create great benefits bringing people who physically cannot spend as much time as they want virtually together. However, such remote experiences can also tend to lose the feeling of traditional experiences. People lack direct visual presence and no haptic feedback is available. In this paper, we tackle this problem by introducing our system called CheckMate. We combine Augmented Reality and capacitive 3D printed objects that can be sensed on an interactive surface to enable remote interaction while providing the same tangible experience as in co-located scenarios. As a proof-of-concept, we implemented a sample application based on the traditional chess game.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188647
ER  - 

TY  - CHAP
AU  - Park, Seonwook; Aksan, Emre; Zhang, Xucong; Hilliges, Otmar
TI  - ECCV (12) - Towards End-to-end Video-based Eye-Tracking
PY  - 2020
AB  - Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user’s eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to \(28\%\) improvement in Point-of-Gaze estimates (resulting in \(2.49^\circ \) in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE.
SP  - 747
EP  - 763
JF  - Computer Vision – ECCV 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58610-2_44
ER  - 

TY  - JOUR
AU  - Yaman, Ulas; Dolen, Melik; Hoffmann, Christoph M.
TI  - Generation of patterned indentations for additive manufacturing technologies
PY  - 2018
AB  - This article proposes a novel approach to generate patterned indentations for different additive manufacturing methodologies. Surface textures have many practical applications in various fields, bu...
SP  - 209
EP  - 217
JF  - IISE Transactions
VL  - 51
IS  - 2
PB  - 
DO  - 10.1080/24725854.2018.1491076
ER  - 

TY  - BOOK
AU  - Günther, Sebastian; Schmitz, Martin; Müller, Florian; Riemann, Jan; Mühlhäuser, Max
TI  - SmartObject@IUI - BYO*: Utilizing 3D Printed Tangible Tools for Interaction on Interactive Surfaces
PY  - 2017
AB  - Sharing and manipulating information are essential for collaborative work in meeting scenarios. Nowadays, people tend to bring their own devices as a result of increasing mobility possibilities. However, transferring data from one device to another can be cumbersome and tedious if restrictions like different platforms, form factors or environmental limitations apply. In this paper, we present two concepts to enrich interaction on and between devices through 3D printed customized tangibles: 1) Bring your own information, and 2) bring your own tools. For this, we enable interactivity for low-cost and passive tangible 3D printed objects by adding conductive material and make use of touch-enabled surfaces. Our system allows users to easily share digital contents across various devices and to manipulate them with individually designed tools without additional hardware required.
SP  - 21
EP  - 26
JF  - Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3038450.3038456
ER  - 

TY  - JOUR
AU  - Schwab, Michail; Saffo, David; Zhang, Yixuan; Sinha, Shash; Nita-Rotaru, Cristina; Tompkin, James; Dunne, Cody; Borkin, Michelle A.
TI  - VisConnect: Distributed Event Synchronization for Collaborative Visualization
PY  - 2021
AB  - Tools and interfaces are increasingly expected to be synchronous and distributed to accommodate remote collaboration. Yet, adoption of these techniques for data visualization is low partly because development is difficult: existing collaboration software systems either do not support simultaneous interaction or require expensive redevelopment of existing visualizations. We contribute VisConnect: a web-based synchronous distributed collaborative visualization system that supports most web-based SVG data visualizations, balances system safety with responsiveness, and supports simultaneous interaction from many collaborators. VisConnect works with existing visualization implementations with little-to-no code changes by synchronizing low-level JavaScript events across clients such that visualization updates proceed transparently across clients. This is accomplished via a peer-to-peer system that establishes consensus among clients on the per-element sequence of events, and uses a lock service to grant access over elements to clients. We contribute collaborative extensions of traditional visualization interaction techniques, such as drag, brush, and lasso, and discuss different strategies for collaborative visualization interactions. To demonstrate the utility of VisConnect, we present novel examples of collaborative visualizations in the healthcare domain, remote collaboration with annotation, and show in an education case study for e-learning with 22 participants that students found the ability to remotely collaborate on class activities helpful and enjoyable for understanding concepts. A free copy of this paper and source code are available on OSF at osf.io/ut7e6 and at visconnect.us.
SP  - 347
EP  - 357
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030366
ER  - 

TY  - NA
AU  - Gröschel, Carla; Dalsgaard, Peter; Klokmose, Clemens Nylandsted; Korsgaard, Henrik; Eriksson, Eva; Bats, Raphaëlle; Tabard, Aurélien; Ducros, Alix; Serholt, Sofia
TI  - CHI Extended Abstracts - PARTICIPATE: Capturing Knowledge in Public Library Activities
PY  - 2018
AB  - We present PARTICIPATE, a technology probe exploring how to strengthen the connection between activities taking place at public libraries and their collections, both in the digital realm and in the physical space. Based on ethnographic studies and participatory design activities, we derive three core implications for place- and activity centric library services. These implications led us to design PARTICIPATE in collaboration with library staff from three European countries. The probe is a mean to investigate how place- and activity-centric digital services in the library space can engage participants in co-creating knowledge, and enable libraries to integrate activities with library collections.
SP  - 1
EP  - 6
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188605
ER  - 

TY  - CHAP
AU  - Dong, Chen; Li, Bin; Yang, Li; Geng, Liang; Zhang, Fengjun
TI  - AUIF: An Adaptive User Interface Framework for Multiple Devices
PY  - 2020
AB  - Nowadays, it is becoming increasingly common for users to use multiple devices in their daily lives. Smartphones, tablets, laptops and other devices constitute a multi-device environment for users. Making full use of the multi-device environments so that users can work conveniently and efficiently has been a challenging problem in the field of user interface (UI) research. Here, we present AUIF, a web-based application framework for dynamically creating distributed user interfaces (DUIs) across multiple devices. We propose multi-device collaboration patterns and UI components allocation algorithm and combine them in our implementation. A prototype system based on AUIF is designed as an illustration of concept. Finally, user experiments based on the prototype system are carried out, and the result verifies the effectiveness of AUIF in actual use.
SP  - 248
EP  - 263
JF  - Image and Graphics Technologies and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-33-6033-4_19
ER  - 

TY  - JOUR
AU  - Liu, Jiali; Eagan, James
TI  - ADQDA: A Cross-device Affinity Diagramming Tool for Fluid and Holistic Qualitative Data Analysis
PY  - 2021
AB  - Affinity diagramming is widely applied to analyze qualitative data such as interview transcripts. It involves multiple analytic processes and is often performed collaboratively. Drawing on interviews with three practitioners and upon our own experience, we show how practitioners combine multiple analytic processes and adopt different artifacts to help them analyze their data. Current tools, however, fail to adequately support mixing analytic processes, devices, and collaboration styles. We present a vision and prototype ADQDA, a cross-device, collaborative affinity diagramming tool for qualitative data analysis, implemented using distributed web technologies. We show how this approach enables analysts to appropriate available pertinent digital devices as they fluidly migrate between analytic phases or adopt different methods and representations, all while preserving consistent analysis artifacts. We validate this approach through a set of application scenarios that explore how it enables new ways of analyzing qualitative data that better align with identified analytic practices.
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3488534
ER  - 

TY  - NA
AU  - Degraen, Donald; Zenner, André; Krüger, Antonio
TI  - CHI - Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures
PY  - 2019
AB  - Experiencing materials in virtual reality (VR) is enhanced by combining visual and haptic feedback. While VR easily allows changes to visual appearances, modifying haptic impressions remains challenging. Existing passive haptic techniques require access to a large set of tangible proxies. To reduce the number of physical representations, we look towards fabrication to create more versatile counterparts. In a user study, 3D-printed hairs with length varying in steps of 2.5 mm were used to influence the feeling of roughness and hardness. By overlaying fabricated hair with visual textures, the resolution of the user's haptic perception increased. As changing haptic sensations are able to elicit perceptual switches, our approach can extend a limited set of textures to a much broader set of material impressions. Our results give insights into the effectiveness of 3D-printed hair for enhancing texture perception in VR.
SP  - 249
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300479
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Miyashita, Homei
TI  - CHI - Expressive Fused Deposition Modeling by Controlling Extruder Height and Extrusion Amount
PY  - 2017
AB  - Fused deposition modeling (FDM) 3D printers form objects by stacking layers having a linear structure. To print fine structures, an appropriate choice of parameters is necessary, or printing error occurs. On the other hand, the printing error is exploited as an expression technique. However, the relation between the printed structure and the parameters causing the printing error is unclear. In this paper, we focus on the height position of the extruder and the amount of extruded material, and explore the combination of these parameters to enhance the capability of FDM. By extending an equation that calculates the amount of material from the layer height, we investigate the behavior and structure of material extruded from various height positions. On the basis of experimental results, the printed structure is classified into six categories according to the structural feature. We describe these structural features and demonstrate examples with new inherent expressions for FDM.
SP  - 5065
EP  - 5074
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025933
ER  - 

TY  - JOUR
AU  - Lambrichts, Mannu; Ramakers, Raf; Hodges, Steve; Coppers, Sven; Devine, James
TI  - A Survey and Taxonomy of Electronics Toolkits for Interactive and Ubiquitous Device Prototyping
PY  - 2021
AB  - Over the past two decades, many toolkits for prototyping interactive and ubiquitous electronic devices have been developed. Although their technical specifications are often easy to look up, they vary greatly in terms of design, features and target audience, resulting in very real strengths and weaknesses depending on the intended application. These less technical characteristics are often reported inconsistently, if at all. In this paper we provide a comprehensive survey of interactive and ubiquitous device prototyping toolkits, systematically analysing their characteristics within the framework of a new taxonomy that we present. In addition to the specific characteristics we cover, we introduce a way to evaluate toolkits more holistically, covering user needs such as 'ease of construction' and 'ease of moving from prototype to product' rather than features. We also present results from an online survey which offers new insights on how the surveyed users prioritize these characteristics during prototyping, and what techniques they use to move beyond prototyping. We hope our analysis will be valuable for others in the community who need to build and potentially scale out prototypes as part of their research. We end by identifying gaps that have not yet been addressed by existing offerings and discuss opportunities for future research into electronics prototyping toolkits.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463523
ER  - 

TY  - JOUR
AU  - Fuhl, Wolfgang; Tonsen, Marc; Bulling, Andreas; Kasneci, Enkelejda
TI  - Pupil detection for head-mounted eye tracking in the wild: an evaluation of the state of the art
PY  - 2016
AB  - Robust and accurate detection of the pupil position is a key building block for head-mounted eye tracking and prerequisite for applications on top, such as gaze-based human---computer interaction or attention analysis. Despite a large body of work, detecting the pupil in images recorded under real-world conditions is challenging given significant variability in the eye appearance (e.g., illumination, reflections, occlusions, etc.), individual differences in eye physiology, as well as other sources of noise, such as contact lenses or make-up. In this paper we review six state-of-the-art pupil detection methods, namely ElSe (Fuhl et al. in Proceedings of the ninth biennial ACM symposium on eye tracking research & applications, ACM. New York, NY, USA, pp 123---130, 2016), ExCuSe (Fuhl et al. in Computer analysis of images and patterns. Springer, New York, pp 39---51, 2015), Pupil Labs (Kassner et al. in Adjunct proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing (UbiComp), pp 1151---1160, 2014. doi:10.1145/2638728.2641695), SET (Javadi et al. in Front Neuroeng 8, 2015), Starburst (Li et al. in Computer vision and pattern recognition-workshops, 2005. IEEE Computer society conference on CVPR workshops. IEEE, pp 79---79, 2005), and źwirski (źwirski et al. in Proceedings of the symposium on eye tracking research and applications (ETRA). ACM, pp 173---176, 2012. doi:10.1145/2168556.2168585). We compare their performance on a large-scale data set consisting of 225,569 annotated eye images taken from four publicly available data sets. Our experimental results show that the algorithm ElSe (Fuhl et al. 2016) outperforms other pupil detection methods by a large margin, offering thus robust and accurate pupil positions on challenging everyday eye images.
SP  - 1275
EP  - 1288
JF  - Machine Vision and Applications
VL  - 27
IS  - 8
PB  - 
DO  - 10.1007/s00138-016-0776-4
ER  - 

TY  - JOUR
AU  - Drewes, Jan; Feder, Sascha; Einhäuser, Wolfgang
TI  - Gaze During Locomotion in Virtual Reality and the Real World.
PY  - 2021
AB  - How vision guides gaze in realistic settings has been researched for decades. Human gaze behavior is typically measured in laboratory settings that are well controlled but feature-reduced and movement-constrained, in sharp contrast to real-life gaze control that combines eye, head, and body movements. Previous real-world research has shown environmental factors such as terrain difficulty to affect gaze; however, real-world settings are difficult to control or replicate. Virtual reality (VR) offers the experimental control of a laboratory, yet approximates freedom and visual complexity of the real world (RW). We measured gaze data in 8 healthy young adults during walking in the RW and simulated locomotion in VR. Participants walked along a pre-defined path inside an office building, which included different terrains such as long corridors and flights of stairs. In VR, participants followed the same path in a detailed virtual reconstruction of the building. We devised a novel hybrid control strategy for movement in VR: participants did not actually translate: forward movements were controlled by a hand-held device, rotational movements were executed physically and transferred to the VR. We found significant effects of terrain type (flat corridor, staircase up, and staircase down) on gaze direction, on the spatial spread of gaze direction, and on the angular distribution of gaze-direction changes. The factor world (RW and VR) affected the angular distribution of gaze-direction changes, saccade frequency, and head-centered vertical gaze direction. The latter effect vanished when referencing gaze to a world-fixed coordinate system, and was likely due to specifics of headset placement, which cannot confound any other analyzed measure. Importantly, we did not observe a significant interaction between the factors world and terrain for any of the tested measures. This indicates that differences between terrain types are not modulated by the world. The overall dwell time on navigational markers did not differ between worlds. The similar dependence of gaze behavior on terrain in the RW and in VR indicates that our VR captures real-world constraints remarkably well. High-fidelity VR combined with naturalistic movement control therefore has the potential to narrow the gap between the experimental control of a lab and ecologically valid settings.
SP  - 656913
EP  - 656913
JF  - Frontiers in neuroscience
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnins.2021.656913
ER  - 

TY  - JOUR
AU  - Karolus, Jakob; Kiss, Francisco; Eckerth, Caroline; Viot, Nicolas; Bachmann, Felix; Schmidt, Albrecht; Wozniak, Pawel W.
TI  - EMBody: A Data-Centric Toolkit for EMG-Based Interface Prototyping and Experimentation
PY  - 2021
AB  - Body movements, from a short smile to a marathon run, are driven by muscle activity. Despite the fact that measuring muscle activity with electromyography (EMG) is technically well established, it is highly complex and its use in interfaces has been limited. Easy access to muscle sensing can offer new opportunities to Human-Computer Interaction (HCI) research. Off-the-shelf sensors often only provide low-level access, hence requiring expertise in signal processing and widening the gulf of execution for users without engineering skills. To address this challenge, we introduce EMBody, a data-centric toolkit for EMG-based interface prototyping and experimentation. EMBody offers multiple levels of prototyping fidelity for EMG sensing, signal processing, and data interpretation. Our data-centric toolkit encapsulates the different data representation stages, offering a wide range of customization opportunities to experts while also allowing non-technical designers to focus on creating new interaction techniques. EMBody features a lightweight form factor and wireless connectivity. Additionally, the system leverages an exploration-centered workflow by allowing rapid access to measurement data via the accompanying software. Users define a set of motions to be recognized and interactively provide example data points. The toolkit then handles signal processing and classification. The recognized movements are streamed on the local network, ready to be used by interactive applications. This paper reports on how to use EMBody and its implementation. We iteratively developed the toolkit in a series of workshops and example applications. Users who had none or very limited knowledge of EMG could rapidly create engaging functional prototypes, while experts appreciated the modularity of the software component allowing for a high degree of customization. We contribute the software and hardware components of EMBody as a tool for the research community to stimulate creative exploration of EMG systems.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3457142
ER  - 

TY  - NA
AU  - Putze, Felix; Küster, Dennis; Urban, Timo; Zastrow, Alexander; Kampen, Marvin
TI  - ICMI - Attention Sensing through Multimodal User Modeling in an Augmented Reality Guessing Game
PY  - 2020
AB  - We developed an attention-sensitive system that is capable of playing the children's guessing game "I spy with my litte eye" with a human user. In this game, the user selects an object from a given scene and provides the system with a single-sentence clue about it. For each trial, the system tries to guess the target object. Our approach combines top-down and bottom-up machine learning for object and color detection, automatic speech recognition, natural language processing, a semantic database, eye tracking, and augmented reality. Our evaluation demonstrates performance significantly above chance level, and results for most of the individual machine learning components are encouraging. Participants reported very high levels of satisfaction and curiosity about the system. The collected data shows that our guessing game generates a complex and rich data set. We discuss the capabilities and challenges of our system and its components with respect to multimodal attention sensing.
SP  - 33
EP  - 40
JF  - Proceedings of the 2020 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3382507.3418865
ER  - 

TY  - NA
AU  - Zheng, Clement; Kim, Jeeeun; Leithinger, Daniel; Gross, Mark D.; Yi-Luen, Ellen
TI  - Tangible and Embedded Interaction - Mechamagnets: Designing and Fabricating Haptic and Functional Physical Inputs with Embedded Magnets
PY  - 2019
AB  - We present Mechamagnets, a technique for facilitating the design and fabrication of haptic and functional inputs for physical interfaces. This technique consists of a set of 3D printed spatial constraints which facilitate different physical movements, as well as unpowered haptic profiles created by embedding static magnets in 3D printed parts. We propose the Mechamagnets taxonomy to map the design space of this technique for designers and makers. Furthermore, we leverage the use of magnets by instrumenting these objects with linear Hall effect sensors to create functional digital inputs. We showcase Mechamagnets with a series of novel physical interfaces made with this technique.
SP  - 325
EP  - 334
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295622
ER  - 

TY  - JOUR
AU  - Johnston, William; Sharma, Bhisham
TI  - Additive Manufacturing of Fibrous Sound Absorbers
PY  - 2021
AB  - We investigate the possibility of additively manufacturing fibrous sound absorbers using fused deposition modeling. Two methods for 3D printing fibers are proposed. The fiber bridging method involves extruding the filament between two points with no underlying supports. The extrude-and-pull method involves extruding a filament droplet before pulling away the extruder rapidly to generate thin fibers. Both methods can produce fibers with aspect ratios greater than 100. Optical microscopy is used to investigate the effect of various printing parameters on the fiber characteristics. The sound absorption coefficient of samples printed using the two techniques are measured using a two-microphone normal incidence impedance tube setup. Effects of printing parameters and fiber density variables are experimentally studied. The experimental studies are supported by the Johnson-Champoux-Allard semi-empirical analytical model informed using an inverse characterization approach. The analytical model is then utilized to understand the effect of fiber parameters on the acoustical transport parameters. It is observed that the two methods result in individual fibers with distinct characteristics. On average, the fiber bridging method results in thicker fibers, which results in comparatively higher sound absorption. However, the extrude-and-pull method results in fibers with hair-like characteristics (thick base with progressively decreasing thickness) and one may easily incorporate it within existing additive manufacturing routines to add fibers to a base surface, thus opening up a new route towards fiber-enhanced multifunctional structures.
SP  - 101984
EP  - NA
JF  - Additive Manufacturing
VL  - 41
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2021.101984
ER  - 

TY  - BOOK
AU  - Tejada, Carlos E.; Fujimoto, Osamu; Li, Zhiyuan; Ashbrook, Daniel
TI  - SmartObjects@CHI - Enabling the Fabrication of Smart Devices.
PY  - 2018
AB  - NA
SP  - 1
EP  - 8
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Koskela, Matias; Viitanen, Timo; Jääskeläinen, Pekka; Takala, Jarmo
TI  - ISVC (1) - Foveated Path Tracing
PY  - 2016
AB  - Virtual Reality (VR) places demanding requirements on the rendering pipeline: the rendering is stereoscopic and the refresh rate should be as high as 95 Hz to make VR immersive. One promising technique for making the final push to meet these requirements is foveated rendering, where the rendering effort is prioritized on the areas where the user’s gaze lies. This requires rapid adjustment of level of detail based on screen space coordinates. Path tracing allows this kind of changes without much extra work. However, real-time path tracing is fairly new concept. This paper is a literature review of techniques related to optimizing path tracing with foveated rendering. In addition, we provide a theoretical estimation of performance gains available and calculate that 94% of the paths could be omitted. For this reason we predict that path tracing can soon meet the demanding rendering requirements of VR.
SP  - 723
EP  - 732
JF  - Advances in Visual Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-50835-1_65
ER  - 

TY  - NA
AU  - Gadea, Cristian; Ionescu, Bogdan; Ionescu, Dan
TI  - A Hybrid FSM Rule-Based Approach for the Real-Time Control of Web-Based Collaborative Platforms
PY  - 2018
AB  - Web-based collaborative platforms and co-editing tools are now vital components of modern organizations seeking to enhance team productivity and efficiency. Although the underlying optimistic consistency maintenance techniques for keeping content synchronized across multiple sites have been studied by researchers since the eighties, no investigations currently exist that apply the principles of control theory and Finite State Machines (FSMs) to model the real-time distributed system behavior. Such modeling is important to enable the next generation of collaborative web-based experiences, which can increasingly benefit from real-time synchronization of complex hierarchical content such as HTML. So far no approaches can preserve the user's intentions when capturing any change made to the HTML Document Object Model (DOM) in a way efficient for sending and replaying on another browser. What is needed is a way of observing and encoding all possible changes made to a web page's DOM in such a way that they can be transmitted to other users as a feedback signal via a central server. This paper will apply a real-time feedback control mechanism to develop a Web-Based Collaborative Platform based on a new Operation Transformation (OT) algorithm. Operations are deduced by executing a series of rules and by using a Virtual DOM (VDOM) to ensure that operations can be re-integrated by receiving users, where transformation rules are applied. The feedback mechanisms are demonstrated by modeling and simulating the components using FSMs, which are optimally combined with the Rule-Based approach such that real-time deadlines are observed.
SP  - 000027
EP  - 000032
JF  - 2018 IEEE 22nd International Conference on Intelligent Engineering Systems (INES)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ines.2018.8523986
ER  - 

TY  - BOOK
AU  - Basman, Antranig; Church, Luke; Klokmose, Clemens Nylandsted; Clark, Colin B. D.
TI  - PPIG - Software and How it Lives On - Embedding Live Programs in the World Around Them.
PY  - 2016
AB  - NA
SP  - 19
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Grubert, Jens; Langlotz, Tobias; Zollmann, Stefanie; Regenbrecht, Holger
TI  - Towards Pervasive Augmented Reality: Context-Awareness in Augmented Reality
PY  - 2016
AB  - Augmented Reality is a technique that enables users to interact with their physical environment through the overlay of digital information. While being researched for decades, more recently, Augmented Reality moved out of the research labs and into the field. While most of the applications are used sporadically and for one particular task only, current and future scenarios will provide a continuous and multi-purpose user experience. Therefore, in this paper, we present the concept of Pervasive Augmented Reality, aiming to provide such an experience by sensing the user’s current context and adapting the AR system based on the changing requirements and constraints. We present a taxonomy for Pervasive Augmented Reality and context-aware Augmented Reality, which classifies context sources and context targets relevant for implementing such a context-aware, continuous Augmented Reality experience. We further summarize existing approaches that contribute towards Pervasive Augmented Reality. Based our taxonomy and survey, we identify challenges for future research directions in Pervasive Augmented Reality.
SP  - 1706
EP  - 1724
JF  - IEEE transactions on visualization and computer graphics
VL  - 23
IS  - 6
PB  - 
DO  - 10.1109/tvcg.2016.2543720
ER  - 

TY  - JOUR
AU  - Zinenko, Oleksandr; Huot, Stéphane; Bastoul, Cédric
TI  - Visual Program Manipulation in the Polyhedral Model
PY  - 2018
AB  - Parallelism is one of the key performance sources in modern computer systems. When heuristics-based automatic parallelization fails to improve performance, a cumbersome and error-prone manual transformation is often required. As a solution, we propose an interactive visual approach building on the polyhedral model that visualizes exact dependencies and parallelism; decomposes and replays a complex automatically computed transformation step by step; and allows for directly manipulating the visual representation as a means of transforming the program with immediate feedback. User studies suggest that our visualization is understood by experts and nonexperts alike, and that it may favor an exploratory approach.
SP  - 1
EP  - 25
JF  - ACM Transactions on Architecture and Code Optimization
VL  - 15
IS  - 1
PB  - 
DO  - 10.1145/3177961
ER  - 

TY  - JOUR
AU  - Bo, Zhitao; Lu, Lin; Sharf, Andrei; Xia, Yang; Deussen, Oliver; Chen, Baoquan
TI  - Printable 3D Trees
PY  - 2017
AB  - With the growing popularity of 3D printing, different shape classes such as fibers and hair have been shown, driving research toward class-specific solutions. Among them, 3D trees are an important class, consisting of unique structures, characteristics and botanical features. Nevertheless, trees are an especially challenging case for 3D manufacturing. They typically consist of non-volumetric patch leaves, an extreme amount of small detail often below printable resolution and are often physically weak to be self-sustainable. We introduce a novel 3D tree printability method which optimizes trees through a set of geometry modifications for manufacturing purposes. Our key idea is to formulate tree modifications as a minimal constrained set which accounts for the visual appearance of the model and its structural soundness. To handle non-printable fine details, our method modifies the tree shape by gradually abstracting details of visible parts while reducing details of non-visible parts. To guarantee structural soundness and to increase strength and stability, our algorithm incorporates a physical analysis and adjusts the tree topology and geometry accordingly while adhering to allometric rules. Our results show a variety of tree species with different complexity that are physically sound and correctly printed within reasonable time. The printed trees are correct in terms of their allometry and of high visual quality, which makes them suitable for various applications in the realm of outdoor design, modeling and manufacturing.publishe
SP  - 29
EP  - 40
JF  - Computer Graphics Forum
VL  - 36
IS  - 7
PB  - 
DO  - 10.1111/cgf.13269
ER  - 

TY  - JOUR
AU  - Ma, Li-Ke; Zhang, Yizhonc; Liu, Yang; Zhou, Kun; Tong, Xin
TI  - Computational design and fabrication of soft pneumatic objects with desired deformations
PY  - 2017
AB  - We present an end-to-end solution for design and fabrication of soft pneumatic objects with desired deformations. Given a 3D object with its rest and deformed target shapes, our method automatically optimizes the chamber structure and material distribution inside the object volume so that the fabricated object can deform to all the target deformed poses with controlled air injection. To this end, our method models the object volume with a set of chambers separated by material shells. Each chamber has individual channels connected to the object surface and thus can be separately controlled with a pneumatic system, while the shell is comprised of base material with an embedded frame structure. A two-step algorithm is developed to compute the geometric layout of the chambers and frame structure as well as the material properties of the frame structure from the input. The design results can be fabricated with 3D printing and deformed by a controlled pneumatic system. We validate and demonstrate the efficacy of our method with soft pneumatic objects that have different shapes and deformation behaviors.
SP  - 239
EP  - 12
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 6
PB  - 
DO  - 10.1145/3130800.3130850
ER  - 

TY  - JOUR
AU  - Kruijff, Ernst; Orlosky, Jason; Kishishita, Naohiro; Trepkowski, Christina; Kiyokawa, Kiyoshi
TI  - The Influence of Label Design on Search Performance and Noticeability in Wide Field of View Augmented Reality Displays
PY  - 2018
AB  - In Augmented Reality (AR), search performance for outdoor tasks is an important metric for evaluating the success of a large number of AR applications. Users must be able to find content quickly, labels and indicators must not be invasive but still clearly noticeable, and the user interface should maximize search performance in a variety of conditions. To address these issues, we have set up a series of experiments to test the influence of virtual characteristics such as color, size, and leader lines on the performance of search tasks and noticeability in both real and simulated environments. We evaluate two primary areas, including 1) the effects of peripheral field of view (FOV) limitations and labeling techniques on target acquisition during outdoor mobile search, and 2) the influence of local characteristics such as color, size, and motion on text labels over dynamic backgrounds. The first experiment showed that limited FOV will severely limit search performance, but that appropriate placement of labels and leaders within the periphery can alleviate this problem without interfering with walking or decreasing user comfort. In the second experiment, we found that different types of motion are more noticeable in optical versus video see-through displays, but that blue coloration is most noticeable in both. Results can aid in designing more effective view management techniques, especially for wider field of view displays.
SP  - 2821
EP  - 2837
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 9
PB  - 
DO  - 10.1109/tvcg.2018.2854737
ER  - 

TY  - NA
AU  - Shi, Lei; Zhao, Yuhang; Azenkot, Shiri
TI  - UIST - Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations
PY  - 2017
AB  - As three-dimensional printers become more available, 3D printed models can serve as important learning materials, especially for blind people who perceive the models tactilely. Such models can be much more powerful when augmented with audio annotations that describe the model and their elements. We present Markit and Talkit, a low-barrier toolkit for creating and interacting with 3D models with audio annotations. Makers (e.g., hobbyists, teachers, and friends of blind people) can use Markit to mark model elements and associate then with text annotations. A blind user can then print the augmented model, launch the Talkit application, and access the annotations by touching the model and following Talkit's verbal cues. Talkit uses an RGB camera and a microphone to sense users' inputs so it can run on a variety of devices. We evaluated Markit with eight sighted "makers" and Talkit with eight blind people. On average, non-experts added two annotations to a model in 275 seconds (SD=70) with Markit. Meanwhile, with Talkit, blind people found a specified annotation on a model in an average of 7 seconds (SD=8).
SP  - 493
EP  - 506
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126650
ER  - 

TY  - NA
AU  - Marky, Karola; Weiß, Andreas; Matviienko, Andrii; Brandherm, Florian; Wolf, Sebastian; Schmitz, Martin; Krell, Florian; Müller, Florian; Mühlhäuser, Max; Kosch, Thomas
TI  - CHI - Let’s Frets! Assisting Guitar Students During Practice via Capacitive Sensing
PY  - 2021
AB  - Learning a musical instrument requires regular exercise. However, students are often on their own during their practice sessions due to the limited time with their teachers, which increases the likelihood of mislearning playing techniques. To address this issue, we present Let’s Frets - a modular guitar learning system that provides visual indicators and capturing of finger positions on a 3D-printed capacitive guitar fretboard. We based the design of Let’s Frets on requirements collected through in-depth interviews with professional guitarists and teachers. In a user study (N=24), we evaluated the feedback modules of Let’s Frets against fretboard charts. Our results show that visual indicators require the least time to realize new finger positions while a combination of visual indicators and position capturing yielded the highest playing accuracy. We conclude how Let’s Frets enables independent practice sessions that can be translated to other musical instruments.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445595
ER  - 

TY  - NA
AU  - Huber, Bernd; Shin, Hijung Valentina; Russell, Bryan; Wang, Oliver; Mysore, Gautham J.
TI  - B-Script: Transcript-based B-roll Video Editing with Recommendations
PY  - 2019
AB  - In video production, inserting B-roll is a widely used technique to enrich the story and make a video more engaging. However, determining the right content and positions of B-roll and actually inserting it within the main footage can be challenging, and novice producers often struggle to get both timing and content right. We present B-Script, a system that supports B-roll video editing via interactive transcripts. B-Script has a built-in recommendation system trained on expert-annotated data, recommending users B-roll position and content. To evaluate the system, we conducted a within-subject user study with 110 participants, and compared three interface variations: a timeline-based editor, a transcript-based editor, and a transcript-based editor with recommendations. Users found it easier and were faster to insert B-roll using the transcript-based interface, and they created more engaging videos when recommendations were provided.
SP  - 81
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300311
ER  - 

TY  - NA
AU  - Ledo, David; Houben, Steven; Vermeulen, Jo; Marquardt, Nicolai; Oehlberg, Lora; Greenberg, Saul
TI  - CHI - Evaluation Strategies for HCI Toolkit Research
PY  - 2018
AB  - Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.
SP  - 36
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173610
ER  - 

TY  - NA
AU  - Huang, Michael Xuelin; Bulling, Andreas
TI  - SacCalib: Reducing Calibration Distortion for Stationary Eye Trackers Using Saccadic Eye Movements
PY  - 2019
AB  - Recent methods to automatically calibrate stationary eye trackers were shown to effectively reduce inherent calibration distortion. However, these methods require additional information, such as mouse clicks or on-screen content. We propose the first method that only requires users' eye movements to reduce calibration distortion in the background while users naturally look at an interface. Our method exploits that calibration distortion makes straight saccade trajectories appear curved between the saccadic start and end points. We show that this curving effect is systematic and the result of distorted gaze projection plane. To mitigate calibration distortion, our method undistorts this plane by straightening saccade trajectories using image warping. We show that this approach improves over the common six-point calibration and is promising for reducing distortion. As such, it provides a non-intrusive solution to alleviating accuracy decrease of eye tracker during long-term use.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Pezutti-Dyer, Franklin; Buechley, Leah
TI  - Extruder-Turtle: A Library for 3D Printing Delicate, Textured, and Flexible Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501312
ER  - 

TY  - NA
AU  - Khandwala, Kandarp; Guo, Philip J.
TI  - L@S - Codemotion: expanding the design space of learner interactions with computer programming tutorial videos
PY  - 2018
AB  - Love them or hate them, videos are a pervasive format for delivering online education at scale. They are especially popular for computer programming tutorials since videos convey expert narration alongside the dynamic effects of editing and running code. However, these screencast videos simply consist of raw pixels, so there is no way to interact with the code embedded inside of them. To expand the design space of learner interactions with programming videos, we developed Codemotion, a computer vision algorithm that automatically extracts source code and dynamic edits from existing videos. Codemotion segments a video into regions that likely contain code, performs OCR on those segments, recognizes source code, and merges together related code edits into contiguous intervals. We used Codemotion to build a novel video player and then elicited interaction design ideas from potential users by running an elicitation study with 10 students followed by four participatory design workshops with 12 additional students. Participants collectively generated ideas for 28 kinds of interactions such as inline code editing, code-based skimming, pop-up video search, and in-video coding exercises.
SP  - 57
EP  - NA
JF  - Proceedings of the Fifth Annual ACM Conference on Learning at Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3231644.3231652
ER  - 

TY  - NA
AU  - Li, Daniel; Chen, Thomas; Tung, Albert; Chilton, Lydia B.
TI  - Hierarchical Summarization for Longform Spoken Dialog
PY  - 2021
AB  - Every day we are surrounded by spoken dialog. This medium delivers rich diverse streams of information auditorily; however, systematically understanding dialog can often be non-trivial. Despite the pervasiveness of spoken dialog, automated speech understanding and quality information extraction remains markedly poor, especially when compared to written prose. Furthermore, compared to understanding text, auditory communication poses many additional challenges such as speaker disfluencies, informal prose styles, and lack of structure. These concerns all demonstrate the need for a distinctly speech tailored interactive system to help users understand and navigate the spoken language domain. While individual automatic speech recognition (ASR) and text summarization methods already exist, they are imperfect technologies; neither consider user purpose and intent nor address spoken language induced complications. Consequently, we design a two stage ASR and text summarization pipeline and propose a set of semantic segmentation and merging algorithms to resolve these speech modeling challenges. Our system enables users to easily browse and navigate content as well as recover from errors in these underlying technologies. Finally, we present an evaluation of the system which highlights user preference for hierarchical summarization as a tool to quickly skim audio and identify content of interest to the user.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Beaudouin-Lafon, Michel
TI  - CHItaly - Towards Unified Principles of Interaction
PY  - 2017
AB  - Even though today's computers are used for many different types of tasks, they still rely on user interfaces designed for office workers in the 1980s. HCI researchers have produced a slew of innovative interaction styles, from gestural interaction to mixed reality and tangible interfaces, but they have not replaced traditional GUIs. I argue that we must devise fundamental principles of interaction that unify, rather than separate, interaction styles in order to support the diversity of uses and users. I describe ongoing work on my ERC advanced grant, ONE, which explores how the concepts of information substrates and interaction instruments create digital environments that users can appropriate and (re)combine at will.
SP  - 1
EP  - 2
JF  - Proceedings of the 12th Biannual Conference on Italian SIGCHI Chapter
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3125571.3125602
ER  - 

TY  - NA
AU  - Liu, Meng; Li, Youfu; Liu, Hai
TI  - IROS - 3D Gaze Estimation for Head-Mounted Devices based on Visual Saliency
PY  - 2020
AB  - Compared with the maturity of 2D gaze tracking technology, 3D gaze tracking has gradually become a research hotspot in recent years. The head-mounted gaze tracker has shown great potential for gaze estimation in 3D space due to its appealing flexibility and portability. The general challenge for 3D gaze tracking algorithms is that calibration is necessary before the usage, and calibration targets cannot be easily applied in some situations or might be blocked by moving human and objects. Besides, the accuracy on depth direction has always come to be a crucial problem. Regarding the issues mentioned above, a 3D gaze estimation with auto-calibration method is proposed in this study. We use an RGBD camera as the scene camera to acquire the accurate 3D structure of the environment. The automatic calibration is achieved by uniting gaze vectors with saliency maps of the scene which aligned depth information. Finally, we determine the 3D gaze point through a point cloud generated from the RGBD camera. The experiment result demonstrates that our proposed method achieves 4.34◦ of average angle error in the field from 0.5m to 3m and the average depth error is 23.22mm, which is sufficient for 3D gaze estimation in the real scene.
SP  - 10611
EP  - 10616
JF  - 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iros45743.2020.9341755
ER  - 

TY  - JOUR
AU  - Liu, Meng; Li, Youfu; Liu, Hai
TI  - Robust 3-D Gaze Estimation via Data Optimization and Saliency Aggregation for Mobile Eye-Tracking Systems
PY  - 2021
AB  - In order to precisely predict 3-D gaze points, calibration is needed for each subject prior to first use the mobile gaze tracking system. However, traditional calibration methods normally expect the user to stare at predefined targets in the scene, which is troublesome and time-consuming. In this study, we proposed a novel method to remove the explicit user calibration and achieve robust 3-D gaze estimation in the room-scale area. Our proposed framework treats salient regions in the scene as possible 3-D locations of gaze points. To improve the efficiency of predicting 3-D gaze from visual saliency, the bag-of-word algorithm is adopted for eliminating redundant scene image data based on their similarities. After the elimination, saliency maps are generated from those scene images, and the geometrical relationship among the scene and eye cameras is obtained through aggregating 3-D salient targets with eye visual directions. Finally, we calculate the 3-D point of regard (PoR) by utilizing 3-D structures of the scene. The experimental results indicate that our method enhances the reliability of saliency maps and achieves promising performances on 3-D gaze estimation with different subjects.
SP  - 1
EP  - 10
JF  - IEEE Transactions on Instrumentation and Measurement
VL  - 70
IS  - NA
PB  - 
DO  - 10.1109/tim.2021.3065437
ER  - 

TY  - JOUR
AU  - Rhodin, Helge; Richardt, Christian; Casas, Dan; Insafutdinov, Eldar; Shafiei, Mohammad; Seidel, Hans-Peter; Schiele, Bernt; Theobalt, Christian
TI  - EgoCap: egocentric marker-less motion capture with two fisheye cameras
PY  - 2016
AB  - Marker-based and marker-less optical skeletal motion-capture methods use an outside-in arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort with marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds. Alternative suit-based systems use several inertial measurement units or an exoskeleton to capture motion with an inside-in setup, i.e. without external sensors. This makes capture independent of a confined volume, but requires substantial, often constraining, and hard to set up body instrumentation. Therefore, we propose a new method for real-time, marker-less, and egocentric motion capture: estimating the full-body skeleton pose from a lightweight stereo pair of fisheye cameras attached to a helmet or virtual reality headset - an optical inside-in method, so to speak. This allows full-body motion capture in general indoor and outdoor scenes, including crowded scenes with many people nearby, which enables reconstruction in larger-scale activities. Our approach combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a large new dataset. It is particularly useful in virtual reality to freely roam and interact, while seeing the fully motion-captured virtual body.
SP  - 162
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 35
IS  - 6
PB  - 
DO  - 10.1145/2980179.2980235
ER  - 

TY  - JOUR
AU  - Lazarus, Nathan; Bedair, Sarah S.
TI  - Creating 3D printed sensor systems with conductive composites
PY  - 2020
AB  - <jats:title>Abstract</jats:title> <jats:p>Fused filament fabrication (FFF), the printing of parts through the deposition of layers of melted thermoplastic, is one of the most widely used 3D printing processes due to its ease of use, low cost and accessibility. In this work, integration of 3D printed sensors and interconnect between embedded components into a printed part is demonstrated in an FFF process for the first time. The use of printed active materials for sensing allows interactivity with the end user through mechanisms like touch and temperature. Through dual extrusion in a low cost commercial printer, printing of both a conductive thermoplastic composite and a non-conducting filament are combined to create complex patterns. The piezoresistive and thermally responsive properties of the thermoplastic composite are used to create several different sensor modalities including a piezoresistive strain sensor, a contact switch and a resistive temperature sensor. A heated insertion technique is then developed for embedding of electrical components. The conductive thermoplastic is also used to incorporate a 3D printed circuit board in the same part, including in-line embedding within the body of the part during the printing process. The sensor performance and component embedding properties are characterized, and the process is used to print systems including relaxation oscillators and op-amp interface circuits for sensor monitoring.</jats:p>
SP  - 015020
EP  - NA
JF  - Smart Materials and Structures
VL  - 30
IS  - 1
PB  - 
DO  - 10.1088/1361-665x/abcbe2
ER  - 

TY  - NA
AU  - Avellino, Ignacio; Fleury, Cédric; Mackay, Wendy E.; Beaudouin-Lafon, Michel
TI  - CHI - CamRay: Camera Arrays Support Remote Collaboration on Wall-Sized Displays
PY  - 2017
AB  - Remote collaboration across wall-sized displays creates a key challenge: how to support audio-video communication among users as they move in front of the display. We present CamRay, a platform that uses camera arrays embedded in wall-sized displays to capture video of users and present it on remote displays according to the users' positions. We investigate two settings: in Follow-Remote, the position of the video window follows the position of the remote user; in Follow-Local, the video window always appears in front of the local user. We report the results of a controlled experiment showing that with Follow-Remote, participants are faster, use more deictic instructions, interpret them more accurately, and use fewer words. However, some participants preferred the virtual face-to-face created by Follow-Local when checking for their partners' understanding. We conclude with design recommendations to support remote collaboration across wall-sized displays.
SP  - 6718
EP  - 6729
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025604
ER  - 

TY  - JOUR
AU  - Mohanto, Bipul; Islam, Abm Tariqul; Gobbetti, Enrico; Staadt, Oliver G.
TI  - An integrative view of foveated rendering
PY  - 2022
AB  - Abstract Foveated rendering adapts the image synthesis process to the user’s gaze. By exploiting the human visual system’s limitations, in particular in terms of reduced acuity in peripheral vision, it strives to deliver high-quality visual experiences at very reduced computational, storage, and transmission costs. Despite the very substantial progress made in the past decades, the solution landscape is still fragmented, and several research problems remain open. In this work, we present an up-to-date integrative view of the domain from the point of view of the rendering methods employed, discussing general characteristics, commonalities, differences, advantages, and limitations. We cover, in particular, techniques based on adaptive resolution, geometric simplification, shading simplification, chromatic degradation, as well spatio-temporal deterioration. Next, we review the main areas where foveated rendering is already in use today. We finally point out relevant research issues and analyze research trends.
SP  - 474
EP  - 501
JF  - Computers & Graphics
VL  - 102
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.10.010
ER  - 

TY  - NA
AU  - Savage, Valkyrie
TI  - Fabbed to Sense: Integrated Design of Geometry and Sensing Algorithms for Interactive Objects
PY  - 2016
AB  - Author(s): Savage, Valkyrie Arline | Advisor(s): Hartmann, Bjorn | Abstract: Task-specific tangible input devices, like video game controllers, improve user speed and accuracy in input tasks compared to the more general-purpose touchscreen or mouse and keyboard. However, while modifying a graphical user interface (GUI) to accept mouse and keyboard inputs for new and specific tasks is relatively easy and requires only software knowledge, tangible input devices are challenging to prototype and build.Rapid prototyping digital fabrication machines, such as vinyl cutters, laser cutters, and 3D printers, now permeate the design process for such devices. Using these tools, designers can realize a new tangible design faster than ever. In a typical design process, these machines are not used to create the interaction in these interactive product prototypes: they merely create the shell, case, or body, leaving the designer to, in an entirely separate process, assemble and program electronics for sensing a user's input. What are the most cost-effective, fast, and flexible ways of sensing rapid-prototyped input devices? In this dissertation, we investigate how 2D and 3D models for input devices can be automatically generated or modified in order to employ standard, off-the-shelf sensing techniques for adding interactivity to those objects: we call this ``fabbing to sense.''We describe the capabilities of modern rapid prototyping machines, linking these abilities to potential sensing mechanisms when possible. We plunge more deeply into three examples of sensing/fabrication links: we build analysis and design tools that help users design, fabricate, assemble, and \emph{use} input devices sensed through these links. First, we discuss Midas, a tool for building capacitive sensing interfaces on non-screen surfaces, like the back of a phone. Second, we describe Lamello, a technique that generates lasercut and 3D printed tine structures and simulates their vibrational frequencies for training-free audio sensing. Finally, we present Sauron, a tool that automatically modifies the interior of 3D input models to allow sensing via a single embedded camera. We demonstrate each technique's flexibility to be used for many types of input devices through a series of example objects.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Dunn, David; Tippets, Cary A.; Torell, Kent; Kellnhofer, Petr; Akşit, Kaan; Didyk, Piotr; Myszkowski, Karol; Luebke, David; Fuchs, Henry
TI  - Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors
PY  - 2017
AB  - Accommodative depth cues, a wide field of view, and ever-higher resolutions all present major hardware design challenges for near-eye displays. Optimizing a design to overcome one of these challenges typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution — a new wide field of view, gaze-tracked near-eye display for augmented reality applications. The key component of our solution is the use of a single see-through, varifocal deformable membrane mirror for each eye reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane which is determined by the gaze tracker. The benefits of using the membranes include wide field of view (100° diagonal) and fast depth switching (from 20 cm to infinity within 300 ms). Our subjective experiment verifies the prototype and demonstrates its potential benefits for near-eye see-through displays.
SP  - 1322
EP  - 1331
JF  - IEEE transactions on visualization and computer graphics
VL  - 23
IS  - 4
PB  - 
DO  - 10.1109/tvcg.2017.2657058
ER  - 

TY  - JOUR
AU  - Yao, Xinwei; Fried, Ohad; Fatahalian, Kayvon; Agrawala, Maneesh
TI  - Iterative Text-Based Editing of Talking-Heads Using Neural Retargeting
PY  - 2021
AB  - We present a text-based tool for editing talking-head video that enables an iterative editing workflow. On each iteration users can edit the wording of the speech, further refine mouth motions if necessary to reduce artifacts, and manipulate non-verbal aspects of the performance by inserting mouth gestures (e.g., a smile) or changing the overall performance style (e.g., energetic, mumble). Our tool requires only 2 to 3 minutes of the target actor video and it synthesizes the video for each iteration in about 40 seconds, allowing users to quickly explore many editing possibilities as they iterate. Our approach is based on two key ideas. (1) We develop a fast phoneme search algorithm that can quickly identify phoneme-level subsequences of the source repository video that best match a desired edit. This enables our fast iteration loop. (2) We leverage a large repository of video of a source actor and develop a new self-supervised neural retargeting technique for transferring the mouth motions of the source actor to the target actor. This allows us to work with relatively short target actor videos, making our approach applicable in many real-world editing scenarios. Finally, our, refinement and performance controls give users the ability to further fine-tune the synthesized results.
SP  - 1
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 3
PB  - 
DO  - 10.1145/3449063
ER  - 

TY  - JOUR
AU  - Wang, Yu-Ping; Xie, Sen-Wei; Wang, Li-Hui; Xu, Hongjin; Tabata, Satoshi; Ishikawa, Masatoshi
TI  - ARSlice: Head-Mounted Display Augmented with Dynamic Tracking and Projection
PY  - 2022
AB  - NA
SP  - 666
EP  - 679
JF  - Journal of Computer Science and Technology
VL  - 37
IS  - 3
PB  - 
DO  - 10.1007/s11390-022-2173-y
ER  - 

TY  - NA
AU  - Moradi, Hedieh; Torres, César I.
TI  - Conference on Designing Interactive Systems - Siloseam: A Morphogenetic Workflow for the Design and Fabrication of Inflatable Silicone Bladders
PY  - 2020
AB  - Silicone is a transformative design material found within a variety of emerging HCI practices including shape-changing interfaces, soft robotics, and wearables. However, workflows for designing and fabricating silicone forms require a time-intensive mold-cast-cure pipeline that limits the experiential knowledge that can be gained from working directly with silicone. In this work, we conduct a material-centric exploration of silicone and develop designerly workflows for creating inflatable silicone bladders. We present Siloseam, a creative framework that streamlines a bladder design and fabrication process, collects tacit knowledge involved in recovering from errors, and introduces new workflows that reuse existing molds. A set of exemplar artifacts demonstrates an expanded repertoire of silicone forms that leverage various configurations of airtight seams to create playful, haptic interactions. We discuss the remaining challenges in integrating silicone with a broader range of materials and opportunities for developing designerly workflows for other mold-and-cast processes.
SP  - 1995
EP  - 2006
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395473
ER  - 

TY  - JOUR
AU  - Roels, Reinout; Signer, Beat
TI  - A Conceptual Framework and Content Model for Next Generation Presentation Solutions
PY  - 2019
AB  - Mainstream presentation tools such as Microsoft PowerPoint were originally built to mimic physical media like photographic slides and still exhibit the same characteristics. However, the state of the art in presentation tools shows that more recent solutions start to go beyond the classic presentation paradigms. For instance, presentations are becoming increasingly non-linear, content is quickly evolving beyond simple text and images and the way we author our presentations is becoming more collaborative. Nevertheless, existing presentation content models are often based on assumptions that do not apply to the current state of presentations any more, making them incompatible for some use cases and limiting the potential of end-user presentation solutions. In order to support state-of-the-art presentation functionality, we rethink the concept of a presentation and introduce a conceptual framework for presentation content. We then present a new content model for presentation solutions based on the Resource-Selector-Link (RSL) hypermedia metamodel. We further discuss an implementation of our model and show some example use cases. We conclude by outlining how design choices in the model address currently unmet needs with regards to extensibility, content reuse, collaboration, semantics, user access management, non-linearity, and context awareness, resulting in better support for the corresponding end-user functionality in presentation tools.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - EICS
PB  - 
DO  - 10.1145/3331149
ER  - 

TY  - NA
AU  - Hamanishi, Natsuki; Kono, Michinari; Suwa, Shunichi; Miyaki, Takashi; Rekimoto, Jun
TI  - CHI Extended Abstracts - Fibritary: Rotary Jet-Spinning for Personal Fiber Fabrication
PY  - 2019
AB  - The development of personal fabrication technologies has enabled end users to model and prototype desired objects. 3D printing technologies have eased our access to solid models, however, it is still a challenge to develop thin fibers rapidly at personal levels that may help enriching textures of models. We propose a system and method inspired by cotton candy making, which uses rotary jet-spinning to extract thin plastic fibers at high speed. We report our exploration of the proposed method where we studied various plastic materials, the effects of the rotation speed, and the hole size of the fiber exit. The method allows plastic fibers to be extracted at micro-scale, and we propose various examples of use cases. Our approach can be used in combination with traditional 3D printing techniques, where soft and/or hairy models are required to design the texture of a 3D model.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312934
ER  - 

TY  - NA
AU  - Fraser, C. Ailie; Kim, Joy O.; Shin, Hijung Valentina; Brandt, Joel; Dontcheva, Mira
TI  - CHI - Temporal Segmentation of Creative Live Streams
PY  - 2020
AB  - Many artists broadcast their creative process through live streaming platforms like Twitch and YouTube, and people often watch archives of these broadcasts later for learning and inspiration. Unfortunately, because live stream videos are often multiple hours long and hard to skim and browse, few can leverage the wealth of knowledge hidden in these archives. We present an approach for automatic temporal segmentation of creative live stream videos. Using an audio transcript and a log of software usage, the system segments the video into sections that the artist can optionally label with meaningful titles. We evaluate this approach by gathering feedback from expert streamers and comparing automatic segmentations to those made by viewers. We find that, while there is no one "correct" way to segment a live stream, our automatic method performs similarly to viewers, and streamers find it useful for navigating their streams after making slight adjustments and adding section titles.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376437
ER  - 

TY  - BOOK
AU  - Litt, Geoffrey; Jackson, Daniel
TI  - Programming - Wildcard: spreadsheet-driven customization of web applications
PY  - 2020
AB  - Many Web applications do not meet the precise needs of their users. Browser extensions offer a way to customize web applications, but most people do not have the programming skills to implement their own extensions. In this paper, we present spreadsheet-driven customization, a technique that enables end users to customize software without doing any traditional programming. The idea is to augment an application’s UI with a spreadsheet that is synchronized with the application’s data. When the user manipulates the spreadsheet, the underlying data is modified and the changes are propagated to the UI, and vice versa. We have implemented this technique in a prototype browser extension called Wildcard. Through concrete examples, we demonstrate that Wildcard can support useful customizations—ranging from sorting lists of search results to showing related data from web APIs—on top of existing websites. We also present the design principles underlying our prototype. Customization can lead to dramatically better experiences with software. We think that spreadsheet-driven customization offers a promising new approach to unlocking this benefit for all users, not just programmers.
SP  - 126
EP  - 135
JF  - Conference Companion of the 4th International Conference on Art, Science, and Engineering of Programming
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397537.3397541
ER  - 

TY  - CONF
AU  - Perrin, Marc-Emmanuel; Eagan, James; Beaudouin-Lafon, Michel
TI  - Human-oriented Infrastructures for Multi-surface Environments
PY  - 2016
AB  - From offices to public spaces, dynamic multi-surface environments that can leverage the devices that users carry with them are becoming more common. However these environments are often implicit and therefore hard to discover, as are the multi-device interactions that they support. This position paper outlines the challenges that designers of multi-surface environments face to improve service discoverability, to support interactions that leverage users' devices, and to provide software tools to design and develop cross-devices applications.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zagermann, Johannes; Pfeil, Ulrike; Acevedo, Carmela; Reiterer, Harald
TI  - MUM - Studying the benefits and challenges of spatial distribution and physical affordances in a multi-device workspace
PY  - 2017
AB  - In recent years, research on cross-device interaction has become a popular topic in HCI leading to novel interaction techniques mutually interfering with new evolving theoretical paradigms. Building on previous research, we implemented an individual multi-device work environment for creative activities. In a study with 20 participants, we compared a traditional toolbar-based condition with two conditions facilitating spatially distributed tools on digital panels and on physical devices. We analyze participants' interactions with the tools, encountered problems and corresponding solutions, as well as subjective task load and user experience. Our findings show that the spatial distribution of tools indeed offers advantages, but also elicits new problems, that can partly be leveraged by the physical affordances of mobile devices.
SP  - 249
EP  - 259
JF  - Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3152832.3152855
ER  - 

TY  - JOUR
AU  - Shigemune, Hiroki; Maeda, Shingo; Cacucciolo, Vito; Iwata, Yoshitaka; Iwase, Eiji; Hashimoto, Shuji; Sugano, Shigeki
TI  - Printed Paper Robot Driven by Electrostatic Actuator
PY  - 2017
AB  - Effective design and fabrication of 3-D electronic circuits are among the most pressing issues for future engineering. Although a variety of flexible devices have been developed, most of them are still designed two-dimensionally. In this letter, we introduce a novel idea to fabricate a 3-D wiring board. We produced the 3-D wiring board from one desktop inkjet printer by printing conductive pattern and a 2-D pattern to induce self-folding. We printed silver ink onto a paper to realize the conductive trace. Meanwhile, a 3-D structure was constructed with self-folding induced by water-based ink printed from the same printer. The paper with the silver ink self-folds along the printed line. The printed silver ink is sufficiently thin to be flexible. Even if the silver ink is already printed, the paper can self-fold or self-bend to consist the 3-D wiring board. A paper scratch driven robot was developed using this method. The robot traveled 56 mm in 15 s according to the vibration induced by the electrostatic force of the printed electrode. The size of the robot is 30 × 15 × 10 mm. This work proposes a new method to design 3-D wiring board, and shows extended possibilities for printed paper mechatronics.
SP  - 1001
EP  - 1007
JF  - IEEE Robotics and Automation Letters
VL  - 2
IS  - 2
PB  - 
DO  - 10.1109/lra.2017.2658942
ER  - 

TY  - NA
AU  - Zheng, Clement; Yi-Luen, Ellen
TI  - Tangible and Embedded Interaction - Mechamagnets: Tactile Mechanisms with Embedded Magnets
PY  - 2018
AB  - This paper presents Mechamagnets, a technique to rapidly prototype tactile mechanisms for tangible interfaces. We demonstrate how to embed different passive tactile mechanisms in physical systems through a combination of magnets and digitally fabricated parts. We also discuss how DIY materials and 3-axis magnetometers can instrument Mechamagnet interfaces into functional prototypes.
SP  - 57
EP  - 64
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173268
ER  - 

TY  - JOUR
AU  - Badam, Sriram Karthik; Mathisen, Andreas; Rädle, Roman; Klokmose, Clemens Nylandsted; Elmqvist, Niklas
TI  - Vistrates: A Component Model for Ubiquitous Analytics
PY  - 2018
AB  - Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components—the building blocks of this model—can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce V istrates , a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic “anytime” and “anywhere” motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.
SP  - 586
EP  - 596
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865144
ER  - 

TY  - BOOK
AU  - Fuhl, Wolfgang; Geisler, David; Santini, Thiago; Appel, Tobias; Rosenstiel, Wolfgang; Kasneci, Enkelejda
TI  - ETRA - CBF: circular binary features for robust and real-time pupil center detection
PY  - 2018
AB  - Modern eye tracking systems rely on fast and robust pupil detection, and several algorithms have been proposed for eye tracking under real world conditions. In this work, we propose a novel binary feature selection approach that is trained by computing conditional distributions. These features are scalable and rotatable, allowing for distinct image resolutions, and consist of simple intensity comparisons, making the approach robust to different illumination conditions as well as rapid illumination changes. The proposed method was evaluated on multiple publicly available data sets, considerably outperforming state-of-the-art methods, and being real-time capable for very high frame rates. Moreover, our method is designed to be able to sustain pupil center estimation even when typical edge-detection-based approaches fail - e.g., when the pupil outline is not visible due to occlusions from reflections or eye lids / lashes. As a consequece, it does not attempt to provide an estimate for the pupil outline. Nevertheless, the pupil center suffices for gaze estimation - e.g., by regressing the relationship between pupil center and gaze point during calibration.
SP  - 8
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204559
ER  - 

TY  - JOUR
AU  - Kasprowski, Pawel; Harezlak, Katarzyna; Skurowski, Przemysław
TI  - Implicit Calibration Using Probable Fixation Targets.
PY  - 2019
AB  - Proper calibration of eye movement signal registered by an eye tracker seems to be one of the main challenges in popularizing eye trackers as yet another user-input device. Classic calibration methods taking time and imposing unnatural behavior on eyes must be replaced by intelligent methods that are able to calibrate the signal without conscious cooperation by the user. Such an implicit calibration requires some knowledge about the stimulus a user is looking at and takes into account this information to predict probable gaze targets. This paper describes a possible method to perform implicit calibration: it starts with finding probable fixation targets (PFTs), then it uses these targets to build a mapping-probable gaze path. Various algorithms that may be used for finding PFTs and mappings are presented in the paper and errors are calculated using two datasets registered with two different types of eye trackers. The results show that although for now the implicit calibration provides results worse than the classic one, it may be comparable with it and sufficient for some applications.
SP  - 216
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 19
IS  - 1
PB  - 
DO  - 10.3390/s19010216
ER  - 

TY  - CHAP
AU  - Borowski, Marcel; Larsen-Ledet, Ida
TI  - IS-EUD - Lessons Learned from Using Reprogrammable Prototypes with End-User Developers.
PY  - 2021
AB  - Involving end-users in the development of a product before it is deployed has great potential to increase the fit between a product and individual users’ needs. While end-users can be directly involved in modifying low-fidelity prototypes, they are left out when it comes to high-fidelity interactive prototypes—in part because these cannot be modified directly or require time-consuming edit-compile-run cycles. High-fidelity prototypes, however, are more engaging for users. We created a reprogrammable high-fidelity prototype and explored its use in short-term prototyping workshops with end-user developers, i.e. end-users with programming experience, in the domain of collaborative writing. We report observations and pitfalls, and distill four lessons learned into guidelines on how to use reprogrammable high-fidelity prototypes with end-users in contexts with limited resources. Our experiences demonstrate, among other things, that reprogrammable high-fidelity prototypes are difficult to work with—even for experienced programmers—and emphasize the need for careful attention to guiding participants, time for familiarization, and catering to multiple levels of programming experience.
SP  - 136
EP  - 152
JF  - End-User Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-79840-6_9
ER  - 

TY  - JOUR
AU  - Bao, Jun; Liu, Buyu; Yu, Jun
TI  - An Individual-Difference-Aware Model for Cross-Person Gaze Estimation.
PY  - 2022
AB  - We propose a novel method on refining cross-person gaze prediction task with eye/face images only by explicitly modelling the person-specific differences. Specifically, we first assume that we can obtain some initial gaze prediction results with existing method, which we refer to as InitNet, and then introduce three modules, the Validity Module (VM), Self-Calibration (SC) and Person-specific Transform (PT) module. By predicting the reliability of current eye/face images, VM is able to identify invalid samples, e.g. eye blinking images, and reduce their effects in modelling process. SC and PT module then learn to compensate for the differences on valid samples only. The former models the translation offsets by bridging the gap between initial predictions and dataset-wise distribution. And the later learns more general person-specific transformation by incorporating the information from existing initial predictions of the same person. We validate our ideas on three publicly available datasets, EVE, XGaze, and MPIIGaze dataset. We demonstrate that our proposed method outperforms the SOTA methods significantly on all of them, e.g. respectively 21.7%, 36.0%, and 32.9% relative performance improvements. We are the winner of the GAZE 2021 EVE Challenge and our code can be found here https://github.com/bjj9/EVE_SCPT.
SP  - 3322
EP  - 3333
JF  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
VL  - 31
IS  - NA
PB  - 
DO  - 10.1109/tip.2022.3171416
ER  - 

TY  - BOOK
AU  - Steil, Julian; Koelle, Marion; Heuten, Wilko; Boll, Susanne; Bulling, Andreas
TI  - PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric Scene Image and Eye Movement Features
PY  - 2019
AB  - Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking but the first-person camera required to map a user's gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker's first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users' eye movements. To open the shutter without visual input, PrivacEye detects changes in users' eye movements alone to gauge changes in the "privacy level" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.
SP  - 26
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319913
ER  - 

TY  - JOUR
AU  - Cohen-Kalaf, Miki; Lanir, Joel; Bak, Peter; Mokryn, Osnat
TI  - Movie emotion map: an interactive tool for exploring movies according to their emotional signature
PY  - 2021
AB  - We present Movie Emotion Map - a novel system that enables to view and browse through a large collection of movies according to the movies’ emotional characteristics. The system enables to view both the high-level structure of the movies emotional space and the low-level details of a single movie. We create an eight-dimensional emotional signature for each movie based on Plutchik’s theory of emotions, according to its reviews obtained from IMDb. We projected glyphs representing emotional signatures of the movies on a 2D plane using dimension reduction, thus, providing a topology of emotions for easy browsing and exploring. Results from a qualitative evaluation with 18 participants indicate that users could easily browse through movies according to the visualized landscape and that the tool enabled them to search, filter and find movies based on their emotional characteristics.
SP  - 1
EP  - 22
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Anderson, Zann; Jones, Michael; Seppi, Kevin D.
TI  - Tangible and Embedded Interaction - W.O.U.S.: Widgets of Unusual Size
PY  - 2018
AB  - Recent work in tangible interfaces, including widget sets like .NET Gadgeteer and Phidgets, has enabled prototyping of rich physical interaction at a handheld or tabletop scale. But it remains unclear how participants respond to physical widgets at larger scales. What kinds of interaction would larger widgets enable, and what kinds of systems - if any - can or should be built with them? We built unusually-sized widgets, or "mega-widgets" in order to explore this territory. We present the results of two iterations of building mega-widgets and accompanying user studies designed to help understand participants» reactions to mega-widgets and probe possible applications. Responses indicated, among other things, a correlation between widget size and the perceived size or importance of what it might control. Mega-widgets were also perceived as increasing the precision of user input control and providing a fun and playful element. We hope that knowledge gained from this exploratory work can help lay groundwork for further exploration of widgets at larger scales.
SP  - 221
EP  - 230
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173246
ER  - 

TY  - NA
AU  - Bhat, Gayatri; Saluja, Avneesh; Dye, Melody; Florjanczyk, Jan
TI  - Hierarchical Encoders for Modeling and Interpreting Screenplays
PY  - 2021
AB  - While natural language understanding of long-form documents remains an open challenge, such documents often contain structural information that can inform the design of models encoding them. Movie scripts are an example of such richly structured text – scripts are segmented into scenes, which decompose into dialogue and descriptive components. In this work, we propose a neural architecture to encode this structure, which performs robustly on two multi-label tag classification tasks without using handcrafted features. We add a layer of insight by augmenting the encoder with an unsupervised ‘interpretability’ module, which can be used to extract and visualize narrative trajectories. Though this work specifically tackles screenplays, we discuss how the underlying approach can be generalized to a range of structured documents.
SP  - 1
EP  - 12
JF  - Proceedings of the Third Workshop on Narrative Understanding
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2021.nuse-1.1
ER  - 

TY  - CHAP
AU  - Minagawa, Tatsuya; Ochiai, Yoichi
TI  - HCI (19) - A Case Study of Augmented Physical Interface by Foot Access with 3D Printed Attachment.
PY  - 2021
AB  - We propose an attachment creation framework that allows foot access to existing physical interfaces designed to use hands such as doorknobs. The levers, knobs, and switches of furniture and electronic devices are designed for the human hand. These interfaces may not be accessible for hygienic and physical reasons. Due to the high cost of parts and initial installation, sensing or automation is not preferable. Therefore, there is a need for a low-cost way to access physical interfaces without hands. We have enabled foot access by extending the hand-accessible interface with 3D-printed attachments. Finally, we proposed a mechanism (component set) that transmits movement from a foot-accessed pedal to an interface with attachments. And we attached it to the doorknob, water faucet, and lighting switch interface. A case study was conducted to verify the system’s effectiveness, which consisted of 3D-printed attachments and pedals.
SP  - 315
EP  - 332
JF  - Design, User Experience, and Usability: Design for Diversity, Well-being, and Social Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78224-5_22
ER  - 

TY  - NA
AU  - Tchernavskij, Philip
TI  - Decomposing Interactive Systems
PY  - 2017
AB  - I argue that systems-oriented HCI should explore software engineering principles and architectures that emphasize user interaction over designer control. Many researchers have argued that user-empowering interaction should decouple tools from the objects they act on. Implementing this decoupling requires actively subverting the traditional architectures of interactive systems, including the encapsulation of interactive systems into closed applications, and the overly coupled event-driven programming model. I present a sketch of an architecture where interaction instruments are a first-class object to address these issues.
SP  - 4
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Tripathi, Subarna; Guenter, Brian
TI  - A Statistical Approach to Continuous Self-Calibrating Eye Gaze Tracking for Head-Mounted Virtual Reality Systems
PY  - 2016
AB  - We present a novel, automatic eye gaze tracking scheme inspired by smooth pursuit eye motion while playing mobile games or watching virtual reality contents. Our algorithm continuously calibrates an eye tracking system for a head mounted display. This eliminates the need for an explicit calibration step and automatically compensates for small movements of the headset with respect to the head. The algorithm finds correspondences between corneal motion and screen space motion, and uses these to generate Gaussian Process Regression models. A combination of those models provides a continuous mapping from corneal position to screen space position. Accuracy is nearly as good as achieved with an explicit calibration step.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jones, Michael; Anderson, Zann; Walker, Casey; Seppi, Kevin D.
TI  - CHI Extended Abstracts - PHUI-kit: A Tool for Physical User Interface Layout
PY  - 2018
AB  - PHUIs are interfaces built from tangible widgets arranged on the surfaces of physical objects. PHUI-kit is tool for making physical user interface (PHUI) design more like graphical user interface (GUI) design in the context of 3D printed object housings. The tool includes a drag-and drop interface to place, reposition and delete physical widgets on a curved housing in a 3D modeling tool. Automatic cable routing simplifies assembly planning and execution. The objects are printed, the widgets snapped onto a cable and snapped into the housing to create an interactive object. A cable guide printed on paper guides the maker while snapping widgets onto the cable.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186518
ER  - 

TY  - NA
AU  - Huang, Forrest; Schoop, Eldon; Ha, David; Canny, John
TI  - IUI - Scones: towards conversational authoring of sketches
PY  - 2020
AB  - Iteratively refining and critiquing sketches are crucial steps to developing effective designs. We introduce Scones, a mixed-initiative, machine-learning-driven system that enables users to iteratively author sketches from text instructions. Scones is a novel deep-learning-based system that iteratively generates scenes of sketched objects composed with semantic specifications from natural language. Scones exceeds state-of-the-art performance on a text-based scene modification task, and introduces a mask-conditioned sketching model that can generate sketches with poses specified by high-level scene information. In an exploratory user evaluation of Scones, participants reported enjoying an iterative drawing task with Scones, and suggested additional features for further applications. We believe Scones is an early step towards automated, intelligent systems that support human-in-the-loop applications for communicating ideas through sketching in art and design.
SP  - 313
EP  - 323
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377485
ER  - 

TY  - NA
AU  - Basman, Antranig; Lewis, Clayton; Clark, Colin B. D.
TI  - Onward! - The open authorial principle: supporting networks of authors in creating externalisable designs
PY  - 2018
AB  - We introduce a new principle, the open authorial principle, that characterises desirable properties of languages supporting networks of authors. We survey the growth in generosity of authorial systems, in a progression starting with traditional object-orientation, continuing through aspect-oriented, subject-oriented, context-oriented and dependency injection systems, and concluding with the most recent generation of highly dynamic systems such as Korz and Newspeak. We follow the implications of our principle for the externalisation of application designs, resulting from the need to promote the representation of differences between programs as valid programs themselves. This raises conceptual and practical parallels with technologies and idioms supporting the web, such as REST, realised document structures supported by the DOM, and the negotiated space of CSS selectors. These parallels lead to a quite different organisation for the language and runtime of an openly authorable system, which emphasises a publicly addressable cellular structure and a largely static dispatch.
SP  - 29
EP  - 43
JF  - Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3276954.3276963
ER  - 

TY  - NA
AU  - Borowski, Marcel; Rädle, Roman; Klokmose, Clemens Nylandsted
TI  - CHI Extended Abstracts - Codestrate Packages: An Alternative to "One-Size-Fits-All" Software
PY  - 2018
AB  - We present Codestrate Packages, a package-based system to create extensible software within Codestrates. Codestrate Packages turns content creation from an application-centric model into a document-centric model. Codestrate Packages no longer restrict users to the feature set of the application. Instead packages allow users to add new features to their documents while already working on them. They can match the features to their current task at hand. Supporting the reprogrammable nature of Codestrates, new features can also be implemented by users themselves and shared with other people without having to leave the document. We illustrate the application of Codestrate Packages in an example scenario and present its technical concepts. We plan to conduct multiple user studies to investigate the benefits and barriers of Codestrate Packages' document-centric approach.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188563
ER  - 

TY  - JOUR
AU  - Dominguez, Ana; Florez, Julian; Lafuente, Alberto; Masneri, Stefano; Tamayo, Inigo; Zorrilla, Mikel
TI  - A Methodology for User Interface Adaptation of Multi-Device Broadcast-Broadband Services
PY  - 2020
AB  - New audiovisual experiences involve consuming several contents displayed through multiple internet-connected devices. The TV is still the central hub of the living room, but it is often used simultaneously with other screens. Consequently, the user has the chance to consume all different contents at once across multiple devices. However, no existing adaptation models are available to dynamically adapt such a multitude of contents in multi-device contexts. To address this gap, this article proposes a novel multi-device adaptation methodology to build adaptive User Interfaces for multi-screen hybrid broadcast-broadband TV experiences. The methodology is extensible to any kind of content, device and user, and is applicable to different contexts considering technological evolution and other fields of application. The proposed methodology is the outcome of extensive research that arose from a previous multi-device media service deployment with broadcasters.
SP  - 211048
EP  - 211062
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.3039616
ER  - 

TY  - NA
AU  - Zhdanov, Andrey; Zhdanov, Dmitry; Potemin, Igor S.; Bogdanov, Nikolay; Bykovskii, Sergei
TI  - Possibility of vergence disagreement reducing on the base of approximate restoration of the depth map
PY  - 2019
AB  - The article describes the approach that allows to reconstruct the image formed by the video see-through mixed reality system corresponding to the convergence of the device user eyes. Convergence is defined by the user eye pupils position acquired from the mixed reality device eye tracking system. The image reconstruction method is based on the use of an extended (2.5-dimensional) representation of the image obtained, for example, using a 3D scanner that builds a depth map of the scene. In the proposed solution, lens optical systems that form images of the real world on LCD screens and eyepieces that project these images into the user eyes do not change their characteristics and position. The image is reconstructed by projecting the points of the original image to the image points corresponding to the required convergence by the method of "refocusing" at a distance for each point. The advantages and disadvantages of this method are shown. An approach is proposed that reduces visual perception discomfort caused by an ambiguous distance to the image point, for example, in the case of mirror or transparent objects. Virtual prototyping of the mixed reality system showed the benefits of the proposed approach to reduce the visual perception discomfort caused by the mismatch between the convergence of human eyes and the images formed by the lenses of the mixed reality system.
SP  - 1118517
EP  - NA
JF  - Optical Design and Testing IX
VL  - 11185
IS  - NA
PB  - 
DO  - 10.1117/12.2537753
ER  - 

TY  - NA
AU  - Rivera, Michael L.; Hudson, Scott E.
TI  - CHI - Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles
PY  - 2019
AB  - We present a new type of 3D printer that combines rigid plastic printing with melt electrospinning? a technique that uses electrostatic forces to create thin fibers from a molten polymer. Our printer enables custom-shaped textile sheets (similar in feel to wool felt) to be produced alongside rigid plastic using a single material (i.e., PLA) in a single process. We contribute open-source firmware, hardware specifications, and printing parameters to achieve melt electrospinning. Our approach offers new opportunities for fabricating interactive objects and sensors that blend the flexibility, absorbency and softness of produced electrospun textiles with the structure and rigidity of hard plastic for actuation, sensing, and tactile experiences.
SP  - 204
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300434
ER  - 

TY  - NA
AU  - Gutwin, Carl; van der Kamp, Michael; Uddin, Sami; Stanley, Kevin G.; Stavness, Ian; Vail, Sally
TI  - CHI - Improving Early Navigation in Time-Lapse Video with Spread-Frame Loading
PY  - 2019
AB  - Time-lapse videos are often navigated by scrubbing with a slider. When networks are slow or images are large, however, even thumbnail versions load so slowly that scrubbing is limited to the start of the video. We developed a frame-loading technique called spread-loading that enables scrubbing regardless of delivery rate. Spread-loading orders frame delivery to maximize coverage of the entire sequence; this provides a temporal overview of the entire video that can be fully navigated at any time during delivery. The overview initially has a coarse temporal resolution, becoming finer-grained with each new frame. We compared spread-loading with traditional linear loading in a study where participants were asked to find specific episodes in a long time-lapse sequence, using three views with increasing levels of detail. Results show that participants found target episodes significantly and substantially faster with spread-loading, regardless of whether they could click to change the load point. Users rated spread-loading as requiring less effort, and strongly preferred the new technique.
SP  - 555
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300785
ER  - 

TY  - NA
AU  - Santos, Pedro Albuquerque; Madeira, Rui Neves; Correia, Nuno
TI  - Applications across Co-located Devices: User Interface Distribution, State Management and Collaboration
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - The 23rd International Conference on Information Integration and Web Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3487664.3487748
ER  - 

TY  - BOOK
AU  - Husmann, Maria; Spiegel, Michael; Murolo, Alfonso; Norrie, Moira C.
TI  - ISS - UI Testing Cross-Device Applications
PY  - 2016
AB  - The increasing number of devices available to a user has prompted the research community to explore how these can be used in combination. Frameworks and toolkits have been proposed to facilitate the design and implementation of these cross-device applications. Still, implementing cross-device applications remains complex because of the fragmentation of the user interface and logic across devices and the flexibility required to adapt to different combinations of devices. Testing in particular has been identified as a critical challenge. To address these issues, we introduce XD-Testing, a library that provides explicit and implicit device selectors, device templates and scenarios, as well as a visualiser for application screenshots. In a case study, we demonstrate how we used the library to author human-readable tests for a cross-device gallery that verify if a UI distributes correctly and if it works as expected despite being distributed.
SP  - 179
EP  - 188
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992177
ER  - 

TY  - NA
AU  - Umetani, Nobuyuki; Schmidt, Ryan
TI  - SurfCuit: Surface Mounted Circuits on 3D Prints
PY  - 2016
AB  - We present, SurfCuit, a novel approach to design and construction of electric circuits on the surface of 3D prints. Our surface mounting technique allows durable construction of circuits on the surface of 3D prints. SurfCuit does not require tedious circuit casing design or expensive set-ups, thus we can expedite the process of circuit construction for 3D models. Our technique allows the user to construct complex circuits for consumer-level desktop fused decomposition modeling (FDM) 3D printers. The key idea behind our technique is that FDM plastic forms a strong bond with metal when it is melted. This observation enables construction of a robust circuit traces using copper tape and soldering. We also present an interactive tool to design such circuits on arbitrary 3D geometry. We demonstrate the effectiveness of our approach through various actual construction examples.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Biskjaer, Michael Mose; Frich, Jonas; Vermeulen, Lindsay MacDonald; Remy, Christian; Dalsgaard, Peter
TI  - ECCE - How Time Constraints in a Creativity Support Tool Affect the Creative Writing Experience
PY  - 2019
AB  - Being able to write ‘on cue’ is critical in text-based, creative domains, since time is often a limited resource. Although numerous digital-interactive Creativity Support Tools (CSTs) are available, writing applications with features to support creative writing as a time-constrained activity have scarcely evolved since the 1960s. This is striking, since studies show that time constraints may benefit creativity. We present an exploratory survey-based, qualitative user experience study of how a writing application prototype designed to accelerate text production by imposing time constraints affects a creative writing task among high school students (n=45). Using thematic analysis, we report how implicit and explicit time constraints built into the GUI (graphical user interface) influence how users experience time pressure and the quantity vs. quality, initiation, and revision of their creative writing process. We discuss how writing applications may develop to incorporate time constraints to support creative writing, as well as the need for increased tool literacy.
SP  - 100
EP  - 107
JF  - Proceedings of the 31st European Conference on Cognitive Ergonomics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3335082.3335084
ER  - 

TY  - CHAP
AU  - Bjørnstada, Nina; Morrison, Andrew
TI  - Additive Manufacturing: Design, Methods, and Processes - Chapter 5: Visual 3D Form in the Context of Additive Manufacturing
PY  - 2017
AB  - NA
SP  - 143
EP  - 164
JF  - Additive Manufacturing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1201/9781315196589-6
ER  - 

TY  - NA
AU  - Jakubovic, Joel; Petricek, Tomas
TI  - Ascending the Ladder to Self-Sustainability: Achieving Open Evolution in an Interactive Graphical System
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3563835.3568736
ER  - 

TY  - NA
AU  - Fuhl, Wolfgang; Schneider, Johannes; Kasneci, Enkelejda
TI  - 1000 Pupil Segmentations in a Second using Haar Like Features and Statistical Learning
PY  - 2021
AB  - NA
SP  - 3466
EP  - 3476
JF  - 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccvw54120.2021.00386
ER  - 

TY  - JOUR
AU  - Kinoshita, Yuichiro; Suzuki, Keisuke; Go, Kentaro
TI  - Impression-Based Fabrication Support for Shadow Box Expressions
PY  - 2020
AB  - NA
SP  - 987
EP  - 997
JF  - Journal of Japan Society for Fuzzy Theory and Intelligent Informatics
VL  - 32
IS  - 6
PB  - 
DO  - 10.3156/jsoft.32.6_987
ER  - 

TY  - NA
AU  - Oney, Steve; Lundgard, Alan; Krosnick, Rebecca; Nebeling, Michael; Lasecki, Walter S.
TI  - UIST - Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture
PY  - 2018
AB  - Many web pages developed today require navigation by visual interaction-seeing, hovering, pointing, clicking, and dragging with the mouse over dynamic page content. These forms of interaction are increasingly popular as developer trends have moved from static, logically structured pages to dynamic, interactive pages. However, they are also often inaccessible to blind web users who tend to rely on keyboard-based screen readers to navigate the web. Despite existing web accessibility standards, engineering web pages to be equally accessible via both keyboard and visuomotor mouse-based interactions is often not a priority for developers. Improving access to this kind of visual and interactive web content has been a long-standing goal of HCI researchers, but the barriers have proven to be too varied and unpredictable to be overcome by some of the proposed solutions: promoting guidelines and best practices, automatically generating accessible versions of pre-exisiting web pages, or developing human-assisted solutions, such as screen and cursor-sharing, which tend to diminish an end user's agency. In this paper we present a real-time, collaborative approach to helping blind web users overcome inaccessible parts of existing web pages. We introduce *Arboretum*, a new architecture that enables any web user to seamlessly hand off controlled parts of their browsing session to remote users, while maintaining control over the interface via a "propose and accept/reject" mechanism. We illustrate the benefit of Arboretum by using it to implement *Arbility*, a browser that allows blind users to hand off targeted visual interaction tasks to remote crowd workers. We evaluate the entire system in a study with 9 blind web users, showing that Arbility allows them to interact with web content that was previously difficult to access via a screen reader alone.
SP  - 937
EP  - 949
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242649
ER  - 

TY  - NA
AU  - Günther, Sebastian; Koutny, Reinhard; Dhingra, Naina; Funk, Markus; Hirt, Christian; Miesenberger, Klaus; Mühlhäuser, Max; Kunz, Andreas
TI  - PETRA - MAPVI: meeting accessibility for persons with visual impairments
PY  - 2019
AB  - In recent years, the inclusion of persons with visual impairments (PVI) is taking tremendous steps, especially with regards to group meetings. However, a significant part of communication is conveyed through non-verbal communication which is commonly inaccessible, such as deictic pointing gestures or the mimics and body language of participants. In this vision paper, we present an overview of our project MAPVI. MAPVI proposes new technologies on making meetings more accessible for PVIs. Therefore, we explore which relevant information has to be tracked and how those can be sensed for the users. Finally, those captured information get translated into a multitude of haptic feedback to make them accessible.
SP  - 343
EP  - 352
JF  - Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3316782.3322747
ER  - 

TY  - JOUR
AU  - Koulieris, George Alex; Akşit, Kaan; Stengel, Michael; Mantiuk, Rafal; Mania, Katerina; Richardt, Christian
TI  - Near-Eye Display and Tracking Technologies for Virtual and Augmented Reality
PY  - 2019
AB  - Virtual and augmented reality (VR/AR) are expected to revolutionise entertainment, healthcare, communication and the manufacturing industries among many others. Near-eye displays are an enabling vessel for VR/AR applications, which have to tackle many challenges related to ergonomics, comfort, visual quality and natural interaction. These challenges are related to the core elements of these near-eye display hardware and tracking technologies. In this state-of-the-art report, we investigate the background theory of perception and vision as well as the latest advancements in display engineering and tracking technologies. We begin our discussion by describing the basics of light and image formation. Later, we recount principles of visual perception by relating to the human visual system. We provide two structured overviews on state-of-the-art near-eye display and tracking technologies involved in such near-eye displays. We conclude by outlining unresolved research questions to inspire the next generation of researchers.
SP  - 493
EP  - 519
JF  - Computer Graphics Forum
VL  - 38
IS  - 2
PB  - 
DO  - 10.1111/cgf.13654
ER  - 

TY  - NA
AU  - Leake, Mackenzie; Shin, Hijung Valentina; Kim, Joy O.; Agrawala, Maneesh
TI  - CHI - Generating Audio-Visual Slideshows from Text Articles Using Word Concreteness
PY  - 2020
AB  - We present a system that automatically transforms text articles into audio-visual slideshows by leveraging the notion of word concreteness, which measures how strongly a word or phrase is related to some perceptible concept. In a formative study we learn that people not only prefer such audio-visual slideshows but find that the content is easier to understand compared to text articles or text articles augmented with images. We use word concreteness to select search terms and find images relevant to the text. Then, based on the distribution of concrete words and the grammatical structure of an article, we time-align selected images with audio narration obtained through text-to-speech to produce audio-visual slideshows. In a user evaluation we find that our concreteness-based algorithm selects images that are highly relevant to the text. The quality of our slideshows is comparable to slideshows produced manually using standard video editing tools, and people strongly prefer our slideshows to those generated using a simple keyword-search based approach.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376519
ER  - 

TY  - NA
AU  - Götzelmann, Timo; Schneider, Daniel
TI  - NordiCHI - CapCodes: Capacitive 3D Printable Identification and On-screen Tracking for Tangible Interaction
PY  - 2016
AB  - Electronic markers can be used to link physical representations and virtual content for tangible interaction, such as visual markers commonly used for tabletops. Another possibility is to leverage capacitive touch inputs of smartphones, tablets and notebooks. However, existing approaches either do not couple physical and virtual representations or require significant post-processing. This paper presents and evaluates a novel approach using a coding scheme for the automatic identification of tangibles by touch inputs when they are touched and shifted. The codes can be generated automatically and integrated into a great variety of existing 3D models from the internet. The resulting models can then be printed completely in one cycle by off-the-shelf 3D printers; post processing is not needed. Besides the identification, the object's position and orientation can be tracked by touch devices. Our evaluation examined multiple variables and showed that the CapCodes can be integrated into existing 3D models and the approach could also be applied to untouched use for larger tangibles.
SP  - 32
EP  - NA
JF  - Proceedings of the 9th Nordic Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971485.2971518
ER  - 

TY  - NA
AU  - Liang, Chen; Guo, Anhong; Kim, Jeeeun
TI  - CustomizAR: Facilitating Interactive Exploration and Measurement of Adaptive 3D Designs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533561
ER  - 

TY  - NA
AU  - Subramonyam, Hariharan; Li, Wilmot; Adar, Eytan; Dontcheva, Mira
TI  - UIST - TakeToons: Script-driven Performance Animation
PY  - 2018
AB  - Performance animation is an expressive method for animating characters through human performance. However, character motion is only one part of creating animated stories. The typical workflow also involves writing a script, coordinating actors, and editing recorded performances. In most cases, these steps are done in isolation with separate tools, which introduces friction and hinders iteration. We propose TakeToons, a script-driven approach that allows authors to annotate standard scripts with relevant animation events like character actions, camera positions, and scene backgrounds. We compile this script into a story model that persists throughout the production process and provides a consistent structure for organizing and assembling recorded performances and propagating script or timing edits to existing recordings. TakeToons enables writing, performing and editing to happen in an integrated and interleaved manner that streamlines production and facilitates iteration. Informal feedback from professional animators suggests that our approach can benefit many existing workflows supporting individual authors and production teams with many different contributors.
SP  - 663
EP  - 674
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242618
ER  - 

TY  - NA
AU  - Uddin, Sami; Gutwin, Carl; Goguey, Alix
TI  - SUI - Using artificial landmarks to improve revisitation performance and spatial learning in linear control widgets
PY  - 2017
AB  - Linear interface controllers such as sliders and scrollbars are primary tools for navigating through linear content such as videos or text documents. Linear control widgets provide an abstract representation of the entire document in the body of the widget, in that they map each document location to a different position of the slider knob or scroll thumb. In most cases, however, these linear mappings are visually undifferentiated - all locations in the widget look the same - and so it can be difficult to build up spatial knowledge of the document, and difficult to navigate back to locations that the user has already visited. In this paper, we examine a technique that can address this problem: artificial landmarks that are added to a linear control widget in order to improve spatial understanding and revisitation. We carried out a study with two types of content (a video, and a PDF document) to test the effects of adding artificial landmarks. We compared standard widgets (with no landmarks) to two augmented designs: one that placed arbitrary abstract icons in the body of the widget, and one that added thumbnails extracted from the document. We found that for both kinds of content, adding artificial landmarks significantly improved revisitation performance and user preference, with the thumbnail landmarks fastest and most accurate in both cases. Our study demonstrates that augmenting linear control widgets with artificial landmarks can provide substantial benefits for document navigation.
SP  - 48
EP  - 57
JF  - Proceedings of the 5th Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131277.3132184
ER  - 

TY  - JOUR
AU  - Li, Jianfeng; Li, Shigang; Chen, Tong; Liu, Yiguang
TI  - A Geometry-Appearance-Based Pupil Detection Method for Near-Infrared Head-Mounted Cameras
PY  - 2018
AB  - This paper presents, for the first time, a novel pupil detection method for near-infrared head-mounted cameras, which relies not only on image appearance to pursue the shape and gradient variation of the pupil contour, but also on structure principle to explore the mechanism of pupil projection. There are three main characteristics in the proposed method. First, in order to complement the pupil projection information, an eyeball center calibration method is proposed to build an eye model. Second, by utilizing the deformation model of pupils under head-mounted cameras and the edge gradients of a circular pattern, we find the best fitting ellipse describing the pupil boundary. Third, an eye-model-based pupil fitting algorithm with only three parameters is proposed to fine-tune the final pupil contour. Consequently, the proposed method extracts the geometry-appearance information, effectively boosting the performance of pupil detection. Experimental results show that this method outperforms the state-of-the-art ones. On a widely used public database (LPW), our method achieves 72.62% in terms of detection rate up to an error of five pixels, which is superior to the previous best one.
SP  - 23242
EP  - 23252
JF  - IEEE Access
VL  - 6
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2828400
ER  - 

TY  - NA
AU  - Santini, Thiago; Fuhl, Wolfgang; Kasneci, Enkelejda
TI  - CHI - CalibMe: Fast and Unsupervised Eye Tracker Calibration for Gaze-Based Pervasive Human-Computer Interaction
PY  - 2017
AB  - As devices around us become smart, our gaze is poised to become the next frontier of human-computer interaction (HCI). State-of-the-art mobile eye tracker systems typically rely on eye-model-based gaze estimation approaches, which do not require a calibration. However, such approaches require specialized hardware (e.g., multiple cameras and glint points), can be significantly affected by glasses, and, thus, are not fit for ubiquitous gaze-based HCI. In contrast, regression-based gaze estimations are straightforward approaches requiring solely one eye and one scene camera but necessitate a calibration. Therefore, a fast and accurate calibration is a key development to enable ubiquitous gaze-based HCI. In this paper, we introduce CalibMe, a novel method that exploits collection markers (automatically detected fiducial markers) to allow eye tracker users to gather a large array of calibration points, remove outliers, and automatically reserve evaluation points in a fast and unsupervised manner. The proposed approach is evaluated against a nine-point calibration method, which is typically used due to its relatively short calibration time and adequate accuracy. CalibMe reached a mean angular error of 0.59 (0=0.23) in contrast to 0.82 (0=0.15) for a nine-point calibration, attesting for the efficacy of the method. Moreover, users are able to calibrate the eye tracker anywhere and independently in - 10 s using a cellphone to display the collection marker.
SP  - 2594
EP  - 2605
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025950
ER  - 

TY  - NA
AU  - Rodriguez, Sarah Delgado; Prange, Sarah; Mecke, Lukas; Alt, Florian
TI  - CHI Extended Abstracts - ActPad– A Smart Desk Platform to Enable User Interaction with IoT Devices
PY  - 2021
AB  - ActPad is a desk pad, capable of sensing capacitive touch input in desk setups. Our prototype can sense touches on both, its electrodes and on connected objects. ActPad’s interaction-space is customizable, allowing easy integration and extension of existing desk environments. In smart environments, users may interact with more than one device at the same time. This generates the need for new interaction mechanisms that bundle the control of multiple ubiquitous devices. We support this need through a platform that extends interaction with IoT devices. ActPad accounts for different ways of controlling IoT devices by enabling various modes of interaction – in particular simultaneous, sequential, implicit and explicit – and, hence, a rich input space. As a proof of concept, we illustrate several use cases, including, but not limited to, controlling the browser on a PC, turning lights on/off, switching songs, or preparing coffee.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451825
ER  - 

TY  - NA
AU  - Narazani, Marla; Eghtebas, Chloe; Klinker, Gudrun; Jenney, Sarah Louise; Mühlhaus, Michael; Petzold, Frank
TI  - UIST (Adjunct Volume) - Extending AR Interaction through 3D Printed Tangible Interfaces in an Urban Planning Context
PY  - 2019
AB  - Embedding conductive material into 3D printed objects enables non-interactive objects to become tangible without the need to attach additional components. We present a novel use for such touch-sensitive objects in an augmented reality (AR) setting and explore the use of gestures for enabling different types of interaction with digital and physical content. In our demonstration, the setting is an urban planning scenario. The multi-material 3D printed buildings consist of thin layers of white plastic filament and a conductive wireframe to enable touch gestures. Attendees can either interact with the physical model or with the mobile AR interface for selecting, adding or deleting buildings.
SP  - 116
EP  - 118
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356891
ER  - 

TY  - CHAP
AU  - Zhang, Jianming; Malmberg, Filip; Sclaroff, Stan
TI  - Boolean Map Saliency: A Surprisingly Simple Method
PY  - 2019
AB  - In this chapter, we propose a simple yet powerful saliency detection model for eye fixation prediction based on the surroundedness cue for figure-ground segregation. The essence of surroundedness is the enclosure topological relationship between different visual components. This kind of topological relationship is invariant under homeomorphisms; thus it is a quite fundamental property of a scene, regardless of the scale or the shape of the visual content. It is also worth noting that the topological status of a scene has long been identified as one of the probable attributes that guide the deployment of visual attention.
SP  - 11
EP  - 31
JF  - Visual Saliency: From Pixel-Level to Object-Level Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-04831-0_2
ER  - 

TY  - NA
AU  - Yang, Songzhou; He, Yuan; Jin, Meng
TI  - INFOCOM - vGaze: Implicit Saliency-Aware Calibration for Continuous Gaze Tracking on Mobile Devices
PY  - 2021
AB  - Gaze tracking is a useful human-to-computer interface, which plays an increasingly important role in a range of mobile applications. Gaze calibration is an indispensable component of gaze tracking, which transforms the eye coordinates to the screen coordinates. The existing approaches of gaze tracking either have limited accuracy or require the user’s cooperation in calibration and in turn hurt the quality of experience. We in this paper propose vGaze, implicit saliency-aware calibration for continuous gaze tracking on mobile devices. The design of vGaze stems from our insight on the temporal and spatial dependent relation between the visual saliency and the user’s gaze. vGaze is implemented as a light-weight software that identifies video frames with "useful" saliency information, sensing the user’s head movement, and performs opportunistic calibration using only those "useful" frames. We implement vGaze on a commercial mobile device and evaluate its performance in various scenarios. The results show that vGaze can work at real time with video playback applications. The average error of gaze tracking is 1.51cm.
SP  - 1
EP  - 10
JF  - IEEE INFOCOM 2021 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom42981.2021.9488668
ER  - 

TY  - NA
AU  - Nouwens, Midas; Klokmose, Clemens Nylandsted
TI  - CHI - The Application and Its Consequences for Non-Standard Knowledge Work
PY  - 2018
AB  - Application-centric computing dominates human-computer interactions, yet the concept of an application is ambiguous and the impact of its ubiquity underexplored. We unpack "the application" through the lens of non-standard knowledge work: freelance, self-employed, and fixed-term contract workers who create knowledge in collaboration with a wide variety of stakeholders on a per-project basis. Based on interviews with fourteen participants we describe how: i) their economic value is intertwined with data and skills related to specific applications; ii) their access to this value is systematically jeopardised in collaboration due to the different application practices, preferences, and proficiencies of other stakeholders; and iii) they mitigate the costs of this compromise through cross-application collaboration strategies. We trace these experiences to common characteristics of applications, such as update processes, interface symmetries, application-document relationships, and operating system and hardware dependencies. By empirically and analytically focusing on "the application", we reveal the implications of the current application-centric computing paradigm and discuss how variations within this model create qualitatively different human-computer interactions.
SP  - 399
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173973
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Wagner, Tobias; Gugenheimer, Jan; Rukzio, Enrico
TI  - ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact Retrieval using Mixed Reality
PY  - 2022
AB  - Personal fabrication is made more accessible through repositories like Thingiverse, as they replace modeling with retrieval. However, they require users to translate spatial requirements to keywords, which paints an incomplete picture of physical artifacts: proportions or morphology are non-trivially encoded through text only. We explore a vision of in-situ spatial search for (future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to search for 3D models using in-situ sketches blended with textual queries. With ShapeFindAR, users search for geometry, and not necessarily precise labels, while coupling the search process to the physical environment (e.g., by sketching in-situ, extracting search terms from objects present, or tracing them). We developed ShapeFindAR for HoloLens 2, connected to a database of 3D-printable artifacts. We specify in-situ spatial search, describe its advantages, and present walkthroughs using ShapeFindAR, which highlight novel ways for users to articulate their wishes, without requiring complex modeling tools or profound domain knowledge.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517682
ER  - 

TY  - NA
AU  - Eagan, James
TI  - Malleable User Interface Toolkits for Cross-Surface Interaction
PY  - 2017
AB  - Existing user interface toolkits are based on a single user interacting with a single machine with a relatively fixed set of input devices. Today's interactive systems, however, can involve multiple users interacting with a heterogeneous set of input, computational, and output capabilities across a dynamic set of different devices. The abstractions that help programmers create interactive software for one kind of system do not necessarily scale to these new kinds of environments. New toolkits designed around these environments, however, need to be able to bridge existing software and libraries or recreate them from scratch. In this position paper, we examine these new constraints and needs. We look at three strategies for software toolkits that help to bridge existing toolkit models to these new interaction paradigms.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Musaeus, Line Have; Sørensen, Marie-Louise Stisen Kjerstein; Palfi, Blanka Sára; Iversen, Ole Sejer; Klokmose, Clemens Nylandsted; Petersen, Marianne Graves
TI  - CoTinker: Designing a Cross-device Collaboration Tool to Support Computational Thinking in Remote Group Work in High School Biology
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546155.3546709
ER  - 

TY  - NA
AU  - Gustafsson, Viktor; Holme, Benjamin; Mackay, Wendy E.
TI  - FDG - Narrative Substrates: Reifying and Managing Emergent Narratives in Persistent Game Worlds
PY  - 2020
AB  - Players in modern Massively Multiplayer Online Role-Playing Games progress through ambitiously designed narratives, but have no real influence on the game, since only their characters’ data, not the game environment, persists. Although earlier games supported player influence by persisting changes in the world, they relied on players’ capacity to form their own stories and lacked guidance for character progression. We explore how persistence and narrative emergence let us build upon players’ influence rather than restrict it. We ran four studies and found that players highly value first-time and unique events, and often externalize their experiences to the Web to collaborate and socialize, but unintentionally also disrupt some aspects of in-game play. We introduce Narrative Substrates, a theoretical framework for designing game architectures that represent, manage, and persist traces of player activity as unique, interactive content. To illustrate and test the theory, we developed the game We Ride and deployed it as a two-phase technology probe over one year. We identify key benefits and challenges of our approach, and argue that reification of emergent narratives offers new design opportunities for creating truly interactive games.
SP  - 46
EP  - NA
JF  - International Conference on the Foundations of Digital Games
VL  - 12
IS  - NA
PB  - 
DO  - 10.1145/3402942.3403015
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Kim, Jeeeun
TI  - Designing a Hairy Haptic Display using 3D Printed Hairs and Perforated Plates
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558655
ER  - 

TY  - NA
AU  - Thakur, Sanket Kumar; Beyan, Cigdem; Morerio, Pietro; Del Bue, Alessio
TI  - ICMI - Predicting Gaze from Egocentric Social Interaction Videos and IMU Data
PY  - 2021
AB  - Gaze prediction in egocentric videos is a fairly new research topic, which might have several applications for assistive technology (e.g., supporting blind people in their daily interactions), security (e.g., attention tracking in risky work environments), education (e.g., augmented / mixed reality training simulators, immersive games) and so forth. Egocentric gaze is typically estimated from video while few works attempt to use inertial measurement unit (IMU) data, a sensor modality often available in wearable devices (e.g., augmented reality headsets). Instead, in this paper, we examine whether joint learning of egocentric video and corresponding IMU data can improve the first-person gaze prediction compared to using these modalities separately. In this respect, we propose a multimodal network and evaluate it on several unconstrained social interaction scenarios captured by a first-person perspective. The proposed multimodal network achieves better results compared to unimodal methods as well as several (multimodal) baselines, showing that using egocentric video together with IMU data can boost the first-person gaze estimation performance.
SP  - 717
EP  - 722
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3479954
ER  - 

TY  - NA
AU  - Kato, Jun; Goto, Masataka
TI  - VL/HCC - DeployGround: A Framework for Streamlined Programming from API playgrounds to Application Deployment
PY  - 2018
AB  - Interactive web pages for learning programming languages and application programming interfaces (APIs), called “playgrounds,” allow programmers to run and edit example codes in place. Despite the benefits of this live programming experience, programmers need to leave the playground at some point and restart the development from scratch in their own programming environments. This paper proposes “DeployGround,” a framework for creating web-based tutorials that streamlines learning APIs on playgrounds and developing and deploying applications. As a case study, we created a web-based tutorial for browser-based and Node.js-based JavaScript APIs. A preliminary user study found appreciation of the streamlined and social workflow of the DeplovGround framework.
SP  - 259
EP  - 263
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506562
ER  - 

TY  - NA
AU  - Klokmose, Clemens Nylandsted; Remy, Christian; Kristensen, Janus Bager; Bagge, Rolf; Beaudouin-Lafon, Michel; Mackay, Wendy E.
TI  - UIST - Videostrates: Collaborative, Distributed and Programmable Video Manipulation
PY  - 2019
AB  - We present Videostrates, a concept and a toolkit for creating real-time collaborative video editing tools. Videostrates supports both live and recorded video composition with a declarative HTML-based notation, combining both simple and sophisticated editing tools that can be used collaboratively. Videostrates is programmable and unleashes the power of the modern web platform for video manipulation. We demonstrate its potential through three use scenarios: collaborative video editing with multiple tools and devices; orchestration of multiple live streams that are recorded and broadcast to a popular streaming platform; and programmatic creation of video using WebGL and shaders for blue screen effects. These scenarios only scratch the surface of Videostrates' potential, which opens up a design space for novel collaborative video editors with fully programmable interfaces.
SP  - 233
EP  - 247
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - 2019
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347912
ER  - 

TY  - JOUR
AU  - Vieira, E. M. F.; Silva, José; Veltruska, Katerina; Matolín, Vladimír; Pires, Ana L.; Pereira, André; Gomes, M. J. M.; Gonçalves, Luís
TI  - Highly sensitive thermoelectric touch sensor based on p-type SnO x thin film.
PY  - 2019
AB  - Here, the ability of using p-type tin oxide (SnOx) thin films as a thermal sensor has been investigated. Firstly, the thermoelectric performance was optimized by controlling the thickness of the SnOx film from 60 up to 160 nm. A high Seebeck coefficient of + 263 ?VK-1 and electrical conductivity of 4.1 × 102 (S m-1) were achieved in a 60 nm-thick SnOx film, due to a compact nanostructured film and the absence of the Sn metallic phase, which was observed for the thicker SnOx film leading to a typical thermoelectric (TE) transport properties of a n-type Sn film. Moreover, X-ray photoelectron spectroscopy revealed the co-existence of SnO (79.7%) and SnO2 (20.3%) phases in the 60-nm thick SnOx film, while the optical measurements revealed an indirect gap of 1.8 eV and a direct gap of 2.7 eV, respectively. The 60 nm ? SnOx thin film have been tested as a thermoelectric touch sensor, achieving a Vsignal/Vnoise ? 20, with a rise time alt; 1 s. Therefore, this work provides an efficient way for developing highly efficient thermal sensors with potential use in display technologies.a#13; a#13;
SP  - 435502
EP  - 435502
JF  - Nanotechnology
VL  - 30
IS  - 43
PB  - 
DO  - 10.1088/1361-6528/ab33dd
ER  - 

TY  - NA
AU  - Shi, Lei; Lawson, Holly; Zhang, Zhuohao; Azenkot, Shiri
TI  - CHI - Designing Interactive 3D Printed Models with Teachers of the Visually Impaired
PY  - 2019
AB  - Students with visual impairments struggle to learn various concepts in the academic curriculum because diagrams, images, and other visual are not accessible to them. To address this, researchers have design interactive 3D printed models (I3Ms) that provide audio descriptions when a user touches components of a model. In prior work, I3Ms were designed on an ad hoc basis, and it is currently unknown what general guidelines produce effective I3M designs. To address this gap, we conducted two studies with Teachers of the Visually Impaired (TVIs). First, we led two design workshops with 35 TVIs, who modified sample models and added interactive elements to them. Second, we worked with three TVIs to design three I3Ms in an iterative instructional design process. At the end of this process, the TVIs used the I3Ms we designed to teach their students. We conclude that I3Ms should (1) have effective tactile features (e.g., distinctive patterns between components), (2) contain both auditory and visual content (e.g., explanatory animations), and (3) consider pedagogical methods (e.g., overview before details).
SP  - 197
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300427
ER  - 

TY  - NA
AU  - Park, Seonwook; Gebhardt, Christoph; Rädle, Roman; Feit, Anna Maria; Vrzakova, Hana; Dayama, Niraj Ramesh; Yeo, Hui-Shyong; Klokmose, Clemens Nylandsted; Quigley, Aaron; Oulasvirta, Antti; Hilliges, Otmar
TI  - CHI - AdaM: Adapting Multi-User Interfaces for Collaborative Environments in Real-Time
PY  - 2018
AB  - Developing cross-device multi-user interfaces (UIs) is a challenging problem. There are numerous ways in which content and interactivity can be distributed. However, good solutions must consider multiple users, their roles, their preferences and access rights, as well as device capabilities. Manual and rule-based solutions are tedious to create and do not scale to larger problems nor do they adapt to dynamic changes, such as users leaving or joining an activity. In this paper, we cast the problem of UI distribution as an assignment problem and propose to solve it using combinatorial optimization. We present a mixed integer programming formulation which allows real-time applications in dynamically changing collaborative settings. It optimizes the allocation of UI elements based on device capabilities, user roles, preferences, and access rights. We present a proof-of-concept designer-in-the-loop tool, allowing for quick solution exploration. Finally, we compare our approach to traditional paper prototyping in a lab study.
SP  - 184
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173758
ER  - 

TY  - NA
AU  - Elkin, Lisa A.; Beau, Jean-Baptiste; Casiez, Géry; Vogel, Daniel
TI  - CHI - Manipulation, Learning, and Recall with Tangible Pen-Like Input
PY  - 2020
AB  - We examine two key human performance characteristics of a pen-like tangible input device that executes a different command depending on which corner, edge, or side contacts a surface. The manipulation time when transitioning between contacts is examined using physical mock-ups of three representative device sizes and a baseline pen mock-up. Results show the largest device is fastest overall and minimal differences with a pen for equivalent transitions. Using a hardware prototype able to sense all 26 different contacts, a second experiment evaluates learning and recall. Results show almost all 26 contacts can be learned in a two-hour session with an average of 94% recall after 24 hours. The results provide empirical evidence for the practicality, design, and utility for this type of tangible pen-like input.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376772
ER  - 

TY  - JOUR
AU  - Shi, Peiteng; Billeter, Markus; Eisemann, Elmar
TI  - SalientGaze: Saliency-based gaze correction in virtual reality
PY  - 2020
AB  - NA
SP  - 83
EP  - 94
JF  - Computers & Graphics
VL  - 91
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2020.06.007
ER  - 

TY  - BOOK
AU  - Fox, Amy Rae; Guo, Philip J.; Klokmose, Clemens Nylandsted; Dalsgaard, Peter; Satyanarayan, Arvind; Xia, Haijun; Hollan, James D.
TI  - Programming - Towards a dynamic multiscale personal information space: beyond application and document centered views of information
PY  - 2020
AB  - The historical moment when a person worked in front of a single computer has passed. Computers are now ubiquitous and embedded in virtually every new device and system, connecting our personal and professional activities to ever-expanding information resources with previously unimaginable computational power. Yet with all the increases in capacity, speed, and connectivity, our experiences too often remain difficult, awkward, and frustrating. Even after six decades of design evolution there is little of the naturalness and contextual sensitivity required for convivial interaction with computer-mediated information. We envision a future in which the existing world of documents and applications is linked to a multiscale personalized information space in which dynamic visual entities behave in accordance with cognitively motivated rules sensitive to tasks, personal and group interaction histories, and context. The heart of the project is to rethink the nature of computer-mediated information as a basis to begin to fully realize the potential of computers to assist information-based activities. This requires challenging fundamental presuppositions that have led to today’s walled gardens and information silos. Our goal is to catalyze an international research community to rethink the nature of information as a basis for radically advancing the human-centered design of information-based work and helping to ensure the future is one of convivial, effective, and humane systems. In this paper, we propose a new view of information systems, discuss cognitive requirements for a human-centered information space, and sketch a research agenda and approach.
SP  - 136
EP  - 143
JF  - Conference Companion of the 4th International Conference on Art, Science, and Engineering of Programming
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397537.3397542
ER  - 

TY  - JOUR
AU  - Mallalieu, A.; Hajali, T.; Isaksson, O.; Panarotto, M.
TI  - The Role of Digital Infrastructure for the Industrialisation of Design for Additive Manufacturing
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>The use of Additive Manufacturing (AM) can bring opportunities for industry, but several challenges need to be addressed, specifically the digital infrastructure comprising the AM value chain. A combination of a systematic literature review and an industrial use case study concludes that there is low consideration of the digital infrastructure in Design for Additive Manufacturing (DfAM) methods and tools which has a negative impact on the industrialisation of AM. It is therefore recommended that further studies are to be made on how to manage the digital infrastructure in DfAM processes.</jats:p>
SP  - 1401
EP  - 1410
JF  - Proceedings of the Design Society
VL  - 2
IS  - NA
PB  - 
DO  - 10.1017/pds.2022.142
ER  - 

TY  - NA
AU  - Pavel, Amy; Reyes, Gabriel; Bigham, Jeffrey P.
TI  - Rescribe: Authoring and Automatically Editing Audio Descriptions
PY  - 2020
AB  - Audio descriptions make videos accessible to those who cannot see them by describing visual content in audio. Producing audio descriptions is challenging due to the synchronous nature of the audio description that must fit into gaps of other video content. An experienced audio description author will produce content that fits narration necessary to understand, enjoy, or experience the video content into the time available. This can be especially tricky for novices to do well. In this paper, we introduce a tool, Rescribe, that helps authors create and refine their audio descriptions. Using Rescribe, authors first create a draft of all the content they would like to include in the audio description. Rescribe then uses a dynamic programming approach to optimize between the length of the audio description, available automatic shortening approaches, and source track lengthening approaches. Authors can iteratively visualize and refine the audio descriptions produced by Rescribe, working in concert with the tool. We evaluate the effectiveness of Rescribe through interviews with blind and visually impaired audio description users who preferred Rescribe-edited descriptions to extended descriptions. In addition, we invite novice users to create audio descriptions with Rescribe and another tool, finding that users produce audio descriptions with fewer placement errors using Rescribe.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Hamasaki, Takumi; Itoh, Yuta; Hiroi, Yuichi; Iwai, Daisuke; Sugimoto, Maki
TI  - HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection
PY  - 2018
AB  - Spatial augmented reality (SAR) pursues realism in rendering materials and objects. To advance this goal, we propose a hybrid SAR (HySAR) that combines a projector with optical see-through head-mounted displays (OST-HMD). In an ordinary SAR scenario with co-located viewers, the viewers perceive the same virtual material on physical surfaces. In general, the material consists of two components: a view-independent (VI) component such as diffuse reflection, and a view-dependent (VD) component such as specular reflection. The VI component is static over viewpoints, whereas the VD should change for each viewpoint even if a projector can simulate only one viewpoint at one time. In HySAR, a projector only renders the static VI components. In addition, the OST-HMD renders the dynamic VD components according to the viewer's current viewpoint. Unlike conventional SAR, the HySAR concept theoretically allows an unlimited number of co-located viewers to see the correct material over different viewpoints. Furthermore, the combination enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With proof-of-concept systems, we demonstrate HySAR both qualitatively and quantitatively with real objects. First, we demonstrate HySAR by rendering synthetic material properties on a real object from different viewpoints. Our quantitative evaluation shows that our system increases the dynamic range by 2.24 times and the maximum intensity by 2.12 times compared to an ordinary SAR system. Second, we replicate the material properties of a real object by SAR and HySAR, and show that HySAR outperforms SAR in rendering VD specular components.
SP  - 1457
EP  - 1466
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 4
PB  - 
DO  - 10.1109/tvcg.2018.2793659
ER  - 

TY  - NA
AU  - Lu, Qian; Darnal, Aryabhat; Takahashi, Haruki; Muliana, Anastasia Hanifah; Kim, Jeeeun
TI  - User-Centered Property Adjustment with Programmable Filament
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519864
ER  - 

TY  - JOUR
AU  - Santini, Thiago; Fuhl, Wolfgang; Kasneci, Enkelejda
TI  - PuRe: Robust pupil detection for real-time pervasive eye tracking
PY  - 2018
AB  - Abstract Real-time, accurate, and robust pupil detection is an essential prerequisite to enable pervasive eye-tracking and its applications – e.g., gaze-based human computer interaction, health monitoring, foveated rendering, and advanced driver assistance. However, automated pupil detection has proved to be an intricate task in real-world scenarios due to a large mixture of challenges such as quickly changing illumination and occlusions. In this paper, we introduce the Pu pil Re constructor (PuRe), a method for pupil detection in pervasive scenarios based on a novel edge segment selection and conditional segment combination schemes; the method also includes a confidence measure for the detected pupil. The proposed method was evaluated on over 316,000 images acquired with four distinct head-mounted eye tracking devices. Results show a pupil detection rate improvement of over 10 percentage points w.r.t. state-of-the-art algorithms in the two most challenging data sets (6.46 for all data sets), further pushing the envelope for pupil detection. Moreover, we advance the evaluation protocol of pupil detection algorithms by also considering eye images in which pupils are not present and contributing a new data set of mostly closed eyes images. In this aspect, PuRe improved precision and specificity w.r.t. state-of-the-art algorithms by 25.05 and 10.94 percentage points, respectively, demonstrating the meaningfulness of PuRe’s confidence measure. PuRe operates in real-time for modern eye trackers (at 120 fps) and is fully integrated into EyeRecToo – an open-source state-of-the-art software for pervasive head-mounted eye tracking. The proposed method and data set are available at http://www.ti.uni-tuebingen.de/perception .
SP  - 40
EP  - 50
JF  - Computer Vision and Image Understanding
VL  - 170
IS  - NA
PB  - 
DO  - 10.1016/j.cviu.2018.02.002
ER  - 

TY  - BOOK
AU  - Ducros, Alix; Klokmose, Clemens Nylandsted; Tabard, Aurélien
TI  - ISS - Situated Sketching and Enactment for Pervasive Displays
PY  - 2019
AB  - Situated sketching and enactment aim at grounding designs in the spatial, social and cultural practices of a particular place. This is particularly relevant when designing for public places in which human activities are open-ended, multi-faceted, and difficult to anticipate, such as libraries, train stations, or commercial areas. In order to investigate situated sketching and enactment, we developed Ebauche. It enables designers to collaboratively sketch interfaces, distribute them across multiple displays and enact use cases. We present the lessons learned from six situated sketching and enactment workshops on public displays with Ebauche. And we present the results of a controlled study with 8 pairs of designers who used paper and Ebauche. We present the various ways in which participants leveraged the place, and how paper or Ebauche influenced the integration of their designs in the place. Looking at the design outcomes, our results suggest that paper leads to broader exploration of ideas and deeper physical integration in the environment. Whereas Ebauche leads to more refined sketches and more animated enactments.
SP  - 217
EP  - 228
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359702
ER  - 

TY  - BOOK
AU  - Fuhl, Wolfgang; Gao, Hong; Kasneci, Enkelejda
TI  - ETRA Short Papers - Neural networks for optical vector and eye ball parameter estimation
PY  - 2020
AB  - In this work we evaluate neural networks, support vector machines and decision trees for the regression of the center of the eyeball and the optical vector based on the pupil ellipse. In the evaluation we analyze single ellipses as well as window-based approaches as input. Comparisons are made regarding accuracy and runtime. The evaluation gives an overview of the general expected accuracy with different models and amounts of input ellipses. A simulator was implemented for the generation of the training and evaluation data. For a visual evaluation and to push the state of the art in optical vector estimation, the best model was applied to real data. This real data came from public data sets in which the ellipse is already annotated by an algorithm. The optical vectors on real data and the generator are made publicly available. Link to the generator and models.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379156.3391346
ER  - 

TY  - NA
AU  - Schmitz, Martin
TI  - Tangible and Embedded Interaction - Exploring 3D Printed Interaction
PY  - 2016
AB  - 3D printing is widely used to physically prototype the look and feel of 3D objects. However, interaction possibilities of these prototypes are often limited to mechanical parts or post-assembled electronics. Moreover, fabricating interactive 3D printed objects is still an expert task. In my dissertation, I therefore explore how to support users in the design of interactive 3D objects and how to automate the generation of adequate sensing structures. Further, I investigate tangible interaction concepts for 3D printed objects. In this paper, I outline my past and future research towards the fabrication of 3D objects in terms of (1) user-friendly design, (2) automation of fabrication, and (3) tangible interaction concepts for the input modalities touch and deformation.
SP  - 705
EP  - 708
JF  - Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2839462.2854105
ER  - 

TY  - NA
AU  - Robinson, Simon; Pearson, Jennifer; Jones, Matt; Joshi, Anirudha; Ahire, Shashank
TI  - MobileHCI - Better together: disaggregating mobile services for emergent users
PY  - 2017
AB  - Mainstream mobile interactions are focused around individual devices, with any collaboration happening via `the cloud'. We carried out design workshops with emergent users, revealing opportunities for novel collocated collaborative interactions. In this paper we present Better Together - a framework for disaggregating services, splitting interaction elements over separate mobiles. This distribution supports both sharing of resources (such as screen real-estate, or mobile data); and, scaffolding of inclusive interaction in mixed groups (e.g., in terms of literacy or prior technology exposure). We developed two prototypes to explore the concept, trialling the first---collocated group-based shopping list making---with emergent users in South Africa and India. We deployed the second probe, which splits YouTube into its constituent parts across separate mobiles, in a longitudinal study with users in Kenya, South Africa and India. We describe the concept and design process, and report on the design's suitability for emergent users based on our results.
SP  - 44
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3098534
ER  - 

TY  - NA
AU  - Ikegawa, Koshi; Aoyama, Shuhei; Tsuchikiri, Shogo; Nakamura, Takuto; Hashimoto, Yuki; Shizuki, Buntarou
TI  - Tangible and Embedded Interaction - Investigation of Touch Interfaces Using Multilayered Urushi Circuit
PY  - 2018
AB  - Urushi (Japanese lacquer) is a natural resin paint with electrical insulating capability. By using it as a base material and coating material for electronic circuits, it is possible to construct a circuit with an elegant appearance and feel. It is also possible to build a multilayered electronic circuit by using urushi as insulation layers. In this study, we investigate techniques to construct touch interfaces using a multilayered electronic circuit (urushi circuit). At first, we fabricated an urushi circuit with a touch electrode consisting of two layers. To improve its appearance, we fabricated urushi circuits in which all touch electrodes are arranged on the top layer and all wires are hidden in the bottom layer. Moreover, as an extension of the touch interface, we built a grid of touch electrodes that realizes two-dimensional touch sensing.
SP  - 115
EP  - 122
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173285
ER  - 

TY  - NA
AU  - Gu, Jianzhe; Breen, David E.; Hu, Jenny; Zhu, Lifeng; Tao, Ye; Van de Zande, Tyson; Wang, Guanyun; Zhang, Yongjie Jessica; Yao, Lining
TI  - CHI - Geodesy: Self-rising 2.5D Tiles by Printing along 2D Geodesic Closed Path
PY  - 2019
AB  - Thermoplastic and Fused Deposition Modeling (FDM) based 4D printing are rapidly expanding to allow for space- and material-saving 2D printed sheets morphing into 3D shapes when heated. However, to our knowledge, all the known examples are either origami-based models with obvious folding hinges, or beam-based models with holes on the morphing surfaces. Morphing continuous double-curvature surfaces remains a challenge, both in terms of a tailored toolpath-planning strategy and a computational model that simulates it. Additionally, neither approach takes surface texture as a design parameter in its computational pipeline. To extend the design space of FDM-based 4D printing, in Geodesy, we focus on the morphing of continuous double-curvature surfaces or surface textures. We suggest a unique tool path - printing thermoplastics along 2D closed geodesic paths to form a surface with one raised continuous double-curvature tiles when exposed to heat. The design space is further extended to more complex geometries composed of a network of rising tiles (i.e., surface textures). Both design components and the computational pipeline are explained in the paper, followed by several printed geometric examples.
SP  - 37
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300267
ER  - 

TY  - NA
AU  - Li, Jiannan; Lyu, Jiahe; Sousa, Maurício; Balakrishnan, Ravin; Tang, Anthony; Grossman, Tovi
TI  - UIST - Route Tapestries: Navigating 360° Virtual Tour Videos Using Slit-Scan Visualizations
PY  - 2021
AB  - An increasingly popular way of experiencing remote places is by viewing 360° virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360° video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating 360°virtual tour videos.
SP  - 223
EP  - 238
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474746
ER  - 

TY  - BOOK
AU  - Bâce, Mihai; Staal, Sander; Sörös, Gábor
TI  - ETRA - Wearable eye tracker calibration at your fingertips
PY  - 2018
AB  - Common calibration techniques for head-mounted eye trackers rely on markers or an additional person to assist with the procedure. This is a tedious process and may even hinder some practical applications. We propose a novel calibration technique which simplifies the initial calibration step for mobile scenarios. To collect the calibration samples, users only have to point with a finger to various locations in the scene. Our vision-based algorithm detects the users' hand and fingertips which indicate the users' point of interest. This eliminates the need for additional assistance or specialized markers. Our approach achieves comparable accuracy to similar marker-based calibration techniques and is the preferred method by users from our study. The implementation is openly available as a plugin for the open-source Pupil eye tracking platform.
SP  - 22
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204592
ER  - 

TY  - NA
AU  - Wang, Xiyue; Ikematsu, Kaori; Fujita, Kazuyuki; Takashima, Kazuki; Kitamura, Yoshifumi
TI  - UIST (Adjunct Volume) - An Investigation of Electrode Design for Physical Touch Extensions on a Capacitive Touch Surface
PY  - 2019
AB  - A simple way to prototype touch interaction is to extend electrodes from a capacitive touch screen to off-screen areas. With that we aim to develop a toolkit that transforms a user-designed layout into a layout of screen-extension electrodes that realizes touch for rapid prototyping. Nevertheless, this kind of extension cannot detect touch if the physical properties of the electrodes become large. In this work, we decompose the physical design properties of the extension electrode into two factors, the target area, and line bridge, and investigate the limitation of each separately. While revealing some factors, such as area, is extremely limited in term of designing freely, we look into the causes by measuring the capacitive charge on-screen and on-extensions.
SP  - 66
EP  - 68
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357117
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Miyashita, Homei
TI  - UIST (Adjunct Volume) - Thickness Control Technique for Printing Tactile Sheets with Fused Deposition Modeling
PY  - 2016
AB  - We present a printing technique that controls the thickness of objects by increasing and decreasing the amount of material extruded during printing. Using this technique, printers can dynamically control thickness and output thicker objects without a staircase effect. This technique allows users to print aesthetic pattern sheets and objects that are tactile without requiring any new hardware. This extends the capabilities of fused deposition modeling (FDM) 3D printers in a simple way. We describe a method of generating and calculating a movement path for printing tactile sheets, and demonstrate the usage and processing of example objects.
SP  - 51
EP  - 53
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985701
ER  - 

TY  - NA
AU  - Gadea, Cristian; Ionescu, Bogdan; Ionescu, Dan
TI  - CIC - New Algorithms and Methods for Collaborative Co-Editing Using HTML DOM Synchronization
PY  - 2018
AB  - The optimistic consistency control method known as Operational Transformation (OT) has been studied by researchers for nearly three decades, with centralized versions lying at the heart of most real-time web co-editing tools in academia and industry. Concurrent document editing is now a "must-have" for the modern workplace, with proven benefits in team productivity and efficiency. Once limited to primitive insert and delete operations, OT algorithms have evolved to support hierarchical data structures such as XML in order to meet the increasingly complex requirements of present-day collaborative applications. However, previous approaches have not focused on the changes that web applications enact upon the Document Object Model (DOM) of the Hypertext Markup Language (HTML) standard. This paper will present a feedback-based real-time architecture that allows arbitrary DOM-based document replicas to remain consistent by defining a new set of operations that preserve the user's editing intentions. The control loop of the architecture enables simultaneous DOM-based modifications by using novel conflict resolution algorithms and methods that bring "Virtual DOM" concepts together with state-of-the-art OT principles to enable advanced operations such as moving, splitting and merging of hierarchical DOM nodes. Through the implementation and evaluation of a rich-text editor, it will be shown how the architecture facilitates and accelerates the development of multi-user interactive web applications that meet today's demanding latency, scalability and accessibility requirements.
SP  - 217
EP  - 226
JF  - 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cic.2018.00038
ER  - 

TY  - JOUR
AU  - Liu, Gang; Yu, Yu; Mora, Kenneth Alberto Funes; Odobez, Jean-Marc
TI  - A Differential Approach for Gaze Estimation
PY  - 2021
AB  - Most non-invasive gaze estimation methods regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as subject dependent biases. Thus, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to her actual gaze. In this article, we introduce a novel approach, which works by directly training a differential convolutional neural network to predict gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by comparing eye images of the same user, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Furthermore, the differential network itself can be adapted via finetuning to make predictions consistent with the available user reference pairs. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or those relying on subject specific gaze adaptation.
SP  - 1092
EP  - 1099
JF  - IEEE transactions on pattern analysis and machine intelligence
VL  - 43
IS  - 3
PB  - 
DO  - 10.1109/tpami.2019.2957373
ER  - 

TY  - JOUR
AU  - Cohen-Kalaf, Miki; Lanir, Joel; Bak, Peter; Mokryn, Osnat
TI  - Movie emotion map: an interactive tool for exploring movies according to their emotional signature
PY  - 2021
AB  - NA
SP  - 14663
EP  - 14684
JF  - Multimedia Tools and Applications
VL  - 81
IS  - 11
PB  - 
DO  - 10.1007/s11042-021-10803-5
ER  - 

TY  - NA
AU  - Schmitz, Martin; Leister, Andreas; Dezfuli, Niloofar; Riemann, Jan; Müller, Florian; Mühlhäuser, Max
TI  - CHI Extended Abstracts - Liquido: Embedding Liquids into 3D Printed Objects to Sense Tilting and Motion
PY  - 2016
AB  - Tilting and motion are widely used as interaction modalities in smart objects such as wearables and smart phones (e.g., to detect posture or shaking). They are often sensed with accelerometers. In this paper, we propose to embed liquids into 3D printed objects while printing to sense various tilting and motion interactions via capacitive sensing. This method reduces the assembly effort after printing and is a low-cost and easy-to-apply way of extending the input capabilities of 3D printed objects. We contribute two liquid sensing patterns and a practical printing process using a standard dual-extrusion 3D printer and commercially available materials. We validate the method by a series of evaluations and provide a set of interactive example applications.
SP  - 2688
EP  - 2696
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892275
ER  - 

TY  - JOUR
AU  - Palma, Gianpaolo; Perry, Sara; Cignoni, Paolo
TI  - Augmented Virtuality Using Touch-Sensitive 3D-Printed Objects
PY  - 2021
AB  - Virtual reality (VR) technologies have become more and more affordable and popular in the last five years thanks to hardware and software advancements. A critical issue for these technologies is finding paradigms that allow user interactions in ways that are as similar as possible to the real world, bringing physicality into the experience. Current literature has shown, with different experiments, that the mapping of real objects in virtual reality alongside haptic feedback significantly increases the realism of the experience and user engagement, leading to augmented virtuality. In this paper, we present a system to improve engagement in a VR experience using inexpensive, physical, and sensorized copies of real artefacts made with cheap 3D fabrication technologies. Based on a combination of hardware and software components, the proposed system gives the user the possibility to interact with the physical replica in the virtual environment and to see the appearance of the original cultural heritage artefact. In this way, we overcome one of the main limitations of mainstream 3D fabrication technologies: a faithful appearance reproduction. Using a consumer device for the real-time hand tracking and a custom electronic controller for the capacitive touch sensing, the system permits the creation of augmented experiences where the user with their hands can change the virtual appearance of the real replica object using a set of personalization actions selectable from a physical 3D-printed palette.
SP  - 2186
EP  - NA
JF  - Remote Sensing
VL  - 13
IS  - 11
PB  - 
DO  - 10.3390/rs13112186
ER  - 

TY  - JOUR
AU  - Butcher, Peter; John, Nigel W.; Ritsos, Panagiotis D.
TI  - VRIA: A Web-Based Framework for Creating Immersive Analytics Experiences
PY  - 2021
AB  - We present VRIA, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. VRIA is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML Document Object Model (DOM). This makes VRIA ubiquitous and platform-independent. Moreover, by using WebVR's progressive enhancement, the experiences VRIA creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on open-standards Web technologies, present the VRIA creation workflow and detail the underlying mechanics of our framework. We also report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability implications of our framework, and present a series of use case applications to demonstrate the various features of VRIA. Finally, we discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.
SP  - 3213
EP  - 3225
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2020.2965109
ER  - 

TY  - JOUR
AU  - Jensen, Mads Møller; Thiel, Sarah-Kristin; Hoggan, Eve; Bødker, Susanne
TI  - Physical Versus Digital Sticky Notes in Collaborative Ideation
PY  - 2018
AB  - In this paper, we compare the use of physical and digital sticky notes in collaborative ideation. Inspired by a case study in a design company, we focus on a collaborative ideation task, which is often part of pair-wise brainstorming in design. For comparison and to focus on the different materiality, we developed a digital sticky notes setup designed to be as close to the physical setup as possible, not adding any advanced digital features, even though technology has reached a stage where more sophisticated use of digital sticky notes on digital boards is possible. In this paper, we present a study of ideation among pairs of experienced sticky note users. The ideation sessions were video recorded and analyzed to focus on how collaboration is supported across the two setups. Based on quantitative analyses of the participants’ interactions with the artefacts, talking patterns, position and attention during the sessions, we qualify how the differences and similarities between the 2 setups have an impact on note handling, ideation techniques, group dynamics and socio-spatial configuration, e.g. the use of the room, the boards and tables. We conclude that, while the physical setup seems more appropriate for creating notes and posting notes, the digital setup invites more note interaction. Nevertheless, we did not find significant differences in the ideation outcome (e.g., number of notes created) or how participants collaborated between the 2 setups. Hence, we argue that collaborative ideation can successfully be supported in a digital setup as well. Consequently, we believe that the next step in a technological setup is not an either or, but should bring the best of the two worlds together.
SP  - 609
EP  - 645
JF  - Computer Supported Cooperative Work (CSCW)
VL  - 27
IS  - 3
PB  - 
DO  - 10.1007/s10606-018-9325-1
ER  - 

TY  - JOUR
AU  - Itoh, Yuta; Kaminokado, Takumi; Akşit, Kaan
TI  - Beaming Displays
PY  - 2021
AB  - Existing near-eye display designs struggle to balance between multiple trade-offs such as form factor, weight, computational requirements, and battery life. These design trade-offs are major obstacles on the path towards an all-day usable near-eye display. In this work, we address these trade-offs by, paradoxically, removing the display from near-eye displays. We present the beaming displays, a new type of near-eye display system that uses a projector and an all passive wearable headset. We modify an off-the-shelf projector with additional lenses. We install such a projector to the environment to beam images from a distance to a passive wearable headset. The beaming projection system tracks the current position of a wearable headset to project distortion-free images with correct perspectives. In our system, a wearable headset guides the beamed images to a user's retina, which are then perceived as an augmented scene within a user's field of view. In addition to providing the system design of the beaming display, we provide a physical prototype and show that the beaming display can provide resolutions as high as consumer-level near-eye displays. We also discuss the different aspects of the design space for our proposal.
SP  - 2659
EP  - 2668
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067764
ER  - 

TY  - NA
AU  - Tripathi, Subarna; Guenter, Brian
TI  - WACV - A Statistical Approach to Continuous Self-Calibrating Eye Gaze Tracking for Head-Mounted Virtual Reality Systems
PY  - 2017
AB  - We present a novel, automatic eye gaze tracking scheme inspired by smooth pursuit eye motion while playing mobile games or watching virtual reality contents. Our algorithm continuously calibrates an eye tracking system for a head mounted display. This eliminates the need for an explicit calibration step and automatically compensates for small movements of the headset with respect to the head. The algorithm finds correspondences between corneal motion and screen space motion, and uses these to generate Gaussian Process Regression models. A combination of those models provides a continuous mapping from corneal position to screen space position. Accuracy is nearly as good as achieved with an explicit calibration step.
SP  - 862
EP  - 870
JF  - 2017 IEEE Winter Conference on Applications of Computer Vision (WACV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wacv.2017.101
ER  - 

TY  - NA
AU  - Jensen, Mads Møller; Rädle, Roman; Klokmose, Clemens Nylandsted; Bødker, Susanne
TI  - CHI - Remediating a Design Tool: Implications of Digitizing Sticky Notes
PY  - 2018
AB  - Sticky notes are ubiquitous in design processes because of their tangibility and ease of use. Yet, they have well-known limitations in professional design processes, as documentation and distribution are cumbersome at best. This paper compares the use of sticky notes in ideation with a remediated digital sticky notes setup. The paper contributes with a nuanced understanding of what happens when remediating a physical design tool into digital space, by emphasizing focus shifts and breakdowns caused by the technology, but also benefits and promises inherent in the digital media. Despite users' preference for creating physical notes, handling digital notes on boards was easier and the potential of proper documentation make the digital setup a possible alternative. While the analogy in our remediation supported a transfer of learned handling, the users' experiences across technological setups impact their use and understanding, yielding new concerns regarding cross-device transfer and collaboration.
SP  - 224
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173798
ER  - 

TY  - JOUR
AU  - Tymms, Chelsea; Gardner, Esther P.; Zorin, Denis
TI  - A Quantitative Perceptual Model for Tactile Roughness
PY  - 2018
AB  - Everyone uses the sense of touch to explore the world, and roughness is one of the most important qualities in tactile perception. Roughness is a major identifier for judgments of material composition, comfort, and friction, and it is tied closely to manual dexterity. The advent of high-resolution 3D printing technology provides the ability to fabricate arbitrary 3D textures with surface geometry that confers haptic properties. In this work, we address the problem of mapping object geometry to tactile roughness. We fabricate a set of carefully designed stimuli and use them in experiments with human subjects to build a perceptual space for roughness. We then match this space to a quantitative model obtained from strain fields derived from elasticity simulations of the human skin contacting the texture geometry, drawing from past research in neuroscience and psychophysics. We demonstrate how this model can be applied to predict and alter surface roughness, and we show several applications in the context of fabrication.
SP  - 168
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 5
PB  - 
DO  - 10.1145/3186267
ER  - 

TY  - NA
AU  - Forman, Jack; Dogan, Mustafa Doga; Forsythe, Hamilton; Ishii, Hiroshi
TI  - UIST - DefeXtiles: 3D Printing Quasi-Woven Fabric via Under-Extrusion
PY  - 2020
AB  - We present DefeXtiles, a rapid and low-cost technique to produce tulle-like fabrics on unmodified fused deposition modeling (FDM) printers. The under-extrusion of filament is a common cause of print failure, resulting in objects with periodic gap defects. In this paper, we demonstrate that these defects can be finely controlled to quickly print thinner, more flexible textiles than previous approaches allow. Our approach allows hierarchical control from micrometer structure to decameter form and is compatible with all common 3D printing materials. In this paper, we introduce the mechanism of DefeXtiles, establish the design space through a set of primitives with detailed workflows, and characterize the mechanical properties of DefeXtiles printed with multiple materials and parameters. Finally, we demonstrate the interactive features and new use cases of our approach through a variety of applications, such as fashion design prototyping, interactive objects, aesthetic patterning, and single-print actuators.
SP  - 1222
EP  - 1233
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415876
ER  - 

TY  - NA
AU  - Syiem, Brandon Victor; Kelly, Ryan; Velloso, Eduardo; Goncalves, Jorge; Dingler, Tilman
TI  - ISMAR - Enhancing Visitor Experience or Hindering Docent Roles: Attentional Issues in Augmented Reality Supported Installations
PY  - 2020
AB  - Studies using augmented reality (AR) technology have suggested that users focus excessively on the virtual content in the AR environment at the expense of the physical world around them. This has implications related to the design of installations that aim to incorporate the user’s physical environment as part of the AR experience. To better understand how user attention is managed in an AR environment, we present an observational study of Rewild Our Planet, a multi-modal installation that combined video, audio, a human docent and mobile AR to promote awareness about environmental issues. We found that, while AR was successful in engaging visitors, it drew attention away from other modalities within the installation. This impacts the work of the human docent and affects how visitors absorb information presented in the installation. Based on these observations, we present guidelines to inform the design of future AR-supported installations with the aim of minimizing or taking advantage of the observed attentional issues.
SP  - 279
EP  - 288
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00053
ER  - 

TY  - NA
AU  - Kang, Seokbin; Norooz, Leyla; Byrne, Virginia L.; Clegg, Tamara; Froehlich, Jon E.
TI  - Tangible and Embedded Interaction - Prototyping and Simulating Complex Systems with Paper Craft and Augmented Reality: An Initial Investigation
PY  - 2018
AB  - We present early work developing an Augmented Reality (AR) system that allows young children to design and experiment with complex systems (e.g., bicycle gears, human circulatory system). Our novel approach combines low-fidelity prototyping to help children represent creative ideas, AR visualization to scaffold iterative design, and virtual simulation to support personalized experiments. To evaluate our approach, we conducted an exploratory study with eight children (ages 8-11) using an initial prototype. Our findings demonstrate the viability of our approach, uncover usability challenges, and suggest opportunities for future work. We also distill additional design implications from a follow-up participatory design session with children.
SP  - 320
EP  - 328
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173264
ER  - 

TY  - JOUR
AU  - Hohman, Fred; Conlen, Matthew; Heer, Jeffrey; Chau, Duen Horng
TI  - Communicating with Interactive Articles
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - Distill
VL  - 5
IS  - 9
PB  - 
DO  - 10.23915/distill.00028
ER  - 

TY  - NA
AU  - Meissner, Janis Lena; Strohmayer, Angelika; Wright, Peter; Fitzpatrick, Geraldine
TI  - CHI - A Schnittmuster for Crafting Context-Sensitive Toolkits
PY  - 2018
AB  - DIY-making can be an expensive pastime if makers are relying on ready-made toolkits, specialised materials and off-shelf components. Many prefabricated commercial kits seek to lower the learning barrier of making and to help beginners to successfully take their first steps in engineering. However, as soon as the novices become a little more advanced, these toolkits often do not fit the specific requirements of personal maker projects anymore. We introduce the idea of a Schnittmuster (or a meta-toolkit) as a novel approach to toolkit design that seeks to address these creativity-limiting factors as well as practical entrance hurdles. To demonstrate the adaptive power of the Schnittmuster concept, we discuss an exemplar in the context of capacitive touch sensing (FlexE-Touch). Implemented under the constraints of materials, user skill sets and making environments, we illustrate how the Schnittmuster facilitated four cheap and flexible toolkit instantiations for crafting custom touch sensor electrodes.
SP  - 151
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173725
ER  - 

TY  - JOUR
AU  - Andersen, Leif; Ballantyne, Michael; Felleisen, Matthias
TI  - Adding interactive visual syntax to textual code
PY  - 2020
AB  - Many programming problems call for turning geometrical thoughts into code: tables, hierarchical structures, nests of objects, trees, forests, graphs, and so on. Linear text does not do justice to such thoughts. But, it has been the dominant programming medium for the past and will remain so for the foreseeable future. This paper proposes a novel mechanism for conveniently extending textual programming languages with problem-specific visual syntax. It argues the necessity of this language feature, demonstrates the feasibility with a robust prototype, and sketches a design plan for adapting the idea to other languages.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - OOPSLA
PB  - 
DO  - 10.1145/3428290
ER  - 

TY  - BOOK
AU  - Tchernavskij, Philip; Klokmose, Clemens Nylandsted; Beaudouin-Lafon, Michel
TI  - Programming - What Can Software Learn From Hypermedia
PY  - 2017
AB  - Most of our interactions with the digital world are mediated by apps: desktop, web, or mobile applications. Apps impose artificial limitations on collaboration among users, distribution across devices, and the changing procedures that constantly occur in real work. These limitations are partially due to the engineering principles of encapsulation and program-data separation. By contrast, the field of hypermedia envisions collaboration, distribution and flexible practices as fundamental features of software. We discuss shareable dynamic media, an alternative model for software that unifies hypermedia and interactive systems, and Webstrates, an experimental implementation of that model. We give examples of patterns and challenges for software architecture that have emerged in our experimentation with Webstrates, and show that it subverts the principles of encapsulation and program-data separation.
SP  - 29
EP  - NA
JF  - Companion to the first International Conference on the Art, Science and Engineering of Programming
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3079368.3079408
ER  - 

TY  - NA
AU  - Schmitz, Martin; Müller, Florian; Mühlhäuser, Max; Riemann, Jan; Le, Huy Viet
TI  - CHI - Itsy-Bits: Fabrication and Recognition of 3D-Printed Tangibles with Small Footprints on Capacitive Touchscreens
PY  - 2021
AB  - Tangibles on capacitive touchscreens are a promising approach to overcome the limited expressiveness of touch input. While research has suggested many approaches to detect tangibles, the corresponding tangibles are either costly or have a considerable minimal size. This makes them bulky and unattractive for many applications. At the same time, they obscure valuable display space for interaction. To address these shortcomings, we contribute Itsy-Bits: a fabrication pipeline for 3D printing and recognition of tangibles on capacitive touchscreens with a footprint as small as a fingertip. Each Itsy-Bit consists of an enclosing 3D object and a unique conductive 2D shape on its bottom. Using only raw data of commodity capacitive touchscreens, Itsy-Bits reliably identifies and locates a variety of shapes in different sizes and estimates their orientation. Through example applications and a technical evaluation, we demonstrate the feasibility and applicability of Itsy-Bits for tangibles with small footprints.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445502
ER  - 

TY  - JOUR
AU  - Kato, Kunihiro; Ikematsu, Kaori; Kawahara, Yoshihiro
TI  - CAPath: 3D-Printed Interfaces with Conductive Points in Grid Layout to Extend Capacitive Touch Inputs
PY  - 2020
AB  - We propose a 3D-printed interface, CAPath, in which conductive contact points are in a grid layout. This structure allows not only specific inputs (e.g., scrolling or pinching) but also general 2D inputs and gestures that fully leverage the "touch surface." We provide the requirements to fabricate the interface and implement a designing system to generate 3D objects in the conductive grid structure. The CAPath interface can be utilized in the uniquely shaped interfaces and opens up further application fields that cannot currently be accessed with existing passive touch extensions. Our contributions also include an evaluation for the recognition accuracy of the touch operations with the implemented interfaces. The results show that our technique is promising to fabricate customizable touch-sensitive interactive objects.
SP  - 193
EP  - 17
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427321
ER  - 

TY  - BOOK
AU  - Casiez, Géry; Granier, Xavier; Hachet, Martin; Lepetit, Vincent; Moreau, Guillaume; Nannipieri, Olivier
TI  - Virtual Reality and Augmented Reality - Towards VE that are More Closely Related to the Real World
PY  - 2018
AB  - This chapter provides an overview of the current state of augmented reality (AR). It examines the questions of pose computation, which could be used to locate a user in an immersive room or to model the real world, interactions in AR, and the concept of presence in environments that combine real and virtual elements. The chapter describes important innovations in the field of 3D interaction over the last decade. While 3D interaction is less central to AR, its importance is rising nonetheless. From simple applications to visualize digital data that is superimposed on the natural view, RA systems today are offering users increasingly more refined interaction systems in order to enhance usage. The chapter overviews recent developments allowing users to easily and efficiently interact with 3D content displayed on tactile surfaces. In the current interfaces, a trend towards an integration of the interaction styles can be observed.
SP  - 217
EP  - 245
JF  - Virtual Reality and Augmented Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1002/9781119341031.ch4
ER  - 

TY  - NA
AU  - Roo, Joan Sol; Hachet, Martin
TI  - UIST - One Reality: Augmenting How the Physical World is Experienced by combining Multiple Mixed Reality Modalities
PY  - 2017
AB  - Most of our daily activities take place in the physical world, which inherently imposes physical constraints. In contrast, the digital world is very flexible, but usually isolated from its physical counterpart. To combine these two realms, many Mixed Reality (MR) techniques have been explored, at different levels in the continuum. In this work we present an integrated Mixed Reality ecosystem that allows users to incrementally transition from pure physical to pure virtual experiences in a unique reality. This system stands on a conceptual framework composed of 6 levels. This paper presents these levels as well as the related interaction techniques.
SP  - 787
EP  - 795
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126638
ER  - 

TY  - NA
AU  - Albaugh, Lea; McCann, James; Hudson, Scott E.; Yao, Lining
TI  - CHI - Engineering Multifunctional Spacer Fabrics Through Machine Knitting
PY  - 2021
AB  - Machine knitting is an increasingly accessible fabrication technology for producing custom soft goods. However, recent machine knitting research has focused on knit shaping, or on adapting hand-knitting patterns. We explore a capability unique to machine knitting: producing multilayer spacer fabrics. These fabrics consist of two face layers connected by a monofilament filler yarn which gives the structure stiffness and volume. We show how to vary knit patterning and yarn parameters in spacer fabrics to produce tactile materials with embedded functionality for forming soft actuated mechanisms and sensors with tunable density, stiffness, material bias, and bristle properties. These soft mechanisms can be rapidly produced on a computationally-controlled v-bed knitting machine and integrated directly into soft objects.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445564
ER  - 

TY  - NA
AU  - Garza, Jorge; Merrill, Devon J.; Swanson, Steven
TI  - CHI - Appliancizer: Transforming Web Pages into Electronic Devices
PY  - 2021
AB  - Prototyping electronic devices that meet today’s consumer standards is a time-consuming task that requires multi-domain expertise. Consumers expect electronic devices to have visually appealing interfaces with both tactile and screen-based interfaces. Appliancizer, our interactive computational design tool, exploits the similarities between graphical and tangible interfaces, allowing web pages to be rapidly transformed into physical electronic devices. Using a novel technique we call essential interface mapping, our tool converts graphical user interface elements (e.g., an HTML button) into tangible interface components (e.g., a physical button) without changing the application source code. Appliancizer automatically generates the PCB and low-level code from web-based prototypes and HTML mock-ups. This makes the prototyping of mixed graphical-tangible interactions as easy as modifying a web page and allows designers to leverage the well-developed ecosystem of web technologies. We demonstrate how our technique simplifies and accelerates prototyping by developing two devices with Appliancizer.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445732
ER  - 

TY  - NA
AU  - Visschedijk, Aaron; Kim, Hyunyoung; Tejada, Carlos; Ashbrook, Daniel
TI  - ClipWidgets: 3D-printed Modular Tangible UI Extensions for Smartphones
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501314
ER  - 

TY  - BOOK
AU  - Burks, Andrew; Renambot, Luc; Johnson, Andrew
TI  - PEARC - VisSnippets: A Web-Based System for Impromptu Collaborative Data Exploration on Large Displays
PY  - 2020
AB  - The VisSnippets system is designed to facilitate effective collaborative data exploration. VisSnippets leverages SAGE2 middleware that enables users to manage the display of digital media content on large displays, thereby providing collaborators with a high-resolution common workspace. Based in JavaScript, VisSnippets provides users with the flexibility to implement and/or select visualization packages and to quickly access data in the cloud. By simplifying the development process, VisSnippets removes the need to scaffold and integrate interactive visualization applications by hand. Users write reusable blocks of code called “snippets” for data retrieval, transformation, and visualization. By composing dataflows from the group’s collective snippet pool, users can quickly execute and explore complementary or contrasting analyses. By giving users the ability to explore alternative scenarios, VisSnippets facilitates parallel work for collaborative data exploration leveraging large-scale displays. We describe the system, its design and implementation, and showcase its flexibility through two example applications.
SP  - 144
EP  - 151
JF  - Practice and Experience in Advanced Research Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311790.3396666
ER  - 

TY  - BOOK
AU  - Steil, Julian; Huang, Michael Xuelin; Bulling, Andreas
TI  - ETRA - Fixation detection for head-mounted eye tracking based on visual similarity of gaze targets
PY  - 2018
AB  - Fixations are widely analysed in human vision, gaze-based interaction, and experimental psychology research. However, robust fixation detection in mobile settings is profoundly challenging given the prevalence of user and gaze target motion. These movements feign a shift in gaze estimates in the frame of reference defined by the eye tracker's scene camera. To address this challenge, we present a novel fixation detection method for head-mounted eye trackers. Our method exploits that, independent of user or gaze target motion, target appearance remains about the same during a fixation. It extracts image information from small regions around the current gaze position and analyses the appearance similarity of these gaze patches across video frames to detect fixations. We evaluate our method using fine-grained fixation annotations on a five-participant indoor dataset (MPIIEgoFixation) with more than 2,300 fixations in total. Our method outperforms commonly used velocity- and dispersion-based algorithms, which highlights its significant potential to analyse scene image information for eye movement detection.
SP  - 23
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204538
ER  - 

TY  - JOUR
AU  - Xu, Weipeng; Chatterjee, Avishek; Zollhöfer, Michael; Rhodin, Helge; Fua, Pascal; Seidel, Hans-Peter; Theobalt, Christian
TI  - Mo 2 Cap 2 : Real-time Mobile 3D Mo tion Capture with a Cap -mounted Fisheye Camera
PY  - 2019
AB  - We propose the first real-time system for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60 Hz on a consumer-level GPU. In addition to the lightweight hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.
SP  - 2093
EP  - 2101
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2019.2898650
ER  - 

TY  - JOUR
AU  - Lonauer, Paul; Holzmann, David; Leitner, Christina; Probst, Alexander; Pöchhacker, Stefan; Oberpeilsteiner, Stefan; Schönböck, Johannes; Jetter, Hans-Christian
TI  - A Multi-Layer Architecture for Near Real-Time Collaboration during Distributed Modeling and Simulation of Cyberphysical Systems
PY  - 2021
AB  - Abstract We present a Web-based multi-layer architecture and implementation to enable near real-time collaboration within partially-distributed engineering teams in this paper. Our architecture named Distributed Modeling and Simulation (DisMoSim) enables collaborative 3D modeling and simulation of complex cyberphysical systems (CPS) across different servers, offices, lab spaces, or organizations. More specifically, DisMoSim creates networks of DisMoSim nodes that communicate using Web protocols with little or no perceptible delay. Each node provides different CPS services, e.g., user interface nodes provide collaborative 3D modeling; computational nodes provide simulations of multi-body dynamics or kinematics; storage nodes allow persisting and sharing models and simulation results; hardware-in-the-loop nodes connect physical testing workbenches to provide real-world sensor measurements as simulation input; software-in-the-loop nodes provide control signals to simulate hardware controllers. In the following, we derive a set of design goals and propose our DisMoSim architecture. We illustrate how we used our multi-layer conceptual architecture to implement a variety of CPS services for an example scenario. By this, we demonstrate the practicability and versatility of our approach and conclude by discussing limitations and future work.
SP  - 190
EP  - 199
JF  - Procedia Computer Science
VL  - 180
IS  - NA
PB  - 
DO  - 10.1016/j.procs.2021.01.156
ER  - 

TY  - JOUR
AU  - Runji, Joel Murithi; Lin, Chyi-Yeu
TI  - Switchable Glass Enabled Contextualization for a Cyber-Physical Safe and Interactive Spatial Augmented Reality PCBA Manufacturing Inspection System.
PY  - 2020
AB  - Augmented reality (AR) has been demonstrated to improve efficiency by up to thrice the level of traditional methods. Specifically, the adoption of visual AR is performed widely using handheld and head-mount technologies. Despite spatial augmented reality (SAR) addressing several shortcomings of wearable AR, its potential is yet to be fully explored. To date, it enhances the cooperation of users with its wide field of view and supports hands-free mobile operation, yet it has remained a challenge to provide references without relying on restrictive static empty surfaces of the same object or nearby objects for projection. Towards this end, we propose a novel approach that contextualizes projected references in real-time and on demand, onto and through the surface across a wireless network. To demonstrate the effectiveness of the approach, we apply the method to the safe inspection of printed circuit board assembly (PCBA) wirelessly networked to a remote automatic optical inspection (AOI) system. A defect detected and localized by the AOI system is wirelessly remitted to the proposed remote inspection system for prompt guidance to the inspector by augmenting a rectangular bracket and a reference image. The rectangular bracket transmitted through the switchable glass aids defect localization over the PCBA, whereas the image is projected over the opaque cells of the switchable glass to provide reference to a user. The developed system is evaluated in a user study for its robustness, precision and performance. Results indicate that the resulting contextualization from variability in occlusion levels not only positively affect inspection performance but also supersedes the state of the art in user preference. Furthermore, it supports a variety of complex visualization needs including varied sizes, contrast, online or offline tracking, with a simple robust integration requiring no additional calibration for registration.
SP  - 4286
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 15
PB  - 
DO  - 10.3390/s20154286
ER  - 

TY  - JOUR
AU  - Fraga-Lamas, Paula; Fernández-Caramés, Tiago M.; Blanco-Novoa, Oscar; Vilar-Montesinos, Miguel
TI  - A Review on Industrial Augmented Reality Systems for the Industry 4.0 Shipyard
PY  - 2018
AB  - Shipbuilding companies are upgrading their inner workings in order to create Shipyards 4.0, where the principles of Industry 4.0 are paving the way to further digitalized and optimized processes in an integrated network. Among the different Industry 4.0 technologies, this paper focuses on augmented reality, whose application in the industrial field has led to the concept of industrial augmented reality (IAR). This paper first describes the basics of IAR and then carries out a thorough analysis of the latest IAR systems for industrial and shipbuilding applications. Then, in order to build a practical IAR system for shipyard workers, the main hardware and software solutions are compared. Finally, as a conclusion after reviewing all the aspects related to IAR for shipbuilding, it proposed an IAR system architecture that combines cloudlets and fog computing, which reduce latency response and accelerate rendering tasks while offloading compute intensive tasks from the cloud.
SP  - 13358
EP  - 13375
JF  - IEEE Access
VL  - 6
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2808326
ER  - 

TY  - NA
AU  - Liu, Xin; Han, Bo; Zheng, Clement; Yen, Ching Chiuan
TI  - Tribo Tribe: Triboelectric Interaction Sensing with 3D Physical Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519759
ER  - 

TY  - NA
AU  - Günther, Sebastian; Nelles, Frank; Horn, Florian; Mühlhäuser, Max
TI  - Demonstrating MagneTisch: Tangibles in Motion on an Interactive Surface
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3547520
ER  - 

TY  - BOOK
AU  - Eivazi, Shaharam; Santini, Thiago; Keshavarzi, Alireza; Kübler, Thomas C.; Mazzei, Andrea
TI  - ETRA - Improving real-time CNN-based pupil detection through domain-specific data augmentation
PY  - 2019
AB  - Deep learning is a promising technique for real-world pupil detection. However, the small amount of available accurately-annotated data poses a challenge when training such networks. Here, we utilize non-challenging eye videos where algorithmic approaches perform virtually without errors to automatically generate a foundational data set containing subpixel pupil annotations. Then, we propose multiple domain-specific data augmentation methods to create unique training sets containing controlled distributions of pupil-detection challenges. The feasibility, convenience, and advantage of this approach is demonstrated by training a CNN with these datasets. The resulting network outperformed current methods in multiple publicly-available, realistic, and challenging datasets, despite being trained solely with the augmented eye images. This network also exhibited better generalization w.r.t. the latest state-of-the-art CNN: Whereas on datasets similar to training data, the nets displayed similar performance, on datasets unseen to both networks, ours outperformed the state-of-the-art by a27% in terms of detection rate.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319914
ER  - 

TY  - NA
AU  - Jones, Michael; Anderson, Zann; Walker, Casey; Seppi, Kevin D.
TI  - CHI - PHUI-kit: Interface Layout and Fabrication on Curved 3D Printed Objects
PY  - 2018
AB  - We seek to make physical user interface (PHUI) design more like graphical user interface (GUI) design by using a drag-and drop interface to place widgets, allowing widgets to be repositioned and by hiding implementation details. PHUIs are interfaces built from tangible widgets arranged on the surfaces of physical objects. PHUI layout will become more important as we move from rectangular screens to purpose-built interactive devices. Approaches to PHUI layout based on sculpture make it difficult to reposition widgets, and software approaches do not involve placing widgets on the device exterior. We created PHUI-kit, a software approach to PHUI layout on 3D printed enclosures, which has a drag-and-drop interface, supports repositioning of widgets, and hides implementation details. We describe algorithms for placing widgets on curved surfaces, modifying the enclosure geometry, and routing wiring inside the enclosure. The tool is easy to use and supports a wide range of design possibilities.
SP  - 110
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173684
ER  - 

TY  - NA
AU  - Conlen, Matthew; Vo, Megan; Tan, Alan; Heer, Jeffrey
TI  - UIST - Idyll Studio: A Structured Editor for Authoring Interactive & Data-Driven Articles
PY  - 2021
AB  - Interactive articles are an effective medium of communication in education, journalism, and scientific publishing, yet are created using complex general-purpose programming tools. We present Idyll Studio, a structured editor for authoring and publishing interactive and data-driven articles. We extend the Idyll framework to support reflective documents, which can inspect and modify their underlying program at runtime, and show how this functionality can be used to reify the constituent parts of a reactive document model—components, text, state, and styles—in an expressive, interoperable, and easy-to-learn graphical interface. In a study with 18 diverse participants, all could perform basic editing and composition, use datasets and variables, and specify relationships between components. Most could choreograph interactive visualizations and dynamic text, although some struggled with advanced uses requiring unstructured code editing. Our findings suggest Idyll Studio lowers the threshold for non-experts to create interactive articles and allows experts to rapidly specify a wide range of article designs.
SP  - 1
EP  - 12
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474731
ER  - 

TY  - NA
AU  - Iyer, Vikram; Chan, Justin; Culhane, Ian; Mankoff, Jennifer; Gollakota, Shyamnath
TI  - UIST - Wireless Analytics for 3D Printed Objects
PY  - 2018
AB  - We present the first wireless physical analytics system for 3D printed objects using commonly available conductive plastic filaments. Our design can enable various data capture and wireless physical analytics capabilities for 3D printed objects, without the need for electronics. To achieve this goal, we make three key contributions: (1) demonstrate room scale backscatter communication and sensing using conductive plastic filaments, (2) introduce the first backscatter designs that detect a variety of bi-directional motions and support linear and rotational movements, and (3) enable data capture and storage for later retrieval when outside the range of the wireless coverage, using a ratchet and gear system. We validate our approach by wirelessly detecting the opening and closing of a pill bottle, capturing the joint angles of a 3D printed e-NABLE prosthetic hand, and an insulin pen that can store information to track its use outside the range of a wireless receiver.
SP  - 141
EP  - 152
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242639
ER  - 

TY  - JOUR
AU  - Haladjian, Juan
TI  - The Wearables Development Toolkit: An Integrated Development Environment for Activity Recognition Applications
PY  - 2019
AB  - Although the last two decades have seen an increasing number of activity recognition applications with wearable devices, there is still a lack of tools specifically designed to support their development. The development of activity recognition algorithms for wearable devices is particularly challenging because of the several requirements that have to be met simultaneously (e.g., low energy consumption, small and lightweight, accurate recognition). Activity recognition applications are usually developed in a series of iterations to annotate sensor data and to analyze, develop and assess the performance of a recognition algorithm. This paper presents the Wearables Development Toolkit, an Integrated Development Environment designed to lower the entrance barrier to the development of activity recognition applications with wearables. It specifically focuses on activity recognition using on-body inertial sensors. The toolkit offers a repository of high-level reusable components and a set of tools with functionality to annotate data, to analyze and develop activity recognition algorithms and to assess their recognition and computational performance. We demonstrate the versatility of the toolkit with three applications and describe how we developed it incrementally based on two user studies.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 4
PB  - 
DO  - 10.1145/3369813
ER  - 

TY  - NA
AU  - Chow, Derrek; Xia, Gracie; Ou, Jasmine
TI  - FormSense: A Fabrication Method to Support Shape Exploration of Interactive Prototypes
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558730
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Kato, Kunihiro; Kawahara, Yoshihiro
TI  - CHI - LightTouch Gadgets: Extending Interactions on Capacitive Touchscreens by Converting Light Emission to Touch Inputs
PY  - 2021
AB  - We present LightTouch, a 3D-printed passive gadget to enhance touch interactions on unmodified capacitive touchscreens. The LightTouch gadgets simulate finger operations such as tapping, swiping, and multi-touch gestures by means of conductive materials and light-dependent resistors (LDR) embedded in the object. The touchscreen emits visible light and the LDR senses the level of this light, which changes its resistance value. By controlling the screen brightness, it intentionally connects or disconnects the path between the GND and the touchscreen, thus allowing the touch inputs to be controlled. In contrast to conventional physical extensions for touchscreens, our technique requires neither continuous finger contact on the conductive part nor the use of batteries. As such, it opens up new possibilities for touchscreen interactions beyond the simple automation of touch inputs, such as establishing a communication channel between devices, enhancing the trackability of tangibles, and inter-application operations.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445581
ER  - 

TY  - NA
AU  - Normand, Erwan; McGuffin, Michael J.
TI  - ISMAR - Enlarging a Smartphone with AR to Create a Handheld VESAD (Virtually Extended Screen-Aligned Display)
PY  - 2018
AB  - We investigate using augmented reality to extend the screen of a smartphone beyond its physical limits with a virtual surface that is co-planar with the phone and that follows as the phone is moved. We call this extension a VESAD, or Virtually Extended Screen-Aligned Display. We illustrate and describe several ways that a VESAD could be used to complement the physical screen of a phone, and describe two novel interaction techniques: one where the user performs a quick rotation of the phone to switch the information shown in the VESAD, and another called "slide-and-hang" whereby the user can detach a VESAD and leave it hanging in mid-air, using the phone to establish the initial position and orientation of the virtual window. We also report an experiment that compared three interfaces used for an abstract classification task: the first using only a smartphone, the second using the phone for input but with a VESAD for output, and the third where the user performed input in mid-air on the VESAD (as detected by a Leap Motion). The second user interface was found to be superior in time and selection count (a metric of mistakes committed by users) and was also subjectively preferred over the other two interfaces. This demonstrates the added value of a VESAD for output over a phone's physical screen, and also demonstrates that input on the phone's screen was better than input in mid-air in our experiment.
SP  - 123
EP  - 133
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar.2018.00043
ER  - 

TY  - NA
AU  - Shi, Lei; Zhao, Yuhang; Penuela, Ricardo Gonzalez; Kupferstein, Elizabeth; Azenkot, Shiri
TI  - CHI - Molder: An Accessible Design Tool for Tactile Maps
PY  - 2020
AB  - Tactile materials are powerful teaching aids for students with visual impairments (VIs). To design these materials, designers must use modeling applications, which have high learning curves and rely on visual feedback. Today, Orientation and Mobility (O&M) specialists and teachers are often responsible for designing these materials. However, most of them do not have professional modeling skills, and many are visually impaired themselves. To address this issue, we designed Molder, an accessible design tool for interactive tactile maps, an important type of tactile materials that can help students learn O&M skills. A designer uses Molder to design a map using tangible input techniques, and Molder provides auditory feedback and high-contrast visual feedback. We evaluated Molder with 12 participants (8 with VIs, 4 sighted). After a 30-minute training session, the participants were all able to use Molder to design maps with customized tactile and interactive information.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376431
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Chen, Yu; Yang, Yue; Tao, Ye; Wang, Guanyun; Yao, Lining
TI  - CHI Extended Abstracts - 4DTexture: A Shape-Changing Fabrication Method for 3D Surfaces with Texture
PY  - 2020
AB  - 4D printing with a hobbyist FDM printer has enabled a rapid fabrication and self-assembly process for 3D shapes. Researchers have leveraged novel structure design and material techniques to create a wide range of 4D shapes. Meanwhile, 4D printing texture (i.e., shape-changing texture) on objects which could easily augment the haptic sensation, has drawn more attention to this field. In this paper, we introduce 4DTexture, a novel design and fabrication approach that integrates texture design through the 4D printing process. Compared to conventional 3D printing surfaces with texture, which usually requires support structures, 4DTexture can effectively reduce production material, time and the post-process effort after printing. Specifically, 4DTexture prints flat substrates with a customized texture design on its top surfaces, which can easily be triggered and can self-morph to target 3D shapes. Overall, our approach enables the design and fabrication of 3D surfaces with texture and can be leveraged by designers and researchers in the field of personal fabrication.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383053
ER  - 

TY  - NA
AU  - Zhang, Xiong; Guo, Philip J.
TI  - UIST - Fusion: Opportunistic Web Prototyping with UI Mashups
PY  - 2018
AB  - Modern web development is rife with complexity at all layers, ranging from needing to configure backend services to grappling with frontend frameworks and dependencies. To lower these development barriers, we introduce a technique that enables people to prototype opportunistically by borrowing pieces of desired functionality from across the web without needing any access to their underlying codebases, build environments, or server backends. We implemented this technique in a browser extension called Fusion, which lets users create web UI mashups by extracting components from existing unmodified webpages and hooking them together using transclusion and JavaScript glue code. We demonstrate the generality and versatility of Fusion via a case study where we used it to create seven UI mashups in domains such as programming tools, data science, web design, and collaborative work. Our mashups include replicating portions of prior HCI systems (Blueprint for in-situ code search and DS.js for in-browser data science), extending the p5.js IDE for Processing with real-time collaborative editing, and integrating Python Tutor code visualizations into static tutorials. These UI mashups each took less than 15 lines of JavaScript glue code to create with Fusion.
SP  - 951
EP  - 962
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242632
ER  - 

TY  - CONF
AU  - Piya, Cecil; Vinayak, Vinayak; Zhang, Yunbo; Ramani, Karthik
TI  - Graphics Interface - RealFusion: An Interactive Workflow for Repurposing Real-World Objects towards Early-stage Creative Ideation
PY  - 2016
AB  - We present RealFusion, an interactive workflow that supports early stage design ideation in a digital 3D medium. RealFusion is inspired by the practice of found-object-art, wherein new representations are created by composing existing objects. The key motivation behind our approach is direct creation of 3D artifacts during design ideation, in contrast to conventional practice of employing 2D sketching. RealFusion comprises of three creative states where users can (a) repurpose physical objects as modeling components, (b) modify the components to explore different forms, and (c) compose them into a meaningful 3D model. We demonstrate RealFusion using a simple interface that comprises of a depth sensor and a smartphone. To achieve direct and efficient manipulation of modeling elements, we also utilize mid-air interactions with the smartphone. We conduct a user study with novice designers to evaluate the creative outcomes that can be achieved using RealFusion.
SP  - 85
EP  - 92
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kato, Jun; Goto, Masataka
TI  - Conference on Designing Interactive Systems - f3.js: A Parametric Design Tool for Physical Computing Devices for Both Interaction Designers and End-users
PY  - 2017
AB  - Although the exploration of design alternatives is crucial for interaction designers and customization is required for end-users, the current development tools for physical computing devices have focused on single versions of an artifact. We propose the parametric design of devices including their enclosure layouts and programs to address this issue. A Web-based design tool called f3.js is presented as an example implementation, which allows devices assembled from laser-cut panels with sensors and actuator modules to be parametrically created and customized. It enables interaction designers to write code with dedicated APIs, declare parameters, and interactively tune them to produce the enclosure layouts and programs. It also provides a separate user interface for end-users that allows parameter tuning and dynamically generates instructions for device assembly. The parametric design approach and the tool were evaluated through two user studies with interaction designers, university students, and end-users.
SP  - 1099
EP  - 1110
JF  - Proceedings of the 2017 Conference on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3064663.3064681
ER  - 

TY  - NA
AU  - Lu, Binghao; Fu, Jirui; Hosseini, Saba M.; Park, Joon-Hyuk
TI  - Modeling and Characterization of 3D Printed Flexible Mesh Structure for Wearable Interface
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/biorob52689.2022.9925405
ER  - 

TY  - NA
AU  - Marti, Marcel; Vieli, Jodok; Witon, Wojciech; Sanghrajka, Rushit; Inversini, Daniel; Wotruba, Diana; Simo, Isabel; Schriber, Sasha; Kapadia, Mubbasir; Gross, Markus
TI  - IUI - CARDINAL: Computer Assisted Authoring of Movie Scripts
PY  - 2018
AB  - We present Cardinal, a tool for computer-assisted authoring of movie scripts. Cardinal provides a means of viewing a script through a variety of perspectives, for interpretation as well as editing. This is made possible by virtue of intelligent automated analysis of natural language scripts and generating different intermediate representations. Cardinal generates 2-D and 3-D visualizations of the scripted narrative and also presents interactions in a timeline-based view. The visualizations empower the scriptwriter to understand their story from a spatial perspective, and the timeline view provides an overview of the interactions in the story. The user study reveals that users of the system demonstrated confidence and comfort using the system.
SP  - 509
EP  - 519
JF  - 23rd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3172944.3172972
ER  - 

TY  - NA
AU  - Larsen-Ledet, Ida; Borowski, Marcel
TI  - OZCHI - “It Looks Like You Don’t Agree”: Idiosyncratic Practices and Preferences in Collaborative Writing
PY  - 2020
AB  - This paper addresses collaborative writing in academia. Recent research has indicated that while many tools for collaborative writing exist and continue to be developed, co-writers frequently employ workarounds and cumbersome substitutions to accommodate their writing approaches and collaborative needs. As part of a process to address these issues, we conducted a co-design study on collaborative academic writing with 18 participants. The paper details a three-stage co-design approach developed for this purpose. During this three-stage workshop series, the participants discussed needs, frustrations, and desires in their experiences with collaborative writing. These discussions revealed how participants’ different ways of practicing and experiencing collaborative writing entail contrasting needs that are difficult to balance. Based on an analysis of discussions and artifacts from the workshops, we argue that researchers and designers should aim to support diverse practices and propose a protocol for examining and drawing on the contradictions that arise from co-writers’ idiosyncratic preferences.
SP  - 339
EP  - 354
JF  - 32nd Australian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441000.3441032
ER  - 

TY  - NA
AU  - Gardony, Aaron L.; Lindeman, Robert W.; Brunyé, Tad T.
TI  - Eye-tracking for human-centered mixed reality: promises and challenges
PY  - 2020
AB  - Eye-tracking hardware and software are being rapidly integrated into mixed reality (MR) technology. Cognitive science and human-computer interaction (HCI) research demonstrate several ways eye-tracking can be used to gauge user characteristics, intent, and status as well as provide active and passive input control to MR interfaces. In this paper, we argue that eye-tracking can be used to ground MR technology in the cognitive capacities and intentions of users and that such human-centered MR is important for MR designers and engineers to consider. We detail relevant and timely research in eye-tracking and MR and offer suggestions and recommendations to accelerate the development of eye-tracking-enabled human-centered MR, with a focus on recent research findings. We identify several promises that eye-tracking holds for improving MR experiences. In the near term, these include user authentication, gross interface interactions, monitoring visual attention across real and virtual scene elements, and adaptive graphical rendering enabled by relatively coarse eyetracking metrics. In the far term, hardware and software advances will enable gaze-depth aware foveated MR displays and attentive MR user interfaces that track user intent and status using fine and dynamic aspects of gaze. Challenges, such as current technological limitations, difficulties in translating lab-based eye-tracking metrics to MR, and heterogeneous MR use cases are considered alongside cutting-edge research working to address them. With a focused research effort grounded in an understanding of the promises and challenges for eye-tracking, human-centered MR can be realized to improve the efficacy and user experience of MR.
SP  - 230
EP  - 247
JF  - Optical Architectures for Displays and Sensing in Augmented, Virtual, and Mixed Reality (AR, VR, MR)
VL  - 11310
IS  - NA
PB  - 
DO  - 10.1117/12.2542699
ER  - 

TY  - BOOK
AU  - Santini, Thiago; Fuhl, Wolfgang; Kasneci, Enkelejda
TI  - ETRA - PuReST: robust pupil tracking for real-time pervasive eye tracking
PY  - 2018
AB  - Pervasive eye-tracking applications such as gaze-based human computer interaction and advanced driver assistance require real-time, accurate, and robust pupil detection. However, automated pupil detection has proved to be an intricate task in real-world scenarios due to a large mixture of challenges - for instance, quickly changing illumination and occlusions. In this work, we introduce the Pupil Reconstructor with Subsequent Tracking (PuReST), a novel method for fast and robust pupil tracking. The proposed method was evaluated on over 266,000 realistic and challenging images acquired with three distinct head-mounted eye tracking devices, increasing pupil detection rate by 5.44 and 29.92 percentage points while reducing average run time by a factor of 2.74 and 1.1. w.r.t. state-of-the-art 1) pupil detectors and 2) vendor provided pupil trackers, respectively. Overall, PuReST outperformed other methods in 81.82% of use cases.
SP  - 61
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204578
ER  - 

TY  - BOOK
AU  - Valkov, Dimitar; Thiele, Sebastian; Huesmann, Karim; Gebauer, Eike; Risse, Benjamin
TI  - VR Workshops - Touch Recognition on Complex 3D Printed Surfaces using Filter Response Analysis
PY  - 2021
AB  - Touch sensing on various surfaces has played a prominent role in human-computer interaction in the last decades. However, current technologies are mostly suited for flat or sufficiently smooth surfaces and touch sensing on complex geometries remains a challenging task, especially when the sensing hardware needs to be embedded into the interactive object. In this paper, we introduce a novel sensing approach based on the observation that conductive materials and the user’s hand or finger can be considered a complex filter system with well-conditioned input-output relationships. Different hand postures can be disambiguated by mapping the response of these filters using an intentionally small convolutional neural network. Our experiments show that even straight-forward electrode geometries provided by common 3D printers and filaments can be used to achieve high accuracy, rendering expressive interactions with complex 3D shapes possible while allowing to integrate the touch surface directly into the interactive object. Ultimately, our low-cost and versatile sensing approach enables rich interaction on a variety of objects and surfaces which is demonstrated through a series of illustrative experiments.
SP  - 195
EP  - 200
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00043
ER  - 

TY  - NA
AU  - Ou, Jifei; Dublon, Gershon; Cheng, Chin-Yi; Heibeck, Felix; Willis, Karl D.D.; Ishii, Hiroshi
TI  - CHI - Cilllia: 3D Printed Micro-Pillar Structures for Surface Texture, Actuation and Sensing
PY  - 2016
AB  - This work presents a method for 3D printing hair-like structures on both flat and curved surfaces. It allows a user to design and fabricate hair geometries that are smaller than 100 micron. We built a software platform to let users quickly define the hair angle, thickness, density, and height. The ability to fabricate customized hair-like structures not only expands the library of 3D-printable shapes, but also enables us to design passive actuators and swipe sensors. We also present several applications that show how the 3D-printed hair can be used for designing everyday interactive objects.
SP  - 5753
EP  - 5764
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858257
ER  - 

TY  - JOUR
AU  - Lazna, Richard; Barvir, Radek; Vondrakova, Alena; Brus, Jan
TI  - Creating a Haptic 3D Model of Wenceslas Hill in Olomouc
PY  - 2022
AB  - <jats:p>Interactivity in today’s society finds its way into many facets of life and can be used in various ways, including 3D printing. For example, various 3D models can be incorporated into museum exhibitions and serve as interactive media for visitors, deepening their experience. One of the advantages of haptic 3D models is the immediate haptic feedback. Such models can have various uses, from being a part of an interactive exhibition to providing assistance to people with visual impairment. This article describes the process of creating a haptic 3D model depicting Wenceslas Hill in Olomouc in the eighteenth century. The model has several surface elements printed from conductive material that react to touch. The interactive model itself is unchanged from its original modelled 3D version, meaning the shape of the object stays the exact same throughout modifications. The resulting model conveys additional information about the object or its parts by means of a web interface via a connected tablet device. To implement the desired functionality, TouchIt3D technology was used. This technology uses a combination of conductive and non-conductive materials for 3D printing. The conductive material serves to propagate an electrical signal caused by touching a chosen part of the model. A 3D printer with two extruders was used for printing the model, allowing simultaneous printing of two different materials. The model’s scalability is advantageous for potential use by people with visual impairment. The model shall serve as a tool for enriching historical knowledge about the object by using interactivity.</jats:p>
SP  - 10817
EP  - 10817
JF  - Applied Sciences
VL  - 12
IS  - 21
PB  - 
DO  - 10.3390/app122110817
ER  - 

TY  - NA
AU  - Katakura, Shohei; Watanabe, Keita
TI  - CHI Extended Abstracts - ProtoHole: Prototyping Interactive 3D Printed Objects Using Holes and Acoustic Sensing
PY  - 2018
AB  - A prototype process of a physical user interface includes not only connection of electronic parts and an enclosure design, but also the arrangement and configuration of the electronic parts in the enclosure. This process is complicated and difficult for people who do not have modeling skills. We propose ProtoHole, a system for prototyping interactive 3D printed objects using holes, internal cavity and swept-frequency acoustic sensing. By emitting a high-frequency sweep signal inside the object, our system enables to classify changes in resonance properties when closing holes using a machine learning technique. Therefore, an object can be easily made interactive without considering an arrangement of internal electronic parts and wiring. We show examples of prototypes created with our system.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188471
ER  - 

TY  - NA
AU  - Wang, Jianan; Li, Boyang; Fan, Xiangyu; Lin, Jing; Fu, Yanwei
TI  - Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions
PY  - 2020
AB  - The task of video and text sequence alignment is a prerequisite step toward joint understanding of movie videos and screenplays. However, supervised methods face the obstacle of limited realistic training data. With this paper, we attempt to enhance data efficiency of the end-to-end alignment network NeuMATCH [15]. Recent research [56] suggests that network components dealing with different modalities may overfit and generalize at different speeds, creating difficulties for training. We propose to employ (1) layer-wise adaptive rate scaling (LARS) to align the magnitudes of gradient updates in different layers and balance the pace of learning and (2) sequence-wise batch normalization (SBN) to align the internal feature distributions from different modalities. Finally, we leverage random projection to reduce the dimensionality of input features. On the YouTube Movie Summary dataset, the combined use of these technique closes the performance gap when the pretraining on the LSMDC dataset is omitted and achieves the state-of-the-art result. Extensive empirical comparisons and analysis reveal that these techniques improve optimization and regularize the network more effectively than two different setups of layer normalization.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Takada, Ryosuke; Shizuki, Buntarou; Tanaka, Jiro
TI  - CHI Extended Abstracts - MonoTouch: Single Capacitive Touch Sensor that Differentiates Touch Gestures
PY  - 2016
AB  - We show a capacitive touch sensor called MonoTouch, which differentiates taps, swipe gestures, and swipe directions. MonoTouch consists of only an electrode and a circuit. To differentiate touch gestures with a single electrode, we designed the electrode's layout to satisfy the following two requirements: (1) The number of responses is different between the gestures; (2) The response time is different between swipe directions. We then developed an electrode that differentiates taps and four directional swipe gestures. When our MonoTouch electrode is downsized, gesture differentiation accuracy decreases because a finger might cross two or more conductive parts. To solve this "Multiple Crossing Problem", we added embossments on the electrode surface. Our evaluation of the MonoTouch sensor indicates that using the embossments solved the "Multiple Crossing Problem".
SP  - 2736
EP  - 2743
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892350
ER  - 

TY  - NA
AU  - Li, Daniel; Chen, Thomas; Tung, Albert; Chilton, Lydia B.
TI  - UIST - Hierarchical Summarization for Longform Spoken Dialog
PY  - 2021
AB  - Every day we are surrounded by spoken dialog. This medium delivers rich diverse streams of information auditorily; however, systematically understanding dialog can often be non-trivial. Despite the pervasiveness of spoken dialog, automated speech understanding and quality information extraction remains markedly poor, especially when compared to written prose. Furthermore, compared to understanding text, auditory communication poses many additional challenges such as speaker disfluencies, informal prose styles, and lack of structure. These concerns all demonstrate the need for a distinctly speech tailored interactive system to help users understand and navigate the spoken language domain. While individual automatic speech recognition (ASR) and text summarization methods already exist, they are imperfect technologies; neither consider user purpose and intent nor address spoken language induced complications. Consequently, we design a two stage ASR and text summarization pipeline and propose a set of semantic segmentation and merging algorithms to resolve these speech modeling challenges. Our system enables users to easily browse and navigate content as well as recover from errors in these underlying technologies. Finally, we present an evaluation of the system which highlights user preference for hierarchical summarization as a tool to quickly skim audio and identify content of interest to the user.
SP  - 582
EP  - 597
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474771
ER  - 

TY  - NA
AU  - Rädle, Roman; Nouwens, Midas; Antonsen, Kristian; Eagan, James; Klokmose, Clemens Nylandsted
TI  - UIST - Codestrates: Literate Computing with Webstrates
PY  - 2017
AB  - We introduce Codestrates, a literate computing approach to developing interactive software. Codestrates blurs the distinction between the use and development of applications. It builds on the literate computing approach, commonly found in interactive notebooks such as Jupyter notebook. Literate computing weaves together prose and live computation in the same document. However, literate computing in interactive notebooks are limited to computation and it is challenging to extend their user interface, reprogram their functionality, or develop stand-alone applications. Codestrates builds literate computing capabilities on top of Webstrates and demonstrates how it can be used for (i) collaborative interactive notebooks, (ii) extending its functionality from within itself, and (iii) developing reprogrammable applications.
SP  - 715
EP  - 725
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126642
ER  - 

TY  - NA
AU  - Li, Dingzeyu
TI  - Interacting with Acoustic Simulation and Fabrication
PY  - 2017
AB  - Incorporating accurate physics-based simulation into interactive design tools is challenging. However, adding the physics accurately becomes crucial to several emerging technologies. For example, in virtual/augmented reality (VR/AR) videos, the faithful reproduction of surrounding audios is required to bring the immersion to the next level. Similarly, as personal fabrication is made possible with accessible 3D printers, more intuitive tools that respect the physical constraints can help artists to prototype designs. One main hurdle is the sheer amount of computation complexity to accurately reproduce the real-world phenomena through physics-based simulation. In my thesis research, I develop interactive tools that implement efficient physics-based simulation algorithms for automatic optimization and intuitive user interaction.
SP  - 99
EP  - 102
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131842
ER  - 

TY  - NA
AU  - He, Liang
TI  - UIST (Adjunct Volume) - Designing, Controlling, and Fabricating In-Place Augmented Structures
PY  - 2020
AB  - Emerging 3D printing technology has enabled the rapid development of physical objects. However, 3D-printed objects are rarely interactive and adding interactivity to printed objects is inherently challenging. To boost 3D printing for a wider spectrum of applications, I introduce in-place augmented structures, a class of 3D printable parametric structures that can be integrated with physical objects and spaces for augmented behaviors. In my research, I explore how 3D printing can support interaction (e.g., sensing and actuation) by creating novel design techniques and building interactive design tools that enable end-users to design and control desired behaviors. With these techniques and tools, I fabricate the in-place structures with readily available fabrication techniques and demonstrate my approach with a suite of applications across different domains.
SP  - 169
EP  - 173
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415804
ER  - 

TY  - NA
AU  - Pavel, Amy; Goldman, Dan B.; Hartmann, Björn; Agrawala, Maneesh
TI  - UIST - VidCrit: Video-based Asynchronous Video Review
PY  - 2016
AB  - Video production is a collaborative process in which stakeholders regularly review drafts of the edited video to indicate problems and offer suggestions for improvement. Although practitioners prefer in-person feedback, most reviews are conducted asynchronously via email due to scheduling and location constraints. The use of this impoverished medium is challenging for both providers and consumers of feedback. We introduce VidCrit, a system for providing asynchronous feedback on drafts of edited video that incorporates favorable qualities of an in-person review. This system consists of two separate interfaces: (1) A feedback recording interface captures reviewers' spoken comments, mouse interactions, hand gestures and other physical reactions. (2) A feedback viewing interface transcribes and segments the recorded review into topical comments so that the video author can browse the review by either text or timelines. Our system features novel methods to automatically segment a long review session into topical text comments, and to label such comments with additional contextual information. We interviewed practitioners to inform a set of design guidelines for giving and receiving feedback, and based our system's design on these guidelines. Video reviewers using our system preferred our feedback recording interface over email for providing feedback due to the reduction in time and effort. In a fixed amount of time, reviewers provided 10.9 (σ=5.09) more local comments than when using text. All video authors rated our feedback viewing interface preferable to receiving feedback via e-mail.
SP  - 517
EP  - 528
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984552
ER  - 

TY  - JOUR
AU  - Götzelmann, Timo
TI  - 3D-Druck für blinde Menschen
PY  - 2017
AB  - Neben herkommlichen taktilen Drucktechniken fur blinde Menschen findet auch der 3D-Druck zunehmend Verbreitung. Wahrend anfangliche Ansatze beabsichtigten, mit dieser alternativen Drucktechnologie qualitativ ahnliche Druckresultate zu erzielen, nutzen neuere Ansatze deren Potenzial, um interaktive Drucke zu erstellen. Ausgehend von dieser Entwicklung verschafft dieser Artikel einen Uberblick uber wesentliche Ansatze fur die Erstellung von vielfaltigen taktilen Materialen mittels 3D-Druckern. Er zeigt dabei insbesondere den Wandel von statischen zu interaktiven Ansatzen auf. Dabei muss bei Letzteren eine Kopplung zwischen den taktilen 3D-Drucken und elektronischen Entitaten erfolgen, welche durch unterschiedliche Sensorik umgesetzt werden kann. Zukunftige Entwicklungen konnten es erlauben, die Interaktion des Benutzers mit der kompletten Oberflache von 3D-Drucken sensorisch zu erfassen und somit komplexe neue Interaktionsmoglichkeiten zu erschliesen, welche blinden wie auch sehenden Menschen hilfreich sein konnen.
SP  - 511
EP  - 515
JF  - Informatik-Spektrum
VL  - 40
IS  - 6
PB  - 
DO  - 10.1007/s00287-017-1068-8
ER  - 

TY  - NA
AU  - Gadea, Cristian; Hong, Daniel; Ionescu, Dan; Ionescu, Bogdan
TI  - An architecture for web-based collaborative 3D virtual spaces using DOM synchronization
PY  - 2016
AB  - The advent of affordable virtual reality (VR) head-mounted displays (HMDs) has sparked significant research activity for creating new principles and algorithms to enable the display of VR content on these devices. However, the development of current VR content is mostly limited to experts using proprietary tools and protocols. A major goal of VR is to allow multiple people to meet and collaborate in the same virtual space, but the implementation time and complexity of creating multi-user 3D environments and games is presently a large barrier to entry for developers. This paper presents a web-based architecture that combines a 3D graphics framework using HTML-like markup with a Document Object Model (DOM) synchronization technique to enable the simple creation of social, user generated, and persistent virtual worlds. By ensuring that relevant 3D scene data remains in the DOM and that synchronization takes place using a novel DOM synchronization mechanism, a social VR experience is created that allows multiple users to collaboratively modify the 3D environment in real-time.
SP  - 1
EP  - 6
JF  - 2016 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/civemsa.2016.7524313
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Siio, Itiro
TI  - CHI - Ohmic-Touch: Extending Touch Interaction by Indirect Touch through Resistive Objects
PY  - 2018
AB  - When an object is interposed between a touch surface and a finger/touch pen, the change in impedance caused by the object can be measured by the driver software. This phenomenon has been used to develop new interaction techniques. Unlike previous works that focused on the capacitance component in impedance, Ohmic-Touch enhances touch input modality by sensing resistance. Using 3D printers or inkjet printers with conductive materials and off-the-shelf electronic components/sensors, resistance is easily and precisely controllable. We implement mechanisms on touch surfaces based on the electrical resistance of the object: for example, to sense the touching position on an interposed object, to identify each object, and to sense light, force, or temperature by using resistors and sensors. Additionally, we conduct experimental studies that demonstrate that our technology has a recognition accuracy of the resistance value of 97%.
SP  - 521
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - 50
PB  - 
DO  - 10.1145/3173574.3174095
ER  - 

TY  - NA
AU  - Pei, Siyou; Chari, Pradyumna; Wang, Xue; Yang, Xiaoying; Kadambi, Achuta; Zhang, Yang
TI  - ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545622
ER  - 

TY  - JOUR
AU  - Geilinger, Moritz; Poranne, Roi; Desai, Ruta; Thomaszewski, Bernhard; Coros, Stelian
TI  - Skaterbots: optimization-based design and motion synthesis for robotic creatures with legs and wheels
PY  - 2018
AB  - We present a computation-driven approach to design optimization and motion synthesis for robotic creatures that locomote using arbitrary arrangements of legs and wheels. Through an intuitive interface, designers first create unique robots by combining different types of servomotors, 3D printable connectors, wheels and feet in a mix-and-match manner. With the resulting robot as input, a novel trajectory optimization formulation generates walking, rolling, gliding and skating motions. These motions emerge naturally based on the components used to design each individual robot. We exploit the particular structure of our formulation and make targeted simplifications to significantly accelerate the underlying numerical solver without compromising quality. This allows designers to interactively choreograph stable, physically-valid motions that are agile and compelling. We furthermore develop a suite of user-guided, semi-automatic, and fully-automatic optimization tools that enable motion-aware edits of the robot's physical structure. We demonstrate the efficacy of our design methodology by creating a diverse array of hybrid legged/wheeled mobile robots which we validate using physics simulation and through fabricated prototypes.
SP  - 1
EP  - 12
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201368
ER  - 

TY  - NA
AU  - Siivonen, Kari; Sainio, Joose; Viitanen, Marko; Vanne, Jarno; Hämäläinen, Timo
TI  - ISM - Open framework for error-compensated gaze data collection with eye tracking glasses
PY  - 2018
AB  - Eye tracking is nowadays the primary method for collecting training data for neural networks in the Human Visual System modelling. Our recommendation is to collect eye tracking data from videos with eye tracking glasses that are more affordable and applicable to diverse test conditions than conventionally used screen based eye trackers. Eye tracking glasses are prone to moving during the gaze data collection but our experiments show that the observed displacement error accumulates fairly linearly and can be compensated automatically by the proposed framework. This paper describes how our framework can be used in practice with videos up to 4K resolution. The proposed framework and the data collected during our sample experiment are made publicly available.
SP  - 299
EP  - 302
JF  - 2018 IEEE International Symposium on Multimedia (ISM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ism.2018.00067
ER  - 

TY  - NA
AU  - Chang, Minsuk; Huh, Mina; Kim, Juho
TI  - CHI - RubySlippers: Supporting Content-based Voice Navigation for How-to Videos
PY  - 2021
AB  - Directly manipulating the timeline, such as scrubbing for thumbnails, is the standard way of controlling how-to videos. However, when how-to videos involve physical activities, people inconveniently alternate between controlling the video and performing the tasks. Adopting a voice user interface allows people to control the video with voice while performing the tasks with hands. However, naively translating timeline manipulation into voice user interfaces (VUI) results in temporal referencing (e.g. “rewind 20 seconds”), which requires a different mental model for navigation and thereby limiting users’ ability to peek into the content. We present RubySlippers, a system that supports efficient content-based voice navigation through keyword-based queries. Our computational pipeline automatically detects referenceable elements in the video, and finds the video segmentation that minimizes the number of needed navigational commands. Our evaluation (N=12) shows that participants could perform three representative navigation tasks with fewer commands and less frustration using RubySlippers than the conventional voice-enabled video interface.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445131
ER  - 

TY  - CHAP
AU  - Voutilainen, Jari-Pekka; Mikkonen, Tommi; Systä, Kari
TI  - ICWE Workshops - Synchronizing Application State Using Virtual DOM Trees
PY  - 2016
AB  - We will all soon have numerous computing devices we use every day interchangeably. Liquid software, a concept where software is allowed to flow from one computer to another, is a programming framework that aims at simplifying the development and use of such multi-device software. The existing research has discovered three major architecture challenges for liquid software: (1) adaptation of the user interface to different devices, (2) availability of the relevant data in all devices, and (3) transfer of the application state. This paper addresses the last challenge and differs from the earlier work by concentrating in application state that is in the DOM tree, a key element in today’s Web applications.
SP  - 142
EP  - 154
JF  - Current Trends in Web Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-46963-8_12
ER  - 

TY  - JOUR
AU  - Yang, Yang; Song, Xuan; Li, Xiangjia; Chen, Zeyu; Zhou, Chi; Zhou, Qifa; Chen, Yong
TI  - Recent Progress in Biomimetic Additive Manufacturing Technology: From Materials to Functional Structures.
PY  - 2018
AB  - Nature has developed high-performance materials and structures over millions of years of evolution and provides valuable sources of inspiration for the design of next-generation structural materials, given the variety of excellent mechanical, hydrodynamic, optical, and electrical properties. Biomimicry, by learning from nature's concepts and design principles, is driving a paradigm shift in modern materials science and technology. However, the complicated structural architectures in nature far exceed the capability of traditional design and fabrication technologies, which hinders the progress of biomimetic study and its usage in engineering systems. Additive manufacturing (three-dimensional (3D) printing) has created new opportunities for manipulating and mimicking the intrinsically multiscale, multimaterial, and multifunctional structures in nature. Here, an overview of recent developments in 3D printing of biomimetic reinforced mechanics, shape changing, and hydrodynamic structures, as well as optical and electrical devices is provided. The inspirations are from various creatures such as nacre, lobster claw, pine cone, flowers, octopus, butterfly wing, fly eye, etc., and various 3D-printing technologies are discussed. Future opportunities for the development of biomimetic 3D-printing technology to fabricate next-generation functional materials and structures in mechanical, electrical, optical, and biomedical engineering are also outlined.
SP  - 1706539
EP  - NA
JF  - Advanced materials (Deerfield Beach, Fla.)
VL  - 30
IS  - 36
PB  - 
DO  - 10.1002/adma.201706539
ER  - 

TY  - NA
AU  - Götzelmann, Timo; Althaus, Christopher
TI  - PETRA - TouchSurfaceModels: Capacitive Sensing Objects through 3D Printers
PY  - 2016
AB  - Nowadays, 3D models can be downloaded from the internet and increasingly be printed by low cost 3D printers. In the future, blind people could benefit from this tendency. Unfortunately, many of these models are rather complex and not appropriate for the purely tactile exploration. To obtain quantitative data about how 3D printable models for blind people should be constructed, the tactile exploration can be recorded by video. However, the analysis of these videos is quite time consuming and expensive. Additionally, inaccuracies and masking effects may impede the use of this technique. In this paper we introduce a novel approach to automatically equip existing 3D models with a mesh of conductive wires which enable a touch sensitive surface for the printed 3D objects. These touch sensing 3D models can be printed in one turn by off-the-shelf 3D printers and used as an alternative to video recording. It allows exact registration of when and where the 3D object has been touched. In our multi-touch solution, particular attention has been paid to limit the number of necessary wires between 3D object and sensing electronics. Finally, our approach is evaluated by a feasibility study.
SP  - 22
EP  - NA
JF  - Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2910674.2910690
ER  - 

TY  - JOUR
AU  - Borowski, Marcel; Fog, Bjarke V.; Griggio, Carla F.; Eagan, James R.; Klokmose, Clemens N.
TI  - Between Principle and Pragmatism: Reflections on Prototyping Computational Media with Webstrates
PY  - 2022
AB  - <jats:p>Computational media describes a vision of software, which, in contrast to application-centric software, is (1) malleable, so users can modify existing functionality, (2) computable, so users can run custom code, (3) distributable, so users can open documents across different devices, and (4) shareable, so users can easily share and collaborate on documents. Over the last ten years, the Webstrates and Codestrates projects aimed to realize this vision of computational media. Webstrates is a server application that synchronizes the DOM of websites. Codestrates builds on top of Webstrates and adds an authoring environment, which blurs the use and development of applications. Grounded in a chronology of the development of Webstrates and Codestrates, we present eight tensions that we needed to balance during their development. We use these tensions as an analytical lens in three case studies and a game challenge in which participants created games using Codestrates. We discuss the results of the game challenge based on these tensions and present key takeaways for six of them. Finally, we present six lessons learned from our endeavor to realize the vision of computational media, demonstrating the balancing act of weighing the vision against the pragmatics of implementing a working system.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569895
ER  - 

TY  - NA
AU  - Roumen, Thijs; Shigeyama, Jotaro; Rudolph, Julius Cosmo Romeo; Grzelka, Felix; Baudisch, Patrick
TI  - UIST - SpringFit: Joints and Mounts that Fabricate on Any Laser Cutter
PY  - 2019
AB  - Joints are crucial to laser cutting as they allow making three-dimensional objects; mounts are crucial because they allow embedding technical components, such as motors. Unfortunately, mounts and joints tend to fail when trying to fabricate a model on a different laser cutter or from a different material. The reason for this lies in the way mounts and joints hold objects in place, which is by forc-ing them into slightly smaller openings. Such "press fit" mechanisms unfortunately are susceptible to the small changes in diameter that occur when switching to a ma-chine that removes more or less material ("kerf"), as well as to changes in stiffness, as they occur when switching to a different material. We present a software tool called springFit that resolves this problem by replacing the problematic press fit-based mounts and joints with what we call canti¬lever-based mounts and joints. A cantilever spring is simply a long thin piece of material that pushes against the object to be held. Unlike press fits, cantilever springs are robust against variations in kerf and material; they can even handle very high variations, simply by using longer springs. SpringFit converts models in the form of 2D cutting plans by replacing all contained mounts, notch joints, finger joints, and t-joints. In our technical evaluation, we used springFit to convert 14 models downloaded from the web.
SP  - 727
EP  - 738
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347930
ER  - 

TY  - JOUR
AU  - Kirshenbaum, Nurit; Davidson, Kylie; Harden, Jesse; North, Chris; Kobayashi, Dylan; Theriot, Ryan; Tabalba, Roderick S.; Rogers, Michael L.; Belcaid, NA; Burks, Andrew; Bharadwaj, Krishna; Renambot, Luc; Johnson, Andrew E.; Long, Lance; Leigh, Jason
TI  - Traces of Time through Space: Advantages of Creating Complex Canvases in Collaborative Meetings
PY  - 2021
AB  - Technology have long been a partner of workplace meeting facilitation. The recent outbreak of COVID-19 and the cautionary measures to reduce its spread have made it more prevalent than ever before in the form of online-meetings. In this paper, we recount our experiences during weekly meetings in three modalities: using SAGE2 - a collaborative sharing software designed for large displays - for co-located meetings, using a conventional projector for co-located meetings, and using the Zoom video-conferencing tool for distributed meetings. We view these meetings through the lens of effective meeting attributes and share ethnographic observations and attitudinal survey conducted in our research lab. We discuss patterns of content sharing, either sequential, parallel, or semi-parallel, and the potential advantages of creating complex canvases of content. We see how the SAGE2 tool affords parallel content sharing to create complex canvases, which represent queues of ideas and contributions (past, present, and future) using the space on a large display to suggest the progression of time through the meeting.
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3488552
ER  - 

TY  - NA
AU  - Jones, Michael; Seppi, Kevin D.; Olsen, Dan R.
TI  - CHI - What you Sculpt is What you Get: Modeling Physical Interactive Devices with Clay and 3D Printed Widgets
PY  - 2016
AB  - We present a method for fabricating prototypes of interactive computing devices from clay sculptures without requiring the designer to be skilled in CAD software. The method creates a "what you sculpt is what you get" process that mimics the "what you see is what you get" processes used in interface design for 2D screens. Our approach uses clay for modeling the basic shape of the device around 3D printed representations, which we call "blanks", of physical interaction widgets such as buttons, sliders, knobs and other electronics. Each blank includes 4 fiducial markers uniquely arranged on a visible surface. After scanning the sculpture, these fiducial marks allow our software to identify widget types and locations in the scanned model. The software then converts the scan into a printable prototype by positioning mounting surfaces, openings for the controls and a splitting plane for assembly. Because the blanks fit in the sculpted shape, they will reliably fit in the interactive prototype. Creating an interactive prototype requires about 30 minutes of human effort for sculpting, and after scanning, involves a single button click to use the process.
SP  - 876
EP  - 886
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858493
ER  - 

TY  - NA
AU  - Alvina, Jessalyn; Bunt, Andrea; Chilana, Parmit K.; Malacria, Sylvain; McGrenere, Joanna
TI  - Conference on Designing Interactive Systems - Where is that Feature?: Designing for Cross-Device Software Learnability
PY  - 2020
AB  - People increasingly access cross-device applications from their smartphones while on the go. Yet, they do not fully use the mobile versions for complex tasks, preferring the desktop version of the same application. We conducted a survey (N=77) to identify challenges when switching back and forth between devices. We discovered significant cross-device learnability issues, including that users often find exploring the mobile version frustrating, which leads to prematurely giving up on using the mobile version. Based on the findings, we created four design concepts as video prototypes to explore how to support cross-device learnability. The concepts vary in four key dimensions: the device involved, automation, temporality, and learning approach. Interviews (N=20) probing the design concepts identified individual differences affecting cross-device learning preferences, and that users are more motivated to use cross-device applications when offered the right cross-device learnability support. We conclude with future design directions for supporting seamless cross-device learnability.
SP  - 1103
EP  - 1115
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395506
ER  - 

TY  - NA
AU  - Gadea, Cristian
TI  - Architectures and Algorithms for Real-Time Web-Based Collaboration
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.20381/ruor-26166
ER  - 

TY  - JOUR
AU  - Kim, WonJin; Kim, Minseong; Kim, GeunHyung
TI  - 3D-Printed Biomimetic Scaffold Simulating Microfibril Muscle Structure
PY  - 2018
AB  - NA
SP  - 1800405
EP  - NA
JF  - Advanced Functional Materials
VL  - 28
IS  - 26
PB  - 
DO  - 10.1002/adfm.201800405
ER  - 

TY  - BOOK
AU  - Kobayashi, Dylan; Ready, Matthew; Martinez, Alberto Gonzalez; Kirshenbaum, Nurit; Seto-Mook, Tyson; Leigh, Jason; Haga, Jason H.
TI  - ISS - Sage River Disaster Information (SageRDI): Demonstrating Application Data Sharing In SAGE2
PY  - 2018
AB  - The Scalable Amplified Group Environment (SAGE2) is an open-source, web-based middleware for driving tiled display walls. SAGE2 allows running multiple applications at once within its workspace. In large display walls, users tend to collaborate using multiple applications in the same space for simultaneous interaction and review. Unfortunately, many of these applications were created independently by different developers and were never intended to interoperate which greatly limits their potential reusability. SAGE2 developers face system limitations where applications are data segregated and cannot easily communicate with others. To counter this problem, we developed the SAGE2 data sharing components. We describe the Sage River Disaster Information (SageRDI) application and the SAGE2 architectural implementations necessary for its operation. SageRDI enables river.go.jp, an existing website that provides water sensor data for Japan, to interact with other SAGE2 applications without modifying the website's server, hosted files, nor any of the default SAGE2 applications.
SP  - 33
EP  - 42
JF  - Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3279778.3279798
ER  - 

TY  - NA
AU  - Bhat, Gayatri; Saluja, Avneesh; Dye, Melody; Florjanczyk, Jan
TI  - Hierarchical Encoders for Modeling and Interpreting Screenplays
PY  - 2020
AB  - While natural language understanding of long-form documents is still an open challenge, such documents often contain structural information that can inform the design of models for encoding them. Movie scripts are an example of such richly structured text - scripts are segmented into scenes, which are further decomposed into dialogue and descriptive components. In this work, we propose a neural architecture for encoding this structure, which performs robustly on a pair of multi-label tag classification datasets, without the need for handcrafted features. We add a layer of insight by augmenting an unsupervised "interpretability" module to the encoder, allowing for the extraction and visualization of narrative trajectories. Though this work specifically tackles screenplays, we discuss how the underlying approach can be generalized to a range of structured documents.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Fuhl, Wolfgang
TI  - 1000 Pupil Segmentations in a Second using Haar Like Features and Statistical Learning
PY  - 2021
AB  - In this paper we present a new approach for pupil segmentation. It can be computed and trained very efficiently, making it ideal for online use for high speed eye trackers as well as for energy saving pupil detection in mobile eye tracking. The approach is inspired by the BORE and CBF algorithms and generalizes the binary comparison by Haar features. Since these features are intrinsically very susceptible to noise and fluctuating light conditions, we combine them with conditional pupil shape probabilities. In addition, we also rank each feature according to its importance in determining the pupil shape. Another advantage of our method is the use of statistical learning, which is very efficient and can even be used online. this https URL
SP  - NA
EP  - NA
JF  - arXiv: Image and Video Processing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Nisser, Martin; Cheng, Leon; Makaram, Yashaswini; Suzuki, Ryo; Mueller, Stefanie
TI  - UIST (Adjunct Volume) - Programmable Polarities: Actuating Interactive Prototypes with Programmable Electromagnets
PY  - 2021
AB  - This demo introduces a framework that uses programmable electromagnets as a method to rapidly prototype interactive objects. Our approach allows users to to quickly and inexpensively embed actuation mechanisms into otherwise static prototypes in order to make them dynamic and interactive. Underpinning the technique is the insight of using electromagnets to interchangeably create attractive and repulsive forces between adjacent parts, and programmatically setting their polarities in a way that allows objects to translate rotationally and linearly, respond haptically, assemble, and locomote.
SP  - 121
EP  - 123
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480198
ER  - 

TY  - NA
AU  - Nisser, Martin; Makaram, Yashaswini; Covarrubias, Lucian; Bah, Amadou Yaye; Faruqi, Faraz; Suzuki, Ryo; Mueller, Stefanie
TI  - Mixels: Fabricating Interfaces using Programmable Magnetic Pixels
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545698
ER  - 

TY  - NA
AU  - Tseng, Tiffany; Kawahara, Yoshihiro
TI  - Conference on Designing Interactive Systems - Circuit Assemblies: Electronic Modules for Interactive 3D-Prints
PY  - 2021
AB  - This pictorial presents Circuit Assemblies, a design system for beginners to create 3D-printed interactive objects with embedded electronics. Circuit Assemblies are modules used to create objects that light up, move, or spin using basic electronic components like LEDs, batteries, and motors. To support beginners incorporating Circuit Assemblies into 3D designs, a set of virtual components were added to Tinkercad, a popular browser-based 3D CAD application with over 10 million users. In this paper, we begin with a set of design considerations gathered from interviews with three K-12 educators that teach electronics. We then present four different Circuit Assembly modules designed with these considerations in mind, highlighting the unique challenges that arise from combining electronics and 3D design for beginners, both in CAD software and physical assembly. We then present four different Circuit Assembly modules designed with these considerations in mind, highlighting the unique challenges that arise from combining electronics and 3D design for beginners, both in CAD software and physical assembly.
SP  - 1115
EP  - 1128
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462024
ER  - 

TY  - NA
AU  - Berry, Jaclyn
TI  - The good, the bad, and the facts : multimodal representation of medical conversations for patient understanding
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Othman, Mohammad; Amaral, Telmo; McNaney, Roisin; Smeddinck, Jan D.; Vines, John; Olivier, Patrick
TI  - MobileHCI - CrowdEyes: crowdsourcing for robust real-world mobile eye tracking
PY  - 2017
AB  - Current eye tracking technologies have a number of drawbacks when it comes to practical use in real-world settings. Common challenges, such as high levels of daylight, eyewear (e.g. spectacles or contact lenses) and eye make-up, give rise to noise that undermines their utility as a standard component for mobile computing, design, and evaluation. To work around these challenges, we introduce CrowdEyes, a mobile eye tracking solution that utilizes crowdsourcing for increased tracking accuracy and robustness. We present a pupil detection task design for crowd workers together with a study that demonstrates the high-level accuracy of crowdsourced pupil detection in comparison to state-of-the-art pupil detection algorithms. We further demonstrate the utility of our crowdsourced analysis pipeline in a fixation tagging task. In this paper, we validate the accuracy and robustness of harnessing the crowd as both an alternative and complement to automated pupil detection algorithms, and explore the associated costs and quality of our crowdsourcing approach.
SP  - 18
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3098559
ER  - 

TY  - NA
AU  - Aoki, Hiroto; Rekimoto, Jun
TI  - SUI - Extramission: A Large Scale Interactive Virtual Environment Using Head Mounted Projectors and Retro-reflectors
PY  - 2019
AB  - We present Extramission, a method to a large scale interactive virtual environment. It consists of dual head mounted pico projectors and retro-reflective materials. With high-accuracy retro-reflective materials, laser beams scanned on user’s retina makes clear and free-focus vision. In this retinal scanning configuration, even if the luminance of the projector is low, scanned images can be seen clearly, which helps to evade overlaps between projected images. Due to small overlaps, Extramission can provide multi-user virtual experiences showing different images to each individual, and dual pico projectors can provide each user with stereoscopic vision. Moreover, the tolerance of low luminance allows larger distance between users and retro-reflectors, which is required for large scale virtual experiences using head mounted projectors. In this paper, we describe the principle and the implementation of Extramission. We also see its performance of displaying images.
SP  - 8
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357251.3357592
ER  - 

TY  - NA
AU  - Borowski, Marcel; Klokmose, Clemens Nylandsted
TI  - Webstrates, Codestrates v2, and Varv: A Software Stack for Computational Media
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3547522.3547714
ER  - 

TY  - NA
AU  - Uddin, Sami; Gutwin, Carl; Cockburn, Andy
TI  - CHI - The Effects of Artificial Landmarks on Learning and Performance in Spatial-Memory Interfaces
PY  - 2017
AB  - Spatial memory is a powerful way for users to become expert with an interface, because remembering item locations means that users do not have to carry out slow visual search. Spatial learning in the real world benefits greatly from landmarks in the environment, but user interfaces often provide very few visual landmarks. In this paper we explore the use of artificial landmarks as a way to improve people's spatial memory in spatially-stable grid menus called CommandMaps. We carried out three studies to test the effects of three types of artificial landmarks (standard grid, simple anchor marks, and a transparent image) on spatial learning. We found that for small grid menus, the artificial landmarks had little impact on performance, whereas for medium and large grids, the simple anchor marks significantly improved performance. The simple visual anchors were faster and less error-prone than the visually richer transparent image. Our studies show that artificial landmarks can be a valuable addition to spatial interfaces.
SP  - 3843
EP  - 3855
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025497
ER  - 

TY  - NA
AU  - Yan, Xin; Lu, Lin; Sharf, Andrei; Yu, Xing; Sun, Yulu
TI  - Man-made by Computer: On-the-Fly Fine Texture 3D Printing
PY  - 2021
AB  - Applying textures to 3D models are means for creating realistic looking objects. This is especially important in the 3D manufacturing domain as manufactured models should ideally comprise a natural and realistic appearance. Nevertheless, natural material textures usually consist of dense patterns and fine details. Their embedding onto 3D models is typically cumbersome, requiring large processing time and resulting in large size meshes. This paper presents a novel approach for direct embedding of fine scale geometric textures onto 3D printed models by on-the-fly modification of the 3D printer’s head. Our idea is to embed 3D textures by revising the 3D printer’s G-code, i.e., incorporating texture details through modification of the printer’s path. Direct manipulation of the printer’s head movement allows for fine-scale texture mapping and editing on-the-fly in the 3D printing process. Thus, our method avoids the computationally expensive texture mapping, mesh processing and manufacturing preprocessing. This allows embedding detailed geometric textures of unlimited density which can model manual manufacturing artifacts and natural material properties. Results demonstrate that our direct G-code textured models are printed robustly and efficiently in both space and time compared to traditional methods.
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485114.3485119
ER  - 

TY  - NA
AU  - George, Anjith; Routray, Aurobinda
TI  - ESCaF: Pupil Centre Localization Algorithm with Candidate Filtering.
PY  - 2018
AB  - Algorithms for accurate localization of pupil centre is essential for gaze tracking in real world conditions. Most of the algorithms fail in real world conditions like illumination variations, contact lenses, glasses, eye makeup, motion blur, noise, etc. We propose a new algorithm which improves the detection rate in real world conditions. The proposed algorithm uses both edges as well as intensity information along with a candidate filtering approach to identify the best pupil candidate. A simple tracking scheme has also been added which improves the processing speed. The algorithm has been evaluated in Labelled Pupil in the Wild (LPW) dataset, largest in its class which contains real world conditions. The proposed algorithm outperformed the state of the art algorithms while achieving real-time performance.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Eriksson, Eva; Iversen, Ole Sejer; Baykal, Gökçe Elif; Van Mechelen, Maarten; Smith, Rachel Charlotte; Wagner, Marie-Louise; Fog, Bjarke Vognstrup; Klokmose, Clemens Nylandsted; Cumbo, Bronwyn J.; Hjorth, Arthur; Musaeus, Line Have; Petersen, Marianne Graves; Bouvin, Niels Olof
TI  - FabLearn - Widening the scope of FabLearn Research: Integrating Computational Thinking, Design and Making
PY  - 2019
AB  - FabLearn has primarily been concerned with studies of digital fabrication technologies in education, however, we witness an increased interest in integrating other related topics such as computational thinking, digital design and empowerment as an integrated whole. In this paper, we present a five years design research program for digital fabrication, computational thinking and design, to highlight why the FabLearn community should embrace this wider agenda to accomplish its ultimate goal to encourage a new generation to critically and constructively engage in the design of digital technology. The contribution of this paper is a number of open questions and considerations regarding the scope of European FabLearn research that we hope the community will consider and that might give rise to further discussions.
SP  - 3335070
EP  - NA
JF  - Proceedings of the FabLearn Europe 2019 Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3335055.3335070
ER  - 

TY  - JOUR
AU  - Kim, Kyuyoung; Park, Jaeho; Suh, Ji-Hoon; Kim, Min Seong; Jeong, Yongrok; Park, Inkyu
TI  - 3D printing of multiaxial force sensors using carbon nanotube (CNT)/thermoplastic polyurethane (TPU) filaments
PY  - 2017
AB  - NA
SP  - 493
EP  - 500
JF  - Sensors and Actuators A: Physical
VL  - 263
IS  - NA
PB  - 
DO  - 10.1016/j.sna.2017.07.020
ER  - 

TY  - NA
AU  - Truong, Anh; Berthouzoz, Floraine; Li, Wilmot; Agrawala, Maneesh
TI  - UIST - QuickCut: An Interactive Tool for Editing Narrated Video
PY  - 2016
AB  - We present QuickCut, an interactive video editing tool designed to help authors efficiently edit narrated video. QuickCut takes an audio recording of the narration voiceover and a collection of raw video footage as input. Users then review the raw footage and provide spoken annotations describing the relevant actions and objects in the scene. QuickCut time-aligns a transcript of the annotations with the raw footage and a transcript of the narration to the voiceover. These aligned transcripts enable authors to quickly match story events in the narration with semantically relevant video segments and form alignment constraints between them. Given a set of such constraints, QuickCut applies dynamic programming optimization to choose frame-level cut points between the video segments while maintaining alignments with the narration and adhering to low-level film editing guidelines. We demonstrate QuickCut's effectiveness by using it to generate a variety of short (less than 2 minutes) narrated videos. Each result required between 14 and 52 minutes of user time to edit (i.e. between 8 and 31 minutes for each minute of output video), which is far less than typical authoring times with existing video editing workflows.
SP  - 497
EP  - 507
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984569
ER  - 

TY  - NA
AU  - Hamdan, Nur Al-huda; Voelker, Simon; Borchers, Jan
TI  - CHI - Sketch&Stitch: Interactive Embroidery for E-textiles
PY  - 2018
AB  - E-Textiles are fabrics that integrate electronic circuits and components. Makers use them to create interactive clothing, furniture, and toys. However, this requires significant manual labor and skills, and using technology-centric design tools. We introduce Sketch&Stitch, an interactive embroidery system to create e-textiles using a traditional crafting approach: Users draw their art and circuit directly on fabric using colored pens. The system takes a picture of the sketch, converts it to embroidery patterns, and sends them to an embroidery machine. Alternating between sketching and stitching, users build and test their design incrementally. Sketch&Stitch features Circuitry Stickers representing circuit boards, components, and custom stitch patterns for wire crossings to insulate, and various textile touch sensors such as pushbuttons, sliders, and 2D touchpads. Circuitry Stickers serve as placeholders during design. Using computer vision, they are recognized and replaced later in the appropriate embroidery phases. We close with technical considerations and application examples.
SP  - 82
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173656
ER  - 

TY  - BOOK
AU  - Huang, Michael Xuelin; Bulling, Andreas
TI  - ETRA - SacCalib: reducing calibration distortion for stationary eye trackers using saccadic eye movements
PY  - 2019
AB  - Recent methods to automatically calibrate stationary eye trackers were shown to effectively reduce inherent calibration distortion. However, these methods require additional information, such as mouse clicks or on-screen content. We propose the first method that only requires users' eye movements to reduce calibration distortion in the background while users naturally look at an interface. Our method exploits that calibration distortion makes straight saccade trajectories appear curved between the saccadic start and end points. We show that this curving effect is systematic and the result of distorted gaze projection plane. To mitigate calibration distortion, our method undistorts this plane by straightening saccade trajectories using image warping. We show that this approach improves over the common six-point calibration and is promising for reducing distortion. As such, it provides a non-intrusive solution to alleviating accuracy decrease of eye tracker during long-term use.
SP  - 71
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3317956.3321553
ER  - 

TY  - NA
AU  - Desai, Ruta; McCann, James; Coros, Stelian
TI  - UIST - Assembly-aware Design of Printable Electromechanical Devices
PY  - 2018
AB  - From smart toys and household appliances to personal robots, electromechanical devices play an increasingly important role in our daily lives. Rather than relying on gadgets that are mass-produced, our goal is to enable casual users to custom-design such devices based on their own needs and preferences. To this end, we present a computational design system that leverages the power of digital fabrication and the emergence of affordable electronics such as sensors and microcontrollers. The input to our system consists of a 3D representation of the desired device's shape, and a set of user-preferred off-the-shelf components. Based on this input, our method generates an optimized, 3D printable enclosure that can house the required components. To create these designs automatically, we formalize a new spatio-temporal model that captures the entire assembly process, including the placement of the components within the device, mounting structures and attachment strategies, the order in which components must be inserted, and collision-free assembly paths. Using this model as a technical core, we then leverage engineering design guidelines and efficient numerical techniques to optimize device designs. In a user study, which also highlights the challenges of designing such devices, we find our system to be effective in reducing the entry barriers faced by casual users in creating such devices. We further demonstrate the versatility of our approach by designing and fabricating three devices with diverse functionalities.
SP  - 457
EP  - 472
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242655
ER  - 

TY  - NA
AU  - Lin, Ying-Ju; Punpongsanon, Parinya; Wen, Xin; Iwai, Daisuke; Sato, Kosuke; Obrist, Marianna; Mueller, Stefanie
TI  - CHI - FoodFab: Creating Food Perception Illusions using Food 3D Printing
PY  - 2020
AB  - Personalization of eating such that everyone consumes only what they need allows improving our management of food waste. In this paper, we explore the use of food 3D printing to create perceptual illusions for controlling the level of perceived satiety given a defined amount of calories. We present FoodFab, a system that allows users to control their food intake through modifying a food's internal structure via two 3D printing parameters: infill pattern and infill density. In two experiments with a total of 30 participants, we studied the effect of these parameters on users' chewing time that is known to affect people's feeling of satiety. Our results show that we can indeed modify the chewing time by varying infill pattern and density, and thus control perceived satiety. Based on the results, we propose two computational models and integrate them into a user interface that simplifies the creation of personalized food structures.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376421
ER  - 

TY  - NA
AU  - Wang, Jianan; Li, Boyang; Fan, Xiangyu; Lin, Jing; Fu, Yanwei
TI  - WACV - Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions
PY  - 2021
AB  - The task of video and text sequence alignment is a pre-requisite step toward joint understanding of movie videos and screenplays. However, supervised methods face the obstacle of limited realistic training data. With this pa-per, we attempt to enhance data efficiency of the end-to-end alignment network NeuMATCH [15]. Recent research [56] suggests that network components dealing with different modalities may overfit and generalize at different speeds, creating difficulties for training. We propose to employ (1) layer-wise adaptive rate scaling (LARS) to align the magnitudes of gradient updates in different layers and balance the pace of learning and (2) sequence-wise batch normalization (SBN) to align the internal feature distributions from different modalities. Finally, we leverage random projection to reduce the dimensionality of input features. On the YouTube Movie Summary dataset, the combined use of these technique closes the performance gap when the pretraining on the LSMDC dataset is omitted and achieves the state-of-the-art result. Extensive empirical comparisons and analysis reveal that these techniques improve optimization and regularize the network more effectively than two different setups of layer normalization.
SP  - 665
EP  - 675
JF  - 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wacv48630.2021.00071
ER  - 

TY  - JOUR
AU  - Kim, Nam Wook; Bach, Benjamin; Im, Hyejin; Schriber, Sasha; Gross, Markus; Pfister, Hanspeter
TI  - Visualizing Nonlinear Narratives with Story Curves
PY  - 2017
AB  - In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.
SP  - 595
EP  - 604
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2017.2744118
ER  - 

TY  - NA
AU  - Gadea, Cristian; Ionescu, Bogdan; Ionescu, Dan
TI  - SACI - A Control Loop-based Algorithm for Operational Transformation
PY  - 2020
AB  - Operational Transformation (OT) has emerged as a viable theoretical principle for the implementation of real-time collaboration applications. In such systems, the collaboration consists of operations generated by members of a group who are performing concurrent actions on the same document or content. This powerful multi-user co-editing has been researched ever since the seminal works of the late 1980s. As the web evolved into a dominant platform for content consumption and creation, classes of algorithms like OT and Conflict-free Replicated Data Types (CRDT) have enabled flexible content synchronization for applications such as online word processors. Despite their long history in academia, OT and CRDT continue to have unsolved issues due to the centralized approach required for scalable and reliable web-based document editing. This paper proposes a Control Loop-based OT approach based on a serverless architecture and on Finite State Automata (FSA). A control loop principle is used to design a series of algorithms for distributed conflict resolution. The proposed architecture consists of a series of blocks which internally contain a number of multi-level Finite State Machines. The architecture of the new serverless approach for OT is introduced and the basic FSAs that model the co-editing processes are described. Cases encountered in the dynamics of the co-editing processes were modeled to prove that the essential OT properties of causality preservation, convergence, and intention preservation are all satisfied. Simulation results are given at the end of the paper.
SP  - 000247
EP  - 000254
JF  - 2020 IEEE 14th International Symposium on Applied Computational Intelligence and Informatics (SACI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/saci49304.2020.9118822
ER  - 

TY  - NA
AU  - Marky, Karola; Schmitz, Martin; Zimmermann, Verena; Herbers, Martin; Kunze, Kai; Mühlhäuser, Max
TI  - CHI - 3D-Auth: Two-Factor Authentication with Personalized 3D-Printed Items
PY  - 2020
AB  - Two-factor authentication is a widely recommended security mechanism and already offered for different services. However, known methods and physical realizations exhibit considerable usability and customization issues. In this paper, we propose 3D-Auth, a new concept of two-factor authentication. 3D-Auth is based on customizable 3D-printed items that combine two authentication factors in one object. The object bottom contains a uniform grid of conductive dots that are connected to a unique embedded structure inside the item. Based on the interaction with the item, different dots turn into touch-points and form an authentication pattern. This pattern can be recognized by a capacitive touchscreen. Based on an expert design study, we present an interaction space with six categories of possible authentication interactions. In a user study, we demonstrate the feasibility of 3D-Auth items and show that the items are easy to use and the interactions are easy to remember.
SP  - 3376189
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376189
ER  - 

TY  - JOUR
AU  - Hajas, Daniel; Ablart, Damien; Schneider, Oliver; Obrist, Marianna
TI  - I can feel it moving: Science Communicators Talking About the Potential of Mid-Air Haptics
PY  - 2020
AB  - We explored the potential of haptics for improving science communication, and discovered the mid-air haptic feedback supports flexible, engaging discussions in a way that other technologies, like 3D printed models and VR headsets, may not. While science instruction often focuses on the cognitive domain of acquiring new knowledge, in science communication the primary goal is to produce personal responses, such as awareness, enjoyment, or interest in science. Science communicators often use new technologies to produce personal responses. Here, we explore how mid-air haptics technology could play a role in communicating scientific concepts. We prototyped six mid-air haptic probes for three thematic areas (i.e. particle physics, quantum mechanics, cell biology) and conducted three workshops with domain experts who are also active science communicators. Participants were impressed by the dynamic features of mid-air haptics, its ability to produce shared experiences, and its flexibility in communicating scientific concepts. We discuss how mid-air haptics can complement existing approaches (e.g. 3D printing, virtual reality) and help create enjoyment, interest, and maybe understanding in science
SP  - 534974
EP  - NA
JF  - Frontiers in Computer Science
VL  - 2
IS  - NA
PB  - 
DO  - 10.3389/fcomp.2020.534974
ER  - 

TY  - NA
AU  - Otto, Michael
TI  - The virtual manufacturing station : a framework for collaborative assessment of manual assembly tasks
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Hiroi, Yuichi; Itot, Yuta; Hamasaki, Takumi; Iwai, Daisuke; Sugimoto, Maki
TI  - VR - HySAR: Hybrid material rendering by an optical see-through head-mounted display with spatial augmented reality projection
PY  - 2017
AB  - We propose a hybrid SAR concept combining a projector and Optical See-Through Head-Mounted Displays (OST-HMD). Our proposed hybrid SAR system utilizes OST-HMD as an extra rendering layer to render a view-dependent property in OST-HMDs according to the viewer's viewpoint. Combined with view-independent components created by a static projector, the viewer can see richer material contents. Unlike conventional SAR systems, our system theoretically allows unlimited number of viewers seeing enhanced contents in the same space while keeping the existing SAR experiences. Furthermore, the system enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With a proof-of-concept system that consists of a projector and an OST-HMD, we qualitatively demonstrate that our system successfully creates hybrid rendering on a hemisphere object from five horizontal viewpoints. Our quantitative evaluation also shows that our system increases the dynamic range by 2.1 times and the maximum intensity by 1.9 times compared to an ordinary SAR system.
SP  - 211
EP  - 212
JF  - 2017 IEEE Virtual Reality (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2017.7892251
ER  - 

TY  - NA
AU  - Zhang, Xucong; Huang, Michael Xuelin; Sugano, Yusuke; Bulling, Andreas
TI  - CHI - Training Person-Specific Gaze Estimators from User Interactions with Multiple Devices
PY  - 2018
AB  - Learning-based gaze estimation has significant potential to enable attentive user interfaces and gaze-based interaction on the billions of camera-equipped handheld devices and ambient displays. While training accurate person- and device-independent gaze estimators remains challenging, person-specific training is feasible but requires tedious data collection for each target device. To address these limitations, we present the first method to train person-specific gaze estimators across multiple devices. At the core of our method is a single convolutional neural network with shared feature extraction layers and device-specific branches that we train from face images and corresponding on-screen gaze locations. Detailed evaluations on a new dataset of interactions with five common devices (mobile phone, tablet, laptop, desktop computer, smart TV) and three common applications (mobile game, text editing, media center) demonstrate the significant potential of cross-device training. We further explore training with gaze locations derived from natural interactions, such as mouse or touch input.
SP  - 624
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174198
ER  - 

TY  - NA
AU  - Grønbæk, Jens Emil; Saatçi, Banu; Griggio, Carla F.; Klokmose, Clemens Nylandsted
TI  - CHI - MirrorBlender: Supporting Hybrid Meetings with a Malleable Video-Conferencing System
PY  - 2021
AB  - In hybrid meetings, multiple co-located participants communicate with remote participants through video. But video communication inhibits non-verbal cues, and this often causes remote participants to feel excluded. To address this issue, we built MirrorBlender: a What-You-See-Is-What-I-See video-conferencing system for blending, repositioning, and resizing mirrors. Mirrors here denote shared video feeds of people and screens. In a qualitative study of MirrorBlender with three hybrid meeting sessions, we found that the shared control of mirrors supported users in negotiating a blended interpersonal space. Moreover, it enabled diverse acts of inclusion of remote participants. In particular, remote participants brought attention to themselves by manipulating the position, scale, and translucency of their camera and screen feeds. Participants also embodied and leveraged their mirror images for deictic gestures and playful interactions. Based on these findings, we discuss new opportunities for supporting video-mediated collaboration.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445698
ER  - 

TY  - NA
AU  - Schön, Dominik; Kosch, Thomas; Schmitz, Martin; Müller, Florian; Günther, Sebastian; Kreutz, Johannes; Mühlhäuser, Max
TI  - TrackItPipe: A Fabrication Pipeline To Incorporate Location and Rotation Tracking Into 3D Printed Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558719
ER  - 

TY  - NA
AU  - Tejada, Carlos E.
TI  - CHI Extended Abstracts - Print-and-Play: 3D-printed Interactive Objects Without Assembly or Calibration
PY  - 2020
AB  - In recent years, 3D-printing technology has become widely accessible to non-experts and hobbyists, enabling them to fabricate objects of varying geometries. In contrast to this new ease of producing new forms, fabricating objects that can sense user input traditionally requires the assembly of complex circuits and physical parts. With my work, I explore what I call \pap: 3D-printed interactive objects without requiring any post-print activities such as assembly or calibration. I approach this by by externally sensing how the user's actions interact with well-studied physical phenomena (e.g., acoustic resonance and fluid dynamics). I have developed two novel techniques using this principle: , a blow-based interaction technique for fabricated objects using acoustic resonance; and , a technique for fabricating touch-sensitive objects using pneumatic sensing and fluid dynamics principles.
SP  - 1
EP  - 6
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375025
ER  - 

TY  - NA
AU  - Ru, Zhang; Shi, Yuanchun; Schuller, Björn; André, Elisabeth; Oviatt, Sharon; Quigley, Aaron; Marquardt, Nicolai; Aslan, Ilhan; Ju, Ran
TI  - CHI Extended Abstracts - User Experience for Multi-Device Ecosystems: Challenges and Opportunities
PY  - 2021
AB  - Smart devices have pervaded every aspect of humans’ daily lives. Though single device UX products are relatively successful, the experience of cross-device interaction is still far from satisfactory and can be a source of frustration. Inconsistent UI styles, unclear coordination, varying fidelity, pairwise interactions, lack of understanding intent, limited data sharing and security, and other problems typically degrade the experience in a multi-device ecosystem. Redesigning the UX, tailored to multi-device ecosystems to enhance the user experience, turns out to be challenging but at the same time affording many new opportunities. This workshop brings together researchers, practitioners and developers with different backgrounds, including from fields such as computationally design, affective computing, and multimodal interaction to exchange views, share ideas, and explore future directions on UX for distributed scenarios, especially for those heterogeneous cross-device ecosystems. The topics cover but are not limited to distributed UX design, accessibility, cross-device HCI, human factors in distributed scenarios, user-centric interfaces, and multi-device ecosystems.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3441325
ER  - 

TY  - NA
AU  - Lu, Chin-Yuan; Hsieh, Han-Wei; Liang, Rong-Hao; Lee, Chi-Jung; Yang, Ling-Chien; Xue, Mengru; Guo, Jr-Ling; Hsieh, Meng-Ju; Chen, Bing-Yu
TI  - CHI - Combining Touchscreens with Passive Rich-ID Building Blocks to Support Context Construction in Touchscreen Interactions
PY  - 2021
AB  - This research investigates the design space of combining touchscreens with passive rich-ID building block systems to support the physical construction of contexts in touchscreen interactions. With two proof-of-concept systems, RFIPillars and RFITiles, we explore various schemes for using tangible inputs for context enrichment in touchscreen interactions. Instead of incorporating an electronic touchscreen module that requires per-module maintenance, this work intentionally makes each tangible object passive. We explore rear-projection solutions to integrate touchscreen interactions into these passive building blocks with capacitive touch sensing techniques and deliberate physical forgiving to retain the merits of being both batteryless and wireless. The presented research artifacts embody the interaction designs and elucidate scalability challenges in integrating touchscreen interactions into this emerging tangible user interface.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445722
ER  - 

TY  - BOOK
AU  - Brudy, Frederik; Houben, Steven; Marquardt, Nicolai; Rogers, Yvonne
TI  - ISS - CurationSpace: Cross-Device Content Curation Using Instrumental Interaction
PY  - 2016
AB  - For digital content curation of historical artefacts, curators collaboratively collect, analyze and edit documents, images, and other digital resources in order to display and share new representations of that information to an audience. Despite their increasing reliance on digital documents and tools, current technologies provide little support for these specific collaborative content curation activities. We introduce CurationSpace -- a novel cross-device system to provide more expressive tools for curating and composing digital historical artefacts. Based on the concept of Instrumental Interaction, CurationSpace allows users to interact with digital curation artefacts on shared interactive surfaces using personal smartwatches as selectors for instruments or modifiers (applied to either the whole curation space, individual documents, or fragments). We introduce a range of novel interaction techniques that allow individuals or groups of curators to more easily create, navigate and share resources during content curation. We report insights from our user study about people's use of instruments and modifiers for curation activities.
SP  - 159
EP  - 168
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992175
ER  - 

TY  - NA
AU  - Fu, Daniel Y.; Crichton, Will; Hong, James; Yao, Xinwei; Zhang, Haotian; Truong, Anh; Narayan, Avanika; Agrawala, Maneesh; Ré, Christopher; Fatahalian, Kayvon
TI  - Rekall: Specifying Video Events using Compositions of Spatiotemporal Labels
PY  - 2019
AB  - Many real-world video analysis applications require the ability to identify domain-specific events in video, such as interviews and commercials in TV news broadcasts, or action sequences in film. Unfortunately, pre-trained models to detect all the events of interest in video may not exist, and training new models from scratch can be costly and labor-intensive. In this paper, we explore the utility of specifying new events in video in a more traditional manner: by writing queries that compose outputs of existing, pre-trained models. To write these queries, we have developed Rekall, a library that exposes a data model and programming model for compositional video event specification. Rekall represents video annotations from different sources (object detectors, transcripts, etc.) as spatiotemporal labels associated with continuous volumes of spacetime in a video, and provides operators for composing labels into queries that model new video events. We demonstrate the use of Rekall in analyzing video from cable TV news broadcasts, films, static-camera vehicular video streams, and commercial autonomous vehicle logs. In these efforts, domain experts were able to quickly (in a few hours to a day) author queries that enabled the accurate detection of new events (on par with, and in some cases much more accurate than, learned approaches) and to rapidly retrieve video clips for human-in-the-loop tasks such as video content curation and training data curation. Finally, in a user study, novice users of Rekall were able to author queries to retrieve new events in video given just one hour of query development time.
SP  - NA
EP  - NA
JF  - arXiv: Databases
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yao, Xinwei; Fried, Ohad; Fatahalian, Kayvon; Agrawala, Maneesh
TI  - Iterative Text-based Editing of Talking-heads Using Neural Retargeting.
PY  - 2020
AB  - We present a text-based tool for editing talking-head video that enables an iterative editing workflow. On each iteration users can edit the wording of the speech, further refine mouth motions if necessary to reduce artifacts and manipulate non-verbal aspects of the performance by inserting mouth gestures (e.g. a smile) or changing the overall performance style (e.g. energetic, mumble). Our tool requires only 2-3 minutes of the target actor video and it synthesizes the video for each iteration in about 40 seconds, allowing users to quickly explore many editing possibilities as they iterate. Our approach is based on two key ideas. (1) We develop a fast phoneme search algorithm that can quickly identify phoneme-level subsequences of the source repository video that best match a desired edit. This enables our fast iteration loop. (2) We leverage a large repository of video of a source actor and develop a new self-supervised neural retargeting technique for transferring the mouth motions of the source actor to the target actor. This allows us to work with relatively short target actor videos, making our approach applicable in many real-world editing scenarios. Finally, our refinement and performance controls give users the ability to further fine-tune the synthesized results.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Bouvin, Niels Olof; Klokmose, Clemens Nylandsted
TI  - HT - Classical Hypermedia Virtues on the Web with Webstrates
PY  - 2016
AB  - We show and analyze herein how Webstrates can augment the Web from a classical hypermedia perspective. Webstrates turns the DOM of Web pages into persistent and collaborative objects. We demonstrate how this can be applied to realize bidirectional links, shared collaborative annotations, and in-browser authorship and development.
SP  - 207
EP  - 212
JF  - Proceedings of the 27th ACM Conference on Hypertext and Social Media
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2914586.2914622
ER  - 

TY  - NA
AU  - Heller, Florian
TI  - TEI - Muffidgets: Detecting and Identifying Edible Pastry Tangibles on Capacitive Touchscreens
PY  - 2021
AB  - Detecting tangibles on capacitive touchscreen has seen vast attention over the past decade. Current state of the art allows a capacitive touchscreen to detect and identify a number of tangibles based on a unique footprint of interconnected conductive elements on the base of the tangible. In most cases, this conductive material is either metal or some carbon-based conductor, possibly integrated into a 3D-printing process. The choice of conductive material is, however, not limited to these technical elements. In this paper, we showcase how the concept of tangible detection on capacitive touchscreens can be transferred to pastry, creating an ephemeral, edible user interface. The detection and identification of specific pieces of pastry open applications in the area of entertainment, but also food safety.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3442449
ER  - 

TY  - JOUR
AU  - Paternò, Fabio; Santoro, Carmen
TI  - End-user development for personalizing applications, things, and robots
PY  - 2019
AB  - NA
SP  - 120
EP  - 130
JF  - International Journal of Human-Computer Studies
VL  - 131
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.06.002
ER  - 

TY  - NA
AU  - Pavel, Amy; Reyes, Gabriel; Bigham, Jeffrey P.
TI  - UIST - Rescribe: Authoring and Automatically Editing Audio Descriptions
PY  - 2020
AB  - Audio descriptions make videos accessible to those who cannot see them by describing visual content in audio. Producing audio descriptions is challenging due to the synchronous nature of the audio description that must fit into gaps of other video content. An experienced audio description author will produce content that fits narration necessary to understand, enjoy, or experience the video content into the time available. This can be especially tricky for novices to do well. In this paper, we introduce a tool, Rescribe, that helps authors create and refine their audio descriptions. Using Rescribe, authors first create a draft of all the content they would like to include in the audio description. Rescribe then uses a dynamic programming approach to optimize between the length of the audio description, available automatic shortening approaches, and source track lengthening approaches. Authors can iteratively visualize and refine the audio descriptions produced by Rescribe, working in concert with the tool. We evaluate the effectiveness of Rescribe through interviews with blind and visually impaired audio description users who give feedback on Rescribe results. In addition, we invite novice users to create audio descriptions with Rescribe and another tool, finding that users produce audio descriptions with fewer placement errors using Rescribe.
SP  - 747
EP  - 759
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415864
ER  - 

TY  - NA
AU  - Yung, Amanda K.; Li, Zhiyuan; Ashbrook, Daniel
TI  - IDC - Printy3D: in-situ tangible three-dimensional design for augmented fabrication
PY  - 2018
AB  - Three-dimensional design software is challenging for novices and non-experts; when working with objects that already exist, the task becomes even more difficult. We present Printy3D, a system that enables children to design customized containers for electronic modules using tangible interaction and spatially augmented reality feedback. Our system allows users to position physical objects in three dimensions relative to a virtual container, providing feedback on placement location and validity. We implemented two different interaction styles and conducted a user study with 26 participants, 23 of them children. We detail the results of our study and suggest implications for design as well as opportunities for future research for systems of this kind.
SP  - 181
EP  - 194
JF  - Proceedings of the 17th ACM Conference on Interaction Design and Children
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3202185.3202751
ER  - 

TY  - NA
AU  - James, Raphaël; Bezerianos, Anastasia; Chapuis, Olivier; Cordeil, Maxime; Dwyer, Tim; Prouzeau, Arnaud
TI  - Personal+Context navigation: combining AR and shared displays in network path-following
PY  - 2020
AB  - Shared displays are well suited to public viewing and collaboration, however they lack personal space to view private information and act without disturbing others. Combining them with Augmented Reality (AR) headsets allows interaction without altering the context on the shared display. We study a set of such interaction techniques in the context of network navigation, in particular path following, an important network analysis task. Applications abound, for example planning private trips on a network map shown on a public display.The proposed techniques allow for hands-free interaction, rendering visual aids inside the headset, in order to help the viewer maintain a connection between the AR cursor and the network that is only shown on the shared display. In two experiments on path following, we found that adding persistent connections between the AR cursor and the network on the shared display works well for high precision tasks, but more transient connections work best for lower precision tasks. More broadly, we show that combining personal AR interaction with shared displays is feasible for network navigation.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Stemasov, Evgeny; Rukzio, Enrico; Gugenheimer, Jan
TI  - The Road to Ubiquitous Personal Fabrication: Modeling-Free Instead of Increasingly Simple
PY  - 2021
AB  - The tools for personal digital fabrication (DF) are on the verge of reaching mass-adoption beyond technology enthusiasts, empowering consumers to fabricate personalized artifacts. We argue that to achieve similar outreach and impact as personal computing, personal fabrication research may have to venture beyond ever-simpler interfaces for creation, toward lowest-effort workflows for remixing. We surveyed novice-friendly DF workflows from the perspective of HCI. Through this survey, we found two distinct approaches for this challenge: 1) simplifying expert modeling tools (AutoCAD →Tinkercad) and 2) enriching tools not involving primitive-based modeling with powerful customization (e.g., Thingiverse). Drawing parallel to content creation domains such as photography, we argue that the bulk of content is created via remixing (2). In this article, we argue that to be able to include the majority of the population in DF, research should embrace omission of workflow steps, shifting toward automation, remixing, and templates, instead of modeling from the ground up.
SP  - 19
EP  - 27
JF  - IEEE Pervasive Computing
VL  - 20
IS  - 1
PB  - 
DO  - 10.1109/mprv.2020.3029650
ER  - 

TY  - NA
AU  - Potts, Dominic; Dabravalskis, Martynas; Houben, Steven
TI  - TangibleTouch: A Toolkit for Designing Surface-based Gestures for Tangible Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3502263
ER  - 

TY  - NA
AU  - Hofmann, Megan; Hann, Gabriella; Hudson, Scott E.; Mankoff, Jennifer
TI  - CHI - Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models
PY  - 2018
AB  - With the increasing popularity of consumer-grade 3D printing, many people are creating, and even more using, objects shared on sites such as Thingiverse. However, our formative study of 962 Thingiverse models shows a lack of re-use of models, perhaps due to the advanced skills needed for 3D modeling. An end user program perspective on 3D modeling is needed. Our framework (PARTs) empowers amateur modelers to graphically specify design intent through geometry. PARTs includes a GUI, scripting API and exemplar library of assertions which test design expectations and integrators which act on intent to create geometry. PARTs lets modelers integrate advanced, model specific functionality into designs, so that they can be re-used and extended, without programming. In two workshops, we show that PARTs helps to create 3D printable models, and modify existing models more easily than with a standard tool.
SP  - 301
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173875
ER  - 

TY  - NA
AU  - O'Bard, Bryce; Larson, Alex; Herrera, Joshua; Nega, Dominic; George, Kiran
TI  - ICHI - Electrooculography Based iOS Controller for Individuals with Quadriplegia or Neurodegenerative Disease
PY  - 2017
AB  - As the use of tablet computers and cell phones has become a standard medium of access to information, entertainment, and communication around the world, the reliance on having access to such devices has increased tremendously. For individuals with quadriplegia or neurodegenerative diseases, the access to these mobile devices is greatly hindered due to their inherent touchscreen design. Assistive technology solutions available to such patients today require families of patients to invest thousands of dollars in standalone tablet systems. There are few known options for allowing such patients to connect to their existing tablets or smartphones, which already have access to apps that can assist them in communication and daily activities. For this reason, we present in this paper a low-cost commercial off the shelf (COTS) assistive communication device to allow individuals with such conditions to access iOS based devices through electrooculography signals captured from their eye movements. Signals are captured through electrodes placed on the users face around the eyes. These signals are filtered, amplified, and processed to detect key eye movements mapped to perform control outputs sent to the iOS device. The communication capabilities are tested through the administration of a typing test to measure characters typed per minute (cpm). Testing of the device includes timed trials of directed tasks carried out by both healthy subjects and patients with ALS (PALS). It was determined that a user can type an average of 3.25 ~ 6.11 cpm using the device with an average accuracy of 89%. This could be significantly improved using a better suited keyboard application on the phone.
SP  - 101
EP  - 106
JF  - 2017 IEEE International Conference on Healthcare Informatics (ICHI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ichi.2017.90
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Withana, Anusha; Paradiso, Joseph A.; Steimle, Jürgen
TI  - UIST - Multi-Touch Kit: A Do-It-Yourself Technique for Capacitive Multi-Touch Sensing Using a Commodity Microcontroller
PY  - 2019
AB  - Mutual capacitance-based multi-touch sensing is now a ubiquitous and high-fidelity input technology. However, due to the complexity of electrical and signal processing requirements, it remains very challenging to create interface prototypes with custom-designed multi-touch input surfaces. In this paper, we introduce Multi-Touch Kit, a technique enabling electronics novices to rapidly prototype customized capacitive multi-touch sensors. In contrast to existing techniques, it works with a commodity microcontroller and open-source software and does not require any specialized hardware. Evaluation results show that our approach enables multi-touch sensors with a high spatial and temporal resolution and can accurately detect multiple simultaneous touches. A set of application examples demonstrates the versatile uses of our approach for sensors of different scales, curvature, and materials.
SP  - 1071
EP  - 1083
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347895
ER  - 

TY  - NA
AU  - Grønbæk, Jens Emil; Rasmussen, Majken Kirkegaard; Halskov, Kim; Petersen, Marianne Graves
TI  - CHI - KirigamiTable: Designing for Proxemic Transitions with a Shape-Changing Tabletop
PY  - 2020
AB  - A core challenge in tabletop research is to support transitions between individual activities and team work. Shape-changing tabletops open up new opportunities for addressing this challenge. However, interaction design for shape-changing furniture is in its early stages - so far, research has mainly focused on triggering shape-changes, and less on the actual interface transitions. We present KirigamiTable - a novel actuated shape-changing tabletop for supporting transitions in collaborative work. Our work builds on the concept of Proxemic Transitions, considering the dynamic interplay between social interactions, interactive technologies and furniture. With KirigamiTable, we demonstrate the potential of interactions for proxemic transitions that combine transformation of shape and digital contents. We highlight challenges for shape-changing tabletops: initiating shape and content transformations, cooperative control, and anticipating shape-change. To address these challenges, we propose a set of novel interaction techniques, including shape-first and content-first interaction, cooperative gestures, and physical and digital preview of shape-changes.
SP  - 1
EP  - 15
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376834
ER  - 

TY  - NA
AU  - Wang, Xiyue; Kayukawa, Seita; Takagi, Hironobu; Asakawa, Chieko
TI  - BentoMuseum: 3D and Layered Interactive Museum Map for Blind Visitors
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 24th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517428.3544811
ER  - 

TY  - NA
AU  - Fraser, C. Ailie; Ngoon, Tricia J.; Dontcheva, Mira; Klemmer, Scott R.
TI  - CHI - RePlay: Contextually Presenting Learning Videos Across Software Applications
PY  - 2019
AB  - Complex activities often require people to work across multiple software applications. However, people frequently lack valuable knowledge about at least one application, especially as software changes and new software emerges. Existing help systems either lack contextual knowledge or are tightly-knit into a single application. We introduce an application-independent approach for contextually presenting video learning resources and demonstrate it through the RePlay system. RePlay uses accessibility APIs to gather context about the user's activity. It leverages an existing search engine to present relevant videos and highlights key segments within them using video captions. We report on a week-long field study (n=7) and a lab study (n=24) showing that contextual assistance helps people spend less time away from their task than web video search and replaces current video navigation strategies. Our findings highlight challenges with representing and using context across applications.
SP  - 297
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300527
ER  - 

TY  - JOUR
AU  - Mankoff, Jennifer; Hofmann, Megan; Chen, Xiang 'Anthony'; Hudson, Scott E.; Hurst, Amy; Kim, Jeeeun
TI  - Consumer-grade fabrication and its potential to revolutionize accessibility
PY  - 2019
AB  - <jats:p>Digital fabrication technologies open new doors---and challenges---for real-world support.</jats:p>
SP  - 64
EP  - 75
JF  - Communications of the ACM
VL  - 62
IS  - 10
PB  - 
DO  - 10.1145/3339824
ER  - 

TY  - NA
AU  - Santos, Pedro A.; Madeira, Rui Neves; Correia, Nuno
TI  - MobiQuitous - YanuX: pervasive distribution of the user interface by co-located devices
PY  - 2019
AB  - We currently live surrounded by many different computing devices. Therefore, it is important to take better advantage of those devices by coming up with smart ways of integrating and combining them. We have been exploring the possibility of building applications that present user interfaces pervasively distributed across different co-located devices. We designed the YanuX framework, which generalizes and supports the development of this new type of applications. A key issue is the automatic distribution of user interface (UI) components among co-located devices. We created the tools set needed to describe the capabilities of each of the devices present in the environment and the requirements of each of the applications' components as configured by the developers. Restrictions of the components should match the capabilities of the devices leading to a UI component distribution decision that should reflect developers' intentions and expectations of the users. Besides detailing YanuX's components, the paper also presents YouTube Viewer as a proof-of-concept application based on YanuX. The application was also used in a user study to evaluate the concept and the experience supported by the framework. The results presented here are positive and very promising.
SP  - 368
EP  - 377
JF  - Proceedings of the 16th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3360774.3360832
ER  - 

TY  - NA
AU  - Klokmose, Clemens Nylandsted; Eagan, James; Baader, Siemen; Mackay, Wendy E.; Beaudouin-Lafon, Michel
TI  - CSCW Companion - Webstrates: Demonstrating the potential of Shareable Dynamic Media
PY  - 2016
AB  - Shareable Dynamic Media is our vision for an interactive computing environment that blurs the distinction between documents and applications, characterized by three key properties: malleability, so that users can appropriate and repurpose media in idiosyncratic ways; shareability, so that users can collaborate on any type of content; and dis-tributability, so that users can access and manipulate media across diverse devices and platforms. Webstrates is a pro-totype web-based environment designed to explore this vision. It supports real-time sharing of any web content, as well as transclusion to flexibly combine and assemble multi-ple media together. The demonstration illustrates the power of Webstrates and Shared Dynamic Media through sev-eral scenarios, such as collaborative authoring, distributed slideshow presentation with audience participation, collabo-rative programming, and shareable window management.
SP  - 61
EP  - 64
JF  - Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2818052.2874325
ER  - 

TY  - JOUR
AU  - Oh, Seungjae; So, Hyo-Jeong; Gaydos, Matthew
TI  - Hybrid Augmented Reality for Participatory Learning: The Hidden Efficacy of Multi-User Game-Based Simulation
PY  - 2018
AB  - The goal for this research is to articulate and test a new hybrid Augmented Reality (AR) environment for conceptual understanding. From the theoretical lens of embodied interaction, we have designed a multi-user participatory simulation called ARfrac t where visitors in a science museum can learn about complex scientific concepts on the refraction of light through full-body immersion using optical see-through AR glasses, projection-based AR, and gesture technology. In particular, we developed two different types of simulations for ARfract , namely a game-based simulation and a non-game simulation to explore how the order of different AR simulations influences the perceived usability, user behaviors, learning experiences, and learning outcomes. For the experiment, 10 dyads were randomly assigned to one of the two experimental conditions: 1) the game-to-non-game condition and 2) the non-game-to-game condition. The results indicate that the learners who experienced the game-based simulation before the non-game simulation performed better than did the other group with the reversed experience order. This paper also reports the usability, user behaviors, and learning experience issues regarding the affordances of hybrid AR technologies. The major contribution of this proof-of concept research is that it articulates our understanding of how particular configurations (i.e., order) of the emerging technologies (i.e., hybrid Augmented Reality systems) and its use can lead to different learning outcomes.
SP  - 115
EP  - 127
JF  - IEEE Transactions on Learning Technologies
VL  - 11
IS  - 1
PB  - 
DO  - 10.1109/tlt.2017.2750673
ER  - 

TY  - JOUR
AU  - Chlibi, Nadia; Silva, José; Vieira, E. M. F.; Gonçalves, Luís; Moreira, Joaquim Agostinho; Chahboun, Adil; Dahman, H.; Pereira, Mário; Gomes, M. J. M.; Mir, Lassaad El
TI  - Touch sensor and photovoltaic characteristics of CuSbS2 thin films
PY  - 2021
AB  - NA
SP  - 22594
EP  - 22603
JF  - Ceramics International
VL  - 47
IS  - 16
PB  - 
DO  - 10.1016/j.ceramint.2021.04.271
ER  - 

TY  - NA
AU  - Marky, Karola; Weiß, Andreas; Müller, Florian; Schmitz, Martin; Mühlhäuser, Max; Kosch, Thomas
TI  - CHI Extended Abstracts - Let’s Frets! Mastering Guitar Playing with Capacitive Sensing and Visual Guidance
PY  - 2021
AB  - Mastering the guitar requires regular exercise to develop new skills and maintain existing abilities. We present Let’s Frets - a modular guitar support system that provides visual guidance through LEDs that are integrated into a capacitive fretboard to support the practice of chords, scales, melodies, and exercises. Additional feedback is provided through a 3D-printed fretboard that senses the finger positions through capacitive sensing. We envision Let’s Frets as an integrated guitar support system that raises the awareness of guitarists about their playing styles, their training progress, the composition of new pieces, and facilitating remote collaborations between teachers as well as guitar students. This interactivity demonstrates Let’s Frets with an augmented fretboard and supporting software that runs on a mobile device.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451536
ER  - 

TY  - NA
AU  - Litt, Geoffrey; Jackson, Daniel; Millis, Tyler; Quaye, Jessica Ayeley
TI  - Onward! - End-user software customization by direct manipulation of tabular data
PY  - 2020
AB  - Customizing software should be as easy as using it. Unfortunately, most customization methods require users to abruptly shift from using a graphical interface to writing scripts in a programming language. We introduce data-driven customization, a new way for end users to extend software by direct manipulation without doing traditional programming. We augment existing user interfaces with a table view showing the structured data inside the application. When users edit the table, their changes are reflected in the original UI. This simple model accommodates a spreadsheet formula language and custom data-editing widgets, providing enough power to implement a variety of useful extensions. We illustrate the approach with Wildcard, a browser extension that implements data-driven customization on the web using web scraping. Through concrete examples, we show that this paradigm can support useful extensions to many real websites, and we share reflections from our experiences using the tool. Finally, we share our broader vision for data-driven customization: a future where end users have more access to the data inside their applications, and can more flexibly repurpose that data as part of everyday software usage.
SP  - 18
EP  - 33
JF  - Proceedings of the 2020 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3426428.3426914
ER  - 

TY  - JOUR
AU  - Tymms, Chelsea; Wang, Siqi; Zorin, Denis
TI  - Appearance-preserving tactile optimization
PY  - 2020
AB  - Textures are encountered often on various common objects and surfaces. Many textures combine visual and tactile aspects, each serving important purposes; most obviously, a texture alters the object's appearance or tactile feeling as well as serving for visual or tactile identification and improving usability. The tactile feel and visual appearance of objects are often linked, but they may interact in unpredictable ways. Advances in high-resolution 3D printing enable highly flexible control of geometry to permit manipulation of both visual appearance and tactile properties. In this paper, we propose an optimization method to independently control the tactile properties and visual appearance of a texture. Our optimization is enabled by neural network-based models, and allows the creation of textures with a desired tactile feeling while preserving a desired visual appearance at a relatively low computational cost, for use in a variety of applications.
SP  - 212
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 6
PB  - 
DO  - 10.1145/3414685.3417857
ER  - 

TY  - NA
AU  - Grønbæk, Jens Emil; Korsgaard, Henrik; Petersen, Marianne Graves; Birk, Morten Henriksen; Krogh, Peter Gall
TI  - CHI - Proxemic Transitions: Designing Shape-Changing Furniture for Informal Meetings
PY  - 2017
AB  - The field of Shape-Changing Interfaces explores the qualities of physically dynamic artifacts. At furniture-scale, such artifacts have the potential of changing the ways we collaborate and engage with interiors and physical spaces. Informed by theories of proxemics, empirical studies of informal meetings and design work with shape-changing furniture, we develop the notion of proxemic transitions. We present three design aspects of proxemic transitions: transition speed, stepwise reconfiguration, and radical shifts. The design aspects focus on how to balance between physical and digital transformations in designing for proxemic transitions. Our contribution is three-fold: 1) the notion of proxemic transitions, 2) three design aspects to consider in designing for proxemic transitions, and 3) initial exploration of how these design aspects might generate designs of dynamic furniture. These contributions outline important aspects to consider when designing shape-changing furniture for informal workplace meetings.
SP  - 7029
EP  - 7041
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025487
ER  - 

TY  - NA
AU  - Lee, Chi-Jung; Tsai, Hsin-Ruey; Chen, Bing-Yu
TI  - CHI - HairTouch: Providing Stiffness, Roughness and Surface Height Differences Using Reconfigurable Brush Hairs on a VR Controller
PY  - 2021
AB  - Tactile feedback is widely used to enhance realism in virtual reality (VR). When touching virtual objects, stiffness and roughness are common and obvious factors perceived by the users. Furthermore, when touching a surface with complicated surface structure, differences from not only stiffness and roughness but also surface height are crucial. To integrate these factors, we propose a pin-based handheld device, HairTouch, to provide stiffness differences, roughness differences, surface height differences and their combinations. HairTouch consists of two pins for the two finger segments close to the index fingertip, respectively. By controlling brush hairs’ length and bending direction to change the hairs’ elasticity and hair tip direction, each pin renders various stiffness and roughness, respectively. By further independently controlling the hairs’ configuration and pins’ height, versatile stiffness, roughness and surface height differences are achieved. We conducted a perception study to realize users’ distinguishability of stiffness and roughness on each of the segments. Based on the results, we performed a VR experience study to verify that the tactile feedback from HairTouch enhances VR realism.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445285
ER  - 

TY  - NA
AU  - Dalsgaard, Peter; Inie, Nanna; Hansen, Nicolai Brodersen
TI  - Creativity &amp; Cognition - How can Computers Support, Enrich, and Transform Collaborative Creativity?
PY  - 2017
AB  - The aim of the workshop is to examine and discuss how computers can support, enrich, and transform collaborative creative processes. By exploring and combining methodological, theoretical, and design-oriented perspectives, we wish to examine the implications, potentials, and limitations of different approaches to providing digital support for collaborative creativity. Participation in the workshop requires participants to actively document and identify salient themes in one or more examples of computer-supported collaborative creativity, and the resulting material will serve as the empirical grounding for workshop discussions.
SP  - 554
EP  - 560
JF  - Proceedings of the 2017 ACM SIGCHI Conference on Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3059454.3059483
ER  - 

TY  - BOOK
AU  - Dierkes, Kai; Kassner, Moritz; Bulling, Andreas
TI  - ETRA - A fast approach to refraction-aware eye-model fitting and gaze prediction
PY  - 2019
AB  - By temporally integrating information about pupil contours extracted from eye images, model-based methods for glint-free gaze estimation can mitigate pupil detection noise. However, current approaches require time-consuming iterative solving of a nonlinear minimization problem to estimate key parameters, such as eyeball position. Based on the method presented by [Swirski and Dodgson 2013], we propose a novel approach to glint-free 3D eye-model fitting and gaze prediction using a single near-eye camera. By recasting model optimization as a least-squares intersection of lines, we make it amenable to a fast non-iterative solution. We further present a method for estimating deterministic refraction-correction functions from synthetic eye images and validate them on both synthetic and real eye images. We demonstrate the robustness of our method in the presence of pupil detection noise and show the benefit of temporal integration of pupil contour information on eyeball position and gaze estimation accuracy.
SP  - 23
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319819
ER  - 

TY  - NA
AU  - Wu, Chi-Jui; Houben, Steven; Marquardt, Nicolai
TI  - CHI - EagleSense: Tracking People and Devices in Interactive Spaces using Real-Time Top-View Depth-Sensing
PY  - 2017
AB  - Real-time tracking of people's location, orientation and activities is increasingly important for designing novel ubiquitous computing applications. Top-view camera-based tracking avoids occlusion when tracking people while collaborating, but often requires complex tracking systems and advanced computer vision algorithms. To facilitate the prototyping of ubiquitous computing applications for interactive spaces, we developed EagleSense, a real-time human posture and activity recognition system with a single top-view depth-sensing camera. We contribute our novel algorithm and processing pipeline, including details for calculating silhouette-extremities features and applying gradient tree boosting classifiers for activity recognition optimized for top-view depth sensing. EagleSense provides easy access to the real-time tracking data and includes tools for facilitating the integration into custom applications. We report the results of a technical evaluation with 12 participants and demonstrate the capabilities of EagleSense with application case studies.
SP  - 3929
EP  - 3942
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025562
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Punpongsanon, Parinya; Kim, Jeeeun
TI  - UIST - Programmable Filament: Printed Filaments for Multi-material 3D Printing
PY  - 2020
AB  - From full-color objects to functional capacitive artifacts, 3D printing multi-materials became essential to broaden the application areas of digital fabrication. We present Programmable Filament, a novel technique that enables multi-material printing using a commodity FDM 3D printer, requiring no hardware upgrades. Our technique builds upon an existing printing technique in which multiple filament segments are printed and spliced into a single threaded filament. We propose an end-to-end pipeline for 3D printing an object in multi-materials, with an introduction of the design systems for end-users. Optimized for low-cost, single-nozzle FDM 3D printers, the system is built upon our computational analysis and experiments to enhance its validity over various printers and materials to design and produce a programmable filament. Finally, we discuss application examples and speculate the future with its potential, such as custom filament manufacturing on-demand.
SP  - 1209
EP  - 1221
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415863
ER  - 

TY  - NA
AU  - Lau, Samuel; Drosos, Ian; Markel, Julia M.; Guo, Philip J.
TI  - VL/HCC - The Design Space of Computational Notebooks: An Analysis of 60 Systems in Academia and Industry
PY  - 2020
AB  - Computational notebooks such as Jupyter are now used by millions of data scientists, machine learning engineers, and computational researchers to do exploratory and end-user programming. In recent years, dozens of different notebook systems have been developed across academia and industry. However, we still lack an understanding of how their individual designs relate to one another and what their tradeoffs are. To provide a holistic view of this rapidly-emerging landscape, we performed, to our knowledge, the first comprehensive design analysis of dozens of notebook systems. We analyzed 60 notebooks (16 academic papers, 29 industry products, and 15 experimental/R&D projects) and formulated a design space that succinctly captures variations in system features. Our design space covers 10 dimensions that include diverse ways of importing data, editing code and prose, running code, and publishing notebook outputs. We conclude by suggesting ways for researchers to push future projects beyond the current bounds of this space.
SP  - 1
EP  - 11
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127201
ER  - 

TY  - BOOK
AU  - Biele, Cezary; Kobyliński, Paweł
TI  - IHSI - Surface Recalibration as a New Method Improving Gaze-Based Human-Computer Interaction
PY  - 2017
AB  - The main problem of a gaze-based interaction is the correct mapping from an output of eye tracker to a gaze point. In this paper we propose a new method of improvement of the gaze-based human computer interaction using: (a) a procedure to estimate the error introduced by screen tracking algorithms (surface recalibration) and (b) using the obtained error data to transform the eye-tracking data in real-time (data transformation). In order to test the developed method, we conducted initial pilot study using simple target pointing procedure. Initial data gathered during these tests shows that our method may increase the effectiveness (measured as target pointing speed) of the gaze-based interaction using mobile eye trackers. In future studies it is worth testing this method using stationary eye trackers as it can be an effective way of facilitating gaze-based interaction by counteracting calibration errors that would yield gaze-based system unusable.
SP  - 197
EP  - 202
JF  - Intelligent Human Systems Integration
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-73888-8_31
ER  - 

TY  - JOUR
AU  - Riemann, Jan; Schmitz, Martin; Hendrich, Alexander; Mühlhäuser, Max
TI  - FlowPut: Environment-Aware Interactivity for Tangible 3D Objects
PY  - 2018
AB  - Tangible interaction has shown to be beneficial in a wide variety of scenarios since it provides more direct manipulation and haptic feedback. Further, inherently three-dimensional information is represented more naturally by a 3D object than by a flat picture on a screen. Yet, today's tangibles have often pre-defined form factors and limited input and output facilities. To overcome this issue, the combination of projection and depth cameras is used as a fast and flexible way of non-intrusively adding input and output to tangibles. However, tangibles are often quite small and hence the space for output and interaction on their surface is limited. Therefore, we propose FlowPut: an environment-aware framework that utilizes the space available on and around a tangible object for projected visual output. By means of an optimization-based layout approach, FlowPut considers the environment of the objects to avoid interference between projection and real-world objects. Moreover, we contribute an occlusion resilient object recognition and tracking for tangible objects based on their 3D model and a point-cloud based multi-touch detection, that allows sensing touches also on the side of a tangible. Flowput is validated through a series of technical experiments, a user study, and two example applications.
SP  - 31
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 1
PB  - 
DO  - 10.1145/3191763
ER  - 

TY  - NA
AU  - Xu, Weipeng; Chatterjee, Avishek; Zollhoefer, Michael; Rhodin, Helge; Fua, Pascal; Seidel, Hans-Peter; Theobalt, Christian
TI  - Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera
PY  - 2018
AB  - We propose the first real-time approach for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60Hz on a consumer-level GPU. In addition to the novel hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a novel disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Everitt, Aluna; Eady, Alexander Keith; Girouard, Audrey
TI  - Enabling Multi-Material 3D Printing for Designing and Rapid Prototyping of Deformable and Interactive Wearables
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3490635
ER  - 

TY  - NA
AU  - Andersen, Leif; Ballantyne, Michael; Felleisen, Matthias
TI  - Adding Interactive Visual Syntax to Textual Code
PY  - 2020
AB  - Many programming problems call for turning geometrical thoughts into code: tables, hierarchical structures, nests of objects, trees, forests, graphs, and so on. Linear text does not do justice to such thoughts. But, it has been the dominant programming medium for the past and will remain so for the foreseeable future. This paper proposes a novel mechanism for conveniently extending textual programming languages with problem-specific visual syntax. It argues the necessity of this language feature, demonstrates the feasibility with a robust prototype, and sketches a design plan for adapting the idea to other languages.
SP  - NA
EP  - NA
JF  - arXiv: Programming Languages
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Akşit, Kaan; Kautz, Jan; Luebke, David
TI  - Gaze-Sensing LEDs for Head Mounted Displays
PY  - 2020
AB  - We introduce a new gaze tracker for Head Mounted Displays (HMDs). We modify two off-the-shelf HMDs to be gaze-aware using Light Emitting Diodes (LEDs). Our key contribution is to exploit the sensing capability of LEDs to create low-power gaze tracker for virtual reality (VR) applications. This yields a simple approach using minimal hardware to achieve good accuracy and low latency using light-weight supervised Gaussian Process Regression (GPR) running on a mobile device. With our hardware, we show that Minkowski distance measure based GPR implementation outperforms the commonly used radial basis function-based support vector regression (SVR) without the need to precisely determine free parameters. We show that our gaze estimation method does not require complex dimension reduction techniques, feature extraction, or distortion corrections due to off-axis optical paths. We demonstrate two complete HMD prototypes with a sample eye-tracked application, and report on a series of subjective tests using our prototypes.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Yeh, Tom; Yatani, Koji; Gross, Mark D.
TI  - Autocomplete Textures for 3D Printing.
PY  - 2017
AB  - Texture is an essential property of physical objects that affects aesthetics, usability, and functionality. However, designing and applying textures to 3D objects with existing tools remains difficult and time-consuming; it requires proficient 3D modeling skills. To address this, we investigated an auto-completion approach for efficient texture creation that automates the tedious, repetitive process of applying texture while allowing flexible customization. We developed techniques for users to select a target surface, sketch and manipulate a texture with 2D drawings, and then generate 3D printable textures onto an arbitrary curved surface. In a controlled experiment our tool sped texture creation by 80% over conventional tools, a performance gain that is higher with more complex target surfaces. This result confirms that auto-completion is powerful for creating 3D textures.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wang, Miao; Yang, Guo-Wei; Hu, Shi-Min; Yau, Shing-Tung; Shamir, Ariel
TI  - Write-a-video: computational video montage from themed text
PY  - 2019
AB  - We present Write-A-Video, a tool for the creation of video montage using mostly text-editing. Given an input themed text and a related video repository either from online websites or personal albums, the tool allows novice users to generate a video montage much more easily than current video editing tools. The resulting video illustrates the given narrative, provides diverse visual content, and follows cinematographic guidelines. The process involves three simple steps: (1) the user provides input, mostly in the form of editing the text, (2) the tool automatically searches for semantically matching candidate shots from the video repository, and (3) an optimization method assembles the video montage. Visual-semantic matching between segmented text and shots is performed by cascaded keyword matching and visual-semantic embedding, that have better accuracy than alternative solutions. The video assembly is formulated as a hybrid optimization problem over a graph of shots, considering temporal constraints, cinematography metrics such as camera movement and tone, and user-specified cinematography idioms. Using our system, users without video editing experience are able to generate appealing videos.
SP  - 1
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 6
PB  - 
DO  - 10.1145/3355089.3356520
ER  - 

TY  - NA
AU  - Nouwens, Midas; Borowski, Marcel; Fog, Bjarke Vognstrup; Klokmose, Clemens Nylandsted
TI  - CHI - Between Scripts and Applications: Computational Media for the Frontier of Nanoscience
PY  - 2020
AB  - The popularity of computational notebooks heralds a return of software as computational media rather than turn-key applications. We believe this software model has potential beyond supporting just the computationally literate. We studied a biomolecular nano-design lab that works on a current frontier of science - RNA origami - whose researchers depend on computational tools to do their work, yet are not trained as programmers. Using a participatory design process, we developed a computational labbook to concretise what computational media could look like, using the principles of computability, malleability, shareability, and distributability suggested by previous work. We used this prototype to co-reflect with the nanoscientists about how it could transform their practice. We report on the computational culture specific to this research area; the scientists' struggles managing their computational environments; and their subsequent disempowerment yet dependence. Lastly, we discuss the generative potential and limitations of the four design principles for the future of computational media.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376287
ER  - 

TY  - JOUR
AU  - Etemad, Katayoon; Samavati, Faramarz; Dawson, Peter
TI  - Multi-scale physicalization of polar heritage at risk in the western canadian arctic
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Visual Computer
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s00371-022-02439-9
ER  - 

TY  - NA
AU  - Tejada, Carlos E.; Ramakers, Raf; Boring, Sebastian; Ashbrook, Daniel
TI  - CHI - AirTouch: 3D-printed Touch-Sensitive Objects Using Pneumatic Sensing
PY  - 2020
AB  - 3D printing technology can be used to rapidly prototype the look and feel of 3D objects. However, the objects produced are passive. There has been increasing interest in making these objects interactive, yet they often require assembling components or complex calibration. In this paper, we contribute AirTouch, a technique that enables designers to fabricate touch-sensitive objects with minimal assembly and calibration using pneumatic sensing. AirTouch-enabled objects are 3D printed as a single structure using a consumer-level 3D printer. AirTouch uses pre-trained machine learning models to identify interactions with fabricated objects, meaning that there is no calibration required once the object has completed printing. We evaluate our technique using fabricated objects with various geometries and touch sensitive locations, obtaining accuracies of at least 90% with 12 interactive locations.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376136
ER  - 

TY  - JOUR
AU  - Klug, Christina; Schmitz, Thomas H.
TI  - Examining the Interactions of Design Parameters in the LDM of Clay as the Basis for New Design Paradigms
PY  - 2022
AB  - <jats:p>In the future, architects will not only develop digital designs based on existing technologies and concepts, but will also pursue and experiment with various forms of self-developed processes. The following empirical study deals with the materialization of digital instructions and machine parameters in 3D-printed clay elements. Specifically, forms of materialization are investigated in the transitional area where ideal geometrically defined data and process-related material information intersect. Liquid materials generate additional information quality through their material-immanent shape-forming properties. In previous studies, this somewhat complex material behavior was considered rather problematic and attempts were made to reduce the flow behavior of materials in the printing process. In contrast, this study examines the special possibility of liquid deposition modelling and present new ways of dealing with the material viscosity during and after the printing process.</jats:p>
SP  - 131
EP  - 147
JF  - Ceramics
VL  - 5
IS  - 1
PB  - 
DO  - 10.3390/ceramics5010012
ER  - 

TY  - NA
AU  - Dunn, David; Tippets, Cary A.; Torell, Kent; Fuchs, Henry; Kellnhofer, Petr; Myszkowski, Karol; Didyk, Piotr; Akşit, Kaan; Luebke, David
TI  - SIGGRAPH Emerging Technologies - Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes
PY  - 2017
AB  - Accommodative depth cues, a wide field of view, and ever-higher resolutions present major design challenges for near-eye displays. Optimizing a design to overcome one of them typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution - a novel display for augmented reality. The key components of our solution are two see-through, varifocal deformable membrane mirrors reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane. The benefits of the membranes include a wide field of view and fast depth switching.
SP  - 15
EP  - NA
JF  - ACM SIGGRAPH 2017 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3084822.3084846
ER  - 

TY  - JOUR
AU  - Jin, Zeyu; Mysore, Gautham J.; Diverdi, Stephen; Lu, Jingwan; Finkelstein, Adam
TI  - VoCo
PY  - 2017
AB  - <jats:p>Editing audio narration using conventional software typically involves many painstaking low-level manipulations. Some state of the art systems allow the editor to work in a text transcript of the narration, and perform select, cut, copy and paste operations directly in the transcript; these operations are then automatically applied to the waveform in a straightforward manner. However, an obvious gap in the text-based interface is the ability to type new words not appearing in the transcript, for example inserting a new word for emphasis or replacing a misspoken word. While high-quality voice synthesizers exist today, the challenge is to synthesize the new word in a voice that matches the rest of the narration. This paper presents a system that can synthesize a new word or short phrase such that it blends seamlessly in the context of the existing narration. Our approach is to use a text to speech synthesizer to say the word in a generic voice, and then use voice conversion to convert it into a voice that matches the narration. Offering a range of degrees of control to the editor, our interface supports fully automatic synthesis, selection among a candidate set of alternative pronunciations, fine control over edit placements and pitch profiles, and even guidance by the editors own voice. The paper presents studies showing that the output of our method is preferred over baseline methods and often indistinguishable from the original voice.</jats:p>
SP  - 1
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 4
PB  - 
DO  - 10.1145/3072959.3073702
ER  - 

TY  - NA
AU  - Griggio, Carla F.; Nouwens, Midas; Klokmose, Clemens Nylandsted
TI  - Caught in the Network: The Impact of WhatsApp's 2021 Privacy Policy Update on Users' Messaging App Ecosystems
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502032
ER  - 

TY  - JOUR
AU  - Beaudouin-Lafon, Michel; Bødker, Susanne; Mackay, Wendy E.
TI  - Generative Theories of Interaction
PY  - 2021
AB  - Although Human–Computer Interaction research has developed various theories and frameworks for analyzing new and existing interactive systems, few address the generation of novel technological solu...
SP  - 1
EP  - 54
JF  - ACM Transactions on Computer-Human Interaction
VL  - 28
IS  - 6
PB  - 
DO  - 10.1145/3468505
ER  - 

TY  - JOUR
AU  - Ahmetovic, Dragan; Bernareggi, Cristian; Leporini, Barbara; Mascetti, Sergio
TI  - WordMelodies: Supporting the Acquisition of Literacy Skills by Children with Visual Impairment through a Mobile App
PY  - 2022
AB  - <jats:p>WordMelodies is a mobile app that aims to support inclusive teaching of literacy skills for primary school students. Thus it was designed to be accessible both visually and through screen reader, and it includes over 80 different types of exercises for practicing literacy skills, each with adjustable difficulty levels, in Italian and in English. WordMelodies is freely available for iOS and Android devices. However, it has not been previously evaluated with children having visual impairments. Thus, in this paper, we evaluate the app usability, its perceived ease of use, appreciation and children’s autonomy while using it, as well as the characteristics of the end users. To achieve this, we conducted a user study with 11 primary school students with visual impairments, and we analyzed app usage logs collected from 408 users in over one year from the app publication. We show that app usability is high, and most exercises can be completed autonomously. The exercises are also perceived to be easy to perform, and they are appreciated by the participants. Finally, we provide insights on how to address the identified app limitations and propose future research directions.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Accessible Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565029
ER  - 

TY  - JOUR
AU  - Han, Changyo; Naemura, Takeshi
TI  - [Papers] BumpMarker: a 3D-printed tangible marker for simultaneous tagging, tracking, and weight measurement
PY  - 2019
AB  - NA
SP  - 11
EP  - 19
JF  - ITE Transactions on Media Technology and Applications
VL  - 7
IS  - 1
PB  - 
DO  - 10.3169/mta.7.11
ER  - 

TY  - NA
AU  - Bouvin, Niels Olof
TI  - HT - From NoteCards to Notebooks: There and Back Again
PY  - 2019
AB  - Fifty years since the beginning of the Internet, and three decades of the Dexter Hypertext Reference Model and the World Wide Web mark an opportune time to take stock and consider how hypermedia has developed, and in which direction it might be headed. The modern Web has on one hand turned into a place where very few, very large companies control all major platforms with some highly unfortunately consequences. On the other hand, it has also led to the creation of a highly flexible and nigh ubiquitous set of technologies and practices, which can be used as the basis for future hypermedia research with the rise of computational notebooks as a prime example of a new kind of collaborative and highly malleable applications.
SP  - 19
EP  - 28
JF  - Proceedings of the 30th ACM Conference on Hypertext and Social Media
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3342220.3343666
ER  - 

TY  - NA
AU  - Park, Seonwook; Aksan, Emre; Zhang, Xucong; Hilliges, Otmar
TI  - Towards End-to-end Video-based Eye-Tracking
PY  - 2020
AB  - Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user's eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to a 28 percent improvement in Point-of-Gaze estimates (resulting in 2.49 degrees in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at this https URL
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Zhenliang; Li, Yue; Guo, Jie; Weng, Dongdong; Liu, Yue; Wang, Yongtian
TI  - Task‐driven latent active correction for physics‐inspired input method in near‐field mixed reality applications
PY  - 2018
AB  - NA
SP  - 496
EP  - 509
JF  - Journal of the Society for Information Display
VL  - 26
IS  - 8
PB  - 
DO  - 10.1002/jsid.728
ER  - 

TY  - NA
AU  - Han, Han L.; Yu, Junhang; Bournet, Raphael; Ciorascu, Alexandre; Mackay, Wendy E.; Beaudouin-Lafon, Michel
TI  - Passages: Interacting with Text Across Documents
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502052
ER  - 

TY  - NA
AU  - Hahn, Jürgen; Wimmer, Raphael
TI  - CHI Extended Abstracts - Sketchable Interaction: Drawing User Interfaces with Interactive Regions
PY  - 2021
AB  - Sketchable Interaction (SI) describes a concept and environment where end-users create regions by drawing on a canvas. These regions apply effects to each other on collision. Attributes of regions, e.g. position, can be linked to each other so that they change together once modified, e.g. moved on the canvas. Within Sketchable Interaction, all entities - mouse pointer, desktop icons, or windows - are implemented as interactive regions. End-users customize this environment by drawing new regions that apply certain actions e.g. tagging files, deleting other regions or automating processes.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451567
ER  - 

TY  - BOOK
AU  - Santini, Thiago; Niehorster, Diederick C; Kasneci, Enkelejda
TI  - ETRA - Get a grip: slippage-robust and glint-free gaze estimation for real-time pervasive head-mounted eye tracking
PY  - 2019
AB  - A key assumption conventionally made by flexible head-mounted eye-tracking systems is often invalid: The eye center does not remain stationary w.r.t. the eye camera due to slippage. For instance, eye-tracker slippage might happen due to head acceleration or explicit adjustments by the user. As a result, gaze estimation accuracy can be significantly reduced. In this work, we propose Grip, a novel gaze estimation method capable of instantaneously compensating for eye-tracker slippage without additional hardware requirements such as glints or stereo eye camera setups. Grip was evaluated using previously collected data from a large scale unconstrained pervasive eye-tracking study. Our results indicate significant slippage compensation potential, decreasing average participant median angular offset by more than 43% w.r.t. a non-slippage-robust gaze estimation method. A reference implementation of Grip was integrated into EyeRecToo, an open-source hardware-agnostic eye-tracking software, thus making it readily accessible for multiple eye trackers (Available at: www.ti.uni-tuebingen.de/perception).
SP  - 17
EP  - NA
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319835
ER  - 

TY  - NA
AU  - Knierim, Pascal; Funk, Markus; Kosch, Thomas; Fedosov, Anton; Müller, Tamara; Schopf, Benjamin; Weise, Marc; Schmidt, Albrecht
TI  - NordiCHI - UbiBeam++: Augmenting Interactive Projection with Head-Mounted Displays
PY  - 2016
AB  - Interactive tabletops or projections became widely utilized in schools, museum exhibitions or conference rooms to teach and illustrate dynamic artifacts or support talks. In such scenarios, all observers, such as pupils and teachers, will perceive the same information even if they hold different positions and could benefit from an adapted and personalized view. We developed the UbiBeam++ mixed reality software toolkit to enable augmentation of an interactive projection surface using optical see-through glasses. Our toolkit supports simultaneous presentation of private, shared, and public content. Private and shared content is registered in space and presented through a head-mounted display, while public content is presented by a projector. Our toolkit simplifies the development of interactive projections with different visualization levels. In a preliminary study, participants understood the concept of personalized information space and appreciated the presentation of additional information. Looking forward, our toolkit supports the development and the exploration of various scenarios not only limited to teaching, presentations or games.
SP  - 112
EP  - NA
JF  - Proceedings of the 9th Nordic Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971485.2996747
ER  - 

TY  - CHAP
AU  - Zhang, Jianming; Malmberg, Filip; Sclaroff, Stan
TI  - Overview
PY  - 2019
AB  - NA
SP  - 1
EP  - 7
JF  - Visual Saliency: From Pixel-Level to Object-Level Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-04831-0_1
ER  - 

TY  - BOOK
AU  - Sanz, Ferran Argelaguet; Arnaldi, Bruno; Burkhardt, Jean-Marie; Casiez, Géry; Donikian, Stéphane; Gosselin, Florian; Granier, Xavier; Le Callet, Patrick; Lepetit, Vincent; Marchal, Maud; Moreau, Guillaume; Perret, Jérôme; Vigier, Toinon
TI  - Virtual Reality and Augmented Reality - Complexity and Scientific Challenges
PY  - 2018
AB  - This chapter discusses the topics for which solutions exist, but which still pose scientific challenges and elements of complexity which would be helpful to discuss in detail. It describes physical models to detect collisions and the problem of the virtual human. The chapter then examines naturalness of the interaction, and proposes an analysis of force feedback. It also discusses the 3D interaction loop as the explanatory framework for the scientific challenges surrounding 3D interaction with virtual or mixed environments. This loop comes from the perception‐action loop, which is very often used in the literature to explain the challenges in virtual reality and augmented reality. In order to improve interaction, different sensory modalities of the user are brought into play: not only sight, but also hearing and touch are fundamental sensory modalities. The chapter finally identifies the scientific challenges related to these different sensory modalities.
SP  - 123
EP  - 216
JF  - Virtual Reality and Augmented Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1002/9781119341031.ch3
ER  - 

TY  - CONF
AU  - Liu, Jiali; Boukhelifa, Nadia; Eagan, James
TI  - Understanding alternatives in data analysis activities
PY  - 2019
AB  - Data workers are non-professional data scientists who engage in data analysis activities as part of their daily work. In this position paper, we share past and on-going work to understand data workers’ sense-making practices. We use multidisciplinary approaches to explore their human-tool partnerships. We introduce our current research on the role of alternatives in data analysis activities. Finally, we conclude with open questions and research directions.
SP  - 5
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wang, Zhimin; Wang, Haofei; Yu, Huangyue; Lu, Feng
TI  - Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System
PY  - 2021
AB  - Multimodal interaction has become a recent research focus since it offers better user experience in augmented reality (AR) systems. However, most existing works only combine two modalities at a time, e.g., gesture and speech. Multimodal interactive system integrating gaze cue has rarely been investigated. In this article, we propose a multimodal interactive system that integrates gaze, gesture, and speech in a flexibly configurable AR system. Our lightweight head-mounted device supports accurate gaze tracking, hand gesture recognition, and speech recognition simultaneously. The system can be easily configured into various modality combinations, which enables us to investigate the effects of different interaction techniques. We evaluate the efficiency of these modalities using two tasks: the lamp brightness adjustment task and the cube manipulation task. We also collect subjective feedback when using such systems. The experimental results demonstrate that the Gaze+Gesture+Speech modality is superior in terms of efficiency, and the Gesture+Speech modality is more preferred by users. Our system opens the pathway toward a multimodal interactive AR system that enables flexible configuration.
SP  - 524
EP  - 534
JF  - IEEE Transactions on Human-Machine Systems
VL  - 51
IS  - 5
PB  - 
DO  - 10.1109/thms.2021.3097973
ER  - 

TY  - JOUR
AU  - Kaur, Harsimran; Jindal, Swati; Manduchi, Roberto
TI  - Rethinking Model-Based Gaze Estimation.
PY  - 2022
AB  - Over the past several years, a number of data-driven gaze tracking algorithms have been proposed, which have been shown to outperform classic model-based methods in terms of gaze direction accuracy. These algorithms leverage the recent development of sophisticated CNN architectures, as well as the availability of large gaze datasets captured under various conditions. One shortcoming of black-box, end-to-end methods, though, is that any unexpected behaviors are difficult to explain. In addition, there is always the risk that a system trained with a certain dataset may not perform well when tested on data from a different source (the "domain gap" problem.) In this work, we propose a novel method to embed eye geometry information in an end-to-end gaze estimation network by means of a "geometric layer". Our experimental results show that our system outperforms other state-of-the-art methods in cross-dataset evaluation, while producing competitive performance over within dataset tests. In addition, the proposed system is able to extrapolate gaze angles outside the range of those considered in the training data.
SP  - 1
EP  - 17
JF  - Proceedings of the ACM on computer graphics and interactive techniques
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3530797
ER  - 

TY  - NA
AU  - Avila, Johann Felipe Gonzalez; McClelland, John C.; Teather, Robert J.; Figueroa, Pablo; Girouard, Audrey
TI  - SUI - Adaptic: A Shape Changing Prop with Haptic Retargeting
PY  - 2021
AB  - We present Adaptic, a novel ”hybrid” active/passive haptic device that can change shape to act as a proxy for a range of virtual objects in VR. We use Adaptic with haptic retargeting to redirect the user’s hand to provide haptic feedback for several virtual objects in arm’s reach using only a single prop. To evaluate the effectiveness of Adaptic with haptic retargeting, we conducted a within-subjects experiment employing a docking task to compare Adaptic to non-matching proxy objects (i.e., Styrofoam balls) and matching shape props. In our study, Adaptic sat on a desk in front of the user and changed shapes between grasps, to provide matching tactile feedback for various virtual objects placed in different virtual locations. Results indicate that the illusion was convincing: users felt they were manipulating several virtual objects in different virtual locations with a single Adaptic device. Docking performance (completion time and accuracy) with Adaptic was comparable to props without haptic retargeting.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485293
ER  - 

TY  - JOUR
AU  - Knierim, Pascal; Hein, Dimitri; Schmidt, Albrecht; Kosch, Thomas
TI  - The SmARtphone Controller - Leveraging Smartphones as Input and Output Modality for Improved Interaction within Mobile Augmented Reality Environments
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>Current interaction modalities for mobile Augmented Reality (AR) are tedious and lack expressiveness. To overcome these prevalent limitations, we developed and evaluated a multimodal interaction concept by pairing a smartphone as an input and output modality for mobile AR. In a user study (n = 24), we investigated the effects on interaction speed, accuracy, and task load for (1) virtual object manipulation as well as (2) interaction with established graphical user interfaces (GUIs). Our findings show that a smartphone-based AR controller results in significantly faster and more accurate object manipulation with reduced task load than state-of-art mid-air gestures. Our results also indicate a significant enhancement for using the physical touchscreen as a modality compared to mid-air gestures for GUI interaction. We conclude that interaction in mobile AR environments can be improved by utilizing a smartphone as an omnipresent controller. Additionally, we discuss how future AR systems can benefit from tangible touchscreens as an additional and complementary interaction modality.</jats:p>
SP  - 49
EP  - 61
JF  - i-com
VL  - 20
IS  - 1
PB  - 
DO  - 10.1515/icom-2021-0003
ER  - 

TY  - NA
AU  - Wei, Ziyun; Trummer, Immanuel; Anderson, Connor
TI  - SIGMOD Conference - Demonstrating Robust Voice Querying with MUVE: Optimally Visualizing Results of Phonetically Similar Queries
PY  - 2021
AB  - Recently proposed voice query interfaces translate voice input into SQL queries. Unreliable speech recognition on top of the intrinsic challenges of text-to-SQL translation makes it hard to reliably interpret user input. We present MUVE (Multiplots for Voice quEries), a system for robust voice querying. MUVE reduces the impact of ambiguous voice queries by filling the screen with multiplots, capturing results of phonetically similar queries. It maps voice input to a probability distribution over query candidates, executes a selected subset of queries, and visualizes their results in a multiplot. Our goal is to maximize probability to show the correct query result. Also, we want to optimize the visualization (e.g., by coloring a subset of likely results) in order to minimize expected time until users find the correct result. Via a user study, we validate a simple cost model estimating the latter overhead. The resulting optimization problem is NP-hard. We propose an exhaustive algorithm, based on integer programming, as well as a greedy heuristic. As shown in a corresponding user study, MUVE enables users to identify accurate results faster, compared to prior work.
SP  - 2798
EP  - 2802
JF  - Proceedings of the 2021 International Conference on Management of Data
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448016.3452753
ER  - 

TY  - NA
AU  - Kim, Youngwon Ryan; Kim, Gerard Jounghyun
TI  - ICCE - HoVR-Type: Smartphone as a typing interface in VR using hovering
PY  - 2017
AB  - We propose a text entry method for VR, using the smartphone and its hovering function, called the HoVR-Type. The hovering function effectively acts as the finger tracking sensor thereby allowing the user to type in the virtual space. When added with the additional phase to correct the initial touch input and having the final key entered upon the finger release, the proposed method showed competitive performance to that of the conventional “aim-and-shoot” method and also exhibited much higher usability. HoVR-Type also showed a significantly faster speed of input for the individual character. However, it remains to improve the interface with regards to reducing the error. Overall, the use of the smartphone leverages on the already established mobile user experience and can be further extended to other VR interaction techniques so that one can use the common smartphone as an all-purpose VR interaction device.
SP  - 200
EP  - 203
JF  - 2017 IEEE International Conference on Consumer Electronics (ICCE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icce.2017.7889285
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Lee, Juyoung; Bianchi, Andrea; Quigley, Aaron
TI  - MobileHCI Adjunct - WatchMI: applications of watch movement input on unmodified smartwatches
PY  - 2016
AB  - In this demo, we show that it is possible to enhance touch interaction on unmodified smartwatch to support continuous pressure touch, twist and pan gestures, by only analyzing the real-time data of Inertial Measurement Unit (IMU). Our evaluation results show that the three proposed input interfaces are accurate, noise-resistant, easy to use and can be deployed to a variety of smartwatches. We then showcase the potential of this work with seven example applications. During the demo session, users can try the prototype.
SP  - 594
EP  - 598
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2957265.2961825
ER  - 

TY  - NA
AU  - Shin, Hijung Valentina; Li, Wilmot; Durand, Frédo
TI  - UIST - Dynamic Authoring of Audio with Linked Scripts
PY  - 2016
AB  - Speech recordings are central to modern media from podcasts to audio books to e-lectures and voice-overs. Authoring these recordings involves an iterative back and forth process between script writing/editing and audio recording/editing. Yet, most existing tools treat the script and the audio separately, making the back and forth workflow very tedious. We present Voice Script, an interface to support a dynamic workflow for script writing and audio recording/editing. Our system integrates the script with the audio such that, as the user writes the script or records speech, edits to the script are translated to the audio and vice versa. Through informal user studies, we demonstrate that our interface greatly facilitates the audio authoring process in various scenarios.
SP  - 509
EP  - 516
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984561
ER  - 

TY  - NA
AU  - Fast, Ethan; Chen, Binbin; Bernstein, Michael S.
TI  - CHI - Empath: Understanding Topic Signals in Large-Scale Text
PY  - 2016
AB  - Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like "bleed" and "punch" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.
SP  - 4647
EP  - 4657
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858535
ER  - 

TY  - CHAP
AU  - Cavazza, Marc; Charles, Fred
TI  - User Interaction for Interactive Storytelling
PY  - 2016
AB  - NA
SP  - 415
EP  - 428
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gao, Yan; Lou, Jian-Guang; Zhang, Dongmei
TI  - A Hybrid Semantic Parsing Approach for Tabular Data Analysis
PY  - 2019
AB  - This paper presents a novel approach to translating natural language questions to SQL queries for given tables, which meets three requirements as a real-world data analysis application: cross-domain, multilingualism and enabling quick-start. Our proposed approach consists of: (1) a novel data abstraction step before the parser to make parsing table-agnosticism; (2) a set of semantic rules for parsing abstracted data-analysis questions to intermediate logic forms as tree derivations to reduce the search space; (3) a neural-based model as a local scoring function on a span-based semantic parser for structured optimization and efficient inference. Experiments show that our approach outperforms state-of-the-art algorithms on a large open benchmark dataset WikiSQL. We also achieve promising results on a small dataset for more complex queries in both English and Chinese, which demonstrates our language expansion and quick-start ability.
SP  - NA
EP  - NA
JF  - arXiv: Artificial Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ghosh, Sarthak; Kim, Hyeong Cheol; Cao, Yang; Wessels, Arne; Perrault, Simon T.; Zhao, Shengdong
TI  - CHI Extended Abstracts - Ringteraction: Coordinated Thumb-index Interaction Using a Ring
PY  - 2016
AB  - The thumb has the unique property of being opposable to the other fingers and is thus used to perform specific tasks such as grasping objects, which cannot be done otherwise. In this paper we present an interactive ring that takes advantage of this biomechanical advantage, by enabling thumb-index interaction. We propose a set of gestures involving the coordinated movement of the thumb against the proximal phalanx of the index finger that we call bi-digit interaction. Further, we present several scenarios where performing bi-digit interaction is quick, easy and advantageous for users.
SP  - 2640
EP  - 2647
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892371
ER  - 

TY  - JOUR
AU  - Srinivasan, Arjun; Drucker, Steven M.; Endert, Alex; Stasko, John
TI  - Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication
PY  - 2018
AB  - Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder , a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.
SP  - 672
EP  - 681
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865145
ER  - 

TY  - NA
AU  - Liu, Zhe; Vogel, Daniel; Wallace, James R.
TI  - PerDis - Applying the Cumulative Fatigue Model to Interaction on Large, Multi-Touch Displays
PY  - 2018
AB  - Large touch displays have long been studied in the lab, and are beginning to see widespread deployment in public spaces. However, a common limitation is fatigue -- often called 'gorilla arm' -- that prevents users from working with large displays for extended periods of time. A first step towards addressing fatigue is quantifying it, and while methods have been developed to quantify users' fatigue in mid-air interactions, there remains little understanding of fatigue on touch-based interfaces. To address this gap, we evaluated the accuracy of Jang et al.'s mid-air Cumulative Fatigue model for touch interaction tasks on a large display. We found that their model underestimates subjective fatigue for multi-touch interaction, but can provide accurate estimates through fine-tuning of model parameters. We discuss the implications of this finding, and the need to further develop tools to evaluate fatigue on large, multi-touch displays.
SP  - 1
EP  - NA
JF  - Proceedings of the 7th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3205873.3205890
ER  - 

TY  - JOUR
AU  - Rencis, Edgars
TI  - On Keyword-Based Ad-Hoc Querying of Hospital Data Stored in Semistar Data Ontologies
PY  - 2018
AB  - Abstract This paper sketches a possible solution to the problem of the currently growing necessity in various domains for domain experts to be able to query the database of the organization in a convenient manner. The paper focuses on the domain of hospital management where the normal practice is to involve a programmer as an intermediary between the managers and the database. This is an error-prone and cumbersome solution. The decision-making process of domain experts would hugely benefit if they could retrieve the information from the database themselves. There have been attempts to develop natural language-based query languages for this exact purpose, but the ultimate goal of the simplicity of querying has not yet been reached. The approach presented in this paper involves letting users define their queries in a very weakly-controlled natural language and then to choose among the offered query translations into a more strongly-controlled natural language which already has an efficient implementation for semistar ontologies. Experiments show that the implemented query language is very readable for non-programmers, because it lacks technical details (thanks to the nature of semistar ontologies), and because it is very intuitive. This phenomenon creates a conviction that the proposed approach is highly viable.
SP  - 27
EP  - 32
JF  - Procedia Computer Science
VL  - 138
IS  - NA
PB  - 
DO  - 10.1016/j.procs.2018.10.005
ER  - 

TY  - JOUR
AU  - Gong, Anmin; Gu, Feng; Nan, Wenya; Qu, Yi; Jiang, Changhao; Fu, Yunfa
TI  - A Review of Neurofeedback Training for Improving Sport Performance From the Perspective of User Experience.
PY  - 2021
AB  - Neurofeedback training (NFT) is a non-invasive, safe, and effective method of regulating the nerve state of the brain. Presently, NFT is widely used to prevent and rehabilitate brain diseases and improve an individual’s external performance. Among the various NFT methods, NFT to improve sport performance (SP-NFT) has become an important research and application focus in worldwide. Several studies have shown that the method is effective in improving brain function and motor control performance. However, appropriate reviews and prospective directions for this technology are lacking. This paper proposes an SP-NFT classification method based on user experience, classifies and discusses various SP-NFT research schemes reported in the existing literature, and reviews the technical principles, application scenarios, and usage characteristics of different SP-NFT schemes. Several key issues in SP-NFT development, including the factors involved in neural mechanisms, scheme selection, learning basis, and experimental implementation, are discussed. Finally, directions for the future development of SP-NFT, including SP-NFT based on other electroencephalograph characteristics, SP-NFT integrated with other technologies, and SP-NFT commercialization, are suggested. These discussions are expected to provide some valuable ideas to researchers in related fields.
SP  - 638369
EP  - 638369
JF  - Frontiers in neuroscience
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnins.2021.638369
ER  - 

TY  - NA
AU  - Fan, Mingming; Hettiarachchi, Anuruddha; Lu, Zhicong; Ha, Seyong; Gupta, Priyank
TI  - CHI Extended Abstracts - Comparing Mid-air Finger Motion with Touch for Small Target Acquisition on Wearable Devices
PY  - 2017
AB  - Mid-air finger motion takes advantage of the vast free 3D space around a device for input. Although previous research has compared mid-air finger motion with touch for mobile and large interactive surfaces, little is known about their performance for small target acquisition on ultra-small screen devices. In this paper, we empirically study the performance of mid-air finger motion and touch as input techniques for small target acquisition on smartwatches with 16 participants. Results show that mid-air finger motion can be as fast as touch but has significantly fewer errors. No statistically significant difference has been found in either mental or physical demand while using two techniques, but mid-air finger motion technique is perceived to have better performance with less frustration compared with touch.
SP  - 1593
EP  - 1600
JF  - Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3027063.3053092
ER  - 

TY  - NA
AU  - Lin, Long-Fei; Teng, Shan-Yuan; Liang, Rong-Hao; Chen, Bing-Yu
TI  - SIGGRAPH ASIA Emerging Technologies - Stylus assistant: designing dynamic constraints for facilitating stylus inputs on portable displays
PY  - 2016
AB  - Accurate stylus tracking has been integrated in portable displays, which have numerous users currently, to realize drawing and writing applications on these displays. Nonetheless, for learning how to use such applications, users heavily rely on on-screen visual guidance, which can be easily occluded by a user's hand resting on the screen while performing stylus input.
SP  - 14
EP  - NA
JF  - SIGGRAPH ASIA 2016 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2988240.2988255
ER  - 

TY  - JOUR
AU  - Kim, Nam Wook; Bylinskii, Zoya; Borkin, Michelle A.; Gajos, Krzysztof Z.; Oliva, Aude; Durand, Frédo; Pfister, Hanspeter
TI  - BubbleView: an interface for crowdsourcing image importance maps and tracking visual attention
PY  - 2017
AB  - In this paper, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal "bubbles" - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both (i) successfully approximate eye fixations on different images, and (ii) be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types.
SP  - 36
EP  - 40
JF  - ACM Transactions on Computer-Human Interaction
VL  - 24
IS  - 5
PB  - 
DO  - 10.1145/3131275
ER  - 

TY  - NA
AU  - Panda, Payod; Ho, Charles; Ham, Derek A.
TI  - Morphaces: Exploring Morphable Surfaces for Tangible Sketching in VR
PY  - 2021
AB  - This pictorial documents our inquiry into the design and utility of morphable surfaces to provide tangible feedback while sketching in Virtual Reality (VR). We explored materials and various structures that could enable a surface to morph. We designed and implemented the Morphace ecosystem that includes 3D printed accessories that enable handheld and deskmounted pen-and-surface interaction for the Oculus Quest VR device. We present this preliminary exploration with the hope that this will be explored further by the design and broader HCI community.
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450741.3465387
ER  - 

TY  - JOUR
AU  - Leake, Mackenzie; Davis, Abe; Truong, Anh; Agrawala, Maneesh
TI  - Computational video editing for dialogue-driven scenes
PY  - 2017
AB  - We present a system for efficiently editing video of dialogue-driven scenes. The input to our system is a standard film script and multiple video takes, each capturing a different camera framing or performance of the complete scene. Our system then automatically selects the most appropriate clip from one of the input takes, for each line of dialogue, based on a user-specified set of film-editing idioms. Our system starts by segmenting the input script into lines of dialogue and then splitting each input take into a sequence of clips time-aligned with each line. Next, it labels the script and the clips with high-level structural information (e.g., emotional sentiment of dialogue, camera framing of clip, etc.). After this pre-process, our interface offers a set of basic idioms that users can combine in a variety of ways to build custom editing styles. Our system encodes each basic idiom as a Hidden Markov Model that relates editing decisions to the labels extracted in the pre-process. For short scenes (
SP  - 130
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 4
PB  - 
DO  - 10.1145/3072959.3073653
ER  - 

TY  - JOUR
AU  - Wang, Xingbo; Ming, Yao; Wu, Tongshuang; Zeng, Haipeng; Wang, Yong; Qu, Huamin
TI  - DeHumor: Visual Analytics for Decomposing Humor.
PY  - 2022
AB  - Despite being a critical communication skill, grasping humor is challenginga successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3097709
ER  - 

TY  - CHAP
AU  - Ehlenz, Matthias; Leonhardt, Thiemo; Schroeder, Ulrik
TI  - OCCE - A Learning Analytics Approach in Web-Based Multi-user Learning Games.
PY  - 2019
AB  - As technology changes, learning games are adapted to target audience and available devices. Analytics methods must keep up with keeping the learner in focus. This work presents the Multi-Touch Learning Game (MTLG) framework, designed to implement cross platform educational games with support for cooperative, collaborative and competitive settings. It shows adaption of a user-centred learning analytics data model, the learning data context model, to fit circumstantial requirements of multi-user settings on a shared device in games implemented using the MTLG framework. A first field study has been conducted, and the results, challenges and lessons learned are discussed.
SP  - 167
EP  - 171
JF  - IFIP Advances in Information and Communication Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-23513-0_17
ER  - 

TY  - NA
AU  - Tahouni, Yasaman; Qamar, Isabel P. S.; Mueller, Stefanie
TI  - Tangible and Embedded Interaction - NURBSforms: A Modular Shape-Changing Interface for Prototyping Curved Surfaces
PY  - 2020
AB  - We present NURBSforms: a modular shape-changing interface for prototyping curved surfaces. Each NURBSform module represents an edge of variable curvature that, when joined together with other modules, enables designers to construct surfaces and adjust their curvature interactively. NURBSform modules vary their curvature using active and passive shape memory materials: an embedded shape memory alloy (SMA) wire increases the curvature when heated, while an elastic material recovers the flat shape when the SMA wire cools down. A hall effect sensor on each module allows users to vary the curvature by adjusting the distance of their hand. In addition, NURBSforms provides functions across multiple modules, such as 'save', 'reset', and 'load', to facilitate design exploration. Since each module is self-contained and individually controllable, NURBSform modules scale well and can be connected into large networks of curves representing various geometries. By giving examples of different NURBSforms assemblies, we demonstrate how the modularity of NURBSforms, together with its integrated computational support, enables designers to quickly explore different versions of a shape in a single integrated design process.
SP  - 403
EP  - 409
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374927
ER  - 

TY  - NA
AU  - Kumar, Chandan; Menges, Raphael; Staab, Steffen
TI  - CBMS - Assessing the Usability of Gaze-Adapted Interface against Conventional Eye-Based Input Emulation
PY  - 2017
AB  - In recent years, eye tracking systems have greatly improved, beginning to play a promising role as an input medium. Eye trackers can be used for application control either by simply emulating the mouse and keyboard devices in the traditional graphical user interface, or by customized interfaces for eye gaze events. In this work, we evaluate these two approaches to assess their impact in usability. We present a gaze-adapted Twitter application interface with direct interaction of eye gaze input, and compare it to Twitter in a conventional browser interface with gaze-based mouse and keyboard emulation. We conducted an experimental study, which indicates a significantly better subjective user experience for the gaze-adapted approach. Based on the results, we argue the need of user interfaces interacting directly to eye gaze input to provide an improved user experience, more specifically in the field of accessibility.
SP  - 793
EP  - 798
JF  - 2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cbms.2017.155
ER  - 

TY  - NA
AU  - Shakil, Asma; Lutteroth, Christof; Weber, Gerald
TI  - CHI - CodeGazer: Making Code Navigation Easy and Natural With Gaze Input
PY  - 2019
AB  - Navigating source code, an activity common in software development, is time consuming and in need of improvement. We present CodeGazer, a prototype for source code navigation using eye gaze for common navigation functions. These functions include actions such as "Go to Definition'' and "Find All Usages'' of an identifier, navigate to files and methods, move back and forth between visited points in code and scrolling. We present user study results showing that many users liked and even preferred the gaze-based navigation, in particular the "Go to Definition'' function. Gaze-based navigation is also holding up well in completion time when compared to traditional methods. We discuss how eye gaze can be integrated into traditional mouse & keyboard applications in order to make "look up'' tasks more natural.
SP  - 76
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300306
ER  - 

TY  - NA
AU  - Kandogan, Eser; Engelke, Ulrich
TI  - HILDA@SIGMOD - Towards a Unified Representation of Insight in Human-in-the-Loop Analytics: A User Study
PY  - 2018
AB  - Understanding what insights people draw from data visualizations is critical for human-in-the loop analytics systems to facilitate mixed-initiative analysis. In this paper we present results from a large user study on insights extracted from commonly used charts. We report several patterns of insights we observed and analyze their semantic structure to identify key considerations towards a unified formal representation of insight, human or computer generated. We also present a model of insight generation process, where humans and computers work cooperatively, building on each other's knowledge, where a common representation acts as the currency of interaction. While not going as far as proposing a formalism, we point to a few potential directions for representing insight. We believe our findings could also inform the design of novel human-in-the-loop analytics systems.
SP  - 3
EP  - NA
JF  - Proceedings of the Workshop on Human-In-the-Loop Data Analytics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3209900.3209912
ER  - 

TY  - NA
AU  - Luo, Danli; Gu, Jianzhe; Qin, Fang; Wang, Guanyun; Yao, Lining
TI  - UIST - E-seed: Shape-Changing Interfaces that Self Drill
PY  - 2020
AB  - As sensors and interactive devices become ubiquitous and transition outdoors and into the wild, we are met with the challenge of mass deployment and actuation. We present E-seed, a biomimetic platform that consumes little power to deploy, harvests energy from nature to install, and functions autonomously in the field. Each seed can individually self-drill into a substrate by harvesting moisture fluctuations in its ambient environment. As such, E-seed acts as a shape-changing interface to autonomously embed functional devices and interfaces into the soil, with the potential of aerial deployment in hard-to-reach locations. Our system is constructed primarily from wood veneer, making it lightweight, inexpensive, and biodegradable. In this paper, we detail our fabrication process and showcase demos that leverage the E-seed platform as a self-drilling interface. We envision that possible applications include soil sensors, sampling, and environmental monitoring for agriculture and reforestation.
SP  - 45
EP  - 57
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415855
ER  - 

TY  - JOUR
AU  - Awano, Naoyuki; Hayashi, Yuki
TI  - Psychological potential field and human eye fixation on binary line-drawing images: A comparative experimental study
PY  - 2020
AB  - Quantitatively evaluating the psychological and perceptual effects of objects is an important issue, but is difficult. In cognitive studies, the psychological potential field (PPF), which represents psychological intensities in vision and can be calculated by applying computational algorithms to digital images, may help with this issue. Although studies have reported using the PPF to evaluate psychological effects, such as impressions, detailed investigations on how the PPF represents psychological perception and its limitations have not yet been performed. Another relevant tool is the fixation map, which visualizes human eye fixations; this map is generated from actual measurements acquired by eye-tracking and does not represent psychological effects directly. Although the PPF and the fixation map are based on visual imaging, they have never been compared. In this paper, we do so for the first time, using psychological and perceptual properties of line-drawing images. The results demonstrate the difference between these methods, including their representation of different properties with respect to visual perception. Moreover, the similarity between the two methods highlights the possibility of assessing perceptual phenomena such as categorization and cognition of objects based on human vision.
SP  - 205
EP  - 214
JF  - Computational Visual Media
VL  - 6
IS  - 2
PB  - 
DO  - 10.1007/s41095-020-0169-5
ER  - 

TY  - JOUR
AU  - Parakkat, Amal Dev; Gowtham, Hari Hara; Joshi, Sarang Anil; Muthuganapathy, Ramanathan
TI  - A digital assistant for shading paper sketches
PY  - 2020
AB  - We present a mixed reality-based assistive system for shading paper sketches. Given a paper sketch made by an artist, our interface helps inexperienced users to shade it appropriately. Initially, using a simple Delaunay-triangulation based inflation algorithm, an approximate depth map is computed. The system then highlights areas (to assist shading) based on a rendering of the 2.5-dimensional inflated model of the input contour. With the help of a mixed reality system, we project the highlighted areas back to aid users. The hints given by the system are used for shading and are smudged appropriately to apply an artistic shading to the sketch. The user is given flexibility at various levels to simulate conditions such as height and light position. Experiments show that the proposed system aids novice users in creating sketches with impressive shading.
SP  - 1
EP  - 16
JF  - Visual computing for industry, biomedicine, and art
VL  - 3
IS  - 1
PB  - 
DO  - 10.1186/s42492-020-00049-7
ER  - 

TY  - NA
AU  - Narumi, Koya; Qin, Fang; Liu, Siyuan; Cheng, Huai-Yu; Gu, Jianzhe; Kawahara, Yoshihiro; Islam, Mohammad; Yao, Lining
TI  - UIST - Self-healing UI: Mechanically and Electrically Self-healing Materials for Sensing and Actuation Interfaces
PY  - 2019
AB  - Living things in nature have long been utilizing the ability to "heal" their wounds on the soft bodies to survive in the outer environment. In order to impart this self-healing property to our daily life interface, we propose Self-healing UI, a soft-bodied interface that can intrinsically self-heal damages without external stimuli or glue. The key material to achieving Self-healing UI is MWCNTs-PBS, a composite material of a self-healing polymer polyborosiloxane (PBS) and a filler material multi-walled carbon nanotubes (MWCNTs), which retains mechanical and electrical self-healability. We developed a hybrid model that combines PBS, MWCNTs-PBS, and other common soft materials including fabric and silicone to build interface devices with self-healing, sensing, and actuation capability. These devices were implemented by layer-by-layer stacking fabrication without glue or any post-processing, by leveraging the materials' inherent self-healing property between two layers. We then demonstrated sensing primitives and interactive applications that extend the design space of shape-changing interfaces with their ability to transform, conform, reconfigure, heal, and fuse, which we believe can enrich the toolbox of human-computer interaction (HCI).
SP  - 293
EP  - 306
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347901
ER  - 

TY  - NA
AU  - Leen, Danny; Veuskens, Tom; Luyten, Kris; Ramakers, Raf
TI  - CHI - JigFab: Computational Fabrication of Constraints to Facilitate Woodworking with Power Tools
PY  - 2019
AB  - We present JigFab, an integrated end-to-end system that supports casual makers in designing and fabricating constructions with power tools. Starting from a digital version of the construction, JigFab achieves this by generating various types of constraints that configure and physically aid the movement of a power tool. Constraints are generated for every operation and are custom to the work piece. Constraints are laser cut and assembled together with predefined parts to reduce waste. JigFab's constraints are used according to an interactive step-by-step manual. JigFab internalizes all the required domain knowledge for designing and building intricate structures, consisting of various types of finger joints, tenon & mortise joints, grooves, and dowels. Building such structures is normally reserved for artisans or automated with advanced CNC machinery.
SP  - 156
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300386
ER  - 

TY  - NA
AU  - Choi, In Kwon; Childers, Taylor; Raveendranath, Nirmal Kumar; Mishra, Swati; Harris, Kyle; Reda, Khairi
TI  - CHI - Concept-Driven Visual Analytics: an Exploratory Study of Model- and Hypothesis-Based Reasoning with Visualizations
PY  - 2019
AB  - Visualization tools facilitate exploratory data analysis, but fall short at supporting hypothesis-based reasoning. We conducted an exploratory study to investigate how visualizations might support a concept-driven analysis style, where users can optionally share their hypotheses and conceptual models in natural language, and receive customized plots depicting the fit of their models to the data. We report on how participants leveraged these unique affordances for visual analysis. We found that a majority of participants articulated meaningful models and predictions, utilizing them as entry points to sensemaking. We contribute an abstract typology representing the types of models participants held and externalized as data expectations. Our findings suggest ways for rearchitecting visual analytics tools to better support hypothesis- and model-based reasoning, in addition to their traditional role in exploratory analysis. We discuss the design implications and reflect on the potential benefits and challenges involved.
SP  - 68
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300298
ER  - 

TY  - NA
AU  - Kim, Tae Soo; Choi, DaEun; Choi, Yoonseo; Kim, Juho
TI  - Stylette: Styling the Web with Natural Language
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501931
ER  - 

TY  - NA
AU  - Mehrpour, Sahar; LaToza, Thomas D.; Sarvari, Hamed
TI  - ESEC/SIGSOFT FSE - RulePad: interactive authoring of checkable design rules
PY  - 2020
AB  - Good documentation offers the promise of enabling developers to easily understand design decisions. Unfortunately, in practice, design documents are often rarely updated, becoming inaccurate, incomplete, and untrustworthy. A better solution is to enable developers to write down design rules which are checked against code for consistency. But existing rule checkers require learning specialized query languages or program analysis frameworks, creating a barrier to writing project-specific rules. We introduce two new techniques for authoring design rules: snippet-based authoring and semi-natural-language authoring. In snippet-based authoring, developers specify characteristics of elements to match by writing partial code snippets. In semi-natural language authoring, a textual representation offers a representation for understanding design rules and resolving ambiguities. We implemented these approaches in RulePad. To evaluate RulePad, we conducted a between-subjects study with 14 participants comparing RulePad to the PMD Designer, a utility for writing rules in a popular rule checker. We found that those with RulePad were able to successfully author 13 times more query elements in significantly less time and reported being significantly more willing to use RulePad in their everyday work.
SP  - 386
EP  - 397
JF  - Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3368089.3409751
ER  - 

TY  - NA
AU  - Al-Sada, Mohammed; Höglund, Thomas; Khamis, Mohamed; Urbani, Jaryd; Nakajima, Tatsuo
TI  - AH - Orochi: Investigating Requirements and Expectations for Multipurpose Daily Used Supernumerary Robotic Limbs
PY  - 2019
AB  - Supernumerary robotic limbs (SRLs) present many opportunities for daily use. However, their obtrusiveness and limitations in interaction genericity hinder their daily use. To address challenges of daily use, we extracted three design considerations from previous literature and embodied them in a wearable we call Orochi. The considerations include the following: 1) multipurpose use, 2) wearability by context, and 3) unobtrusiveness in public. We implemented Orochi as a snake-shaped robot with 25 DoFs and two end effectors, and demonstrated several novel interactions enabled by its limber design. Using Orochi, we conducted hands-on focus groups to explore how multipurpose SRLs are used daily and we conducted a survey to explore how they are perceived when used in public. Participants approved Orochi's design and proposed different use cases and postures in which it could be worn. Orochi's unobtrusive design was generally well received, yet novel interactions raise several challenges for social acceptance. We discuss the significance of our results by highlighting future research opportunities based on the design, implementation, and evaluation of Orochi.
SP  - NA
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311850
ER  - 

TY  - NA
AU  - Neshati, Ali; Rey, Bradley; Faleel, Ahmed Shariff Mohommed; Bardot, Sandra; Latulipe, Celine; Irani, Pourang
TI  - CHI - BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion
PY  - 2021
AB  - We present BezelGlide, a novel suite of bezel interaction techniques, designed to minimize screen occlusion and ‘fat finger’ effects, when interacting with common graphs on smartwatches. To explore the design of BezelGlide, we conducted two user studies. First, we quantified the amount of screen occlusion experienced when interacting with the smartwatch bezel. Next, we designed two techniques that involve gliding the finger along the smartwatch bezel for graph interaction. Full BezelGlide (FBG) and Partial BezelGlide (PBG), use the full or a portion of the bezel, respectively, to reduce screen occlusion while scanning a line chart for data. In the common value detection task, we find that PBG outperforms FBG and Shift, a touchscreen occlusion-free technique, both quantitatively and subjectively, also while mobile. We finally illustrate the generzability potential of PBG to interact with common graph types making it a valuable interaction technique for smartwatch users.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445201
ER  - 

TY  - JOUR
AU  - Henkin, Rafael; Turkay, Cagatay
TI  - Words of Estimative Correlation: Studying Verbalizations of Scatterplots.
PY  - 2022
AB  - Natural language and visualization are being increasingly deployed together for supporting data analysis in different ways, from multimodal interaction to enriched data summaries and insights. Yet, researchers still lack systematic knowledge on how viewers verbalize their interpretations of visualizations, and how they interpret verbalizations of visualizations in such contexts. We describe two studies aimed at identifying characteristics of data and charts that are relevant in such tasks. The first study asks participants to verbalize what they see in scatterplots that depict various levels of correlations. The second study then asks participants to choose visualizations that match a given verbal description of correlation. We extract key concepts from responses, organize them in a taxonomy and analyze the categorized responses. We observe that participants use a wide range of vocabulary across all scatterplots, but particular concepts are preferred for higher levels of correlation. A comparison between the studies reveals the ambiguity of some of the concepts. We discuss how the results could inform the design of multimodal representations aligned with the data and analytical tasks, and present a research roadmap to deepen the understanding about visualizations and natural language.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 4
PB  - 
DO  - 10.1109/tvcg.2020.3023537
ER  - 

TY  - NA
AU  - Sun, Qirui; Li, Shuqin; Yao, Zhihao; Feng, Yuan-Ling; Mi, Haipeng
TI  - PalmBeat: A Kinesthetic Way to Feel Groove With Music
PY  - 2021
AB  - We propose a novel way to enhance the music experience, which evokes listeners to move to the music then feel the musicians’ groove through their body movements. In musical psychology, the groove is defined as an inner urge or a spontaneous behavior to move with music, such as hands tapping, head bobbing, and full-body dance. Although everyone feels the groove while listening to music, professional musicians perform better in catching the music rhythm and translating it to body movements. In this work, we used active force feedback to provide musicians’ hand movements for listeners and conducted a user study to evaluate the approach. Results show a significant influence on listeners’ groove perception, and the influence positively correlates with the frequency of force feedback. Compared to the condition without the force feedback, listening to the music with force feedback will enhancing listeners’ experience of groove, making them feel the groove as the musicians’ feeling.
SP  - NA
EP  - NA
JF  - 12th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460881.3460932
ER  - 

TY  - CHAP
AU  - Shaheen, Sara M.; AlHalawani, Sawsan; Alnabet, Nuha; Alhenaki, Dana
TI  - Analytical Experiments on the Utilization of Data Visualizations
PY  - 2019
AB  - Visualizing data has been well known and widely used to facilitate better understanding of data. Using the right type of visualization enables people to interpret data in a more accurate and correct way and would support decision makers in taking the right decisions. Many researchers have proposed the best types of charts and graphs to visualize data. They recommend certain types of charts based on the data type and the purpose of the visualization. However and up to our knowledge, there has not been studies which would experiment the righteous of such selection of the visualization charts. The main purpose of this research is to survey a large group of people to study the effect of selecting certain chart types on people’s comprehension of the visualized data. We conduct a user study to validate the theoretical assumptions on the selection of the best chart types. We evaluated the use of column chart to visualize categorical single variable data and line chart to visualize temporal data against other charts. We analyzed the user study participants’ performance according to their response time, accuracy of the results and overall satisfaction.
SP  - 148
EP  - 161
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-36365-9_12
ER  - 

TY  - NA
AU  - Shibata, Tomoki; Afergan, Daniel; Kong, Danielle; Yuksel, Beste F.; MacKenzie, Scott; Jacob, Robert J. K.
TI  - CHI Extended Abstracts - Text Entry for Ultra-Small Touchscreens Using a Fixed Cursor and Movable Keyboard
PY  - 2016
AB  - Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry, due to the "fat finger" problem, wherein users struggle to select elements much smaller than their fingers. In order to address the challenge, we introduce DriftBoard, a panning based text entry technique in which the user types by navigating the movable QWERTY keyboard with respect to the fixed cursor point. In this paper, we describe interactions on the proposed typing system, and report a preliminary result of our user study on a watch-size touchscreen with a text copy task comparing DriftBoard to two published ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). Within the user study, DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that the proposed panning-based typing is another promising input form for text entry on ultra-small touchscreens.
SP  - 3770
EP  - 3773
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2890230
ER  - 

TY  - JOUR
AU  - Grundhöfer, Anselm; Iwai, Daisuke
TI  - Recent Advances in Projection Mapping Algorithms, Hardware and Applications
PY  - 2018
AB  - NA
SP  - 653
EP  - 675
JF  - Computer Graphics Forum
VL  - 37
IS  - 2
PB  - 
DO  - 10.1111/cgf.13387
ER  - 

TY  - NA
AU  - Setlur, Vidya; Kumar, Arathi
TI  - IEEE VIS (Short Papers) - Sentifiers: Interpreting Vague Intent Modifiers in Visual Analysis using Word Co-occurrence and Sentiment Analysis
PY  - 2020
AB  - Natural language interaction with data visualization tools often involves the use of vague subjective modifiers in utterances such as "show me the sectors that are performing" and "where is a good neighborhood to buy a house?." Interpreting these modifiers is often difficult for these tools because their meanings lack clear semantics and are in part defined by context and personal user preferences. This paper presents a system called Sentifiers that makes a first step in better understanding these vague predicates. The algorithm employs word co-occurrence and sentiment analysis to determine which data attributes and filters ranges to associate with the vague predicates. The provenance results from the algorithm are exposed to the user as interactive text that can be repaired and refined. We conduct a qualitative evaluation of the Sentifiers that indicates the usefulness of the interface as well as opportunities for better supporting subjective utterances in visual analysis tasks through natural language.
SP  - 216
EP  - 220
JF  - 2020 IEEE Visualization Conference (VIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vis47514.2020.00050
ER  - 

TY  - JOUR
AU  - Rill, Róbert Adrian; Faragó, Kinga Bettina
TI  - Gaze-based Cursor Control Impairs Performance in Divided Attention.
PY  - 2018
AB  - In this work we investigate the effects of switching from mouse cursor control to gaze-based control in a computerized divided attention game. We conducted experiments with nine participants performing a task that requires continuous focused concentration and frequent shifts of attention. Despite carefully controlling experimental and design aspects, the performance of subjects was considerably impaired when using gaze-based control. The participants were experienced users of the mouse control version of the task, we adjusted the difficulty to the more demanding conditions and selected the parameters of gaze input based on previous research findings. In contrast to our assumptions, experienced users could not get used to gaze-based control in the amount of experiments we performed. Additionally we consider the strategies of users, i.e. their method of problem solving, and found that it is possible to make progress in our task even during a short amount of practice. The results of this study provide evidence that the adoption of interfaces controlled by human eye-gaze in cognitively demanding environments require careful design, proper testing and sufficient user training.
SP  - 1071
EP  - 1087
JF  - Acta Cybernetica
VL  - 23
IS  - 4
PB  - 
DO  - 10.14232/actacyb.23.4.2018.6
ER  - 

TY  - JOUR
AU  - Aranyi, Gabor; Pecune, Florian; Charles, Fred; Pelachaud, Catherine; Cavazza, Marc
TI  - Affective Interaction with a Virtual Character Through an fNIRS Brain-Computer Interface.
PY  - 2016
AB  - Affective brain-computer interfaces (BCI) harness Neuroscience knowledge to develop affective interaction from first principles. In this article, we explore affective engagement with a virtual agent through Neurofeedback (NF). We report an experiment where subjects engage with a virtual agent by expressing positive attitudes towards her under a NF paradigm. We use for affective input the asymmetric activity in the dorsolateral prefrontal cortex (DL-PFC), which has been previously found to be related to the high-level affective-motivational dimension of approach/avoidance. The magnitude of left-asymmetric DL-PFC activity, measured using functional near infrared spectroscopy (fNIRS) and treated as a proxy for approach, is mapped onto a control mechanism for the virtual agent’s facial expressions, in which action units (AUs) are activated through a neural network. We carried out an experiment with 18 subjects, which demonstrated that subjects are able to successfully engage with the virtual agent by controlling their mental disposition through NF, and that they perceived the agent’s responses as realistic and consistent with their projected mental disposition. This interaction paradigm is particularly relevant in the case of affective BCI as it facilitates the volitional activation of specific areas normally not under conscious control. Overall, our contribution reconciles a model of affect derived from brain metabolic data with an ecologically valid, yet computationally controllable, virtual affective communication environment.
SP  - 70
EP  - 70
JF  - Frontiers in computational neuroscience
VL  - 10
IS  - NA
PB  - 
DO  - 10.3389/fncom.2016.00070
ER  - 

TY  - CHAP
AU  - Chen, Yuan; Sun, Junwei; Xu, Qiang; Lank, Edward; Irani, Pourang; Li, Wei
TI  - INTERACT (5) - Global Scene Filtering, Exploration, and Pointing in Occluded Virtual Space.
PY  - 2021
AB  - Target acquisition in an occluded environment is challenging given the omni-directional and first-person view in virtual reality (VR). We propose Solar-Casting, a global scene filtering technique to manage occlusion in VR. To improve target search, users control a reference sphere centered at their head through varied occlusion management modes: Hide, SemiT (Semi-Transparent), Rotate. In a preliminary study, we find SemiT to be better suited for understanding the context without sacrificing performance by applying semi-transparency to targets within the controlled sphere. We then compare Solar-Casting to highly efficient selection techniques to acquire targets in a dense and occluded VR environment. We find that Solar-Casting performs competitively to other techniques in known environments, where the target location information is revealed. However, in unknown environments, requiring target search, Solar-Casting outperforms existing approaches. We conclude with scenarios demonstrating how Solar-Casting can be applied to crowded and occluded environments in VR applications.
SP  - 156
EP  - 176
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85607-6_11
ER  - 

TY  - JOUR
AU  - Srinivasan, Arjun; Stasko, John
TI  - Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks
PY  - 2017
AB  - Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.
SP  - 511
EP  - 521
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2017.2745219
ER  - 

TY  - BOOK
AU  - Zhang, Cheng; Bedri, Abdelkareem; Reyes, Gabriel; Bercik, Bailey; Inan, Omer T.; Starner, Thad; Abowd, Gregory D.
TI  - ISS - TapSkin: Recognizing On-Skin Input for Smartwatches
PY  - 2016
AB  - The touchscreen has been the dominant input surface for smartphones and smartwatches. However, its small size compared to a phone limits the richness of the input gestures that can be supported. We present TapSkin, an interaction technique that recognizes up to 11 distinct tap gestures on the skin around the watch using only the inertial sensors and microphone on a commodity smartwatch. An evaluation with 12 participants shows our system can provide classification accuracies from 90.69% to 97.32% in three gesture families -- number pad, d-pad, and corner taps. We discuss the opportunities and remaining challenges for widespread use of this technique to increase input richness on a smartwatch without requiring further on-body instrumentation.
SP  - 13
EP  - 22
JF  - Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2992154.2992187
ER  - 

TY  - NA
AU  - Ahuja, Karan; Streli, Paul; Holz, Christian
TI  - UIST - TouchPose: Hand Pose Prediction, Depth Estimation, and Touch Classification from Capacitive Images
PY  - 2021
AB  - Today’s touchscreen devices commonly detect the coordinates of user input through capacitive sensing. Yet, these coordinates are the mere 2D manifestations of the more complex 3D configuration of the whole hand—a sensation that touchscreen devices so far remain oblivious to. In this work, we introduce the problem of reconstructing a 3D hand skeleton from capacitive images, which encode the sparse observations captured by touch sensors. These low-resolution images represent intensity mappings that are proportional to the distance to the user’s fingers and hands. We present the first dataset of capacitive images with corresponding depth maps and 3D hand pose coordinates, comprising 65,374 aligned records from 10 participants. We introduce our supervised method TouchPose, which learns a 3D hand model and a corresponding depth map using a cross-modal trained embedding from capacitive images in our dataset. We quantitatively evaluate TouchPose’s accuracy in touch classification, depth estimation, and 3D joint reconstruction, showing that our model generalizes to hand poses it has never seen during training and can infer joints that lie outside the touch sensor’s volume. Enabled by TouchPose, we demonstrate a series of interactive apps and novel interactions on multitouch devices. These applications show TouchPose’s versatile capability to serve as a general-purpose model, operating independent of use-case, and establishing 3D hand pose as an integral part of the input dictionary for application designers and developers. We also release our dataset, code, and model to enable future work in this domain.
SP  - 997
EP  - 1009
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474801
ER  - 

TY  - JOUR
AU  - Law, Po-Ming; Basole, Rahul C.; Wu, Yanhong
TI  - Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification
PY  - 2018
AB  - Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.
SP  - 427
EP  - 437
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2864526
ER  - 

TY  - NA
AU  - Di Geronimo, Linda; Canonica, Andrea; Husmann, Maria; Norrie, Moira C.
TI  - EICS - Continuous tilting interaction techniques on mobile devices for controlling public displays
PY  - 2017
AB  - The use of mobile devices to interact with public or semi-public displays has been widely studied. Researchers have investigated the potential use of motion gestures in such settings to allow users to control the display without having to shift either their gaze from the display or their hand position on the device. However, tilting gestures still offer several implementation challenges and multiple variants of such gestures have been proposed. For this reason, we designed a framework that supports the rapid development of web-based applications and enables experimentation with a variety of techniques for continuous tilting interactions. Moreover, we present a study of three different motion gestures (Constant Move, Balance Board and Mapped Container) that can be used to control the cursor on a remote screen.
SP  - 21
EP  - 26
JF  - Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3102113.3102120
ER  - 

TY  - NA
AU  - Wu, Zhengkai; Le, Vu; Tiwari, Ashish; Gulwani, Sumit; Radhakrishna, Arjun; Radiček, Ivan; Soares, Gustavo; Wang, Xinyu; Li, Zhenwen; Xie, Tao
TI  - NL2Viz: natural language to visualization via constrained syntax-guided synthesis
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3540250.3549140
ER  - 

TY  - JOUR
AU  - Xu, Pengfei; Fu, Hongbo; Zheng, Youyi; Singh, Karan; Huang, Hui; Tai, Chiew-Lan
TI  - Model-Guided 3D Sketching
PY  - 2018
AB  - We present a novel 3D model-guided interface for in-situ sketching on 3D planes. Our work is motivated by evolutionary design, where existing 3D objects form the basis for conceptual re-design or further design exploration. We contribute a novel workflow that exploits the geometry of an underlying 3D model to infer 3D planes on which 2D strokes drawn that are on and around the 3D model should be meaningfully projected. This provides users with the nearly modeless fluidity of a sketching interface, and is particularly useful for 3D sketching over planes that are not easily accessible or do not preexist. We also provide an additional set of tools, including sketching with explicit plane selection and model-aware canvas manipulation. Our system is evaluated with a user study, showing that our technique is easy to learn and effective for rapid sketching of product design variations around existing 3D models.
SP  - 2927
EP  - 2939
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 10
PB  - 
DO  - 10.1109/tvcg.2018.2860016
ER  - 

TY  - NA
AU  - Narechania, Arpit; Fourney, Adam; Lee, Bongshin; Ramos, Gonzalo
TI  - IUI - DIY: Assessing the Correctness of Natural Language to SQL Systems
PY  - 2021
AB  - Designing natural language interfaces for querying databases remains an important goal pursued by researchers in natural language processing, databases, and HCI. These systems receive natural language as input, translate it into a formal database query, and execute the query to compute a result. Because the responses from these systems are not always correct, it is important to provide people with mechanisms to assess the correctness of the generated query and computed result. However, this assessment can be challenging for people who lack expertise in query languages. We present Debug-It-Yourself (DIY), an interactive technique that enables users to assess the responses from a state-of-the-art natural language to SQL (NL2SQL) system for correctness and, if possible, fix errors. DIY provides users with a sandbox where they can interact with (1) the mappings between the question and the generated query, (2) a small-but-relevant subset of the underlying database, and (3) a multi-modal explanation of the generated query. End-users can then employ a back-of-the-envelope calculation debugging strategy to evaluate the system’s response. Through an exploratory study with 12 users, we investigate how DIY helps users assess the correctness of the system’s answers and detect & fix errors. Our observations reveal the benefits of DIY while providing insights about end-user debugging strategies and underscore opportunities for further improving the user experience.
SP  - 597
EP  - 607
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450667
ER  - 

TY  - JOUR
AU  - Aurisano, Jillian; Kumar, Abhinav; Alsaiari, Abeer; Di Eugenio, Barbara; Johnson, Andrew
TI  - Many At Once: Capturing Intentions to Create And Use Many Views At Once In Large Display Environments
PY  - 2020
AB  - NA
SP  - 229
EP  - 240
JF  - Computer Graphics Forum
VL  - 39
IS  - 3
PB  - 
DO  - 10.1111/cgf.13976
ER  - 

TY  - JOUR
AU  - Sperrle, Fabian; Jeitler, Astrik; Bernard, Jürgen; Keim, Daniel A.; El-Assady, Mennatallah
TI  - Co-adaptive visual data analysis and guidance processes
PY  - 2021
AB  - NA
SP  - 93
EP  - 105
JF  - Computers & Graphics
VL  - 100
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.06.016
ER  - 

TY  - JOUR
AU  - Yan, Shuo; Ding, Gangyi; Li, Hongsong; Sun, Ningxiao; Guan, Zheng; Wu, Yufeng; Zhang, Longfei; Huang, Tianyu
TI  - Exploring Audience Response in Performing Arts with a Brain-Adaptive Digital Performance System
PY  - 2017
AB  - Audience response is an important indicator of the quality of performing arts. Psychophysiological measurements enable researchers to perceive and understand audience response by collecting their bio-signals during a live performance. However, how the audience respond and how the performance is affected by these responses are the key elements but are hard to implement. To address this issue, we designed a brain-computer interactive system called Brain-Adaptive Digital Performance (BADP) for the measurement and analysis of audience engagement level through an interactive three-dimensional virtual theater. The BADP system monitors audience engagement in real time using electroencephalography (EEG) measurement and tries to improve it by applying content-related performing cues when the engagement level decreased.In this article, we generate EEG-based engagement level and build thresholds to determine the decrease and re-engage moments. In the experiment, we simulated two types of theatre performance to provide participants a high-fidelity virtual environment using the BADP system. We also create content-related performing cues for each performance under three different conditions. The results of these evaluations show that our algorithm could accurately detect the engagement status and the performing cues have a positive impact on regaining audience engagement across different performance types. Our findings open new perspectives in audience-based theatre performance design.
SP  - 16
EP  - 28
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 7
IS  - 4
PB  - 
DO  - 10.1145/3009974
ER  - 

TY  - NA
AU  - Fischer, Michael; Campagna, Giovanni; Xu, Silei; Lam, Monica S.
TI  - MobileHCI - Brassau: automatic generation of graphical user interfaces for virtual assistants
PY  - 2018
AB  - This paper presents Brassau, a graphical virtual assistant that converts natural language commands into GUIs. A virtual assistant with a GUI has the following benefits compared to text or speech based virtual assistants: users can monitor multiple queries simultaneously, it is easy to re-run complex commands, and user can adjust settings using multiple modes of interaction. Brassau introduces a novel template-based approach that leverages a large corpus of images to make GUIs visually diverse and interesting. Brassau matches a command from the user to an image to create a GUI. This approach decouples the commands from GUIs and allows for reuse of GUIs across multiple commands. In our evaluation, users prefer the widgets produced by Brassau over plain GUIs.
SP  - 33
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229481
ER  - 

TY  - JOUR
AU  - Hullman, Jessica; Gelman, Andrew
TI  - Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference
PY  - 2021
AB  - Research and development in computer science and statistics have produced increasingly sophisticated software interfaces for interactive and exploratory analysis, optimized for easy pattern finding and data exposure. But design philosophies that emphasize exploration over other phases of analysis risk confusing a need for flexibility with a conclusion that exploratory visual analysis is inherently “model free” and cannot be formalized. We describe how without a grounding in theories of human statistical inference, research in exploratory visual analysis can lead to contradictory interface objectives and representations of uncertainty that can discourage users from drawing valid inferences. We discuss how the concept of a model check in a Bayesian statistical framework unites exploratory and confirmatory analysis, and how this understanding relates to other proposed theories of graphical inference. Viewing interactive analysis as driven by model checks suggests new directions for software and empirical research around exploratory and visual analysis. For example, systems might enable specifying and explicitly comparing data to null and other reference distributions and better representations of uncertainty. Implications of Bayesian and other theories of graphical inference can be tested against outcomes of interactive analysis by people to drive theory development.
SP  - NA
EP  - NA
JF  - Harvard Data Science Review
VL  - NA
IS  - NA
PB  - 
DO  - 10.1162/99608f92.3ab8a587
ER  - 

TY  - JOUR
AU  - Prabhakar, Gowdham; Biswas, Pradipta
TI  - A Brief Survey on Interactive Automotive UI
PY  - 2021
AB  - Abstract Automotive User Interface (AutoUI) is relatively a new discipline in the context of both Transportation Engineering and Human Machine Interaction (HMI). It covers various HMI aspects both inside and outside vehicle ranging from operating the vehicle itself, undertaking various secondary tasks, driver behaviour analysis, cognitive load estimation and so on. This review paper discusses various interactive HMI inside a vehicle used for undertaking secondary tasks. We divided recent HMIs through four sections on virtual touch interfaces, wearable devices, speech recognition and non-visual interfaces and eye gaze controlled systems. Finally, we summarized advantages and disadvantages of various technologies.
SP  - 100089
EP  - NA
JF  - Transportation Engineering
VL  - 6
IS  - NA
PB  - 
DO  - 10.1016/j.treng.2021.100089
ER  - 

TY  - NA
AU  - Kondaveeti, Sai Anirudh; Vidyapu, Sandeep; Bhattacharya, Samit
TI  - IHCI - Improved Gaze Likelihood based Web Browsing
PY  - 2016
AB  - Web browsing is the dominant activity among all the online based activities. With more people getting connected to the WWW(World Wide Web), its importance is further enhanced. Web browsing tasks are accomplished using the point-and-click technologies such as mouse and touch based interaction. With the advent of new technologies such as Eye gaze tracking, new paradigm of web interaction is evolving based on gaze view. Earlier works used gaze as medium of interaction to select the discrete UI elements (hyperlinks). But, there is an important need to reduce the time spent on selecting a hyperlink, as increase in time lag may lead to frustration and dissatisfaction of web browsing. In this paper, we propose a method to highlight only the probable hyperlinks than all the hyperlinks in a region of interest. This method helps to reduce the time involved in selecting a hyperlink. Our proposed method is evaluated and compared against the state-of-the-art methods. The results show that, our method reduces the number of activated hyperlinks and time to access a hyperlink.
SP  - 84
EP  - 89
JF  - Proceedings of the 8th Indian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3014362.3014371
ER  - 

TY  - NA
AU  - González, Rafael Morales; Appert, Caroline; Bailly, Gilles; Pietriga, Emmanuel
TI  - CHI - Passive yet Expressive TouchTokens
PY  - 2017
AB  - TouchTokens are passive tokens that can be recognized on any capacitive surface based on the spatial configuration of the fingers that hold them. However, interaction with these tokens is confined to the basic two-state model of touch interaction as the system only knows the tokens' position and cannot detect tokens that are not touched. We increase the expressive power of TouchTokens by introducing laser-cut lattice hinges in their design, so as to make them flexible. A new recognizer, that analyzes the micro-movements of the fingers that hold the tokens, enables the system to detect when a token is left on the surface rather than taken off it. It can also detect bend events that can be mapped to command triggers, and a squeezed state that can be used for quasi-modal interaction.
SP  - 3741
EP  - 3745
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025894
ER  - 

TY  - CONF
AU  - Law, Po-Ming; Lo, Leo Yu-Ho; Endert, Alex; Stasko, John; Qu, Huamin
TI  - CHI - Causal Perception in Question-Answering Systems
PY  - 2021
AB  - Root cause analysis is a common data analysis task. While question-answering systems enable people to easily articulate a why question (e.g., why students in Massachusetts have high ACT Math scores on average) and obtain an answer, these systems often produce questionable causal claims. To investigate how such claims might mislead users, we conducted two crowdsourced experiments to study the impact of showing different information on user perceptions of a question-answering system. We found that in a system that occasionally provided unreasonable responses, showing a scatterplot increased the plausibility of unreasonable causal claims. Also, simply warning participants that correlation is not causation seemed to lead participants to accept reasonable causal claims more cautiously. We observed a strong tendency among participants to associate correlation with causation. Yet, the warning appeared to reduce the tendency. Grounded in the findings, we propose ways to reduce the illusion of causality when using question-answering systems.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Saktheeswaran, Ayshwarya; Srinivasan, Arjun; Stasko, John
TI  - Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis
PY  - 2020
AB  - Interaction plays a vital role during visual network exploration as users need to engage with both elements in the view (e.g., nodes, links) and interface controls (e.g., sliders, dropdown menus). Particularly as the size and complexity of a network grow, interactive displays supporting multimodal input (e.g., touch, speech, pen, gaze) exhibit the potential to facilitate fluid interaction during visual network exploration and analysis. While multimodal interaction with network visualization seems like a promising idea, many open questions remain. For instance, do users actually prefer multimodal input over unimodal input, and if so, why? Does it enable them to interact more naturally, or does having multiple modes of input confuse users? To answer such questions, we conducted a qualitative user study in the context of a network visualization tool, comparing speech- and touch-based unimodal interfaces to a multimodal interface combining the two. Our results confirm that participants strongly prefer multimodal input over unimodal input attributing their preference to: 1) the freedom of expression, 2) the complementary nature of speech and touch, and 3) integrated interactions afforded by the combination of the two modalities. We also describe the interaction patterns participants employed to perform common network visualization operations and highlight themes for future multimodal network visualization systems to consider.
SP  - 2168
EP  - 2179
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 6
PB  - 
DO  - 10.1109/tvcg.2020.2970512
ER  - 

TY  - JOUR
AU  - Deufemia, Vincenzo; Risi, Michele
TI  - Multi-Domain Recognition of Hand-Drawn Diagrams Using Hierarchical Parsing
PY  - 2020
AB  - This paper presents an approach for the recognition of multi-domain hand-drawn diagrams, which exploits Sketch Grammars (SkGs) to model the symbols’ shape and the abstract syntax of diagrammatic notations. The recognition systems automatically generated from SkGs process the input sketches according to the following phases: the user’ strokes are first segmented and interpreted as primitive shapes, then by exploiting the domain context they are clustered into symbols of the domain and, finally, an interpretation of whole diagram is given. The main contribution of this paper is an efficient model of parsing suitable for both interactive and non-interactive sketch-based interfaces, configurable to different domains, and able to exploit contextual information for improving recognition accuracy and solving interpretation ambiguities. The proposed approach was evaluated in the domain of UML class diagrams obtaining good results in terms of recognition accuracy and usability.
SP  - 52
EP  - NA
JF  - Multimodal Technologies and Interaction
VL  - 4
IS  - 3
PB  - 
DO  - 10.3390/mti4030052
ER  - 

TY  - BOOK
AU  - Büschel, Wolfgang; Reipschläger, Patrick; Langner, Ricardo; Dachselt, Raimund
TI  - ISS - Investigating the Use of Spatial Interaction for 3D Data Visualization on Mobile Devices
PY  - 2017
AB  - Three-dimensional visualizations employing traditional input and output technologies have well-known limitations. Immersive technologies, natural interaction techniques, and recent developments in data physicalization may help to overcome these issues. In this context, we are specifically interested in the usage of spatial interaction with mobile devices for improved 3D visualizations. To contribute to a better understanding of this interaction style, we implemented example visualizations on a spatially-tracked tablet and investigated their usage and potential. In this paper, we report on a qualitative study comparing spatial interaction with inplace 3D visualizations to classic touch interaction regarding typical visualization tasks: navigation of unknown datasets, comparison of individual data objects, and the understanding and memorization of structures in the data. We identify several distinct usage patterns and derive recommendations for using spatial interaction in 3D data visualization.
SP  - 62
EP  - 71
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134125
ER  - 

TY  - NA
AU  - Prabhakar, Gowdham; Biswas, Pradipta
TI  - A Brief Survey on Interactive Automotive UI
PY  - 2021
AB  - Automotive User Interface (AutoUI) is relatively a new discipline in the context of both Transportation Engineering and Human Machine Interaction (HMI). It covers various HMI aspects both inside and outside vehicle ranging from operating the vehicle itself, undertaking various secondary tasks, driver behaviour analysis, cognitive load estimation and so on. This review paper discusses various interactive HMI inside a vehicle used for undertaking secondary tasks. We divided recent HMIs through four sections on virtual touch interfaces, wearable devices, speech recognition and non-visual interfaces and eye gaze controlled systems. Finally, we summarized advantages and disadvantages of various technologies.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Perteneder, Florian; Bresler, Martin; Grossauer, Eva-Maria; Leong, Joanne; Rendl, Christian; Haller, Michael J.
TI  - CSCW Companion - cLuster: Applications for Smart Clustering of Free-Hand Sketches
PY  - 2016
AB  - Structuring and rearranging free-hand sketches on large interactive surfaces typically requires making multiple stroke selections. Without well-designed selection tools, this can be both time-consuming and fatiguing. Investigating the concept of automated clustering, we conducted a background study to understand the varying perspectives of how elements in sketches can be grouped. In response to these diverse user expectations, we present cLuster, a flexible, domain-independent clustering approach for free-hand sketches. Our approach is designed to accept an initial user selection, which is then used to calculate a linear combination of pre-trained perspectives in real-time. The remaining elements are then clustered. We demonstrate the utility of our approach in a variety of application scenarios.
SP  - 81
EP  - 85
JF  - Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2818052.2874331
ER  - 

TY  - NA
AU  - Teyssier, Marc; Bailly, Gilles; Pelachaud, Catherine; Lecolinet, Eric
TI  - UIST - MobiLimb: Augmenting Mobile Devices with a Robotic Limb
PY  - 2018
AB  - In this paper, we explore the interaction space of MobiLimb, a small 5-DOF serial robotic manipulator attached to a mobile device. It (1) overcomes some limitations of mobile devices (static, passive, motionless); (2) preserves their form factor and I/O capabilities; (3) can be easily attached to or removed from the device; (4) offers additional I/O capabilities such as physical deformation and (5) can support various modular elements such as sensors, lights or shells. We illustrate its potential through three classes of applications: As a tool, MobiLimb offers tangible affordances and an expressive controller that can be manipulated to control virtual and physical objects. As a partner, it reacts expressively to users' actions to foster curiosity and engagement or assist users. As a medium, it provides rich haptic feedback such as strokes, pat and other tactile stimuli on the hand or the wrist to convey emotions during mediated multimodal communications.
SP  - 53
EP  - 63
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - 18
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242626
ER  - 

TY  - NA
AU  - Baudisch, Patrick
TI  - AVI - Personal Fabrication in HCI: Trends and Challenges
PY  - 2016
AB  - In this keynote, I will argue that 3D printing and personal fabrication in general are about to bring massive, disruptive change to interactive computing, as well as to computing as a whole. I discuss the six challenges that need to be addressed for this change to take place, and I explain why I think researchers in HCI will play a key role in it.
SP  - 1
EP  - 2
JF  - Proceedings of the International Working Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909132.2934645
ER  - 

TY  - NA
AU  - Setlur, Vidya; Hoque, Enamul; Kim, Dae Hyun; Chang, Angel X.
TI  - UIST - Sneak Pique: Exploring Autocompletion as a Data Discovery Scaffold for Supporting Visual Analysis
PY  - 2020
AB  - Natural language interaction has evolved as a useful modality to help users explore and interact with their data during visual analysis. Little work has been done to explore how autocompletion can help with data discovery while helping users formulate analytical questions. We developed a system called \system as a design probe to better understand the usefulness of autocompletion for visual analysis. We ran three Mechanical Turk studies to evaluate user preferences for various text- and visualization widget-based autocompletion design variants for helping with partial search queries. Our findings indicate that users found data previews to be useful in the suggestions. Widgets were preferred for previewing temporal, geospatial, and numerical data while text autocompletion was preferred for categorical and hierarchical data. We conducted an exploratory analysis of our system implementing this specific subset of preferred autocompletion variants. Our insights regarding the efficacy of these autocompletion suggestions can inform the future design of natural language interfaces supporting visual analysis.
SP  - 966
EP  - 978
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415813
ER  - 

TY  - JOUR
AU  - Hoque, Enamul; Setlur, Vidya; Tory, Melanie; J, Dykeman Isaac
TI  - Applying Pragmatics Principles for Interaction with Visual Analytics
PY  - 2017
AB  - Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.
SP  - 309
EP  - 318
JF  - IEEE transactions on visualization and computer graphics
VL  - 24
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2017.2744684
ER  - 

TY  - JOUR
AU  - Reicherts, Leon; Rogers, Yvonne; Capra, Licia; Wood, Ethan; Duong, Tu Dinh; Sebire, Neil
TI  - It's Good to Talk: A Comparison of Using Voice Versus Screen-Based Interactions for Agent-Assisted Tasks
PY  - 2022
AB  - <jats:p> Voice assistants have become hugely popular in the home as domestic and entertainment devices. Recently, there has been a move towards developing them for work settings. For example, <jats:italic>Alexa for Business</jats:italic> and <jats:italic>IBM Watson</jats:italic> <jats:italic>for Business</jats:italic> were designed to improve productivity, by assisting with various tasks, such as scheduling meetings and taking minutes. However, this kind of assistance is largely limited to planning and managing user's work. How might they be developed to do more by way of empowering people at work? Our research is concerned with achieving this by developing an agent with the role of a facilitator that assists users during an ongoing task. Specifically, we were interested in whether the modality in which the agent interacts with users makes a difference: How does a voice versus screen-based agent interaction affect user behavior? We hypothesized that voice would be more immediate and emotive, resulting in more fluid conversations and interactions. Here, we describe a user study that compared the benefits of using voice versus screen-based interactions when interacting with a system incorporating an agent, involving pairs of participants doing an exploratory data analysis task that required them to make sense of a series of data visualizations. The findings from the study show marked differences between the two conditions, with voice resulting in more turn-taking in discussions, questions asked, more interactions with the system and a tendency towards more immediate, faster-paced discussions following agent prompts. We discuss the possible reasons for why talking and being prompted by a voice assistant may be preferable and more effective at mediating human-human conversations and we translate some of the key insights of this research into design implications. </jats:p>
SP  - 1
EP  - 41
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3484221
ER  - 

TY  - NA
AU  - Luo, Yuyu; Tang, Nan; Li, Guoliang; Chai, Chengliang; Li, Wenbo; Qin, Xuedi
TI  - SIGMOD Conference - Synthesizing Natural Language to Visualization (NL2VIS) Benchmarks from NL2SQL Benchmarks
PY  - 2021
AB  - Natural language (NL) is a promising interaction paradigm for data visualization (VIS). However, there are not any NL to VIS (NL2VIS) benchmarks available. Our goal is to provide the first NL2VIS benchmark to enable and push the field of NL2VIS, especially with deep learning technologies. In this paper, we propose a NL2VIS synthesizer (NL2SQL-to-NL2VIS) that synthesizes NL2VIS benchmarks by piggybacking NL2SQL benchmarks. The intuition is based on the semantic connection between SQL queries and VIS queries: SQL queries specify what data is needed and VIS queries additionally need to specify how to visualize. However, different from SQL that has well-defined syntax, VIS languages (e.g., Vega-Lite, VizQL, ggplot2) are syntactically very different. To provide NL2VIS benchmarks that can support many VIS languages, we use a unified intermediate representation, abstract syntax trees (ASTs), for both SQL and VIS queries. We can synthesize multiple VIS trees through adding/deleting nodes to/from an SQL tree. Each VIS tree can then be converted to (any) VIS language. The NL for VIS will be modified based on the NL for SQL to reflect corresponding tree edits. We produce the first NL2VIS benchmark (nvBench), by applying NL2SQL-to-NL2VIS on a popular NL2SQL benchmark Spider, which covers 105 domains, supports seven common types of visualizations, and contains 25,750 (NL, VIS) pairs. Our method reduces the man-hour to 5.7% of developing a NL2VIS benchmark from scratch (or building a NL2VIS benchmark from scratch takes 17.5× man-hours of our method). Extensive human validation, through 23 experts and 312 crowd workers, demonstrates the high-quality of nvBench. In order to verify that nvBench can enable learning-based approaches, we develop a SEQ2VIS model. Our experimental results show that SEQ2VIS works well and significantly outperforms the state-of-the-art methods of the NL2VIS task.
SP  - 1235
EP  - 1247
JF  - Proceedings of the 2021 International Conference on Management of Data
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448016.3457261
ER  - 

TY  - NA
AU  - Krosnick, Rebecca; Oney, Steve
TI  - ParamMacros: Creating UI Automation Leveraging End-User Natural Language Parameterization
PY  - 2022
AB  - Prior work in programming-by-demonstration (PBD) has explored ways to enable end-users to create custom automation without needing to write code. We propose a new end-user specification model &#x2013; asking the end-user to explicitly identify parts of their natural language query that can be generalized. We built a PBD system, ParamMacros, where users first generalize a concrete natural language question &#x2013; identifying parameters and their possible values &#x2013; and then create a demonstration of how to answer the question on the website of interest. ParamMacros then infers a generalized program by using the user-provided parameter values to identify relevant patterns in the website&#x2019;s structure. In a lab study we found that participants were able to meaningfully parameterize natural language queries and felt such a parameterization and demonstration process would be useful for creating custom automation.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833005
ER  - 

TY  - NA
AU  - Katsuragawa, Keiko; Pietroszek, Krzysztof; Wallace, James R.; Lank, Edward
TI  - AVI - Watchpoint: Freehand Pointing with a Smartwatch in a Ubiquitous Display Environment
PY  - 2016
AB  - We describe the design and evaluation of a freehand, smartwatch-based, mid-air pointing and clicking interaction technique, called Watchpoint. Watchpoint enables a user to point at a target on a nearby large display by moving their arm. It also enables target selection through a wrist rotation gesture. We validate the use of Watchpoint by comparing its performance with two existing techniques: Myopoint, which uses a specialized forearm mounted motion sensor, and a camera-based (Vicon) motion capture system. We show that Watchpoint is statistically comparable in speed and error rate to both systems and, in fact, outperforms in terms of error rate for small (high Fitts's ID) targets. Our work demonstrates that a commodity smartwatch can serve as an effective pointing device in ubiquitous display environments.
SP  - 128
EP  - 135
JF  - Proceedings of the International Working Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2909132.2909263
ER  - 

TY  - NA
AU  - Kim, Yea-Seul; Dontcheva, Mira; Adar, Eytan; Hullman, Jessica
TI  - CHI - Vocal Shortcuts for Creative Experts
PY  - 2019
AB  - Vocal shortcuts, short spoken phrases to control interfaces, have the potential to reduce cognitive and physical costs of interactions. They may benefit expert users of creative applications (e.g., designers, illustrators) by helping them maintain creative focus. To aid the design of vocal shortcuts and gather use cases and design guidelines for speech interaction, we interviewed ten creative experts. Based on our findings, we built VoiceCuts, a prototype implementation of vocal shortcuts in the context of an existing creative application. In contrast to other speech interfaces, VoiceCuts targets experts' unique needs by handling short and partial commands and leverages document model and application context to disambiguate user utterances. We report on the viability and limitations of our approach based on feedback from creative experts.
SP  - 332
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300562
ER  - 

TY  - NA
AU  - Tory, Melanie; Setlur, Vidya
TI  - VAST - Do What I Mean, Not What I Say! Design Considerations for Supporting Intent and Context in Analytical Conversation
PY  - 2019
AB  - Natural language can be a useful modality for creating and interacting with visualizations but users often have unrealistic expectations about the intelligence of natural language systems. The gulf between user expectations and system capabilities may lead to a disappointing user experience. So — if we want to engineer a natural language system, what are the requirements around system intelligence? This work takes a retrospective look at how we answered this question in the design of Ask Data, a natural language interaction feature for Tableau. We examine two factors contributing to perceived system intelligence: the system's ability to understand the analytic intent behind an input utterance and the ability to interpret an utterance contextually (i.e. taking into account the current visualization state and recent actions). Our aim was to understand the ways in which a system would need to support these two aspects of intelligence to enable a positive user experience. We first describe a pre-design Wizard of Oz study that offered insight into this question and narrowed the space of designs under consideration. We then reflect on the impact of this study on system development, examining how design implications from the study played out in practice. Our work contributes insights for the design of natural language interaction in visual analytics as well as a reflection on the value of pre-design empirical studies in the development of visual analytic systems.
SP  - 93
EP  - 103
JF  - 2019 IEEE Conference on Visual Analytics Science and Technology (VAST)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vast47406.2019.8986918
ER  - 

TY  - NA
AU  - Fast, Ethan; Chen, Binbin; Mendelsohn, Julia; Bassen, Jonathan; Bernstein, Michael S.
TI  - CHI - Iris: A Conversational Agent for Complex Tasks
PY  - 2018
AB  - Today, most conversational agents are limited to simple tasks supported by standalone commands, such as getting directions or scheduling an appointment. To support more complex tasks, agents must be able to generalize from and combine the commands they already understand. This paper presents a new approach to designing conversational agents inspired by linguistic theory, where agents can execute complex requests interactively by combining commands through nested conversations. We demonstrate this approach in Iris, an agent that can perform open-ended data science tasks such as lexical analysis and predictive modeling. To power Iris, we have created a domain-specific language that transforms Python functions into combinable automata and regulates their combinations through a type system. Running a user study to examine the strengths and limitations of our approach, we find that data scientists completed a modeling task 2.6 times faster with Iris than with Jupyter Notebook.
SP  - 473
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174047
ER  - 

TY  - NA
AU  - Guo, Anhong; Kim, Jeeeun; Chen, Xiang 'Anthony'; Yeh, Tom; Hudson, Scott E.; Mankoff, Jennifer; Bigham, Jeffrey P.
TI  - CHI - Facade: Auto-generating Tactile Interfaces to Appliances
PY  - 2017
AB  - Common appliances have shifted toward flat interface panels, making them inaccessible to blind people. Although blind people can label appliances with Braille stickers, doing so generally requires sighted assistance to identify the original functions and apply the labels. We introduce Facade - a crowdsourced fabrication pipeline to help blind people independently make physical interfaces accessible by adding a 3D printed augmentation of tactile buttons overlaying the original panel. Facade users capture a photo of the appliance with a readily available fiducial marker (a dollar bill) for recovering size information. This image is sent to multiple crowd workers, who work in parallel to quickly label and describe elements of the interface. Facade then generates a 3D model for a layer of tactile and pressable buttons that fits over the original controls. Finally, a home 3D printer or commercial service fabricates the layer, which is then aligned and attached to the interface by the blind person. We demonstrate the viability of Facade in a study with 11 blind participants.
SP  - 5826
EP  - 5838
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025845
ER  - 

TY  - JOUR
AU  - Li, Lei; Fu, Hongbo; Tai, Chiew-Lan
TI  - Fast Sketch Segmentation and Labeling With Deep Learning
PY  - 2018
AB  - We present a simple and efficient method based on deep learning to automatically decompose sketched objects into semantically valid parts. We train a deep neural network to transfer existing segmentations and labelings from three-dimensional (3-D) models to freehand sketches without requiring numerous well-annotated sketches as training data. The network takes the binary image of a sketched object as input and produces a corresponding segmentation map with per-pixel labelings as output. A subsequent postprocess procedure with multilabel graph cuts further refines the segmentation and labeling result. We validate our proposed method on two sketch datasets. Experiments show that our method outperforms the state-of-the-art method in terms of segmentation and labeling accuracy and is significantly faster, enabling further integration in interactive drawing systems. We demonstrate the efficiency of our method in a sketch-based modeling application that automatically transforms input sketches into 3-D models by part assembly.
SP  - 38
EP  - 51
JF  - IEEE computer graphics and applications
VL  - 39
IS  - 2
PB  - 
DO  - 10.1109/mcg.2018.2884192
ER  - 

TY  - JOUR
AU  - Wei, Ziyun; Trummer, Immanuel; Anderson, Connor
TI  - Robust voice querying with MUVE: optimally visualizing results of phonetically similar queries
PY  - 2021
AB  - <jats:p>Recently proposed voice query interfaces translate voice input into SQL queries. Unreliable speech recognition on top of the intrinsic challenges of text-to-SQL translation makes it hard to reliably interpret user input. We present MUVE (Multiplots for Voice quEries), a system for robust voice querying. MUVE reduces the impact of ambiguous voice queries by filling the screen with multiplots, capturing results of phonetically similar queries. It maps voice input to a probability distribution over query candidates, executes a selected subset of queries, and visualizes their results in a multiplot.</jats:p> <jats:p>Our goal is to maximize probability to show the correct query result. Also, we want to optimize the visualization (e.g., by coloring a subset of likely results) in order to minimize expected time until users find the correct result. Via a user study, we validate a simple cost model estimating the latter overhead. The resulting optimization problem is NP-hard. We propose an exhaustive algorithm, based on integer programming, as well as a greedy heuristic. As shown in a corresponding user study, MUVE enables users to identify accurate results faster, compared to prior work.</jats:p>
SP  - 2397
EP  - 2409
JF  - Proceedings of the VLDB Endowment
VL  - 14
IS  - 11
PB  - 
DO  - 10.14778/3476249.3476289
ER  - 

TY  - NA
AU  - Bavishi, Rohan; Laddad, Shadaj; Yoshida, Hiroaki; Prasad, Mukul R.; Sen, Koushik
TI  - VizSmith: Automated Visualization Synthesis by Mining Data-Science Notebooks
PY  - 2021
AB  - Visualizations are widely used to communicate findings and make data-driven decisions. Unfortunately creating bespoke and reproducible visualizations requires the use of procedural tools such as matplotlib. These tools present a steep learning curve as their documentation often lacks sufficient usage examples to help beginners get started or accomplish a specific task. Forums such as StackOverflow have long helped developers search for code online and adapt it for their use. However, developers still have to sift through search results and understand the code before adapting it for their use.We built a tool called VizSmith which enables code reuse for visualizations by mining visualization code from Kaggle notebooks and creating a database of 7176 reusable Python functions. Given a dataset, columns to visualize and a text query from the user, VizSmith searches this database for appropriate functions, runs them and displays the generated visualizations to the user. At the core of VizSmith is a novel metamorphic testing based approach to automatically assess the reusability of functions, which improves end-to-end synthesis performance by 10% and cuts the number of execution failures by 50%.
SP  - NA
EP  - NA
JF  - 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ase51524.2021.9678696
ER  - 

TY  - CHAP
AU  - Ojuroye, Olivia; Torah, Russel; Beeby, Stephen; Wilde, Adriana
TI  - Smart Textiles for Smart Home Control and Enriching Future Wireless Sensor Network Data
PY  - 2016
AB  - The increasing number of objects within homes connected to the Cloud is not going to recede. Our growing acceptance of automated appliances and items connected in wireless sensor networks (WSN) is gradually making our homes smart. This occurrence is a reflection of the technological advancement of societies around the world. We predict that the future applications of WSN will incorporate smart textiles. These will appear in smart homes, as well as in commercial spaces, in automobile vehicles, in personal or business-owned clothing, and even toys. As the electronics become available to industry, smart textiles could be embedded with electronics capable of receiving and transmitting data packets. The implications are that soft furnishings or any surfaces with a textile have the potential capability of connecting to the Cloud. Considering future applications of smart textiles, whether for personal or commercial usage, we can predict data contents that would be stored in a WSN and discuss how to ensure safety and network stability.
SP  - 159
EP  - 183
JF  - Sensors for Everyday Life
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-47319-2_9
ER  - 

TY  - NA
AU  - Chen, Xiuli; Acharya, Aditya; Oulasvirta, Antti; Howes, Andrew
TI  - CHI - An Adaptive Model of Gaze-based Selection
PY  - 2021
AB  - Gaze-based selection has received significant academic attention over a number of years. While advances have been made, it is possible that further progress could be made if there were a deeper understanding of the adaptive nature of the mechanisms that guide eye movement and vision. Control of eye movement typically results in a sequence of movements (saccades) and fixations followed by a ‘dwell’ at a target and a selection. To shed light on how these sequences are planned, this paper presents a computational model of the control of eye movements in gaze-based selection. We formulate the model as an optimal sequential planning problem bounded by the limits of the human visual and motor systems and use reinforcement learning to approximate optimal solutions. The model accurately replicates earlier results on the effects of target size and distance and captures a number of other aspects of performance. The model can be used to predict number of fixations and duration required to make a gaze-based selection. The future development of the model is discussed.
SP  - 288
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445177
ER  - 

TY  - JOUR
AU  - Stokes, Chase; Setlur, Vidya; Cogley, Bridget; Satyanarayan, Arvind; Hearst, Marti A
TI  - Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts.
PY  - 2022
AB  - While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts containing text with varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways referring to statistics or relational comparisons than text describing elemental or encoded components. Finally, we find different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. We compile these results into four chart design guidelines and discuss future implications for the combination of text and charts.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209383
ER  - 

TY  - NA
AU  - Kovacs, Robert; Seufert, Anna; Wall, Ludwig; Chen, Hsiang-Ting; Meinel, Florian; Müller, Willi; You, Sijing; Brehm, Maximilian; Striebel, Jonathan; Kommana, Yannis; Popiak, Alexander; Bläsius, Thomas; Baudisch, Patrick
TI  - CHI - TrussFab: Fabricating Sturdy Large-Scale Structures on Desktop 3D Printers
PY  - 2017
AB  - We present TrussFab, an integrated end-to-end system that allows users to fabricate large scale structures that are sturdy enough to carry human weight. TrussFab achieves the large scale by complementing 3D print with plastic bottles. It does not use these bottles as "bricks" though, but as beams that form structurally sound node-link structures, also known as trusses, allowing it to handle the forces resulting from scale and load. TrussFab embodies the required engineering knowledge, allowing non-engineers to design such structures and to validate their design using integrated structural analysis. We have used TrussFab to design and fabricate tables and chairs, a 2.5 m long bridge strong enough to carry a human, a functional boat that seats two, and a 5 m diameter dome.
SP  - 2606
EP  - 2616
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3026016
ER  - 

TY  - JOUR
AU  - Pruszko, Laura; Coutrix, Céline; Laurillau, Yann; Piranda, Benoît; Bourgeois, Julien
TI  - Molecular HCI
PY  - 2021
AB  - <jats:p>Shape-changing User Interfaces attract growing interest in Human-Computer Interaction. Modular robotics offer a great opportunity for their implementation. However, the current theoretical and technical advances of modular robotics are fragmented and little centered on the user. To unify existing work and center future research on the user, we perform a systematic literature review enabling us to build a unifying space for the design of modular shape-changing user interfaces.Our aim is to bridge the gap between HCI and robotics. We relate properties of different domains and identify inconsistencies to structure the design space. Towards this aim, we conduct a thorough cross-disciplinary survey to propose: 1) a set of design properties at the scale of the interface (macro-scale) and at the scale of the modules (micro-scale) and 2) the impact of these properties on each other. This paper can be used to describe and compare existing modular shape-changing UIs and generate new design ideas by building upon knowledge from robotics and HCI.</jats:p>
SP  - 1
EP  - 33
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3461733
ER  - 

TY  - CHAP
AU  - Liu, Chuanyi; Liu, Chang; Mao, Hao; Su, Wei
TI  - ICIC (3) - Tilt-Scrolling: A Comparative Study of Scrolling Techniques for Mobile Devices
PY  - 2019
AB  - Scrolling is a frequent operation on a mobile screen. The current scrolling method for a mobile device is both time-consuming and fatigue-prone, especially for one-handed interaction mode. In this study, we systematically evaluated six tilt-based scrolling techniques together with the base line through a repeated measure experiment participated by twenty-one subjects. The experimental results revealed that tilt-based scrolling techniques were more suitable for one-handed interaction with higher interaction performance and were less fatigue-prone than the base line. Tilt scrolling techniques with adaptive reference points outperformed those with the horizontal reference point in terms of scrolling speed and accuracy. The Step control function had higher interaction efficiency than the other two functions but was more error-prone. The results of our study have some implications for the design of scrolling techniques on mobile screens.
SP  - 189
EP  - 200
JF  - Intelligent Computing Methodologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-26766-7_18
ER  - 

TY  - NA
AU  - Law, Po-Ming; Lo, Leo Yu-Ho; Endert, Alex; Stasko, John; Qu, Huamin
TI  - Causal Perception in Question-Answering Systems
PY  - 2021
AB  - Root cause analysis is a common data analysis task. While question-answering systems enable people to easily articulate a why question (e.g., why students in Massachusetts have high ACT Math scores on average) and obtain an answer, these systems often produce questionable causal claims. To investigate how such claims might mislead users, we conducted two crowdsourced experiments to study the impact of showing different information on user perceptions of a question-answering system. We found that in a system that occasionally provided unreasonable responses, showing a scatterplot increased the plausibility of unreasonable causal claims. Also, simply warning participants that correlation is not causation seemed to lead participants to accept reasonable causal claims more cautiously. We observed a strong tendency among participants to associate correlation with causation. Yet, the warning appeared to reduce the tendency. Grounded in the findings, we propose ways to reduce the illusion of causality when using question-answering systems.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445444
ER  - 

TY  - BOOK
AU  - Pietroszek, Krzysztof
TI  - Encyclopedia of Computer Graphics and Games - Raycasting in Virtual Reality
PY  - 2018
AB  - NA
SP  - 1
EP  - 3
JF  - Encyclopedia of Computer Graphics and Games
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-08234-9_180-1
ER  - 

TY  - NA
AU  - Li, Yuntao; Chen, Bei; Liu, Qian; Gao, Yan; Lou, Jian-Guang; Zhang, Yan; Zhang, Dongmei
TI  - EMNLP (1) - “What Do You Mean by That?” A Parser-Independent Interactive Approach for Enhancing Text-to-SQL
PY  - 2020
AB  - In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users' natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.
SP  - 6913
EP  - 6922
JF  - Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2020.emnlp-main.561
ER  - 

TY  - NA
AU  - Xiong, Cindy; Setlur, Vidya; Bach, Benjamin; Lin, Kylie; Koh, Eunyee; Franconeri, Steven
TI  - Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways
PY  - 2021
AB  - Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart, and coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the 'right' takeaway.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Richter, Christoph
TI  - Digital sketching. Aesthetic practices and technological entanglements
PY  - 2020
AB  - This article explores the ways aesthetic practices and emerging digital technologies are entangled in the practices of industrial designers. Despite the increased interest in the socio-materiality ...
SP  - 334
EP  - 349
JF  - Ethnography and Education
VL  - 15
IS  - 3
PB  - 
DO  - 10.1080/17457823.2020.1724170
ER  - 

TY  - NA
AU  - Kurosawa, Hiroki; Sakamoto, Daisuke; Ono, Tetsuo
TI  - MobileHCI - MyoTilt: a target selection method for smartwatches using the tilting operation and electromyography
PY  - 2018
AB  - We present the MyoTilt target selection method for smartwatches, which employs a combination of a tilt operation and electromyography (EMG). First, a user tilts his/her arm to indicate the direction of cursor movement on the smartwatch; then s/he applies forces on the arm. EMG senses the force and moves the cursor to the direction where the user is tilting his/her arm to manipulate the cursor. In this way, the user can simply manipulate the cursor on the smartwatch with minimal effort, by tiling the arm and applying force to it. We conducted an experiment to investigate its performance and to understand its usability. Result showed that participants selected small targets with an accuracy greater than 93.89%. In addition, performance significantly improved compared to previous tilting operation methods. Likewise, its accuracy was stable as targets became smaller, indicating that the method is unaffected by the "fat finger problem".
SP  - 43
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229457
ER  - 

TY  - NA
AU  - McClelland, John C.; Teather, Robert J.; Girouard, Audrey
TI  - SUI - Haptobend: shape-changing passive haptic feedback in virtual reality
PY  - 2017
AB  - We present HaptoBend, a novel shape-changing input device providing passive haptic feedback (PHF) for a wide spectrum of objects in virtual reality (VR). Past research in VR shows that PHF increases presence and improves user task performance. However, providing PHF for multiple objects usually requires complex, immobile systems, or multiple props. HaptoBend addresses this problem by allowing users to bend the device into 2D plane-like shapes and multi-surface 3D shapes. We believe HaptoBend's physical approximations of virtual objects can provide realistic haptic feedback through research demonstrating the dominance of human vision over other senses in VR. To test the effectiveness of HaptoBend in matching 2D planar and 3D multi-surface shapes, we conducted an experiment modeled after gesture elicitation studies with 20 participants. High goodness and ease scores show shape-changing passive haptic devices, like HaptoBend, are an effective approach to generalized haptics. Further analysis supports the use of physical approximations for realistic haptic feedback.
SP  - 82
EP  - 90
JF  - Proceedings of the 5th Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131277.3132179
ER  - 

TY  - JOUR
AU  - Zhao, Liangliang; Zhao, Jingdong; Liu, Hong
TI  - Solving the Inverse Kinematics Problem of Multiple Redundant Manipulators with Collision Avoidance in Dynamic Environments
PY  - 2021
AB  - This article presents an approach for collision-free kinematics of multiple redundant manipulators in complex environments. The approach describes a representation of task space and joint limit constraints for redundant manipulators and handles collision-free constraints by micromanipulator dynamic model and velocity obstacles. A new algorithm based on Newton-based and first-order techniques is proposed to generate collision-free inverse kinematics solutions. The present approach is applied in simulation for the redundant manipulators in a various working environments with dynamic obstacles. The physical experiments using a Baxter robot in a various working environments with dynamic obstacles are also performed. The results demonstrate the effectiveness of the proposed approach compared with existing methods regarding working environment and computational cost.
SP  - 1
EP  - 18
JF  - Journal of Intelligent & Robotic Systems
VL  - 101
IS  - 2
PB  - 
DO  - 10.1007/s10846-020-01279-w
ER  - 

TY  - JOUR
AU  - Pjanic, Petar; Grundhöfer, Anselm
TI  - Paxel: A Generic Framework to Superimpose High-Frequency Print Patterns Using Projected Light
PY  - 2018
AB  - In this paper, we propose Paxel , a generic framework for modeling the interaction between a projector and a high-frequency pattern surface . Using this framework, we present two different application setups [cf. Fig. 1(a) ]: a novel color-changing effect , created with a single projected image and only when the projection surface is changed from a pattern surface to a uniform white surface . The observed effect relies on the spatially different reflectance properties of these two surfaces. Using this approach, one can alter color proprieties of the projected image such as hue or chroma. Furthermore, for a specific color range, defined by a full color-changing sub-gamut , one can embed two completely different images, within a single static projection, from which either one will be revealed depending on the surface. The second application allows the creation of color images using a single channel projector. For this application, we present a full color projection created using a 365-nm ultraviolet projector in combination with fluorescent pigments [cf. Fig. 1(b) ], enabling new display possibilities, such as projection through participating media, e.g., fog, while hiding the scattering of the projection light outside of the visible spectrum. Both presented approaches create effects that might be striking to the observer, making this framework useful for art exhibitions, advertisements, entertainment, and visual cryptography. Finally, in Section VI , we provide an in-depth analysis of the reproducible colors based on input parameters, used in the presented algorithm, such as pattern layout, dot size of the pattern, and the number of the clusters formed by k-means algorithm ( Section IV-B ).
SP  - 3541
EP  - 3555
JF  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
VL  - 27
IS  - 7
PB  - 
DO  - 10.1109/tip.2018.2824120
ER  - 

TY  - NA
AU  - Rencis, Edgars
TI  - Towards a natural language-based interface for querying hospital data
PY  - 2018
AB  - There is a growing necessity in various domains for non-programmers to be able to retrieve information gathered about the operation of the organization and stored in its databases. This information could hugely benefit the decision making process of the managers of the institution, but it is not often exploited due to the complexity of extracting the information from the existing data. In this paper we sketch a way how that information could be managed by the domain experts themselves by the means of a natural language-based query language that works upon data stored in the ontology. Our experiments show that the proposed approach is indeed easy-to-use by our target end-users - managers and physicians of hospitals -, because lacking technical details the query language is very intuitive to use.
SP  - 25
EP  - 28
JF  - Proceedings of 2018 International Conference on Big Data Technologies - ICBDT '18
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3226116.3226133
ER  - 

TY  - JOUR
AU  - Sidenmark, Ludwig; Parent, Mark; Wu, Chi-Hao; Chan, Joannes; Glueck, Michael; Wigdor, Daniel; Grossman, Tovi; Giordano, Marcello
TI  - Weighted Pointer: Error-aware Gaze-based Interaction through Fallback Modalities.
PY  - 2022
AB  - Gaze-based interaction is a fast and ergonomic type of hands-free interaction that is often used with augmented and virtual reality when pointing at targets. Such interaction, however, can be cumbersome whenever user, tracking, or environmental factors cause eye tracking errors. Recent research has suggested that fallback modalities could be leveraged to ensure stable interaction irrespective of the current level of eye tracking error. This work thus presents Weighted Pointer interaction, a collection of error-aware pointing techniques that determine whether pointing should be performed by gaze, a fallback modality, or a combination of the two, depending on the level of eye tracking error that is present. These techniques enable users to accurately point at targets when eye tracking is accurate and inaccurate. A virtual reality target selection study demonstrated that Weighted Pointer techniques were more performant and preferred over techniques that required the use of manual modality switching.
SP  - 3585
EP  - 3595
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203096
ER  - 

TY  - NA
AU  - Huang, Da-Yuan; Guo, Ruizhen; Gong, Jun; Wang, Jingxian; Graham, John; Yang, De-Nian; Yang, Xing-Dong
TI  - UIST - RetroShape: Leveraging Rear-Surface Shape Displays for 2.5D Interaction on Smartwatches
PY  - 2017
AB  - The small screen size of a smartwatch limits user experience when watching or interacting with media. We propose a supplementary tactile feedback system to enhance the user experience with a method unique to the smartwatch form factor. Our system has a deformable surface on the back of the watch face, allowing the visual scene on screen to extend into 2.5D physical space. This allows the user to watch and feel virtual objects, such as experiencing a ball bouncing against the wrist. We devised two controlled experiments to analyze the influence of tactile display resolution on the illusion of virtual object presence. Our first study revealed that on average, a taxel can render virtual objects between 70% and 138% of its own size without shattering the illusion. From the second study, we found visual and haptic feedback can be separated by 4.5mm to 16.2mm for the tested taxels. Based on the results, we developed a prototype (called RetroShape) with 4×4 10mm taxels using micro servo motors, and demonstrated its unique capability through a set of tactile-enhanced games and videos. A preliminary user evaluation showed that participants welcome RetroShape as a useful addition to existing smartwatch output.
SP  - 539
EP  - 551
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126610
ER  - 

TY  - NA
AU  - Siddhpuria, Shaishav; Malacria, Sylvain; Nancel, Mathieu; Lank, Edward
TI  - CHI - Pointing at a Distance with Everyday Smart Devices
PY  - 2018
AB  - Large displays are becoming commonplace at work, at home, or in public areas. However, interaction at a distance -- anything greater than arms-length -- remains cumbersome, restricts simultaneous use, and requires specific hardware augmentations of the display: touch layers, cameras, or dedicated input devices. Yet a rapidly increasing number of people carry smartphones and smartwatches, devices with rich input capabilities that can easily be used as input devices to control interactive systems. We contribute (1) the results of a survey on possession and use of smart devices, and (2) the results of a controlled experiment comparing seven distal pointing techniques on phone or watch, one- and two-handed, and using different input channels and mappings. Our results favor using a smartphone as a trackpad, but also explore performance tradeoffs that can inform the choice and design of distal pointing techniques for different contexts of use.
SP  - 173
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173747
ER  - 

TY  - CHAP
AU  - Dragicevic, Pierre; Jansen, Yvonne; Vande Moere, Andrew
TI  - Data Physicalization
PY  - 2021
AB  - Data physicalization is a rich and vast research area that studies the use of physical artifacts to convey data. It overlaps with a number of research areas including information visualization, scientific visualization, visual analytics, tangible user interfaces, shape-changing interfaces, personal fabrication interfaces, as well as graphic design, architecture, and art. This chapter surveys academic work on data physicalization up to 2018 and also provides a broad overview of nonacademic work. It discusses how data physicalization has been used for analytical purposes, communication and education, accessibility, self-reflection and self-expression, and finally for enjoyment and meaning. It also discusses enabling technologies, reviews empirical studies, and surveys models and theories of data physicalization.
SP  - 1
EP  - 51
JF  - Handbook of Human Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-27648-9_94-1
ER  - 

TY  - NA
AU  - Kettner, Romina; Bader, Patrick; Kosch, Thomas; Schneegass, Stefan; Schmidt, Albrecht
TI  - MobileHCI - Towards pressure-based feedback for non-stressful tactile notifications
PY  - 2017
AB  - Smartphones, wearables, and other mobile devices often use tactile feedback for notifying users. This feedback type proved to be beneficial since it does not occupy the visual or auditory channel. However, it still can be distracting in other situations such as when users are already stressed. To investigate tactile feedback patterns which do not increase the user's stress level, we developed two wrist-worn prototypes capable of providing tactile feedback (i.e., vibrotactile and pressure-based feedback). Further, we conducted a user-study with 14 participants comparing both feedback types. The results suggest that vibrotactile feedback increases the user's stress level more, compared to pressure-based feedback particularly applied when the user currently has a low stress level. Consequently, we present implications for designing notifications for mobile and wearable devices.
SP  - 89
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3122132
ER  - 

TY  - JOUR
AU  - CrovariPietro, ; PidòSara, ; PinoliPietro, ; BernasconiAnna, ; CanakogluArif, ; GarzottoFranca, ; CeriStefano, 
TI  - GeCoAgent: A Conversational Agent for Empowering Genomic Data Extraction and Analysis
PY  - 2021
AB  - With the availability of reliable and low-cost DNA sequencing, human genomics is relevant to a growing number of end-users, including biologists and clinicians. Typical interactions require applyin...
SP  - 1
EP  - 29
JF  - ACM Transactions on Computing for Healthcare
VL  - 3
IS  - 1
PB  - 
DO  - 10.1145/3464383
ER  - 

TY  - CHAP
AU  - Cardoso, Bruno; Cohn, Neil; Truyen, Frederik; Brosens, Koenraad
TI  - INTERACT (3) - Explore Data, Enjoy Yourself - KUbism, A Playful Approach to Data Exploration
PY  - 2021
AB  - The increasing relevance of information in today’s world has grown the challenge of accessible data exploration. Most existing approaches are primarily utilitarian, where the motivation for exploration remains mostly extrinsic, correlated to the value of the outcome. In light of our natural drives for exploration, we argue that data exploration can be made more rewarding and enjoyable via interfaces that leverage the intrinsic motivations of users. To that end, we present KUbism, a prototype emphasizing the hedonic value of data exploration through a playful interface built with elements of Minecraft gameplay. Implemented as a Terasology module, KUbism invites users to explore data in voxel worlds populated with data blocks. We conducted a user study (n = 41) to validate our approach in both hedonic and utilitarian dimensions. Our results demonstrate that KUbism allows effective data exploration while arousing curiosity and joy, thereby reinforcing the extrinsic value of data exploration with intrinsic rewards.
SP  - 43
EP  - 64
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85613-7_4
ER  - 

TY  - NA
AU  - Siddiqui, Tarique; Wang, Zesheng; Luh, Paul; Karahalios, Karrie; Parameswaran, Aditya
TI  - ShapeSearch: A Flexible and Efficient System for Shape-based Exploration of Trendlines
PY  - 2018
AB  - Identifying trendline visualizations with desired patterns is a common and fundamental data exploration task. Existing visual analytics tools offer limited flexibility and expressiveness for such tasks, especially when the pattern of interest is under-specified and approximate, and do not scale well when the pattern searching needs are ad-hoc, as is often the case. We propose ShapeSearch, an efficient and flexible pattern-searching tool, that enables the search for desired patterns via multiple mechanisms: sketch, natural-language, and visual regular expressions. We develop a novel shape querying algebra, with a minimal set of primitives and operators that can express a large number of ShapeSearch queries, and design a natural-language and regex-based parser to automatically parse and translate user queries to the algebra representation. To execute these queries within interactive response times, ShapeSearch uses a fast shape algebra-based execution engine with query-aware optimizations, and perceptually-aware scoring methodologies. We present a thorough evaluation of the system, including a general-purpose user study, a case study involving genomic data analysis, as well as performance experiments, comparing against state-of-the-art time series shape matching approaches---that together demonstrate the usability and scalability of ShapeSearch.
SP  - NA
EP  - NA
JF  - arXiv: Databases
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Qi, Anran; Gryaditskaya, Yulia; Xiang, Tao; Song, Yi-Zhe
TI  - One Sketch for All: One-Shot Personalized Sketch Segmentation.
PY  - 2022
AB  - We present the first one-shot personalized sketch segmentation method. We aim to segment all sketches belonging to the same category provisioned with a single sketch with a given part annotation while (i) preserving the parts semantics embedded in the exemplar, and (ii) being robust to input style and abstraction. We refer to this scenario as personalized. With that, we importantly enable a much-desired personalization capability for downstream fine-grained sketch analysis tasks. To train a robust segmentation module, we deform the exemplar sketch to each of the available sketches of the same category. Our method generalizes to sketches not observed during training. Our central contribution is a sketch-specific hierarchical deformation network. Given a multi-level sketch-strokes encoding obtained via a graph convolutional network, our method estimates rigid-body transformation from the target to the exemplar, on the upper level. Finer deformation from the exemplar to the globally warped target sketch is further obtained through stroke-wise deformations, on the lower-level. Both levels of deformation are guided by mean squared distances between the keypoints learned without supervision, ensuring that the stroke semantics are preserved. We evaluate our method against the state-of-the-art segmentation and perceptual grouping baselines re-purposed for the one-shot setting and against two few-shot 3D shape segmentation methods. We show that our method outperforms all the alternatives by more than 10% on average. Ablation studies further demonstrate that our method is robust to personalization: changes in input part semantics and style differences.
SP  - 2673
EP  - 2682
JF  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
VL  - 31
IS  - NA
PB  - 
DO  - 10.1109/tip.2022.3160076
ER  - 

TY  - JOUR
AU  - Cannard, Cedric; Brandmeyer, Tracy; Wahbeh, Helané; Delorme, Arnaud
TI  - Self-health monitoring and wearable neurotechnologies.
PY  - 2020
AB  - NA
SP  - 207
EP  - 232
JF  - Handbook of clinical neurology
VL  - 168
IS  - NA
PB  - 
DO  - 10.1016/b978-0-444-63934-9.00016-0
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Tappa, Jordan L; Zheng, Yi; Forman, Jack; Leong, Joanne; Koenig, Sven; Ishii, Hiroshi
TI  - (Dis)Appearables: A Concept and Method for Actuated Tangible UIs to Appear and Disappear based on Stages
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501906
ER  - 

TY  - CHAP
AU  - Wenzel, Matthias; Meinel, Christoph
TI  - Prototyper: A Virtual Remote Prototyping Space
PY  - 2019
AB  - Collaborative virtual environment groupware is—despite of notable research efforts over several years—still not common in users’ workplaces. Reasons are high costs of engaging in collaboration next to the loss of information and capabilities that people enjoy in co-located settings. Low-fidelity prototyping is a way for co-located teams to create joint understandings and to gather feedback in early design stages. When it comes to geographically dispersed teams, dedicated tools are required that help to fulfill tasks at hand, while enabling team members as much as possible to apply working modes known from co-located settings. We present a web browser-based collaborative virtual environment that supports the joint real time creation of three-dimensional low-fidelity prototypes. It is a cross-platform application that runs on a multitude of hardware devices. While focusing on usage with virtual reality hardware, users may also freely participate when there are only traditional input and output devices available. The system provides enhanced awareness through visual remote user embodiment combined with spatial audio communication.
SP  - 171
EP  - 184
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-28960-7_11
ER  - 

TY  - JOUR
AU  - Li, Zhi; Zhao, Maozheng; Wang, Yifan; Rashidian, Sina; Baig, Furqan; Liu, Rui; Liu, Wanyu; Beaudouin-Lafon, Michel; Ellison, Brooke; Wang, Fusheng; Bi, Xiaojun
TI  - BayesGaze: A Bayesian Approach to Eye-Gaze Based Target Selection
PY  - 2021
AB  - Selecting targets accurately and quickly with eye-gaze input remains an open research question. In this paper, we introduce BayesGaze, a Bayesian approach of determining the selected target given an eye-gaze trajectory. This approach views each sampling point in an eye-gaze trajectory as a signal for selecting a target. It then uses the Bayes' theorem to calculate the posterior probability of selecting a target given a sampling point, and accumulates the posterior probabilities weighted by sampling interval to determine the selected target. The selection results are fed back to update the prior distribution of targets, which is modeled by a categorical distribution. Our investigation shows that BayesGaze improves target selection accuracy and speed over a dwell-based selection method, and the Center of Gravity Mapping (CM) method. Our research shows that both accumulating posterior and incorporating the prior are effective in improving the performance of eye-gaze based target selection.
SP  - 231
EP  - 240
JF  - Proceedings. Graphics Interface (Conference)
VL  - 2021
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Lu, Zhicong; Fan, Mingming; Wang, Yun; Zhao, Jian; Annett, Michelle; Wigdor, Daniel
TI  - InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming
PY  - 2018
AB  - Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget-NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.
SP  - 277
EP  - 287
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2864887
ER  - 

TY  - NA
AU  - Sun, Ke; Wang, Yuntao; Yu, Chun; Yan, Yukang; Wen, Hongyi; Shi, Yuanchun
TI  - CHI - Float: One-Handed and Touch-Free Target Selection on Smartwatches
PY  - 2017
AB  - Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the "fat finger" problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (>98.9%) and walking (>71.5%) contexts.
SP  - 692
EP  - 704
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3026027
ER  - 

TY  - NA
AU  - Kovacs, Robert
TI  - UIST (Adjunct Volume) - Human-Scale Personal Fabrication
PY  - 2021
AB  - Building large structures from small elements, creating life-size animated creatures, or making contraptions that we can ride on have almost certainly been everyone's childhood dreams. However, researchers and practitioners of personal fabrication have been mainly focusing on creating objects that fit into a human palm, also called “hand-size” objects. The reason behind this is not only because of the size limitation of consumer-grade fabrication machinery but also because of the very long printing time and high material costs of large-scale prototypes. To overcome these limitations, I combine 3D printed hubs and ready-made objects, such as plastic bottles, as well as welding steel rods into a certain type of node-link structures called “trusses”. However, the actual challenge behind my work is not only about achieving the size, but ensuring that the resulting large structures withstand the orders of magnitude larger forces than their hand-sized counterparts. Designing such structures requires substantial engineering know-how that users of personal fabrication equipment, such as makers, typically do not possess. By providing the lacking engineering know-how, my three end-to-end software systems TrussFab, TrussFormer, and Trusscillator enable non-experts to build such human-scale static, kinetic, and human-powered dynamic devices, such as pavilions, large-scale animatronic devices, and playground equipment. These systems achieve this by allowing users to focus on high-level design aspects, such as shape, animation, or riding experience, while abstracting away the underlying technicalities of forces, inertia, eigenfrequencies, etc. To help building the designs, the software generates the connector pieces and assembly instructions. With this body of work, I aim at democratizing engineering that enables individuals to design and fabricate large-scale objects and mechanisms that involve human-scale forces.
SP  - 162
EP  - 165
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3477588
ER  - 

TY  - JOUR
AU  - Subramonyam, Hariharan; Adar, Eytan
TI  - SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays
PY  - 2018
AB  - Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). ‘Queries’ to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.
SP  - 597
EP  - 607
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865231
ER  - 

TY  - NA
AU  - Zuckerman, Oren; Sadka, Ofir; Gissin, Ron; Erel, Hadas
TI  - CHI - TUI as Social Entity: a Study of Joint-actuation and Turn-taking-actuation in Actuated-interfaces
PY  - 2021
AB  - We present an actuated-interface that is not only a tangible interface but also an autonomous object, designed as an independent entity that takes a similar role to the user’s role in an anagram word game. We highlight two leading interaction paradigms: Turn-taking-actuation and Joint-actuation, and evaluate both in a qualitative interaction study with the autonomous actuated-interface. Our findings reveal that all participants perceived the interaction as a social experience. The different interaction paradigms led to different interpretations: Turn-taking-actuation was interpreted as a competitive experience, while Joint-actuation was interpreted as a collaborative experience. The interaction paradigms also influenced the intensity of emotions and perception of control, with Joint-actuation leading to more intense emotions and higher sensitivity to control in the interaction. To conclude, our findings show that it is possible to design an actuated-interface that users perceive both as a tangible interface and as a social entity with its own intent.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445468
ER  - 

TY  - NA
AU  - Vasey, Lauren; Grossman, Tovi; Kerrick, Heather; Nagy, Danil
TI  - SIGGRAPH Talks - The hive: a human and robot collaborative building process
PY  - 2016
AB  - The Hive Pavilion exhibited at Autodesk University (2015) investigated whether untrained workers and industrial robots could work collaboratively together towards the common goal of fabricating and assembling an architectural scale structure through the utilization of computational design, wearables, and interconnected devices. Though many narratives promote the idea that robots will displace humans in the workforce, Hive set out to challenge that assumption and propose an alternative view of human and robot collaboration; where the dexterity and cognitive abilities of humans can augment the precision and repeatability of robots to enable previously impossible tasks.
SP  - 83
EP  - NA
JF  - ACM SIGGRAPH 2016 Talks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2897839.2927404
ER  - 

TY  - JOUR
AU  - Wu, Aoyu; Wang, Yun; Zhou, Mengyu; He, Xinyi; Zhang, Haidong; Qu, Huamin; Zhang, Dongmei
TI  - MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation.
PY  - 2021
AB  - We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2021.3114826
ER  - 

TY  - NA
AU  - Figueiredo, Pedro; Fonseca, Manuel J.
TI  - ICMI - EyeLinks: A Gaze-Only Click Alternative for Heterogeneous Clickables
PY  - 2018
AB  - In this paper, we introduce a novel gaze-only interaction technique called EyeLinks, which was designed i) to support various types of discrete clickables (e.g. textual links, buttons, images, tabs, etc.); ii) to be easy to learn and use; iii) to mitigate the inaccuracy of affordable eye trackers. Our technique uses a two-step fixation approach: first, we assign numeric identifiers to clickables in the region where users gaze at and second, users select the desired clickable by performing a fixation on the corresponding confirm button, displayed in a sidebar. This two-step selection enables users to freely explore Web pages, avoids the Midas touch problem and improves accuracy. We evaluated our approach by comparing it against the mouse and another gaze-only technique (Actigaze). The results showed no statistically significant difference between EyeLinks and Actigaze, but users considered EyeLinks easier to learn and use than Actigaze and it was also the most preferred. Of the three, the mouse was the most accurate and efficient technique.
SP  - 307
EP  - 314
JF  - Proceedings of the 20th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242969.3243021
ER  - 

TY  - JOUR
AU  - Cui, Weiwei; Zhang, Xiaoyu; Wang, Yun; Huang, He; Chen, Bei; Fang, Lei; Zhang, Haidong; Lou, Jian-Guan; Zhang, Dongmei
TI  - Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements
PY  - 2019
AB  - Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.
SP  - 906
EP  - 916
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2019.2934785
ER  - 

TY  - NA
AU  - Nguyen, Duc; Vrzakova, Hana; Bednarik, Roman
TI  - UbiComp Adjunct - WTP: web-tracking plugin for real-time automatic AOI annotations
PY  - 2016
AB  - Studies of visual attention during web-browsing belong to long standing applications of eye-tracking. Before researchers can analyze characteristics of visual attention, they often have to manually annotate each Area of Interest (AOI) at the web page. The longer, more complex, and dynamic the web pages are, the more time the researcher spends on creating the bounding box around each AOI. To speed up this process, we designed a browser extension that automatically extracts coordinates of visible web elements at the page as the user interacts with the page, along with the mouse activity. In our case study, the WTP automatically annotated all web elements, mouse clicks and mouse movements, and we linked the annotations with the stream of eye-tracking data. As a demonstration, we report on distribution of fixations over specific web elements during daily browsing activities. To our knowledge, this is a first tool that supports real-time automatic annotations of dynamic web-content, with a particular fit for eye-tracking research.
SP  - 1696
EP  - 1705
JF  - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2968219.2968338
ER  - 

TY  - NA
AU  - Ishii, Akira; Shizuki, Buntarou; Tanaka, Jiro
TI  - CHI Extended Abstracts - Evaluation of Callout Design for Ultra-small Touch Screen Devices
PY  - 2016
AB  - Ultra-small touch screen devices tend to suffer from occlusion or the fat finger problem owing to their limited input area. Callout design, a design principle that involves the placement of a callout in a non-occluded area in order to display the occluded area, could eliminate occlusion. However, callout designs for ultra-small touch screen devices have not yet been explored. In this study, we conducted an experiment to examine eight callout designs for ultra-small touch screen devices. The results show that the selection speed was higher when the content of the callout was changed continuously, the error rate decreased when a pointer was displayed to indicate the touched position within the callout, and the workload decreased when the content was changed continuously. Further, the score that subjectively evaluates the performance decreased when the position of the callout was fixed.
SP  - 2511
EP  - 2518
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892434
ER  - 

TY  - JOUR
AU  - Walny, Jagoda; Frisson, Christian; West, Mieka; Kosminsky, Doris; Knudsen, Søren; Carpendale, Sheelagh; Willett, Wesley
TI  - Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff
PY  - 2019
AB  - Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations . Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.
SP  - 12
EP  - 22
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2019.2934538
ER  - 

TY  - NA
AU  - Law, Po-Ming; Das, Subhajit; Basole, Rahul C.
TI  - CHI - Comparing Apples and Oranges: Taxonomy and Design of Pairwise Comparisons within Tabular Data
PY  - 2019
AB  - Asking pairwise comparison questions is common. Yet, we often find ourselves comparing apples and oranges --- the two entities of interest are not readily comparable. To understand how technologies can extend our capabilities to conduct pairwise comparisons during data analysis, we analyzed pairwise comparison questions collected from crowd workers and propose a taxonomy of pairwise comparisons. We demonstrate how the taxonomy can be adopted by incorporating pairwise comparison capabilities into Duo, a spreadsheet application that supports comparing two groups of records in a data table. Duo decomposes a pairwise comparison question into rules and showcases sloppy rules, a query technique for specifying pairwise comparisons. We conducted a user study comparing sloppy rules and natural language. The findings suggest that for easier pairwise comparison tasks, the two techniques are comparable in efficiency and preference and that for more difficult pairwise comparison tasks, sloppy rules allow faster specification and are more preferable.
SP  - 179
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300409
ER  - 

TY  - NA
AU  - Xia, Haijun
TI  - UIST - Crosspower: Bridging Graphics and Linguistics
PY  - 2020
AB  - Despite the ubiquity of direct manipulation techniques available in computer-aided design applications, creating digital content remains a tedious and indirect task. This is because applications require users to perform numerous low-level editing operations rather than allowing them to directly indicate high-level design goals. Yet, the creation of graphic content, such as videos, animations, and presentations often begins with a description of design goals in natural language, such as screenplays, scripts, outlines. Therefore, there is an opportunity for language-oriented authoring, i.e., leveraging the information found in the structure of a language to facilitate the creation of graphic content. We present a systematic exploration of the identification, graphic description, and interaction with various linguistic structures to assist in the creation of visual content. The prototype system, Crosspower, and its proposed interaction techniques, enables content creators to indicate and customize their desired visual content in a flexible and direct manner.
SP  - 722
EP  - 734
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415845
ER  - 

TY  - NA
AU  - Ehlenz, Matthias; Roepke, Rene; Schroeder, Ulrik
TI  - PerDis - Towards Multi-touch Learning Applications in Collaborative Education
PY  - 2018
AB  - Educational contexts evolve with the increasing impact of multi-touch technologies on everyday life. When bringing multi-touch technologies into a classroom, high-quality content applications must be developed to add value to students' learning. This contribution presents a framework for multi-touch learning games which incorporates learning analytics components and tangibles to improve collaborative learning experiences.
SP  - 35
EP  - NA
JF  - Proceedings of the 7th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3205873.3210709
ER  - 

TY  - NA
AU  - Bott, Jared
TI  - The WOZ Recognizer: A Tool For Understanding User Perceptions of Sketch-Based Interfaces
PY  - 2016
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Tsao, Chih-An; Liao, Fang-Ying; Yong, Seraphina; Yeh, Tom; Chen, Bing-Yu
TI  - UIST - Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture
PY  - 2021
AB  - Design tools and research regarding laser-cut architectures have been widely explored in the past decade. However, such discussion has mostly revolved around technical and structural design questions instead of another essential element of laser-cut models — assembly — a process that relies heavily on components’ visual affordance, therefore less accessible to blind or low vision (BLV) people. To narrow the gap in this area, we co-designed with 7 BLV people to examine their assembly experience with different laser-cut architectures. From their feedback, we proposed several design heuristics and guidelines for Daedalus, a generative design tool that can produce tactile aids for laser-cut assembly given a few high-level manual inputs. We validate the proposed aids in a user study with 8 new BLV participants. Our results revealed that BLV users can manage laser-cut assembly more efficiently with Daedalus. Going forth from this design iteration, we discuss implications for future research on accessible laser-cut assembly.
SP  - 344
EP  - 358
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474754
ER  - 

TY  - NA
AU  - Yan, Xinghui; Madier, Katy; Park, Sun Young; Newman, Mark W.
TI  - Conference on Designing Interactive Systems (Companion Volume) - Towards Low-burden In-situ Self-reporting: A Design Space Exploration
PY  - 2019
AB  - In-situ self-reporting is a widely-used data collection method which offers many benefits in the clinical, psychological and social research fields. However, high capture burden issues have surfaced as in-situ self-reporting expands and diversifies in various studies. Thus, we draw attention to the design space of low-burden in-situ self-reporting. In this work-in-progress, drawing on literature analysis, we explore the design space by analyzing and mapping context-dependent attention resources, current interactive methods, and associated design requirements. In the case study, we further demonstrate the use of the design space to derive low-burden experience sampling solutions. Overall, we stress that reducing in-situ self-report burdens requires research attention, and the design space can help designers make sensible design decisions.
SP  - 337
EP  - 346
JF  - Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301019.3323905
ER  - 

TY  - CHAP
AU  - Liu, Chuanyi; Wu, Ningning; Zhang, Jiali; Su, Wei
TI  - INTERACT (3) - Tilt Space: A Systematic Exploration of Mobile Tilt for Design Purpose
PY  - 2019
AB  - Various application scenarios of a smartphone sometimes require one-handed and/or eyes-free interaction. Tilt-based interfaces have the potential to meet these requirements. Taking multiple application scenarios into account, we conducted an experiment to systematically investigate human ability in controlling tilt input of a mobile phone. Three visual feedback levels, i.e., fully visual feedback (FV), partially visual feedback (PV), and no visual feedback (NV), were investigated. Under the NV condition, the participants performed a task using an eyes-free method. The results revealed that trials were performed the fastest but were the most error-prone under the NV condition. The participants could easily distinguish 4 tilt orientation levels (TOLs) and 2 tilt magnitude levels (TMLs) or 8 TOLs and 2 TMLs under the NV condition with tolerance of an error rate 10% or 15%, respectively. We also found out that the participants’ abilities to control tilt input were related to tilt orientation directions. The results have some implications for non-visual interface designs using tilt as primitive input.
SP  - 497
EP  - 517
JF  - Human-Computer Interaction – INTERACT 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29387-1_29
ER  - 

TY  - CHAP
AU  - Aksan, Emre; Hilliges, Otmar
TI  - Generative Ink: Data-Driven Computational Models for Digital Ink
PY  - 2021
AB  - Digital ink promises to combine the flexibility of pen and paper interaction and the versatility of digital devices. Computational models of digital ink often focus on recognition of the content by following discriminative techniques such as classification, albeit at the cost of ignoring or losing personalized style. In this chapter, we propose augmenting the digital ink framework via generative modeling to achieve a holistic understanding of the ink content. Our focus particularly lies in developing novel generative models to gain fine-grained control by preserving user style. To this end, we model the inking process and learn to create ink samples similar to users. We first present how digital handwriting can be disentangled into style and content to implement editable digital ink, enabling content synthesis and editing. Second, we address a more complex setup of free-form sketching and propose a novel approach for modeling stroke-based data efficiently. Generative ink promises novel functionalities, leading to compelling applications to enhance the inking experience for users in an interactive and collaborative manner.
SP  - 417
EP  - 461
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_13
ER  - 

TY  - NA
AU  - Fennedy, Katherine; Lee, Hyowon; Ko, Insuk; Tan, Yi Ren; Zhang, Dongliang; Li, Chunxiao
TI  - Augmenting User-Maintained Interaction Through Mode Locking and Reversing
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Electronic Workshops in Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.14236/ewic/hci2018.9
ER  - 

TY  - CHAP
AU  - Ribeiro, Dalai dos Santos; de Sousa, Alysson Gomes; de Almeida, Rodrigo B.; Furtado, Pedro Henrique Thompson; Lopes, Hélio; Barbosa, Simone Diniz Junqueira
TI  - HCI (4) - Exploring Ontology-Based Information Through the Progressive Disclosure of Visual Answers to Related Queries
PY  - 2020
AB  - Web search has become the predominant method for people to fulfill their information needs. Although widespread, the traditional model for search result pages is only satisfactory if the user knows quite precisely how to phrase their query to match their intended information. We propose a new model for search page results, which goes beyond providing a navigable list of visualization search results, by implicitly generating related queries to expand the search space and progressively disclosing the corresponding results.
SP  - 104
EP  - 124
JF  - Human Interface and the Management of Information. Designing Information
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50020-7_7
ER  - 

TY  - JOUR
AU  - Jiang, Qi; Sun, Guodao; Dong, Yue; Liang, Ronghua
TI  - DT2VIS: A Focus+Context Answer Generation System to Facilitate Visual Exploration of Tabular Data
PY  - 2021
AB  - The visual analysis dialog system utilizing natural language interface is emerging as a promising data analysis tool. However, previous work mostly focused on accurately understanding the query intention of a user but not on generating answers and inducing explorations. A focus+context answer generation approach, which allows users to obtain insight and contextual information simultaneously, is proposed in this work to address the incomplete user query (i.e., input query cannot reflect all possible intentions of the user). A query recommendation algorithm, which applies the historical query information of a user to recommend a follow-up query, is also designed and implemented to provide an in-depth exploration. These ideas are implemented in a system called DT2VIS. Specific cases of utilizing DT2VIS are also provided to analyze data. Finally, the results show that DT2VIS could help users easily and efficiently reach their analysis goals in a comparative study.
SP  - 45
EP  - 56
JF  - IEEE computer graphics and applications
VL  - 41
IS  - 5
PB  - 
DO  - 10.1109/mcg.2021.3097326
ER  - 

TY  - NA
AU  - Sindhwani, Shyamli; Lutteroth, Christof; Weber, Gerald
TI  - CHI - ReType: Quick Text Editing with Keyboard and Gaze
PY  - 2019
AB  - When a user needs to reposition the cursor during text editing, this is often done using the mouse. For experienced typists especially, the switch between keyboard and mouse can slow down the keyboard editing workflow considerably. To address this we propose ReType, a new gaze-assisted positioning technique combining keyboard with gaze input based on a new 'patching' metaphor. ReType allows users to perform some common editing operations while keeping their hands on the keyboard. We present the result of two studies. A free-use study indicated that ReType enhances the user experience of text editing. ReType was liked by many participants, regardless of their typing skills. A comparative user study showed that ReType is able to match or even beat the speed of mouse-based interaction for small text edits. We conclude that the gaze-augmented user interface can make common interactions more fluent, especially for professional keyboard users.
SP  - 203
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300433
ER  - 

TY  - JOUR
AU  - Maior, Horia A.; Wilson, Max L.; Sharples, Sarah
TI  - Workload Alerts—Using Physiological Measures of Mental Workload to Provide Feedback During Tasks
PY  - 2018
AB  - Feedback is valuable for allowing us to improve on tasks. While retrospective feedback can help us improve for next time, feedback ‘in action’ can allow us to improve the outcome of on-going tasks. In this article, we use data from functional Near InfraRed Spectroscopy to provide participants with feedback about their mental workload levels during high-workload tasks. We evaluate the impact of this feedback on task performance and perceived task performance, in comparison to industry standard mid-task self-assessments, and explore participants’ perceptions of this feedback. In line with previous work, we confirm that deploying self-reporting methods affect both perceived and actual performance. Conversely, we conclude that our objective concurrent feedback correlated more closely with task demand, supported reflection in action, and did not negatively affect performance. Future work, however, should focus on the design of this feedback and the potential behaviour changes that will result.
SP  - 9
EP  - 30
JF  - ACM Transactions on Computer-Human Interaction
VL  - 25
IS  - 2
PB  - 
DO  - 10.1145/3173380
ER  - 

TY  - NA
AU  - Cui, Weiwei; Zhang, Xiaoyu; Wang, Yun; Huang, He; Chen, Bei; Fang, Lei; Zhang, Haidong; Lou, Jian-Guan; Zhang, Dongmei
TI  - Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements
PY  - 2019
AB  - Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Zheng, Clement; Kakehi, Yasuaki; Yeh, Tom; Yi-Luen, Ellen; Gross, Mark D.; Leithinger, Daniel
TI  - ShapeBots: Shape-changing Swarm Robots
PY  - 2019
AB  - We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.
SP  - 493
EP  - 505
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347911
ER  - 

TY  - NA
AU  - Baldauf, Matthias; Adegeye, Florence; Alt, Florian; Harms, Johannes
TI  - PerDis - Your browser is the controller: advanced web-based smartphone remote controls for public screens
PY  - 2016
AB  - In recent years, a lot of research focused on using smartphones as input devices for distant screens, in many cases by means of native applications. At the same time, prior work often ignored the downsides of native applications for practical usage, such as the need for download and the required installation process. This hampers the spontaneous use of an interactive service. To address the aforementioned drawbacks, we introduce ATREUS, an open-source framework which enables creating and provisioning manifold mobile remote controls as plain web applications. We describe the basic architecture of ATREUS and present four functional remote controls realized using the framework. Two sophisticated controls, the Mini Video and the Smart Lens approach, have been previously implemented as native applications only. Furthermore, we report on lessons learned for realizing web-based remote controls during functional tests and finally present the results of an informal user study.
SP  - 175
EP  - 181
JF  - Proceedings of the 5th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2914920.2915026
ER  - 

TY  - NA
AU  - Chen, Yilan; Meng, Wenlong; Xin, Shiqing; Fu, Hongbo
TI  - SIGGRAPH ASIA (Posters) - Smartsweep: context-aware modeling on a single image
PY  - 2017
AB  - Creating complementary objects for existing items is common in personal fabrication. However, most supportive systems focus more on 3D inputs [Weichel et al. 2014] and use less 2D information, even though images are easier to obtain than 3D objects in practice. Besides, even with a reference 3D model, it still requires tedious manual work to create a complementary object.
SP  - 39
EP  - NA
JF  - SIGGRAPH Asia 2017 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3145690.3145736
ER  - 

TY  - NA
AU  - Suh, Jihoon; Kim, Wooshik; Bianchi, Andrea
TI  - Tangible and Embedded Interaction - Button+: Supporting User and Context Aware Interaction through Shape-Changing Interfaces
PY  - 2017
AB  - Shape-changing interfaces are an emerging topic in HCI research: they merge the simplicity of tangible interfaces with the expressiveness of dynamic physical affordances. However, while prior work largely focused on technical aspects and proposed classifications of shape-changing interfaces based on the physical properties of the actuators and the user's levels of control, this work presents a classification of shape-changing interfaces based on the context and identity of the users. After introducing a new prototype for a shape-changing pushbutton, we conducted a series of workshop studies with designers and engineers to explore the design space and potential applications for this interface. We used the result of our workshops to propose a generalized taxonomy of interactions, and built two applications that reflect the proposed model. The paper concludes by highlighting future possible research directions for context and user aware shape-changing interfaces.
SP  - 261
EP  - 268
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3024980
ER  - 

TY  - NA
AU  - Fu, Siwei; Xiong, Kai; Ge, Xiaodong; Tang, Siliang; Chen, Wei; Wu, Yingcai
TI  - Quda: Natural Language Queries for Visual Data Analytics
PY  - 2020
AB  - Visualization-oriented natural language interfaces (V-NLIs) have been explored and developed in recent years. One challenge faced by V-NLIs is in the formation of effective design decisions that usually requires a deep understanding of user queries. Learning-based approaches have shown potential in V-NLIs and reached state-of-the-art performance in various NLP tasks. However, because of the lack of sufficient training samples that cater to visual data analytics, cutting-edge techniques have rarely been employed to facilitate the development of V-NLIs. We present a new dataset, called Quda, to help V-NLIs understand free-form natural language. Our dataset contains 14;035 diverse user queries annotated with 10 low-level analytic tasks that assist in the deployment of state-of-the-art techniques for parsing complex human language. We achieve this goal by first gathering seed queries with data analysts who are target users of V-NLIs. Then we employ extensive crowd force for paraphrase generation and validation. We demonstrate the usefulness of Quda in building V-NLIs by creating a prototype that makes effective design decisions for free-form user queries. We also show that Quda can be beneficial for a wide range of applications in the visualization community by analyzing the design tasks described in academic publications.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cherek, Christian; Brocker, Anke; Voelker, Simon; Borchers, Jan
TI  - CHI - Tangible Awareness: How Tangibles on Tabletops Influence Awareness of Each Other's Actions
PY  - 2018
AB  - Tangibles on multitouch tabletops increase speed, accuracy, and eyes-free operability for individual users, and verbal and behavioral social interaction among multiple users around smaller tables with a shared focus of attention. Modern multitouch tables, however, provide sizes and resolutions that let groups work alongside each other in separate workspaces. But how aware do these users remain of each other's actions, and what impact can tangibles have on their awareness? In our study, groups of 2--4 users around the table played an individual game grabbing their attention as primary task, while they also had to occasionally become aware of other players'actions and react as secondary task. We found that players were significantly more aware of other players'actions using tangibles than those using pure multitouch interaction, indicated by faster reaction times. This effect was especially strong with more players. We close with qualitative user feedback and design recommendations. We found that players were significantly more aware of other players'actions using tangibles than those using pure multitouch interaction, indicated by faster reaction times. This effect was especially strong with more players. We close with qualitative user feedback and design recommendations.
SP  - 298
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173872
ER  - 

TY  - JOUR
AU  - Pengnate, Supavich; Lehmberg, Derek; Tangpong, Chanchai
TI  - Top management's communication in economic crisis and the firm's subsequent performance: sentiment analysis approach
PY  - 2020
AB  - In economic crisis, where tensions create anxiety and test the emotions of the firms' shareholders, communication from top management is very crucial as it provides the reflection of the managers' interpretation of the firms' situation and potential strategies. The goal of this paper is to investigate the relationships between sentiment, as an aspect of emotions extracted from the letters to shareholders, managerial discretion and the firms' subsequent performance and performance trajectory during crisis.,A sentiment analysis was conducted to extract the sentiment from the letters to shareholders, which were collected from firms in two countries with different levels of managerial discretion (US vs. Japan). Hypotheses were developed and tested using a series of regression analysis.,The primary findings indicate that (1) managerial sentiment identified in letters to shareholders can potentially be related to the firm's subsequent performance in the economic crisis, and (2) managerial discretion moderates the relationship between managerial sentiment and subsequent firm performance.,When the managerial discretion is high, firms' shareholders can use the sentiment in top management communications to gauge whether the firms' situation would be improving in the near future.,This study expands the current research on sentiment analysis and firm performance to the context of economic crisis by suggesting that managerial sentiment can be substantially provoked as firms are facing with stressful economic conditions. The study also highlights the moderating role of managerial discretion on the firms' subsequent performance.
SP  - 187
EP  - 205
JF  - Corporate Communications: An International Journal
VL  - 25
IS  - 2
PB  - 
DO  - 10.1108/ccij-07-2019-0094
ER  - 

TY  - NA
AU  - Mazumder, Sahisnu; Liu, Bing; Wang, Shuai; Esmaeilpour, Sepideh
TI  - Building an Application Independent Natural Language Interface.
PY  - 2019
AB  - Traditional approaches to building natural language (NL) interfaces typically use a semantic parser to parse the user command and convert it to a logical form, which is then translated to an executable action in an application. However, it is still challenging for a semantic parser to correctly parse natural language. For a different domain, the parser may need to be retrained or tuned, and a new translator also needs to be written to convert the logical forms to executable actions. In this work, we propose a novel and application independent approach to building NL interfaces that does not need a semantic parser or a translator. It is based on natural language to natural language matching and learning, where the representation of each action and each user command are both in natural language. To perform a user intended action, the system only needs to match the user command with the correct action representation, and then execute the corresponding action. The system also interactively learns new (paraphrased) commands for actions to expand the action representations over time. Our experimental results show the effectiveness of the proposed approach.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Ramakrishnan, Aparna; Madan, Modiksha; Prabhakar, Gowdham; Deshmukh, Sachin; Biswas, Pradipta
TI  - Eye Gaze Controlled Head-up Display
PY  - 2020
AB  - A myriad of infotainment systems has found its applications in the automobile industry with the burgeoning demand for user comfort and interaction. However, operating such infotainment systems entertain secondary tasks to be carried out at the expense of the primary task of driving. This can increase the cognitive load on the driver and has the potential to keep road safety at stake. This paper presents an intelligent interactive head-up display (HUD) on the windscreen of the driver that does not require them to take eyes off the road while undertaking secondary tasks like playing music, operating vent controls, watching navigation map, and so on. The interactive HUD allows the user to navigate and make selections using eye gaze. The HUD also incorporates provisions to estimate driver’s cognitive load and distraction level. User studies show that the system improves driving performance in terms of mean deviation from lane in an ISO 26022 lane changing task compared to touch screen system and participants can undertake ISO 9241 pointing tasks in less than 2 s on average inside a car.
SP  - 471
EP  - 479
JF  - ICT Analysis and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-15-0630-7_46
ER  - 

TY  - JOUR
AU  - Chun, Jaemin; Dey, Anind K.; Lee, Kyung-Taek; Kim, Seungjun
TI  - A qualitative study of smartwatch usage and its usability
PY  - 2018
AB  - NA
SP  - 186
EP  - 199
JF  - Human Factors and Ergonomics in Manufacturing & Service Industries
VL  - 28
IS  - 4
PB  - 
DO  - 10.1002/hfm.20733
ER  - 

TY  - JOUR
AU  - Hoque, E.; Kavehzadeh, P.; Masry, A.
TI  - Chart Question Answering: State of the Art and Future Directions
PY  - 2022
AB  - Information visualizations such as bar charts and line charts are very common for analyzing data and discovering critical insights. Often people analyze charts to answer questions that they have in mind. Answering such questions can be challenging as they often require a significant amount of perceptual and cognitive effort. Chart Question Answering (CQA) systems typically take a chart and a natural language question as input and automatically generate the answer to facilitate visual data analysis. Over the last few years, there has been a growing body of literature on the task of CQA. In this survey, we systematically review the current state-of-the-art research focusing on the problem of chart question answering. We provide a taxonomy by identifying several important dimensions of the problem domain including possible inputs and outputs of the task and discuss the advantages and limitations of proposed solutions. We then summarize various evaluation techniques used in the surveyed papers. Finally, we outline the open challenges and future research opportunities related to chart question answering.
SP  - 555
EP  - 572
JF  - Computer Graphics Forum
VL  - 41
IS  - 3
PB  - 
DO  - 10.1111/cgf.14573
ER  - 

TY  - JOUR
AU  - Steuerlein, Benedict; Mayer, Sven
TI  - Conductive Fiducial Tangibles for Everyone
PY  - 2022
AB  - <jats:p>While tangibles enrich the interaction with touchscreens, with projected capacitive screens being mainstream, the recognition possibilities of tangibles are nearly lost. Deep learning approaches to improve the recognition of conductive triangles require collecting huge amounts of data and domain-specific knowledge for hyperparameter tuning. To overcome this drawback, we present a toolkit that allows everyone to train a deep learning tangible recognizer based on simulated data. Our toolkit uses a pre-trained Generative Adversarial Network to simulate the imprint of fiducial tangibles, which we then use to train a deployable recognizer based on our pre-defined neuronal network architecture. Our evaluation shows that our approach can recognize fiducial tangibles such as AprilTags with an average accuracy of 99.3% and an average rotation error of only 4.9°. Thus, our toolkit is a plug-and-play solution requiring no domain knowledge and no data collection but allows designers to use deep learning approaches in their design process.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546718
ER  - 

TY  - NA
AU  - Setlur, Vidya; E, Battersby Sarah; Wong, Tracy
TI  - GeoSneakPique: Visual Autocompletion for Geospatial Queries
PY  - 2021
AB  - How many crimes occurred in the city center? And exactly which part of town is the 'city center'? While location is at the heart of many data questions, geographic location can be difficult to specify in natural language (NL) queries. This is especially true when working with fuzzy cognitive regions or regions that may be defined based on data distributions instead of absolute administrative location (e.g., state, country). GeoSneakPique presents a novel method for using a mapping widget to support the NL query process, allowing users to specify location via direct manipulation with data-driven guidance on spatial distributions to help select the area of interest. Users receive feedback to help them evaluate and refine their spatial selection interactively and can save spatial definitions for re-use in subsequent queries. We conduct a qualitative evaluation of the GeoSneakPique that indicates the usefulness of the interface as well as opportunities for better supporting geospatial workflows in visual analysis tasks employing cognitive regions.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Deshpande, Manoj
TI  - TOWARDS CO-BUILD: AN ARCHITECTURE MACHINE FOR CO-CREATIVE FORM-MAKING
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Huang, Yijiang; Zhang, Juyong; Hu, Xin; Song, Guoxian; Liu, Zhong-Yuan; Yu, Lei; Liu, Ligang
TI  - FrameFab: robotic fabrication of frame shapes
PY  - 2016
AB  - Frame shapes, which are made of struts, have been widely used in many fields, such as art, sculpture, architecture, and geometric modeling, etc. An interest in robotic fabrication of frame shapes via spatial thermoplastic extrusion has been increasingly growing in recent years. In this paper, we present a novel algorithm to generate a feasible fabrication sequence for general frame shapes. To solve this non-trivial combinatorial problem, we develop a divide-and-conquer strategy that first decomposes the input frame shape into stable layers via a constrained sparse optimization model. Then we search a feasible sequence for each layer via a local optimization method together with a backtracking strategy. The generated sequence guarantees that the already-printed part is in a stable equilibrium state at all stages of fabrication, and that the 3D printing extrusion head does not collide with the printed part during the fabrication. Our algorithm has been validated by a built prototype robotic fabrication system made by a 6-axis KUKA robotic arm with a customized extrusion head. Experimental results demonstrate the feasibility and applicability of our algorithm.
SP  - 224
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 35
IS  - 6
PB  - 
DO  - 10.1145/2980179.2982401
ER  - 

TY  - NA
AU  - Appert, Caroline; Pietriga, Emmanuel; Bartenlian, Éléonore; González, Rafael Morales
TI  - AVI - Custom-made tangible interfaces with touchtokens
PY  - 2018
AB  - TouchTokens were introduced recently as a means to design low-cost tangible interfaces. The technique consists in recognizing multi-touch patterns associated with specific tokens, and works on any touch-sensitive surface, with passive tokens that can be made out of any material. TouchTokens have so far been limited to a few basic geometrical shapes only, which puts a significant practical limit to how tailored token sets can be. In this article, we introduce TouchTokenBuilder and TouchTokenTracker that, taken together, aim at facilitating the development of tailor-made tangible interfaces. TouchTokenBuilder is an application that assists interface designers in creating token sets using a simple direct-manipulation interface. TouchTokenTracker is a library that enables tracking the tokens' full geometry. We report on experiments with those tools, showing the strengths and limitations of tangible interfaces with passive tokens.
SP  - 15
EP  - NA
JF  - Proceedings of the 2018 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206505.3206509
ER  - 

TY  - NA
AU  - Schrapel, Maximilian; Herzog, Florian; Ryll, Steffen; Rohs, Michael
TI  - CHI Extended Abstracts - Watch my Painting: The Back of the Hand as a Drawing Space for Smartwatches
PY  - 2020
AB  - Smartwatches can be used independently from smartphones, but input tasks like messaging are cumbersome due to the small display size. Parts of the display are hidden during interaction, which can lead to incorrect input. For simplicity, instead of general text input a small set of answer options are often provided, but these are limited and impersonal. In contrast, free-form drawings can answer messages in a very personal way, but are difficult to produce on small displays. To enable precise drawing input on smartwatches we present a magnetic stylus that is tracked on the back of the hand. In an evaluation of several algorithms we show that 3D position estimation with a 7.5x20mm magnet reaches a worst-case 6% relative position error on the back of the hand. Furthermore, the results of a user study are presented, which show that in the case of drawing applications the presented technique is faster and more precise than direct finger input.
SP  - 1
EP  - 10
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383040
ER  - 

TY  - NA
AU  - Sarmah, Ritam Jyoti; Ding, Yunpeng; Wang, Di; Lee, Cheuk Yin Phipson; Li, Toby Jia-Jun; Chen, Xiang 'Anthony'
TI  - Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications
PY  - 2020
AB  - Supporting voice commands in applications presents significant benefits to users. However, adding such support to existing GUI-based web apps is effort-consuming with a high learning barrier, as shown in our formative study, due to the lack of unified support for creating multimodal interfaces. We present Geno---a developer tool for adding the voice input modality to existing web apps without requiring significant NLP expertise. Geno provides a high-level workflow for developers to specify functionalities to be supported by voice (intents), create language models for detecting intents and the relevant information (parameters) from user utterances, and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app. Geno further supports multimodal references to GUI context in voice commands (e.g. "move this [event] to next week" while pointing at an event with the cursor). In a study, developers with little NLP expertise were able to add multimodal voice command support for two existing web apps using Geno.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Kegel, Friedrich; Hemmert, Fabian
TI  - Mensch &amp; Computer - SandExplorer: Exploring Geospatial Data, Grain by Grain
PY  - 2019
AB  - In this paper, we introduce SandExplorer: a device to physically explore geospatial data by creating a 2.5-dimensional, topographic sand sculpture. This sculpture is created semi-manually, by moving a handle over a sandbox, which serves as a plotting canvas. The sandbox' shape represents the geospatial region the data is about. The amount of sand flowing from the system's sand reservoir through the handle is regulated by a mapping algorithm, depending on the underlying data. With SandExplorer, users can playfully create a sand-based data 'physicalization'.
SP  - 457
EP  - 461
JF  - Proceedings of Mensch und Computer 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340764.3344439
ER  - 

TY  - NA
AU  - Zhang, Cheng; Yang, Junrui; Southern, Caleb; Starner, Thad; Abowd, Gregory D.
TI  - ISWC - WatchOut: extending interactions on a smartwatch with inertial sensing
PY  - 2016
AB  - Current interactions on a smartwatch are generally limited to a tiny touchscreen, physical buttons or knobs, and speech. We present WatchOut, a suite of interaction techniques that includes three families of tap and swipe gestures which extend input modalities to the watch's case, bezel, and band. We describe the implementation of a user-independent gesture recognition pipeline based on data from the watch's embedded inertial sensors. In a study with 12 participants using both a round- and square-screen watch, the average gesture classification accuracies ranged from 88.7% to 99.4%. We demonstrate applications of this richer interaction capability, and discuss the strengths, limitations, and future potential for this work.
SP  - 136
EP  - 143
JF  - Proceedings of the 2016 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2971763.2971775
ER  - 

TY  - CONF
AU  - Kumar, Abhinav; Aurisano, Jillian; Di Eugenio, Barbara; Johnson, Andrew
TI  - FLAIRS Conference - Intelligent Assistant for Exploring Data Visualizations.
PY  - 2020
AB  - NA
SP  - 538
EP  - 543
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jung, Daekyoung; Kim, Wonjae; Song, Hyunjoo; Hwang, Jeong-in; Lee, Bongshin; Kim, Bohyoung; Seo, Jinwook
TI  - CHI - ChartSense: Interactive Data Extraction from Chart Images
PY  - 2017
AB  - Charts are commonly used to present data in digital documents such as web pages, research papers, or presentation slides. When the underlying data is not available, it is necessary to extract the data from a chart image to utilize the data for further analysis or improve the chart for more accurate perception. In this paper, we present ChartSense, an interactive chart data extraction system. ChartSense first determines the chart type of a given chart image using a deep learning based classifier, and then extracts underlying data from the chart image using semi-automatic, interactive extraction algorithms optimized for each chart type. To evaluate chart type classification accuracy, we compared ChartSense with ReVision, a system with the state-of-the-art chart type classifier. We found that ChartSense was more accurate than ReVision. In addition, to evaluate data extraction performance, we conducted a user study, comparing ChartSense with WebPlotDigitizer, one of the most effective chart data extraction tools among publicly accessible ones. Our results showed that ChartSense was better than WebPlotDigitizer in terms of task completion time, error rate, and subjective preference.
SP  - 6706
EP  - 6717
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025957
ER  - 

TY  - NA
AU  - Sankar, Aditya; Seitz, Steve
TI  - UIST - Interactive Room Capture on 3D-Aware Mobile Devices
PY  - 2017
AB  - We propose a novel interactive system to simplify the process of indoor 3D CAD room modeling. Traditional room modeling methods require users to measure room and furniture dimensions, and manually select models that match the scene from large catalogs. Users then employ a mouse and keyboard interface to construct walls and place the objects in their appropriate locations. In contrast, our system leverages the sensing capabilities of a 3D aware mobile device, recent advances in object recognition, and a novel augmented reality user interface, to capture indoor 3D room models in-situ. With a few taps, a user can mark the surface of an object, take a photo, and the system retrieves and places a matching 3D model into the scene, from a large online database. User studies indicate that this modality is significantly quicker, more accurate, and requires less effort than traditional desktop tools.
SP  - 415
EP  - 426
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126629
ER  - 

TY  - NA
AU  - Makiguchi, Motohiro; Sakamoto, Daisuke; Takada, Hideaki; Honda, Kengo; Ono, Tetsuo
TI  - UIST - Interactive 360-Degree Glasses-Free Tabletop 3D Display
PY  - 2019
AB  - We present an interactive 360-degree tabletop display system for collaborative work around a round table. Users are able to see 3D objects on the tabletop display anywhere around the table without 3D glasses. The system uses a visual perceptual mechanism for smooth motion parallax in the horizontal direction with fewer projectors than previous works. A 360-degree camera mounted above the table and image recognition software detects users' positions around the table and the heights of their faces (eyes) as they move around the table in real-time. Those mechanics help display correct vertical and horizontal direction motion parallax for different users simultaneously. Our system also has a user interaction function with a tablet device that manipulates 3D objects displayed on the table. These functions support collaborative work and communication between users. We implemented a prototype system and demonstrated the collaborative features of the 360-degree tabletop display system.
SP  - 625
EP  - 637
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347948
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Fukumoto, Masaaki; Siio, Itiro
TI  - UIST - Ohmic-Sticker: Force-to-Motion Type Input Device that Extends Capacitive Touch Surface
PY  - 2019
AB  - We propose "Ohmic-Sticker'', a novel force-to-motion type input device to extend capacitive touch surfaces. It realizes various types of force-sensitive inputs by simply attaching on to commercial touchpads or touchscreens. A simple force-sensitive-resistor (FSR)-based structure enables thin (less than 2 mm) form factors and battery-less operation. The applied force vector is detected as the leakage current from the corresponding touch surface electrodes by using Ohmic-Touch technology. Ohmic-Sticker can be used for adding force-sensitive interactions to touch surfaces, such as analog push buttons, TrackPoint-like devices, and full 6 DoF controllers for navigating virtual spaces. In this paper, we report a series of investigations on the design requirements of Ohmic-Sticker and some prototypes.We also evaluate the performance of Ohmic-Sticker as a pointing device.
SP  - 1021
EP  - 1030
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - 36
IS  - 3
PB  - 
DO  - 10.1145/3332165.3347903
ER  - 

TY  - JOUR
AU  - Fletcher-Watson, Ben; Crompton, Catherine J.; Hutchison, Mary; Lu, Hongjin
TI  - Strategies for enhancing success in digital tablet use by older adults: A pilot study
PY  - 2016
AB  - Purpose Building on recent digital literacy initiatives, three strategies were identified for exploration, relating to successful use of digital tablets by older adults who lacked previous experience. The questions under investigation were: What are the implications of one-to-one support for self-efficacy and promoting attendance at digital literacy sessions? Could free tablets assist in overcoming economic and social barriers to participation? By what means could age-related physical problems with digital technology be combated? Method Between June and July 2016, eight older adults (five men and three women aged 70 to 87) attended a six-week course in digital literacy, supported by four volunteer tutors. Tablets were loaned to participants who did not own a device. A variety of accessories, such as styluses and hard cases, were discussed and shared. Results and discussion Weekly attendance was almost 100%, with no participants withdrawing from the course, and only occasional absences due to other commitments. The group displayed a wide spectrum of ability, from complete beginners to regular computer users, although all participants initially rated themselves as unconfident in relation to tablets. By the end of the course, self-efficacy had increased from 44% to 71%. Accessories proved popular with a number of participants, particularly for those with fine motor control issues. Conclusion Teams of tutors may promote attendance in comparison to lone-tutor-led classes or peer learning scenarios, and have the potential to impact positively on perceptions of self-efficacy. For older adults, particularly those from areas of multiple deprivations, access to free or borrowed devices may be key to participation, although a lack of access to home broadband can reduce ability to practice new skills between sessions. Modern capacitive screens offer reduced haptic feedback by comparison with the resistive screens on older mobile devices, leading some older adults to require further accessories in order to engage successfully with tablet computers.
SP  - 162
EP  - 170
JF  - Gerontechnology
VL  - 15
IS  - 3
PB  - 
DO  - 10.4017/gt.2016.15.3.005.00
ER  - 

TY  - JOUR
AU  - Zhang, Futian; Katsuragawa, Keiko; Lank, Edward
TI  - Conductor: Intersection-Based Bimanual Pointing in Augmented and Virtual Reality
PY  - 2022
AB  - <jats:p>Pointing is an elementary interaction in virtual and augmented reality environments, and, to effectively support selection, techniques must deal with the challenges of occlusion and depth specification. Most of the previous techniques require two explicit steps to handle occlusion. In this paper, we propose Conductor, an intuitive, plane-ray, intersection-based, 3D pointing technique where users leverage bimanual input to control a ray and intersecting plane. Conductor allows users to use the non-dominant hand to adjust the cursor distance on the ray while pointing with the dominant hand. We evaluate Conductor against Raycursor, a state-of-the-art VR pointing technique, and show that Conductor outperforms Raycursor for selection tasks. Given our results, we argue that bimanual selection techniques merit additional exploration to support object selection and placement within virtual environments.</jats:p>
SP  - 103
EP  - 117
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567713
ER  - 

TY  - JOUR
AU  - Chen, Zhaokang; Shi, Bertram E.
TI  - Using variable dwell time to accelerate gaze-based web browsing with two-step selection
PY  - 2018
AB  - ABSTRACTIn order to avoid the “Midas Touch” problem, gaze-based interfaces for selection often introduce a dwell time: a fixed amount of time the user must fixate upon an object before it is selected. Past interfaces have used a uniform dwell time across all objects. Here, we propose a gaze-based browser using a two-step selection policy with variable dwell time. In the first step, a command (e.g., “back” or “select”) is chosen from a menu using a dwell time that is constant across the different commands. In the second step, if the “select” command is chosen, the user selects a hyperlink using a dwell time that varies between different hyperlinks. We assign shorter dwell times to more likely hyperlinks and longer dwell times to less likely hyperlinks. In order to infer the likelihood each hyperlink will be selected, we have developed a probabilistic model of natural gaze behavior while surfing the web. We have evaluated a number of heuristic and probabilistic methods for varying the dwell times using both...
SP  - 240
EP  - 255
JF  - International Journal of Human–Computer Interaction
VL  - 35
IS  - 3
PB  - 
DO  - 10.1080/10447318.2018.1452351
ER  - 

TY  - JOUR
AU  - Gaba, Aimen; Setlur, Vidya; Srinivasan, Arjun; Hoffswell, Jane; Xiong, Cindy
TI  - Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design.
PY  - 2022
AB  - The language for expressing comparisons is often complex and nuanced, making supporting natural language-based visual comparison a non-trivial task. To better understand how people reason about comparisons in natural language, we explore a design space of utterances for comparing data entities. We identified different parameters of comparison utterances that indicate what is being compared (i.e., data variables and attributes) as well as how these parameters are specified (i.e., explicitly or implicitly). We conducted a user study with sixteen data visualization experts and non-experts to investigate how they designed visualizations for comparisons in our design space. Based on the rich set of visualization techniques observed, we extracted key design features from the visualizations and synthesized them into a subset of sixteen representative visualization designs. We then conducted a follow-up study to validate user preferences for the sixteen representative visualizations corresponding to utterances in our design space. Findings from these studies suggest guidelines and future directions for designing natural language interfaces and recommendation tools to better support natural language comparisons in visual analytics.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209456
ER  - 

TY  - NA
AU  - Wang, Yihan; Shao, Yutong; Nakashole, Ndapa
TI  - NAACL-HLT (Demonstrations) - Interactive Plot Manipulation using Natural Language.
PY  - 2021
AB  - We present an interactive Plotting Agent, a system that enables users to directly manipulate plots using natural language instructions within an interactive programming environment. The Plotting Agent maps language to plot updates. We formulate this problem as a slot-based task-oriented dialog problem, which we tackle with a sequence-to-sequence model. This plotting model while accurate in most cases, still makes errors, therefore, the system allows a feedback mode, wherein the user is presented with a top-k list of plots, among which the user can pick the desired one. From this kind of feedback, we can then, in principle, continuously learn and improve the system. Given that plotting is widely used across data-driven fields, we believe our demonstration will be of interest to both practitioners such as data scientists broadly defined, and researchers interested in natural language interfaces.
SP  - 92
EP  - 98
JF  - Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2021.naacl-demos.11
ER  - 

TY  - NA
AU  - Le, Khanh-Duy; Tran, Tanh Quang; Chlasta, Karol; Krejtz, Krzysztof; Fjeld, Morten; Kunz, Andreas
TI  - Conference on Designing Interactive Systems - VXSlate: Exploring Combination of Head Movements and Mobile Touch for Large Virtual Display Interaction
PY  - 2021
AB  - Virtual Reality (VR) headsets can open opportunities for users to accomplish complex tasks on large virtual displays using compact and portable devices. However, interacting with such large virtual displays using existing interaction techniques might cause fatigue, especially for precise manipulation tasks, due to the lack of physical surfaces. To deal with this issue, we explored the design of VXSlate, an interaction technique that uses a large virtual display as an expansion of a tablet. We combined a user’s head movements as tracked by the VR headset, and touch interaction on the tablet. Using VXSlate, a user head movements positions a virtual representation of the tablet together with the user’s hand, on the large virtual display. This allows the user to perform fine-tuned multi-touch content manipulations. In a user study with seventeen participants, we investigated the effects of VXSlate on users in problem-solving tasks involving content manipulations at different levels of difficulty, such as translation, rotation, scaling, and sketching. As a baseline for comparison, off-the-shelf touch-controller interactions were used. Overall, VXSlate allowed participants to complete the task with completion times and accuracy that are comparable to touch-controller interactions. After an interval of use, VXSlate significantly reduced users’ time to perform scaling tasks in content manipulations, as well as reducing perceived effort. We reflected on the advantages and disadvantages of VXSlate in content manipulation on large virtual displays and explored further applications within the VXSlate design space.
SP  - 283
EP  - 297
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462076
ER  - 

TY  - JOUR
AU  - Srinivasan, Arjun; Lee, Bongshin; Stasko, John
TI  - Interweaving Multimodal Interaction With Flexible Unit Visualizations for Data Exploration
PY  - 2021
AB  - Multimodal interfaces that combine direct manipulation and natural language have shown great promise for data visualization. Such multimodal interfaces allow people to stay in the flow of their visual exploration by leveraging the strengths of one modality to complement the weaknesses of others. In this article, we introduce an approach that interweaves multimodal interaction combining direct manipulation and natural language with flexible unit visualizations. We employ the proposed approach in a proof-of-concept system, DataBreeze. Coupling pen, touch, and speech-based multimodal interaction with flexible unit visualizations, DataBreeze allows people to create and interact with both systematically bound (e.g., scatterplots, unit column charts) and manually customized views, enabling a novel visual data exploration experience. We describe our design process along with DataBreeze's interface and interactions, delineating specific aspects of the design that empower the synergistic use of multiple modalities. We also present a preliminary user study with DataBreeze, highlighting the data exploration patterns that participants employed. Finally, reflecting on our design process and preliminary user study, we discuss future research directions.
SP  - 3519
EP  - 3533
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 8
PB  - 
DO  - 10.1109/tvcg.2020.2978050
ER  - 

TY  - NA
AU  - Srinivasan, Arjun; Dontcheva, Mira; Adar, Eytan; Walker, Seth
TI  - IUI - Discovering natural language commands in multimodal interfaces
PY  - 2019
AB  - Discovering what to say and how to say it remains a challenge for users of multimodal interfaces supporting speech input. Users end up "guessing" commands that a system might support, often leading to interpretation errors and frustration. One solution to this problem is to display contextually relevant command examples as users interact with a system. The challenge, however, is deciding when, how, and which examples to recommend. In this work, we describe an approach for generating and ranking natural language command examples in multimodal interfaces. We demonstrate the approach using a prototype touch- and speech-based image editing tool. We experiment with augmentations of the UI to understand when and how to present command examples. Through an online user study, we evaluate these alternatives and find that in-situ command suggestions promote discovery and encourage the use of speech input.
SP  - 661
EP  - 672
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302292
ER  - 

TY  - NA
AU  - Srinivasan, Arjun; Lee, Bongshin; Riche, Nathalie Henry; Drucker, Steven M.; Hinckley, Ken
TI  - CHI - InChorus: Designing Consistent Multimodal Interactions for Data Visualization on Tablet Devices
PY  - 2020
AB  - While tablet devices are a promising platform for data visualization, supporting consistent interactions across different types of visualizations on tablets remains an open challenge. In this paper, we present multimodal interactions that function consistently across different visualizations, supporting common operations during visual data analysis. By considering standard interface elements (e.g., axes, marks) and grounding our design in a set of core concepts including operations, parameters, targets, and instruments, we systematically develop interactions applicable to different visualization types. To exemplify how the proposed interactions collectively facilitate data exploration, we employ them in a tablet-based system, InChorus that supports pen, touch, and speech input. Based on a study with 12 participants performing replication and factchecking tasks with InChorus, we discuss how participants adapted to using multimodal input and highlight considerations for future multimodal visualization systems.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376782
ER  - 

TY  - NA
AU  - Lai, Chufan; Lin, Zhixian; Jiang, Ruike; Han, Yun; Liu, Can; Yuan, Xiaoru
TI  - CHI - Automatic Annotation Synchronizing with Textual Description for Visualization
PY  - 2020
AB  - In this paper, we propose a technique for automatically annotating visualizations according to the textual description. In our approach, visual elements in the target visualization, along with their visual properties, are identified and extracted with a Mask R-CNN model. Meanwhile, the description is parsed to generate visual search requests. Based on the identification results and search requests, each descriptive sentence is displayed beside the described focal areas as annotations. Different sentences are presented in various scenes of the generated animation to promote a vivid step-by-step presentation. With a user-customized style, the animation can guide the audience's attention via proper highlighting such as emphasizing specific features or isolating part of the data. We demonstrate the utility and usability of our method through a user study with use cases.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376443
ER  - 

TY  - NA
AU  - Pietroszek, Krzysztof; Tahai, Liudmila; Wallace, James R.; Lank, Edward
TI  - 3DUI - Watchcasting: Freehand 3D interaction with off-the-shelf smartwatch
PY  - 2017
AB  - We describe a mid-air, watch-based 3D interaction technique called Watchcasting. The technique enables target selection and translation by mapping z-coordinate position to forearm rotation. By replicating a large display 3D selection study, we show that Watchcasting provides comparable performance to smartphone (Smartcasting) and electrical impedance myography (Myopoint) techniques. Our work demonstrates that an off-the-shelf smartwatch is a practical alternative for 3D interaction using specialized devices.
SP  - 172
EP  - 175
JF  - 2017 IEEE Symposium on 3D User Interfaces (3DUI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/3dui.2017.7893335
ER  - 

TY  - JOUR
AU  - Srinivasan, Arjun; Stasko, John; Keefe, Daniel F.; Tory, Melanie
TI  - How to Ask What to Say?: Strategies for Evaluating Natural Language Interfaces for Data Visualization
PY  - 2020
AB  - In this article, we discuss challenges and strategies for evaluating natural language interfaces (NLIs) for data visualization. Through an examination of prior studies and reflecting on own experiences in evaluating visualization NLIs, we highlight benefits and considerations of three task framing strategies: Jeopardy-style facts, open-ended tasks, and target replication tasks. We hope the discussions in this article can guide future researchers working on visualization NLIs and help them avoid common challenges and pitfalls when evaluating these systems. Finally, to motivate future research, we highlight topics that call for further investigation including development of new evaluation metrics, and considering the type of natural language input (spoken versus typed), among others.
SP  - 96
EP  - 103
JF  - IEEE computer graphics and applications
VL  - 40
IS  - 4
PB  - 
DO  - 10.1109/mcg.2020.2986902
ER  - 

TY  - NA
AU  - Bae, S. Sandra; Zheng, Clement; West, Mary Etta; Do, Ellen Yi-Luen; Huron, Samuel; Szafir, Danielle Albers
TI  - Making Data Tangible: A Cross-disciplinary Design Space for Data Physicalization
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501939
ER  - 

TY  - NA
AU  - Lindlbauer, David; Mueller, Jörg; Alexa, Marc
TI  - CHI - Changing the Appearance of Real-World Objects By Modifying Their Surroundings
PY  - 2017
AB  - We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.
SP  - 3954
EP  - 3965
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025795
ER  - 

TY  - NA
AU  - Wall, Ludwig; Jacobson, Alec; Vogel, Daniel; Schneider, Oliver
TI  - CHI - Scrappy: Using Scrap Material as Infill to Make Fabrication More Sustainable
PY  - 2021
AB  - We present a software system for fused deposition modelling 3D printing that replaces infill material with scrap to reduce material and energy consumption. Example scrap objects include unused 3D prints from prototyping and calibration, household waste like coffee cups, and off-cuts from other fabrication projects. To achieve this, our system integrates into an existing CAD workflow and manages a database of common items, previous prints, and manually entered objects. While modelling in a standard CAD application, the system suggests objects to insert, ranked by how much infill material they could replace. This computation extends an existing nesting algorithm to determine which objects fit, optimize their alignment, and adjust the enclosing mesh geometry. While printing, the system uses custom tool-paths and animated instructions to enable anyone nearby to manually insert the scrap material.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445187
ER  - 

TY  - NA
AU  - Afsar, Ozgun Kilic; Shtarbanov, Ali; Mor, Hila; Nakagaki, Ken; Forman, Jack; Modrei, Karen; Jeong, Seung Hee; Hjort, Klas; Höök, Kristina; Ishii, Hiroshi
TI  - UIST - OmniFiber: Integrated Fluidic Fiber Actuators for Weaving Movement based Interactions into the ‘Fabric of Everyday Life’
PY  - 2021
AB  - Fiber – a primitive yet ubiquitous form of material – intertwines with our bodies and surroundings, from constructing our fibrous muscles that enable our movement, to forming fabrics that intimately interface with our skin. In soft robotics and advanced materials science research, actuated fibers are gaining interest as thin, flexible materials that can morph in response to external stimuli. In this paper, we build on fluidic artificial muscles research to develop OmniFiber - a soft, line-based material system for designing movement-based interactions. We devised actuated thin (oouter
SP  - 1010
EP  - 1026
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474802
ER  - 

TY  - NA
AU  - Runge, Nina; Hellmeier, Marius; Wenig, Dirk; Malaka, Rainer
TI  - MobileHCI Adjunct - Tag your emotions: a novel mobile user interface for annotating images with emotions
PY  - 2016
AB  - People tend to collect more and more data, this is especially true for images on mobile devices. Tagging images is a good way to sort such collections. While automatic tagging systems are often focused on the content, such as objects or persons in the image, manual annotations are very important to describe the context of an image. Often especially emotions are important, e.g., when a person reflects a situation, shows images from a very personal collection to others, or when using images to illustrate presentations. Unfortunately, manual annotation is often very boring and users are not very motivated to do so. While there are many approaches to motivate people to annotate data in a conventional way, none of them has focused on emotions. In this poster abstract, we present EmoWheel; an innovative interface to annotate images with emotional tags. We conducted a user study with 18 participants. Results show that the EmoWheel can enhance the motivation to annotate images.
SP  - 846
EP  - 853
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2957265.2961836
ER  - 

TY  - JOUR
AU  - Zhu, Sujia; Sun, Guodao; Jiang, Qi; Zha, Meng; Liang, Ronghua
TI  - A survey on automatic infographics and visualization recommendations
PY  - 2020
AB  - Abstract Automatic infographics generators employ machine learning algorithms/user-defined rules and visual embellishments into the creation of infographics. It is an emerging topic in the field of information visualization that has requirements in many sectors, such as dashboard design, data analysis, and visualization recommendation. The growing popularity of visual analytics in recent years brings increased attention to automatic infographics. This creates the need for a broad survey that reviews and assesses the significant advances in this field. Automatic tools aim to lower the barrier for visually analyzing data by automatically generating visualizations for analysts to search and make a choice, instead of manually specifying. This survey reviews and classifies automatic tools and papers of visualization recommendations into a set of application categories including network-graph visualizations, annotation visualizations, and storytelling visualization. More importantly, this report presents several challenges and promising directions for future work in the field of automatic infographics and visualization recommendations.
SP  - 24
EP  - 40
JF  - Visual Informatics
VL  - 4
IS  - 3
PB  - 
DO  - 10.1016/j.visinf.2020.07.002
ER  - 

TY  - NA
AU  - Shibata, Tomoki; Afergan, Daniel; Kong, Danielle; Yuksel, Beste F.; MacKenzie, I. Scott; Jacob, Robert J. K.
TI  - UIST - DriftBoard: A Panning-Based Text Entry Technique for Ultra-Small Touchscreens
PY  - 2016
AB  - Emerging ultra-small wearables like smartwatches pose a design challenge for touch-based text entry. This is due to the "fat-finger problem," wherein users struggle to select elements much smaller than their fingers. To address this challenge, we developed DriftBoard, a panning-based text entry technique where the user types by positioning a movable qwerty keyboard on an interactive area with respect to a fixed cursor point. In this paper, we describe the design and implementation of DriftBoard and report results of a user study on a watch-size touchscreen. The study compared DriftBoard to two ultra-small keyboards, ZoomBoard (tapping-based) and Swipeboard (swiping-based). DriftBoard performed comparably (no significant difference) to ZoomBoard in the major metrics of text entry speed and error rate, and outperformed Swipeboard, which suggests that panning-based typing is a promising input method for text entry on ultra-small touchscreens.
SP  - 575
EP  - 582
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984591
ER  - 

TY  - NA
AU  - Li, Yuntao; Chen, Bei; Liu, Qian; Gao, Yan; Lou, Jian-Guang; Zhang, Yan; Zhang, Dongmei
TI  - "What Do You Mean by That?" A Parser-Independent Interactive Approach for Enhancing Text-to-SQL
PY  - 2020
AB  - In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users' natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Kim, Junhyeok; Delamare, William; Irani, Pourang
TI  - Graphics Interface - ThumbText: Text Entry for Wearable Devices Using a Miniature Ring
PY  - NA
AB  - Users can benefit from using an auxiliary peripheral that could mitigate many concerns with direct text entry on wearable devices. We introduce ThumbText, a thumb-operated text entry approach for a ring-sized touch surface. Through a multi-part exploration, we first identify a suitable discretization of the miniature touch surface for thumb input. We then design a number of two-step selection techniques for supporting the input of at least 28 characters. On a miniature touch surface, we find that a continuous touch-slide-lift selection technique in a 2×3 grid discretization offers improved performance gains over other selection methods. Finally, we evaluate ThumbText against techniques also designed for wearable devices and find that ThumbText allows for higher text entry rates than SwipeBoard and H4-Writer.
SP  - 18
EP  - 25
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.20380/gi2018.04
ER  - 

TY  - NA
AU  - Srinivasan, Arjun; Lee, Bongshin; Riche, Nathalie Henry; Drucker, Steven M.; Hinckley, Ken
TI  - InChorus: Designing Consistent Multimodal Interactions for Data Visualization on Tablet Devices
PY  - 2020
AB  - While tablet devices are a promising platform for data visualization, supporting consistent interactions across different types of visualizations on tablets remains an open challenge. In this paper, we present multimodal interactions that function consistently across different visualizations, supporting common operations during visual data analysis. By considering standard interface elements (e.g., axes, marks) and grounding our design in a set of core concepts including operations, parameters, targets, and instruments, we systematically develop interactions applicable to different visualization types. To exemplify how the proposed interactions collectively facilitate data exploration, we employ them in a tablet-based system, InChorus that supports pen, touch, and speech input. Based on a study with 12 participants performing replication and fact-checking tasks with InChorus, we discuss how participants adapted to using multimodal input and highlight considerations for future multimodal visualization systems.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Yu, Bowen; Silva, Cláudio T.
TI  - FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System
PY  - 2019
AB  - Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2019.2934668
ER  - 

TY  - NA
AU  - Venkataramani, Shrikant; Smaragdis, Paris; Mysore, Gautham J.
TI  - UIST - AutoDub: Automatic Redubbing for Voiceover Editing
PY  - 2017
AB  - Redubbing is an extensively used technique to correct errors in voiceover recordings. It involves re-recording a part of a voiceover, identifying the corresponding section of audio in the original recording that needs to be replaced, and using low level audio tools to replace the audio. Although this sequence of steps can be performed using traditional audio editing tools, the process can be tedious when dealing with long voiceover recordings and prohibitively difficult for users not familiar with such tools. To address this issue, we present AutoDub, a novel system for redubbing voiceover recordings. Using our system, a user simply needs to re-record the part of the voiceover that needs to be replaced. Our system automatically locates the corresponding part in the original recording and performs the low level audio processing to replace it. The system can be easily incorporated in any existing sophisticated audio editor or can be employed as a functionality in an audio-guided user interface. User studies involving participation from novice, knowledgeable and expert users indicate that our tool is preferred to a traditional audio editor based redubbing approach by all categories of users due to its faster and easier redubbing capabilities.
SP  - 533
EP  - 538
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126661
ER  - 

TY  - NA
AU  - Dhamdhere, Kedar; McCurley, Kevin Snow; Nahmias, Ralfi; Sundararajan, Mukund; Yan, Qiqi
TI  - IUI - Analyza: Exploring Data with Conversation
PY  - 2017
AB  - We describe Analyza, a system that helps lay users explore data. Analyza has been used within two large real world systems. The first is a question-and-answer feature in a spreadsheet product. The second provides convenient access to a revenue/inventory database for a large sales force. Both user bases consist of users who do not necessarily have coding skills, demonstrating Analyza's ability to democratize access to data. We discuss the key design decisions in implementing this system. For instance, how to mix structured and natural language modalities, how to use conversation to disambiguate and simplify querying, how to rely on the ``semantics' of the data to compensate for the lack of syntactic structure, and how to efficiently curate the data.
SP  - 493
EP  - 504
JF  - Proceedings of the 22nd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025171.3025227
ER  - 

TY  - NA
AU  - Sumon, Md. Kazi Abdul Halim; Ashmafee, Md. Hamjajul; Islam, Md Rafiqul; Mostofa Kamal, Abu Raihan
TI  - Explainable NLQ-based Visual Interactive System: Challenges and Objectives
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2nd International Conference on Computing Advancements
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3542954.3543014
ER  - 

TY  - NA
AU  - Setlur, Vidya; E, Battersby Sarah; Tory, Melanie; Gossweiler, Rich; Chang, Angel X.
TI  - UIST - Eviza: A Natural Language Interface for Visual Analysis
PY  - 2016
AB  - Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches.The result of an interaction is a change to the view (e.g., filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis.
SP  - 365
EP  - 377
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984588
ER  - 

TY  - NA
AU  - Navarro, Diego; Sundstedt, Veronica
TI  - SIGGRAPH Asia Technical Briefs - Simplifying game mechanics: gaze as an implicit interaction method
PY  - 2017
AB  - This paper explores the possibilities of using player gaze as an implicit interaction method, to simplify game mechanics in a space shooting video game. First, a set of five experienced players were eye-tracked while playing the game Ikaruga where gaze information was recorded. The observed gaze patterns from these players were used to design two novel implicit interaction techniques. The techniques aimed to reduce the complexity of common game mechanics from this genre. Two video game prototypes were then developed. Both made use of traditional mechanics, but only one featured the gaze-based implicit interaction techniques (Figure 1). Nine participants then played both prototypes in a user study. A questionnaire was used to evaluate their experience with the implicit interaction techniques. Also, the final scores of each player were recorded to analyze if there was any significant change in performance between the prototypes, due to an unfair advantage provided by the interaction techniques. Results show that participants had an overall positive experience and agreeing that the gaze interaction provided a game mechanic simplification. Additionally, there was no significant difference in the score results when comparing both of the interaction techniques.
SP  - 4
EP  - NA
JF  - SIGGRAPH Asia 2017 Technical Briefs
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3145749.3149446
ER  - 

TY  - NA
AU  - Kim, Hyunyoung; Everitt, Aluna; Tejada, Carlos E.; Zhong, Mengyu; Ashbrook, Daniel
TI  - CHI - MorpheesPlug: A Toolkit for Prototyping Shape-Changing Interfaces
PY  - 2021
AB  - Toolkits for shape-changing interfaces (SCIs) enable designers and researchers to easily explore the broad design space of SCIs. However, despite their utility, existing approaches are often limited in the number of shape-change features they can express. This paper introduces MorpheesPlug , a toolkit for creating SCIs that covers seven of the eleven shape-change features identified in the literature. MorpheesPlug is comprised of (1) a set of six standardized widgets that express the shape-change features with user-definable parameters; (2) software for 3D-modeling the widgets to create 3D-printable pneumatic SCIs; and (3) a hardware platform to control the widgets. To evaluate MorpheesPlug we carried out ten open-ended interviews with novice and expert designers who were asked to design a SCI using our software. Participants highlighted the ease of use and expressivity of the MorpheesPlug.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445786
ER  - 

TY  - JOUR
AU  - Mack, Kelly; Hofmann, Megan; Lakshmi, Udaya; Cao, Jerry; Auradkar, Nayha; Arriaga, Rosa I.; Hudson, Scott E.; Mankoff, Jennifer
TI  - Rapid Convergence: The Outcomes of Making PPE during a Healthcare Crisis
PY  - 2022
AB  - <jats:p>The U.S. National Institute of Health (NIH) 3D Print Exchange is a public, open-source repository for 3D printable medical device designs with contributions from clinicians, expert-amateur makers, and people from industry and academia. In response to the COVID-19 pandemic, the NIH formed a collection to foster submissions of low-cost, locally-manufacturable personal protective equipment (PPE). We evaluated the 623 submissions in this collection to understand: what makers contributed, how they were made, who made them, and key characteristics of their designs. We found an immediate design convergence to manufacturing-focused remixes of a few initial designs affiliated with NIH partners and major for-profit groups. The NIH worked to review safe, effective designs but was overloaded by manufacturing-focused design adaptations. Our work contributes insights into: the outcomes of distributed, community-based medical making; the features that the community accepted as “safe” making; and how platforms can support regulated maker activities in high-risk domains.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3542923
ER  - 

TY  - NA
AU  - Chen, Yiru; Wu, Eugene
TI  - PI2: End-to-end Interactive Visualization Interface Generation from Queries
PY  - 2022
AB  - Interactive visual analysis interfaces are critical in nearly every data task. However, creating new interfaces is deeply challenging, as it requires the developer to understand the queries needed to express the desired analysis task, design the appropriate interface to express those queries for the task, and implement the interface using a combination of visualization, browser, server, and database technologies. Although prior work generates a set of interactive widgets that can express an input query log, this paper presents PI2, the first system to generate fully functional visual analysis interfaces from an example sequence of analysis queries. PI2 analyzes queries syntactically and represents a set of queries using a novel Difftree structure that encodes systematic variations between query abstract syntax trees. PI2 then maps each Difftree to a visualization that renders its results, the variations in each Difftree to interactions, and generates a good layout for the interface. We show that PI2 can express data-oriented interactions in existing visualization interaction taxonomies, reproduce or improve several real-world visual analysis interfaces, generate interfaces in 2-19s (median 6s), and scale linearly with the number of queries.
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 International Conference on Management of Data
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3514221.3526166
ER  - 

TY  - NA
AU  - Romat, Hugo; Marquardt, Nicolai; Hinckley, Ken; Henry Riche, Nathalie
TI  - Style Blink: Exploring Digital Inking of Structured Information via Handcrafted Styling as a First-Class Object
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501988
ER  - 

TY  - NA
AU  - Tao, Ye; Wang, Guanyun; Zhang, Caowei; Nannan, Lu; Zhang, Xiaolian; Yao, Cheng; Ying, Fangtian
TI  - CHI - WeaveMesh: A Low-Fidelity and Low-Cost Prototyping Approach for 3D Models Created by Flexible Assembly
PY  - 2017
AB  - To meet the increasing requirements of HCI researchers who are prototyping a variety of forms to create novel interfaces under a ubiquitous situation, we present WeaveMesh, a low-fidelity and low-cost rapid prototyping system that produces 3D objects in a mesh structure. Inspired by hand-weaving craft, WeaveMesh supports a highly customizable software platform, which is applicable for simulating and facilitating freeform surface constructions composed of woven lines arranged in a regular grid, which can serve as a guide for easy assembly. In addition, mobilizable connectors are suggested to support flexible assembly, which can be revised, recycled, and reused to facilitate short iterations. Furthermore, compared to common additive and subtractive techniques, WeaveMesh has a better balance between time and material saving. In this paper, we will introduce the system in detail and demonstrate the feasibility of the technique through various 3D models in the area of interactive media, products and architecture.
SP  - 509
EP  - 518
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025699
ER  - 

TY  - NA
AU  - Darvishzadeh, Amirali; Entezari, Negin; Stahovich, Thomas F.
TI  - ICFHR - Finding the Answer: Techniques for Locating Students' Answers in Handwritten Problem Solutions
PY  - 2018
AB  - In many academic subjects, especially science, engineering, and math, paper-based problem solving is an important part of education. However, grading such work can be prohibitively expensive in large university courses. As a remedy, we have developed techniques to support the automated grading of handwritten problem solutions. Students complete their work using Livescribe digital pens and draw boxes around the final answers. We developed techniques that identify the answers by locating the boxes. This problem is challenging as the written work contains a mixture of diagrams and equations, and boxes frequently appear as a part of the diagrams. Additionally, the boxes must be segmented from the remainder of the writing. Thus, a simple shape recognizer is inadequate for this task. Our techniques efficiently locate answer boxes within a complex page of free-form writing. Furthermore, our techniques are designed to be robust to the wide range of variations in the way students write. In a test on 2022 pages of homework problems, our techniques correctly located 95.3% of the 4473 answer boxes. These techniques are an important step towards automated grading of handwritten work because once the answer boxes are located, a variety of handwriting recognition methods can be used to interpret the answers.
SP  - 587
EP  - 592
JF  - 2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icfhr-2018.2018.00108
ER  - 

TY  - JOUR
AU  - Wang, Yun; Hou, Zhitao; Shen, Leixian; Wu, Tongshuang; Wang, Jiaqi; Huang, He; Zhang, Haidong; Zhang, Dongmei
TI  - Towards Natural Language-Based Visualization Authoring.
PY  - 2022
AB  - A key challenge to visualization authoring is the process of getting familiar with the complex user interfaces of authoring tools. Natural Language Interface (NLI) presents promising benefits due to its learnability and usability. However, supporting NLIs for authoring tools requires expertise in natural language processing, while existing NLIs are mostly designed for visual analytic workflow. In this paper, we propose an authoring-oriented NLI pipeline by introducing a structured representation of users' visualization editing intents, called editing actions, based on a formative study and an extensive survey on visualization construction tools. The editing actions are executable, and thus decouple natural language interpretation and visualization applications as an intermediate layer. We implement a deep learning-based NL interpreter to translate NL utterances into editing actions. The interpreter is reusable and extensible across authoring tools. The authoring tools only need to map the editing actions into tool-specific operations. To illustrate the usages of the NL interpreter, we implement an Excel chart editor and a proof-of-concept authoring tool, VisTalk. We conduct a user study with VisTalk to understand the usage patterns of NL-based authoring systems. Finally, we discuss observations on how users author charts with natural language, as well as implications for future research.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209357
ER  - 

TY  - JOUR
AU  - Rovetti, Joseph; Goy, Huiwen; Zara, Michael; Russo, Frank A.
TI  - Reduced Semantic Context and Signal-to-Noise Ratio Increase Listening Effort As Measured Using Functional Near-Infrared Spectroscopy.
PY  - 2021
AB  - OBJECTIVES Understanding speech-in-noise can be highly effortful. Decreasing the signal-to-noise ratio (SNR) of speech increases listening effort, but it is relatively unclear if decreasing the level of semantic context does as well. The current study used functional near-infrared spectroscopy to evaluate two primary hypotheses: (1) listening effort (operationalized as oxygenation of the left lateral PFC) increases as the SNR decreases and (2) listening effort increases as context decreases. DESIGN Twenty-eight younger adults with normal hearing completed the Revised Speech Perception in Noise Test, in which they listened to sentences and reported the final word. These sentences either had an easy SNR (+4 dB) or a hard SNR (-2 dB), and were either low in semantic context (e.g., "Tom could have thought about the sport") or high in context (e.g., "She had to vacuum the rug"). PFC oxygenation was measured throughout using functional near-infrared spectroscopy. RESULTS Accuracy on the Revised Speech Perception in Noise Test was worse when the SNR was hard than when it was easy, and worse for sentences low in semantic context than high in context. Similarly, oxygenation across the entire PFC (including the left lateral PFC) was greater when the SNR was hard, and left lateral PFC oxygenation was greater when context was low. CONCLUSIONS These results suggest that activation of the left lateral PFC (interpreted here as reflecting listening effort) increases to compensate for acoustic and linguistic challenges. This may reflect the increased engagement of domain-general and domain-specific processes subserved by the dorsolateral prefrontal cortex (e.g., cognitive control) and inferior frontal gyrus (e.g., predicting the sensory consequences of articulatory gestures), respectively.
SP  - 836
EP  - 848
JF  - Ear and hearing
VL  - 43
IS  - 3
PB  - 
DO  - 10.1097/aud.0000000000001137
ER  - 

TY  - NA
AU  - Dementyev, Artem; Kao, Hsin-Liu; Choi, Inrak; Ajilo, Deborah; Xu, Maggie; Paradiso, Joseph A.; Schmandt, Chris; Follmer, Sean
TI  - UIST - Rovables: Miniature On-Body Robots as Mobile Wearables
PY  - 2016
AB  - We introduce Rovables, a miniature robot that can move freely on unmodified clothing. The robots are held in place by magnetic wheels, and can climb vertically. The robots are untethered and have an onboard battery, microcontroller, and wireless communications. They also contain a low-power localization system that uses wheel encoders and IMU, allowing Rovables to perform limited autonomous navigation on the body. In the technical evaluations, we found that Rovables can operate continuously for 45 minutes and can carry up to 1.5N. We propose an interaction space for mobile on-body devices spanning sensing, actuation, and interfaces, and develop application scenarios in that space. Our applications include on-body sensing, modular displays, tactile feedback and interactive clothing and jewelry.
SP  - 111
EP  - 120
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984531
ER  - 

TY  - NA
AU  - Bhowmick, Shimmila; Singh, Aditi; Protim Borah, Pranjal; Goswami, Vrushin; Sorathia, Keyur
TI  - Novel Input Interactions for a Möbius Shaped Flexible Handheld Device
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - India HCI 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3506469.3506489
ER  - 

TY  - NA
AU  - Kim, Young-Ho; Lee, Bongshin; Srinivasan, Arjun; Choe, Eun Kyoung
TI  - Data@Hand: Fostering Visual Exploration of Personal Data on Smartphones Leveraging Speech and Touch Interaction
PY  - 2021
AB  - Most mobile health apps employ data visualization to help people view their health and activity data, but these apps provide limited support for visual data exploration. Furthermore, despite its huge potential benefits, mobile visualization research in the personal data context is sparse. This work aims to empower people to easily navigate and compare their personal health data on smartphones by enabling flexible time manipulation with speech. We designed and developed Data@Hand, a mobile app that leverages the synergy of two complementary modalities: speech and touch. Through an exploratory study with 13 long-term Fitbit users, we examined how multimodal interaction helps participants explore their own health data. Participants successfully adopted multimodal interaction (i.e., speech and touch) for convenient and fluid data exploration. Based on the quantitative and qualitative findings, we discuss design implications and opportunities with multimodal interaction for better supporting visual data exploration on mobile devices.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445421
ER  - 

TY  - BOOK
AU  - Huang, Zanzhen; Zhu, Yaxin; Mao, Xiaofei; Su, Tianxin; Fu, Xinyi; Fei, Guangzheng
TI  - DSA - coisTable: An Individual-and-Spatial-Aware Tabletop System for Co-located Collaboration
PY  - 2020
AB  - We present coisTable, a tabletop system that integrates a shared interactive surface with spatial-aware and identifiable tangible devices to support co-located collaborative work. We describe the underlying technical contributions: a hybrid interaction paradigm of graphical and tangible interfaces to foster the collaboration space; a workspace partition strategy to facilitate coordination; a natural identification to promote participation using tangible terminals. In addition, we detail the system prototype including the hardware implementation and the software architecture. Capabilities and interaction modalities are illustrated with the comparison of three conditions of a project management simulation. Finally, we report on the experimental results and the limitations of the tabletop system.
SP  - 91
EP  - 99
JF  - 2019 6th International Conference on Dependable Systems and Their Applications (DSA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/dsa.2019.00019
ER  - 

TY  - JOUR
AU  - Williams, Kristin; Pulivarthy, Rajitha; Hudson, Scott E.; Hammer, Jessica
TI  - Understanding Family Collaboration Around Lightweight Modification of Everyday Objects in the Home
PY  - 2019
AB  - The internet-of-things (IoT) carries substantial costs by urging households to replace their possessions with new, internet connected versions of everyday objects. Beyond financial, these costs include waste, work to arrange and orchestrate objects to suit households, and that of acquiring new skills. Upcycling domestic objects could offer households greater discretion and control over these costs by supporting the ability to tailor IoT to the home. To understand how households might do this, we conducted a home study with 10 diverse American households over 7 days to surface the approaches families are likely to use when tailoring IoT to their existing possessions. We asked family members to enact their process using endowed sticker props---IoT Stickers---to modify objects in their home. We develop a framework of how families make light weight modifications of domestic possessions, summarize trends of their object modifications, and describe the burdens such a system could impose.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359287
ER  - 

TY  - NA
AU  - Inoue, Akifumi; Fukunaga, Takeru; Ishikawa, Ryuta
TI  - VR - Transformable Game Controller and Its Application to Action Game
PY  - 2019
AB  - Dedicated controllers can provide rich gaming experience. However, a player must prepare many controllers to enjoy modern games that have many game items. Depending on circumstances, the shape of the controller and the shape of the game item are mismatched. This may reduce the player's sense of unity with the game character. In this paper, we propose a transformable game controller. This controller can be multiple dedicated controllers by changing its three-dimensional shape. The game system can also change the shape of the controller immediately when the shape of the corresponding item changes. The continuous shape synchronization between real controller and virtual item can provide the player rich sense of unity with the game character.
SP  - 1317
EP  - 1318
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798230
ER  - 

TY  - NA
AU  - Wieland, Jacob Lyng; Larsen, Lars Bo; Laursen, Jeanette Kølbæk; Jørgensen, Lotte Ishøy; Jessen, Anne Mette Karnøe; Jensen, Charlotte Thodberg
TI  - TVX - Towards Biometric Assessment of Audience Affect
PY  - 2016
AB  - This paper investigates how reliable affective responses can be obtained using objective biometric measures for media audience research. We use Galvanic Skin Response (GSR) to detect sixteen respondents' arousal levels and as an objective measure to show how self-reporting disrupts the experience of respondents watching video content. The subjective experiences from nine subjects were captured by self-reporting via the widely used SAM pictogram scale every three minutes. We found that interruptions induced by the self-reporting events cause them to consistently exhibit arousal peaks. Our post test measures show a negative effect on the subjects' overall experience of the video and their empathy and identification with the main characters compared to the remaining subjects. We observed a 30 second return period from abnormal arousal levels after the self-reporting interruptions.
SP  - 155
EP  - 162
JF  - Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2932206.2933563
ER  - 

TY  - NA
AU  - Schüsselbauer, Dennis; Schmid, Andreas; Wimmer, Raphael
TI  - TEI - Dothraki: Tracking Tangibles Atop Tabletops Through De-Bruijn Tori
PY  - 2021
AB  - Tangibles are small, graspable objects that act as input devices or physical representations of digital data. Oftentimes, it is desirable to track the position of tangibles on a surface and their relation to each other. However, outside-in tracking techniques - such as capacitive touchscreens or cameras - require setting up elaborate infrastructure and are prone to occlusion or interference. We propose Dothraki, an inside-out tracking technique for tangibles on flat surfaces. An optical mouse sensor embedded in the tangible captures a small (36×36 pixel / 1×1 mm), unique section of a black-and-white De-Bruijn dot pattern printed on the surface. Our system efficiently searches the pattern space in order to determine the precise location of the tangible with sub-millimeter accuracy. Our proof-of-concept implementation offers a recognition rate of up to 95%, robust error detection, an update rate of 14 Hz, and a low-latency relative tracking mode.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440656
ER  - 

TY  - NA
AU  - Mazumder, Sahisnu; Riva, Oriana
TI  - NAACL-HLT - FLIN: A Flexible Natural Language Interface for Web Navigation
PY  - 2021
AB  - AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.
SP  - 2777
EP  - 2788
JF  - Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2021.naacl-main.222
ER  - 

TY  - NA
AU  - Whiting, Emily; Ouf, Nada; Makatura, Liane; Mousas, Christos; Shu, Zhenyu; Kavan, Ladislav
TI  - CHI - Environment-Scale Fabrication: Replicating Outdoor Climbing Experiences
PY  - 2017
AB  - Despite rapid advances in 3D printing, fabricating large, durable and robust artifacts is impractical with current technology. We focus on a particularly challenging environment-scale artifact: rock climbing routes. We propose a prototype fabrication method to replicate part of an outdoor climbing route and enable the same sensorimotor experience in an indoor gym. We start with 3D reconstruction of the rock wall using multi-view stereo and use reference videos of a climber in action to identify localized rock features that are necessary for ascent. We create 3D models akin to traditional indoor climbing holds, fabricated using rapid prototyping, molding and casting techniques. This results in robust holds accurately replicating the features and configuration of the original rock route. Validation was performed on two rock climbing sites in New Hampshire and Utah. We verified our results by comparing climbers' moves on the indoor replicas and original outdoor routes.
SP  - 1794
EP  - 1804
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025465
ER  - 

TY  - JOUR
AU  - Chowdhury, Imran; Moeid, Abdul; Hoque, Enamul; Kabir, Muhammad Ashad; Hossain, Sabir; Islam, Mohammad Mainul
TI  - Designing and Evaluating Multimodal Interactions for Facilitating Visual Analysis With Dashboards
PY  - 2020
AB  - Exploring and analyzing data using visualizations is at the heart of many decision-making tasks. Typically, people perform visual data analysis using mouse and touch interactions. While such interactions are often easy to use, they can be inadequate for users to express complex information and may require many steps to complete a task. Recently natural language interaction has emerged as a promising technique for supporting exploration with visualization, as the user can express a complex analytical question more easily. In this paper, we investigate how to synergistically combine language and mouse-based direct manipulations so that the weakness of one modality can be complemented by the other. To this end, we have developed a novel system, named Multimodal Interactions System for Visual Analysis ( MIVA ), that allows user to provide input using both natural language (e.g., through speech) and direct manipulation (e.g., through mouse or touch) and presents the answer accordingly. To answer the current question in the context of past interactions, the system incorporates previous utterances and direct manipulations made by the user within a finite-state model. The uniqueness of our approach is that unlike most previous approaches which typically support multimodal interactions with a single visualization, MIVA enables multimodal interactions with multiple coordinated visualizations of a dashboard that visually summarizes a dataset. We tested MIVA’s applicability on several dashboards including a COVID-19 dashboard that visualizes coronavirus cases around the globe. We further empirically evaluated our system through a user study with twenty participants. The results of our study revealed that MIVA system enhances the flow of visual analysis by enabling fluid, iterative exploration and refinement of data in a dashboard with multiple-coordinated views.
SP  - 60
EP  - 71
JF  - IEEE access : practical innovations, open solutions
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2020.3046623
ER  - 

TY  - NA
AU  - Zhao, Yiwei; Follmer, Sean
TI  - CHI - A Functional Optimization Based Approach for Continuous 3D Retargeted Touch of Arbitrary, Complex Boundaries in Haptic Virtual Reality
PY  - 2018
AB  - Passive or actuated physical props can provide haptic feedback, leading to a satisfying sense of presence and realism in virtual reality. However, the mismatch between the physical and virtual surfaces (boundaries) can diminish user experience. Haptic retargeting can overcome this limitation by utilizing visio-haptic effects. Previous investigations in haptic retargeting have focused on methods for point based position retargeting and techniques for remapping 2D shapes or simple 3D shape changes. Our approach extends haptic retargeting to complex, arbitrary shapes that provide a continuous mapping across all points on a boundary. This new approach also allows for multi-finger interaction. We describe a functional optimization to find the ideal spatial warping function with different goals: a maximum mapping smoothness, a minimum mismatch between the real and virtual world, or the combination of the two. We report on a preliminary user study of different optimization goals and elaborate potential applications through a set of demonstrations.
SP  - 544
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174118
ER  - 

TY  - JOUR
AU  - Huang, Jieying; Xi, Yang; Hu, Junnan; Tao, Jun
TI  - FlowNL: Asking the Flow Data in Natural Languages.
PY  - 2022
AB  - Flow visualization is essentially a tool to answer domain experts' questions about flow fields using rendered images. Static flow visualization approaches require domain experts to raise their questions to visualization experts, who develop specific techniques to extract and visualize the flow structures of interest. Interactive visualization approaches allow domain experts to ask the system directly through the visual analytic interface, which provides flexibility to support various tasks. However, in practice, the visual analytic interface may require extra learning effort, which often discourages domain experts and limits its usage in real-world scenarios. In this paper, we propose FlowNL, a novel interactive system with a natural language interface. FlowNL allows users to manipulate the flow visualization system using plain English, which greatly reduces the learning effort. We develop a natural language parser to interpret user intention and translate textual input into a declarative language. We design the declarative language as an intermediate layer between the natural language and the programming language specifically for flow visualization. The declarative language provides selection and composition rules to derive relatively complicated flow structures from primitive objects that encode various kinds of information about scalar fields, flow patterns, regions of interest, connectivities, etc. We demonstrate the effectiveness of FlowNL using multiple usage scenarios and an empirical evaluation.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209453
ER  - 

TY  - NA
AU  - Campos Zamora, Daniel; He, Liang; Zhang, Yueqian; Xu, Xuhai; Mankoff, Jennifer; Froehlich, Jon E.
TI  - sPrintr: Towards In-Situ Personal Fabrication using a Mobile 3D Printer
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3565587
ER  - 

TY  - NA
AU  - Srinivasan, Arjun; Setlur, Vidya
TI  - Snowy: Recommending Utterances for Conversational Visual Analysis.
PY  - 2021
AB  - Natural language interfaces (NLIs) have become a prevalent medium for conducting visual data analysis, enabling people with varying levels of analytic experience to ask questions of and interact with their data. While there have been notable improvements with respect to language understanding capabilities in these systems, fundamental user experience and interaction challenges including the lack of analytic guidance (i.e., knowing what aspects of the data to consider) and discoverability of natural language input (i.e., knowing how to phrase input utterances) persist. To address these challenges, we investigate utterance recommendations that contextually provide analytic guidance by suggesting data features (e.g., attributes, values, trends) while implicitly making users aware of the types of phrasings that an NLI supports. We present SNOWY, a prototype system that generates and recommends utterances for visual analysis based on a combination of data interestingness metrics and language pragmatics. Through a preliminary user study, we found that utterance recommendations in SNOWY support conversational visual analysis by guiding the participants' analytic workflows and making them aware of the system's language interpretation capabilities. Based on the feedback and observations from the study, we discuss potential implications and considerations for incorporating recommendations in future NLIs for visual analysis.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Ruser, Heinrich; Kirsh, Ilan
TI  - HCI (41) - “Point at It with Your Smartphone”: Assessing the Applicability of Orientation Sensing of Smartphones to Operate IoT Devices
PY  - 2021
AB  - The built-in orientation and motion sensors of smartphones along with their wireless communication abilities are utilized to control connected IoT devices from any place in a room, by pointing at them with the smartphone in the hand. The information of which device is targeted will be derived from the user’s actual location, the spatial orientation of the smartphone and pre-knowledge regarding the positions of devices. Chosen devices are remotely operated with simple mid-air gestures performed with the smartphone. The feasibility of this cost-effective approach is assessed by user experiments. The continuous readings of the smartphone’s inclination, rotation and magnetic field sensors are recorded with a dedicated freeware app. An algorithm combines the sensor readings to deliver the actual spatial orientation. Our preliminary experiments with different smartphone models and several users show that pointing at defined positions and performing gestures with a smartphone in the user’s hand can be accurately sensed without latency and with small deviations of the orientation measurements in the range of up to 5 degrees, indicating the feasibility of this novel approach.
SP  - 115
EP  - 131
JF  - HCI International 2021 - Late Breaking Papers: Multimodality, eXtended Reality, and Artificial Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90963-5_10
ER  - 

TY  - NA
AU  - Nayyar, Aanand; Dwivedi, Utkarsh; Ahuja, Karan; Rajput, Nitendra; Nagar, Seema; Dey, Kuntal
TI  - IUI - OptiDwell: Intelligent Adjustment of Dwell Click Time
PY  - 2017
AB  - Gaze based navigation with digital screens offer a hands-free and touchless interaction, which is often useful in providing a hygienic interaction experience in a public kiosk scenario. The goodness of such a navigation system depends not only on the accuracy of detecting the eye gaze but also on the ability to determine whether a user is interested in clicking a button or is just looking at the button. The time for which a user needs to gaze at a particular button before it is considered as a click action is called the dwell time. In this paper, we explore intelligent adjustment of dwell times, where mouse click events on the buttons of a given application are emulated with user gaze. A constant dwell-time for all buttons and for all users may not provide an efficient and intuitive interface. We thereby propose a model to dynamically adjust dwell-time values used to emulate user mouse click events, exploiting the user's experience with different portions of a given application. The adjustment happens at a per-user, per-button granularity, as a function of the user's (a) prior usage experience of the given button within the application and (b) Midas touch characteristics for the given button. We propose OptiDwell, inspired by the action-value method based solutions to the Multi-Armed Bandits problem, for dwell click time adaptation. We experiment OptiDwell using an interactive TV channel browsing interface application, constituting of a mix of text and image buttons, over 10 computer-savvy users generating over 9000 click tasks. We observe significant improvement of user comfort level over the sessions, quantified by (a) improved (reduced) dwell times and (b) reduced number of Midas touches in spite of faster dwell-clicks, as high as 10-fold reduction in the best case. Our work is useful for creating an interface, with accurate, fast and comfortable dwell-clicks for each interface element (e.g., buttons), and each user.
SP  - 193
EP  - 204
JF  - Proceedings of the 22nd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025171.3025202
ER  - 

TY  - JOUR
AU  - Kreimeier, Julian; Bielmeier, Thomas; Götzelmann, Timo
TI  - Evaluation of Capacitive Markers Fabricated by 3D Printing, Laser Cutting and Prototyping
PY  - 2018
AB  - With Tangible User Interfaces, the computer user is able to interact in a fundamentally different and more intuitive way than with usual 2D displays. By grasping real physical objects, information can also be conveyed haptically, i.e., the user not only sees information on a 2D display, but can also grasp physical representations. To recognize such objects (“tangibles”) it is skillful to use capacitive sensing, as it happens in most touch screens. Thus, real objects can be located and identified by the touch screen display automatically. Recent work already addressed such capacitive markers, but focused on their coding scheme and automated fabrication by 3D printing. This paper goes beyond the fabrication by 3D printers and, for the first time, applies the concept of capacitive codes to laser cutting and another immediate prototyping approach using modeling clay. Beside the evaluation of additional properties, we adapt recent research results regarding the optimized detection of tangible objects on capacitive screens. As a result of our comprehensive study, the detection performance is affected by the type of capacitive signal processing (respectively the device) and the geometry of the marker. 3D printing revealed to be the most reliable technique, though laser cutting and immediate prototyping of markers showed promising results. Based on our findings, we discuss individual strengths of each capacitive marker type.
SP  - 9
EP  - NA
JF  - Inventions
VL  - 3
IS  - 1
PB  - 
DO  - 10.3390/inventions3010009
ER  - 

TY  - NA
AU  - Kim, Dae Hyun; Hoque, Enamul; Agrawala, Maneesh
TI  - CHI - Answering Questions about Charts and Generating Visual Explanations
PY  - 2020
AB  - People often use charts to analyze data, answer questions and explain their answers to others. In a formative study, we find that such human-generated questions and explanations commonly refer to visual features of charts. Based on this study, we developed an automatic chart question answering pipeline that generates visual explanations describing how the answer was obtained. Our pipeline first extracts the data and visual encodings from an input Vega-Lite chart. Then, given a natural language question about the chart, it transforms references to visual attributes into references to the data. It next applies a state-of-the-art machine learning algorithm to answer the transformed question. Finally, it uses a template-based approach to explain in natural language how the answer is determined from the chart's visual features. A user study finds that our pipeline-generated visual explanations significantly outperform in transparency and are comparable in usefulness and trust to human-generated explanations.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376467
ER  - 

TY  - CHAP
AU  - Cavazza, Marc; Charles, Fred
TI  - User Interaction for Interactive Storytelling
PY  - 2016
AB  - NA
SP  - 1
EP  - 14
JF  - Handbook of Digital Games and Entertainment Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-4560-52-8_57-1
ER  - 

TY  - NA
AU  - Kumar, Abhinav; Aurisano, Jillian; Di Eugenio, Barbara; Johnson, Andrew; Gonzalez, Alberto; Leigh, Jason
TI  - SIGDIAL Conference - Towards a dialogue system that supports rich visualizations of data
PY  - 2016
AB  - NA
SP  - 304
EP  - 309
JF  - Proceedings of the 17th Annual Meeting of the Special Interest Group
          on Discourse and Dialogue
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/w16-3639
ER  - 

TY  - NA
AU  - Hoffswell, Jane; Liu, Zhicheng
TI  - CHI - Interactive Repair of Tables Extracted from PDF Documents on Mobile Devices
PY  - 2019
AB  - PDF documents often contain rich data tables that offer opportunities for dynamic reuse in new interactive applications. We describe a pipeline for extracting, analyzing, and parsing PDF tables based on existing machine learning and rule-based techniques. Implementing and deploying this pipeline on a corpus of 447 documents with 1,171 tables results in only 11 tables that are correctly extracted and parsed. To improve the results of automatic table analysis, we first present a taxonomy of errors that arise in the analysis pipeline and discuss the implications of cascading errors on the user experience. We then contribute a system with two sets of lightweight interaction techniques (gesture and toolbar), for viewing and repairing extraction errors in PDF tables on mobile devices. In an evaluation with 17 users involving both a phone and a tablet, participants effectively repaired common errors in 10 tables, with an average time of about 2 minutes per table.
SP  - 293
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300523
ER  - 

TY  - NA
AU  - Sarmah, Ritam Jyoti; Ding, Yunpeng; Wang, Di; Lee, Cheuk Yin Phipson; Li, Toby Jia-Jun; Chen, Xiang 'Anthony'
TI  - UIST - Geno: A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications
PY  - 2020
AB  - Supporting voice commands in applications presents significant benefits to users. However, adding such support to existing GUI-based web apps is effort-consuming with a high learning barrier, as shown in our formative study, due to the lack of unified support for creating multi-modal interfaces. We develop Geno---a developer tool for adding the voice input modality to existing web apps without requiring significate NLP expertise. Geno provides a unified workflow for developers to specify functionalities to support by voice (intents), create language models for detecting intents and the relevant information (parameters) from user utterances, and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app. Geno further supports references to GUI context in voice commands (e.g., "add this to the playlist"). In a study, developers with little NLP expertise were able to add the multi-modal support for two existing web apps using Geno.
SP  - 1169
EP  - 1181
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415848
ER  - 

TY  - NA
AU  - Kassel, Jan-Frederik; Rohs, Michael
TI  - CHI Extended Abstracts - Valletto: A Multimodal Interface for Ubiquitous Visual Analytics
PY  - 2018
AB  - Modern technologies enable data analysis in scenarios where keyboard and mouse are not available. Research on multimodality in visual analytics is facing this challenge. But existing approaches consider exclusively static environments with large screens. Therefore, we envision Valletto, a prototypical tablet app which allows the user to generate and specify visualizations through a speech-based conversational interface, through multitouch gestures, and through a conventional GUI interface. We conducted an initial expert evaluation to gain information on the modality function mapping and for the integration of different modalities. Our aim is to discuss design and interaction considerations in a mobile context which fits the user's daily life.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188445
ER  - 

TY  - NA
AU  - Darbar, Rajkumar; Sen, Prasanta Kr.; Samanta, Debasis
TI  - CHI Extended Abstracts - PressTact: Side Pressure-Based Input for Smartwatch Interaction
PY  - 2016
AB  - Smartwatches have gained a lot of public interest as one of the most popular wearable devices in recent times, but their diminutive touch screens mar the user experiences. The small screen of watch suffers from visual occlusion and the fat finger problem. To address these issues, we present PressTact that extends interaction space beyond the watch surface to the sides of the device. It augments smartwatches with four pressure sensors - two sensors on the left side of a watch and another two on the right side. It enables users to input different levels of pressure that can be used for bi-directional navigation (zooming, scrolling, rotation) on smartwatches. In this paper, we explore the pressure event based input vocabulary set. Our preliminary user study shows that participants can input different pressure levels (light press, medium press, and strong press) in discrete and continuous mode with an acceptable accuracy. Finally, we develop several example applications to illustrate the potential of the proposed technique.
SP  - 2431
EP  - 2438
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892436
ER  - 

TY  - JOUR
AU  - Narechania, Arpit; Srinivasan, Arjun; Stasko, John
TI  - NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries
PY  - 2021
AB  - Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.
SP  - 369
EP  - 379
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030378
ER  - 

TY  - JOUR
AU  - Besançon, Lonni; Ynnerman, Anders; Keefe, Daniel F.; Yu, Lingyun; Isenberg, Tobias
TI  - The State of the Art of Spatial Interfaces for 3D Visualization
PY  - 2021
AB  - We survey the state of the art of spatial interfaces for 3D visualization. Interaction techniques are crucial to data visualization processes and the visualization research community has been calling for more research on interaction for years. Yet, research papers focusing on interaction techniques, in particular for 3D visualization purposes, are not always published in visualization venues, sometimes making it challenging to synthesize the latest interaction and visualization results. We therefore introduce a taxonomy of interaction technique for 3D visualization. The taxonomy is organized along two axes: the primary source of input on the one hand and the visualization task they support on the other hand. Surveying the state of the art allows us to highlight specific challenges and missed opportunities for research in 3D visualization. In particular, we call for additional research in: (1) controlling 3D visualization widgets to help scientists better understand their data, (2) 3D interaction techniques for dissemination, which are under-explored yet show great promise for helping museum and science centers in their mission to share recent knowledge, and (3) developing new measures that move beyond traditional time and errors metrics for evaluating visualizations that include spatial interaction.
SP  - 293
EP  - 326
JF  - Computer Graphics Forum
VL  - 40
IS  - 1
PB  - 
DO  - 10.1111/cgf.14189
ER  - 

TY  - NA
AU  - Müller, Florian; Günther, Sebastian; Dezfuli, Niloofar; Khalilbeigi, Mohammadreza; Mühlhäuser, Max
TI  - CHI Extended Abstracts - ProxiWatch: Enhancing Smartwatch Interaction through Proximity-based Hand Input
PY  - 2016
AB  - Smartwatches allow ubiquitous and mobile interaction with digital contents. Because of the small screen sizes, traditional interaction techniques are often not applicable. In this work, we show how the degree of freedom offered by the elbow joint, i.e., flexion and extension, can be leveraged as an additional one-handed input modality for smartwatches. By moving the watch towards or away from the body, the user is able to provide input to the smartwatch without a second hand. We present the results of a controlled experiment focusing on the human capabilities for proximity-based interaction. Based on the results, we propose guidelines for designing proximity-based smartwatch interfaces and present ProxiWatch: a one-handed and proximity-based input modality for smartwatches alongside a prototypical implementation.
SP  - 2617
EP  - 2624
JF  - Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2851581.2892450
ER  - 

TY  - CHAP
AU  - Di Fuccio, Raffaele; Siano, Giovanni; De Marco, Antonio
TI  - WorldCIST (2) - TriPOD: A Prototypal System for the Recognition of Capacitive Widget on Touchscreen Addressed for Montessori-Like Educational Applications
PY  - 2017
AB  - TriPOD is a hardware/software prototypal in the field of the Tangible User Interfaces. The proposed system is able to recognize physical objects that could be directly placed on the screen of the common tablet/smartphone. The authors propose an algorithm for the recognition of the tangible objects, equipped by three capacitive pins, usable on commercial touchscreens (tablet or smartphone). TriPOD software is an APP that connects the tablet feedbacks with a main controller (i.e. a PC) with a Wi-Fi connection. The prototype is addressed for educational applications and it is designed in order to exploit the central role of the manipulation and the multisensory approach in education, then to be applied with Montessori-like exercises. The TriPOD system recognizes 24 objects, the Dienes’ logic blocks, allowing the usage with tablet’s touchscreen as an active board for the didactic games.
SP  - 664
EP  - 676
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-56538-5_68
ER  - 

TY  - NA
AU  - Mazumder, Sahisnu; Riva, Oriana
TI  - FLIN: A Flexible Natural Language Interface for Web Navigation
PY  - 2020
AB  - AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Leigh, Sang-won; Denton, Timothy; Parekh, Kush; Peebles, William; Johnson, Magnus H.; Maes, Pattie
TI  - Tangible and Embedded Interaction - Morphology Extension Kit: A Modular Robotic Platform for Physically Reconfigurable Wearables
PY  - 2018
AB  - Various forms of wearable robotics challenge the notion of the human body, in that the robots render the acquired capabilities in physical forms. However, majority of such systems are designed for specific purposes, where rapidly changing environments pose a diverse set of problems that are difficult to solve with a single interface. To address this, we propose a modular hardware platform that allows its users or designers to build and customize wearable robots. The process of building an augmentation is simply to connect actuator and sensor blocks and attach them to the body. The current list of designed components includes servomotor modules and sensor modules, that can be programmed to incorporate additional electronics for desired sensing capabilities. Our electrical and mechanical connector designs can be extended to utilize any motors within afforded power, size, and weight constraints. We also show how our platform can be used in various applications, in addition to how the proposed design can be extended as well as challenges for future systems.
SP  - 11
EP  - 18
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173239
ER  - 

TY  - CHAP
AU  - Mei, Chao; Yang, Yifan; Xu, Yi
TI  - SightX: A 3D Selection Technique for XR
PY  - 2022
AB  - AbstractMany 3D Interaction techniques use virtual ray-casting for general selection and manipulation tasks. Some virtual ray-casting techniques are equipped with gadgets to fit specific interaction scenarios, such as selecting an object with special shapes, or a part of an object. These techniques are intuitive and largely successful. However, there are still some common situations under which the capabilities of virtual ray-casting are limited. When a user needs to select an object that is far away and small, the movement of the ray on the far end greatly amplifies hand movement of the user, which in turn results in inefficient and inaccurate selection operations. Moreover, in an Extended Reality (XR) space, especially in an Augmented Reality (AR) space, where a user’s ability of performing teleportation or shifting to another view is limited, selecting an object that is occluded or from a cluster of objects with high density would be less flexible. We developed and evaluated SightX, a virtual ray-casting mechanism augmented with a remote anchor and a sight view with see-through capability for XR interactions. Our user studies suggested this design can significantly improve the performances and user experiences over the standard virtual ray-casting for 3D objects selection tasks. KeywordsHuman-centered computing3D user interfaces
SP  - 22
EP  - 35
JF  - Virtual Reality and Mixed Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-16234-3_2
ER  - 

TY  - NA
AU  - Lei, Yuxuan; Lu, Qi; Xu, Yingqing
TI  - O&O: A DIY toolkit for designing and rapid prototyping olfactory interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502033
ER  - 

TY  - JOUR
AU  - Xiong, Cindy; Setlur, Vidya; Bach, Benjamin; Lin, Kylie; Koh, Eunyee; Franconeri, Steven
TI  - Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways.
PY  - 2021
AB  - Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the "right" takeaway.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2021.3114823
ER  - 

TY  - CHAP
AU  - Wasser, Joscha; Bloch, Marten; Bielecki, Konrad; Vorst, Daria; Lopez, Daniel; Baltzer, Marcel; Flemisch, Frank
TI  - Gaze Based Interaction for Object Classification in Reconnaissance Missions Using Highly Automated Platforms
PY  - 2021
AB  - A gaze based interface was used as the basis for a interaction system supporting a remote operator of a ground reconnaissance robot when classifying objects previously identified by a software. Using the gaze point as a cursor, a pre-selection of the object in the sightline is made, allowing the user to effect the symbol via a haptic interface. An extension of the system enables the user to manually re-adjust the pre-selection if highlights are overlapping. The results from a first exploration were encouraging; the system was easily understood and used but does require further testing with scenarios that are more complex.
SP  - 836
EP  - 842
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-68017-6_124
ER  - 

TY  - BOOK
AU  - Menges, Raphael; Kumar, Chandan; Müller, Daniel; Sengupta, Korok
TI  - W4A - GazeTheWeb: A Gaze-Controlled Web Browser
PY  - 2017
AB  - Web is essential for most people, and its accessibility should not be limited to conventional input sources like mouse and keyboard. In recent years, eye tracking systems have greatly improved, beginning to play an important role as input medium. In this work, we present GazeTheWeb, a Web browser accessible solely by eye gaze input. It effectively supports all browsing operations like search, navigation and bookmarks. GazeTheWeb is based on a Chromium powered framework, comprising Web extraction to classify interactive elements, and application of gaze interaction paradigms to represent these elements.
SP  - 25
EP  - NA
JF  - Proceedings of the 14th International Web for All Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3058555.3058582
ER  - 

TY  - JOUR
AU  - Zhang, Yang; Mysore, Gautham J.; Berthouzoz, Floraine; Hasegawa-Johnson, Mark
TI  - Analysis of prosody increment induced by pitch accents for automatic emphasis correction
PY  - 2016
AB  - NA
SP  - 79
EP  - 83
JF  - Speech Prosody 2016
VL  - NA
IS  - NA
PB  - 
DO  - 10.21437/speechprosody.2016-17
ER  - 

TY  - CONF
AU  - Srinivasan, Arjun; Stasko, John
TI  - EuroVis (Short Papers) - Natural language interfaces for data analysis with visualization: considering what has and could be asked
PY  - NA
AB  - Natural language is emerging as a promising interaction paradigm for data analysis with visualization. Designing and implementing Natural Language Interfaces (NLIs) is a challenging task, however. In addition to being able to process and understand natural language expressions, NLIs for data visuailzation must consider other factors including input modalities, providing input affordances, and explaining system results, among others. In this article, we examine existing NLIs for data analysis with visualization, and compare and contrast them based on the tasks they allow people to perform. We discuss open research opportunities and themes for emerging NLIs in the visualization community. We also provide examples from the existing literature in the broader HCI community that may help explore some of the highlighted themes for future work. Our goal is to assist readers to understand the subtleties and challenges in designing NLIs and encourage the community to think further about NLIs for data analysis with visualization.
SP  - 55
EP  - 59
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.2312/eurovisshort.20171133
ER  - 

TY  - CONF
AU  - Srinivasan, Arjun; Nyapathy, Nikhila; Lee, Bongshin; Drucker, Steven M.; Stasko, John
TI  - CHI - Collecting and Characterizing Natural Language Utterances for Specifying Data Visualizations
PY  - 2021
AB  - Natural language interfaces (NLIs) for data visualization are becoming increasingly popular both in academic research and in commercial software. Yet, there is a lack of empirical understanding of how people specify visualizations through natural language. We conducted an online study (N = 102), showing participants a series of visualizations and asking them to provide utterances they would pose to generate the displayed charts. From the responses, we curated a dataset of 893 utterances and characterized the utterances according to (1) their phrasing (e.g., commands, queries, questions) and (2) the information they contained (e.g., chart types, data aggregations). To help guide future research and development, we contribute this utterance dataset and discuss its applications toward the creation and benchmarking of NLIs for visualization.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Luo, Yuyu; Tang, Nan; Li, Guoliang; Tang, Jiawei; Chai, Chengliang; Qin, Xuedi
TI  - Natural Language to Visualization by Neural Machine Translation.
PY  - 2021
AB  - Supporting the translation from natural language (NL) query to visualization (NL2VIS) can simplify the creation of data visualizations because if successful, anyone can generate visualizations by their natural language from the tabular data. The stateof-the-art NL2VIS approaches (e.g., NL4DV and FlowSense) are based on semantic parsers and heuristic algorithms, which are not end-to-end and are not designed for supporting (possibly) complex data transformations. Deep neural network powered neural machine translation models have made great strides in many machine translation tasks, which suggests that they might be viable for NL2VIS as well. In this paper, we present ncNet, a Transformer-based sequence-to-sequence model for supporting NL2VIS, with several novel visualization-aware optimizations, including using attention-forcing to optimize the learning process, and visualization-aware rendering to produce better visualization results. To enhance the capability of machine to comprehend natural language queries, ncNet is also designed to take an optional chart template (e.g., a pie chart or a scatter plot) as an additional input, where the chart template will be served as a constraint to limit what could be visualized. We conducted both quantitative evaluation and user study, showing that ncNet achieves good accuracy in the nvBench benchmark and is easy-to-use.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2021.3114848
ER  - 

TY  - NA
AU  - Setlur, Vidya; Tory, Melanie
TI  - How do you Converse with an Analytical Chatbot? Revisiting Gricean Maxims for Designing Analytical Conversational Behavior
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501972
ER  - 

TY  - NA
AU  - Chung, Jungmin; Oh, Changhoon; Park, So-Hyun; Suh, Bongwon
TI  - CHI Extended Abstracts - PairRing: A Ring-Shaped Rotatable Smartwatch Controller
PY  - 2018
AB  - Smartwatch users often report usability problems despite its handiness and convenience. Fingers often block the screen, and both hands are often required to operate the gadget. To address these issues, we present PairRing, a ring-shaped rotatable smartwatch controller. PairRing allows users to scroll up and down listed items by turning the ring on their index finger with their thumb. To determine the optimal ring shape and rotation speed, we designed and conducted a user study and report the results. We found that (1) users could perform tasks better with the angular ring prototype; (2) the rapid rotation speed was better suited for browsing ordered lists; and (3) overall, participants were positive about the feasibility of the prototype. We conclude with a discussion on the design implications of PairRing as well as its future applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188590
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Wong, Andrew; Qi, Yini; Blumberg, Lotta-Gili; Kim, Yoonji; Mueller, Stefanie
TI  - Conference on Designing Interactive Systems - Adapt2Learn: A Toolkit for Configuring the Learning Algorithm for Adaptive Physical Tools for Motor-Skill Learning
PY  - 2021
AB  - A recent study on motor-skill training showed that adaptive training tools that use shape-change to adapt the training difficulty based on learners’ performance can lead to higher learning gains. However, to date, no support tools exist to help designers create adaptive learning tools. Our formative study shows that developing the adaptive learning algorithm poses a particular challenge. To address this, we built Adapt2Learn, a toolkit that auto-generates the learning algorithm for adaptive tools. Designers choose their tool’s sensors and actuators, Adapt2Learn then configures the learning algorithm and generates a microcontroller script that designers can deploy on the tool. Once uploaded, the script assesses the learner’s performance via the sensors, computes the training difficulty, and actuates the tool to adapt the difficulty. Adapt2Learn’s visualization tool then lets designers visualize their tool’s adaptation and evaluate the learning algorithm. To validate that Adapt2Learn can generate adaptation algorithms for different tools, we built several application examples that demonstrate successful deployment.
SP  - 1301
EP  - 1312
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462128
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Lee, Juyoung; Bianchi, Andrea; Quigley, Aaron
TI  - MobileHCI - WatchMI: pressure touch, twist and pan gesture input on unmodified smartwatches
PY  - 2016
AB  - The screen size of a smartwatch provides limited space to enable expressive multi-touch input, resulting in a markedly difficult and limited experience. We present WatchMI: Watch Movement Input that enhances touch interaction on a smartwatch to support continuous pressure touch, twist, pan gestures and their combinations. Our novel approach relies on software that analyzes, in real-time, the data from a built-in Inertial Measurement Unit (IMU) in order to determine with great accuracy and different levels of granularity the actions performed by the user, without requiring additional hardware or modification of the watch. We report the results of an evaluation with the system, and demonstrate that the three proposed input interfaces are accurate, noise-resistant, easy to use and can be deployed on a variety of smartwatches. We then showcase the potential of this work with seven different applications including, map navigation, an alarm clock, a music player, pan gesture recognition, text entry, file explorer and controlling remote devices or a game character.
SP  - 394
EP  - 399
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2935334.2935375
ER  - 

TY  - CONF
AU  - Srinivasan, Arjun; Lee, Bongshin; Stasko, John
TI  - Facilitating Spreadsheet Manipulation on Mobile Devices Leveraging Speech
PY  - 2018
AB  - Due to the prevalent use of spreadsheet applications, their user interface on a desktop has evolved significantly over time. However, user interfaces for spreadsheet applications on mobile devices are still in a nascent stage and are typically “ported” versions of their desktop counterparts. In this article, we present a simple tabular data manipulation scenario on mobile devices, leveraging speech input. We describe ways to complement direct manipulation-based input via touch or pen with minimalistic speech-based input. We also briefly discuss an open area for future research, where we could explore the role of speech input for interacting with spreadsheets along the spectrum of command-like utterances to a conversational dialog.
SP  - 1
EP  - 6
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Tabalba, Roderick; Kirshenbaum, Nurit; Leigh, Jason; Bhatacharya, Abari; Johnson, Andrew; Grosso, Veronica; Di Eugenio, Barbara; Zellner, Moira
TI  - Articulate+ : An Always-Listening Natural Language Interface for Creating Data Visualizations
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 4th Conference on Conversational User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543829.3544534
ER  - 

TY  - NA
AU  - Chaturvedi, Sumit; Lukáč, Michal; Chaudhuri, Siddhartha
TI  - ReGroup: Recursive Neural Networks for Hierarchical Grouping of Vector Graphic Primitives
PY  - 2021
AB  - Selection functionality is as fundamental to vector graphics as it is for raster data. But vector selection is quite different: instead of pixel-level labeling, we make a binary decision to include or exclude each vector primitive. In the absence of intelligible metadata, this becomes a perceptual grouping problem. These have previously relied on heuristics derived from empirical principles like Gestalt Theory, but since these are ill-defined and subjective, they often result in ambiguity. Here we take a data-centric approach to the problem. By exploiting the recursive nature of perceptual grouping, we interpret the task as constructing a hierarchy over the primitives of a vector graphic, which is amenable to learning with recursive neural networks with few human annotations. We verify this by building a dataset of these hierarchies on which we train a hierarchical grouping network. We then demonstrate how this can underpin a prototype selection tool.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Liu, Can; Han, Yun; Jiang, Ruike; Yuan, Xiaoru
TI  - PacificVis - ADVISor: Automatic Visualization Answer for Natural-Language Question on Tabular Data
PY  - 2021
AB  - We propose an automatic pipeline to generate visualization with annotations to answer natural-language questions raised by the public on tabular data. With a pre-trained language representation model, the input natural language questions and table headers are first encoded into vectors. According to these vectors, a multi-task end-to-end deep neural network extracts related data areas and corresponding aggregation type. We present the result with carefully designed visualization and annotations for different attribute types and tasks. We conducted a comparison experiment with state-of-the-art works and the best commercial tools. The results show that our method outperforms those works with higher accuracy and more effective visualization.
SP  - 11
EP  - 20
JF  - 2021 IEEE 14th Pacific Visualization Symposium (PacificVis)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/pacificvis52677.2021.00010
ER  - 

TY  - NA
AU  - Wang, Xingbo; Zeng, Haipeng; Wang, Yong; Wu, Aoyu; Sun, Zhida; Ma, Xiaojuan; Qu, Huamin
TI  - VoiceCoach: Interactive Evidence-based Training for Voice Modulation Skills in Public Speaking
PY  - 2020
AB  - The modulation of voice properties, such as pitch, volume, and speed, is crucial for delivering a successful public speech. However, it is challenging to master different voice modulation skills. Though many guidelines are available, they are often not practical enough to be applied in different public speaking situations, especially for novice speakers. We present VoiceCoach, an interactive evidence-based approach to facilitate the effective training of voice modulation skills. Specifically, we have analyzed the voice modulation skills from 2623 high-quality speeches (i.e., TED Talks) and use them as the benchmark dataset. Given a voice input, VoiceCoach automatically recommends good voice modulation examples from the dataset based on the similarity of both sentence structures and voice modulation skills. Immediate and quantitative visual feedback is provided to guide further improvement. The expert interviews and the user study provide support for the effectiveness and usability of VoiceCoach.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376726
ER  - 

TY  - NA
AU  - Chen, Yuan; Katsuragawa, Keiko; Lank, Edward
TI  - CHI - Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality
PY  - 2020
AB  - Personal smart devices have demonstrated a variety of efficient techniques for pointing and selecting on physical displays. However, when migrating these input techniques to augmented reality, it is both unclear what the relative performance of different techniques will be given the immersive nature of the environment, and it is unclear how viewport-based versus world-based pointing methods will impact performance. To better understand the impact of device and viewing perspectives on pointing in augmented reality, we present the results of two controlled experiments comparing pointing conditions that leverage various smartphone- and smartwatch-based external display pointing techniques and examine viewport-based versus world-based target acquisition paradigms. Our results demonstrate that viewport-based techniques offer faster selection and that both smartwatch- and smartphone-based pointing techniques represent high-performance options for performing distant target acquisition tasks in augmented reality.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376592
ER  - 

TY  - NA
AU  - Kassel, Jan-Frederik; Rohs, Michael
TI  - Conference on Designing Interactive Systems - Talk to Me Intelligibly: Investigating An Answer Space to Match the User's Language in Visual Analysis
PY  - 2019
AB  - Conversational interfaces (CIs) have the potential to empower a broader spectrum of users to independently conduct visual analysis. Yet, recent approaches do not fully consider the user's characteristics. In particular, the objective of matching the user's language has been understudied in visual analysis. In order to close this gap, we introduce an answer space motivated by Grice's cooperative principle for framing personalized communication in complex data situations. We conducted both an online survey (N=76) to analyze communication preferences and a qualitative experiment (N=10) to investigate personalized conversations with an existing CI. In order to match the user's language properly, our results suggest to consider additional user characteristics along with their knowledge level. While mismatching communication preferences triggers negative reactions, a preference-aligned communication evokes positive reactions. As our analysis confirms the importance of matching the user's language in visual analysis, we provide design implications for future CIs.
SP  - 1517
EP  - 1529
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322282
ER  - 

TY  - JOUR
AU  - Siddiqui, Tarique; Luh, Paul; Wang, Zesheng; Karahalios, Karrie; Parameswaran, Aditya
TI  - Shapesearch: flexible pattern-based querying of trend line visualizations
PY  - 2018
AB  - Finding visualizations with desired patterns is a common goal during data exploration. However, due to the limited expressiveness and flexibility of existing visual analytics systems, pattern-based querying of visualizations has largely been a manual process. We demonstrate ShapeSearch, a system that enables users to express their desired patterns in trend lines using multiple flexible mechanisms --- including natural language and visual regular expressions, and automates the search via an optimized execution engine. Internally, the system leverages an expressive shape query algebra that supports a range of operators and primitives for representing ShapeSearch queries. In our demonstration, conference attendees will learn how the various components of ShapeSearch help accelerate scientific discovery by automating the search for meaningful patterns in trend lines in domains such as genomics and material science.
SP  - 1962
EP  - 1965
JF  - Proceedings of the VLDB Endowment
VL  - 11
IS  - 12
PB  - 
DO  - 10.14778/3229863.3236235
ER  - 

TY  - NA
AU  - Xia, Haijun; Araujo, Bruno; Wigdor, Daniel
TI  - CHI - Collection Objects: Enabling Fluid Formation and Manipulation of Aggregate Selections
PY  - 2017
AB  - Despite the long development of Graphical User Interfaces, working with multiple graphical objects remains a challenge, due to the difficulties of forming complex selections, ambiguities of operations, and tediousness of repetitively unselect-reselect or ungroup-regroup objects. Instead of tackling them as individual problems, we attribute it to the lack of system support to the general selection-action cycles. We propose Collection Objects to not only support a single fast selection-action cycle but also allow multiple cycles to be chained together into a fluid workflow. Collection Objects unifies selection, grouping, and manipulation of aggregate selections into a single object, with which selection can be composed with various techniques, modified for later actions, grouped with objects inside still directly accessible, and quasi-moded for less context switching. We implemented Collection Object in the context of a vector drawing application with simultaneous pen and touch input. Results of an expert evaluation show that Collection Objects holds considerable promises for fluid interaction with multiple objects.
SP  - 5592
EP  - 5604
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025554
ER  - 

TY  - CHAP
AU  - Cavazza, Marc; Charles, Fred
TI  - User Interaction for Interactive Storytelling
PY  - 2016
AB  - NA
SP  - 415
EP  - 428
JF  - Handbook of Digital Games and Entertainment Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-4560-50-4_57
ER  - 

TY  - JOUR
AU  - Annett, Michelle; Grossman, Tovi; Wigdor, Daniel; Fitzmaurice, George
TI  - Exploring and Understanding the Role of Workshop Environments in Personal Fabrication Processes
PY  - 2019
AB  - Growing interest in personal fabrication has resulted in many ways to ideate, design, and prototype, in addition to studies of who a maker is and the challenges they face. Less attention, however, has focused on the role of the environment in fabrication processes. By understanding how interactions with tools, fixtures, materials, and spaces shape workflows, we can better determine how to design the next generation of workshops, design tools, and fabrication equipment to support personal fabrication activities. To build this understanding, site visits and interviews at local makerspaces, fabrication studios, and workshops were conducted. These visits uncovered the rich practices and roadblocks generated by workshops today. The observations identified the importance of spatial layouts, territoriality and occupant agency, distributed knowledge, and organizational flux, among others, to design and fabrication processes. These observations were further synthesized into one possible direction for such spaces: hybrid workshops (i.e., environments that can leverage computation and responsive architecture to enhance a maker's ability to design and fabricate). This work identifies how such spaces could harness the rich practices and eliminate the challenges found with workshops today and discusses the technical innovations and philosophical questions that hybrid workshops will pose to the future of personal fabrication.
SP  - 10
EP  - 43
JF  - ACM Transactions on Computer-Human Interaction
VL  - 26
IS  - 2
PB  - 
DO  - 10.1145/3301420
ER  - 

TY  - NA
AU  - Ishii, Akira; Shizuki, Buntarou
TI  - OZCHI - Exploring callout design in selection task for ultra-small touch screen devices
PY  - 2016
AB  - Ultra-small touch screen devices tend to suffer from occlusion or the fat finger problem owing to their limited input area. A callout could solve these problems by displaying a copy of the occluded area in a non-occluded area. However, callout designs for ultra-small touch screen devices have not yet been explored in depth. In this study, we chose three design factors (each factor has two levels) from various factors and conducted an experiment to examine eight callout designs in the selection task for ultra-small touch screen devices. The results of our experiment matched the results from previous research; however, we also obtained results unique to ultra-small devices. The results showed that the selection speed was higher when the content of the callout was changed continuously, the error rate decreased when the content of the callout was changed continuously and a pointer was displayed to indicate the touched position within the callout, and the workload decreased when the content was changed continuously. Further, as a design factor, the position of the callout would not affect the selection performance.
SP  - 426
EP  - 434
JF  - Proceedings of the 28th Australian Conference on Computer-Human Interaction - OzCHI '16
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3010915.3010922
ER  - 

TY  - NA
AU  - Xia, Haijun; Hinckley, Ken; Pahud, Michel; Tu, Xiao; Buxton, Bill
TI  - CHI - WritLarge: Ink Unleashed by Unified Scope, Action, & Zoom
PY  - 2017
AB  - WritLarge is a freeform canvas for early-stage design on electronic whiteboards with pen+touch input. The system aims to support a higher-level flow of interaction by 'chunking' the traditionally disjoint steps of selection and action into unified selection-action phrases. This holistic goal led us to address two complementary aspects: SELECTION, for which we devise a new technique known as the Zoom-Catcher that integrates pinch-to-zoom and selection in a single gesture for fluidly selecting and acting on content; plus: ACTION, where we demonstrate how this addresses the combined issues of navigating, selecting, and manipulating content. In particular, the designer can transform select ink strokes in flexible and easily-reversible representations via semantic, structural, and temporal axes of movement that are defined as conceptual 'moves' relative to the specified content. This approach dovetails zooming with lightweight specification of scope as well as the evocation of context-appropriate commands, at-hand, in a location-independent manner. This establishes powerful new primitives that can help to scaffold higher-level tasks, thereby unleashing the expressive power of ink in a compelling manner.
SP  - 3227
EP  - 3240
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025664
ER  - 

TY  - JOUR
AU  - Gao, BoYu; Kim, Hyung-Seok; Lee, Hasup; Lee, Jooyoung; Kim, Jee-In
TI  - Effects of Continuous Auditory Feedback on Drawing Trajectory-Based Finger Gestures
PY  - 2018
AB  - The well-known “fat finger” issue limits the interaction performance of trajectory-based finger gestures. To alleviate this issue, this work focuses on the possibility of using additional continuous auditory feedback to assist trajectory-based finger gestures. First, the experiment validated that, with the visual feedback only, the bare fingertip led to more errors in drawing of intersectional points, endpoints of closed gestures, and gestural length and shape variability compared to when the finger-attached pen was used. Then, we designed different types of auditory feedback (discrete beep, static, gradual) to provide additional information on the spatial relationship between finger-contact point and the endpoints or intersections of predefined gestures. An experiment that evaluates the effects of individual or combination of designed auditory feedback on trajectory-based finger gestures was conducted. These results show a few differences between them. However, a combination of gradual (amplitude and frequency) continuous sound and beep reached the highest drawing accuracy for trajectory-based finger gestures, which is similar to that of a finger-attached pen. This research offers insights and implications for the future design of continuous auditory feedback on small touchscreens.
SP  - 658
EP  - 669
JF  - IEEE Transactions on Human-Machine Systems
VL  - 48
IS  - 6
PB  - 
DO  - 10.1109/thms.2018.2850329
ER  - 

TY  - NA
AU  - Zhang, Haoci; Raj, Viraj; Sellam, Thibault; Wu, Eugene
TI  - SIGMOD Conference - Precision Interfaces for Different Modalities
PY  - 2018
AB  - Building interactive tools to support data analysis is hard because it is not always clear what to build and how to build it. To address this problem, we present Precision Interfaces, a semi-automatic system to generate task-specific data analytics interfaces. Precision Interface can turn a log of executed programs into an interface, by identifying micro-variations between the programs and mapping them to interface components. This paper focuses on SQL query logs, but we can generalize the approach to other languages. Our system operates in two steps: it first builds an interaction graph, which describes how the queries can be transformed into each other. Then, it finds a set of UI components that covers a maximal number of transformations. To restrict the domain of changes to be detected, our system uses a domain-specific language, PILang. We describe each of Precision Interface's components, showcase an early prototype on real program logs, and discuss future research opportunities. This demonstration highlights the potential for data-driven interactive interface mining from query logs. We will first walk participants through the process that Precision Interfaces goes through to generate interactive analysis interfaces from query logs. We will then show the versatility of Precision Interfaces by letting participants choose from multiple different interface modalities, interaction designs, and query logs to generate 2D point-and-click, gestural, and even natural language analysis interfaces for commonly performed analyses.
SP  - 1777
EP  - 1780
JF  - Proceedings of the 2018 International Conference on Management of Data
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3183713.3193570
ER  - 

TY  - JOUR
AU  - Fu, Yunfa; Chen, Rui; Gong, Anmin; Qian, Qian; Ding, Ning; Zhang, Wei; Su, Lei; Zhao, Lei
TI  - Recognition of Flexion and Extension Imagery Involving the Right and Left Arms Based on Deep Belief Network and Functional Near-Infrared Spectroscopy.
PY  - 2021
AB  - Brain-computer interaction based on motor imagery (MI) is an important brain-computer interface (BCI). Most methods for MI classification are based on electroencephalogram (EEG), and few studies have investigated signal processing based on MI-Functional Near-Infrared Spectroscopy (fNIRS). In addition, there is a need to improve the classification accuracy for MI fNIRS methods. In this study, a deep belief network (DBN) based on a restricted Boltzmann machine (RBM) was used to classify fNIRS signals of flexion and extension imagery involving the left and right arms. fNIRS signals from 16 channels covering the motor cortex area were recorded for each of 10 subjects executing or imagining flexion and extension involving the left and right arms. Oxygenated hemoglobin (HbO) concentration was used as a feature to train two RBMs that were subsequently stacked with an additional softmax regression output layer to construct DBN. We also explored the DBN model classification accuracy for the test dataset from one subject using training dataset from other subjects. The average DBN classification accuracy for flexion and extension movement and imagery involving the left and right arms was 84.35 ± 3.86% and 78.19 ± 3.73%, respectively. For a given DBN model, better classification results are obtained for test datasets for a given subject when the model is trained using dataset from the same subject than when the model is trained using datasets from other subjects. The results show that the DBN algorithm can effectively identify flexion and extension imagery involving the right and left arms using fNIRS. This study is expected to serve as a reference for constructing online MI-BCI systems based on DBN and fNIRS.
SP  - 5533565
EP  - 5533565
JF  - Journal of healthcare engineering
VL  - 2021
IS  - NA
PB  - 
DO  - 10.1155/2021/5533565
ER  - 

TY  - JOUR
AU  - Desideri, Lorenzo; Tarabelloni, Giulia; Nanni, Ivan; Malavasi, Massimiliano; Nori, Raffaella; Bonifacci, Paola
TI  - An eye-controlled version of the Kaufman Brief Intelligence Test 2 (KBIT-2) to assess cognitive functioning
PY  - 2016
AB  - NA
SP  - 502
EP  - 508
JF  - Computers in Human Behavior
VL  - 63
IS  - NA
PB  - 
DO  - 10.1016/j.chb.2016.05.077
ER  - 

TY  - NA
AU  - Lee, Bongshin; Srinivasan, Arjun; Stasko, John; Tory, Melanie; Setlur, Vidya
TI  - AVI - Multimodal interaction for data visualization
PY  - 2018
AB  - Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.
SP  - 11
EP  - NA
JF  - Proceedings of the 2018 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206505.3206602
ER  - 

TY  - NA
AU  - Mayer, Sven; Xu, Xiangyu; Harrison, Chris
TI  - CHI - Super-Resolution Capacitive Touchscreens
PY  - 2021
AB  - Capacitive touchscreens are near-ubiquitous in today’s touch-driven devices, such as smartphones and tablets. By using rows and columns of electrodes, specialized touch controllers are able to capture a 2D image of capacitance at the surface of a screen. For over a decade, capacitive “pixels” have been around 4 millimeters in size – a surprisingly low resolution that precludes a wide range of interesting applications. In this paper, we show how super-resolution techniques, long used in fields such as biology and astronomy, can be applied to capacitive touchscreen data. By integrating data from many frames, our software-only process is able to resolve geometric details finer than the original sensor resolution. This opens the door to passive tangibles with higher-density fiducials and also recognition of every-day metal objects, such as keys and coins. We built several applications to illustrate the potential of our approach and report the findings of a multipart evaluation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445703
ER  - 

TY  - JOUR
AU  - Buschek, Daniel; Kinshofer, Julia; Alt, Florian
TI  - A Comparative Evaluation of Spatial Targeting Behaviour Patterns for Finger and Stylus Tapping on Mobile Touchscreen Devices
PY  - 2018
AB  - Models of 2D targeting error patterns have been applied as a valuable computational tool for analysing finger touch behaviour on mobile devices, improving touch accuracy and inferring context. However, their use in stylus input is yet unexplored. This paper presents the first empirical study and analyses of such models for tapping with a stylus. In a user study (N = 28), we collected targeting data on a smartphone, both for stationary use (sitting) and walking. We compare targeting patterns between index finger input and three stylus variations -- two stylus widths and nib types as well as the addition of a hover cursor. Our analyses reveal that stylus targeting patterns are user-specific, and that offset models improve stylus tapping accuracy, but less so than for finger touch. Input method has a stronger influence on targeting patterns than mobility, and stylus width is more influential than the hover cursor. Stylus models improve finger accuracy as well, but not vice versa. The extent of the stylus accuracy advantage compared to the finger depends on screen location and mobility. We also discuss patterns related to mobility and gliding of the stylus on the screen. We conclude with implications for target sizes and offset model applications.
SP  - 126
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161160
ER  - 

TY  - NA
AU  - Aksan, Emre; Pece, Fabrizio; Hilliges, Otmar
TI  - CHI - DeepWriting: Making Digital Ink Editable via Deep Generative Modeling
PY  - 2018
AB  - Digital ink promises to combine the flexibility and aesthetics of handwriting and the ability to process, search and edit digital text. Character recognition converts handwritten text into a digital representation, albeit at the cost of losing personalized appearance due to the technical difficulties of separating the interwoven components of content and style. In this paper, we propose a novel generative neural network architecture that is capable of disentangling style from content and thus making digital ink editable. Our model can synthesize arbitrary text, while giving users control over the visual appearance (style). For example, allowing for style transfer without changing the content, editing of digital ink at the word level and other application scenarios such as spell-checking and correction of handwritten text. We furthermore contribute a new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.
SP  - 205
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173779
ER  - 

TY  - NA
AU  - Wu, Aoyu; Wang, Yun; Zhou, Mengyu; He, Xinyi; Zhang, Haidong; Qu, Huamin; Zhang, Dongmei
TI  - MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation
PY  - 2021
AB  - We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the need of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Lankes, Michael; Haslinger, Andreas; Wolff, Christian
TI  - gEYEded: Subtle and Challenging Gaze-Based Player Guidance in Exploration Games
PY  - 2019
AB  - This paper investigates the effects of gaze-based player guidance on the perceived game experience, performance, and challenge in a first-person exploration game. In contrast to existing research, the proposed approach takes the game context into account by providing players not only with guidance but also granting them an engaging game experience with a focus on exploration. This is achieved by incorporating gaze-sensitive areas that indicate the location of relevant game objects. A comparative study was carried out to validate our concept and to examine if a game supported with a gaze guidance feature triggers a more immersive game experience in comparison to a crosshair guidance version and a solution without any guidance support. In general, our study findings reveal a more positive impact of the gaze-based guidance approach on the experience and performance in comparison to the other two conditions. However, subjects had a similar impression concerning the game challenge in all conditions.
SP  - 61
EP  - NA
JF  - Multimodal Technologies and Interaction
VL  - 3
IS  - 3
PB  - 
DO  - 10.3390/mti3030061
ER  - 

TY  - NA
AU  - Pruszko, Laura; Laurillau, Yann; Piranda, Benoît; Bourgeois, Julien; Coutrix, Céline
TI  - ICMI - Impact of the Size of Modules on Target Acquisition and Pursuit for Future Modular Shape-changing Physical User Interfaces
PY  - 2021
AB  - Shape-changing User Interfaces (UIs) explore the ability of a UI to change its physical shape to support multiple interaction modalities for users’ input and/or system’s output. An approach currently studied to implement such interfaces at a high resolution is based on mm-sized, round, and self-actuated modules. The problem we tackle in this paper is to find the range of usable sizes of such modules, to better inform the trade-off between usability and technological feasibility. We assessed four sliders in a controlled user study: a standard slider and three sliders made of mock-up rounded modules of 1 mm, 2.5 mm, and 5 mm. Experimental results show that (1) 5 mm modules significantly impair performance for the pursuit task and subjective perception for both tasks, (2) performance increases when the size of modules decreases, but (3) users reportedly enjoyed the haptic feedback provided by 1 mm to 2.5 mm modules. These results provide deeper understanding on the impact of the size of modules on performance and subjective perception to inform current technological development of physical user interfaces made of small robotic modules.
SP  - 297
EP  - 307
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3479936
ER  - 

TY  - NA
AU  - Robertson, Samantha; Harley, Kim G.; Salehi, Niloufar
TI  - Opportunities and risks for engaging research participants with self-logged menstrual health data
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the Workshop on Human-In-the-Loop Data Analytics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546930.3547501
ER  - 

TY  - NA
AU  - Vasquez, Eldy S. Lazaro
TI  - Tangible and Embedded Interaction - Auto-adjustable Bra for Women with a Pronounced Alteration in Breast Volume
PY  - 2019
AB  - The auto-adjustable bra combines new technologies such as soft robotics, computational design, and e-textiles to develop a bra which uses a pneumatic system to compensate severe asymmetries in breast volume (Anisomastia). In the present work, the bra aims to adjust to the measurements of a woman's breasts through air channels which are located in the internal mesh of the bra cup. This inflatable structure gives a balance in the breasts' volume while holding them. Furthermore, the conductive fabric which covers the bra cup works as a sensor to control the air injection system by comparing the fabric's resistance in both bra cups to send a signal to the air pump to stop the air injection. Thus, the bra keeps in shape and the air pump is disengaged. The project could have a global impact on women with Anisomastia by raising their self-esteem, recovering their emotional balance, and possibly enhancing their social and sexual relationships.
SP  - 429
EP  - 435
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3300982
ER  - 

TY  - NA
AU  - Wicaksono, Irmandy; Rozendo, Caroline; Ye, Runzhou; Trapp, Jaleesa; Bove, V. Michael; Dagdeviren, Canan; Ishii, Hiroshi
TI  - CHI Extended Abstracts - PerForm: Deformable Interface for Exploring Sound through Shapes
PY  - 2018
AB  - UPDATED-22 February 2018. In this paper, we describe a novel electronic musical interface, consisting of a strand-like object that can be physically transformed by bending to create various shapes and signifiers. Users are encouraged to explore a visual language in a musical context, as each new shape portrays a different musical instrument with unique sonic behavior. We apply concepts from research in the area of cross-modal perception as guidance for mapping shapes and signifiers to corresponding sounds.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188478
ER  - 

TY  - NA
AU  - Kovacs, Robert; Wall, Ludwig; Seufert, Anna; Chen, Hsiang-Ting; Müller, Willi; Meinel, Florian; Kommana, Yannis; Bläsius, Thomas; Schneider, Oliver; Roumen, Thijs; Baudisch, Patrick
TI  - UIST (Adjunct Volume) - Demonstrating TrussFab's Editor: Designing Sturdy Large-Scale Structures
PY  - 2017
AB  - We demonstrate TrussFab's editor for creating large-scale structures that are sturdy enough to carry human weight. TrussFab achieves the large scale by using plastic bottles as beams that form structurally sound node-link structures, also known as trusses, allowing it to handle the forces resulting from scale and load. During this hands-on demo at UIST, attendees will use the TrussFab software to design their own structures, validate their design using integrated structural analysis, and export their designs for 3D printing.
SP  - 43
EP  - 45
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131802
ER  - 

TY  - JOUR
AU  - Parameswaran, Aditya
TI  - Enabling data science for the majority
PY  - 2019
AB  - Despite great strides in the generation, collection, and processing of data at scale, data science is still extremely inconvenient for the vast majority of the population. The driving goal of our research, over the past half decade, has been to make it easy for individuals and teams---regardless of programming or analysis expertise---manage, analyze, make sense of, and draw insights from large datasets. In this article, we reflect on a comprehensive suite of tools that we've been building to empower everyone to perform data science more efficiently and effortlessly, including DataSpread, a scalable spreadsheet tool that combines the benefits of spreadsheets and databases, and ZenVisage, a visual exploration tool that accelerates the discovery of trends or patterns. Our tools have been developed in collaboration with experts in various disciplines, including neuroscience, battery science, genomics, astrophysics, and ad analytics. We will discuss some of the key technical challenges underlying the development of these tools, and how we addressed them, drawing from ideas in multiple disciplines. in the process, we will outline a research agenda for tool development to empower everyone to tap into the hidden potential in their datasets at scale.
SP  - 2309
EP  - 2322
JF  - Proceedings of the VLDB Endowment
VL  - 12
IS  - 12
PB  - 
DO  - 10.14778/3352063.3352148
ER  - 

TY  - NA
AU  - Jacobs, Jennifer
TI  - Dynamic drawing : broadening practice and participation in procedural art
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Campos, Cuauhtli; Sandak, Jakub; Kljun, Matjaž; Čopič Pucihar, Klen
TI  - The Hybrid Stylus: A Multi-Surface Active Stylus for Interacting with and Handwriting on Paper, Tabletop Display or Both.
PY  - 2022
AB  - The distinct properties and affordances of paper provide benefits that enabled paper to maintain an important role in the digital age. This is so much so, that some pen-paper interaction has been imitated in the digital world with touchscreens and stylus pens. Because digital medium also provides several advantages not available to physical paper, there is a clear benefit to merge the two mediums. Despite the plethora of concepts, prototypes and systems to digitise handwritten information on paper, these systems require specially prepared paper, complex setups and software, which can be used solely in combination with paper, and, most importantly, do not support the concurrent precise interaction with both mediums (paper and touchscreen) using one pen only. In this paper, we present the design, fabrication and evaluation of the <i>Hybrid Stylus.</i> The <i>Hybrid Stylus</i> is assembled with the infinity pencil tip (nib) made of graphite and a specially designed shielded tip holder that is attached to an active stylus. The stylus can be used for writing on a physical paper, while it still maintains all the features needed for tablet interaction. Moreover, the stylus also allows simultaneous digitisation of handwritten information on the paper when the paper is placed on the tablet screen. In order to evaluate the concept, we also add a user-friendly manual alignment of paper position on the underlying tablet computer The evaluation demonstrates that the system achieves almost perfect digitisation of strokes (98.6% of strokes were correctly registered with only 1.2% of ghost strokes) whilst maintaining excellent user experience of writing with a pencil on the paper.
SP  - 7058
EP  - 7058
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 18
PB  - 
DO  - 10.3390/s22187058
ER  - 

TY  - JOUR
AU  - Glauser, Oliver; Ma, Wan-Chun; Panozzo, Daniele; Jacobson, Alec; Hilliges, Otmar; Sorkine-Hornung, Olga
TI  - Rig animation with a tangible and modular input device
PY  - 2016
AB  - We propose a novel approach to digital character animation, combining the benefits of tangible input devices and sophisticated rig animation algorithms. A symbiotic software and hardware approach facilitates the animation process for novice and expert users alike. We overcome limitations inherent to all previous tangible devices by allowing users to directly control complex rigs using only a small set (5-10) of physical controls. This avoids oversimplification of the pose space and excessively bulky device configurations. Our algorithm derives a small device configuration from complex character rigs, often containing hundreds of degrees of freedom, and a set of sparse sample poses. Importantly, only the most influential degrees of freedom are controlled directly, yet detailed motion is preserved based on a pose interpolation technique. We designed a modular collection of joints and splitters, which can be assembled to represent a wide variety of skeletons. Each joint piece combines a universal joint and two twisting elements, allowing to accurately sense its configuration. The mechanical design provides a smooth inverse kinematics-like user experience and is not prone to gimbal locking. We integrate our method with the professional 3D software Autodesk Maya® and discuss a variety of results created with characters available online. Comparative user experiments show significant improvements over the closest state-of-the-art in terms of accuracy and time in a keyframe posing task.
SP  - 144
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 35
IS  - 4
PB  - 
DO  - 10.1145/2897824.2925909
ER  - 

TY  - NA
AU  - Cherek, Christian; Voelker, Simon; Thar, Jan; Linden, Rene; Busch, Florian; Borchers, Jan
TI  - ITS - PERCs Demo: Persistently Trackable Tangibles on Capacitive Multi-Touch Displays
PY  - 2015
AB  - Tangible objects on capacitive multi-touch surfaces are usually only detected while the user is touching them. When the user lets go of such a tangible, the system cannot distinguish whether the user just released the tangible, or picked it up and removed it from the surface. In this demo we demonstrate PERCs , persistent capacitive tangibles that "know" whether they are currently on a capacitive touch surface or not. This is achieved by adding a small field sensor to the tangible to detect the touch screen's own, weak electromagnetic touch detection probing signal. In this demo we present two applications that make use of PERC tangibles -- An air hockey like game for two players and a single person arcade game.
SP  - 389
EP  - 392
JF  - Proceedings of the 2015 International Conference on Interactive Tabletops & Surfaces - ITS '15
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2817721.2823474
ER  - 

TY  - NA
AU  - Shen, Leixian; Shen, Enya; Luo, Yuyu; Yang, Xiaocong; Hu, Xuming; Zhang, Xiongshuai; Tai, Zhiwei; Wang, Jianmin
TI  - Towards Natural Language Interfaces for Data Visualization: A Survey.
PY  - 2021
AB  - Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation for visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than worrying about operating the interface to visualization tools. In the past two decades, leveraging advanced natural language processing technologies, numerous V-NLI systems have been developed both within academic research and commercial software, especially in recent years. In this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each paper, we develop categorical dimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used: query understanding, data transformation, visual mapping, view transformation, human interaction, context management, and presentation. Finally, we also shed light on several promising directions for future work in the community.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Plaumann, Katrin; Babic, Milos; Drey, Tobias; Hepting, Witali; Stooss, Daniel; Rukzio, Enrico
TI  - Improving Input Accuracy on Smartphones for Persons who are Affected by Tremor using Motion Sensors
PY  - 2018
AB  - Having a hand tremor often complicates interactions with touchscreens on mobile devices. Due to the uncontrollable oscillations of both hands, hitting targets can be hard, and interaction can be slow. Correcting input needs additional time and mental effort. We propose a method for automatically correcting such inputs based on motion data, gathered both with the devices' sensors and a small wearable sensor on the finger used for tapping. The development was informed by interviews with persons with tremor. Two empirical studies showed that our method, involving both smartphone and finger motion sensors without changing the user interface, allows users with tremor to select objects with up to 40 % fewer misses.
SP  - 156
EP  - 30
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 4
PB  - 
DO  - 10.1145/3161169
ER  - 

TY  - JOUR
AU  - Veuskens, Tom; Luyten, Kris; Ramakers, Raf
TI  - Rataplan: Resilient Automation of User Interface Actions with Multi-modal Proxies
PY  - 2020
AB  - We present Rataplan, a robust and resilient pixel-based approach for linking multi-modal proxies to automated sequences of actions in graphical user interfaces (GUIs). With Rataplan, users demonstrate a sequence of actions and answer human-readable follow-up questions to clarify their desire for automation. After demonstrating a sequence, the user can link a proxy input control to the action which can then be used as a shortcut for automating a sequence. Alternatively, output proxies use a notification model in which content is pushed when it becomes available. As an example use case, Rataplan uses keyboard shortcuts and tangible user interfaces (TUIs) as input proxies, and TUIs as output proxies. Instead of relying on available APIs, Rataplan automates GUIs using pixel-based reverse engineering. This ensures our approach can be used with all applications that offer a GUI, including web applications. We implemented a set of important strategies to support robust automation of modern interfaces that have a flat and minimal style, have frequent data and state changes, and have dynamic viewports.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397329
ER  - 

TY  - BOOK
AU  - Casarini, Matteo; Porta, Marco; Dondi, Piercarlo
TI  - ETRA Adjunct - A Gaze-Based Web Browser with Multiple Methods for Link Selection
PY  - 2020
AB  - This paper presents a gaze-based web browser that allows hands-free navigation through five different link selection methods (namely, Menu, Discrete Cursor, Progressive Zoom, Quick Zoom, and Free Pointing) and two page scrolling techniques. For link selection, the purpose of this multi-approach solution is two-fold. On the one hand, we want users to be able to choose either their preferred methods or those that, in each specific case, are the most suitable (e.g., depending on the kind of link to activate). On the other hand, we wanted to assess the performance and appreciation level of the different approaches through formal tests, to identify their strengths and weaknesses. The browser, which is conceived as an assistive technology tool, also includes a built-in on-screen keyboard and the possibility to save and retrieve bookmarks.
SP  - 1
EP  - 8
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - 17
IS  - NA
PB  - 
DO  - 10.1145/3379157.3388929
ER  - 

TY  - JOUR
AU  - Trinh, Ha; Asadi, Reza; Edge, Darren; Bickmore, Timothy
TI  - RoboCOP: A Robotic Coach for Oral Presentations
PY  - 2017
AB  - Rehearsing in front of a live audience is invaluable when preparing for important presentations. However, not all presenters take the opportunity to engage in such rehearsal, due to time constraints, availability of listeners who can provide constructive feedback, or public speaking anxiety. We present RoboCOP, an automated anthropomorphic robot head that acts as a coach to provide spoken feedback during presentation rehearsals at both the individual slide and overall presentation level. The robot offers conversational coaching on three key aspects of presentations: speech quality, content coverage, and audience orientation. The design of the feedback strategies was informed by findings from an exploratory study with academic professionals who were experienced in mentoring students on their presentations. In a within-subjects study comparing RoboCOP to visual feedback and spoken feedback without a robot, the robotic coach was shown to lead to significant improvement in the overall experience of presenters. Results of a second within-subjects evaluation study comparing RoboCOP with existing rehearsal practices show that our system creates a natural, interactive, and motivating rehearsal environment that leads to improved presentation quality.
SP  - 27
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 1
IS  - 2
PB  - 
DO  - 10.1145/3090092
ER  - 

TY  - NA
AU  - Hearst, Marti A.; Tory, Melanie
TI  - IEEE VIS (Short Papers) - Would You Like A Chart With That? Incorporating Visualizations into Conversational Interfaces
PY  - 2019
AB  - Conversational interfaces, such as chatbots, are increasing in prevalence, and have been shown to be preferred by and help users to complete tasks more efficiently than standard web interfaces in some cases. However, little is understood about if and how information should be visualized during the course of an interactive conversation. This paper describes studies in which participants report their preferences for viewing visualizations in chat-style interfaces when answering questions about comparisons and trends. We find a significant split in preferences among participants; approximately 40% prefer not to see charts and graphs in the context of a conversational interface. For those who do prefer to see charts, most preferred to see additional supporting context beyond the direct answer to the question. These results have important ramifications for the design of conversational interfaces to data.
SP  - 36
EP  - 40
JF  - 2019 IEEE Visualization Conference (VIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/visual.2019.8933766
ER  - 

TY  - NA
AU  - Pulliza, Jonathan Luis
TI  - Let the robot do it for me: assessing voice as a modality for visual analytics for novice users
PY  - NA
AB  - The growth of Visual Analytics (VA) systems has been driven by the need to explore and understand large datasets across many domains. Applications such as Tableau were developed with the goal of better supporting novice users to generate data visualizations and complete their tasks. However, novice users still face many challenges in using VA systems, especially in complex tasks outside of simple trend identification, such as exploratory tasks. Many of the issues stem from the novice users’ inability to reconcile their questions or representations of the data with the visualizations presented using the interactions provided by the system. With the improvement in natural language processing technology and the increased prevalence of voice interfaces, there is a renewed interest in developing voice interactions for VA systems. The goal is to enable users to ask questions directly to the system or to indicate specific actions using natural language, which may better facilitate access to functions available in the VA system. Previous approaches have tended to build systems in a screen-only environment in order to encourage interaction through voice. Though they did produce significant results and guidance for the technical challenges of voice in VA, it is important to understand how the use of a voice system would affect novice users within their most common context instead of moving them into new environments. It is also important to understand when a novice user would choose to use a voice modality when the traditional keyboard and mouse modality is also available. This study is an attempt to understand the circumstances under which novice users of a VA system would choose to interact with using their voice in a traditional desktop environment, and whether the voice system better facilitates access to available functionalities. Given the users choose the voice system, do they choose different functions than those with only a keyboard and a mouse? Using a Wizard of Oz set up in the place of an automated voice system, we find that the participants chose to use the voice system because of its convenience, ability to get a quick start on their work, and in some situations where they could not find a specific function in the interface. Overall function choices were not found to be significantly different between those who had access to the voice system versus those who did not, though there were a few cases where participants were able to access less common functions compared to a control group. Participants refrained from choosing voice because their previous experiences with voice systems had led them to believe all voice systems were not capable of addressing their task needs. They also felt using the voice system was incongruent with gaining mastery of the underlying VA system, as the convenience of using the voice system could lead to its use as a crutch. Participants then often chose to struggle with the visual interface instead of using the voice system for assistance.Ph.D.Includes bibliographical reference
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.7282/t3-rq3n-zz18
ER  - 

TY  - JOUR
AU  - Yang, Lumin; Zhuang, Jiajie; Fu, Hongbo; Wei, Xiangzhi; Zhou, Kun; Zheng, Youyi
TI  - SketchGNN: Semantic Sketch Segmentation with Graph Neural Networks
PY  - 2021
AB  - We introduce SketchGNN, a convolutional graph neural network for semantic segmentation and labeling of freehand vector sketches. We treat an input stroke-based sketch as a graph with nodes representing the sampled points along input strokes and edges encoding the stroke structure information. To predict the per-node labels, our SketchGNN uses graph convolution and a static-dynamic branching network architecture to extract the features at three levels, i.e., point-level, stroke-level, and sketch-level. SketchGNN significantly improves the accuracy of the state-of-the-art methods for semantic sketch segmentation (by 11.2% in the pixel-based metric and 18.2% in the component-based metric over a large-scale challenging SPG dataset) and has magnitudes fewer parameters than both image-based and sequence-based methods.
SP  - 1
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 3
PB  - 
DO  - 10.1145/3450284
ER  - 

TY  - NA
AU  - Zhang, Zhong-Yi; Chen, Hong-Xian; Wang, Shih-Hao; Tsai, Hsin-Ruey
TI  - ELAXO : Rendering Versatile Resistive Force Feedback for Fingers Grasping and Twisting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545677
ER  - 

TY  - CHAP
AU  - Iwai, Daisuke
TI  - CCIW - Computational Imaging in Projection Mapping
PY  - 2019
AB  - Projection mapping or spatial augmented reality (SAR) has been tremendously widespread over the world. The goal is to seamlessly merge physical and virtual worlds by superimposing computer generated graphics onto real surfaces. In projection mapping applications, target surfaces are generally not suitable for projection. They are textured and non-planar and conventional projectors are specifically designed to display high quality images onto uniformly white and flat surfaces only. Although researchers developed various algorithms to alleviate image quality degradations, the performances were limited by upper bounds resulting from the hardware. I briefly overview the recent advances in the projection mapping research field, particularly focuses on the computational imaging and display approach to overcome technical limitations of current projection hardware in terms of dynamic range, refresh rate, and depth-of-field. I also covers an emerging issue in the projection mapping research, which is dynamic projection mapping. This article is written by reorganizing a previously published state-of-the-art report paper by the same author [13] for an invited talk at the IAPR Computational Color Imaging Workshop (CCIW) 2019.
SP  - 14
EP  - 25
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-13940-7_2
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Vink, Luke; Counts, Jared; Windham, Daniel; Leithinger, Daniel; Follmer, Sean; Ishii, Hiroshi
TI  - CHI - Materiable: Rendering Dynamic Material Properties in Response to Direct Physical Touch with Shape Changing Interfaces
PY  - 2016
AB  - Shape changing interfaces give physical shapes to digital data so that users can feel and manipulate data with their hands and bodies. However, physical objects in our daily life not only have shape but also various material properties. In this paper, we propose an interaction technique to represent material properties using shape changing interfaces. Specifically, by integrating the multi-modal sensation techniques of haptics, our approach builds a perceptive model for the properties of deformable materials in response to direct manipulation. As a proof-of-concept prototype, we developed preliminary physics algorithms running on pin-based shape displays. The system can create computationally variable properties of deformable materials that are visually and physically perceivable. In our experiments, users identify three deformable material properties (flexibility, elasticity and viscosity) through direct touch interaction with the shape display and its dynamic movements. In this paper, we describe interaction techniques, our implementation, future applications and evaluation on how users differentiate between specific properties of our system. Our research shows that shape changing interfaces can go beyond simply displaying shape allowing for rich embodied interaction and perceptions of rendered materials with the hands and body.
SP  - 2764
EP  - 2772
JF  - Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2858036.2858104
ER  - 

TY  - NA
AU  - Zhao, Liangliang; Zhao, Jingdong; Liu, Hong; Manocha, Dinesh
TI  - Collision-Free Kinematics for Redundant Manipulators in Dynamic Scenes using Optimal Reciprocal Velocity Obstacles.
PY  - 2018
AB  - We present a novel algorithm for collision-free kinematics of multiple manipulators in a shared workspace with moving obstacles. Our optimization-based approach simultaneously handles collision-free constraints based on reciprocal velocity obstacles and inverse kinematics constraints for high-DOF manipulators. We present an efficient method based on particle swarm optimization that can generate collision-free configurations for each redundant manipulator. Furthermore, our approach can be used to compute safe and oscillation-free trajectories in a few milli-seconds. We highlight the real-time performance of our algorithm on multiple Baxter robots with 14-DOF manipulators operating in a workspace with dynamic obstacles. Videos are available at this https URL
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhu, Anjie; Cheng, Shiwei; Fan, Jing
TI  - UbiComp/ISWC Adjunct - Eye Tracking and Gesture Based Interaction for Target Selection on Large Displays
PY  - 2018
AB  - Mouse based target selection requires much movement when locating targets across long distance on large displays. Eye tracking technique can locate targets more easily and quickly across long distance, and has a high potential for fast targets selection on large displays. However, eye tracking still faces to challenges, such as low accuracy, and high error rate of selection operation, especially for gaze-only interaction. This paper proposed a multimodal interaction method that combined gaze with gestures. This method utilized gaze for rough selection first, and then utilized hand gesture to confirm accurate selection. Furthermore, when targets are small and crowded, we used semi-fixed gaze cursor and secondary selection mechanism to optimize the selection process. Finally we conducted a pilot study, and the results showed that the selection speed and accuracy rate of our method was higher than the gaze only input and similar to mouse input.
SP  - 319
EP  - 322
JF  - Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267305.3267607
ER  - 

TY  - NA
AU  - Hearst, Marti A.; Tory, Melanie; Setlur, Vidya
TI  - IEEE VIS (Short Papers) - Toward Interface Defaults for Vague Modifiers in Natural Language Interfaces for Visual Analysis
PY  - 2019
AB  - Natural language interfaces for data visualizations tools are growing in importance, but little research has been done on how a system should respond to questions that contain vague modifiers like "high" and "expensive." This paper makes a first step toward design guidelines for this problem, based on existing research from cognitive linguistics and the results of a new empirical study with 274 crowdsourcing participants. A comparison of four bar chart-based views finds that highlighting the top items according to distribution-sensitive values is preferred in most cases and is a good starting point as a design guideline.
SP  - 21
EP  - 25
JF  - 2019 IEEE Visualization Conference (VIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/visual.2019.8933569
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Luo, Danli; Fang, Ziqi; Fan, Yitao; Yu, Zhi; Chen, Yu; Pan, Deying; Yang, Yue; Zhao, Yijun; Gu, Jianzhe; Yao, Lining; Tao, Ye; Wang, Guanyun
TI  - CHI Extended Abstracts - Fashion Design with FlexTruss Approach
PY  - 2021
AB  - This demo presents FlexTruss, a design and construction pipeline based on the assembly of modularized truss-like objects fabricated with conventional 3D printers and assembled by threading. To create an end-to-end system, a parametric design tool with an optimal Euler path calculation method is developed, which can support both inverse and forward design workflow and multi-material construction of modular parts. In addition, the workflow guides the assembly of printed truss modules by threading. Finally, a series of application cases to demonstrate the affordance of FlexTruss are presented. We believe that FlexTruss extends the design space of 3D printing beyond typically hard and fixed forms, and it will provide new capabilities for designers and researchers to explore the use of such flexible truss structures in human-object interaction.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451572
ER  - 

TY  - NA
AU  - Hamanishi, Natsuki; Kono, Michinari; Suwa, Shunichi; Miyaki, Takashi; Rekimoto, Jun
TI  - IUI Companion - Flufy: Recyclable and Edible Rapid Prototyping using Fluffed Sugar
PY  - 2018
AB  - Low-Fab (low-fidelity fabrication) has allowed designers to speed up the initial prototyping of 3D objects. However, the method is used to quickly verify the key aspects of the model, and are not the final desired object. Therefore, the object generated by Low-Fab are temporal objects in which the disposability and/or reusability of the material should be considered. Here we present a novel prototyping method beyond the concept of Low-Fab, to quickly fabricate large, disposable, reusable and edible objects by using fluffed sugar for the material. We have implemented a fabrication system that supports the user to create desired objects in high-speed, which is based on the idea of cotton candy making. Our idea is to combine food fabrication techniques with rapid prototyping methods, which we believe that it can contribute to both designers and gastronomy.
SP  - 27
EP  - NA
JF  - Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3180308.3180335
ER  - 

TY  - NA
AU  - Luukkonen, Timo; Colley, Ashley; Seppänen, Tapio; Häkkilä, Jonna
TI  - AHs - Cough Activated Dynamic Face Visor
PY  - 2021
AB  - In this demo, we present a cough activated face visor, where the transparent visor screen moves to cover the wearer’s face when coughing sounds are detected. The cough detection is performed by a TinyML machine learning model, running on a microcontroller integrated to the visor’s headband. We make no claims regarding the direct impact of our prototype in preventing the spread of Covid-19, but hope it inspires discussion on future smart solutions in the area of personal protective equipment (PPE). Additionally, we note that visibility of the visor’s operation potentially encourages observers to adopt preventative measures such as hygiene practices and social distancing.
SP  - 295
EP  - 297
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3459000
ER  - 

TY  - NA
AU  - Srinivasan, Arjun; Setlur, Vidya
TI  - UIST - Snowy: Recommending Utterances for Conversational Visual Analysis
PY  - 2021
AB  - Natural language interfaces (NLIs) have become a prevalent medium for conducting visual data analysis, enabling people with varying levels of analytic experience to ask questions of and interact with their data. While there have been notable improvements with respect to language understanding capabilities in these systems, fundamental user experience and interaction challenges including the lack of analytic guidance (i.e., knowing what aspects of the data to consider) and discoverability of natural language input (i.e., knowing how to phrase input utterances) persist. To address these challenges, we investigate utterance recommendations that contextually provide analytic guidance by suggesting data features (e.g., attributes, values, trends) while implicitly making users aware of the types of phrasings that an NLI supports. We present Snowy, a prototype system that generates and recommends utterances for visual analysis based on a combination of data interestingness metrics and language pragmatics. Through a preliminary user study, we found that utterance recommendations in Snowy support conversational visual analysis by guiding the participants’ analytic workflows and making them aware of the system’s language interpretation capabilities. Based on the feedback and observations from the study, we discuss potential implications and considerations for incorporating recommendations in future NLIs for visual analysis.
SP  - 864
EP  - 880
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474792
ER  - 

TY  - NA
AU  - Iwafune, Miyu; Ohshima, Taisuke; Ochiai, Yoichi
TI  - SIGGRAPH Asia Technical Briefs - Coded skeleton: shape changing user interface with mechanical metamaterial
PY  - 2018
AB  - We propose a design method for fabricating a novel shape-changing user interface, called "Coded Skeleton", by computationally integrating actuators and sensors using a mechanical metamaterial. This design method realizes the deformation of various curves using simple expansion and contraction actuators, leveraging the fact that the Coded Skeleton is flexible in one deformation mode but stiff in other. We describe the design method and structural analysis of the mechanical metamaterial that can uniquely define deformation along with outlining the creation and control method of the Coded Skeleton using this structure. Finally, we propose three applications of the Coded Skeleton.
SP  - 11
EP  - NA
JF  - SIGGRAPH Asia 2018 Technical Briefs
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3283254.3283255
ER  - 

TY  - JOUR
AU  - Xu, Kai; Ottley, Alvitta; Walchshofer, Conny; Streit, Marc; Chang, Remco; Wenskovitch, John
TI  - Survey on the Analysis of User Interactions and Visualization Provenance
PY  - 2020
AB  - There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.
SP  - 757
EP  - 783
JF  - Computer Graphics Forum
VL  - 39
IS  - 3
PB  - 
DO  - 10.1111/cgf.14035
ER  - 

TY  - JOUR
AU  - Weiler, Jennifer; Ingalls, Todd; Kuznetsov, Stacey
TI  - Lithobox: Exploring Hybrid Crafting Practices through a Collaboration across Digital Fabrication and Fine Arts
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>New digital and physical fabrication tools are increasingly being integrated with traditional craft techniques to enable hybrid crafting practices. Inspired by the traditional lithophane technique whereby designs are molded in porcelain and visible only when backlit, the authors developed Lithobox: a software system, physical kit and workflow for creating illuminated 3D-printed lithophanes. They explored Lithobox as a creative tool in workshops with nine artists and presented the finished 3D-printed lithophanes and software tool as part of an international art exhibition. Through these collaborations and creative interactions, the authors’ work reveals how the amalgamation of material, technology and productive constraints can influence current art practices.</jats:p>
SP  - 230
EP  - 234
JF  - Leonardo
VL  - 55
IS  - 3
PB  - 
DO  - 10.1162/leon_a_02197
ER  - 

TY  - NA
AU  - Yu, Jin; Sakhardande, Prabodh; Parmar, Ruchita; Oh, HyunJoo
TI  - Strawctures: A Modular Electronic Construction Kit for Human-Scale Interactive Structures
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501322
ER  - 

TY  - NA
AU  - Menin, Aline; Chardonnel, Sonia; Davoine, Paule-Annick; Nedel, Luciana
TI  - SIBGRAPI - eSTIMe: Towards an All-in-One Geovisualization Environment for Daily Mobility Analysis
PY  - 2019
AB  - Daily mobility data describes individual displacements over 24-hours periods and are an important source of information to understand the real rhythm of a city, to provide appropriate transportation policies, and to support investment decisions. Geovisualization researchers have designed multiple coordinated views environments, combining spatial and temporal dimensions, and providing indicators comparison. Daily mobility analysis is complex and requires simultaneous exploration and combination of different indicators at different spatial and temporal granularity levels. The design of effective geovisualization environments supporting this analysis evokes several challenges due to the diversity and multiplicity of indicators, the granularity of space and time, and time integration. In this paper, we propose a geovisualization approach enabling the dynamic visualization of diverse indicators, as well as the exploration of space, time, and other attributes.We use multiple screens embedding customizable dashboards and allowing the users to arrange views and compare indicators as it better fits their analysis. It also integrates a mobile device serving as a display and interaction tool to physically control the evolution of the visualization on time.
SP  - 39
EP  - 46
JF  - 2019 32nd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sibgrapi.2019.00014
ER  - 

TY  - NA
AU  - Arabi, Abul Al; Li, Jiahao; Chen, Xiang 'Anthony; Kim, Jeeeun
TI  - Mobiot: Augmenting Everyday Objects into Moving IoT Devices Using 3D Printed Attachments Generated by Demonstration
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517645
ER  - 

TY  - NA
AU  - Grandi, Jeronimo G; Debarba, Henrique Galvan; Nedel, Luciana; Maciel, Anderson
TI  - CHI - Design and Evaluation of a Handheld-based 3D User Interface for Collaborative Object Manipulation
PY  - 2017
AB  - Object manipulation in 3D virtual environments demands a combined coordination of rotations, translations and scales, as well as the camera control to change the user's viewpoint. Then, for many manipulation tasks, it would be advantageous to share the interaction complexity among team members. In this paper we propose a novel 3D manipulation interface based on a collaborative action coordination approach. Our technique explores a smartphone -- the touchscreen and inertial sensors -- as input interface, enabling several users to collaboratively manipulate the same virtual object with their own devices. We first assessed our interface design on a docking and an obstacle crossing tasks with teams of two users. Then, we conducted a study with 60 users to understand the influence of group size in collaborative 3D manipulation. We evaluated teams in combinations of one, two, three and four participants. Experimental results show that teamwork increases accuracy when compared with a single user. The accuracy increase is correlated with the number of individuals in the team and their work division strategy.
SP  - 5881
EP  - 5891
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025935
ER  - 

TY  - BOOK
AU  - Rencis, Edgars
TI  - ICISDM - Natural Language-Based Knowledge Extraction in Healthcare Domain
PY  - 2019
AB  - There is a growing amount of data in the databases of hospitals. These data could be exploited to alleviate the decision-making process of hospital managers, physicians and researchers. However, these types of end-users often lack the expertise necessary for extracting those data from the database. Several approaches exist in the field of how to allow non-programmers writing queries in a convenient manner, but none of them has yet reached fully satisfactory results. This paper sketches a solution to this problem by introducing means for writing queries in a keywords-containing natural language thus alleviating the query writing process for the end-user. Introducing this approach in the knowledge management system of the organization would greatly benefit the domain experts by allowing them to carry out the decision-making process in a more rapid and less erroneous manner.
SP  - 138
EP  - 142
JF  - Proceedings of the 2019 3rd International Conference on Information System and Data Mining
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3325917.3325948
ER  - 

TY  - CONF
AU  - Kim, Young-Ho; Lee, Bongshin; Srinivasan, Arjun; Choe, Eun Kyoung
TI  - Data@Hand: Fostering Visual Exploration of Personal Data on Smartphones Leveraging Speech and Touch Interaction
PY  - 2021
AB  - Most mobile health apps employ data visualization to help people view their health and activity data, but these apps provide limited support for visual data exploration. Furthermore, despite its huge potential benefits, mobile visualization research in the personal data context is sparse. This work aims to empower people to easily navigate and compare their personal health data on smartphones by enabling flexible time manipulation with speech. We designed and developed Data@Hand, a mobile app that leverages the synergy of two complementary modalities: speech and touch. Through an exploratory study with 13 long-term Fitbit users, we examined how multimodal interaction helps participants explore their own health data. Participants successfully adopted multimodal interaction (i.e., speech and touch) for convenient and fluid data exploration. Based on the quantitative and qualitative findings, we discuss design implications and opportunities with multimodal interaction for better supporting visual data exploration on mobile devices.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - 誠之, 平井; 啓, 中丸; 圭博, 川原; 康明, 筧
TI  - xSlate：素材硬度制御と機械的伸縮制御による形状・サイズ可変なフィジカルインタフェース
PY  - 2019
AB  - NA
SP  - 303
EP  - 312
JF  - NA
VL  - 24
IS  - 3
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Pan, Lihang; Yu, Chun; Li, JiaHui; Huang, Tian; Bi, Xiaojun; Shi, Yuanchun
TI  - Automatically Generating and Improving Voice Command Interface from Operation Sequences on Smartphones
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517459
ER  - 

TY  - JOUR
AU  - Chen, Qiaochu; Pailoor, Shankara; Barnaby, Celeste; Criswell, Abby; Wang, Chenglong; Durrett, Greg; Dillig, Işil
TI  - Type-directed synthesis of visualizations from natural language queries
PY  - 2022
AB  - <jats:p>We propose a new technique based on program synthesis for automatically generating visualizations from natural language queries. Our method parses the natural language query into a refinement type specification using the intents-and-slots paradigm and leverages type-directed synthesis to generate a set of visualization programs that are most likely to meet the user's intent. Our refinement type system captures useful hints present in the natural language query and allows the synthesis algorithm to reject visualizations that violate well-established design guidelines for the input data set. We have implemented our ideas in a tool called Graphy and evaluated it on NLVCorpus, which consists of 3 popular datasets and over 700 real-world natural language queries. Our experiments show that Graphy significantly outperforms state-of-the-art natural language based visualization tools, including transformer and rule-based ones.</jats:p>
SP  - 532
EP  - 559
JF  - Proceedings of the ACM on Programming Languages
VL  - 6
IS  - OOPSLA2
PB  - 
DO  - 10.1145/3563307
ER  - 

TY  - JOUR
AU  - Ng, Pai Chet; She, James; Jeon, Kang Eun; Baldauf, Matthias
TI  - When Smart Devices Interact With Pervasive Screens: A Survey
PY  - 2017
AB  - The meeting of pervasive screens and smart devices has witnessed the birth of screen-smart device interaction (SSI), a key enabler to many novel interactive use cases. Most current surveys focus on direct human-screen interaction, and to the best of our knowledge, none have studied state-of-the-art SSI. This survey identifies three core elements of SSI and delivers a timely discussion on SSI oriented around the screen, the smart device, and the interaction modality. Two evaluation metrics (i.e., interaction latency and accuracy) have been adopted and refined to match the evaluation criterion of SSI. The bottlenecks that hinder the further advancement of the current SSI in connection with this metrics are studied. Last, future research challenges and opportunities are highlighted in the hope of inspiring continuous research efforts to realize the next generation of SSI.
SP  - 55
EP  - 23
JF  - ACM Transactions on Multimedia Computing, Communications, and Applications
VL  - 13
IS  - 4
PB  - 
DO  - 10.1145/3115933
ER  - 

TY  - NA
AU  - Genç, Çağlar; Launne, Emilia; Häkkilä, Jonna
TI  - Interactive Mycelium Composites: Material Exploration on Combining Mushroom with Off-the-shelf Electronic Components
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546155.3546689
ER  - 

TY  - NA
AU  - Walny, Jagoda; Frisson, Christian; West, Mieka; Kosminsky, Doris; Knudsen, Søren; Carpendale, Sheelagh; Willett, Wesley
TI  - Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff
PY  - 2019
AB  - Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Isomoto, Toshiya; Yamanaka, Shota; Shizuki, Buntarou
TI  - Interaction Design of Dwell Selection Toward Gaze-based AR/VR Interaction
PY  - 2022
AB  - In this paper, we first position the current dwell selection among gaze-based interactions and its advantages against head-gaze selection, which is the mainstream interface for HMDs. Next, we show how dwell selection and head-gaze selection are used in an actual interaction situation. By comparing these two selection methods, we describe the potential of dwell selection as an essential AR/VR interaction.
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3531628
ER  - 

TY  - BOOK
AU  - Ho, Tien-Yu; Wang, Hao-Chuan; Lai, Shong-Hong
TI  - CCHI - Non-native Language Reading Support with Display of Machine Translation Based on Eye-Tracking and Sentence-Level Mapping
PY  - 2018
AB  - In the era of information explosion, individuals often need to read first-hand information online in English, which is a non-native language of most of the people in the world. However, the lack of proficiency toward a non-native language makes it difficult for second-language readers to efficiently understand the contents on the English webpages, especially the long articles. In this work, we propose a novel reading interface to support English as second-language readers with adaptive display of machine translation (MT) using eye tracking, accompanied with sentence-level mapping using background colors. We conducted an experiment to investigate how different methods affect the second-language readers. We found that active translation could help second-language readers comprehend the English article without dazzling them. In addition, sentence-level mapping using background colors not only benefits the mapping between original sentences and their corresponding ones, but also alleviates the problems of line skipping and reading resumption.
SP  - 57
EP  - 63
JF  - Proceedings of the Sixth International Symposium of Chinese CHI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3202667.3202675
ER  - 

TY  - JOUR
AU  - Luo, Yuyu; Qin, Xuedi; Chai, Chengliang; Tang, Nan; Li, Guoliang; Li, Wenbo
TI  - Steerable Self-Driving Data Visualization
PY  - 2022
AB  - NA
SP  - 475
EP  - 490
JF  - IEEE Transactions on Knowledge and Data Engineering
VL  - 34
IS  - 1
PB  - 
DO  - 10.1109/tkde.2020.2981464
ER  - 

TY  - JOUR
AU  - Nakagaki, Ken; Follmer, Sean; Dementyev, Artem; Paradiso, Joseph A.; Ishii, Hiroshi
TI  - Designing Line-Based Shape-Changing Interfaces
PY  - 2017
AB  - This article starts with an overview of work on shape-changing line interfaces in the field of HCI, including the authors’ previous work on actuated-line interfaces, LineFORM and ChainFORM. Related research from other fields, such as robotics and material science, are also introduced. Then, several potential implementation methods are compared and discussed in depth with regards to their potential for future research and applications. The authors also investigate the interaction design space around actuated line interfaces, categorized into four groups: physical display, tangible interaction, constraints, and customization. Leveraging this design space, they present potential applications and demonstrate their use with the LineFORM and ChainFORM prototypes. Envisioning a future where shape-changing lines are woven into daily life, this article aims to explore and initiate a broad research space around line-based shape-changing interfaces and to encourage future researchers and designers to investigate these novel directions. This article is part of a special issue on shape-changing interfaces.
SP  - 36
EP  - 46
JF  - IEEE Pervasive Computing
VL  - 16
IS  - 4
PB  - 
DO  - 10.1109/mprv.2017.3971127
ER  - 

TY  - NA
AU  - Speicher, Marco; Daiber, Florian; Gehring, Sven; Krüger, Antonio
TI  - PerDis - Exploring 3D manipulation on large stereoscopic displays
PY  - 2016
AB  - In the last years stereoscopic 3D has seen a drastic increase in popularity especially in terms of consumer-ready hardware and software. While the technology for input (smart-phone, Kinect, etc.) as well as output (passive/active stereoscopic and auto-stereoscopic displays etc.) is market ready, only few solutions for natural interaction with such devices exist. In this paper we propose an approach for mobile and gestural interaction with stereoscopic 3D content. We evaluate our technique in a 3D docking task on a large 3D display. In an experiment the interaction technique was evaluated for monoscopic and stereoscopic displayed data. Our results show that the translation and rotation precision benefits from the usage of stereoscopic 3D. Especially for tasks that require simultaneous rotation on all three axes stereoscopic displays outperform monoscopic 3D drastically.
SP  - 59
EP  - 66
JF  - Proceedings of the 5th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2914920.2915018
ER  - 

TY  - NA
AU  - Mitra, Rishab; Narechania, Arpit; Endert, Alex; Stasko, John
TI  - Facilitating Conversational Interaction in Natural Language Interfaces for Visualization
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE Visualization and Visual Analytics (VIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vis54862.2022.00010
ER  - 

TY  - BOOK
AU  - Biswas, Pradipta; Prabhakar, Gowdham; Rajesh, J.; Pandit, Kavesh; Halder, Ankit
TI  - BCS HCI - Improving eye gaze controlled car dashboard using simulated annealing
PY  - 2017
AB  - This paper proposes and evaluates a new algorithm to improve performance of pointing and selection tasks in an eye gaze controlled graphical user interface in automotive environment. The algorithm aims to reduce number of wrong selection of targets due to inaccuracy of either the eye gaze tracker or the calibration routine. The proposed method does not require changing layout of the interface rather puts a set of hotspots on clickable targets using Simulated Annealing algorithm. A couple of user studies involving a driving simulator and ISO 26022 lane changing task found that the proposed method can reduce number of wrong selection and can also improve driving performance compared to an existing touchscreen system.
SP  - 39
EP  - NA
JF  - Electronic Workshops in Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.14236/ewic/hci2017.39
ER  - 

TY  - CHAP
AU  - Richter, Christoph
TI  - Digitale Materialitäten und die Artikulation des (noch nicht) Gewussten – Eine Untersuchung am Beispiel gestalterischer Praktiken des Skizzierens
PY  - 2020
AB  - Gestalterische Praktiken des Skizzierens stellen eine grundlegende Form epistemischer Artikulationsprozesse dar, die sich unter dem Einfluss digitaler Technologien wandeln. Obwohl die soziomaterielle Bedingtheit gestalterischer Praktiken zunehmend diskutiert und untersucht wird, ist noch weitgehend unklar, wie sich die Materialitat und Widerstandigkeit digitaler Technologien konzeptionell fassen lasst. In Abgrenzung zu analytischen Ansatzen, die auf die Gestalt- und Formbarkeit digitaler Technologien im Zuge ihrer softwaretechnischen Entwicklung verweisen, wird im vorliegenden Beitrag eine Perspektive entwickelt, die die Materialitat und Widerstandigkeit im Gebrauch digitaler Technologien verortet. Die Perspektive wird anhand einer empirischen Fallstudie illustriert, die die Rolle der verwendeten Hardware, die Ubersetzung manueller Eingaben in digitale Objekte wie auch den Umgang mit komplexen digitalen Objekten thematisiert. Die abschliesende Diskussion verdeutlicht die enge Verwobenheit der digitalen Technologien mit den jeweiligen epistemischen Praktiken und umreist Implikationen fur ein nicht-reduktionistisches Verstandnis digitaler Bildung.
SP  - 171
EP  - 193
JF  - Big Data, Datafizierung und digitale Artefakte
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-658-28398-8_10
ER  - 

TY  - JOUR
AU  - Kaduk, Sylwia I.; Roberts, Aaron P. J.; Stanton, Neville A.
TI  - The circadian effect on psychophysiological driver state monitoring
PY  - 2020
AB  - Driving is an everyday activity but also brings a substantial risk. Driver state monitoring is a potential method to alleviate such risk by detecting unsafe driver states. Monitoring systems can in...
SP  - 619
EP  - 649
JF  - Theoretical Issues in Ergonomics Science
VL  - 22
IS  - 5
PB  - 
DO  - 10.1080/1463922x.2020.1842548
ER  - 

TY  - NA
AU  - Usuba, Hiroki; Yamanaka, Shota; Miyashita, Homei
TI  - MobileHCI - Touch Pointing Performance for Uncertain Touchable Sizes of 1D Targets
PY  - 2019
AB  - When users operate smartphones and desktop interfaces with their fingers, there are differences between the motor and visual widths. For example, when a user selects an item from a vertical menu, the area that is physically touched by the user is often larger than the visual width (e.g., of the label for the item selected). Therefore, the user aims for the label assuming that the label width (the visual width) means the motor width. Consequently, the user performs operations more carefully than necessary. We conducted an experiment to investigate the effect of the motor and visual widths on finger pointing. After asking participants to explore the motor width, they performed an experimental task. Our experiment shows that the users' movement time depends on the motor width and can be predicted. We also analyze existing interfaces and discuss the implications.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340131
ER  - 

TY  - JOUR
AU  - Prabhakar, Gowdham; Ramakrishnan, Aparna; Madan, Modiksha; Murthy, L. R. D.; Sharma, Vinay; Deshmukh, Sachin; Biswas, Pradipta
TI  - Interactive gaze and finger controlled HUD for cars
PY  - 2019
AB  - Modern infotainment systems in automobiles facilitate driving at the cost of secondary tasks in addition to the primary task of driving. These secondary tasks have considerable chance to distract a driver from his primary driving task, thereby reducing safety or increasing cognitive workload. This paper presents an intelligent interactive head up display (HUD) on the windscreen of the driver that does not require them to take eyes off from road while undertaking secondary tasks like playing music, operating vent controls, watching navigation map and so on. The interactive HUD allows interaction in the form of pointing and selection just like traditional graphical user interfaces, however tracking operators’ eye gaze or finger movements. Additionally, the system can also estimate drivers’ cognitive load and distraction level. User studies show the system improves driving performance in terms of mean deviation from lane in an ISO 26022 lane changing task compared to touchscreen system and participants can undertake ISO 9241 pointing tasks in less than 2 s on average inside a Toyota Etios car.
SP  - 101
EP  - 121
JF  - Journal on Multimodal User Interfaces
VL  - 14
IS  - 1
PB  - 
DO  - 10.1007/s12193-019-00316-9
ER  - 

TY  - BOOK
AU  - Usuba, Hiroki; Yamanaka, Shota; Miyashita, Homei
TI  - ISS - Comparing Lassoing Criteria and Modeling Straight-line and One-loop Lassoing Motions Considering Criteria
PY  - 2019
AB  - In graphical user interfaces, users can select multiple objects simultaneously via lasso selection. This can be implemented, for example, by having users select objects by looping around their centers or entire areas. Based on differences in lassoing criteria, we presume that the performance of the criteria also differs. In this study, we compare three lassoing criteria and model lassoing motions while considering these criteria. We conducted two experiments; participants steered through straight-line and one-loop paths by using three criteria. The participants handled the lassoing criteria correctly and performed lassoing at appropriate speeds for each path shape. Although the drawn trajectories varied depending on the lassoing criteria, the criteria in the performance and subjective evaluations did not differ significantly. Additionally, from our results, we build a baseline model to predict the movement time by considering the lassoing criteria. We also discuss further experiments to predict movement time under more complex conditions.
SP  - 181
EP  - 191
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359707
ER  - 

TY  - NA
AU  - Chen, Zhutian; Xia, Haijun
TI  - CrossData: Leveraging Text-Data Connections for Authoring Data Documents
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517485
ER  - 

TY  - NA
AU  - Fan, Lihua; Yu, Chun; Shi, Yuanchun
TI  - Guided social sharing of emotions through drawing art therapy: generation of deep emotional expression and helpful emotional responses
PY  - 2019
AB  - In online social sharing of emotions (SSE), individuals use free text to express their emotions and audiences use free text to give affective responses. The role of online SSE is to adjust users' emotions. There is currently a need for a new SSE model that can generate deeper emotional expressions, generate more helpful emotional responses than online SSE, and meet people's need for deep emotional communication. We integrated humanistic drawing-art therapy's thought and method with crowdsourcing to create a guided SSE model. In a crowdsourcing platform, undergraduate students expressed their emotions by drawings, students and counselors crowds used ways of "reflection of feeling" to respond to the drawings. For our questions, we analyzed the response data and designed questionnaires evaluated by all of the participants. Our results revealed: (1) two crowds could generate responses helping painters to understand their emotions; (2) painters could generate deep emotional expression through drawings; and (3) our model could meet people's online deep emotional communication needs.
SP  - 65
EP  - 78
JF  - Proceedings of the Seventh International Symposium of Chinese CHI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332169.3333571
ER  - 

TY  - NA
AU  - Yuan, Linping; Chen, Yuanzhe; Fu, Siwei; Wu, Aoyu; Qu, Huamin
TI  - BigComp - SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features
PY  - 2019
AB  - Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring high-quality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level-of-details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.
SP  - 1
EP  - 8
JF  - 2019 IEEE International Conference on Big Data and Smart Computing (BigComp)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bigcomp.2019.8679261
ER  - 

TY  - CONF
AU  - Kumar, Abhinav; Di Eugenio, Barbara; Aurisano, Jillian; Johnson, Andrew
TI  - LREC - Augmenting Small Data to Classify Contextualized Dialogue Acts for Exploratory Visualization
PY  - 2020
AB  - Our goal is to develop an intelligent assistant to support users explore data via visualizations. We have collected a new corpus of conversations, CHICAGO-CRIME-VIS, geared towards supporting data visualization exploration, and we have annotated it for a variety of features, including contextualized dialogue acts. In this paper, we describe our strategies and their evaluation for dialogue act classification. We highlight how thinking aloud affects interpretation of dialogue acts in our setting and how to best capture that information. A key component of our strategy is data augmentation as applied to the training data, since our corpus is inherently small. We ran experiments with the Balanced Bagging Classifier (BAGC), Condiontal Random Field (CRF), and several Long Short Term Memory (LSTM) networks, and found that all of them improved compared to the baseline (e.g., without the data augmentation pipeline). CRF outperformed the other classification algorithms, with the LSTM networks showing modest improvement, even after obtaining a performance boost from domain-trained word embeddings. This result is of note because training a CRF is far less resource-intensive than training deep learning models, hence given a similar if not better performance, traditional methods may still be preferable in order to lower resource consumption.
SP  - 590
EP  - 599
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yang, Lumin; Zhuang, Jiajie; Fu, Hongbo; Zhou, Kun; Zheng, Youyi
TI  - SketchGCN: Semantic Sketch Segmentation with Graph Convolutional Networks.
PY  - 2020
AB  - We introduce SketchGCN, a graph convolutional neural network for semantic segmentation and labeling of free-hand sketches. We treat an input sketch as a 2D pointset, and encode the stroke structure information into graph node/edge representations. To predict the per-point labels, our SketchGCN uses graph convolution and a global-local branching network architecture to extract both intra-stroke and inter-stroke features. SketchGCN significantly improves the accuracy of the state-of-the-art methods for semantic sketch segmentation (by 11.4% in the pixel-basedmetric and 18.2% in the component-based metric over a large-scale challenging SPG dataset) and has magnitudes fewer parameters than both image-based and sequence-based methods.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yeh, Yen-Ting; Matulic, Fabrice; Vogel, Daniel
TI  - Demonstrating Finger-Based Dexterous Phone Gestures
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558645
ER  - 

TY  - NA
AU  - Setlur, Vidya; Tory, Melanie; Djalali, Alex
TI  - IUI - Inferencing underspecified natural language utterances in visual analysis
PY  - 2019
AB  - Handling ambiguity and underspecification of users' utterances is challenging, particularly for natural language interfaces that help with visual analytical tasks. Constraints in the underlying analytical platform and the users' expectations of high precision and recall require thoughtful inferencing to help generate useful responses. In this paper, we introduce a system to resolve partial utterances based on syntactic and semantic constraints of the underlying analytical expressions. We extend inferencing based on best practices in information visualization to generate useful visualization responses. We employ heuristics to help constrain the solution space of possible inferences, and apply ranking logic to the interpretations based on relevancy. We evaluate the quality of inferred interpretations based on relevancy and analytical usefulness.
SP  - 40
EP  - 51
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302270
ER  - 

TY  - NA
AU  - Li, Chao; Zhang, Qian; Zhao, Ziping; Gu, Li; Cummins, Nicholas; Schuller, Björn
TI  - IJCNN - Analysing and Inferring of Intimacy Based on fNIRS Signals and Peripheral Physiological Signals
PY  - 2019
AB  - Intimacy refers to a relatively long-lasting affinity relationship between individuals, which involves complex neuronal activities and physiological changes in the body. Recent advancements in the field of neuroimaging have demonstrated that functional near-infrared spectroscopy (fNIRS) has excellent potential for intimate relationship analysis. Signals such as fNIRS and physiological signals are increasingly utilised in this regard due to their consistency and complementarity. In this paper, first, we apply fNIRS and physiological database collected from 26 subjects when viewing lover, friend and stranger pictures to analyse and infer the intimacy. Then, the time domain information from both the fNIRS and physiological signals are utilised to exploit the representation of intimacy by General Linear Model (GLM) and Complex Brain Network Analysis (CBNA) methods. Based on these two methods, the intimacy can be analysed with different brain activation patterns. Finally, different machine learning techniques are utilised to predict the intimate relationship. The results demonstrate that multi-modal features are more efficient for intimacy research. Moreover, the average classification accuracy of ensemble learning is 98.72% whereas for KNN it is 91.03%.
SP  - 1
EP  - 8
JF  - 2019 International Joint Conference on Neural Networks (IJCNN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ijcnn.2019.8852418
ER  - 

TY  - NA
AU  - Lee, Bokyung; Han, Gyeol; Park, Jundong; Saakes, Daniel
TI  - CHI - Consumer to Creator: How Households Buy Furniture to Inform Design and Fabrication Interfaces
PY  - 2017
AB  - Emerging technologies for digital design and fabrication let people participate in the making of objects that were previously dominated by professional designers. A growing body of work in HCI provides understanding in the activities of designing and making by novices and in maker communities. However, we know little about how casual users might employ these technologies with the goal of having an object in their home that satisfies a need. We present a long-term qualitative study in which we followed 16 households during a purchasing process of furniture items for their homes. We looked into how families discover what they need, find solutions, realize a solution in their house and put it to use. The results provide insights into their design activities and workflow and we identify two distinct stages: understanding needs and prototyping a solution. Based on the findings, we discuss the social practice of acquiring and appropriating furniture in the home and within families, and identify design opportunities for digital design and fabrication to support people as they create the objects they need, want and desire.
SP  - 484
EP  - 496
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025666
ER  - 

TY  - CHAP
AU  - Petrovich, Mathis; Black, Michael J.; Varol, Gül
TI  - TEMOS: Generating Diverse Human Motions from Textual Descriptions
PY  - 2022
AB  - NA
SP  - 480
EP  - 497
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-20047-2_28
ER  - 

TY  - JOUR
AU  - Cheema, Sehrish Munawar; Tariq, Saman; Pires, Ivan Miguel
TI  - A natural language interface for automatic generation of data flow diagram using web extraction techniques
PY  - 2023
AB  - NA
SP  - NA
EP  - NA
JF  - Journal of King Saud University - Computer and Information Sciences
VL  - NA
IS  - NA
PB  - 
DO  - 10.1016/j.jksuci.2023.01.006
ER  - 

TY  - NA
AU  - Diogo, João; Rodrigues, Rui; Madeira, Rui; Correia, Nuno
TI  - Video Annotation Tool using Human Pose Estimation for Sports Training
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3570592
ER  - 

TY  - JOUR
AU  - Putze, Felix; Putze, Susanne; Sagehorn, Merle; Micek, Christopher; Solovey, Erin T.
TI  - Understanding HCI Practices and Challenges of Experiment Reporting with Brain Signals: Towards Reproducibility and Reuse
PY  - 2022
AB  - <jats:p>In human-computer interaction (HCI), there has been a push towards open science, but to date, this has not happened consistently for HCI research utilizing brain signals due to unclear guidelines to support reuse and reproduction. To understand existing practices in the field, this paper examines 110 publications, exploring domains, applications, modalities, mental states and processes, and more. This analysis reveals variance in how authors report experiments, which creates challenges to understand, reproduce, and build on that research. It then describes an overarching experiment model that provides a formal structure for reporting HCI research with brain signals, including definitions, terminology, categories, and examples for each aspect. Multiple distinct reporting styles were identified through factor analysis and tied to different types of research. The paper concludes with recommendations and discusses future challenges. This creates actionable items from the abstract model and empirical observations to make HCI research with brain signals more reproducible and reusable.</jats:p>
SP  - 1
EP  - 43
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 4
PB  - 
DO  - 10.1145/3490554
ER  - 

TY  - BOOK
AU  - Santos, André L.
TI  - Programming - Javardeye: Gaze Input for Cursor Control in a Structured Editor
PY  - 2021
AB  - Programmers spend a considerable time jumping through editing positions in the source code, often requiring the use of the mouse and/or arrow keys to position the cursor at the desired editing position. We developed Javardeye, a prototype code editor for Java integrated with eye tracking technology for controlling the editing cursor. Our implementation is based on a structured editor, leveraging on its particular characteristics, and augmenting it with a secondary—latent cursor—controlled by eye gaze. This paper describes the main design decisions and tradeoffs of our approach.
SP  - 31
EP  - 35
JF  - Companion Proceedings of the 5th International Conference on the Art, Science, and Engineering of Programming
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3464432.3464435
ER  - 

TY  - NA
AU  - Srinivasan, Arjun; Nyapathy, Nikhila; Lee, Bongshin; Drucker, Steven M.; Stasko, John
TI  - Collecting and Characterizing Natural Language Utterances for Specifying Data Visualizations.
PY  - 2021
AB  - Natural language interfaces (NLIs) for data visualization are becoming increasingly popular both in academic research and in commercial software. Yet, there is a lack of empirical understanding of how people specify visualizations through natural language. To bridge this gap, we conducted an online study with 102 participants. We showed participants a series of ten visualizations for a given dataset and asked them to provide utterances they would pose to generate the displayed charts. The curated list of utterances generated from the study is provided below. This corpus of utterances can be used to evaluate existing NLIs for data visualization as well as for creating new systems and models to generate visualizations from natural language utterances.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445400
ER  - 

TY  - NA
AU  - Siddiqui, Tarique; Luh, Paul; Wang, Zesheng; Karahalios, Karrie; Parameswaran, Aditya
TI  - SIGMOD Conference - ShapeSearch: A Flexible and Efficient System for Shape-based Exploration of Trendlines
PY  - 2020
AB  - Identifying trendline visualizations with desired patterns is a common task during data exploration. Existing visual analytics tools offer limited flexibility, expressiveness, and scalability for such tasks, especially when the pattern of interest is under-specified and approximate. We propose ShapeSearch, an efficient and flexible pattern-searching tool, that enables the search for desired patterns via multiple mechanisms: sketch, natural-language, and visual regular expressions. We develop a novel shape querying algebra, with a minimal set of primitives and operators that can express a wide variety of shape search queries, and design a natural- language and regex-based parser to translate user queries to the algebraic representation. To execute these queries within interactive response times, ShapeSearch uses a fast shape algebra execution engine with query-aware optimizations, and perceptually-aware scoring methodologies. We present a thorough evaluation of the system, including a user study, a case study involving genomics data analysis, as well as performance experiments, comparing against state-of-the-art trendline shape matching approaches-that together demonstrate the usability and scalability of ShapeSearch.
SP  - 51
EP  - 65
JF  - Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3318464.3389722
ER  - 

TY  - NA
AU  - Petersen, Marianne Graves; Rasmussen, Majken Kirkegaard; Trettvik, Johan
TI  - Conference on Designing Interactive Systems - Affordances of Shape-Changing Interfaces: An Information Perspective on Transformability and Movement
PY  - 2020
AB  - Affordances is an important term in the field of Shape-Changing Interfaces (SCI). But the field is challenged by a multitude of vaguely defined concepts around affordances such as dynamic, spatial, or tactile affordances. In line with this, Alexander et al [1] recently pointed to lack of theory as one of the grand challenges for SCI. This paper proposes a re-analysis of Gibson for resolving the conceptual cacophony in the field of SCI. Essential to the ecological approach to affordances, is that perception is dynamic and people pick up information through exploring objects and environments over time. From this perspective, affordances of Shape-Changing Interfaces become a matter of providing Information - through the design - regarding a) the interfaces' ability to transform its shape and b) how movement suggests level of animacy. Through analyzing scholarly SCI prototypes, from this perspective, we provide an initial set of design strategies for affordances of shape-changing interfaces.
SP  - 1959
EP  - 1971
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395521
ER  - 

TY  - JOUR
AU  - Moritz, Dominik; Wang, Chenglong; Nelson, Greg L.; Lin, Halden; Smith, Adam M.; Howe, Bill; Heer, Jeffrey
TI  - Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco
PY  - 2018
AB  - There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.
SP  - 438
EP  - 448
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865240
ER  - 

TY  - BOOK
AU  - Chowdhury, Imran; Moeid, Abdul; Hoque, Enamul; Kabir, Muhammad Ashad; Hossain, Sabir; Islam, Mohammad Mainul
TI  - IV - MIVA: Multimodal Interactions for Facilitating Visual Analysis with Multiple Coordinated Views
PY  - 2020
AB  - Typically, people perform visual data analysis using mouse and touch interactions. While such interactions are often easy to use, they can be inadequate for users to express complex information and may require many steps to complete a task. Recently natural language interaction has emerged as a promising technique for supporting exploration with visualization, as the user can express a complex analytical question more easily. In this paper, we investigate how to synergistically combine language and mouse-based direct manipulations so that weakness of one modality can be complemented by the other. To this end, we have developed a novel system, named Multimodal Interactions System for Visual Analysis (MIVA), that allows user to provide input using both natural language (e.g., through speech) and direct manipulation (e.g., through mouse or touch) and presents the answer accordingly. To answer the current question in the context of past interactions, the system incorporates previous utterances and direct manipulations made by the user within a finite-state model. We tested the applicability of MIVA on several dashboards including a COVID-19 dashboard that visualizes coronavirus cases around the globe. Our demonstration provides initial indication that the MIVA system enhances the flow of visual analysis by enabling fluid, iterative exploration and refinement of data in a dashboard with multiple-coordinated views.
SP  - 674
EP  - 677
JF  - 2020 24th International Conference Information Visualisation (IV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iv51561.2020.00124
ER  - 

TY  - NA
AU  - Shibata, Tomoki; Borisenko, Alena; Hakone, Anzu; August, Tal; Deligiannidis, Leonidas; Yu, Chen-Hsiang; Russell, Matthew; Olwal, Alex; Jacob, Robert J. K.
TI  - AH - An Implicit Dialogue Injection System for Interruption Management
PY  - 2019
AB  - This paper presents our efforts in redesigning the conventional on/off interruption management tactic (a.k.a. "Do Not Disturb Mode") for situations where interruptions are inevitable. We introduce an implicit dialogue injection system, in which the computer implicitly observes the user's state of busyness from passive measurement of the prefrontal cortex to determine how to interrupt the user. We use functional Near-Infrared Spectroscopy (fNIRS), a noninvasive brain-sensing technique. In this paper, we describe our system architecture and report results of our proof-of-concept study, in which we compared two contrasting interruption strategies; the computer either forcibly interrupts the user with a secondary task or requests the user's participation before presenting it. The latter yielded improved user experience (e.g. lower reported annoyance), in addition to showing a potential improvement in task performance (i.e. retaining context information) when the user was busier. We conclude that tailoring the presentation of interruptions based on real-time user state provides a step toward making computers more considerate of their users.
SP  - 27
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311875
ER  - 

TY  - NA
AU  - Fujia, Hiroki; Nakano, Akito; Hada, Hisakazu
TI  - Tangible and Embedded Interaction - Elsa: Temporary Ice Jet 3D Printing
PY  - 2017
AB  - In this paper, we propose a method to instantly create and insert 3D printing by using ice spray, which vanishes completely and does not produce any waste. Ice-spray printing provides a handy and speedy object molding capability, which allows individuals to freely print their ideas. We present a system that creates ice pillars almost identical in lengths by using image-processing technology and Arduino to control the movement of ice sprays.
SP  - 559
EP  - 563
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3025093
ER  - 

TY  - NA
AU  - Akiyama, Yoh; Miyashita, Homei
TI  - UIST (Adjunct Volume) - Fitter: A System for Easily Printing Objects that Fit Real Objects
PY  - 2016
AB  - When printing both self-making and existing 3D models, users often create models to fit to a real object within it. Fitting models to the size of a real object is a delicate problem. To address it, we present a concept to capture the size of a real object, create or modify a model that conforms to the captured image, and print the model on the spot. We create a 3D printer to realize this concept by installing a touch panel display in the build plate system. In this paper, we focus on creating containers that fit accessories. We create containers for a pair of scissors, a smart watch, a drone, a pair of glasses, and a pen holder.
SP  - 129
EP  - 131
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984751.2985730
ER  - 

TY  - JOUR
AU  - Rencis, Edgars
TI  - Application of a Configurable Keywords-Based Query Language to the Healthcare Domain
PY  - 2021
AB  - NA
SP  - 142
EP  - 147
JF  - Journal of Advances in Information Technology
VL  - 12
IS  - 2
PB  - 
DO  - 10.12720/jait.12.2.142-147
ER  - 

TY  - NA
AU  - Fast, Ethan; Chen, Binbin; Mendelsohn, Julia; Bassen, Jonathan; Bernstein, Michael S.
TI  - Iris: A Conversational Agent for Complex Tasks
PY  - 2017
AB  - Today's conversational agents are restricted to simple standalone commands. In this paper, we present Iris, an agent that draws on human conversational strategies to combine commands, allowing it to perform more complex tasks that it has not been explicitly designed to support: for example, composing one command to "plot a histogram" with another to first "log-transform the data". To enable this complexity, we introduce a domain specific language that transforms commands into automata that Iris can compose, sequence, and execute dynamically by interacting with a user through natural language, as well as a conversational type system that manages what kinds of commands can be combined. We have designed Iris to help users with data science tasks, a domain that requires support for command combination. In evaluation, we find that data scientists complete a predictive modeling task significantly faster (2.6 times speedup) with Iris than a modern non-conversational programming environment. Iris supports the same kinds of commands as today's agents, but empowers users to weave together these commands to accomplish complex goals.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Priya, Khyati
TI  - Perceived Affordances in Programmable Matter
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Companion Proceedings of the 2022 Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532104.3571463
ER  - 

TY  - NA
AU  - Mutasim, Aunnoy; Batmaz, Anil Ufuk; Hudhud Mughrabi, Moaaz; Stuerzlinger, Wolfgang
TI  - Performance Analysis of Saccades for Primary and Confirmatory Target Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565619
ER  - 

TY  - JOUR
AU  - Paredes, Luis; McMillan, Caroline; Chan, Wan Kyn; Chandrasegaran, Senthil; Singh, Ramyak; Ramani, Karthik; Wilde, Danielle
TI  - CHIMERA
PY  - 2021
AB  - <jats:p>Wearable technologies draw on a range of disciplines, including fashion, textiles, HCI, and engineering. Due to differences in methodology, wearables researchers can experience gaps or breakdowns in values, goals, and vocabulary when collaborating. This situation makes wearables development challenging, even more so when technologies are in the early stages of development and their technological and cultural potential is not fully understood. We propose a common ground to enhance the accessibility of wearables-related resources. The objective is to raise awareness and create a convergent space for researchers and developers to both access and share information across domains. We present CHIMERA, an online search interface that allows users to explore wearable technologies beyond their discipline. CHIMERA is powered by a Wearables Taxonomy and a database of research, tutorials, aesthetic approaches, concepts, and patents. To validate CHIMERA, we used a design task with multidisciplinary designers, an open-ended usability study with experts, and a usability survey with students of a wearables design class. Our findings suggest that CHIMERA assists users with different mindsets and skillsets to engage with information, expand and share knowledge when developing wearables. It forges common ground across divergent disciplines, encourages creativity, and affords the formation of inclusive, multidisciplinary perspectives in wearables development.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494974
ER  - 

TY  - NA
AU  - Zhu, Fengyuan; Lyu, Zhuoyue; Sousa, Mauricio; Grossman, Tovi
TI  - Touching The Droid: Understanding and Improving Touch Precision With Mobile Devices in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00099
ER  - 

TY  - NA
AU  - Chen, Zhaokang; Shi, Bertram E.
TI  - Improving Gaze-based Selection using Variable Dwell Time.
PY  - 2017
AB  - In order to avoid the "Midas Touch" problem, gaze-based interfaces for selection often introduce a dwell time: a fixed amount of time the user must fixate upon an object before it is selected. Past interfaces have used a uniform dwell time across all objects. Here, we propose an algorithm for adjusting the dwell times of different objects based on the inferred probability that the user wishes to select them. In particular, we introduce a probabilistic model of gaze behavior and train it on natural gaze trajectories collected while users surf the web through a gaze-based web browser. Using this model, we can infer the probability that each hyperlink on the web page is the one the user intends to select. We assign shorter dwell times to more likely hyperlinks and longer dwell times to less likely hyperlinks. The resulting variable dwell time gaze-based browser enables subjects to surf the web faster and/or more reliably. We use a two-stage hidden Markov model based algorithm to model the natural eye gaze trajectories. We have evaluated this method objectively both in simulation and experimentally, and subjectively through questionnaires. Our results demonstrate that the proposed algorithm achieves a better tradeoff between accuracy and speed. This work demonstrates that natural eye movements can be used to infer user intent, and that this information can be used to improve gaze-based selection without increased cognitive load on users.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wu, Jiang; Liu, Dongyu; Guo, Ziyang; Wu, Yingcai
TI  - RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports.
PY  - 2022
AB  - Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data - which is often recorded as multivariate event sequences - to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209452
ER  - 

TY  - NA
AU  - Setlur, Vidya; Kumar, Arathi
TI  - Sentifiers: Interpreting Vague Intent Modifiers in Visual Analysis using Word Co-occurrence and Sentiment Analysis
PY  - 2020
AB  - Natural language interaction with data visualization tools often involves the use of vague subjective modifiers in utterances such as "show me the sectors that are performing" and "where is a good neighborhood to buy a house?." Interpreting these modifiers is often difficult for these tools because their meanings lack clear semantics and are in part defined by context and personal user preferences. This paper presents a system called \system that makes a first step in better understanding these vague predicates. The algorithm employs word co-occurrence and sentiment analysis to determine which data attributes and filters ranges to associate with the vague predicates. The provenance results from the algorithm are exposed to the user as interactive text that can be repaired and refined. We conduct a qualitative evaluation of the Sentifiers system that indicates the usefulness of the interface as well as opportunities for better supporting subjective utterances in visual analysis tasks through natural language.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Turner, Colton J.; Chaparro, Barbara S.; He, Jibo
TI  - Texting while walking: is it possible with a smartwatch?
PY  - 2018
AB  - Smartwatches are quickly becoming a popular complement to smartphones for notifications and activity tracking, yet most lack an effective method for text input. Typing on a smartwatch with an onscreen keyboard was originally thought to be impractical due to the small screen size. As a result, alternative keyboards that use "zoom" features to enlarge key size were developed as a potential solution. However, observed typing speeds with alternative keyboards are slow, and they often have a steep learning curve. Recent research, in a lab setting using a more familiar full QWERTY onscreen keyboard, demonstrated that it is possible to type quickly on a smartwatch while seated. Given the ubiquitous and mobile nature of smartwatches, this study examines typing performance using a full QWERTY keyboard while mobile. Participants typed using two different text input methods---trace and tap---with their index finger while standing and while walking. Results show participants typed faster with trace (35 words per minute) than tap (30 words per minute), regardless of whether they were standing or walking or whether they had prior experience with trace input. These typing speeds are among the fastest reported in the smartwatch literature. Typing accuracy was also better for trace than for tap and better when standing than while walking. Subjectively, participants rated trace easier to use, preferred it over tap, and suggested they would use it in the future if available. Recommendations to include a full QWERTY keyboard on all smartwatch designs are discussed.
SP  - 94
EP  - 118
JF  - Journal of Usability Studies archive
VL  - 13
IS  - 2
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Vatavu, Radu-Daniel; Mossel, Annette; Schönauer, Christian
TI  - MobileHCI - Digital vibrons: understanding users' perceptions of interacting with invisible, zero-weight matter
PY  - 2016
AB  - We investigate in this work users' perceptions of interacting with invisible, zero-weight digital matter for smart mobile scenarios. To this end, we introduce the concept of a digital vibron as vibrational manifestation of a digital object located outside its container device. We exemplify gesture-based interactions for digital vibrons and show how thinking about interactions in terms of digital vibrons can lead to new interactive experiences in the physical-digital space. We present the results of a user study that showed high scores of users' perceived experience, usability, and desirability, and we discuss users' preferences for vibration patterns to inform the design of vibrotactile feedback for digital vibrons. We hope that this work will inspire researchers and practitioners to further explore and develop digital vibrons to design localized vibrotactile feedback for digital objects outside their smart devices toward new interactive experiences in the physical-digital space.
SP  - 217
EP  - 226
JF  - Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2935334.2935364
ER  - 

TY  - BOOK
AU  - Siddiqui, Nadia; Hoque, Enamul
TI  - IV - ConVisQA: A Natural Language Interface for Visually Exploring Online Conversations
PY  - 2020
AB  - There has been an exponential growth of asynchronous online conversations thanks to the rise of social media. Analyzing and gaining insights from such conversations can be quite challenging for a user, especially when the discussion becomes very long. Traditional sites present a conversation in a paginated list view, making it very difficult to find comments of interests about a specific topic and/or opinions which may be scattered around a long thread of discussion. In this paper, we introduce a natural language interface that supports the user to quickly locate and browse through the comments that are relevant to her information needs. Our system takes a question asked by the reader about a conversation as input and then automatically finds the answer using natural language processing techniques. It then presents the results by highlighting in a visual interface, enabling the user to quickly navigate through the comments that match her information needs. Our case studies with three users suggest that the system can help the user to effectively fulfill her information needs by highlighting the relevant comments to their question.
SP  - 440
EP  - 447
JF  - 2020 24th International Conference Information Visualisation (IV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iv51561.2020.00077
ER  - 

TY  - JOUR
AU  - Islam, Md Rafiqul; Razzak, Imran; Wang, Xianzhi; Tilocca, Peter; Xu, Guandong
TI  - Natural language interactions enhanced by data visualization to explore insurance claims and manage risk
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Annals of Operations Research
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10479-021-04465-7
ER  - 

TY  - JOUR
AU  - Lee, Doris Jung Lin; Parameswaran, Aditya
TI  - The Case for a Visual Discovery Assistant: A Holistic Solution for Accelerating Visual Data Exploration.
PY  - 2018
AB  - NA
SP  - 3
EP  - 14
JF  - IEEE Data(base) Engineering Bulletin
VL  - 41
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chung, Jonathan
TI  - Analysis of Visual Scanning Behaviour in Patients with Psychiatric Disorders
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Kato, Kunihiro; Kawahara, Yoshihiro
TI  - UIST (Adjunct Volume) - LightTouch: Passive Gadgets for Extending Interactions on Capacitive Touchscreens by Automating Touch Inputs
PY  - 2020
AB  - We present LightTouch, a passive gadget to enhance touch interactions on unmodified capacitive touchscreens. It simulates finger operations such as tapping, swiping, or multi-touch gestures using conductive materials and photoresistors embedded inside the objects. The touchscreen emits visible light and the photoresistor senses the level of this light, which changes its resistance value. By controlling the screen brightness, it connects or disconnects the path between the GND and the touchscreen, thus allowing the touch inputs to be controlled. In contrast to conventional physical extensions for touchscreens, our technique does not require continuous finger contact on the conductive part nor the use of batteries. Our technique opens up new possibilities for touch interaction such as for enhancing the trackability of tangibles beyond the simple automation of touch inputs.
SP  - 10
EP  - 12
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416169
ER  - 

TY  - NA
AU  - Shi, Danqing; Guo, Yi; Guo, Mingjuan; Wu, Yanqiu; Chen, Qing; Cao, Nan
TI  - Talk2Data: High-Level Question Decomposition for Data-Oriented Question and Answering
PY  - 2021
AB  - Through a data-oriented question and answering system, users can directly "ask" the system for the answers to their analytical questions about the input tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on providing a concrete query for users or untangling the ambiguities in a specific question so that the system could better understand questions and provide more correct and precise answers. However, when users have little knowledge about the data, it is difficult for them to ask concrete questions. Instead, high-level questions are frequently asked, which cannot be easily solved with the existing techniques. To address the issue, in this paper, we introduce Talk2Data, a data-oriented online question and answering system that supports answering both low-level and high-level questions. It leverages a novel deep-learning model to resolve high-level questions into a series of low-level questions that can be answered by data facts. These low-level questions could be used to gradually elaborate the users' requirements. We design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We evaluate the effectiveness of the Talk2Data system via a series of evaluations including case studies, performance validation, and a controlled user study. The results show the power of the system.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Shao, Yutong; Nakashole, Ndapa
TI  - ACL - ChartDialogs: Plotting from Natural Language Instructions.
PY  - 2020
AB  - This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a non-trivial challenge for future research on this task.
SP  - 3559
EP  - 3574
JF  - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2020.acl-main.328
ER  - 

TY  - JOUR
AU  - Chen, Zhutian; Yang, Qisen; Xie, Xiao; Beyer, Johanna; Xia, Haijun; Wu, Yingcai; Pfister, Hanspeter
TI  - Sporthesia: Augmenting Sports Videos Using Natural Language.
PY  - 2022
AB  - Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural language, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we propose a three-step approach - 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video - and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia's applicability in two exemplar scenarios, i.e., authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209497
ER  - 

TY  - NA
AU  - Stojanov, Stefan; Talathi, Sachin S; Sharma, Abhishek
TI  - The Benefits of Depth Information for Head-Mounted Gaze Estimation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529638
ER  - 

TY  - JOUR
AU  - Khurana, Diksha; Koli, Aditya; Khatter, Kiran; Singh, Sukhdev
TI  - Natural language processing: state of the art, current trends and challenges.
PY  - 2022
AB  - Natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. In this paper, we first distinguish four phases by discussing different levels of NLP and components of <b>N</b>atural <b>L</b>anguage <b>G</b>eneration followed by presenting the history and evolution of NLP. We then discuss in detail the state of the art presenting the various applications of NLP, current trends, and challenges. Finally, we present a discussion on some available datasets, models, and evaluation metrics in NLP.
SP  - 3713
EP  - 3744
JF  - Multimedia tools and applications
VL  - 82
IS  - 3
PB  - 
DO  - 10.1007/s11042-022-13428-4
ER  - 

TY  - NA
AU  - Rodrigues, Rui; Neves Madeira, Rui; Correia, Nuno
TI  - Studying Natural User Interfaces for Smart Video Annotation towards Ubiquitous Environments
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3490672
ER  - 

TY  - JOUR
AU  - Pandey, Aditeya; Srinivasan, Arjun; Setlur, Vidya
TI  - MEDLEY: Intent-based Recommendations to Support Dashboard Composition.
PY  - 2022
AB  - Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present MEDLEY, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. MEDLEY also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how MEDLEY's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209421
ER  - 

TY  - NA
AU  - Hullman, Jessica; Gelman, Andrew
TI  - To design interfaces for exploratory data analysis, we need theories of graphical inference.
PY  - 2021
AB  - Research and development in computer science and statistics have produced increasingly sophisticated software interfaces for interactive and exploratory analysis, optimized for easy pattern finding and data exposure. But design philosophies that emphasize exploration over other phases of analysis risk confusing a need for flexibility with a conclusion that exploratory visual analysis is inherently model-free and cannot be formalized. We describe how without a grounding in theories of human statistical inference, research in exploratory visual analysis can lead to contradictory interface objectives and representations of uncertainty that can discourage users from drawing valid inferences. We discuss how the concept of a model check in a Bayesian statistical framework unites exploratory and confirmatory analysis, and how this understanding relates to other proposed theories of graphical inference. Viewing interactive analysis as driven by model checks suggests new directions for software and empirical research around exploratory and visual analysis. For example, systems should enable specifying and explicitly comparing data to null and other reference distributions and better representations of uncertainty. Implications of Bayesian and other theories of graphical inference should be tested against outcomes of interactive analysis by people to drive theory development.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yang, Lumin; Zhuang, Jiajie; Fu, Hongbo; Wei, Xiangzhi; Zhou, Kun; Zheng, Youyi
TI  - SketchGNN: Semantic Sketch Segmentation with Graph Neural Networks.
PY  - 2020
AB  - We introduce SketchGNN, a convolutional graph neural network for semantic segmentation and labeling of freehand vector sketches. We treat an input stroke-based sketch as a graph, with nodes representing the sampled points along input strokes and edges encoding the stroke structure information. To predict the per-node labels, our SketchGNN uses graph convolution and a static-dynamic branching network architecture to extract the features at three levels, i.e., point-level, stroke-level, and sketch-level. SketchGNN significantly improves the accuracy of the state-of-the-art methods for semantic sketch segmentation (by 11.2% in the pixel-based metric and 18.2% in the component-based metric over a large-scale challenging SPG dataset) and has magnitudes fewer parameters than both image-based and sequence-based methods.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Onishi, Ryoya; Morisaki, Tao; Suzuki, Shun; Mizutani, Saya; Kamigaki, Takaaki; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - GazeBreath: Input Method Using Gaze Pointing and Breath Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519405
ER  - 

TY  - NA
AU  - Neshati, Ali; Salo, Aaron; Faleel, Shariff Am; Li, Ziming; Liang, Hai-Ning; Latulipe, Celine; Irani, Pourang
TI  - EdgeSelect: Smartwatch Data Interaction with Minimal Screen Occlusion
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556586
ER  - 

TY  - NA
AU  - Orii, Lisa; Ogawa, Nami; Hatada, Yuji; Narumi, Takuji
TI  - Designing for Speech Practice Systems: How Do User-Controlled Voice Manipulation and Model Speakers Impact Self-Perceptions of Voice?
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502093
ER  - 

TY  - NA
AU  - Choi, Myungguen; Sakamoto, Daisuke; Ono, Tetsuo
TI  - Kuiper Belt: Utilizing the "Out-of-natural Angle" Region in the Eye-gaze Interaction for Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517725
ER  - 

TY  - NA
AU  - Wasser, Joscha; Baltzer, Marcel C. A.; Flemisch, Frank O.
TI  - A Gaze- vs. Joystick-Based Interaction Method for a Remote Reconnaissance Task
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544999.3554784
ER  - 

TY  - NA
AU  - Seetharaman, Prem; Mysore, Gautham J.; Pardo, Bryan; Smaragdis, Paris; Gomes, Celso
TI  - CHI - VoiceAssist: Guiding Users to High-Quality Voice Recordings
PY  - 2019
AB  - Voice recording is a challenging task with many pitfalls due to sub-par recording environments, mistakes in recording setup, microphone quality, etc. Newcomers to voice recording often have difficulty recording their voice, leading to recordings with low sound quality. Many amateur recordings of poor quality have two key problems: too much reverberation (echo), and too much background noise (e.g. fans, electronics, street noise). We present VoiceAssist, a system that helps inexperienced users produce high quality recordings by providing real-time visual feedback on audio quality. We integrate modern audio quality measures into an interactive human-machine feedback loop, so that the audio quality can be maximized at capture-time. We demonstrate the utility of this feedback for improving the recording quality with a user study. When presented with visual feedback about recording quality, users produced recordings that were strongly preferred by third-party listeners, when compared to recordings made without this feedback.
SP  - 309
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300539
ER  - 

TY  - NA
AU  - Zhang, Tianyi; Chen, Zhiyang; Zhu, Yuanli; Vaithilingam, Priyan; Wang, Xinyu; Glassman, Elena L.
TI  - CHI - Interpretable Program Synthesis
PY  - 2021
AB  - Program synthesis, which generates programs based on user-provided specifications, can be obscure and brittle: users have few ways to understand and recover from synthesis failures. We propose interpretable program synthesis, a novel approach that unveils the synthesis process and enables users to monitor and guide a synthesizer. We designed three representations that explain the underlying synthesis process with different levels of fidelity. We implemented an interpretable synthesizer for regular expressions and conducted a within-subjects study with eighteen participants on three challenging regex tasks. With interpretable synthesis, participants were able to reason about synthesis failures and provide strategic feedback, achieving a significantly higher success rate compared with a state-of-the-art synthesizer. In particular, participants with a high engagement tendency (as measured by NCS-6) preferred a deductive representation that shows the synthesis process in a search tree, while participants with a relatively low engagement tendency preferred an inductive representation that renders representative samples of programs enumerated during synthesis.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445646
ER  - 

TY  - JOUR
AU  - Lee, Tak Yeon; Bederson, Benjamin B.
TI  - Give the people what they want: studying end-user needs for enhancing the web
PY  - 2016
AB  - <jats:p>End-user programming (EUP) is a common approach for helping ordinary people create small programs for their professional or daily tasks. Since end-users may not have programming skills or strong motivation for learning them, tools should provide what end-users want with minimal costs of learning –i.e., they must decrease the barriers to entry. However, it is often hard to address these needs, especially for fast-evolving domains such as the Web. To better understand these existing and ongoing challenges, we conducted two formative studies with Web users –a semi-structured interview study, and a Wizard-of-Oz study. The interview study identifies challenges that participants have with their daily experiences on the Web. The Wizard-of-Oz study investigates how participants would naturally explain three computational tasks to an interviewer acting as a hypothetical computer agent. These studies demonstrate a disconnect between what end-users want and what existing EUP systems support, and thus open the door for a path towards better support for end user needs. In particular, our findings include: (1) analysis of challenges that end-users experience on the Web with solutions; (2) seven core functionalities of EUP for addressing these challenges; (3) characteristics of non-programmers describing three common computation tasks; (4) design implications for future EUP systems.</jats:p>
SP  - e91
EP  - NA
JF  - PeerJ Computer Science
VL  - 2
IS  - NA
PB  - 
DO  - 10.7717/peerj-cs.91
ER  - 

TY  - JOUR
AU  - Epp, Felix Anand
TI  - Expressive Wearables: Practices-Oriented Codesign for New Forms of Social Mobile Technology
PY  - 2019
AB  - <jats:p>Self-expression is a vital practice for a functioning social life. Wearables have become expressive everyday products, while studies showed how physical collocation can be an opportunity for social technology. This article identifies a perspective for future design of wearables as an extension of the body in its social context: designing for diversity in expression with respect to social boundaries. The collected literature demonstrates the development of new forms of expressive wearables that challenges norms of dress and three groups of participatory methods enable re-search into everyday life practices. The two initial studies—inquiring into everyday life and exploring the wearable design for new practices—exemplify these methods and point a way forward with a focus for design on distinct practices of self-expression.</jats:p>
SP  - 1
EP  - 15
JF  - International Journal of Mobile Human Computer Interaction
VL  - 11
IS  - 4
PB  - 
DO  - 10.4018/ijmhci.2019100101
ER  - 

TY  - NA
AU  - Uhde, Alarith; Hassenzahl, Marc
TI  - Towards a Better Understanding of Social Acceptability
PY  - 2021
AB  - Social contexts play an important role in understanding acceptance and use of technology. However, current approaches used in HCI to describe contextual influence do not capture it appropriately. On the one hand, the often used Technology Acceptance Model and related frameworks are too rigid to account for the nuanced variations of social situations. On the other hand, Goffman's dramaturgical model of social interactions emphasizes interpersonal relations but mostly overlooks the material (e.g., technology) that is central to HCI. As an alternative, we suggest an approach based on Social Practice Theory. We conceptualize social context as interactions between co-located social practices and acceptability as a matter of their (in)compatibilities. Finally, we outline how this approach provides designers with a better understanding of different types of social acceptability problems and helps finding appropriate solutions.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451649
ER  - 

TY  - NA
AU  - Lee, Tak Yeon; Dugan, Casey; Bederson, Benjamin B.
TI  - IUI - Towards Understanding Human Mistakes of Programming by Example: An Online User Study
PY  - 2017
AB  - Programming-by-Example (PBE) enables users to create programs without writing a line of code. However, there is little research on people's ability to accomplish complex tasks by providing examples, which is the key to successful PBE solutions. This paper presents an online user study, which reports observations on how well people decompose complex tasks, and disambiguate sub-tasks. Our findings suggest that disambiguation and decomposition are difficult for inexperienced users. We identify seven types of mistakes made, and suggest new opportunities for actionable feedback based on unsuccessful examples, with design implications for future PBE systems.
SP  - 257
EP  - 261
JF  - Proceedings of the 22nd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025171.3025203
ER  - 

TY  - NA
AU  - Narita, Minori; Maudet, Nolwenn; Lu, Yi; Igarashi, Takeo
TI  - IUI - Data-centric disambiguation for data transformation with programming-by-example
PY  - 2021
AB  - Programming-by-example (PBE), can be a powerful tool to reduce manual work in repetitive data transformation tasks. However, few examples often leave ambiguity and may cause undesirable data transformation by the system. This ambiguity can be resolved by allowing the user to directly edit the synthesized programs; however, this is difficult for non-programmers. Here, we present a novel approach: data-centric disambiguation for data transformation, where users resolve the ambiguity in data transformation by examining and modifying the output rather than the program. The key idea is to focus on the given set of data the user wants to transform instead of pursuing the synthesized program’s generality or completeness. Our system provides visualization and interaction methods that allow users to efficiently examine and fix the transformed outputs, which is much simpler than understanding and modifying the program itself. The user study suggests that our system can successfully help non-programmers to more easily and efficiently process data.
SP  - 454
EP  - 463
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450680
ER  - 

TY  - NA
AU  - Banavar, Mahesh K.; Wickramasinghe, Shandeepa; Achalla, Monalisa; Sun, Jie
TI  - Ordinal UNLOC: Target Localization with Noisy and Incomplete Distance Measures
PY  - 2021
AB  - A main challenge in target localization arises from the lack of reliable distance measures. This issue is especially pronounced in indoor settings due to the presence of walls, floors, furniture, and other dynamically changing conditions such as the movement of people and goods, varying temperature, and airflows. Here, we develop a new computational framework to estimate the location of a target without the need for reliable distance measures. The method, which we term Ordinal UNLOC, uses only ordinal data obtained from comparing the signal strength from anchor pairs at known locations to the target. Our estimation technique utilizes rank aggregation, function learning as well as proximity-based unfolding optimization. As a result, it yields accurate target localization for common transmission models with unknown parameters and noisy observations that are reminiscent of practical settings. Our results are validated by both numerical simulations and hardware experiments.
SP  - NA
EP  - NA
JF  - arXiv: Signal Processing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Koelle, Marion; Boll, Susanne; Olsson, Thomas; Williamson, Julie R.; Profita, Halley; Kane, Shaun K.; Mitchell, Robb
TI  - CHI Extended Abstracts - (Un)Acceptable!?!: Re-thinking the Social Acceptability of Emerging Technologies
PY  - 2018
AB  - A central viewpoint to understanding the human aspects of interactive systems is the concept of technology acceptance. Actual, or imagined disapproval from other people can have a major impact on how information technological innovations are received, but HCI lacks comprehensive, up-to date, and actionable, articulations of "social acceptability". The spread of information and communication technologies (ICT) into all aspects of our lives appears to have dramatically increased the range and scale of potential issues with social acceptance. This workshop brings together academics and practitioners to discuss what social acceptance and acceptability mean in the context of various emerging technologies and modern human-computer interaction. We aim to bring the concept of social acceptability in line with the current technology landscape, as well as to identify relevant research steps for making it more useful, actionable and researchable with well-operationalized metrics.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3170620
ER  - 

TY  - NA
AU  - Logas, Jacob; Lin, Georgianna; Belan, Kelsie; Gogate, Advait; Starner, Thad
TI  - AHs - Conversational Partner’s Perception of Subtle Display Use for Monitoring Notifications
PY  - 2021
AB  - We examine whether the gaze direction of a user reveals the use of a subtle display during a face-to-face conversation with a partner who is not initially aware of the display. We measure twelve participants’ perceptions of a casual conversational partner’s engagement between a control condition of no notification and notifications displayed behind the participant’s head at 0, 10, and 20 degrees to the right of the conversational partner’s line of sight. No differences in reported conversational engagement were found. However, once the presence of the display was revealed, engagement scores went down over all conditions compared to the prior uninformed variant of the experiment. Still, no difference was found between the control and the subtle display conditions, and informed participants were only 40% accurate on average in detecting the use of the display. In a second study comparing subtle display user engagement with smartwatch user engagement, six participants rated a conversational partner more distracted when the partner used a smartwatch to monitor notifications than when the partner used a display secretly mounted behind the participant’s head. Participants in both studies did not realize the presence of the display until it was revealed. These results suggest that eye movement when using a subtle display detracts less from the conversational experience than the use of a smartwatch.
SP  - 101
EP  - 110
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458942
ER  - 

TY  - NA
AU  - Pearson, Jennifer; Robinson, Simon; Jones, Matt; Joshi, Anirudha; Ahire, Shashank; Sahoo, Deepak Ranjan; Subramanian, Sriram
TI  - CHI - Chameleon Devices: Investigating More Secure and Discreet Mobile Interactions via Active Camouflaging
PY  - 2017
AB  - Many users value the ability to have quick and frequent sight of their mobiles when in public settings. However, in doing so, they expose themselves to potential risks, ranging from being targets of robbery to the more subtle social losses through being seen to be rude or inattentive to those around them. In nature, some animals can blend into their environments to avoid being eaten or to reduce their impact on the ecosystem around them. Taking inspiration from these evolved systems we investigate the notion of chameleon approaches for mobile interaction design. Our probes were motivated, inspired and refined through extended interactions with people drawn from contexts with differing ranges of security and privacy concerns. Through deployments on users' own devices, our prototypes show the value of the concept. The encouraging results motivate further research in materials and form factors that can provide more effective automatic plain-sight hiding.
SP  - 5184
EP  - 5196
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025482
ER  - 

TY  - JOUR
AU  - ChughRavi, ; HempelBrian, ; SpradlinMitchell, ; AlbersJacob, 
TI  - Programmatic and direct manipulation, together at last
PY  - 2016
AB  - Direct manipulation interfaces and programmatic systems have distinct and complementary strengths. The former provide intuitive, immediate visual feedback and enable rapid prototyping, whereas the ...
SP  - 341
EP  - 354
JF  - ACM SIGPLAN Notices
VL  - 51
IS  - 6
PB  - 
DO  - 10.1145/2980983.2908103
ER  - 

TY  - JOUR
AU  - Gavran, Ivan; Darulova, Eva; Majumdar, Rupak
TI  - Interactive synthesis of temporal specifications from examples and natural language
PY  - 2020
AB  - Motivated by applications in robotics, we consider the task of synthesizing linear temporal logic (LTL) specifications based on examples and natural language descriptions. While LTL is a flexible, expressive, and unambiguous language to describe robotic tasks, it is often challenging for non-expert users. In this paper, we present an interactive method for synthesizing LTL specifications from a single example trace and a natural language description. The interaction is limited to showing a small number of behavioral examples to the user who decides whether or not they exhibit the original intent. Our approach generates candidate LTL specifications and distinguishing examples using an encoding into optimization modulo theories problems. Additionally, we use a grammar extension mechanism and a semantic parser to generalize synthesized specifications to parametric task descriptions for subsequent use. Our implementation in the tool LtlTalk starts with a domain-specific language that maps to a fragment of LTL and expands it through example-based user interactions, thus enabling natural language-like robot programming, while maintaining the expressive power and precision of a formal language. Our experiments show that the synthesis method is precise, quick, and asks only a few questions to the users, and we demonstrate in a case study how LtlTalk generalizes from the synthesized tasks to other, yet unseen, tasks.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - OOPSLA
PB  - 
DO  - 10.1145/3428269
ER  - 

TY  - NA
AU  - Zhang, Tianyi; Lowmanstone, London; Wang, Xinyu; Glassman, Elena L.
TI  - UIST - Interactive Program Synthesis by Augmented Examples
PY  - 2020
AB  - Programming-by-example (PBE) has become an increasingly popular component in software development tools, human-robot interaction, and end-user programming. A long-standing challenge in PBE is the inherent ambiguity in user-provided examples. This paper presents an interaction model to disambiguate user intent and reduce the cognitive load of understanding and validating synthesized programs. Our model provides two types of augmentations to user-given examples: 1) semantic augmentation where a user can specify how different aspects of an example should be treated by a synthesizer via light-weight annotations, and 2) data augmentation where the synthesizer generates additional examples to help the user understand and validate synthesized programs. We implement and demonstrate this interaction model in the domain of regular expressions, which is a popular mechanism for text processing and data wrangling and is often considered hard to master even for experienced programmers. A within-subjects user study with twelve participants shows that, compared with only inspecting and annotating synthesized programs, interacting with augmented examples significantly increases the success rate of finishing a programming task with less time and increases users? confidence of synthesized programs.
SP  - 627
EP  - 648
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415900
ER  - 

TY  - NA
AU  - Shriver, David; Elbaum, Sebastian; Stolee, Kathryn T.
TI  - ICSE-NIER - At the end of synthesis: narrowing program candidates
PY  - 2017
AB  - Program synthesis is succeeding in supporting the generation of programs within increasingly complex domains. The use of weaker specifications, such as those consisting of input/output examples or test cases, has helped to fuel the success of program synthesis by lowering adoption barriers. Yet, employing weaker specifications has the side effect of generating a potentially large number of candidate programs. This was not a problem for simpler and smaller program domains, but it is becoming evident that differentiating among many synthesized programs is a challenge that needs addressing. We sketch an approach to mitigate this challenge, requiring less effort from the user while automatically identifying inputs that can differentiate clusters of synthesized programs. The approach has the potential to more cost-effectively narrow the space of candidate programs in a range of synthesis applications.
SP  - 19
EP  - 22
JF  - 2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse-nier.2017.7
ER  - 

TY  - NA
AU  - Tang, Ding; Huang, Dingbang
TI  - Analysis on developmental trend of microfluidic chip technology and the integration mode with the wearable devices
PY  - 2016
AB  - This paper conducts research on the developmental trend of microfluidic chip technology and the integration mode with the wearable devices. Miniaturization and integration is the important direction of the development of biochemical analysis, and microfluidic chip is one of the frontier and the technology, which is based on micro-electromechanical processing, structure of microchannel network characteristics, the goal is to will be involved in the biochemical analysis in areas such as sampling, pretreatment, separation, mixing, reaction, detection, and some or all of the operating unit integrated in a chip the size of a few square centimeters. To make the application scenarios of the microfluidic chip technology broader, we combine the review of the wearable devices to propose the potential integration of the two techniques. The smart hard ware and wearable device are the two major orientations of the intelligent world. In the further research, we will experimentally analyze the mode.
SP  - 1
EP  - 5
JF  - 2016 International Conference on Inventive Computation Technologies (ICICT)
VL  - 2016
IS  - NA
PB  - 
DO  - 10.1109/inventive.2016.7824845
ER  - 

TY  - NA
AU  - Knibbe, Jarrod; Freire, Rachel; Koelle, Marion; Strohmeier, Paul
TI  - TEI - Skill-Sleeves: Designing Electrode Garments for Wearability
PY  - 2021
AB  - Many existing explorations of wearables for HCI consider functionality first and wearability second. Typically, as the technologies, designs, and experiential understandings develop, attention can shift towards questions of deployment and wearability. To support this shift of focus we present a case study of the iterative design of electrode sleeves. We consider the design motivations and background that led to the existing, prototype EMS sleeves, and the resultant challenges around their wearability. Through our own design research practice, we seek to reveal design criteria towards the wearability of such a sleeve, and provide designs that optimise for those criteria. We contribute (1) new electrode sleeve designs, which begin to make it practicable to take EMS beyond the lab, (2) new fabrication processes that support rapid production and personalisation, and (3) reflections on criteria for wearability across new eTextile garments.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440652
ER  - 

TY  - JOUR
AU  - de Guzman, Jaybie A.; Thilakarathna, Kanchana; Seneviratne, Aruna
TI  - Security and Privacy Approaches in Mixed Reality: A Literature Survey
PY  - 2019
AB  - Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, there is only barely a few working on the privacy and security implications of this technology. This survey paper aims to put in to light these risks, and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality (AR), virtual reality (VR), and human-computer interaction (HCI) as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things (IoT). We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed.
SP  - 110
EP  - 37
JF  - ACM Computing Surveys
VL  - 52
IS  - 6
PB  - 
DO  - 10.1145/3359626
ER  - 

TY  - NA
AU  - Koelle, Marion; Heuten, Wilko; Boll, Susanne
TI  - MobileHCI - Are you hiding it?: usage habits of lifelogging camera wearers
PY  - 2017
AB  - Though still a rare sight, body-worn lifelogging cameras such as Mofily's YoCam or the Narrative Clip have become increasingly popular amongst tech-savvy audiences. In this paper, we investigate whether users of those devices prefer to wear them openly or in a concealed, less obtrusive manner. We discuss the camouflage of lifelogging cameras based on results from an online study (N=117), including the why (not) and how as well as qualitative insights on how social contexts influence usage habits. The results of our study provide empirical evidence that deliberate concealment can be perceived unethical, and that moderate noticeability is favoured. We furthermore found contrary effects of lifelogging cameras in interpersonal relationships, including self-censorship by the user, avoidance behaviour by her/his peers and conversation starting character of the device itself. We conclude by highlighting design challenges concerning ubiquitous, body-worn cameras.
SP  - 80
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3122123
ER  - 

TY  - JOUR
AU  - PolozovOleksandr, ; GulwaniSumit, 
TI  - FlashMeta: a framework for inductive program synthesis
PY  - 2015
AB  - Inductive synthesis, or programming-by-examples (PBE) is gaining prominence with disruptive applications for automating repetitive tasks in end-user programming. However, designing, developing, and...
SP  - 107
EP  - 126
JF  - ACM SIGPLAN Notices
VL  - 50
IS  - 10
PB  - 
DO  - 10.1145/2858965.2814310
ER  - 

TY  - NA
AU  - Rong, Xin; Yan, Shiyan; Oney, Stephen; Dontcheva, Mira; Adar, Eytan
TI  - UIST - CodeMend: Assisting Interactive Programming with Bimodal Embedding
PY  - 2016
AB  - Software APIs often contain too many methods and parameters for developers to memorize or navigate effectively. Instead, developers resort to finding answers through online search engines and systems such as Stack Overflow. However, the process of finding and integrating a working solution is often very time-consuming. Though code search engines have increased in quality, there remain significant language- and workflow-gaps in meeting end-user needs. Novice and intermediate programmers often lack the language to query, and the expertise in transferring found code to their task. To address this problem, we present CodeMend, a system to support finding and integration of code. CodeMend leverages a neural embedding model to jointly model natural language and code as mined from large Web and code datasets. We also demonstrate a novel, mixed-initiative, interface to support query and integration steps. Through CodeMend, end-users describe their goal in natural language. The system makes salient the relevant API functions, the lines in the end-user's program that should be changed, as well as proposing the actual change. We demonstrate the utility and accuracy of CodeMend through lab and simulation studies.
SP  - 247
EP  - 258
JF  - Proceedings of the 29th Annual Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2984511.2984544
ER  - 

TY  - NA
AU  - Hempel, Brian; Lubin, Justin; Chugh, Ravi
TI  - UIST - Sketch-n-Sketch: Output-Directed Programming for SVG
PY  - 2019
AB  - For creative tasks, programmers face a choice: Use a GUI and sacrifice flexibility, or write code and sacrifice ergonomics? To obtain both flexibility and ease of use, a number of systems have explored a workflow that we call output-directed programming. In this paradigm, direct manipulation of the program's graphical output corresponds to writing code in a general-purpose programming language, and edits not possible with the mouse can still be enacted through ordinary text edits to the program. Such capabilities provide hope for integrating graphical user interfaces into what are currently text-centric programming environments. To further advance this vision, we present a variety of new output-directed techniques that extend the expressive power of Sketch-n-Sketch, an output-directed programming system for creating programs that generate vector graphics. To enable output-directed interaction at more stages of program construction, we expose intermediate execution products for manipulation and we present a mechanism for contextual drawing. Looking forward to output-directed programming beyond vector graphics, we also offer generic refactorings through the GUI, and our techniques employ a domain-agnostic provenance tracing scheme. To demonstrate the improved expressiveness, we implement a dozen new parametric designs in Sketch-n-Sketch without text-based edits. Among these is the first demonstration of building a recursive function in an output-directed programming setting.
SP  - 281
EP  - 292
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347925
ER  - 

TY  - JOUR
AU  - Villarreal-Narvaez, Santiago; Sluÿters, Arthur; Vanderdonckt, Jean; Mbaki Luzayisu, Efrem
TI  - Theoretically-Defined vs. User-Defined Squeeze Gestures
PY  - 2022
AB  - <jats:p>This paper presents theoretical and empirical results about user-defined gesture preferences for squeezable objects by focusing on a particular object: a deformable cushion. We start with a theoretical analysis of potential gestures for this squeezable object by defining a multi-dimension taxonomy of squeeze gestures composed of 82 gesture classes. We then empirically analyze the results of a gesture elicitation study resulting in a set of N=32 participants X 21 referents = 672 elicited gestures, further classified into 26 gesture classes. We also contribute to the practice of gesture elicitation studies by explaining why we started from a theoretical analysis (by systematically exploring a design space of potential squeeze gestures) to end up with an empirical analysis (by conducting a gesture elicitation study afterward): the intersection of the results from these sources confirm or disconfirm consensus gestures. Based on these findings, we extract from the taxonomy a subset of recommended gestures that give rise to design implications for gesture interaction with squeezable objects.</jats:p>
SP  - 73
EP  - 102
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567805
ER  - 

TY  - NA
AU  - Wang, Jingbo; Sung, Chungha; Raghothaman, Mukund; Wang, Chao
TI  - ICSE - Data-Driven Synthesis of Provably Sound Side Channel Analyses
PY  - 2021
AB  - We propose a data-driven method for synthesizing static analyses to detect side-channel information leaks in cryptographic software. Compared to the conventional way of manually crafting such static analyzers, which can be tedious, error prone and suboptimal, our learning-based technique is not only automated but also provably sound. Our analyzer consists of a set of type-inference rules learned from the training data, i.e., example code snippets annotated with the ground truth. Internally, we use syntax-guided synthesis (SyGuS) to generate new recursive features and decision tree learning (DTL) to generate analysis rules based on these features. We guarantee soundness by proving each learned analysis rule via a technique called query containment checking. We have implemented our technique in the LLVM compiler and used it to detect power side channels in C programs that implement cryptographic protocols. Our results show that, in addition to being automated and provably sound during synthesis, our analyzer can achieve the same empirical accuracy as two state-of-the-art, manually-crafted analyzers while being 300X and 900X faster, respectively.
SP  - 810
EP  - 822
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse43902.2021.00079
ER  - 

TY  - NA
AU  - Koelle, Marion; Wallbaum, Torben; Heuten, Wilko; Boll, Susanne
TI  - CHI Extended Abstracts - Evaluating a Wearable Camera's Social Acceptability In-the-Wild
PY  - 2019
AB  - With increasing ubiquity, wearable technologies are becoming part of everyday life where they may cause controversy, discomfort and social tension. Particularly, body-worn "always-on" cameras raise social acceptability concerns as their form factors hinder bystanders to infer whether they are "in the frame". Screen-based status indicators have been suggested as remedy, but not evaluated in-the-wild. Simultaneously, best practices for evaluating social acceptability in field studies are rare. This work contributes to closing both gaps. First, we contribute results of an in-the-wild evaluation of a screen-based status indicator testing the suitability of the "displayed camera image" design strategy. Second, we discuss methodical implications for evaluating social acceptability in the field, and cover lessons learned from collecting hypersubjective self-reports. We provide a self-critical, in-depth discussion of our field experiment, including study-related behavior patterns, and prototype fidelity. Our work may serve as a reference for field studies evaluating social acceptability.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312837
ER  - 

TY  - JOUR
AU  - Álvarez-Ayllón, Alejandro; Palomo-Duarte, Manuel; Dodero, Juan Manuel
TI  - Interactive Data Exploration of Distributed Raw Files: A Systematic Mapping Study
PY  - 2019
AB  - When exploring big amounts of data without a clear target, providing an interactive experience becomes really difficult, since this tentative inspection usually defeats any early decision on data structures or indexing strategies. This is also true in the physics domain, specifically in high-energy physics, where the huge volume of data generated by the detectors are normally explored via C++ code using batch processing, which introduces a considerable latency. An interactive tool, when integrated into the existing data management systems, can add a great value to the usability of these platforms. Here, we intend to review the current state-of-the-art of interactive data exploration, aiming at satisfying three requirements: access to raw data files, stored in a distributed environment, and with a reasonably low latency. This paper follows the guidelines for systematic mapping studies, which is well suited for gathering and classifying available studies. We summarize the results after classifying the 242 papers that passed our inclusion criteria. While there are many proposed solutions that tackle the problem in different manners, there is little evidence available about their implementation in practice. Almost all of the solutions found by this paper cover a subset of our requirements, with only one partially satisfying the three. The solutions for data exploration abound. It is an active research area and, considering the continuous growth of data volume and variety, is only to become harder. There is a niche for research on a solution that covers our requirements, and the required building blocks are there.
SP  - 10691
EP  - 10717
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2882244
ER  - 

TY  - CHAP
AU  - Gulwani, Sumit
TI  - IJCAR - Programming by Examples: Applications, Algorithms, and Ambiguity Resolution
PY  - 2016
AB  - 99i¾?% of computer end users do not know programming, and struggle with repetitive tasks. Programming by Examples PBE can revolutionize this landscape by enabling users to synthesize intended programs from example based specifications. A key technical challenge in PBE is to search for programs that are consistent with the examples provided by the user. Our efficient search methodology is based on two key ideas: i Restriction of the search space to an appropriate domain-specific language that offers balanced expressivity and readability ii A divide-and-conquer based deductive search paradigm that inductively reduces the problem of synthesizing a program of a certain kind that satisfies a given specification into sub-problems that refer to sub-programs or sub-specifications. Another challenge in PBE is to resolve the ambiguity in the example based specification. We will discuss two complementary approaches: a machine learning based ranking techniques that can pick an intended program from among those that satisfy the specification, and b active-learning based user interaction models. The above concepts will be illustrated using FlashFill, FlashExtract, and FlashRelate--PBE technologies for data manipulation domains. These technologies, which have been released inside various Microsoft products, are useful for data scientists who spend 80i¾?% of their time wrangling with data. The Microsoft PROSE SDK allows easy construction of such technologies.
SP  - 9
EP  - 14
JF  - Automated Reasoning
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-40229-1_2
ER  - 

TY  - JOUR
AU  - Gao, Xiang; Barke, Shraddha; Radhakrishna, Arjun; Soares, Gustavo; Gulwani, Sumit; Leung, Alan; Nagappan, Nachiappan; Tiwari, Ashish
TI  - Feedback-driven semi-supervised synthesis of program transformations
PY  - 2020
AB  - While editing code, it is common for developers to make multiple related repeated edits that are all instances of a more general program transformation. Since this process can be tedious and error-prone, we study the problem of automatically learning program transformations from past edits, which can then be used to predict future edits. We take a novel view of the problem as a semi-supervised learning problem: apart from the concrete edits that are instances of the general transformation, the learning procedure also exploits access to additional inputs (program subtrees) that are marked as positive or negative depending on whether the transformation applies on those inputs. We present a procedure to solve the semi-supervised transformation learning problem using anti-unification and programming-by-example synthesis technology. To eliminate reliance on access to marked additional inputs, we generalize the semi-supervised learning procedure to a feedback-driven procedure that also generates the marked additional inputs in an iterative loop. We apply these ideas to build and evaluate three applications that use different mechanisms for generating feedback. Compared to existing tools that learn program transformations from edits, our feedback-driven semi-supervised approach is vastly more effective in successfully predicting edits with significantly lesser amounts of past edit data.
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - OOPSLA
PB  - 
DO  - 10.1145/3428287
ER  - 

TY  - NA
AU  - Sivaraman, Aishwarya; Zhang, Tianyi; Van den Broeck, Guy; Kim, Miryung
TI  - ICSE - Active inductive logic programming for code search
PY  - 2019
AB  - Modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code. The key insight of our interactive code search technique Alice is that user feedback can be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, Alice automatically extracts a logic query from code features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). Alice then infers a new logic query that separates positive examples from negative examples via active inductive logic programming. Our comprehensive simulation experiment shows that Alice removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to user labeling mistakes. Our choice of leveraging both positive and negative examples and using nested program structure as an inductive bias is effective in refining search queries. Compared with an existing interactive code search technique, Alice does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with much fewer search iterations. A case study with real developers shows that Alice is easy to use and helps express complex code patterns.
SP  - 292
EP  - 303
JF  - 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse.2019.00044
ER  - 

TY  - JOUR
AU  - Padhi, Saswat; Jain, Prateek; Perelman, Daniel; Polozov, Oleksandr; Gulwani, Sumit; Millstein, Todd
TI  - FlashProfile: A Framework for Synthesizing Data Profiles
PY  - 2018
AB  - We address the problem of learning a syntactic profile for a collection of strings, i.e. a set of regex-like patterns that succinctly describe the syntactic variations in the strings. Real-world datasets, typically curated from multiple sources, often contain data in various syntactic formats. Thus, any data processing task is preceded by the critical step of data format identification. However, manual inspection of data to identify the different formats is infeasible in standard big-data scenarios. Prior techniques are restricted to a small set of pre-defined patterns (e.g. digits, letters, words, etc.), and provide no control over granularity of profiles. We define syntactic profiling as a problem of clustering strings based on syntactic similarity, followed by identifying patterns that succinctly describe each cluster. We present a technique for synthesizing such profiles over a given language of patterns, that also allows for interactive refinement by requesting a desired number of clusters. Using a state-of-the-art inductive synthesis framework, PROSE, we have implemented our technique as FlashProfile. Across $153$ tasks over $75$ large real datasets, we observe a median profiling time of only $\sim\,0.7\,$s. Furthermore, we show that access to syntactic profiles may allow for more accurate synthesis of programs, i.e. using fewer examples, in programming-by-example (PBE) workflows such as FlashFill.
SP  - 150
EP  - 28
JF  - Proceedings of the ACM on Programming Languages
VL  - 2
IS  - OOPSLA
PB  - 
DO  - 10.1145/3276520
ER  - 

TY  - NA
AU  - Wang, Chenglong; Cheung, Alvin; Bodik, Rastislav
TI  - PLDI - Synthesizing highly expressive SQL queries from input-output examples
PY  - 2017
AB  - SQL is the de facto language for manipulating relational data. Though powerful, many users find it difficult to write SQL queries due to highly expressive constructs. While using the programming-by-example paradigm to help users write SQL queries is an attractive proposition, as evidenced by online help forums such as Stack Overflow, developing techniques for synthesizing SQL queries from given input-output (I/O) examples has been difficult, due to the large space of SQL queries as a result of its rich set of operators. In this paper, we present a new scalable and efficient algorithm for synthesizing SQL queries based on I/O examples. The key innovation of our algorithm is development of a language for abstract queries, i.e., queries with uninstantiated operators, that can be used to express a large space of SQL queries efficiently. Using abstract queries to represent the search space nicely decomposes the synthesis problem into two tasks: 1) searching for abstract queries that can potentially satisfy the given I/O examples, and 2) instantiating the found abstract queries and ranking the results. We have implemented this algorithm in a new tool called Scythe and evaluated it using 193 benchmarks collected from Stack Overflow. Our evaluation shows that Scythe can efficiently solve 74% of the benchmarks, most in just a few seconds, and the queries range from simple ones involving a single selection to complex queries with 6 nested subqueires.
SP  - 452
EP  - 466
JF  - Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation
VL  - 52
IS  - 6
PB  - 
DO  - 10.1145/3062341.3062365
ER  - 

TY  - JOUR
AU  - Huang, Zheng; Li, Yi-Na; Kong, Jun
TI  - Investigating the multimedia pointing techniques in the tabletop-centric cross-device interaction
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Group collaboration needs the support of a digital interaction system composed of a tabletop-centric shared display and individual operational physical cursors and mobile devices. We examine the usability of a fundamental function of the system, i.e., the pointing techniques. Direct pointing refers to the selection with a cursor placed on the target object, and proximity pointing requires the cursor nearby the target object. The empirical evidence from sixty-two participants shows that the direct pointing technique, although proved as the best choice in single device interaction, is inferior to proximity pointing in cross-device interaction in completion time and operation correctness. We also examine how the display for confirmation on mobile devices fits for pointing technique, discuss how users habituated to single device interaction adapt to cross-device interaction, and suggest guidelines for the design practice.</jats:p>
SP  - NA
EP  - NA
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11042-022-12975-0
ER  - 

TY  - NA
AU  - Rolim, Reudismam; Soares, Gustavo; D'Antoni, Loris; Polozov, Oleksandr; Gulwani, Sumit; Gheyi, Rohit; Suzuki, Ryo; Hartmann, Björn
TI  - ICSE - Learning syntactic program transformations from examples
PY  - 2017
AB  - Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present Refazer, a technique for automatically learning program transformations. Refazer builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, Refazer leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate Refazer in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, Refazer learns the intended program transformation in 84% of the cases using only 2.9 examples on average.
SP  - 404
EP  - 415
JF  - 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse.2017.44
ER  - 

TY  - NA
AU  - Rolim, Reudismam
TI  - SIGSOFT FSE - Automating repetitive code changes using examples
PY  - 2016
AB  - While adding features, fixing bugs, or refactoring the code, developers may perform repetitive code edits. Although Integrated Development Environments (IDEs) automate some transformations such as renaming, many repetitive edits are performed manually, which is error-prone and time-consuming. To help developers to apply these edits, we propose a technique to perform repetitive edits using examples. The technique receives as input the source code before and after the developer edits some target locations of the change and produces as output the top-ranked program transformation that can be applied to edit the remaining target locations in the codebase. The technique uses a state-of-the-art program synthesis methodology and has three main components: a) a DSL for describing program transformations; b) synthesis algorithms to learn program transformations in this DSL; c) ranking algorithms to select the program transformation with the higher probability of performing the desired repetitive edit. In our preliminary evaluation, in a dataset of 59 repetitive edit cases taken from real C# source code repositories, the technique performed, in 83% of the cases, the intended transformation using only 2.8 examples.
SP  - 1063
EP  - 1065
JF  - Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2950290.2983944
ER  - 

TY  - NA
AU  - Koelle, Marion; Ananthanarayan, Swamy; Boll, Susanne
TI  - CHI - Social Acceptability in HCI: A Survey of Methods, Measures, and Design Strategies
PY  - 2020
AB  - With the increasing ubiquity of personal devices, social acceptability of human-machine interactions has gained relevance and growing interest from the HCI community. Yet, there are no best practices or established methods for evaluating social acceptability. Design strategies for increasing social acceptability have been described and employed, but so far not been holistically appraised and evaluated. We offer a systematic literature analysis (N=69) of social acceptability in HCI and contribute a better understanding of current research practices, namely, methods employed, measures and design strategies. Our review identified an unbalanced distribution of study approaches, shortcomings in employed measures, and a lack of interweaving between empirical and artifact-creating approaches. The latter causes a discrepancy between design recommendations based on user research, and design strategies employed in artifact creation. Our survey lays the groundwork for a more nuanced evaluation of social acceptability, the development of best practices, and a future research agenda.
SP  - 3376162
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376162
ER  - 

TY  - JOUR
AU  - Ens, Barrett; Irani, Pourang
TI  - Spatial Analytic Interfaces: Spatial User Interfaces for In Situ Visual Analytics
PY  - 2016
AB  - As wearable devices gain acceptance, we need to ask, What will user interfaces look like in a post-smartphone world? Will these future interfaces support sophisticated interactions in a mobile context? The authors draw from visual analytics concepts to address the growing need for individuals to manage information on personal devices. Spatial analytic interfaces (SAIs) can leverage the benefits of spatial interaction to enable everyday visual analytics tasks to be performed in-situ, at the most beneficial place and time. They explore the possibilities for such interfaces using head-worn display technology and discuss current developments and future research goals for the successful development of SAIs.
SP  - 66
EP  - 79
JF  - IEEE computer graphics and applications
VL  - 37
IS  - 2
PB  - 
DO  - 10.1109/mcg.2016.38
ER  - 

TY  - NA
AU  - Polozov, Oleksandr; Gulwani, Sumit
TI  - OOPSLA - FlashMeta: a framework for inductive program synthesis
PY  - 2015
AB  - Inductive synthesis, or programming-by-examples (PBE) is gaining prominence with disruptive applications for automating repetitive tasks in end-user programming. However, designing, developing, and maintaining an effective industrial-quality inductive synthesizer is an intellectual and engineering challenge, requiring 1-2 man-years of effort. Our novel observation is that many PBE algorithms are a natural fall-out of one generic meta-algorithm and the domain-specific properties of the operators in the underlying domain-specific language (DSL). The meta-algorithm propagates example-based constraints on an expression to its subexpressions by leveraging associated witness functions, which essentially capture the inverse semantics of the underlying operator. This observation enables a novel program synthesis methodology called data-driven domain-specific deduction (D4), where domain-specific insight, provided by the DSL designer, is separated from the synthesis algorithm. Our FlashMeta framework implements this methodology, allowing synthesizer developers to generate an efficient synthesizer from the mere DSL definition (if properties of the DSL operators have been modeled). In our case studies, we found that 10+ existing industrial-quality mass-market applications based on PBE can be cast as instances of D4. Our evaluation includes reimplementation of some prior works, which in FlashMeta become more efficient, maintainable, and extensible. As a result, FlashMeta-based PBE tools are deployed in several industrial products, including Microsoft PowerShell 3.0 for Windows 10, Azure Operational Management Suite, and Microsoft Cortana digital assistant.
SP  - 107
EP  - 126
JF  - Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications
VL  - 50
IS  - 10
PB  - 
DO  - 10.1145/2814270.2814310
ER  - 

TY  - JOUR
AU  - Wang, Yanjun; Li, Zixuan; Jiang, Chuan; Qiu, Xiaokang; Rao, Sanjay
TI  - Comparative Synthesis: Learning Near-Optimal Network Designs by Query
PY  - 2023
AB  - <jats:p>When managing wide-area networks, network architects must decide how to balance multiple conflicting metrics, and ensure fair allocations to competing traffic while prioritizing critical traffic. The state of practice poses challenges since architects must precisely encode their intent into formal optimization models using abstract notions such as utility functions, and ad-hoc manually tuned knobs. In this paper, we present the first effort to synthesize optimal network designs with indeterminate objectives using an interactive program-synthesis-based approach. We make three contributions. First, we present comparative synthesis, an interactive synthesis framework which produces near-optimal programs (network designs) through two kinds of queries (Validate and Compare), without an objective explicitly given. Second, we develop the first learning algorithm for comparative synthesis in which a voting-guided learner picks the most informative query in each iteration. We present theoretical analysis of the convergence rate of the algorithm. Third, we implemented Net10Q, a system based on our approach, and demonstrate its effectiveness on four real-world network case studies using black-box oracles and simulation experiments, as well as a pilot user study comprising network researchers and practitioners. Both theoretical and experimental results show the promise of our approach.</jats:p>
SP  - 91
EP  - 120
JF  - Proceedings of the ACM on Programming Languages
VL  - 7
IS  - POPL
PB  - 
DO  - 10.1145/3571197
ER  - 

TY  - NA
AU  - He, Liang; Laput, Gierad; Brockmeyer, Eric; Froehlich, Jon E.
TI  - Tangible and Embedded Interaction - SqueezaPulse: Adding Interactive Input to Fabricated Objects Using Corrugated Tubes and Air Pulses
PY  - 2017
AB  - We present SqueezaPulse, a technique for embedding interactivity into fabricated objects using soft, passive, low-cost bellow-like structures. When a soft cavity is squeezed, air pulses travel along a flexible pipe and into a uniquely designed corrugated tube that shapes the airflow into predictable sound signatures. A microphone captures and identifies these air pulses enabling interactivity. We describe the underlying acoustic theory used to inform our design, an informal examination of the effect of different 3D-printed corrugations on air signatures, and our resulting SqueezaPulse implementation. To demonstrate and evaluate the potential of SqueezaPulse, we present four prototype applications and a small, lab-based user study (N=9). Our evaluations show that our approach is accurate across users and robust to external noise. We conclude with a discussion of limitations and future work.
SP  - 341
EP  - 350
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3024976
ER  - 

TY  - NA
AU  - Chugh, Ravi; Hempel, Brian; Spradlin, Mitchell; Albers, Jacob
TI  - Programmatic and Direct Manipulation, Together at Last
PY  - 2016
AB  - Direct manipulation interfaces and programmatic systems have distinct and complementary strengths. The former provide intuitive, immediate visual feedback and enable rapid prototyping, whereas the latter enable complex, reusable abstractions. Unfortunately, existing systems typically force users into just one of these two interaction modes. We present a system called Sketch-n-Sketch that integrates programmatic and direct manipulation for the particular domain of Scalable Vector Graphics (SVG). In Sketch-n-Sketch, the user writes a program to generate an output SVG canvas. Then the user may directly manipulate the canvas while the system immediately infers a program update in order to match the changes to the output, a workflow we call live synchronization. To achieve this, we propose (i) a technique called trace-based program synthesis that takes program execution history into account in order to constrain the search space and (ii) heuristics for dealing with ambiguities. Based on our experience with examples spanning 2,000 lines of code and from the results of a preliminary user study, we believe that Sketch-n-Sketch provides a novel workflow that can augment traditional programming systems. Our approach may serve as the basis for live synchronization in other application domains, as well as a starting point for yet more ambitious ways of combining programmatic and direct manipulation.
SP  - 341
EP  - 354
JF  - Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation
VL  - 51
IS  - 6
PB  - 
DO  - 10.1145/2908080.2908103
ER  - 

TY  - NA
AU  - Lee, Juyoung; Yeo, Hui-Shyong; Dhuliawala, Murtaza; Akano, Jedidiah; Shimizu, Junichi; Starner, Thad; Quigley, Aaron; Woo, Woontack; Kunze, Kai
TI  - ISWC - Itchy nose: discreet gesture interaction using EOG sensors in smart eyewear
PY  - 2017
AB  - We propose a sensing technique for detecting finger movements on the nose, using EOG sensors embedded in the frame of a pair of eyeglasses. Eyeglasses wearers can use their fingers to exert different types of movement on the nose, such as flicking, pushing or rubbing. These subtle gestures can be used to control a wearable computer without calling attention to the user in public. We present two user studies where we test recognition accuracy for these movements.
SP  - 94
EP  - 97
JF  - Proceedings of the 2017 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3123021.3123060
ER  - 

TY  - JOUR
AU  - Yuan, Yifei; Lin, Dong; Anil, Siri; Verma, Harsh; Chelluri, Anirudh; Alur, Rajeev; Loo, Boon Thau
TI  - NetEgg: A Scenario-Based Programming Toolkit for SDN Policies
PY  - 2018
AB  - Recent emergence of software-defined networks offers an opportunity to design domain-specific programming abstractions aimed at network operators. In this paper, we propose scenario-based programming, a framework that allows network operators to program network policies by describing example behaviors in representative scenarios. Given these scenarios, our synthesis algorithm automatically infers the controller state that needs to be maintained along with the rules to process network events and update state. We have developed the NetEgg scenario-based programming tool, which can execute the generated policy implementation on top of a centralized controller, but also automatically infers flow-table rules that can be pushed to switches to improve throughput. We evaluate the performance of NetEgg based on the computational requirements of our synthesis algorithm as well as the overhead introduced by the generated policy implementation, and we study the usability of NetEgg based on a user study. Our results show that our synthesis algorithm can generate policy implementations in less than a second for all policies we studied, and the automatically generated policy implementations have performance comparable to their hand-crafted implementations. Our user study shows that the proposed scenario-based programming approach can reduce the programming time by 50% and the error rate by 32% compared with an alternative programming approach.
SP  - 2104
EP  - 2117
JF  - IEEE/ACM Transactions on Networking
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tnet.2018.2861919
ER  - 

TY  - JOUR
AU  - Yoon, Sang Ho; Paredes, Luis; Huo, Ke; Ramani, Karthik
TI  - MultiSoft: Soft Sensor Enabling Real-Time Multimodal Sensing with Contact Localization and Deformation Classification
PY  - 2018
AB  - We introduce MultiSoft, a multilayer soft sensor capable of sensing real-time contact localization, classification of deformation types, and estimation of deformation magnitudes. We propose a multimodal sensing pipeline that carries out both inverse problem solving and machine learning tasks. Specifically, we employ an electrical impedance tomography (EIT) for contact localization and a support vector machine (SVM) for classifying deformations and regressing their magnitudes. We propose a deformation-aware system which enables maintaining a persistent single-point contact localization throughout the deformation. By updating a time-varying distribution of conductivity change caused by deformations, a single-point contact localization can be maintained and restored to support interaction using both contact localization and deformations.We devise a multilayer structure to fabricate a highly stretchable and flexible soft sensor with a short sensor settlement after excitations. Through a series of experiments and evaluations, we validate both raw sensor and multimodal sensing performance with the proposed method. We further demonstrate applicability and feasibility of MultiSoft with example applications.
SP  - 145
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 3
PB  - 
DO  - 10.1145/3264955
ER  - 

TY  - NA
AU  - Park, Chunjong; Lim, Junsung; Kim, Juho; Lee, Sung-Ju; Lee, Dongman
TI  - CSCW - Don't Bother Me. I'm Socializing!: A Breakpoint-Based Smartphone Notification System
PY  - 2017
AB  - Smartphone notifications provide application-specific information in real-time, but could distract users from in-person social interactions when delivered at inopportune moments. We explore breakpoint-based notification management, in which the smartphone defers notifications until an opportune moment. With a video survey where participants selected appropriate moments for notifications from a video-recorded social interaction, we identify four breakpoint types: long silence, a user leaving the table, others using smartphones, and a user left alone. We introduce a Social Context-Aware smartphone Notification system, SCAN, that uses build-in sensors to detect social context and identifies breakpoints to defer smartphone notifications until a breakpoint. We conducted a controlled study with ten friend groups who had SCAN installed on their smartphones while dining at a restaurant. Results show that SCAN accurately detects breakpoints (precision=92.0%, recall=82.5%), and reduces notification interruptions by 54.1%. Most participants reported that SCAN helped them to focus better on in-person social interaction and found selected breakpoints appropriate.
SP  - 541
EP  - 554
JF  - Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2998181.2998189
ER  - 

TY  - NA
AU  - Mirzamohammadi, Saeed; Chen, Justin A.; Sani, Ardalan Amiri; Mehrotra, Sharad; Tsudik, Gene
TI  - SenSys - Ditio: Trustworthy Auditing of Sensor Activities in Mobile & IoT Devices
PY  - 2017
AB  - Mobile and Internet-of-Things (IoT) devices, such as smartphones, tablets, wearables, smart home assistants (e.g., Google Home and Amazon Echo), and wall-mounted cameras, come equipped with various sensors, notably camera and microphone. These sensors can capture extremely sensitive and private information. There are several important scenarios where, for privacy reasons, a user might require assurance about the use (or non-use) of these sensors. For example, the owner of a home assistant might require assurance that the microphone on the device is not used during a given time of the day. Similarly, during a confidential meeting, the host needs assurance that attendees do not record any audio or video. Currently, there are no means to attain such assurance in modern mobile and IoT devices. To this end, this paper presents Ditio, a system approach for auditing sensor activities. Ditio records sensor activity logs that can be later inspected by an auditor and checked for compliance with a given policy. It is based on a hybrid security monitor architecture that leverages both ARM's virtualization hardware and TrustZone. Ditio includes an authentication protocol for establishing a logging session with a trusted server and a formally verified companion tool for log analysis. Ditio prototypes on ARM Juno development board and Nexus 5 smartphone show that it introduces negligible performance overhead for both the camera and microphone. However, it incurs up to 17% additional power consumption under heavy use for the Nexus 5 camera.
SP  - 28
EP  - NA
JF  - Proceedings of the 15th ACM Conference on Embedded Network Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131672.3131688
ER  - 

TY  - JOUR
AU  - Hirskyj-Douglas, Ilyena; Kytö, Mikko; McGookin, David
TI  - Head-mounted Displays, Smartphones, or Smartwatches? -- Augmenting Conversations with Digital Representation of Self
PY  - 2019
AB  - Technologies that augment face-to-face interactions with a digital sense of self have been used to support conversations. That work has employed one homogenous technology, either 'off-the-shelf' or with a bespoke prototype, across all participants. Beyond speculative instances, it is unclear what technology individuals themselves would choose, if any, to augment their social interactions; what influence it may exert; or how use of heterogeneous devices may affect the value of this augmentation. This is important, as the devices that we use directly affect our behaviour, influencing affordances and how we engage in social interactions. Through a study of 28 participants, we compared head-mounted display, smartphones, and smartwatches to support digital augmentation of self during face-to-face interactions within a group. We identified a preference among participants for head-mounted displays to support privacy, while smartwatches and smartphones better supported conversational events (such as grounding and repair), along with group use through screen-sharing. Accordingly, we present software and hardware design recommendations and user interface guidelines for integrating a digital form of self into face-to-face conversations.
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359281
ER  - 

TY  - NA
AU  - Mori, Hideki; Takahashi, Yoshihisa; Shiono, Koichi; Kaneko, Hirofumi; Matsugami, Hiroya; Nishidate, Masaomi
TI  - Self-Shape-Sensing Device with Flexible Mechanical Axes for Deformable Input Interface
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2021 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3476122.3484839
ER  - 

TY  - NA
AU  - Zhou, Zhanhui; Tang, Man To; Pan, Qiping; Tan, Shangyin; Wang, Xinyu; Zhang, Tianyi
TI  - INTENT: Interactive Tensor Transformation Synthesis
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545653
ER  - 

TY  - NA
AU  - Koelle, Marion; Ali, Abdallah El; Cobus, Vanessa; Heuten, Wilko; Boll, Susanne
TI  - CHI - All about Acceptability?: Identifying Factors for the Adoption of Data Glasses
PY  - 2017
AB  - Innovations often trigger objections before becoming widely accepted. This paper assesses whether a familiarisation over time can be expected for data glasses, too. While user attitudes towards those devices have been reported to be prevalently negative [14], it is still unclear, to what extent this initial, negative user attitude might impede adoption. However, indepth understanding is crucial for reducing barriers early in order to gain access to potential benefits from the technology. With this paper we contribute to a better understanding of factors affecting data glasses adoption, as well as current trends and opinions. Our multiple-year case study (N=118) shows, against expectations, no significant change towards a more positive attitude between 2014 and 2016. We complement these findings with an expert survey (N=51) investigating prognoses, challenges and discussing the relevance of social acceptability. We elicit and contrast a controversial spectrum of expert opinions, and assess whether initial objections can be overwritten. Our analysis shows that while social acceptability is considered relevant for the time being, utility and usability are more valued for long-term adoption.
SP  - 295
EP  - 300
JF  - Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3025453.3025749
ER  - 

TY  - NA
AU  - Kytö, Mikko; Hirskyj-Douglas, Ilyena; McGookin, David
TI  - AHs - From Strangers to Friends: Augmenting Face-to-face Interactions with Faceted Digital Self-Presentations
PY  - 2021
AB  - Sharing a digital presentation of self amongst collocated people can be used to enhance social interactions by supporting conversations. However, as there are different levels of disclosure within social relationships, it is currently unknown how to facet people’s digital content towards others. This research investigates faceting digital self-presentations according to the audience by looking at the differences in the creation and usage of private profiles (shared with a friend) and public profiles (shared amongst strangers) in face-to-face interactions. Digital profiles were accessed through head-mounted displays in social gatherings. Over three gatherings with twenty participants, we identified the importance of having different profiles. We found that, for strangers, public profile supported starting and maintaining conversations. For friends, the private profile was designed to support deeper social penetration, and for close friends, the private profile was designed from the friendship maintenance perspective. Additionally, participants wished to disclose content from their private profile to strangers as the conversations developed. These results suggest that there is a need for a tailored way of faceting digital self-presentation towards multiple audiences. We propose using augmentations that consist of a base profile that is shared with all collocated others, and a dynamically tailorable part, which can be targeted to specific individuals.
SP  - 192
EP  - 203
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458954
ER  - 

TY  - NA
AU  - Ku, Pin-Sung; Gong, Jun; Wu, Te-Yen; Wei, Yixin; Tang, Yiwen; Ens, Barrett; Yang, Xing-Dong
TI  - CHI - Zippro: The Design and Implementation of An Interactive Zipper
PY  - 2020
AB  - Zippers are common in a wide variety of objects that we use daily. This work investigates how we can take advantage of such common daily activities to support seamless interaction with technology. We look beyond simple zipper-sliding interactions explored previously to determine how to weave foreground and background interactions into a vocabulary of natural usage patterns. We begin by conducting two user studies to understand how people typically interact with zippers. The findings identify several opportunities for zipper input and sensing, which inform the design of Zippro, a self-contained prototype zipper slider, which we evaluate with a standard jacket zipper. We conclude by demonstrating several applications that make use of the identified foreground and background input methods.
SP  - 627
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376756
ER  - 

TY  - NA
AU  - Neburka, J.; Tlamsa, Zdenek; Benes, Vlastimil; Polak, Ladislav; Kaller, Ondrej; Bolecek, Libor; Sebesta, Jiri; Kratochvil, Tomas
TI  - Study of the performance of RSSI based Bluetooth Smart indoor positioning
PY  - 2016
AB  - Recently, there is a high interest for appropriate systems and methods to localize the position of people and their devices in indoor environments (e.g. home, office, hospital). From technical and economical points of view, Bluetooth Low Energy (BLE) is one of the promising technologies in this field. This paper deals with the study of performances of this technology for these purposes. For the indoor localization, a received signal strength indication (RSSI) method is used. It was tested in ideal (anechoic chamber) and real (in the office) environment conditions. For the measurement campaign and evaluation of the obtained results a generic framework is proposed. Experimental results (simulation vs. measurement) show similar behaviour of the BLE technology in ideal (no signal reflection) and real (multipath propagation) transmission environments.
SP  - 121
EP  - 125
JF  - 2016 26th International Conference Radioelektronika (RADIOELEKTRONIKA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/radioelek.2016.7477344
ER  - 

TY  - NA
AU  - Wang, Bo; Baluta, Teodora; Kolluri, Aashish; Saxena, Prateek
TI  - ESEC/SIGSOFT FSE - SynGuar: guaranteeing generalization in programming by example
PY  - 2021
AB  - Programming by Example (PBE) is a program synthesis paradigm in which the synthesizer creates a program that matches a set of given examples. In many applications of such synthesis (e.g., program repair or reverse engineering), we are to reconstruct a program that is close to a specific target program, not merely to produce some program that satisfies the seen examples. In such settings, we wish that the synthesized program generalizes well, i.e., has as few errors as possible on the unobserved examples capturing the target function behavior. In this paper, we propose the first framework (called SynGuar) for PBE synthesizers that guarantees to achieve low generalization error with high probability. Our main contribution is a procedure to dynamically calculate how many additional examples suffice to theoretically guarantee generalization. We show how our techniques can be used in 2 well-known synthesis approaches: PROSE and STUN (synthesis through unification), for common string-manipulation program benchmarks. We find that often a few hundred examples suffice to provably bound generalization error below 5% with high (≥ 98%) probability on these benchmarks. Further, we confirm this empirically: SynGuar significantly improves the accuracy of existing synthesizers in generating the right target programs. But with fewer examples chosen arbitrarily, the same baseline synthesizers (without SynGuar) overfit and lose accuracy.
SP  - 677
EP  - 689
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3468621
ER  - 

TY  - NA
AU  - Koelle, Marion; Wolf, Katrin; Boll, Susanne
TI  - Tangible and Embedded Interaction - Beyond LED Status Lights - Design Requirements of Privacy Notices for Body-worn Cameras
PY  - 2018
AB  - Privacy notices aim to make users aware of personal data gathered and processed by a system. Body-worn cameras currently lack suitable design strategies for privacy notices that announce themselves and their actions tosecondary andincidental users, such as bystanders, when they are being used in public. Hypothesizing that the commonly used status LED is not optimal for this use case, due to being not sufficiently understandable, noticeable, secure and trustworthy, we explore design requirements of privacy notices for body-worn cameras. Following a two-step approach, we contribute incentives for design alternatives to status LEDs: Starting from 8 design sessions with experts, we discuss 8 physical design artifacts, as well as design strategies and key motives. Finally, we derive design recommendations of the proposed solutions, which we back based on an evaluation with 12 UX & HCI experts.
SP  - 177
EP  - 187
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173234
ER  - 

TY  - NA
AU  - Jarrar, Ahmad; Jaser, Edward
TI  - Context Aware VR Using BLE Beacons
PY  - 2017
AB  - This paper studies the possibility of bringing context awareness to Virtual Reality applications running on an Android smartphone. By leveraging the tools provided by the Android system to scan and analyze the surrounding environment for Bluetooth Low Energy beacons. The data obtained from the beacons is then broadcasted through the Android system to other applications and Virtual Reality games running on the same Android device. Beacons data contain information about the signal received from each beacon. This data is used to estimate the distance between the beacon and the smartphone. The estimated distance can be used in Virtual Reality applications to change the location of object. The paper proposes a method that achieves the required task from the moment a beacon is detected until the data is passed and used inside the Virtual Reality application. The evaluation of the proposed system is mainly based on the resulted latency, accuracy of the estimated distance and CPU usage. The result shows the latency can be controlled and minimized to ensure smooth Virtual Reality experience.
SP  - 303
EP  - 308
JF  - 2017 International Conference on New Trends in Computing Sciences (ICTCS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ictcs.2017.64
ER  - 

TY  - NA
AU  - Pelant, Jan; Tlamsa, Zdenek; Benes, Vlastimil; Polak, Ladislav; Kaller, Ondrej; Bolecek, Libor; Kufa, Jan; Sebesta, Jiri; Kratochvil, Tomas
TI  - BLE device indoor localization based on RSS fingerprinting mapped by propagation modes
PY  - 2017
AB  - Nowadays, Bluetooth Low Energy (BLE) technology has a great attention in the field of wireless localization techniques, especially in the case of indoor scenarios. Study presented in this paper explores BLE localization performance in indoor scenarios based on a received signal strength (RSS). Firstly, we present a Ray-Launching based application to emulate BLE radio frequency (RF) signal propagation in the office environment. Secondly, an appropriate measurement workplace and its setup is proposed to measure RSS values. Obtained results are used to create an RSS-fingerprinting map. Furthermore, accuracy of position determination is calculated and evaluated. Results show advantage of BLE technology for indoor localization purposes (without calibration measurement). However, its performance highly depends on the number of considered BLE nodes and on applied evaluation method (e.g. number of considered sectors).
SP  - 1
EP  - 5
JF  - 2017 27th International Conference Radioelektronika (RADIOELEKTRONIKA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/radioelek.2017.7937584
ER  - 

TY  - NA
AU  - Wang, Chenglong; Feng, Yu; Bodik, Rastislav; Dillig, Isil; Cheung, Alvin; Ko, Amy J.
TI  - Falx: Synthesis-Powered Visualization Authoring
PY  - 2021
AB  - Modern visualization tools aim to allow data analysts to easily create exploratory visualizations. When the input data layout conforms to the visualization design, users can easily specify visualizations by mapping data columns to visual channels of the design. However, when there is a mismatch between data layout and the design, users need to spend significant effort on data transformation. We propose Falx, a synthesis-powered visualization tool that allows users to specify visualizations in a similarly simple way but without needing to worry about data layout. In Falx, users specify visualizations using examples of how concrete values in the input are mapped to visual channels, and Falx automatically infers the visualization specification and transforms the data to match the design. In a study with 33 data analysts on four visualization tasks involving data transformation, we found that users can effectively adopt Falx to create visualizations they otherwise cannot implement.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445249
ER  - 

TY  - NA
AU  - Desai, Aditya; Gulwani, Sumit; Hingorani, Vineet; Jain, Nidhi; Karkare, Amey; Marron, Mark; R, Sailesh; Roy, Subhajit
TI  - ICSE - Program synthesis using natural language
PY  - 2016
AB  - Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks. We present a general framework for constructing program synthesizers that take natural language (NL) inputs and produce expressions in a target DSL. The framework takes as input a DSL definition and training data consisting of NL/DSL pairs. From these it constructs a synthesizer by learning optimal weights and classifiers (using NLP features) that rank the outputs of a keyword-programming based translation. We applied our framework to three domains: repetitive text editing, an intelligent tutoring system, and flight information queries. On 1200+ English descriptions, the respective synthesizers rank the desired program as the top-1 and top-3 for 80% and 90% descriptions respectively.
SP  - 345
EP  - 356
JF  - Proceedings of the 38th International Conference on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2884781.2884786
ER  - 

TY  - NA
AU  - Huang, Da-Yuan; Seyed, Teddy; Li, Linjun; Gong, Jun; Yao, Zhihao; Jiao, Yuchen; Chen, Xiang 'Anthony'; Yang, Xing-Dong
TI  - UIST - Orecchio: Extending Body-Language through Actuated Static and Dynamic Auricular Postures
PY  - 2018
AB  - In this paper, we propose using the auricle - the visible part of the ear - as a means of expressive output to extend body language to convey emotional states. With an initial exploratory study, we provide an initial set of dynamic and static auricular postures. Using these results, we examined the relationship between emotions and auricular postures, noting that dynamic postures involving stretching the top helix in fast (e.g., 2Hz) and slow speeds (1Hz) conveyed intense and mild pleasantness while static postures involving bending the side or top helix towards the center of the ear were associated with intense and mild unpleasantness. Based on the results, we developed a prototype (called Orrechio) with miniature motors, custom-made robotic arms and other electronic components. A preliminary user evaluation showed that participants feel more comfortable using expressive auricular postures with people they are familiar with, and that it is a welcome addition to the vocabulary of human body language.
SP  - 697
EP  - 710
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242629
ER  - 

TY  - NA
AU  - Achalla, Monalisa; Mack, Kevin; Banavar, Mahesh K.; Vanitha, M.; Krishnamoorthi, Harish
TI  - Statistical Methods for Fast LOS Detection for Ranging and Localization
PY  - 2020
AB  - Received signal strength indication (RSSI) data is often used for ranging and localization algorithms, where the data may be obtained using Bluetooth Low Energy (BLE) radios. In the BLE protocol, when a device is in advertising mode, it is possible to obtain RSSI values. However, these RSSI values are noisy and can often fluctuate due to multipath effects, which reduces the accuracy and reliability of ranging and localization. Additionally, the effectiveness of RSSI ranging degrades when there is an absence of line of sight (LOS) or when devices are in rich scattering environments. Therefore, the detection of LOS plays a very significant role in indoor localization and room reconstruction.In this paper, we present algorithms to detect whether there is LOS present between a transmit-receive pair being used for ranging. Our focus in this paper is fast detection with a minimum number of samples. We use measurements such as the energy distance and Mahalanobis distance, and benchmark our results against the Neyman-Pearson detector. Numerical simulations are used to validate our algorithms.
SP  - NA
EP  - NA
JF  - 2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ic-etite47903.2020.419
ER  - 

TY  - JOUR
AU  - Casey, Peter; Baggili, Ibrahim; Yarramreddy, Ananya
TI  - Immersive Virtual Reality Attacks and the Human Joystick
PY  - 2021
AB  - This is one of the first accounts for the security analysis of consumer immersive Virtual Reality (VR) systems. This work breaks new ground, coins new terms, and constructs proof of concept implementations of attacks related to immersive VR. Our work used the two most widely adopted immersive VR systems, the HTC Vive, and the Oculus Rift. More specifically, we were able to create attacks that can potentially disorient users, turn their Head Mounted Display (HMD) camera on without their knowledge, overlay images in their field of vision, and modify VR environmental factors that force them into hitting physical objects and walls. Finally, we illustrate through a human participant deception study the success of being able to exploit VR systems to control immersed users and move them to a location in physical space without their knowledge. We term this the Human Joystick Attack. We conclude our work with future research directions and ways to enhance the security of these systems.
SP  - 550
EP  - 562
JF  - IEEE Transactions on Dependable and Secure Computing
VL  - 18
IS  - 2
PB  - 
DO  - 10.1109/tdsc.2019.2907942
ER  - 

TY  - NA
AU  - Mantere, Eerik
TI  - What Smartphones, Ethnomethodology, and Bystander Inaccessibility Can Teach Us About Better Design?
PY  - 2019
AB  - Smartphones, the ubiquitous mobile screens now normal parts of everyday social situations, have created a kind of ongoing natural experiment for social scientists. According to Garfinkel's ethnomethodology social action gets its meaning not only from its content but also through its context. Mobility, small screen size, and the habitual way of using smartphones ensure that, while offering the biggest variety of activities for the user, in comparison to other everyday items, smartphones offer the least cues to bystanders on what the user is actually doing and how long it might take. This 'bystander inaccessibility' handicaps shared understanding of the social context that the user and collocated others find themselves in. Added considerations and interactive effort in managing the situation is therefore required. Future design needs to relate to this basic building block of collocated interaction to not be met with discontent.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ji, Ruyi; Liang, Jingjing; Xiong, Yingfei; Zhang, Lu; Hu, Zhenjiang
TI  - PLDI - Question selection for interactive program synthesis
PY  - 2020
AB  - Interactive program synthesis aims to solve the ambiguity in specifications, and selecting the proper question to minimize the rounds of interactions is critical to the performance of interactive program synthesis. In this paper we address this question selection problem and propose two algorithms. SampleSy approximates a state-of-the-art strategy proposed for optimal decision tree and has a short response time to enable interaction. EpsSy further reduces the rounds of interactions by approximating SampleSy with a bounded error rate. To implement the two algorithms, we further propose VSampler, an approach to sampling programs from a probabilistic context-free grammar based on version space algebra. The evaluation shows the effectiveness of both algorithms.
SP  - 1143
EP  - 1158
JF  - Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385412.3386025
ER  - 

TY  - NA
AU  - Ramos, Daniel; Pereira, Jorge Manuel Torres; Lynce, Inês; Manquinho, Vasco M.; Martins, Ruben
TI  - ASE - UnchartIt: an interactive framework for program recovery from charts
PY  - 2020
AB  - Charts are commonly used for data visualization. Generating a chart usually involves performing data transformations, including data pre-processing and aggregation. These tasks can be cumbersome and time-consuming, even for experienced data scientists. Reproducing existing charts can also be a challenging task when information about data transformations is no longer available. In this paper, we tackle the problem of recovering data transformations from existing charts. Given an input table and a chart, our goal is to automatically recover the data transformation program underlying the chart. We divide our approach into four steps: (1) data extraction, (2) candidate generation, (3) candidate ranking, and (4) candidate disambiguation. We implemented our approach in a tool called UnchartIt and evaluated it on a set of 50 benchmarks from Kaggle. Experimental results show that UnchartIt successfully ranks the correct data transformation among the top-10 programs in 92% of the benchmarks. To disambiguate the top-ranking programs, we use our new interactive procedure, which successfully disambiguates 98% of the ambiguous benchmarks by asking on average fewer than 2 questions to the user.
SP  - 175
EP  - 186
JF  - Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3324884.3416613
ER  - 

TY  - JOUR
AU  - von Terzi, Pia; Tretter, Stefan; Uhde, Alarith; Hassenzahl, Marc; Diefenbach, Sarah
TI  - Technology-Mediated Experiences and Social Context: Relevant Needs in Private Vs. Public Interaction and the Importance of Others for Positive Affect.
PY  - 2021
AB  - Technologies, such as smartphones or wearables, take a central role in our daily lives. Making their use meaningful and enjoyable requires a better understanding of the prerequisites and underpinnings of positive experiences with such technologies. So far, a focus had been on the users themselves, that is, their individual goals, desires, feelings, and acceptance. However, technology is often used in a social context, observed by others or even used in interaction with others, and thus shapes social dynamics considerably. In the present paper, we start from the notion that meaningful and/or enjoyable experiences (i.e., wellbeing) are a major outcome of technology use. We investigate how these experiences are further shaped by social context, such as potential spectators. More specifically, we gathered private (while being alone) and public (while other people are present) positive experiences with technology and compared need fulfillment and affective experience. In addition, we asked participants to imagine a change in context (from private to public or public to private) and to report the impact of this change on experience. Results support the idea of particular social needs, such as relatedness and popularity, which are especially relevant and better fulfilled in public than in private contexts. Moreover, our findings show that participants experience less positive affect when imaginatively removing the present others from a formerly public interaction, i.e., when they imagine performing the same interaction but without the other people present. Overall, this underlines the importance of social context for Human-Computer Interaction practice and research. Practical implications relate to product development, e.g., designing interactive technologies that can adapt to context (changes) or allow for context-sensitive interaction sets. We discuss limitations related to the experimental exploration of social context, such as the method of data collection, as well as potential alternatives to address those limitations, such as diary studies.
SP  - 718315
EP  - NA
JF  - Frontiers in psychology
VL  - 12
IS  - NA
PB  - 
DO  - 10.3389/fpsyg.2021.718315
ER  - 

TY  - NA
AU  - Pu, Kevin; Fu, Rainey; Dong, Rui; Wang, Xinyu; Chen, Yan; Grossman, Tovi
TI  - SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545691
ER  - 

TY  - CHAP
AU  - Gulwani, Sumit; Jain, Prateek
TI  - APLAS - Programming by Examples: PL Meets ML
PY  - 2017
AB  - Programming by Examples (PBE) involves synthesizing intended programs in an underlying domain-specific language from example-based specifications. PBE systems are already revolutionizing the application domain of data wrangling and are set to significantly impact several other domains including code refactoring.
SP  - 3
EP  - 20
JF  - Programming Languages and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-71237-6_1
ER  - 

TY  - NA
AU  - Ye, Huizhong; Janssen, Charlaine; Noordman, Daan; Liang, Rong-Hao
TI  - Understanding How to Support Remote Co-Design with a Conceptual Modular Shape-Changing Interface Toolkit
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505563
ER  - 

TY  - NA
AU  - Hu, Jingmei; Vaithilingam, Priyan; Chong, Stephen; Seltzer, Margo; Glassman, Elena L.
TI  - UIST - Assuage: Assembly Synthesis Using A Guided Exploration
PY  - 2021
AB  - Assembly programming is challenging, even for experts. Program synthesis, as an alternative to manual implementation, has the potential to enable both expert and non-expert users to generate programs in an automated fashion. However, current tools and techniques are unable to synthesize assembly programs larger than a few instructions. We present Assuage : ASsembly Synthesis Using A Guided Exploration, which is a parallel interactive assembly synthesizer that engages the user as an active collaborator, enabling synthesis to scale beyond current limits. Using Assuage, users can provide two types of semantically meaningful hints that expedite synthesis and allow for exploration of multiple possibilities simultaneously. Assuage exposes information about the underlying synthesis process using multiple representations to help users guide synthesis. We conducted a within-subjects study with twenty-one participants working on assembly programming tasks. With Assuage, participants with a wide range of expertise were able to achieve significantly higher success rates, perceived less subjective workload, and preferred the usefulness and usability of Assuage over a state of the art synthesis tool.
SP  - 134
EP  - 148
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474740
ER  - 

TY  - CONF
AU  - Uhde, Alarith; Hassenzahl, Marc
TI  - CHI Extended Abstracts - Towards a Better Understanding of Social Acceptability
PY  - 2021
AB  - Social contexts play an important role in understanding acceptance and use of technology. However, current approaches used in HCI to describe contextual influence do not capture it appropriately. On the one hand, the often used Technology Acceptance Model and related frameworks are too rigid to account for the nuanced variations of social situations. On the other hand, Goffman’s dramaturgical model of social interactions emphasizes interpersonal relations but mostly overlooks the material (e.g., technology) that is central to HCI. As an alternative, we suggest an approach based on Social Practice Theory. We conceptualize social context as interactions between co-located social practices and acceptability as a matter of their (in)compatibilities. Finally, we outline how this approach provides designers with a better understanding of different types of social acceptability problems and helps finding appropriate solutions.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - An, Shengwei; Singh, Rishabh; Misailovic, Sasa; Samanta, Roopsha
TI  - Augmented example-based synthesis using relational perturbation properties
PY  - 2019
AB  - Example-based specifications for program synthesis are inherently ambiguous and may cause synthesizers to generate programs that do not exhibit intended behavior on unseen inputs. Existing synthesis techniques attempt to address this problem by either placing a domain-specific syntactic bias on the hypothesis space or heavily relying on user feedback to help resolve ambiguity. We present a new framework to address the ambiguity/generalizability problem in example-based synthesis. The key feature of our framework is that it places a semantic bias on the hypothesis space using "relational perturbation properties" that relate the perturbation/change in a program output to the perturbation/change in a program input. An example of such a property is permutation invariance: the program output does not change when the elements of the program input (array) are permuted. The framework is portable across multiple domains and synthesizers and is based on two core steps: (1) automatically augment the set of user-provided examples by "applying" relational perturbation properties and (2) use a generic example-based synthesizer to generate a program consistent with the augmented set of examples. Our framework can be instantiated with three different user interfaces, with varying degrees of user engagement to help infer relevant relational perturbation properties. This includes an interface in which the user only provides examples and our framework automatically infers relevant properties. We implement our framework in a tool SKETCHAX specialized to the SKETCH synthesizer and demonstrate that SKETCHAX is effective in significantly boosting the performance of SKETCH for all three user interfaces.
SP  - 56
EP  - 24
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - POPL
PB  - 
DO  - 10.1145/3371124
ER  - 

TY  - NA
AU  - Steil, Julian; Koelle, Marion; Heuten, Wilko; Boll, Susanne; Bulling, Andreas
TI  - PrivacEye: Privacy-Preserving First-Person Vision Using Image Features and Eye Movement Analysis
PY  - 2018
AB  - As first-person cameras in head-mounted displays become increasingly prevalent, so does the problem of infringing user and bystander privacy. To address this challenge, we present PrivacEye, a proof-of-concept system that detects privacysensitive everyday situations and automatically enables and disables the first-person camera using a mechanical shutter. To close the shutter, PrivacEye detects sensitive situations from first-person camera videos using an end-to-end deep-learning model. To open the shutter without visual input, PrivacEye uses a separate, smaller eye camera to detect changes in users' eye movements to gauge changes in the "privacy level" of the current situation. We evaluate PrivacEye on a dataset of first-person videos recorded in the daily life of 17 participants that they annotated with privacy sensitivity levels. We discuss the strengths and weaknesses of our proof-of-concept system based on a quantitative technical evaluation as well as qualitative insights from semi-structured interviews.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Endo, Kazuomi; Hamamoto, Ryo; Tanaka, Hiroki; Yamashita, Koichi; Kagoshima, Masayuki; Togo, Hiroshi; Mizukoshi, Kouta; Kawaguchi, Hidenaga; Kishino, Yuta
TI  - CANDAR Workshops - A Basic Study on Communication Characteristics by Bluetooth Low Energy for i-Construction
PY  - 2019
AB  - At present, construction machines such as excavators and cranes are built with communication modules in order to connect to networks. Therefore, the construction machines can be controlled through commands sent via the network to improve productivity. This is called i-construction. Two examples of i-construction are remote control and automatic control systems for the construction machine. In these systems, generally, wireless communication is used. However, the wireless communication characteristics that need to be considered in i-construction are still unclear. In order to clarify, this paper seeks to characterize the characteristics of Bluetooth Low Energy (BLE) communication when used in construction machines. In the evaluations, we use a BLE beacon module. Experimental results show that the BLE beacon module can communicate between the sender in the excavator's attachment and receiver in the cabin. The results show that there is a possibility that the wireless communication systems can be applied for the construction machines.
SP  - 47
EP  - 50
JF  - 2019 Seventh International Symposium on Computing and Networking Workshops (CANDARW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/candarw.2019.00015
ER  - 

TY  - NA
AU  - Cheung, Victor; Eady, Alexander Keith; Girouard, Audrey
TI  - Tangible and Embedded Interaction - Exploring Eyes-free Interaction with Wrist-Worn Deformable Materials
PY  - 2017
AB  - Recent work has explored how on-the-wrist devices can be augmented with displays, sensors, and aesthetics to produce a new genre of wearable, digital jewelry. Most often, these devices are augmented with hardware (e.g., touch-screens, gyroscopes) to receive wearer input. Breakthroughs in sensing materials allow novel inputs such as squeeze, bend, and stretch, which may be more suited to interaction with a discreet wearable.We explore interactions with a flexible bracelet that can be manipulated through bend, stretch, and touch. We discuss the process involved in fabricating materials that convert these actions into measurable parameters. We particularly focus on eyes-free interactions with bracelets that take advantage of the tactile feedback from the act of deformation, and demonstrate some early, low-fidelity prototype concepts.
SP  - 521
EP  - 528
JF  - Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3024969.3025087
ER  - 

TY  - NA
AU  - Feldman, Molly Q.; Cho, Ji Yong; Ong, Monica; Gulwani, Sumit; Popović, Zoran; Andersen, Erik
TI  - CHI - Automatic Diagnosis of Students' Misconceptions in K-8 Mathematics
PY  - 2018
AB  - K-8 mathematics students must learn many procedures, such as addition and subtraction. Students frequently learn "buggy' variations of these procedures, which we ideally could identify automatically. This is challenging because there are many possible variations that reflect deep compositions of procedural thought. Existing approaches for K-8 math use manually specified variations which do not scale to new math algorithms or previously unseen misconceptions. Our system examines students' answers and infers how they incorrectly combine basic skills into complex procedures. We evaluate this approach on data from approximately 300 students. Our system replicates 86% of the answers that contain clear systematic mistakes (13%). Investigating further, we found 77% at least partially replicate a known misconception, with 53% matching exactly. We also present data from 29 participants showing that our system can demonstrate inferred incorrect procedures to an educator as successfully as a human expert.
SP  - 264
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173838
ER  - 

TY  - JOUR
AU  - Ji, Ruyi; Xia, Jingtao; Xiong, Yingfei; Hu, Zhenjiang
TI  - Generalizable synthesis through unification
PY  - 2021
AB  - The generalizability of PBE solvers is the key to the empirical synthesis performance. Despite the importance of generalizability, related studies on PBE solvers are still limited. In theory, few existing solvers provide theoretical guarantees on generalizability, and in practice, there is a lack of PBE solvers with satisfactory generalizability on important domains such as conditional linear integer arithmetic (CLIA). In this paper, we adopt a concept from the computational learning theory, Occam learning, and perform a comprehensive study on the framework of synthesis through unification (STUN), a state-of-the-art framework for synthesizing programs with nested if-then-else operators. We prove that Eusolver, a state-of-the-art STUN solver, does not satisfy the condition of Occam learning, and then we design a novel STUN solver, PolyGen, of which the generalizability is theoretically guaranteed by Occam learning. We evaluate PolyGen on the domains of CLIA and demonstrate that PolyGen significantly outperforms two state-of-the-art PBE solvers on CLIA, Eusolver and Euphony, on both generalizability and efficiency.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Programming Languages
VL  - 5
IS  - OOPSLA
PB  - 
DO  - 10.1145/3485544
ER  - 

TY  - CHAP
AU  - Guzman, Jaybie A. De; Thilakarathna, Kanchana; Seneviratne, Aruna
TI  - Privacy and Security Issues and Solutions for Mixed Reality Applications
PY  - 2021
AB  - NA
SP  - 157
EP  - 183
JF  - Springer Handbooks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-67822-7_7
ER  - 

TY  - NA
AU  - Chan, Liwei; Minamizawa, Kouta
TI  - MobileHCI - FrontFace: facilitating communication between HMD users and outsiders using front-facing-screen HMDs
PY  - 2017
AB  - A head-mounted display (HMD) immerses users in a virtual world, but separates them from outsiders in the real world. We present FrontFace, which is a novel HMD that combines an eye-tracker with a front-facing screen, to lower the communication barrier between HMD users and outsiders. The front-facing screen reveals user attention (e.g., the users eye motions) and user presence in the virtual or real world by displaying the scene in the virtual world or a skin background respectively, enabling eye-contact interactions between the HMD user and the outsiders. FrontFace has the following benefits. Firstly, it communicates the presence of the HMD user to outsiders; secondly, it reveals the player's visual attention by introducing the HMD users originally occluded eye motions, enabling outsiders to make sense of the HMD user's reaction in the virtual world or the real world. Three interactive techniques for the outsiders to initiate communication to HMD users are proposed: they are tap-trigger, hand-gesture trigger, and voice-trigger interactions. A small focus group provided feedback.
SP  - 22
EP  - NA
JF  - Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3098279.3098548
ER  - 

TY  - JOUR
AU  - Banavar, Mahesh K.; Wickramasinghe, Shandeepa; Achalla, Monalisa; Sun, Jie
TI  - Ordinal UNLOC: Target Localization With Noisy and Incomplete Distance Measures
PY  - 2021
AB  - A main challenge in target localization arises from the lack of reliable distance measures. This issue is especially pronounced in indoor settings due to the presence of walls, floors, furniture, and other dynamically changing conditions, such as the movement of people and goods, varying temperature and air flows. Here, we develop a new computational framework to estimate the location of a target without the need for reliable distance measures. The method, which we term Ordinal UNLOC, uses only ordinal data obtained from comparing the signal strength from anchor pairs at known locations to the target. Our estimation technique utilizes rank aggregation, function learning as well as proximity-based unfolding optimization. As a result, it yields accurate target localization for common transmission models with unknown parameters and noisy observations that are reminiscent of practical settings. Our results are validated by both numerical simulations and hardware experiments.
SP  - 17111
EP  - 17119
JF  - IEEE Internet of Things Journal
VL  - 8
IS  - 23
PB  - 
DO  - 10.1109/jiot.2021.3078331
ER  - 

TY  - JOUR
AU  - Verbruggen, Gust; Le, Vu; Gulwani, Sumit
TI  - Semantic programming by example with pre-trained models
PY  - 2021
AB  - The ability to learn programs from few examples is a powerful technology with disruptive applications in many domains, as it allows users to automate repetitive tasks in an intuitive way. Existing frameworks on inductive synthesis only perform syntactic manipulations, where they rely on the syntactic structure of the given examples and not their meaning. Any semantic manipulations, such as transforming dates, have to be manually encoded by the designer of the inductive programming framework. Recent advances in large language models have shown these models to be very adept at performing semantic transformations of its input by simply providing a few examples of the task at hand. When it comes to syntactic transformations, however, these models are limited in their expressive power. In this paper, we propose a novel framework for integrating inductive synthesis with few-shot learning language models to combine the strength of these two popular technologies. In particular, the inductive synthesis is tasked with breaking down the problem in smaller subproblems, among which those that cannot be solved syntactically are passed to the language model. We formalize three semantic operators that can be integrated with inductive synthesizers. To minimize invoking expensive semantic operators during learning, we introduce a novel deferred query execution algorithm that considers the operators to be oracles during learning. We evaluate our approach in the domain of string transformations: the combination methodology can automate tasks that cannot be handled using either technologies by themselves. Finally, we demonstrate the generality of our approach via a case study in the domain of string profiling.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Programming Languages
VL  - 5
IS  - OOPSLA
PB  - 
DO  - 10.1145/3485477
ER  - 

TY  - CHAP
AU  - Ferreira, Margarida; Terra-Neves, Miguel; Ventura, Miguel; Lynce, Inês; Martins, Ruben
TI  - TACAS (1) - FOREST: An Interactive Multi-tree Synthesizer for Regular Expressions
PY  - 2021
AB  - Form validators based on regular expressions are often used on digital forms to prevent users from inserting data in the wrong format. However, writing these validators can pose a challenge to some users.
SP  - 152
EP  - 169
JF  - Tools and Algorithms for the Construction and Analysis of Systems
VL  - 12651
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-72016-2_9
ER  - 

TY  - JOUR
AU  - Glauser, Oliver; Panozzo, Daniele; Hilliges, Otmar; Sorkine-Hornung, Olga
TI  - Deformation Capture via Soft and Stretchable Sensor Arrays
PY  - 2019
AB  - We propose a hardware and software pipeline to fabricate flexible wearable sensors and use them to capture deformations without line-of-sight. Our first contribution is a low-cost fabrication pipeline to embed multiple aligned conductive layers with complex geometries into silicone compounds. Overlapping conductive areas from separate layers form local capacitors that measure dense area changes. Contrary to existing fabrication methods, the proposed technique only requires hardware that is readily available in modern fablabs. While area measurements alone are not enough to reconstruct the full 3D deformation of a surface, they become sufficient when paired with a data-driven prior. A novel semi-automatic tracking algorithm, based on an elastic surface geometry deformation, allows us to capture ground-truth data with an optical mocap system, even under heavy occlusions or partially unobservable markers. The resulting dataset is used to train a regressor based on deep neural networks, directly mapping the area readings to global positions of surface vertices. We demonstrate the flexibility and accuracy of the proposed hardware and software in a series of controlled experiments and design a prototype of wearable wrist, elbow, and biceps sensors, which do not require line-of-sight and can be worn below regular clothing.
SP  - 16
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 2
PB  - 
DO  - 10.1145/3311972
ER  - 

TY  - JOUR
AU  - Chasins, Sarah; Bodik, Rastislav
TI  - Skip blocks: reusing execution history to accelerate web scripts
PY  - 2017
AB  - With more and more web scripting languages on offer, programmers have access to increasing language support for web scraping tasks. However, in our experiences collaborating with data scientists, we learned that two issues still plague long-running scraping scripts: i) When a network or website goes down mid-scrape, recovery sometimes requires restarting from the beginning, which users find frustratingly slow. ii) Websites do not offer atomic snapshots of their databases; they update their content so frequently that output data is cluttered with slight variations of the same information — e.g., a tweet from profile 1 that is retweeted on profile 2 and scraped from both profiles, once with 52 responses then later with 53 responses. We introduce the skip block, a language construct that addresses both of these disparate problems. Programmers write lightweight annotations to indicate when the current object can be considered equivalent to a previously scraped object and direct the program to skip over the scraping actions in the block. The construct is hierarchical, so programs can skip over long or short script segments, allowing adaptive reuse of prior work. After network and server failures, skip blocks accelerate failure recovery by 7.9x on average. Even scripts that do not encounter failures benefit; because sites display redundant objects, skipping over them accelerates scraping by up to 2.1x. For longitudinal scraping tasks that aim to fetch only new objects, the second run exhibits an average speedup of 5.2x. Our small user study reveals that programmers can quickly produce skip block annotations.
SP  - 51
EP  - 28
JF  - Proceedings of the ACM on Programming Languages
VL  - 1
IS  - OOPSLA
PB  - 
DO  - 10.1145/3133875
ER  - 

TY  - BOOK
AU  - Bhattacharya, Arka; Hong, Dezhi; Culler, David E.; Ortiz, Jorge; Whitehouse, Kamin; Wu, Eugene
TI  - BuildSys@SenSys - Automated Metadata Construction to Support Portable Building Applications
PY  - 2015
AB  - Commercial buildings consume nearly 19\% of delivered energy in the U.S, nearly half (42%) of which is consumed in buildings with digital control systems comprised of wired sensor networks. These sensors have scant metadata, and are represented by ``tags'' which are obscure, building-specific and not machine parseable. We develop a human-in-the-loop synthesis technique which uses syntactic and data-driven steps to parse these sensor tags into a common namespace, which can enable portable building applications. We show that our technique allows an expert to fully parse a large fraction (~70%) of the tags with 24, 15 and 43 examples for three large commercial buildings comprising 1586, 2522 and 1865 sensors respectively, and deploy three portable applications on two buildings with less than 30 examples.
SP  - 3
EP  - 12
JF  - Proceedings of the 2nd ACM International Conference on Embedded Systems for Energy-Efficient Built Environments
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/2821650.2821667
ER  - 

TY  - NA
AU  - Jayagopal, Dhanya; Lubin, Justin; Chasins, Sarah E.
TI  - Exploring the Learnability of Program Synthesizers by Novice Programmers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545659
ER  - 

TY  - NA
AU  - Zhou, Xiangyu; Bodik, Rastislav; Cheung, Alvin; Wang, Chenglong
TI  - Synthesizing analytical SQL queries from computation demonstration
PY  - 2022
AB  - Analytical SQL is widely used in modern database applications and data analysis. However, its partitioning and grouping operators are challenging for novice users. Unfortunately, programming by example, shown effective on standard SQL, are less attractive because examples for analytical queries are more laborious to solve by hand. To make demonstrations easier to create, we designed a new end-user specification, programming by computation demonstration, that allows the user to demonstrate the task using a (possibly incomplete) cell-level computation trace. This specification is exploited in a new abstraction-based synthesis algorithm to prove that a partially formed query cannot be completed to satisfy the specification, allowing us to prune the search space. We implemented our approach in a tool named Sickle and tested it on 80 real-world analytical SQL tasks. Results show that even from small demonstrations, Sickle can solve 76 tasks, in 12.8 seconds on average, while the prior approaches can solve only 60 tasks and are on average 22.5x slower. Our user study with 13 participants reveals that our specification increases user efficiency and confidence on challenging tasks.
SP  - NA
EP  - NA
JF  - Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519939.3523712
ER  - 

TY  - NA
AU  - Narita, Minori; Maudet, Nolwenn; Lu, Yi; Igarashi, Takeo
TI  - UIST (Adjunct Volume) - FlashAttention: Data-centric Interaction for Data Transformation Using Programming-by-Example
PY  - 2020
AB  - Programming-by-example (PBE) can be a powerful tool for reducing manual work in repetitive data transformation tasks. However, having few examples often leaves ambiguity and may cause undesirable data transformation by the system. This ambiguity can be resolved by allowing the user to directly edit the synthesized programs, but this is difficult for non-programmers. Here, we present a novel approach: data-centric interaction for data transformation, where users resolve the ambiguity in data transformation by examining and modifying the output rather than the program. The key idea is focusing on the given set of data the user wants to transform instead of pursuing the synthesized program's completeness. Our system provides interactive visualization that allows users to efficiently examine and fix the transformed outputs, which is much simpler than understanding and modifying the program itself. Our experiment shows that the number of candidates is much smaller than the number of synthesized programs, which implies the effectiveness of the proposed method.
SP  - 65
EP  - 67
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416187
ER  - 

TY  - NA
AU  - Uhde, Alarith; zum Hoff, Tim; Hassenzahl, Marc
TI  - Obtrusive Subtleness and Why We Should Focus on Meaning, not Form, in Social Acceptability Studies
PY  - 2022
AB  - Nowadays, interactive technologies are used almost everywhere. As a result, designers need to increasingly make them "socially acceptable". Previous work recommends "subtle" forms of interaction to increase social acceptability and avoid negative experiences. Although often appropriate, such uniform recommendations neglect the variety of social situations. We demonstrate this limitation in an experiment (N=35), by comparing the observer experience of different forms of interaction in "face-to-face conversations", a social situation rarely studied. Here, the typically recommended form of interaction ("subtle") led to a more negative observer experience than the usually deprecated form ("suspenseful"), in terms of affective experience and product perception. It also made the user appear less extraverted. We conclude by positioning interactions with technology not as separate from the social situation in which they are performed, but as a constitutive part of it that meaningfully relates to other situated activities.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3568457
ER  - 

TY  - JOUR
AU  - Wang, Chenglong; Cheung, Alvin; Bodik, Rastislav
TI  - Synthesizing highly expressive SQL queries from input-output examples
PY  - 2017
AB  - <jats:p>SQL is the de facto language for manipulating relational data. Though powerful, many users find it difficult to write SQL queries due to highly expressive constructs.</jats:p> <jats:p>While using the programming-by-example paradigm to help users write SQL queries is an attractive proposition, as evidenced by online help forums such as Stack Overflow, developing techniques for synthesizing SQL queries from given input-output (I/O) examples has been difficult, due to the large space of SQL queries as a result of its rich set of operators.</jats:p> <jats:p /> <jats:p>In this paper, we present a new scalable and efficient algorithm for synthesizing SQL queries based on I/O examples. The key innovation of our algorithm is development of a language for abstract queries, i.e., queries with uninstantiated operators, that can be used to express a large space of SQL queries efficiently. Using abstract queries to represent the search space nicely decomposes the synthesis problem into two tasks: 1) searching for abstract queries that can potentially satisfy the given I/O examples, and 2) instantiating the found abstract queries and ranking the results.</jats:p> <jats:p /> <jats:p>We have implemented this algorithm in a new tool called Scythe and evaluated it using 193 benchmarks collected from Stack Overflow. Our evaluation shows that Scythe can efficiently solve 74% of the benchmarks, most in just a few seconds, and the queries range from simple ones involving a single selection to complex queries with 6 nested subqueires.</jats:p>
SP  - 452
EP  - 466
JF  - ACM SIGPLAN Notices
VL  - 52
IS  - 6
PB  - 
DO  - 10.1145/3140587.3062365
ER  - 

TY  - JOUR
AU  - Li, Yunyao; Rafiei, Davood
TI  - Natural Language Data Management and Interfaces
PY  - 2018
AB  - NA
SP  - 1
EP  - 156
JF  - Synthesis Lectures on Data Management
VL  - 10
IS  - 2
PB  - 
DO  - 10.2200/s00866ed1v01y201807dtm049
ER  - 
