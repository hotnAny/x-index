
TY  - JOUR
AU  - Ogawa, Nami; Narumi, Takuji; Hirose, Michitaka
TI  - Effect of Avatar Appearance on Detection Thresholds for Remapped Hand Movements
PY  - 2021
AB  - Hand interaction techniques in virtual reality often exploit visual dominance over proprioception to remap physical hand movements onto different virtual movements. However, when the offset between virtual and physical hands increases, the remapped virtual hand movements are hardly self-attributed, and the users become aware of the remapping. Interestingly, the sense of self-attribution of a body is called the sense of body ownership (SoBO) in the field of psychology, and the realistic the avatar, the stronger is the SoBO. Hence, we hypothesized that realistic avatars (i.e., human hands) can foster self-attribution of the remapped movements better than abstract avatars (i.e., spherical pointers), thus making the remapping less noticeable. In this article, we present an experiment in which participants repeatedly executed reaching movements with their right hand while different amounts of horizontal shifts were applied. We measured the remapping detection thresholds for each combination of shift directions (left or right) and avatar appearances (realistic or abstract). The results show that realistic avatars increased the detection threshold (i.e., lowered sensitivity) by 31.3 percent than the abstract avatars when the leftward shift was applied (i.e., when the hand moved in the direction away from the body-midline). In addition, the proprioceptive drift (i.e., the displacement of self-localization toward an avatar) was larger with realistic avatars for leftward shifts, indicating that visual information was given greater preference during visuo-proprioceptive integration in realistic avatars. Our findings quantifiably show that realistic avatars can make remapping less noticeable for larger mismatches between virtual and physical movements and can potentially improve a wide variety of hand-remapping techniques without changing the mapping itself.
SP  - 3182
EP  - 3197
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2020.2964758
ER  - 

TY  - CHAP
AU  - Rao, Nanjie; Chu, Sharon Lynn; Chenore, NA
TI  - HCI (9) - Flexible Low-Cost Digital Puppet System
PY  - 2021
AB  - Puppet-basedsystems have been developed to help children engage in storytelling and pretend play in much prior literature.Many different approaches have been proposed to implement suchpuppet-based storytelling systems, and new storytelling systems are stillroutinely published, indicating the continued interest in the topic across domains like child-computer interaction, learning technologies, and the broader HCI community. This paper firstpresents a detailed review of the different approaches that have been usedfor puppet-based storytelling system implementations, and then proposesaflexible low-cost approach to puppet-based storytelling system implementation that uses a combination of vision- and sensor-based tracking. We contribute a framework that will help the community to make sense of the myriad of puppet-based storytelling system implementation approaches in the literature, and discuss results from a perceptionstudy that evaluated the performance of the system output using our proposed implementation approach.
SP  - 676
EP  - 694
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77599-5_46
ER  - 

TY  - NA
AU  - Withana, Anusha; Groeger, Daniel; Steimle, Jürgen
TI  - UIST - Tacttoo: A Thin and Feel-Through Tattoo for On-Skin Tactile Output
PY  - 2018
AB  - This paper introduces Tacttoo, a feel-through interface for electro-tactile output on the user's skin. Integrated in a temporary tattoo with a thin and conformal form factor, it can be applied on complex body geometries, including the fingertip, and is scalable to various body locations. At less than 35µm in thickness, it is the thinnest tactile interface for wearable computing to date. Our results show that Tacttoo retains the natural tactile acuity similar to bare skin while delivering high-density tactile output. We present the fabrication of customized Tacttoo tattoos using DIY tools and contribute a mechanism for consistent electro-tactile operation on the skin. Moreover, we explore new interactive scenarios that are enabled by Tacttoo. Applications in tactile augmented reality and on-skin interaction benefit from a seamless augmentation of real-world tactile cues with computer-generated stimuli. Applications in virtual reality and private notifications benefit from high-density output in an ergonomic form factor. Results from two psychophysical studies and a technical evaluation demonstrate Tacttoo's functionality, feel-through properties and durability.
SP  - 365
EP  - 378
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242645
ER  - 

TY  - NA
AU  - Vechev, Velko; Zarate, Juan Jose; Lindlbauer, David; Hinchet, Ronan; Shea, Herbert; Hilliges, Otmar
TI  - VR - TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR
PY  - 2019
AB  - We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm3) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.
SP  - 312
EP  - 320
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797921
ER  - 

TY  - NA
AU  - Friske, Mikhaila; Wu, Shanel; Devendorf, Laura
TI  - CHI - AdaCAD: Crafting Software For Smart Textiles Design
PY  - 2019
AB  - Woven smart textiles are useful in creating flexible electronics because they integrate circuitry into the structure of the fabric itself. However, there do not yet exist tools that support the specific needs of smart textiles weavers. This paper describes the process and development of AdaCAD, an application for composing smart textile weave drafts. By augmenting traditional weaving drafts, AdaCAD allows weavers to design woven structures and circuitry in tandem and offers specific support for common smart textiles techniques. We describe these techniques, how our tool supports them alongside feedback from smart textiles weavers. We conclude with a reflection on smart textiles practice more broadly and suggest that the metaphor of coproduction can be fruitful in creating effective tools and envisioning future applications in this space.
SP  - 345
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300575
ER  - 

TY  - NA
AU  - Yamaoka, Junichi; Nozawa, Kazunori; Asada, Shion; Niiyama, Ryuma; Kawahara, Yoshihiro; Kakehi, Yasuaki
TI  - UIST (Adjunct Volume) - AccordionFab: Fabricating Inflatable 3D Objects by Laser Cutting and Welding Multi-Layered Sheets
PY  - 2018
AB  - In this paper, we propose a method to create 3D inflatable objects by laminating plastic layers. AccordionFab is a fabrication method in which the user can prototype multi-layered inflatable structures rapidly with a common laser cutter. Our key finding is that it is possible to selectively weld the two uppermost plastic sheets out of the stacked sheets by defocusing the laser and inserting the heat-resistant paper below the desired welding layer. As the contribution of our research, we investigated the optimal distance between the lens and the workpiece for cutting and welding and developed an attachment which supports welding process. Next, we developed a mechanism of changing the thickness and bending angle of multi-layered objects and created a simulation software. Using these techniques, the user can create various prototypes such as personal furniture that fits user's body and packing containers that fit the contents.
SP  - 160
EP  - 162
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3271636
ER  - 

TY  - NA
AU  - Chen, Victor; Xu, Xuhai; Li, Richard; Shi, Yuanchun; Patel, Shwetak N.; Wang, Yuntao
TI  - Conference on Designing Interactive Systems - Understanding the Design Space of Mouth Microgestures
PY  - 2021
AB  - As wearable devices move toward the face (i.e. smart earbuds, glasses), there is an increasing need to facilitate intuitive interactions with these devices. Current sensing techniques can already detect many mouth-based gestures; however, users’ preferences of these gestures are not fully understood. In this paper, we investigate the design space and usability of mouth-based microgestures. We first conducted brainstorming sessions (N=16) and compiled an extensive set of 86 user-defined gestures. Then, with an online survey (N=50), we assessed the physical and mental demand of our gesture set and identified a subset of 14 gestures that can be performed easily and naturally. Finally, we conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to various daily smartphone operations under a sitting and walking context. From these studies, we develop a taxonomy for mouth gestures, finalize a practical gesture set for common applications, and provide design guidelines for future mouth-based gesture interactions.
SP  - 1068
EP  - 1081
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462004
ER  - 

TY  - JOUR
AU  - Leung, Yuen-Shan; Kwok, Tsz-Ho; Li, Xiangjia; Yang, Yang; Wang, Charlie C. L.; Chen, Yong
TI  - Challenges and Status on Design and Computation for Emerging Additive Manufacturing Technologies
PY  - 2019
AB  - The revolution of additive manufacturing (AM) has led to many opportunities in fabricating complex and novel products. The increase of printable materials and the emergence of novel fabrication processes continuously expand the possibility of engineering systems in which product components are no longer limited to be single material, single scale, or single function. In fact, a paradigm shift is taking place in industry from geometry-centered usage to supporting functional demands. Consequently, engineers are expected to resolve a wide range of complex and difficult problems related to functional design. Although a higher degree of design freedom beyond geometry has been enabled by AM, there are only very few computational design approaches in this new AM-enabled domain to design objects with tailored properties and functions. The objectives of this review paper are to provide an overview of recent additive manufacturing developments and current computer-aided design methodologies that can be applied to multimaterial, multiscale, multiform, and multifunctional AM technologies. The difficulties encountered in the computational design approaches are summarized and the future development needs are emphasized. In the paper, some present applications and future trends related to additive manufacturing technologies are also discussed.
SP  - 021013
EP  - NA
JF  - Journal of Computing and Information Science in Engineering
VL  - 19
IS  - 2
PB  - 
DO  - 10.1115/1.4041913
ER  - 

TY  - JOUR
AU  - Chen, Tuochao; Li, Yaxuan; Tao, Songyun; Lim, Hyunchul; Sakashita, Mose; Zhang, Ruidong; Guimbretière, François; Zhang, Cheng
TI  - NeckFace: Continuously Tracking Full Facial Expressions on Neck-mounted Wearables
PY  - 2021
AB  - Facial expressions are highly informative for computers to understand and interpret a person's mental and physical activities. However, continuously tracking facial expressions, especially when the user is in motion, is challenging. This paper presents NeckFace, a wearable sensing technology that can continuously track the full facial expressions using a neck-piece embedded with infrared (IR) cameras. A customized deep learning pipeline called NeckNet based on Resnet34 is developed to learn the captured infrared (IR) images of the chin and face and output 52 parameters representing the facial expressions. We demonstrated NeckFace on two common neck-mounted form factors: a necklace and a neckband (e.g., neck-mounted headphones), which was evaluated in a user study with 13 participants. The study results showed that NeckFace worked well when the participants were sitting, walking, or after remounting the device. We discuss the challenges and opportunities of using NeckFace in real-world applications.
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463511
ER  - 

TY  - CHAP
AU  - Nasser, Arshad; Chen, Taizhou; Liu, Can; Zhu, Kening; Rao, P. V. M.
TI  - HCI (2) - FingerTalkie: Designing a Low-Cost Finger-Worn Device for Interactive Audio Labeling of Tactile Diagrams
PY  - 2020
AB  - Traditional tactile diagrams for the visually-impaired (VI) use short Braille keys and annotations to provide additional information in separate Braille legend pages. Frequent navigation between the tactile diagram and the annex pages during the diagram exploration results in low efficiency in diagram comprehension. We present the design of FingerTalkie, a finger-worn device that uses discrete colors on a color-tagged tactile diagram for interactive audio labeling of the graphical elements. Through an iterative design process involving 8 VI users, we designed a unique offset point-and-click technique that enables the bimanual exploration of the diagrams without hindering the tactile perception of the fingertips. Unlike existing camera-based and finger-worn audio-tactile devices, FingerTalkie supports one-finger interaction and can work in any lighting conditions without calibration. We conducted a controlled experiment with 12 blind-folded sighted users to evaluate the usability of the device. Further, a focus-group interview with 8 VI users shows their appreciation for the FingerTalkie’s ease of use, support for two-hand exploration, and its potential in improving the efficiency of comprehending tactile diagrams by replacing Braille labels.
SP  - 475
EP  - 496
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_32
ER  - 

TY  - NA
AU  - Xu, Songlin; Wu, Zhiyuan; Wang, Shunhong; Fan, Rui; Lin, Nan
TI  - UIST (Adjunct Volume) - Hydrauio: Extending Interaction Space on the Pen through Hydraulic Sensing and Haptic Output
PY  - 2020
AB  - We have explored a fluid-based interface(Hydrauio) on the pen body to extend interaction space of human-pen interaction. Users could perform finger gestures on the pen for input and also receive haptic feedback of different profiles from the fluid surface. The user studies showed that Hydrauio could achieve an accuracy of more than 92% for finger gesture recognition and users could distinguish different haptic profiles with an accuracy of more than 95%. Finally, we present application scenarios to demonstrate the potential of Hydrauio to extend interaction space of human-pen interaction.
SP  - 43
EP  - 45
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416180
ER  - 

TY  - NA
AU  - Shi, Lei; Zhao, Yuhang; Kupferstein, Elizabeth; Azenkot, Shiri
TI  - ASSETS - A Demonstration of Molder: An Accessible Design Tool for Tactile Maps
PY  - 2019
AB  - Tactile maps are important tools for people with visual impairments (VIs). Teachers and orientation and mobility (O&M) specialists often design tactile maps to help their VI students and clients learn about geographic areas. To design these maps, a designer must use modeling software applications, which require professional training and rely on visual feedback. However, most teachers and O&M specialists do not have professional modeling skills, and many have visual impairments. The complexity and inaccessibility of current modeling tools thus become major barriers for TVIs and O&M specialists when designing tactile maps. We present Molder, an accessible design tool for tactile maps. A designer creates a draft map model using Molder and prints the model. Then, she uses Molder to modify the draft model by directly interacting with it. Molder provides auditory feedback and high-contrast visuals to assist the designer in the design process.
SP  - 664
EP  - 666
JF  - The 21st International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3308561.3354594
ER  - 

TY  - JOUR
AU  - Joo, Young-Bok; Shin, Eun-Jae; Heo, Yong Hae; Park, Won-Hyeong; Yang, Tae-Heon; Kim, Sang-Youn
TI  - Development of an Electrostatic Beat Module for Various Tactile Sensations in Touch Screen Devices
PY  - 2019
AB  - One of the most dominant factors in developing tactile modules is the ability to generate abundant vibrotactile sensation. This paper presents a new vibrotactile module which can stimulate two mechanoreceptors at the same time without any mechanical vibration motors. To realize that, we first design an electro-tactile beat module (an ETB module) consisting of a lower part, a connection part and an upper part. The two electrodes were designed in an interdigitated pattern and were applied to the upper part. By applying two voltage inputs with slightly different frequencies to two electrodes in the proposed ETB module, respectively, we can create beat-patterned vibration. Furthermore, we can create normal vibration with the proposed ETB module by applying same frequency to the two electrodes. Experiments were conducted to validate the haptic performance of the proposed prototype. The results show that the proposed ETB module can create not only beat-patterned vibration but also normal vibration. The results also show that it can generate strong enough vibration to stimulate mechanoreceptors in wide frequency ranges.
SP  - 1229
EP  - NA
JF  - Applied Sciences
VL  - 9
IS  - 6
PB  - 
DO  - 10.3390/app9061229
ER  - 

TY  - NA
AU  - Dalsgaard, Tor-Salve; Knibbe, Jarrod; Bergström, Joanna
TI  - Modeling Pointing for 3D Target Selection in VR
PY  - 2021
AB  - Virtual reality (VR) allows users to interact similarly to how they do in the physical world, such as touching, moving, and pointing at objects. To select objects at a distance, most VR techniques rely on casting a ray through one or two points located on the user’s body (e.g., on the head and a finger), and placing a cursor on that ray. However, previous studies show that such rays do not help users achieve optimal pointing accuracy nor correspond to how they would naturally point. We seek to find features, which would best describe natural pointing at distant targets. We collect motion data from seven locations on the hand, arm, and body, while participants point at 27 targets across a virtual room. We evaluate the features of pointing and analyse sets of those for predicting pointing targets. Our analysis shows an 87% classification accuracy between the 27 targets for the best feature set and a mean distance of 23.56 cm in predicting pointing targets across the room. The feature sets can inform the design of more natural and effective VR pointing techniques for distant object selection.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489853
ER  - 

TY  - JOUR
AU  - Huang, Ting-Hao Kenneth; Azaria, Amos; Romero, Oscar J.; Bigham, Jeffrey P.
TI  - InstructableCrowd: Creating IF-THEN Rules for Smartphones via Conversations with the Crowd
PY  - 2019
AB  - Natural language interfaces have become a common part of modern digital life. Chatbots utilize text-based conversations to communicate with users; personal assistants on smartphones such as Google Assistant take direct speech commands from their users; and speech-controlled devices such as Amazon Echo use voice as their only input mode. In this paper, we introduce InstructableCrowd, a crowd-powered system that allows users to program their devices via conversation. The user verbally expresses a problem to the system, in which a group of crowd workers collectively respond and program relevant multi-part IF-THEN rules to help the user. The IF-THEN rules generated by InstructableCrowd connect relevant sensor combinations (e.g., location, weather, device acceleration, etc.) to useful effectors (e.g., text messages, device alarms, etc.). Our study showed that non-programmers can use the conversational interface of InstructableCrowd to create IF-THEN rules that have similar quality compared with the rules created manually. InstructableCrowd generally illustrates how users may converse with their devices, not only to trigger simple voice commands, but also to personalize their increasingly powerful and complicated devices.
SP  - 113
EP  - 146
JF  - Human Computation
VL  - 6
IS  - 1
PB  - 
DO  - 10.15346/hc.v6i1.7
ER  - 

TY  - NA
AU  - Shim, Youngbo Aram; Park, Keunwoo; Lee, Geehyuk
TI  - MobileHCI - Using Poke Stimuli to Improve a 3x3 Watch-back Tactile Display
PY  - 2019
AB  - A watch-back tactile display (WBTD) is an attractive output option due to its always-available nature. However, employing commonly-used vibration modality on a WBTD may result in a low efficiency since its stimulation area is relatively wide compared with the small contact area of a watch-back. We considered using a more localized tactile stimulus, a poke, to improve the efficiency of a WBTD. We built a WBTD consisting of overlapping 3×3 poke and vibrotactile tactor arrays so that it may be used either as a poke display or as a vibrotactile display. An experiment was conducted to optimize the parameters of poke stimuli, and its results revealed that four directional patterns were best recognized when poking depth was deepest (3 mm) and sensory saltation was exploited. In the next two experiments, we compared the information transfer capacities of the poke and vibrotactile displays. The information transfer capacity of the poke display (1.55 bits) was shown to be higher than that of the vibrotactile display (1.32 bits) in a simulated environment with the mental load of a primary task. This result confirmed our expectation that using a more localized tactile stimulus would improve the efficiency of a WBTD.
SP  - 23
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340134
ER  - 

TY  - NA
AU  - Schoop, Eldon; Huang, Forrest; Hartmann, Bjoern
TI  - CHI - UMLAUT: Debugging Deep Learning Programs using Program Structure and Model Behavior
PY  - 2021
AB  - Training deep neural networks can generate non-descriptive error messages or produce unusual output without any explicit errors at all. While experts rely on tacit knowledge to apply debugging strategies, non-experts lack the experience required to interpret model output and correct Deep Learning (DL) programs. In this work, we identify DL debugging heuristics and strategies used by experts, andIn this work, we categorize the types of errors novices run into when writing ML code, and map them onto opportunities where tools could help. We use them to guide the design of Umlaut. Umlaut checks DL program structure and model behavior against these heuristics; provides human-readable error messages to users; and annotates erroneous model output to facilitate error correction. Umlaut links code, model output, and tutorial-driven error messages in a single interface. We evaluated Umlaut in a study with 15 participants to determine its effectiveness in helping developers find and fix errors in their DL programs. Participants using Umlaut found and fixed significantly more bugs and were able to implement fixes for more bugs compared to a baseline condition.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445538
ER  - 

TY  - JOUR
AU  - Salemi Parizi, Farshid; Whitmire, Eric; Patel, Shwetak N.
TI  - AuraRing
PY  - 2022
AB  - <jats:p>Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. It has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.</jats:p>
SP  - 34
EP  - 37
JF  - GetMobile: Mobile Computing and Communications
VL  - 25
IS  - 3
PB  - 
DO  - 10.1145/3511285.3511295
ER  - 

TY  - NA
AU  - Dobinson, Rhett; Teyssier, Marc; Steimle, Jürgen; Fruchard, Bruno
TI  - MicroPress: Detecting Pressure and Hover Distance in Thumb-to-Finger Interactions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567698
ER  - 

TY  - NA
AU  - Gao, BoYu; Mai, Ziiun; Tu, Huawei; Duh, Henry Been-Lirn
TI  - VR - Evaluation of Body-centric Locomotion with Different Transfer Functions in Virtual Reality
PY  - 2021
AB  - Body-centric locomotion allows users to navigate virtual environments with body parts (e.g. head tilt, arm swing or torso lean). Transfer functions are an important determinant of the locus of such a locomotion method. However, there is little known about the effects of transfer functions on virtual locomotion with different body parts. In this work, we selected four typical transfer functions (linear function: L, power function: P, a piecewise function with constant and linear functions: CL, and a piecewise function with constant and power functions: CP) and four body parts (head, arm, torso, and knee) from existing works, and conducted an experiment to evaluate their effects on virtual locomotion under three distances (5, 10, and 15 m) in Virtual Reality (VR). Results show that (1) CP function generally led to the longest task time with a low rate of failed trials, while CL function had the shortest task time with a high rate of failed trials; (2) body parts significantly affected the rate of failed trials, but not task time and final position offset. Head and torso resulted in the lowest and highest rate of failed trials respectively; (3) body parts did not differ in User Experience Questionnaire-Short (UEQ-S), UEQ-S Pragmatic and UEQ-S Hedonic. L was rated as the highest score for UEQ-S, UEQ-S Pragmatic and UEQ-S Hedonic, but CP had the lowest score. According to the results, we provide implications of designing body-centric locomotion with different transfer functions in VR.
SP  - 493
EP  - 500
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00073
ER  - 

TY  - BOOK
AU  - Chen, Yan; Jones, Jasmine; Yao, Yaxing
TI  - CSCW Companion - WireOn: Supporting Remote Collaboration for Embedded System Development
PY  - 2020
AB  - The rise of the Maker movement has led to a growing number of developers who prototype and program embedded systems. When programming, these developers often rely on support from various resources-including other developers. However, other developers may not always be available to provide support in person, and existing technologies for online help, such as voice chat or Q&A forms, face the fundamental limitation of inspecting and manipulating developers' circuit boards. As a result, remote helpers can only provide suggestions or guidance, rather than contributing via physical changes made to the devices. And only end-user developers have the ability to carry out the planned tasks. In this paper, we demonstrate WireOn, a programming support research prototype that allows remote helpers to directly perform tasks on end-user developers' circuit board by teleoperating a robot arm. The helpers can control the robot arm via a web user interface to perform simple tasks such as pick-and-place the electronic components, visually inspect the physical artifacts in real time, and also review the code that the end-user sent over to them. The new system has the potential to enable more efficient remote collaboration on embedded system development. (https://youtu.be/uggyxHAlLDQ)
SP  - 7
EP  - 11
JF  - Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3406865.3418564
ER  - 

TY  - JOUR
AU  - Liu, Jingyang; Li, Yunzhi; Goel, Mayank
TI  - A semantic-based approach to digital content placement for immersive environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Visual Computer
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s00371-022-02707-8
ER  - 

TY  - NA
AU  - Xu, Xuhai; Yu, Chun; Dey, Anind K.; Mankoff, Jennifer
TI  - CHI - Clench Interface: Novel Biting Input Techniques
PY  - 2019
AB  - People eat every day and biting is one of the most fundamental and natural actions that they perform on a daily basis. Existing work has explored tooth click location and jaw movement as input techniques, however clenching has the potential to add control to this input channel. We propose clench interaction that leverages clenching as an actively controlled physiological signal that can facilitate interactions. We conducted a user study to investigate users' ability to control their clench force. We found that users can easily discriminate three force levels, and that they can quickly confirm actions by unclenching (quick release). We developed a design space for clench interaction based on the results and investigated the usability of the clench interface. Participants preferred the clench over baselines and indicated a willingness to use clench-based interactions. This novel technique can provide an additional input method in cases where users' eyes or hands are busy, augment immersive experiences such as virtual/augmented reality, and assist individuals with disabilities.
SP  - 275
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300505
ER  - 

TY  - NA
AU  - McGrath, William; Drew, Daniel S.; Warner, Jeremy; Kazemitabaar, Majeed; Karchemsky, Mitchell; Mellis, David A.; Hartmann, Björn
TI  - UIST - Bifröst: Visualizing and Checking Behavior of Embedded Systems across Hardware and Software
PY  - 2017
AB  - The Maker movement has encouraged more people to start working with electronics and embedded processors. A key challenge in developing and debugging custom embedded systems is understanding their behavior, particularly at the boundary between hardware and software. Existing tools such as step debuggers and logic analyzers only focus on software or hardware, respectively. This paper presents a new development environment designed to illuminate the boundary between embedded code and circuits. Bifrost automatically instruments and captures the progress of the user's code, variable values, and the electrical and bus activity occurring at the interface between the processor and the circuit it operates in. This data is displayed in a linked visualization that allows navigation through time and program execution, enabling comparisons between variables in code and signals in circuits. Automatic checks can detect low-level hardware configuration and protocol issues, while user-authored checks can test particular application semantics. In an exploratory study with ten participants, we investigated how Bifrost influences debugging workflows.
SP  - 299
EP  - 310
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126658
ER  - 

TY  - CHAP
AU  - , 
TI  - Summary
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544578
ER  - 

TY  - JOUR
AU  - Rykaczewski, Konrad
TI  - Thermophysiological aspects of wearable robotics: Challenges and opportunities
PY  - 2022
AB  - NA
SP  - 1
EP  - 13
JF  - Temperature
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/23328940.2022.2113725
ER  - 

TY  - JOUR
AU  - Terenti, Mihail; Vatavu, Radu-Daniel
TI  - VIREO: Web-based Graphical Authoring of Vibrotactile Feedback for Interactions with Mobile and Wearable Devices
PY  - 2022
AB  - NA
SP  - 1
EP  - 19
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2109584
ER  - 

TY  - NA
AU  - Lu, Qiuyu; Ou, Jifei; Wilbert, João; Haben, André; Mi, Haipeng; Ishii, Hiroshi
TI  - UIST - milliMorph -- Fluid-Driven Thin Film Shape-Change Materials for Interaction Design
PY  - 2019
AB  - This paper presents a design space, a fabrication system and applications of creating fluidic chambers and channels at millimeter scale for tangible actuated interfaces. The ability to design and fabricate millifluidic chambers allows one to create high frequency actuation, sequential control of flows and high resolution design on thin film materials. We propose a four dimensional design space of creating these fluidic chambers, a novel heat sealing system that enables easy and precise millifluidics fabrication, and application demonstrations of the fabricated materials for haptics, ambient devices and robotics. As shape-change materials are increasingly integrated in designing novel interfaces, milliMorph enriches the library of fluid-driven shape-change materials, and demonstrates new design opportunities that is unique at millimeter scale for product and interaction design.
SP  - 663
EP  - 672
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347956
ER  - 

TY  - NA
AU  - Maeda, Tomosuke; Kurahashi, Tetsuo
TI  - AH - TherModule: Wearable and Modular Thermal Feedback System based on a Wireless Platform
PY  - 2019
AB  - Humans have specific sensory organs and they can feel tactile sensation on the whole body. However, many haptic devices have limitations due to the location of the body part and might not provide natural haptic feedback. Thus, we propose a novel interface, TherModule, which is a wearable and modular thermal feedback system for embodied interactions based on a wireless platform. TherModule can be worn on multiple body parts such as the wrist, forearm, ankle, and neck. In this paper, we describe the system concept, module implementation, and applications. To demonstrate and explore the embodied interaction with thermal feedback, we implemented prototype applications, such as movie experiences, projector-based augmented reality, navigation, and notification based on a wireless platform, with TherModule on multiple parts of the body. The result of an experiment on movie experience showed that participants felt more interactions between temperature and visual stimulus.
SP  - 14
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311826
ER  - 

TY  - JOUR
AU  - Cornelio, Patricia; Velasco, Carlos; Obrist, Marianna
TI  - Multisensory Integration as per Technological Advances: A Review.
PY  - 2021
AB  - Multisensory integration research has allowed us to better understand how humans integrate sensory information to produce a unitary experience of the external world. However, this field is often challenged by the limited ability to deliver and control sensory stimuli, especially when going beyond audio-visual events and outside laboratory settings. In this review, we examine the scope and challenges of new technology in the study of multisensory integration in a world that is increasingly characterized as a fusion of physical and digital/virtual events. We discuss multisensory integration research through the lens of novel multisensory technologies and, thus, bring research in human-computer interaction, experimental psychology, and neuroscience closer together. Today, for instance, displays have become volumetric so that visual content is no longer limited to 2D screens, new haptic devices enable tactile stimulation without physical contact, olfactory interfaces provide users with smells precisely synchronized with events in virtual environments, and novel gustatory interfaces enable taste perception through levitating stimuli. These technological advances offer new ways to control and deliver sensory stimulation for multisensory integration research beyond traditional laboratory settings and open up new experimentations in naturally occurring events in everyday life experiences. Our review then summarizes these multisensory technologies and discusses initial insights to introduce a bridge between the disciplines in order to advance the study of multisensory integration.
SP  - 652611
EP  - 652611
JF  - Frontiers in neuroscience
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnins.2021.652611
ER  - 

TY  - CHAP
AU  - Minagawa, Tatsuya; Suzuki, Ippei; Ochiai, Yoichi
TI  - HCI (38) - Development of a Telepresence System Using a Robot Controlled by Mobile Devices
PY  - 2021
AB  - This study proposes a telepresence system controlled by mobile devices consisting of a conference side and a remote side. The conference side of this system consists of a puppet-type robot that enhances the co-telepresence. The remote side includes a web application that can be accessed by a mobile device that can operate the robot by using a motion sensor. The effectiveness of the robot-based telepresence techniques in the teleconference applications is analyzed by conducting user surveys of the participants in remote and real-world situations. It is observed from the experimental results that the proposed telepresence system enhances the coexistence of remote participants and allows them to attend the conference enjoyably.
SP  - 172
EP  - 180
JF  - HCI International 2021 - Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78642-7_23
ER  - 

TY  - NA
AU  - Huang, Ting-Hao; Chang, Joseph Chee; Bigham, Jeffrey P.
TI  - Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time
PY  - 2018
AB  - Crowd-powered conversational assistants have been shown to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. A promising direction is to combine the two approaches for high quality, low latency, and low cost solutions. In this paper, we introduce Evorus, a crowd-powered conversational assistant built to automate itself over time by (i) allowing new chatbots to be easily integrated to automate more scenarios, (ii) reusing prior crowd answers, and (iii) learning to automatically approve response candidates. Our 5-month-long deployment with 80 participants and 281 conversations shows that Evorus can automate itself without compromising conversation quality. Crowd-AI architectures have long been proposed as a way to reduce cost and latency for crowd-powered systems; Evorus demonstrates how automation can be introduced successfully in a deployed system. Its architecture allows future researchers to make further innovation on the underlying automated components in the context of a deployed open domain dialog system.
SP  - 295
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173869
ER  - 

TY  - NA
AU  - Wei, Tzu-Yun; Tsai, Hsin-Ruey; Liao, Yu-So; Tsai, Chieh; Chen, Yi-Shan; Wang, Chi; Chen, Bing-Yu
TI  - UIST - ElastiLinks: Force Feedback between VR Controllers with Dynamic Points of Application of Force
PY  - 2020
AB  - Force feedback is commonly used to enhance realism in virtual reality (VR). However, current works mainly focus on providing different force types or patterns, but do not investigate how a proper point of application of force (PAF), which means where the resultant force is applied to, affects users' experience. For example, users perceive resistive force without torque when pulling a virtual bow, but with torque when pulling a virtual slingshot. Therefore, we propose a set of handheld controllers, ElastiLinks, to provide force feedback between controllers with dynamic PAFs.A rotatable track on each controller provides a dynamic PAF, and two common types of force feedback, resistive force and impact, are produced by two links, respectively. We performed a force perception study to ascertain users' resistive and impact force level distinguishability between controllers. Based on the results, we conducted another perception study to understand users' distinguishability of PAF offset and rotation differences. Finally, we performed a VR experience study to prove that force feedback with dynamic PAFs enhances VR experience.
SP  - 1023
EP  - 1034
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415836
ER  - 

TY  - NA
AU  - Marciniak, Zofia; Oh, Seo Young; Yoon, Sang Ho
TI  - Guide Ring: Bidirectional Finger-worn Haptic Actuator for Rich Haptic Feedback
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565626
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Chang, Yuan-Chia; Wei, Tzu-Yun; Tsao, Chih-An; Koo, Xander Chin-yuan; Wang, Hao-Chuan; Chen, Bing-Yu
TI  - CHI - GuideBand: Intuitive 3D Multilevel Force Guidance on a Wristband in Virtual Reality
PY  - 2021
AB  - For haptic guidance, vibrotactile feedback is a commonly-used mechanism, but requires users to interpret its complicated patterns especially in 3D guidance, which is not intuitive and increases their mental effort. Furthermore, for haptic guidance in virtual reality (VR), not only guidance performance but also realism should be considered. Since vibrotactile feedback interferes with and reduces VR realism, it may not be proper for VR haptic guidance. Therefore, we propose a wearable device, GuideBand, to provide intuitive 3D multilevel force guidance upon the forearm, which reproduces an effect that the forearm is pulled and guided by a virtual guider or telepresent person in VR. GuideBand uses three motors to pull a wristband at different force levels in 3D space. Such feedback usually requires much larger and heavier robotic arms or exoskeletons. We conducted a just-noticeable difference study to understand users’ force level distinguishability. Based on the results, we performed a study to verify that compared with state-of-the-art vibrotactile guidance, GuideBand is more intuitive, needs a lower level of mental effort, and achieves similar guidance performance. We further conducted a VR experience study to observe how users combine and complement visual and force guidance, and prove that GuideBand enhances realism in VR guidance.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445262
ER  - 

TY  - NA
AU  - Gupta, Aakar; Samad, Majed; Kin, Kenrick; Kristensson, Per Ola; Benko, Hrvoje
TI  - ISMAR - Investigating Remote Tactile Feedback for Mid-Air Text-Entry in Virtual Reality
PY  - 2020
AB  - In this paper, we investigate the utility of remote tactile feedback for freehand text-entry on a mid-air Qwerty keyboard in VR. To that end, we use insights from prior work to design a virtual keyboard along with different forms of tactile feedback, both spatial and non-spatial, for fingers and for wrists. We report on a multi-session text-entry study with 24 participants where we investigated four vibrotactile feedback conditions: on-fingers, on-wrist spatialized, on-wrist non-spatialized, and audio-visual only. We use micro-metrics analyses and participant interviews to analyze the mechanisms underpinning the observed performance and user experience. The results show comparable performance across feedback types. However, participants overwhelmingly prefer the tactile feedback conditions and rate on-fingers feedback as significantly lower in mental demand, frustration, and effort. Results also show that spatialization of vibrotactile feedback on the wrist as a way to provide finger-specific feedback is comparable in performance and preference to a single vibration location. The micro-metrics analyses suggest that users compensated for the lack of tactile feedback with higher visual and cognitive attention, which ensured similar performance but higher user effort.
SP  - 350
EP  - 360
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00062
ER  - 

TY  - NA
AU  - Schoop, Eldon; Huang, Forrest; Hartmann, Björn
TI  - CHI Extended Abstracts - SCRAM: Simple Checks for Realtime Analysis of Model Training for Non-Expert ML Programmers
PY  - 2020
AB  - Many non-expert Machine Learning users wish to apply powerful deep learning models to their own domains but encounter hurdles in the opaque model tuning process. We introduce SCRAM, a tool which uses heuristics to detect potential error conditions in model output and suggests actionable steps and best practices to help such users tune their models. Inspired by metaphors from software engineering, SCRAM extends high-level deep learning development tools to interpret model metrics during training and produce human-readable error messages. We validate SCRAM through three author-created example scenarios with image and text datasets, and by collecting informal feedback from ML researchers with teaching experience. We finally reflect upon our feedback for the design of future ML debugging tools.
SP  - 1
EP  - 10
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382879
ER  - 

TY  - NA
AU  - Brasier, Eugenie; Chapuis, Olivier; Férey, Nicolas; Vezien, Jeanne; Appert, Caroline
TI  - ISMAR - ARPads: Mid-air Indirect Input for Augmented Reality
PY  - 2020
AB  - Interacting efficiently and comfortably with Augmented Reality (AR) headsets remains a major issue. We investigate the concept of mid-air pads as an alternative to gaze or direct hand input to control a cursor in windows anchored in the environment. ARPads allow users to control the cursor displayed in the headset screen through movements on a mid-air plane, which is not spatially aligned with the headset screen. We investigate a design space for ARPads, which takes into account the position of the pad relative to the user’s body, and the orientation of the pad relative to that of the headset screen. Our study suggests that 1) indirect input can achieve the same performance as direct input while causing less fatigue than hand raycast, 2) an ARPad should be attached to the wrist or waist rather than to the thigh, and 3) the ARPad and the screen should have the same orientation.
SP  - 332
EP  - 343
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00060
ER  - 

TY  - NA
AU  - Brulé, Emeline; Bailly, Gilles
TI  - CHI - ”Beyond 3D printers”: Understanding Long-Term Digital Fabrication Practices for the Education of Visually Impaired or Blind Youth
PY  - 2021
AB  - Disability professionals could use digital fabrication tools to provide customised assistive technologies or accessible media beneficial to the education of Blind or visually impaired youth. However, there is little documentation of long-term practices with these tools by professionals in this field, limiting our ability to support their work. We report on such practices in a French organisation, providing disability educational services and using digital fabrication since 2013, for six years. We trace how professionals defined how digital fabrication could and should be used through a range of projects, based on pedagogical uses and the constraints in creation, production and maintenance. We outline new research perspectives going beyond 3D printers and its promises of automation to embrace hybrid approaches currently supported by laser cutters, the learning and documentation process, and the production of accessible tactile media at a regional or national scale.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445403
ER  - 

TY  - NA
AU  - Romat, Hugo; Fender, Andreas; Meier, Manuel; Holz, Christian
TI  - VR - Flashpen: A High-Fidelity and High-Precision Multi-Surface Pen for Virtual Reality
PY  - 2021
AB  - Digital pen interaction has become a first-class input modality for precision tasks such as writing, annotating, drawing, and 2D manipulation. The key enablers of digital inking are the capacitive or resistive sensors that are integrated in contemporary tablet devices. In Virtual Reality (VR), however, users typically provide input across large regions, hence limiting the suitability of using additional tablet devices for accurate pen input. In this paper, we present Flashpen, a digital pen for VR whose sensing principle affords accurately digitizing hand writing and intricate drawing, including small and quick turns. Flashpen re-purposes an inexpensive gaming mouse sensor that digitizes extremely fine grained motions in the micrometer range at over 8 kHz when moving on a surface. We combine Flashpen's high-fidelity relative input with the absolute tracking cues from a VR headset to enable pen interaction across a variety of VR applications. In our two-block evaluation, which consists of a tracing task and a writing task, we compare Flashpen to a professional drawing tablet (Wacom). With this, we demonstrate that Flashpen's fidelity matches the performance of state-of-the-art digitizers and approaches the fidelity of analog pens, while adding the flexibility of supporting a wide range of flat surfaces.
SP  - 306
EP  - 315
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00053
ER  - 

TY  - CHAP
AU  - Thévin, Lauren; Brock, Anke
TI  - ICCHP (2) - Augmented Reality for People with Visual Impairments: Designing and Creating Audio-Tactile Content from Existing Objects
PY  - 2018
AB  - Tactile maps and diagrams are widely used as accessible graphical media for people with visual impairments, in particular in the context of education. They can be made interactive by augmenting them with audio feedback. It is however complicated to create audio-tactile graphics that have rich and realistic tactile textures. To overcome these limitations, we propose a new augmented reality approach allowing novices to easily and quickly augment real objects with audio feedback. In our user study, six teachers created their own audio-augmentation of objects, such as a botanical atlas, within 30 min or less. Teachers found the tool easy to use and were confident about re-using it. The resulting augmented objects allow two modes: exploration mode provides feedback on demand about an element, while quiz mode provides questions and answers. We evaluated the resulting audio-tactile material with five visually impaired children. Participants found the resulting interactive graphics exciting to use independently of their mental imagery skills.
SP  - 193
EP  - 200
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - 10897
PB  - 
DO  - 10.1007/978-3-319-94274-2_26
ER  - 

TY  - NA
AU  - Byun, Jung-Hyun; Ro, Hyocheol; Han, Tack-Don
TI  - IUI Companion - FRISP: Framework for Registering Interactive Spatial Projection
PY  - 2020
AB  - Projection-based augmented reality (AR) is a promising medium for realizing pervasive computing environment, and yet the problem of determining projection-suitable regions and where to project remains. To tackle this problem, we introduce FRISP, a projection-based augmented reality (AR) Framework designed for Registering Interactive Spatial Projection. The FRISP framework utilizes a pantilt projection-camera (pro-cam) system for capturing geometry and projection mapping. The framework scans and analyzes the geometric properties of a room, in order to determine projection-suitable regions and generate multi-window layouts. Once the multi-windows are registered to the real world, they can be interacted with by the users. The users can assign various widgets or applications to the multi-windows, which are then finally augmented onto the real world and can serve as a base units for realizing the pervasive AR environment.
SP  - 93
EP  - 94
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379336.3381458
ER  - 

TY  - NA
AU  - Alghofaili, Rawan; Sawahata, Yasuhito; Huang, Haikun; Wang, Hsueh-Cheng; Shiratori, Takaaki; Yu, Lap-Fai
TI  - CHI - Lost in Style: Gaze-driven Adaptive Aid for VR Navigation
PY  - 2019
AB  - A key challenge for virtual reality level designers is striking a balance between maintaining the immersiveness of VR and providing users with on-screen aids after designing a virtual experience. These aids are often necessary for wayfinding in virtual environments with complex paths. We introduce a novel adaptive aid that maintains the effectiveness of traditional aids, while equipping designers and users with the controls of how often help is displayed. Our adaptive aid uses gaze patterns in predicting user's need for navigation aid in VR and displays mini-maps or arrows accordingly. Using a dataset of gaze angle sequences of users navigating a VR environment and markers of when users requested aid, we trained an LSTM to classify user's gaze sequences as needing navigation help and display an aid. We validated the efficacy of the adaptive aid for wayfinding compared to other commonly-used wayfinding aids.
SP  - 348
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300578
ER  - 

TY  - JOUR
AU  - Buonamici, Francesco; Furferi, Rocco; Governi, Lapo; Lazzeri, Simone; McGreevy, Kathleen S.; Servi, Michaela; Talanti, Emiliano; Uccheddu, Francesca; Volpe, Yary
TI  - A practical methodology for computer-aided design of custom 3D printable casts for wrist fractures
PY  - 2019
AB  - In recent years, breakthroughs in the fields of reverse engineering and additive manufacturing techniques have led to the development of innovative solutions for personalized medicine. 3D technologies are quickly becoming a new treatment concept that hinges on the ability to shape patient-specific devices. Among the wide spectrum of medical applications, the orthopaedic sector is experiencing the most benefits. Several studies proposed modelling procedures for patient-specific 3D-printed casts for wrist orthoses, for example. Unfortunately, the proposed approaches are not ready to be used directly in clinical practice since the design of these devices requires significant interaction among medical staff, reverse engineering experts, additive manufacturing specialists and CAD designers. This paper proposes a new practical methodology to produce 3D printable casts for wrist immobilization with the aim of overcoming these drawbacks. In particular, the idea is to realize an exhaustive system that can be used within a paediatric environment. It should provide both a fast and accurate dedicated scanning of the hand-wrist-arm district, along with a series of easy-to-use semi-automatic tools for the modelling of the medical device. The system was designed to be used directly by the clinical staff after a brief training. It was tested on a set of five case studies with the aim of proving its general reliability and identifying possible major flaws. Casts obtained using the proposed system were manufactured using a commercial 3D printer, and the device’s compliance with medical requirements was tested. Results showed that the designed casts were correctly generated by the medical staff without the need of involving engineers. Moreover, positive feedback was provided by the users involved in the experiment.
SP  - 375
EP  - 390
JF  - The Visual Computer
VL  - 36
IS  - 2
PB  - 
DO  - 10.1007/s00371-018-01624-z
ER  - 

TY  - NA
AU  - Li, Jingyi; Kim, Son; Miele, Joshua A.; Agrawala, Maneesh; Follmer, Sean
TI  - CHI - Editing Spatial Layouts through Tactile Templates for People with Visual Impairments
PY  - 2019
AB  - Spatial layout is a key component in graphic design. While people who are blind or visually impaired (BVI) can use screen readers or magnifiers to access digital content, these tools fail to fully communicate the content's graphic design information. Through semi-structured interviews and contextual inquiries, we identify the lack of this information and feedback as major challenges in understanding and editing layouts. Guided by these insights and a co-design process with a blind hobbyist web developer, we developed an interactive, multimodal authoring tool that lets blind people understand spatial relationships between elements and modify layout templates. Our tool automatically generates tactile print-outs of a web page's layout, which users overlay on top of a tablet that runs our self-voicing digital design tool. We conclude with design considerations grounded in user feedback for improving the accessibility of spatially encoded information and developing tools for BVI authors.
SP  - 206
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300436
ER  - 

TY  - NA
AU  - Han, Teng; Bansal, Shubhi; Shi, Xiaochen; Chen, Yanjun; Quan, Baogang; Tian, Feng; Wang, Hongan; Subramanian, Sriram
TI  - CHI - HapBead: On-Skin Microfluidic Haptic Interface using Tunable Bead
PY  - 2020
AB  - On-skin haptic interfaces using soft elastomers which are thin and flexible have significantly improved in recent years. Many are focused on vibrotactile feedback that requires complicated parameter tuning. Another approach is based on mechanical forces created via piezoelectric devices and other methods for non-vibratory haptic sensations like stretching, twisting. These are often bulky with electronic components and associated drivers are complicated with limited control of timing and precision. This paper proposes HapBead, a new on-skin haptic interface that is capable of rendering vibration like tactile feedback using microfluidics. HapBead leverages a microfluidic channel to precisely and agilely oscillate a small bead via liquid flow, which then generates various motion patterns in channel that creates highly tunable haptic sensations on skin. We developed a proof-of-concept design to implement thin, flexible and easily affordable HapBead platform, and verified its haptic rendering capabilities via attaching it to users' fingertips. A study was carried out and confirmed that participants could accurately tell six different haptic patterns rendered by HapBead. HapBead enables new wearable display applications with multiple integrated functionalities such as on-skin haptic doodles, visuo-haptic displays and haptic illusions.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376190
ER  - 

TY  - CHAP
AU  - Guerrero, Alfrancis; Pietrzak, Thomas; Girouard, Audrey
TI  - INTERACT (4) - HyperBrush: Exploring the Influence of Flexural Stiffness on the Performance and Preference for Bendable Stylus Interfaces
PY  - 2021
AB  - Flexible sensing styluses deliver additional degrees of input for pen-based interaction, yet no research has looked into the integration with creative digital applications as well as the influence of flexural stiffness. We present HyperBrush, a modular flexible stylus with interchangeable flexible components for digital drawing applications. We compare our HyperBrush to rigid pressure styluses in three studies, for brushstroke manipulation, for menu selection and for creative digital drawing tasks. HyperBrush yields comparable results with a commercial pressure pen. We concluded that different flexibilities could pose their own unique advantages analogous to an artist’s assortment of paintbrushes.
SP  - 51
EP  - 71
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85610-6_4
ER  - 

TY  - JOUR
AU  - Leo, Fabrizio; Ferrari, Elisabetta; Baccelliere, Caterina; Zarate, Juan Jose; Shea, Herbert; Cocchi, Elena; Waszkielewicz, Aleksander; Brayda, Luca
TI  - Enhancing general spatial skills of young visually impaired people with a programmable distance discrimination training: a case control study
PY  - 2019
AB  - The estimation of relative distance is a perceptual task used extensively in everyday life. This important skill suffers from biases that may be more pronounced when estimation is based on haptics. This is especially true for the blind and visually impaired, for which haptic estimation of distances is paramount but not systematically trained. We investigated whether a programmable tactile display, used autonomously, can improve distance discrimination ability in blind and severely visually impaired youngsters between 7 and 22 years-old. Training consisted of four weekly sessions in which participants were asked to haptically find, on the programmable tactile display, the pairs of squares which were separated by the shortest and longest distance in tactile images with multiple squares. A battery of haptic tests with raised-line drawings was administered before and after training, and scores were compared to those of a control group that did only the haptic battery, without doing the distance discrimination training on the tactile display. Both blind and severely impaired youngsters became more accurate and faster at the task during training. In haptic battery results, blind and severely impaired youngsters who used the programmable display improved in three and two tests, respectively. In contrast, in the control groups, the blind control group improved in only one test, and the severely visually impaired in no tests. Distance discrimination skills can be trained equally well in both blind and severely impaired participants. More importantly, autonomous training with the programmable tactile display had generalized effects beyond the trained task. Participants improved not only in the size discrimination test but also in memory span tests. Our study shows that tactile stimulation training that requires minimal human assistance can effectively improve generic spatial skills.
SP  - 108
EP  - NA
JF  - Journal of neuroengineering and rehabilitation
VL  - 16
IS  - 1
PB  - 
DO  - 10.1186/s12984-019-0580-2
ER  - 

TY  - JOUR
AU  - Zhu, Kening; Perrault, Simon T.; Chen, Taizhou; Cai, Shaoyu; Peiris, Roshan Lalintha
TI  - A sense of ice and fire: Exploring thermal feedback with multiple thermoelectric-cooling elements on a smart ring
PY  - 2019
AB  - NA
SP  - 234
EP  - 247
JF  - International Journal of Human-Computer Studies
VL  - 130
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.07.003
ER  - 

TY  - NA
AU  - Weigel, Martin; Schön, Oliver; Janssen, Herbert
TI  - UbiComp/ISWC Adjunct - Evaluation of body-worn FPCBs with bluetooth low energy, capacitive touch, and resistive flex sensing
PY  - 2020
AB  - Commercially available flexible printed circuit boards (FPCBs) have the potential to embed electronics, connectivity, and interactivity into the same surface. This makes them an ideal platform for untethered and interactive wearable devices. However, we lack an understanding how well FPCB-based antennas and sensors perform when worn directly on the body. This work contributes an understanding by studying body-worn FPCBs in three technical evaluations: First, we study the integration of Bluetooth Low Energy and compare the signal strength of our body-worn FPCB with a rigid BLE developer board. Second, we study the accuracy of capacitive touch sensing with two electrode sizes. Finally, we develop a resistive flex sensor based on commercially available FPCB materials and compare its accuracy with a state-of-the-art flex sensor. Taken together, our results demonstrate a high usability of FPCB-based wearable devices.
SP  - 147
EP  - 150
JF  - Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410530.3414380
ER  - 

TY  - JOUR
AU  - Kortbeek, Vito; Bakar, Abu; Cruz, Stefany; Yildirim, Kasim; Pawelczak, Przemyslaw; Hester, Josiah
TI  - BFree: Enabling Battery-free Sensor Prototyping with Python
PY  - 2020
AB  - Building and programming tiny battery-free energy harvesting embedded computer systems is hard for the average maker because of the lack of tools, hard to comprehend programming models, and frequent power failures. With the high ecologic cost of equipping the next trillion embedded devices with batteries, it is critical to equip the makers, hobbyists, and novice embedded systems programmers with easy-to-use tools supporting battery-free energy harvesting application development. This way, makers can create untethered embedded systems that are not plugged into the wall, the desktop, or even a battery, providing numerous new applications and allowing for a more sustainable vision of ubiquitous computing. In this paper, we present BFree, a system that makes it possible for makers, hobbyists, and novice embedded programmers to develop battery-free applications using Python programming language and widely available hobbyist maker platforms. BFree provides energy harvesting hardware and a power failure resilient version of Python, with durable libraries that enable common coding practice and off the shelf sensors. We develop demonstration applications, benchmark BFree against battery-powered approaches, and evaluate our system in a user study. This work enables makers to engage with a future of ubiquitous computing that is useful, long-term, and environmentally responsible.
SP  - 3432191
EP  - 39
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432191
ER  - 

TY  - NA
AU  - Peng, Tyler; Pochettino, Mora; Mueller, Stefanie
TI  - CircuitAssist: Automatically Dispensing Electronic Components to Facilitate Circuit Building
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558671
ER  - 

TY  - NA
AU  - Liu, Jingyang
TI  - Intelligent Environments - Semantic Mapping: A Semantics-based Approach to Virtual Content Placement for Immersive Environments
PY  - 2021
AB  - Semantic Mapping is a semantics-based interactive system that enables intuitive virtual content placement for projection mapping in intelligent environments. Our semantic mapping system embeds semantic information of the environment to provide a user with an easy way to control and place projected virtual items in the physical world. In contrast to traditional projection mapping that involves manual adjustments, this semantic mapping system enables efficient manipulation of virtual content through inputs from users via speech or text. To build the system, we first use a commercial depth camera for scene reconstruction and an end-to-end deep learning framework for semantic segmentation at the instance level. We illustrate the system by developing a prototype for a set of proof-of-concept, room-scale applications. The accuracy study and user study results show that the system can provide users with accurate semantic information for effective virtual content placement.
SP  - 1
EP  - 8
JF  - 2021 17th International Conference on Intelligent Environments (IE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ie51775.2021.9486580
ER  - 

TY  - JOUR
AU  - Reales, Reyder; Zambrano, Daniel; García, Jesús M.
TI  - Diseño de la estructura mecánica perteneciente al animatrónico del dinosaurio Laquintasaura: Mechanical structure design pertaining to Laquintasaura animatronic dinosaur
PY  - 2019
AB  - <jats:p>Este artículo describe elproyecto dediseño de unanimatrónico del dinosaurio venezolano llamado Laquintasaura, el cual se desarrollóa partir de un concepto seleccionado tomando en cuenta las especificaciones objetivo derivadas delas necesidades del cliente.Cuenta con 9 grados de libertad accionados mediante 8 mecanismos, los cuales le permitenrealizar los movimientos correspondientes a la apertura de la boca, ascenso y giro de la cabeza,movimientos del cuello, ascenso y descenso del torso, giro de los brazos y movimiento de la cola. Para realizar el diseño del animatrónico se dimensionó laestructura, luego se realizaron una serie de simulaciones dinámicas y cinemáticas para evaluar los movimientos de los mecanismos y se seleccionaron los actuadores que ejecutarían los movimientos. Adicionalmente, seaplicó un análisis de esfuerzos mediante el Método de los Elementos Finitos a varios elementos mecánicos para obtener una estructura robusta que cumpla con los requerimientos solicitados</jats:p>
SP  - 43
EP  - 55
JF  - Revista UIS Ingenierías
VL  - 18
IS  - 4
PB  - 
DO  - 10.18273/revuin.v18n4-2019004
ER  - 

TY  - NA
AU  - Wentzel, Johann; Junuzovic, Sasa; Devine, James; Porter, John; Mott, Martez
TI  - Understanding How People with Limited Mobility Use Multi-Modal Input
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517458
ER  - 

TY  - NA
AU  - Reinders, Samuel; Butler, Matthew; Marriott, Kim
TI  - "Hey Model!" -- Natural User Interactions and Agency in Accessible Interactive 3D Models
PY  - 2020
AB  - While developments in 3D printing have opened up opportunities for improved access to graphical information for people who are blind or have low vision (BLV), they can provide only limited detailed and contextual information. Interactive 3D printed models (I3Ms) that provide audio labels and/or a conversational agent interface potentially overcome this limitation. We conducted a Wizard-of-Oz exploratory study to uncover the multi-modal interaction techniques that BLV people would like to use when exploring I3Ms, and investigated their attitudes towards different levels of model agency. These findings informed the creation of an I3M prototype of the solar system. A second user study with this model revealed a hierarchy of interaction, with BLV users preferring tactile exploration, followed by touch gestures to trigger audio labels, and then natural language to fill in knowledge gaps and confirm understanding.
SP  - 18
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376145
ER  - 

TY  - JOUR
AU  - Lambrichts, Mannu; Ramakers, Raf; Hodges, Steve; Coppers, Sven; Devine, James
TI  - A Survey and Taxonomy of Electronics Toolkits for Interactive and Ubiquitous Device Prototyping
PY  - 2021
AB  - Over the past two decades, many toolkits for prototyping interactive and ubiquitous electronic devices have been developed. Although their technical specifications are often easy to look up, they vary greatly in terms of design, features and target audience, resulting in very real strengths and weaknesses depending on the intended application. These less technical characteristics are often reported inconsistently, if at all. In this paper we provide a comprehensive survey of interactive and ubiquitous device prototyping toolkits, systematically analysing their characteristics within the framework of a new taxonomy that we present. In addition to the specific characteristics we cover, we introduce a way to evaluate toolkits more holistically, covering user needs such as 'ease of construction' and 'ease of moving from prototype to product' rather than features. We also present results from an online survey which offers new insights on how the surveyed users prioritize these characteristics during prototyping, and what techniques they use to move beyond prototyping. We hope our analysis will be valuable for others in the community who need to build and potentially scale out prototypes as part of their research. We end by identifying gaps that have not yet been addressed by existing offerings and discuss opportunities for future research into electronics prototyping toolkits.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463523
ER  - 

TY  - NA
AU  - Chernyshov, George; Tag, Benjamin; Caremel, Cedric; Cao, Feier; Liu, Gemma; Kunze, Kai
TI  - Shape memory alloy wire actuators for soft, wearable haptic devices
PY  - 2018
AB  - This paper presents a new approach to implement wearable haptic devices using Shape Memory Alloy (SMA) wires. The proposed concept allows building silent, soft, flexible and lightweight wearable devices, capable of producing the sense of pressure on the skin without any bulky mechanical actuators. We explore possible design considerations and applications for such devices, present user studies proving the feasibility of delivering meaningful information and use nonlinear autoregressive neural networks to compensate for SMA inherent drawbacks, such as delayed onset, enabling us to characterize and predict the physical behavior of the device.
SP  - 112
EP  - 119
JF  - Proceedings of the 2018 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267242.3267257
ER  - 

TY  - NA
AU  - Muhathir, ; Al-Khowarizmi, 
TI  - Measuring the Accuracy of SVM with Varying Kernel Function for Classification of Indonesian Wayang on Images
PY  - 2020
AB  - Support vector machine (SVM) is a method that is often used in various studies to do pattern recognition of objects in the form of images. SVM is also a classification technique which is a characteristic of carrying out the training process and testing process. In classifying with data in the form of images, SVM is assisted by a feature extract technique where the process normalizes data so that a good training process can be carried out. however, SVM generally uses a linear kernel function in the testing phase. So that there is interest in this paper by designing a thought to make comparisons with several other kernel function techniques such as the cubic kernel function and the quadratic kernel function in classifying the Indonesian Wayang images which are the legacy of Indonesian ancestors. The results of this paper by varying the kernel function in SVM have an accuracy of the cubic kernel function of 83.4%.
SP  - NA
EP  - NA
JF  - 2020 International Conference on Decision Aid Sciences and Application (DASA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/dasa51403.2020.9317197
ER  - 

TY  - NA
AU  - Prakash, Jay; Yang, Zhijian; Wei, Yu-Lin; Hassanieh, Haitham; Choudhury, Romit Roy
TI  - MobiCom - EarSense: earphones as a teeth activity sensor
PY  - 2020
AB  - This paper finds that actions of the teeth, namely tapping and sliding, produce vibrations in the jaw and skull. These vibrations are strong enough to propagate to the edge of the face and produce vibratory signals at an earphone. By re-tasking the earphone speaker as an input transducer - a software modification in the sound card - we are able to sense teeth-related gestures across various models of ear/headphones. In fact, by analyzing the signals at the two earphones, we show the feasibility of also localizing teeth gestures, resulting in a human-to-machine interface. Challenges range from coping with weak signals, distortions due to different teeth compositions, lack of timing resolution, spectral dispersion, etc. We address these problems with a sequence of sensing techniques, resulting in the ability to detect 6 distinct gestures in real-time. Results from 18 volunteers exhibit robustness, even though our system - EarSense - does not depend on per-user training. Importantly, EarSense also remains robust in the presence of concurrent user activities, like walking, nodding, cooking and cycling. Our ongoing work is focused on detecting teeth gestures even while music is being played in the earphone; once that problem is solved, we believe EarSense could be even more compelling.
SP  - NA
EP  - NA
JF  - Proceedings of the 26th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3372224.3419197
ER  - 

TY  - JOUR
AU  - BermejoCarlos, ; HuiPan, 
TI  - A Survey on Haptic Technologies for Mobile Augmented Reality
PY  - 2021
AB  - Augmented reality (AR) applications have gained much research and industry attention. Moreover, the mobile counterpart—mobile augmented reality (MAR) is one of the most explosive growth areas for A...
SP  - 1
EP  - 35
JF  - ACM Computing Surveys
VL  - 54
IS  - 9
PB  - 
DO  - 10.1145/3465396
ER  - 

TY  - NA
AU  - Matthies, Denys J.C.; Weerasinghe, Chamod; Urban, Bodo; Nanayakkara, Suranga
TI  - AHs - CapGlasses: Untethered Capacitive Sensing with Smart Glasses
PY  - 2021
AB  - Augmenting the human body using wearable technology can be particularly interesting to sense context. The user’s context includes the mental and physical state, which is inferable by detecting facial and head related gestures. For the recognition of these gestures, we propose instrumenting a pair of glasses with Capacitive Sensing (CapSense) technology. We demonstrate proximity sensing with CapSense for mobile use despite its commonly known limitations in context of mobility. Moreover, we demonstrate how to incorporate transparent sensing electrodes into the glass and copper electrodes into the frame while being potentially invisible in a future specs product. We demonstrate an untethered battery-powered glasses prototype, CapGlasses, to sense facial expressions and head gestures. We selected a set of 12 gestures and ran a study with 12 users. We obtained an average accuracy of 89.6% by a user-dependent machine learning model. We focused on providing clear documentation to enable a straightforward replication of our technology.
SP  - 121
EP  - 130
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458945
ER  - 

TY  - NA
AU  - Nakao, Takuro; Kunze, Kai; Isogai, Megumi; Shimizu, Shinya; Pai, Yun Suen
TI  - MUM - FingerFlex: Shape Memory Alloy-based Actuation on Fingers for Kinesthetic Haptic Feedback
PY  - 2020
AB  - The tactile and kinesthetic sensation of pushing a button is usually lost when interacting with modern devices like touchscreens and/or virtual reality platforms. We present FingerFlex, a standalone glove wearable actuating the metacarpophalangeal joint (MCP) of each finger via shape memory alloy (SMA). SMA actuation is subtle, silent, and light, making it ideal for actuation of the fingers which we use to simulate the sensation of pressing a button. For our first study, we evaluated the engineering performance of FingerFlex by altering the current and triggering different levels of stimuli to the user’s fingers. We show that users can perceive at least 3 levels of actuation with an accuracy of 73%. For our second study, we found FingerFlex to perform significantly better in terms of input error on a virtual numblock of a keyboard with no significant change in perceived workload.
SP  - 240
EP  - 244
JF  - 19th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3428361.3428404
ER  - 

TY  - NA
AU  - Fang, Cathy; Harrison, Chris
TI  - UIST - Retargeted Self-Haptics for Increased Immersion in VR without Instrumentation
PY  - 2021
AB  - Today’s consumer virtual reality (VR) systems offer immersive graphics and audio, but haptic feedback is rudimentary – delivered through controllers with vibration feedback or is non-existent (i.e., the hands operating freely in the air). In this paper, we explore an alternative, highly mobile and controller-free approach to haptics, where VR applications utilize the user’s own body to provide physical feedback. To achieve this, we warp (retarget) the locations of a user’s hands such that one hand serves as a physical surface or prop for the other hand. For example, a hand holding a virtual nail can serve as a physical backstop for a hand that is virtually hammering, providing a sense of impact in an air-borne and uninstrumented experience. To illustrate this rich design space, we implemented twelve interactive demos across three haptic categories. We conclude with a user study from which we draw design recommendations.
SP  - 1109
EP  - 1121
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474810
ER  - 

TY  - NA
AU  - Shimobayashi, Hideki; Sasaki, Tomoya; Horie, Arata; Arakawa, Riku; Kashino, Zendai; Inami, Masahiko
TI  - AHs - Independent Control of Supernumerary Appendages Exploiting Upper Limb Redundancy
PY  - 2021
AB  - In the field of physical augmentation, researchers have attempted to extend human capabilities by expanding the number of human appendages. To fully realize the potential of having an additional appendage, supernumerary appendages should be independently controllable without interfering with the functionality of existing appendages. Herein, we propose a novel approach for controlling supernumerary appendages by exploiting upper limb redundancy. We present a headphone-style visual sensing device and a recognition system to estimate shoulder movement. Through a set of user experiments, we evaluate the feasibility of our system and reveal the potential of independent control using upper limb redundancy. Our results indicate that participants are able to intentionally give commands through their shoulder motions. Finally, we demonstrate the wide range of supernumerary appendage control applications that our novel approach enables and discuss future prospects for our work.
SP  - 19
EP  - 30
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458980
ER  - 

TY  - NA
AU  - Lindlbauer, David; Feit, Anna Maria; Hilliges, Otmar
TI  - UIST - Context-Aware Online Adaptation of Mixed Reality Interfaces
PY  - 2019
AB  - We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.
SP  - 147
EP  - 160
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347945
ER  - 

TY  - NA
AU  - Cohn, Brian A.; Maselli, Antonella; Ofek, Eyal; Gonzalez-Franco, Mar
TI  - AIVR - SnapMove: Movement Projection Mapping in Virtual Reality
PY  - 2020
AB  - We present SnapMove a technique to reproject reaching movements inside Virtual Reality. SnapMove can be used to reduce the need of large, fatiguing or difficult motions. We designed multiple reprojection techniques, linear or planar, uni-manual, bi-manual or head snap, that can be used for reaching, throwing and virtual tool manipulation. In a user study (n=21) we explore if the self-avatar follower effect can be modulated depending on the cost of the motion introduced by remapping. SnapMove was successful in re-projecting user’s hand position from e.g. a lower area, to a higher avatar-hand position–a mapping which can be ideal for limiting fatigue. It was also successful in preserving avatar embodiment and gradually bring users to perform movements with higher cost energies, which have most interest for rehabilitation scenarios. We implemented applications for menu interaction, climbing, rowing, and throwing darts. Overall, SnapMove can make interactions in virtual environments easier. We discuss the potential impact of SnapMove for application in gaming, accessibility and therapy.
SP  - 74
EP  - 81
JF  - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr50618.2020.00024
ER  - 

TY  - JOUR
AU  - Matthews, Brandon J.; Thomas, Bruce H.; Von Itzstein, G. Stewart; Smith, Ross T.
TI  - Adaptive Reset Techniques for Haptic Retargeted Interaction
PY  - NA
AB  - NA
SP  - 1478
EP  - 1490
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - 29
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2021.3120410
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Shen, Hao-Ping; Wu, Yu-Chian; Chen, Yu-An; Ku, Pin-Sung; Hsu, Ming-Wei; Liu, Jun-You; Lin, Yu-Chih; Chen, Mike Y.
TI  - UIST - CurrentViz: Sensing and Visualizing Electric Current Flows of Breadboarded Circuits
PY  - 2017
AB  - Electric current and voltage are fundamental to learning, understanding, and debugging circuits. Although both can be measured using tools such as multimeters and oscilloscopes, electric current is much more difficult to measure because users have to unplug parts of a circuit and then insert the measuring tools in serial. Furthermore, users need to restore the circuits back to its original state after measurements have been taken. In practice, this cumbersome process poses a formidable barrier to knowing how current flows throughout a circuit. We present CurrentViz, a system that can sense and visualize the electric current flowing through a circuit, which helps users quickly understand otherwise invisible circuit behavior. It supports fully automatic, ubiquitous, and real-time collection of amperage information of breadboarded circuits. It also supports visualization of the amperage data on a circuit schematic to provide an intuitive view into the current state of a circuit.
SP  - 343
EP  - 349
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126646
ER  - 

TY  - NA
AU  - Antoine, Axel; Malacria, Sylvain; Marquardt, Nicolai; Casiez, Géry
TI  - CHI - Interaction Illustration Taxonomy: Classification of Styles and Techniques for Visually Representing Interaction Scenarios
PY  - 2021
AB  - Static illustrations are ubiquitous means to represent interaction scenarios. Across papers and reports, these visuals demonstrate people’s use of devices, explain systems, or show design spaces. Creating such figures is challenging, and very little is known about the overarching strategies for visually representing interaction scenarios. To mitigate this task, we contribute a unified taxonomy of design elements that compose such figures. In particular, we provide a detailed classification of Structural and Interaction strategies, such as composition, visual techniques, dynamics, representation of users, and many others – all in context of the type of scenarios. This taxonomy can inform researchers’ choices when creating new figures, by providing a concise synthesis of visual strategies, and revealing approaches they were not aware of before. Furthermore, to support the community for creating further taxonomies, we also provide three open-source software facilitating the coding process and visual exploration of the coding scheme.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445586
ER  - 

TY  - BOOK
AU  - Fender, Andreas; Müller, Jörg
TI  - ISS - SpaceState: Ad-Hoc Definition and Recognition of Hierarchical Room States for Smart Environments
PY  - 2019
AB  - We present SpaceState, a system for designing spatial user interfaces that react to changes of the physical layout of a room. SpaceState uses depth cameras to measure the physical environment and allows designers to interactively define global and local states of the room. After designers defined states, SpaceState can identify the current state of the physical environment in real-time. This allows applications to adapt the content to room states and to react to transitions between states. Other scenarios include analysis and optimizations of work flows in physical environments. We demonstrate SpaceState by showcasing various example states and interactions. Lastly, we implemented an example application: A projection mapping based tele-presence application, which projects a remote user in the local physical space according to the current layout of the space.
SP  - 303
EP  - 314
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359715
ER  - 

TY  - JOUR
AU  - Feldman, Molly Q.; McInnis, Brian
TI  - How We Write with Crowds
PY  - 2021
AB  - Writing is a common task for crowdsourcing researchers exploring complex and creative work. To better understand how we write with crowds, we conducted both a literature review of crowd-writing systems and structured interviews with designers of such systems. We argue that the cognitive process theory of writing described by Flower and Hayes (1981), originally proposed as a theory of how solo writers write, offers a useful analytic lens for examining the design of crowd-writing systems. This lens enabled us to identify system design challenges that are inherent to the process of writing as well as design challenges that are introduced by crowdsourcing. The findings present both similarities and differences between how solo writers write versus how we write with crowds. To conclude, we discuss how the research community might apply and transcend the cognitive process model to identify opportunities for future research in crowd-writing systems.
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW3
PB  - 
DO  - 10.1145/3432928
ER  - 

TY  - JOUR
AU  - Sun, Zhida; Wang, Sitong; Liu, Chengzhong; Ma, Xiaojuan
TI  - Metaphoraction: Support Gesture-based Interaction Design with Metaphorical Meanings
PY  - 2022
AB  - <jats:p> Previous user experience research emphasizes meaning in interaction design beyond conventional interactive gestures. However, existing exemplars that successfully reify abstract meanings through interactions are usually case-specific, and it is currently unclear how to systematically create or extend meanings for general gesture-based interactions. We present Metaphoraction, a creativity support tool that formulates design ideas for gesture-based interactions to show metaphorical meanings with four interconnected components: <jats:italic>gesture</jats:italic> , <jats:italic>action</jats:italic> , <jats:italic>object</jats:italic> , and <jats:italic>meaning</jats:italic> . To represent the interaction design ideas with these four components, Metaphoraction links interactive gestures to actions based on the similarity of appearances, movements, and experiences; relates actions to objects by applying the immediate association; bridges objects and meanings by leveraging the metaphor TARGET-SOURCE mappings. We build a dataset containing 588,770 unique design idea candidates through surveying related research and conducting two crowdsourced studies to support meaningful gesture-based interaction design ideation. Five design experts validate that Metaphoraction can effectively support creativity and productivity during the ideation process. The paper concludes by presenting insights into meaningful gesture-based interaction design and discussing potential future uses of the tool. </jats:p>
SP  - 1
EP  - 33
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 5
PB  - 
DO  - 10.1145/3511892
ER  - 

TY  - NA
AU  - Khurana, Rushil; Hodges, Steve
TI  - CHI - Beyond the Prototype: Understanding the Challenge of Scaling Hardware Device Production
PY  - 2020
AB  - The hardware research and development communities have invested heavily in tools and materials that facilitate the design and prototyping of electronic devices. Numerous easy-to-access and easy-to-use tools have streamlined the prototyping of interactive and embedded devices for experts and led to a remarkable growth in non-expert builders. However, there has been little exploration of challenges associated with moving beyond a prototype and creating hundreds or thousands of exact replicas - a process that is still challenging for many. We interviewed 25 individuals with experience taking prototype hardware devices into low volume production. We systematically investigated the common issues faced and mitigation strategies adopted. We present our findings in four main categories: (1) gaps in technical knowledge; (2) gaps in non-technical knowledge; (3) minimum viable rigor in manufacturing preparation; and (4) building relationships and a professional network. Our study unearthed several opportunities for new tools and processes to support the transition beyond a working prototype to cost effective low-volume manufacturing. These would complement the aforementioned tools and materials that support design and prototyping.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376761
ER  - 

TY  - JOUR
AU  - Lee, Minkyeong; Je, Seungwoo; Lee, Woojin; Ashbrook, Daniel; Bianchi, Andrea
TI  - ActivEarring: Spatiotemporal Haptic Cues on the Ears
PY  - 2019
AB  - The symmetric configuration and the sensitivity of ears in addition to the long tradition of earrings as adornment open up the possibility for smart ear-worn devices. Taking advantage of these attributes, past research mostly focused on creating novel unobtrusive sensing input devices and auditory displays placed on the ear. Meanwhile, the tactile sensitivity of the ear has long been overshadowed by its auditory capacity, presenting the opportunity to investigate how ears can be exploited for unobtrusive tactile information transfer. With three studies and a total of 38 participants, we suggest the design of ActivEarring, a ear-worn device capable of imparting information by stimulating six different locations on both ears. We evaluated the performance of ActivEarring in a semi-realistic mobile condition and its practical use for information transfer with spatiotemporal patterns. Finally, we demonstrate that ActivEarring can be incorporated in common jewelry design, and present three applications that illustrate promising usage scenarios.
SP  - 554
EP  - 562
JF  - IEEE transactions on haptics
VL  - 12
IS  - 4
PB  - 
DO  - 10.1109/toh.2019.2925799
ER  - 

TY  - NA
AU  - Groeger, Daniel; Steimle, Jürgen
TI  - CHI - LASEC: Instant Fabrication of Stretchable Circuits Using a Laser Cutter
PY  - 2019
AB  - This paper introduces LASEC, the first technique for instant do-it-yourself fabrication of circuits with custom stretchability on a conventional laser cutter and in a single pass. The approach is based on integrated cutting and ablation of a two-layer material using parametric design patterns. These patterns enable the designer to customize the desired stretchability of the circuit, to combine stretchable with non-stretchable areas, or to integrate areas of different stretchability. For adding circuits on such stretchable cut patterns, we contribute routing strategies and a real-time routing algorithm. An interactive design tool assists designers by automatically generating patterns and circuits from a high-level specification of the desired interface. The approach is compatible with off-the-shelf materials and can realize transparent interfaces. Several application examples demonstrate the versatility of the novel technique for applications in wearable computing, interactive textiles, and stretchable input devices.
SP  - 699
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300929
ER  - 

TY  - NA
AU  - Seyed, Teddy; Devine, James; Finney, Joe; Moskal, Michal; de Halleux, Peli; Hodges, Steve; Ball, Thomas; Roseway, Asta
TI  - CHI - Rethinking the Runway: Using Avant-Garde Fashion To Design a System for Wearables
PY  - 2021
AB  - Technology has become increasingly pervasive in the creative and experimental environment of the avant-garde fashion runway, particularly in relation to its garments. However, several disciplines are often necessary when exploring technologies for the construction of expressive garments (e.g. garments that respond to their environment), creating a barrier for fashion designers that has limited their ability to leverage new technologies. To help overcome this barrier, we designed and deployed Brookdale, a prototyping system for wearable technology consisting of new plug-and-play hardware that can be programmed using drag-and-drop software. Brookdale was created using a 24-week participatory design process with 17 novice fashion-tech designers. At the end of the 24 week process, designers showcased their Brookdale-enhanced garment collections at an avant-garde fashion-tech runway show in New York City. We report on the experiences, outcomes, and lessons learned throughout this process, and describe results from interviews with the fashion-tech designers 16 weeks after the fashion show, demonstrating the lasting positive impact of Brookdale.
SP  - 1
EP  - 15
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445643
ER  - 

TY  - CHAP
AU  - Koguchi, Yuto; Oharada, Kazuya; Takagi, Yuki; Sawada, Yoshiki; Shizuki, Buntarou; Takahashi, Shin
TI  - HCI (3) - A Mobile Command Input Through Vowel Lip Shape Recognition
PY  - 2018
AB  - Most recent smartphones are controlled by touch screens, creating a need for hands-free input techniques. Voice is a simple means of input. However, this can be stressful in public spaces, and the recognition rate is low in noisy backgrounds. We propose a touch-free input technique using lip shapes. Vowels are detected by lip shape and used as commands. This creates a touch-free operation (like voice input) without actually requiring voice. We explored the recognition accuracies of each vowel of the Japanese moras. Vowels were identified with high accuracy by means of the characteristic lip shape.
SP  - 297
EP  - 305
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-91250-9_23
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Xu, Zheer; Yang, Xing-Dong; Hodges, Steve; Seyed, Teddy
TI  - CHI - Project Tasca: Enabling Touch and Contextual Interactions with a Pocket-based Textile Sensor
PY  - 2021
AB  - We present Project Tasca, a pocket-based textile sensor that detects user input and recognizes everyday objects that a user carries in the pockets of a pair of pants (e.g., keys, coins, electronic devices, or plastic items). By creating a new fabric-based sensor capable of detecting in-pocket touch and pressure, and recognizing metallic, non-metallic, and tagged objects inside the pocket, we enable a rich variety of subtle, eyes-free, and always-available input, as well as context-driven interactions in wearable scenarios. We developed our prototype by integrating four distinct types of sensing methods, namely: inductive sensing, capacitive sensing, resistive sensing, and NFC in a multi-layer fabric structure into the form factor of a jeans pocket. Through a ten-participant study, we evaluated the performance of our prototype across 11 common objects including hands, 8 force gestures, and 30 NFC tag placements. We yielded a 92.3% personal cross-validation accuracy for object recognition, 96.4% accuracy for gesture recognition, and a 100% accuracy for detecting NFC tags at close distance . We conclude by demonstrating the interactions enabled by our pocket-based sensor in several applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445712
ER  - 

TY  - NA
AU  - Zhang, Tengxiang; Zeng, Xin; Zhang, Yinshuai; Sun, Ke; Wang, Yuntao; Chen, Yiqiang
TI  - CHI - ThermalRing: Gesture and Tag Inputs Enabled by a Thermal Imaging Smart Ring
PY  - 2020
AB  - The heterogeneous and ubiquitous input demands in smart spaces call for an input device that can enable rich and spontaneous interactions. We propose ThermalRing, a thermal imaging smart ring using low-resolution thermal camera for identity-anonymous, illumination-invariant, and power-efficient sensing of both dynamic and static gestures. We also design ThermalTag, thin and passive thermal imageable tags that reflect the heat from the human hand. ThermalTag can be easily made and applied onto everyday objects by users. We develop sensing techniques for three typical input demands: drawing gestures for device pairing, click and slide gestures for device control, and tag scan gestures for quick access. The study results show that ThermalRing can recognize nine drawing gestures with an overall accuracy of 90.9%, detect click gestures with an accuracy of 94.9%, and identify among six ThermalTags with an overall accuracy of 95.0%. Finally, we show the versatility and potential of ThermalRing through various applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376323
ER  - 

TY  - NA
AU  - Mysore, Alok; Guo, Philip J.
TI  - UIST - Porta: Profiling Software Tutorials Using Operating-System-Wide Activity Tracing
PY  - 2018
AB  - It can be hard for tutorial creators to get fine-grained feedback about how learners are actually stepping through their tutorials and which parts lead to the most struggle. To provide such feedback for technical software tutorials, we introduce the idea of tutorial profiling, which is inspired by software code profiling. We prototyped this idea in a system called Porta that automatically tracks how users navigate through a tutorial webpage and what actions they take on their computer such as running shell commands, invoking compilers, and logging into remote servers. Porta surfaces this trace data in the form of profiling visualizations that augment the tutorial with heatmaps of activity hotspots and markers that expand to show event details, error messages, and embedded screencast videos of user actions. We found through a user study of 3 tutorial creators and 12 students who followed their tutorials that Porta enabled both the tutorial creators and the students to provide more specific, targeted, and actionable feedback about how to improve these tutorials. Porta opens up possibilities for performing user testing of technical documentation in a more systematic and scalable way.
SP  - 201
EP  - 212
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242633
ER  - 

TY  - NA
AU  - Nisser, Martin; Zhu, Junyi; Chen, Tianye; Bulovic, Katarina; Punpongsanon, Parinya; Mueller, Stefanie
TI  - Tangible and Embedded Interaction - Sequential Support: 3D Printing Dissolvable Support Material for Time-Dependent Mechanisms
PY  - 2019
AB  - In this paper, we propose a different perspective on the use of support material: rather than printing support structures for overhangs, our idea is to make use of its transient nature, i.e. the fact that it can be dissolved when placed in a solvent, such as water. This enables a range of new use cases, such as quickly dissolving and replacing parts of a prototype during design iteration, printing temporary assembly labels directly on the object that leave no marks when dissolved, and creating time-dependent mechanisms, such as fading in parts of an image in a shadow art piece or releasing relaxing scents from a 3D printed structure sequentially overnight. Since we use regular support material (PVA), our approach works on consumer 3D printers without any modifications. To facilitate the design of objects that leverage dissolvable support, we built a custom 3D editor plugin that includes a simulation showing how support material dissolves over time. In our evaluation, our simulation predicted geometries that are statistically similar to the example shapes within 10% error across all samples.
SP  - 669
EP  - 676
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295630
ER  - 

TY  - CHAP
AU  - Ambu, Rita; Motta, Alessandro; Calì, Michele
TI  - Design of a Customized Neck Orthosis for FDM Manufacturing with a New Sustainable Bio-composite
PY  - 2019
AB  - The interest in developing customized external orthopaedic devices, thanks to the advent of Additive Manufacturing (AM), has grown in recent years. Greater attention was focused on upper limb casts, while applications to other body’s parts, such as the neck, were less investigated. In this paper the computer aided design (CAD) modelling, assessment and 3D printing with fused deposition modelling (FDM) of a customized neck orthosis are reported. The modelling, based on anatomic data of a volunteer subject, was aimed to obtain a lightweight, ventilated, hygienic and comfortable orthosis compared to the produced medical devices generally used for neck injuries. CAD models with different geometrical patterns, introduced for lightening and improving breathability, were considered, specifically, a honeycomb pattern and an elliptical holes pattern. These models were structurally assessed by means of finite elements analysis (FEA). Furthermore, an innovative composite material was considered for 3D printing. The material, Hemp Bio-Plastic® (HBP), composed by polylactic acid (PLA) and hemp shives, offers different advantages including lightweight, improved superficial finish and antibacterial properties. The results obtained in terms of design methodology and manufacturing by 3D printing of a prototype have shown the feasibility to develop customized cervical orthoses, with potentially improved performance with respect to cervical collars available on the market also thanks to the use of the innovative composite material.
SP  - 707
EP  - 718
JF  - Lecture Notes in Mechanical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-31154-4_60
ER  - 

TY  - NA
AU  - Takegawa, Yoshinari; Tokuda, Yutaka; Umezawa, Akino; Suzuki, Katsuhiro; Masai, Katsutoshi; Sugiura, Yuta; Sugimoto, Maki; Plasencia, Diego Martinez; Subramanian, Sriram; Hirata, Keiji
TI  - ISMAR - Digital Full-Face Mask Display with Expression Recognition using Embedded Photo Reflective Sensor Arrays
PY  - 2020
AB  - This paper presents a thin digital full-face mask display that can reflect an entire facial expression of a user onto an avatar to support augmented face-to-face communication in real environments. Although camera-based facial expression recognition technology has enabled people to augment their faces with avatars, application was limited to face-to-face communication in virtual environments. To enable digital facial augmentation with an avatar in a real space, we propose a digital face mask display system that integrates a lightweight flexible display with a thin facial expression recognition system. The thin wearable facial expression recognition system was implemented with photo reflective sensor arrays which can measure facial expressions at 40 feature points distributed across an entire face. We investigated a ten-class facial expression identification model based on an SVM training algorithm. The trained model achieved an average accuracy of 79% when identifying the facial expressions of multiple users. User experiments indicated that the proposed thin digital full-face mask display allows the wearer to control the facial expression of the avatar with a fast response rate and create a positive sense of self-agency and self-ownership toward the augmented avatar face.
SP  - 101
EP  - 108
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00030
ER  - 

TY  - JOUR
AU  - Kakaraparthi, Vimal; Shao, Qijia; Carver, Charles J.; Pham, Tien; Bui, Nam; Nguyen, Phuc; Zhou, Xia; Vu, Tam
TI  - FaceSense: Sensing Face Touch with an Ear-worn System
PY  - 2021
AB  - Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one's face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSense integrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch. Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478129
ER  - 

TY  - JOUR
AU  - Fletcher, Mark D.
TI  - Can Haptic Stimulation Enhance Music Perception in Hearing-Impaired Listeners?
PY  - 2021
AB  - Cochlear implants (CIs) have been remarkably successful at restoring hearing in severely-to-profoundly hearing-impaired individuals. However, users often struggle to deconstruct complex auditory scenes with multiple simultaneous sounds, which can result in reduced music enjoyment and impaired speech understanding in background noise. Hearing aid users often have similar issues, though these are typically less acute. Several recent studies have shown that haptic stimulation can enhance CI listening by giving access to sound features that are poorly transmitted through the electrical CI signal. This "electro-haptic stimulation" improves melody recognition and pitch discrimination, as well as speech-in-noise performance and sound localization. The success of this approach suggests it could also enhance auditory perception in hearing-aid users and other hearing-impaired listeners. This review focuses on the use of haptic stimulation to enhance music perception in hearing-impaired listeners. Music is prevalent throughout everyday life, being critical to media such as film and video games, and often being central to events such as weddings and funerals. It represents the biggest challenge for signal processing, as it is typically an extremely complex acoustic signal, containing multiple simultaneous harmonic and inharmonic sounds. Signal-processing approaches developed for enhancing music perception could therefore have significant utility for other key issues faced by hearing-impaired listeners, such as understanding speech in noisy environments. This review first discusses the limits of music perception in hearing-impaired listeners and the limits of the tactile system. It then discusses the evidence around integration of audio and haptic stimulation in the brain. Next, the features, suitability, and success of current haptic devices for enhancing music perception are reviewed, as well as the signal-processing approaches that could be deployed in future haptic devices. Finally, the cutting-edge technologies that could be exploited for enhancing music perception with haptics are discussed. These include the latest micro motor and driver technology, low-power wireless technology, machine learning, big data, and cloud computing. New approaches for enhancing music perception in hearing-impaired listeners could substantially improve quality of life. Furthermore, effective haptic techniques for providing complex sound information could offer a non-invasive, affordable means for enhancing listening more broadly in hearing-impaired individuals.
SP  - 723877
EP  - 723877
JF  - Frontiers in neuroscience
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnins.2021.723877
ER  - 

TY  - NA
AU  - Gashi, Shkurta; Saeed, Aaqib; Vicini, Alessandra; Di Lascio, Elena; Santini, Silvia
TI  - ICMI - Hierarchical Classification and Transfer Learning to Recognize Head Gestures and Facial Expressions Using Earbuds
PY  - 2021
AB  - Head gestures and facial expressions – like, e.g., nodding or smiling – are important indicators of the quality of human interactions in physical meetings as well as in computer-mediated settings. Computer systems able to recognize such behavioral cues can support and improve human interactions. Several researchers have thus tackled the problem of automatically recognizing head gestures and facial expressions, mainly leveraging video data. In this paper, we instead consider inertial signals collected from unobtrusive, ear-mounted devices. We focus on typical activities performed during social interactions – head shaking, nodding, smiling, talking and yawning – and propose a hierarchical classification approach to discriminate them from each other. Further, we investigate whether the transfer of knowledge learned from publicly available datasets leads to further performance improvements. Our results show that the combined use of our hierarchical approach and transfer learning allows the classifier to discriminate head and mouth activities with an F1 score of 84.79, smile, talk and yawn with an F1 score of 45.42, and nodding and head shaking with an F1 score of 88.24, outperforming shallow classifiers by 2-9 percentage points.
SP  - 168
EP  - 176
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3479921
ER  - 

TY  - JOUR
AU  - Coughlan, James M.; Biggs, Brandon; Rivière, Marc-Aurèle; Shen, Huiying
TI  - ICCHP (1) - An Audio-Based 3D Spatial Guidance AR System for Blind Users.
PY  - 2020
AB  - Augmented reality (AR) has great potential for blind users because it enables a range of applications that provide audio information about specific locations or directions in the user's environment. For instance, the CamIO ("Camera Input-Output") AR app makes physical objects (such as documents, maps, devices and 3D models) accessible to blind and visually impaired persons by providing real-time audio feedback in response to the location on an object that the user is touching (using an inexpensive stylus). An important feature needed by blind users of AR apps such as CamIO is a 3D spatial guidance feature that provides real-time audio feedback to help the user find a desired location on an object. We have devised a simple audio interface to provide verbal guidance towards a target of interest in 3D. The experiment we report with blind participants using this guidance interface demonstrates the feasibility of the approach and its benefit for helping users find locations of interest.
SP  - 475
EP  - 484
JF  - Computers helping people with special needs : ... International Conference, ICCHP ... : proceedings. International Conference on Computers Helping People with Special Needs
VL  - 12376
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58796-3_55
ER  - 

TY  - NA
AU  - Pohl, Henning; Lilija, Klemen; McIntosh, Jess; Hornbæk, Kasper
TI  - CHI - Poros: Configurable Proxies for Distant Interactions in VR
PY  - 2021
AB  - A compelling property of virtual reality is that it allows users to interact with objects as they would in the real world. However, such interactions are limited to space within reach. We present Poros, a system that allows users to rearrange space. After marking a portion of space, the distant marked space is mirrored in a nearby proxy. Thereby, users can arrange what is within their reachable space, making it easy to interact with multiple distant spaces as well as nearby objects. Proxies themselves become part of the scene and can be moved, rotated, scaled, or anchored to other objects. Furthermore, they can be used in a set of higher-level interactions such as alignment and action duplication. We show how Poros enables a variety of tasks and applications and also validate its effectiveness through an expert evaluation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445685
ER  - 

TY  - NA
AU  - Kono, Michinari; Kabashima, Osamu; Sasaki, Norio; Yamaoka, Junichi
TI  - CHI PLAY - Pre-Eating Play: Fabrication Experiences for Playful Human-Food Interaction
PY  - 2020
AB  - Recent progress of human--food interaction (HFI) has led to a growing interest in the merger of digital technologies for playful experiences in eating. Although these interactive technologies have been primarily integrated with eating experiences, we find that progress has also been made in digital food fabrication process in the human--computer interaction (HCI) field. Food fabrication techniques are used to expand the design space of food preparation, and it is accomplished before the eating stage. Many products have introduced playful experiences to this preparation stage. However, we have not yet explored food fabrication techniques in the context of play in depth. This paper discusses the food preparation stage and how it can be designed to be playful by introducing several product cases. Using this knowledge, we further discuss how such methods can be applied to previous digital food fabrication techniques with our own example of a playful food fabrication approach. We aim to enable various stages of human--food interaction to be more playful by understanding food as a toy-like object that can be independent from or related to the eating stage.
SP  - 160
EP  - 168
JF  - Proceedings of the Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410404.3414228
ER  - 

TY  - NA
AU  - Brocker, Anke; Voelker, Simon; Zhang, Tony; Müller, Mathis; Borchers, Jan
TI  - CHI Extended Abstracts - Flowboard: A Visual Flow-Based Programming Environment for Embedded Coding
PY  - 2019
AB  - With Maker-friendly environments like the Arduino IDE, embedded programming has become an important part of STEM education. But learning embedded programming is still hard, requiring both coding and basic electronics skills. To understand if a different programming paradigm can help, we developed Flowboard, which uses Flow-Based Programming (FBP) rather than the usual imperative programming paradigm. Instead of command sequences, learners assemble processing nodes into a graph through which signals and data flow. Flowboard consists of a visual flow-based editor on an iPad, a hardware frame integrating the iPad, an Arduino board and two breadboards next to the iPad, letting learners connect their visual graphs seamlessly to the input and output electronics. Graph edits take effect immediately, making Flowboard a live coding environment.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3313247
ER  - 

TY  - NA
AU  - Amesaka, Takashi; Watanabe, Hiroki; Sugimoto, Masanori
TI  - Facial expression recognition using ear canal transfer function
PY  - 2019
AB  - In this study, we propose a new input method for mobile and wearable computing using facial expressions. Facial muscle movements induce physical deformation in the ear canal. Our system utilizes such characteristics and estimates facial expressions using the ear canal transfer function (ECTF). Herein, a user puts on earphones with an equipped microphone that can record an internal sound of the ear canal. The system transmits ultrasonic band-limited swept sine signals and acquires the ECTF by analyzing the response. An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes. We investigated the performance of our proposed method for 21 facial expressions with 11 participants. Moreover, we proposed a signal correction method that reduces positional errors caused by attaching/detaching the device. The evaluation results confirmed that the f-score was 40.2% for the uncorrected signal method and 62.5% for the corrected signal method. We also investigated the practical performance of six facial expressions and confirmed that the f-score was 74.4% for the uncorrected signal method and 90.0% for the corrected signal method. We found the ECTF can be used for recognizing facial expressions with high accuracy equivalent to other related work.
SP  - 1
EP  - 9
JF  - Proceedings of the 23rd International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341163.3347747
ER  - 

TY  - NA
AU  - Sun, Ruojia; Onose, Ryosuke; Dunne, Margaret; Ling, Andrea; Denham, Amanda; Kao, Hsin-Liu
TI  - Conference on Designing Interactive Systems - Weaving a Second Skin: Exploring Opportunities for Crafting On-Skin Interfaces Through Weaving
PY  - 2020
AB  - Weaving as a craft possesses the structural, textural, aesthetic, and cultural expressiveness for creating a diversity of soft, wearable forms that are capable of technological integration. In this paper, we extend the woven practice for crafting on-skin interfaces, exploring the potential to "weave a second skin." Weaving incorporates circuitry in the textile structure, which, when extended to on-skin interface fabrication, allows for electrical connections between layers while maintaining a slim form. Weaving also supports multi-materials integration in the structure itself, offering richer materiality for on-skin devices. We present the results of extensive design experiments that form a design space for adapting weaving for on-skin interface fabrication. We introduce a fabrication approach leveraging the skin-friendly material of PVA, which enables on-skin adherence, and a series of case studies illustrating the functional and design potential of the approach. To understand the feasibility of on-skin wear, we conducted a user study on device wearability. To understand the expressiveness of the design space, we conducted a workshop study in which textiles practitioners created woven on-skin interfaces. We draw insights from this to understand the potential of adapting weaving for crafting on-skin interfaces.
SP  - 365
EP  - 377
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395548
ER  - 

TY  - NA
AU  - Lin, Richard; Ramesh, Rohit; Jain, Nikhil; Koe, Josephine; Nuqui, Ryan; Dutta, Prabal; Hartmann, Bjoern
TI  - UIST - Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages
PY  - 2021
AB  - In many engineering disciplines such as circuit board, chip, and mechanical design, a hardware description language (HDL) approach provides important benefits over direct manipulation interfaces by supporting concepts like abstraction and generator meta-programming. While several such HDLs have emerged recently and promised power and flexibility, they also present challenges – especially to designers familiar with current graphical workflows. In this work, we investigate an IDE approach to provide a graphical editor for a board-level circuit design HDL. Unlike GUI builders which convert an entire diagram to code, we instead propose generating equivalent HDL from individual graphical edit actions. By keeping code as the primary design input, we preserve the full power of the underlying HDL, while remaining useful even to advanced users. We discuss our concept, design considerations such as performance, system implementation, and report on the results of an exploratory remote user study with four experienced hardware designers.
SP  - 1039
EP  - 1049
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474804
ER  - 

TY  - NA
AU  - Kim, Jin Hee; Huang, Kunpeng; White, Simone L; Conroy, Melissa; Kao, Cindy Hsin-Liu
TI  - Conference on Designing Interactive Systems - KnitDermis: Fabricating Tactile On-Body Interfaces Through Machine Knitting
PY  - 2021
AB  - We present KnitDermis, on-body interfaces that deliver expressive non-vibrating mechanotactile feedback on the wearer’s body. Fabricated through machine knitting, they embed shape-memory alloy micro-springs in knitted channels, which deliver tactile sensations on the skin when activated. KnitDermis interfaces take advantage of machine knitting’s shaping properties which allow it to generate slim, stretchable, and versatile forms that can conform to underexplored body locations, such as protruded joints and convex body locations. We introduce a fabrication approach and a series of case studies to design a wide range of form factors, textures, and tactile patterns, including compression, pinching, brushing, and twisting. We conduct a user study to elicit KnitDermis’ effectiveness and wearability on diverse body locations and engage users to unpack envisioned use cases and perceptions towards the interfaces. We draw insights from our extensive research-through-design investigations on the potential of knitting as a soft approach for close-body and expressive tactile interfaces.
SP  - 1183
EP  - 1200
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462007
ER  - 

TY  - BOOK
AU  - Kudo, Yoshiki; Takashima, Kazuki; Fjeld, Morten; Kitamura, Yoshifumi
TI  - ISS - AdapTable: Extending Reach over Large Tabletops through Flexible Multi-Display Configuration
PY  - 2018
AB  - Large interactive tabletops are beneficial for various tasks involving exploration and visualization, but regions of the screen far from users can be difficult to reach. We propose AdapTable; a concept and prototype of a flexible multi-display tabletop that can physically reconfigure its layout, allowing for interaction with difficult-to-reach regions. We conducted a design study where we found users preferred to change screen layouts for full-screen interaction, motivated by reduced physical demands and frustration. We then prototyped AdapTable using four actuated tabletop displays each propelled by a mobile robot, and a touch menu was chosen to control the layout. Finally, we conducted a user study to evaluate how well AdapTable addresses the reaching problem compared with a conventional panning technique. Our findings show that AdapTable provides a more efficient method for complex full-screen interaction.
SP  - 213
EP  - 225
JF  - Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3279778.3279779
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Chen, Bing-Yu
TI  - UIST - ElastImpact: 2.5D Multilevel Instant Impact Using Elasticity on Head-Mounted Displays
PY  - 2019
AB  - Impact is a common effect in both daily life and virtual reality (VR) experiences, e.g., being punched, hit or bumped. Impact force is instantly produced, which is distinct from other force feedback, e.g., push and pull. We propose ElastImpact to provide 2.5D instant impact on a head-mounted display (HMD) for realistic and versatile VR experiences. ElastImpact consists of three impact devices, also called impactors. Each impactor blocks an elastic band with a mechanical brake using a servo motor and extending it using a DC motor to store the impact power. When releasing the brake, it provides impact instantly. Two impactors are affixed on both sides of the head and connected with the HMD to provide the normal direction impact toward the face (i.e., 0.5D in z-axis). The other impactor is connected with a proxy collider in a barrel in front of the HMD and rotated by a DC motor in the tangential plane of the face to provide 2D impact (i.e., xy-plane). By performing a just-noticeable difference (JND) study, we realize users' impact force perception distinguishability on the heads in the normal direction and tangential plane, separately. Based on the results, we combine normal and tangential impact as 2.5D impact, and performed a VR experience study to verify that the proposed 2.5D impact significantly enhances realism.
SP  - 429
EP  - 437
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347931
ER  - 

TY  - NA
AU  - Sun, Wei; Li, Franklin Mingzhe; Steeper, Benjamin; Xu, Songlin; Tian, Feng; Zhang, Cheng
TI  - TeethTap: Recognizing Discrete Teeth Gestures Using Motion and Acoustic Sensing on an Earpiece
PY  - 2021
AB  - Teeth gestures become an alternative input modality for different situations and accessibility purposes. In this paper, we present TeethTap, a novel eyes-free and hands-free input technique, which can recognize up to 13 discrete teeth tapping gestures. TeethTap adopts a wearable 3D printed earpiece with an IMU sensor and a contact microphone behind both ears, which works in tandem to detect jaw movement and sound data, respectively. TeethTap uses a support vector machine to classify gestures from noise by fusing acoustic and motion data, and implements K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW) distance measurement using motion data for gesture classification. A user study with 11 participants demonstrated that TeethTap could recognize 13 gestures with a real-time classification accuracy of 90.9% in a laboratory environment. We further uncovered the accuracy differences on different teeth gestures when having sensors on single vs. both sides. Moreover, we explored the activation gesture under real-world environments, including eating, speaking, walking and jumping. Based on our findings, we further discussed potential applications and practical challenges of integrating TeethTap into future devices.
SP  - 161
EP  - 169
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450645
ER  - 

TY  - NA
AU  - Boldu, Roger; Wijewardena, Mevan; Zhang, Haimo; Nanayakkara, Suranga
TI  - MobileHCI - MAGHair: A Wearable System to Create Unique Tactile Feedback by Stimulating Only the Body Hair
PY  - 2020
AB  - We present MAGHair, a novel wearable technique that provides subtle haptic sensation by stimulating the body hair without touching the skin. Our approach builds on previous research in magnetic hair stimulation and magnetic locomotion. We use magnetic cosmetics to augment the body hair, which can then be stimulated by a wearable apparatus that combines electromagnets and permanent magnets. We provide technical insights on the implementation of a fully functional wrist-worn form factor and early adaptations into other form factors. In addition, we provide a workflow for evaluating and characterizing the magnetic cosmetic recipes. Finally, we evaluate MAGHair, which demonstrated that users could detect the sensation of hair movement that they described as gentle and unique.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403545
ER  - 

TY  - NA
AU  - Matthies, Denys J.C.; Urban, Bodo; Wolf, Katrin; Schmidt, Albrecht
TI  - OZCHI - Reflexive Interaction: Extending the concept of Peripheral Interaction
PY  - 2019
AB  - Human-computer interaction (HCI) continues to evolve and interaction scenarios have to fulfill mobility, flexibility, and ad-hoc interaction where ever users are. To address this, traditional interaction concepts are being extended. While Peripheral Interaction was previously introduced, it still remains as a rather broad concept, intersecting with others, and thus creating space for further definitions. Therefore, this paper introduces the concept of Reflexive Interaction, which can be viewed as a specific manifestation of Peripheral Interaction. In contrast, Reflexive Interaction is envisioned to be executed at a secondary task without involving substantial cognitive effort. It allows the user to perform very short interactions, shorter than Microinteractions, without straining the user's main interaction channels occupied with the primary task. To clearly classify Reflexive Interaction in respect to previous interaction concepts, we use a taxonomy relying on an attention-based HCI model.
SP  - 266
EP  - 278
JF  - Proceedings of the 31st Australian Conference on Human-Computer-Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3369457.3369478
ER  - 

TY  - NA
AU  - Muthukumarana, Sachith; Elvitigala, Don Samitha; Cortes, Juan Pablo Forero; Matthies, Denys J.C.; Nanayakkara, Suranga
TI  - CHI - Touch me Gently: Recreating the Perception of Touch using a Shape-Memory Alloy Matrix
PY  - 2020
AB  - We present a wearable forearm augmentation that enables the recreation of natural touch sensation by applying shear-forces onto the skin. In contrast to previous approaches, we arrange light-weight and stretchable 3x3cm plasters in a matrix onto the skin. Individual plasters were embedded with lines of shape-memory alloy (SMA) wires to generate shear-forces. Our design is informed by a series of studies investigating the perceptibility of different sizes, spacings, and attachments of plasters on the forearm. Our matrix arrangement enables the perception of touches, for instance, feeling ones wrist being grabbed or the arm being stroked. Users rated the recreated touch sensations as being fairly similar to a real touch (4.1/5). Even without a visual representation, users were able to correctly distinguish them with an overall accuracy of 94.75%. Finally, we explored two use cases showing how AR and VR could be empowered with experiencing recreated touch sensations on the forearm.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376491
ER  - 

TY  - NA
AU  - Gong, Jun; Huang, Da-Yuan; Seyed, Teddy; Lin, Te; Hou, Tao; Liu, Xin; Yang, Molin; Yang, Boyu; Zhang, Yuhan; Yang, Xing-Dong
TI  - CHI - Jetto: Using Lateral Force Feedback for Smartwatch Interactions
PY  - 2018
AB  - Interacting with media and games is a challenging user experience on smartwatches due to their small screens. We propose using lateral force feedback to enhance these experiences. When virtual objects on the smartwatch display visually collide or push the edge of the screen, we add haptic feedback so that the user also feels the impact. This addition creates the illusion of a virtual object that is physically hitting or pushing the smartwatch, from within the device itself. Using this approach, we extend virtual space and scenes into a 2D physical space. To create realistic lateral force feedback, we first examined the minimum change in force magnitude that is detectable by users in different directions and weight levels, finding an average JND of 49% across all tested conditions, with no significant effect of weight and force direction. We then developed a proof-of-concept hardware prototype called Jetto and demonstrated its unique capabilities through a set of impact-enhanced videos and games. Our preliminary user evaluations indicated the concept was welcomed and is regarded as a worthwhile addition to smartwatch output and media experiences.
SP  - 426
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174000
ER  - 

TY  - NA
AU  - He, Liang; Wang, Ruolin; Xu, Xuhai
TI  - CHI Extended Abstracts - PneuFetch: Supporting Blind and Visually Impaired People to Fetch Nearby Objects via Light Haptic Cues
PY  - 2020
AB  - We present PneuFetch, a light haptic cue-based wearable device that supports blind and visually impaired (BVI) people to fetch nearby objects in an unfamiliar environment. In our design, we generate friendly, non-intrusive, and gentle presses and drags to deliver direction and distance cues on BVI user's wrist and forearm. As a concept of proof, we discuss our PneuFetch wearable prototype, contrast it with past work, and describe a preliminary user study.
SP  - 1
EP  - 9
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383095
ER  - 

TY  - JOUR
AU  - Abu Bakar, Zulzikry Hafiz; Bellier, Jean-Pierre; Wan Ngah, Wan Zurinah; Yanagisawa, Daijiro; Mukaisho, Ken-ichi; Tooyama, Ikuo
TI  - Optimization of 3D Immunofluorescence Analysis and Visualization Using IMARIS and MeshLab
PY  - 2023
AB  - <jats:p>The precision of colocalization analysis is enhanced by 3D and is potentially more accurate than 2D. Even though 3D improves the visualization of colocalization analysis, rendering a colocalization model may generate a model with numerous polygons. We developed a 3D colocalization model of FtMt/LC3 followed by simplification. Double immunofluorescence staining of FtMt and LC3 was conducted, and stacked images were acquired. We used IMARIS to render the 3D colocalization model of FtMt/LC3 and further processed it with MeshLab to decimate and generate a less complex colocalization model. We examined the available simplification algorithm using MeshLab in detail and evaluated the feasibility of each procedure in generating a model with less complexity. The quality of the simplified model was subsequently assessed. MeshLab’s available shaders were scrutinized to facilitate the spatial colocalization determination. Finally, we showed that QECD was the most effective method for reducing the polygonal complexity of the colocalization model without compromising its quality. In addition, we would recommend implementing the x-ray shader, which we found useful for visualizing colocalization. As 3D was found to be more accurate in quantifying colocalization, our study provides a novel and dependable method for rendering 3D models for colocalization analysis.</jats:p>
SP  - 218
EP  - NA
JF  - Cells
VL  - 12
IS  - 2
PB  - 
DO  - 10.3390/cells12020218
ER  - 

TY  - JOUR
AU  - Cheng, Huaqin; Liu, Bin; Liu, Meiying; Cao, Wei
TI  - Design of three-dimensional Voronoi strut midsoles driven by plantar pressure distribution
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>The customized production pattern has brought significant innovation to the design and manufacturing of footwear. To improve the matching degree between the consumer’s feet and deepen the customization of the sole’s personalized function, a three-dimensional (3D) Voronoi strut midsole structural design method driven by plantar pressure distribution is proposed in this paper, which not only realizes the functional requirements but also takes into account the aesthetic of midsoles. In this method, the foot characteristics and pressure information obtained by the foot measuring system are employed as the data-driven basic of the midsole structural design, and a weighted random sampling strategy is introduced for constructing the Voronoi sites. Moreover, a Voronoi clipping algorithm is proposed to make the 3D Voronoi diagram adaptive to the midsole boundary. And then, taking the clipped 3D Voronoi edges as skeleton lines, the smooth and continuous 3D Voronoi strut midsoles are generated by the implicit surface modelling technology and implicit function fusion. All the algorithms are integrated into a digital framework by independent programming. And both the static and dynamic tests show that the 3D Voronoi strut midsole can make the plantar pressure distribution more homogenous and can effectively reduce the load on the metatarsal and heel region. What is more, it can provide superior energy absorption and cushioning properties, offer better resilience, bring consumers a more comfortable wearing experience and reduce the probability of joint injury caused by the abnormal plantar pressure concentration.</jats:p>
SP  - 1410
EP  - 1429
JF  - Journal of Computational Design and Engineering
VL  - 9
IS  - 4
PB  - 
DO  - 10.1093/jcde/qwac060
ER  - 

TY  - NA
AU  - Gonzalez, Eric J.; Abtahi, Parastoo; Follmer, Sean
TI  - UIST - REACH+: Extending the Reachability of Encountered-type Haptics Devices through Dynamic Redirection in VR
PY  - 2020
AB  - Encountered-type haptic devices (EHDs) face a number of challenges when physically embodying content in a virtual environment, including workspace limits and device latency. To address these issues, we propose REACH+, a framework for dynamic visuo-haptic redirection to improve the perceived performance of EHDs during physical interaction in VR. Using this approach, we estimate the user's arrival time to their intended target and redirect their hand to a point within the EHD's spatio-temporally reachable space. We present an evaluation of this framework implemented with a desktop mobile robot in a 2D target selection task, tested at four robot speeds (20, 25, 30 and 35 cm/s). Results suggest that REACH+ can improve the performance of lower-speed EHDs, increasing their rate of on-time arrival to the point of contact by up to 25% and improving users? self-reported sense of realism.
SP  - 236
EP  - 248
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415870
ER  - 

TY  - NA
AU  - Strasnick, Evan
TI  - UIST (Adjunct Volume) - Circuit Design Tools for Exploratory Understanding
PY  - 2019
AB  - Effective circuit design and debugging require developing an intimate understanding of the behaviors of a complex system. In my work, I've distilled barriers to such understanding to three fundamental challenges of circuit design: transparency, malleability, and modelability. In turn, my research contributes tools that address these challenges through novel changes to the circuit design workflow: Pinpoint improves transparency and malleability in the debugging of printed circuit boards (PCBs) by augmenting board connections with automatic instrumentation and reconfigurable connectivity. Scanalog similarly improves transparency and malleability in prototyping by providing an interactively reprogrammable platform on which to design and tune fully instrumented mixed-signal circuits. My ongoing work addresses issues in modelability through tools that generate empirically-derived fault models and highlight causal relationships between components in a circuit. By evaluating these interactions, my research examines the role of exploratory understanding in circuit design, asking, "How can tools promote understanding of a circuit by facilitating exploration and reflection?"
SP  - 150
EP  - 153
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356876
ER  - 

TY  - NA
AU  - Lai, Yenchin; Tag, Benjamin; Kunze, Kai; Malaka, Rainer
TI  - AHs - Understanding Face Gestures with a User-Centered Approach Using Personal Computer Applications as an Example
PY  - 2020
AB  - While face gesture input has been proposed by researchers, the issue of practical gestures remains unsolved. We present the first comprehensive investigation of user-defined face gestures as an augmented input modality. Based on a focus group discussion, we developed three sets of tasks, where we asked participants to spontaneously produce face gestures to complete these tasks. We report our findings of a user study and discuss the user preference of face gestures. The results inform the development of future interaction systems utilizing face gestures.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3385333
ER  - 

TY  - NA
AU  - Shang, Jiacheng; Wu, Jie
TI  - MASS - Voice Liveness Detection for Voice Assistants using Ear Canal Pressure
PY  - 2020
AB  - With the success of voice recognition techniques, users can easily control any device in smart home environments by simply saying a voice command. Based on this idea, a new group of smart devices are designed and released, which are called voice assistant. However, the voice itself is not secure and can be attacked in many ways. To defend against various types of voice replay attacks, we present a new voice liveness detection system. The basic insight of our system is that mouth opening movements will change the space size in the ear canal, which further changes the air pressure in ear canals. In this paper, we propose solutions to detect mouth opening movements using the noisy air pressure data and match them with the voices to validate the liveness of the voice source. To evaluate the effectiveness of our system, we develop a prototype on Raspberry Pi and conduct comprehensive evaluations. Experiments with ten volunteers show that our system can accurately accept voice commands from legitimate users with an accuracy of 91.72%. Moreover, our system can effectively defend current voice assistant devices from replay attacks with an accuracy of 97.2%.
SP  - 693
EP  - 701
JF  - 2020 IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/mass50613.2020.00089
ER  - 

TY  - NA
AU  - Yamaoka, Junichi; Dogan, Mustafa Doga; Bulovic, Katarina; Saito, Kazuya; Kawahara, Yoshihiro; Kakehi, Yasuaki; Mueller, Stefanie
TI  - CHI Extended Abstracts - FoldTronics Demo: Creating 3D Objects with Integrated Electronics Using Foldable Honeycomb Structures
PY  - 2019
AB  - We present FoldTronics, a 2D-cutting based fabrication technique to integrate electronics into 3D folded objects. The key idea is to cut and perforate a 2D sheet to make it foldable into a honeycomb structure using a cutting plotter; before folding the sheet into a 3D structure, users place the electronic components and circuitry onto the sheet. The fabrication process only takes a few minutes allowing to rapidly prototype functional interactive devices. The resulting objects are lightweight and rigid, thus allowing for weight-sensitive and force-sensitive applications. Finally, due to the nature of the honeycomb structure, the objects can be folded flat along one axis and thus can be efficiently transported in this compact form factor. We describe the structure of the foldable sheet, and present a design tool that enables users to quickly prototype the desired objects. We showcase a range of examples made with our design tool, including objects with integrated sensors and display elements.
SP  - 628
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300858
ER  - 

TY  - NA
AU  - Bell, Fiona; Hong, Alice; Danielescu, Andreea; Maheshwari, Aditi; Greenspan, Ben; Ishii, Hiroshi; Devendorf, Laura; Alistar, Mirela
TI  - CHI - Self-deStaining Textiles: Designing Interactive Systems with Fabric, Stains and Light
PY  - 2021
AB  - This work introduces “destaining” as an interactive component for the HCI community. While staining happens unintentionally (e.g., spilling coffee), destaining can be used as an intentional design tool that selectively degrades stains on textiles. We explore the design space using silver doped titanium dioxide (TiO2/Ag), stains and light as a set of design primitives for interactive systems. We then developed replicable and accessible fabrication and testing methods that enable HCI researchers and designers to upgrade various fabrics to self-destaining textiles. Next, we demonstrate a Self-deStaining textile interface with embedded Light Emitting Diodes (LEDs) and moisture sensors that activate cleaning. Lastly, we showcase how the textile can be used in everyday objects such as self-cleaning clothes, a patterning station for phone cases, and accessories that change patterns and colors based on the user’s experiences.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445155
ER  - 

TY  - NA
AU  - Parizi, Farshid Salemi; Whitmire, Eric; Cao, Alvin; Li, Tianke; Patel, Shwetak N.
TI  - UIST (Adjunct Volume) - Demo of AuraRing: Precise Electromagnetic Finger Tracking
PY  - 2019
AB  - We present AuraRing, a wearable electromagnetic tracking system for fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the finger. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.
SP  - 122
EP  - 124
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356893
ER  - 

TY  - NA
AU  - Iravantchi, Yasha; Zhang, Yang; Bernitsas, Evi; Goel, Mayank; Harrison, Chris
TI  - CHI - Interferi: Gesture Sensing using On-Body Acoustic Interferometry
PY  - 2019
AB  - Interferi is an on-body gesture sensing technique using acoustic interferometry. We use ultrasonic transducers resting on the skin to create acoustic interference patterns inside the wearer's body, which interact with anatomical features in complex, yet characteristic ways. We focus on two areas of the body with great expressive power: the hands and face. For each, we built and tested a series of worn sensor configurations, which we used to identify useful transducer arrangements and machine learning fea-tures. We created final prototypes for the hand and face, which our study results show can support eleven- and nine-class gestures sets at 93.4% and 89.0% accuracy, re-spectively. We also evaluated our system in four continu-ous tracking tasks, including smile intensity and weight estimation, which never exceed 9.5% error. We believe these results show great promise and illuminate an inter-esting sensing technique for HCI applications.
SP  - 276
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300506
ER  - 

TY  - JOUR
AU  - Li, Jing; Dong, Yabo; Fang, Shengkai; Zhang, Haowen; Xu, Duanqing
TI  - User Context Detection for Relay Attack Resistance in Passive Keyless Entry and Start System.
PY  - 2020
AB  - In modern cars, the Passive Keyless Entry and Start system (PKES) has been extensively installed. The PKES enables drivers to unlock and start their cars without user interaction. However, it is vulnerable to relay attacks. In this paper, we propose a secure smartphone-type PKES system model based on user context detection. The proposed system uses the barometer and accelerometer embedded in smartphones to detect user context, including human activity and door closing event. These two types of events detection can be used by the PKES to determine the car owner’s position when the car receives an unlocking or a start command. We evaluated the performance of the proposed method using a dataset collected from user activity and 1526 door closing events. The results reveal that the proposed method can accurately and effectively detect user activities and door closing events. Therefore, smartphone-type PKES can prevent relay attacks. Furthermore, we tested the detection of door closing event under multiple environmental settings to demonstrate the robustness of the proposed method.
SP  - 4446
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 16
PB  - 
DO  - 10.3390/s20164446
ER  - 

TY  - NA
AU  - Abe, Takehiro; Sakamoto, Daisuke
TI  - MobileHCI - MagneTrack: Magnetic Field Separation Method for Continuous and Simultaneous 1-DOF Tracking of Two-magnets
PY  - 2021
AB  - This paper presents a method to track two magnets using one magnetometer on a smart device (e.g., smartphone), which detects the locations of the magnets around the device. A magnetometer measures the overlapping data signals generated by the two magnets. Thus, a data-separation technique is required to separately track the continuous and simultaneous signals generated by the two magnets. In this study, we develop a separation algorithm based on nonnegative matrix factorization to continuously and simultaneously track two magnets, and then conduct an experiment to validate our concept. We then compare our method in both simulated and actual environments. In the actual environment, we observe real data signals arising from the two magnets on a magnetometer. Finally, we present example applications to demonstrate the use cases of our model in human–computer interaction systems.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472052
ER  - 

TY  - JOUR
AU  - Gertler, Ifat; Serhat, Gokhan; Kuchenbecker, Katherine J
TI  - Generating Clear Vibrotactile Cues with a Magnet Embedded in a Soft Finger Sheath.
PY  - 2022
AB  - Haptic displays act on the user's body to stimulate the sense of touch and enrich applications from gaming and computer-aided design to rehabilitation and remote surgery. However, when crafted from typical rigid robotic components, they tend to be heavy, bulky, and expensive, while sleeker designs often struggle to create clear haptic cues. This article introduces a lightweight wearable silicone finger sheath that can deliver salient and rich vibrotactile cues using electromagnetic actuation. We fabricate the sheath on a ferromagnetic mandrel with a process based on dip molding, a robust fabrication method that is rarely used in soft robotics but is suitable for commercial production. A miniature rare-earth magnet embedded within the silicone layers at the center of the finger pad is driven to vibrate by the application of alternating current to a nearby air-coil. Experiments are conducted to determine the amplitude of the magnetic force and the frequency response function for the displacement amplitude of the magnet perpendicular to the skin. In addition, high-fidelity finite element analyses of the finger wearing the device are performed to investigate the trends observed in the measurements. The experimental and simulated results show consistent dynamic behavior from 10 to 1000 Hz, with the displacement decreasing after about 300 Hz. These results match the detection threshold profile obtained in a psychophysical study performed by 17 users, where more current was needed only at the highest frequency. A cue identification experiment and a demonstration in virtual reality validate the feasibility of this approach to fingertip haptics.
SP  - NA
EP  - NA
JF  - Soft robotics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1089/soro.2021.0184
ER  - 

TY  - NA
AU  - Kim, Yoonji; Choi, Youngkyung; Lee, Hyein; Lee, Geehyuk; Bianchi, Andrea
TI  - CHI - VirtualComponent: A Mixed-Reality Tool for Designing and Tuning Breadboarded Circuits
PY  - 2019
AB  - Prototyping electronic circuits is an increasingly popular activity, supported by researchers, who develop toolkits to improve the design, debugging, and fabrication of electronics. Although past work mainly dealt with circuit topology, in this paper we propose a system for determining or tuning the values of the circuit components. Based on the results of a formative study with seventeen makers, we designed VirtualComponent, a mixed-reality tool that allows users to digitally place electronic components on a real breadboard, tune their values in software, and see these changes applied to the physical circuit in real-time. VirtualComponent is composed of a set of plug-and-play modules containing banks of components, and a custom breadboard managing the connections and components' values. Through demonstrations and the results of an informal study with twelve makers, we show that VirtualComponent is easy to use and allows users to test components' value configurations with little effort.
SP  - 177
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300407
ER  - 

TY  - NA
AU  - McIntosh, Jess; Zajac, Hubert Dariusz; Stefan, Andreea Nicoleta; Bergström, Joanna; Hornbæk, Kasper
TI  - UIST - Iteratively Adapting Avatars using Task-Integrated Optimisation
PY  - 2020
AB  - Virtual Reality allows users to embody avatars that do not match their real bodies. Earlier work has selected changes to the avatar arbitrarily and it therefore remains unclear how to change avatars to improve users' performance. We propose a systematic approach for iteratively adapting the avatar to perform better for a given task based on users' performance. The approach is evaluated in a target selection task, where the forearms of the avatar are scaled to improve performance. A comparison between the optimised and real arm lengths shows a significant reduction in average tapping time by 18.7%, for forearms multiplied in length by 5.6. Additionally, with the adapted avatar, participants moved their real body and arms significantly less, and subjective measures show reduced physical demand and frustration. In a second study, we modify finger lengths for a linear tapping task to achieve a better performing avatar, which demonstrates the generalisability of the approach.
SP  - 709
EP  - 721
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415832
ER  - 

TY  - NA
AU  - Kovacs, Robert; Ion, Alexandra; Lopes, Pedro; Oesterreich, Tim; Filter, Johannes; Otto, Philipp; Arndt, Tobias; Ring, Nico; Witte, Melvin; Synytsia, Anton; Baudisch, Patrick
TI  - UIST - TrussFormer: 3D Printing Large Kinetic Structures
PY  - 2018
AB  - We present TrussFormer, an integrated end-to-end system that allows users to 3D print large-scale kinetic structures, i.e., structures that involve motion and deal with dynamic forces. TrussFormer builds on TrussFab, from which it inherits the ability to create static large-scale truss structures from 3D printed connectors and PET bottles. TrussFormer adds movement to these structures by placing linear actuators into them: either manually, wrapped in reusable components called assets, or by demonstrating the intended movement. TrussFormer verifies that the resulting structure is mechanically sound and will withstand the dynamic forces resulting from the motion. To fabricate the design, TrussFormer generates the underlying hinge system that can be printed on standard desktop 3D printers. We demonstrate TrussFormer with several example objects, including a 6 legged walking robot and a 4m tall animatronics dinosaur with 5 degrees of freedom.
SP  - 113
EP  - 125
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242607
ER  - 

TY  - NA
AU  - Li, Richard; Wu, Jason; Starner, Thad
TI  - AH - TongueBoard: An Oral Interface for Subtle Input
PY  - 2019
AB  - We present TongueBoard, a retainer form-factor device for recognizing non-vocalized speech. TongueBoard enables absolute position tracking of the tongue by placing capacitive touch sensors on the roof of the mouth. We collect a dataset of 21 common words from four user study participants (two native American English speakers and two non-native speakers with severe hearing loss). We train a classifier that is able to recognize the words with 91.01% accuracy for the native speakers and 77.76% accuracy for the non-native speakers in a user dependent, offline setting. The native English speakers then participate in a user study involving operating a calculator application with 15 non-vocalized words and two tongue gestures at a desktop and with a mobile phone while walking. TongueBoard consistently maintains an information transfer rate of 3.78 bits per decision (number of choices = 17, accuracy = 97.1%) and 2.18 bits per second across stationary and mobile contexts, which is comparable to our control conditions of mouse (desktop) and touchpad (mobile) input.
SP  - 1
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311831
ER  - 

TY  - JOUR
AU  - Bardot, Sandra; Rey, Bradley; Audette, Lucas; Fan, Kevin; Huang, Da-Yuan; Li, Jun; Li, Wei; Irani, Pourang
TI  - One Ring to Rule Them All
PY  - 2022
AB  - <jats:p>Smartrings have potential to extend our ubiquitous control through their always available and finger-worn location, as well as their quick and subtle interactions. As such, smartrings have gained popularity in research and in commercial usage; however, they often concentrate on a singular or novel aspect of a smartring's potential. While with any emerging technology the focus on these individual components is important, there is a lack of broader empirical understanding regarding a user's intentions for smartring usage. Thus in this work, we investigate concrete and reported smartring usage scenarios throughout the daily lives of participants. During a two-week in-situ diary study (N = 14), utilizing a mock smartring, we provide an initial understanding of the potential tasks, daily activities, connected devices, and interactions for which augmentation with a smartring was desired. We further highlight patterns of imagined smartring use found by our participants. Finally, we provide and discuss guidelines, grounded through our found knowledge, to inform research and development towards the design of future smartrings.</jats:p>
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550315
ER  - 

TY  - NA
AU  - Noma, Yuta; Narumi, Koya; Okuya, Fuminori; Kawahara, Yoshihiro
TI  - UIST - Pop-up Print: Rapidly 3D Printing Mechanically Reversible Objects in the Folded State
PY  - 2020
AB  - Despite recent advancements in 3D printing technology, which allows users to rapidly produce 3D objects, printing tall and/or large objects still consumes more time and large amount of support material. In order to address these problems, we propose Pop-up Print, a method to 3D print an object in a compact "folded" state and then unfold it after printing to achieve the final artifact. Using this method, we can reduce the object's print height and volume, which directly affects the printing time and support material consumption. In addition, thanks to the reversibility of folding/unfolding, we can reversibly minimize the printed object's volume when unused for storage or transportation, and expand it only in use. To achieve Pop-up Print, we first conducted an experiment using selected printed sample objects with several parameters, in order to determine suitable crease patterns that make both the unfolded and folded state mechanically stable. Based on this result, we developed an interactive design tool to convert 3D models - such as a Stanford Bunny or a Huffman's cone - to the folded shape. Our design tool allows users to decide non-intuitive parameters that may affect the form's mechanical stability, while maintaining both functional crease patterns and the object's original form factor. Finally, we demonstrate the feasibility of our method through several examples of folded objects.
SP  - 58
EP  - 70
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415853
ER  - 

TY  - NA
AU  - Song, Jean Y.; Fok, Raymond; Lundgard, Alan; Yang, Fan; Kim, Juho; Lasecki, Walter S.
TI  - IUI - Two Tools are Better Than One: Tool Diversity as a Means of Improving Aggregate Crowd Performance
PY  - 2018
AB  - Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this paper, we introduce a novel crowdsourcing workflow that leverages multiple tools for the same task to increase output accuracy by reducing systematic error biases introduced by the tools themselves. When a task can no longer be broken down into more-tractable subtasks (the conventional approach taken by microtask crowdsourcing), our multi-tool approach can be used to further improve accuracy by assigning different tools to different workers. We present a series of studies that evaluate our multi-tool approach and show that it can significantly improve aggregate accuracy in semantic image segmentation.
SP  - 559
EP  - 570
JF  - 23rd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3172944.3172948
ER  - 

TY  - NA
AU  - Cascón, Pablo Gallego; Matthies, Denys J.C.; Muthukumarana, Sachith; Nanayakkara, Suranga
TI  - CHI - ChewIt. An Intraoral Interface for Discreet Interactions
PY  - 2019
AB  - Sensing interfaces relying on head or facial gestures provide effective solutions for hands-free scenarios. Most of these interfaces utilize sensors attached to the face, as well as into the mouth, being either obtrusive or limited in input bandwidth. In this paper, we propose ChewIt -- a novel intraoral input interface. ChewIt resembles an edible object that allows users to perform various hands-free input operations, both simply and discreetly. Our design is informed by a series of studies investigating the implications of shape, size, locations for comfort, discreetness, maneuverability, and obstructiveness. Additionally, we evaluated potential gestures that users could use to interact with such an intraoral interface.
SP  - 326
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300556
ER  - 

TY  - CHAP
AU  - Echeverri, Daniel
TI  - Integrating Brechtian Concepts in the Design of a Tangible Narrative: The Case of "The Non-myth of the Noble Red"
PY  - 2022
AB  - NA
SP  - 383
EP  - 394
JF  - Interactive Storytelling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-22298-6_23
ER  - 

TY  - CHAP
AU  - , 
TI  - Years of Referenced Manuscripts
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544581
ER  - 

TY  - NA
AU  - Girard, Carlos; Calderon, Diego; Lemus, Ali; Ferman, Victor; Fajardo, Julio
TI  - ICMRE - A Motion Mapping System for Humanoids that Provides Immersive Telepresence Experiences
PY  - 2020
AB  - Motion capture and mapping systems have been evolving to enhance the virtual immersion experience in a human-in-the-loop model. In this work, a motion mapping system composed by a 3D printed humanoid robot, built with low-cost materials, an IMU-based motion capture suit, a binaural microphone and a virtual reality headset is presented. The movements of the robot are limited by its reduced amount of degrees of freedom, particularly due to its shoulder configuration, which differs from the human's own biomechanics. Once the motion capture suit's information is extracted, a ROS-based architecture maps orientations from the user to obtain the generalized coordinates of the robot in order to imitate the operator's arm's motion. Additionally, the headset is used to project a stereo vision of the robot's surroundings and to map the operator's head motion. Furthermore, the microphones located on each ear provide the ability to capture 3D sound. This project intends to provide an interactive telepresence puppetry system to encourage the involvement of a targeted audience on engineering subjects. The system shows acceptable results with moderate time response.
SP  - 165
EP  - 170
JF  - 2020 6th International Conference on Mechatronics and Robotics Engineering (ICMRE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icmre49073.2020.9065155
ER  - 

TY  - BOOK
AU  - Fender, Andreas; Müller, Jörg
TI  - ISS - Velt: A Framework for Multi RGB-D Camera Systems
PY  - 2018
AB  - We present Velt, a flexible framework for multi RGB-D camera systems. Velt supports modular real-time streaming and processing of multiple RGB, depth and skeleton streams in a camera network. RGB-D data from multiple devices can be combined into 3D data like point clouds. Furthermore, we present an integrated GUI, which enables viewing and controlling all streams, as well as debugging and profiling performance. The node-based GUI provides access to everything from high level parameters like frame rate to low level properties of each individual device. Velt supports modular preprocessing operations like downsampling and cropping of streaming data. Furthermore, streams can be recorded and played back. This paper presents the architecture and implementation of Velt.
SP  - 73
EP  - 83
JF  - Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3279778.3279794
ER  - 

TY  - NA
AU  - Sun, Wei; Li, Franklin Mingzhe; Huang, Congshu; Lei, Zhenyu; Steeper, Benjamin; Tao, Songyun; Tian, Feng; Zhang, Cheng
TI  - MobileHCI - ThumbTrak: Recognizing Micro-finger Poses Using a Ring with Proximity Sensing
PY  - 2021
AB  - ThumbTrak is a novel wearable input device that recognizes 12 micro-finger poses in real-time. Poses are characterized by the thumb touching each of the 12 phalanges on the hand. It uses a thumb-ring, built with a flexible printed circuit board, which hosts nine proximity sensors. Each sensor measures the distance from the thumb to various parts of the palm or other fingers. ThumbTrak uses a support-vector-machine (SVM) model to classify finger poses based on distance measurements in real-time. A user study with ten participants showed that ThumbTrak could recognize 12 micro finger poses with an average accuracy of 93.6%. We also discuss potential opportunities and challenges in applying ThumbTrak in real-world applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472060
ER  - 

TY  - NA
AU  - Zenner, André; Kruqer, Antonio
TI  - VR - Estimating Detection Thresholds for Desktop-Scale Hand Redirection in Virtual Reality
PY  - 2019
AB  - Virtual reality (VR) interaction techniques like haptic retargeting offset the user's rendered virtual hand from the real hand location to redirect the user's physical hand movement. This paper explores the order of magnitude of hand redirection that can be applied without the user noticing it. By deriving lower-bound estimates of detection thresholds, we quantify the range of unnoticeable redirection for the three basic redirection dimensions, horizontal, vertical and gain-based hand warping. In a two-alternative forced choice (2AFC) experiment, we individually explore these three hand warping dimensions each in three different scenarios: a very conservative scenario without any distraction and two conservative but more realistic scenarios that distract users from the redirection. Additionally, we combine the results of all scenarios to derive robust recommendations for each redirection technique. Our results indicate that within a certain range, desktop-scale VR hand redirection can go unnoticed by the user, but that this range is narrow. The findings show that the virtual hand can be unnoticeably displaced horizontally or vertically by up to 4.5° in either direction, respectively. This allows for a range of ca. 9°, in which users cannot reliably detect applied redirection. For our gain-based hand redirection technique, we found that gain factors between g = 0.88 and g = 1.07 can go unnoticed, which corresponds to a user grasping up to 13.75% further or up to 6.18% less far than in virtual space. Our findings are of value for the development of VR applications that aim to redirect users in an undetectable manner, such as for haptic retargeting.
SP  - 47
EP  - 55
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798143
ER  - 

TY  - JOUR
AU  - Shang, Jiacheng; Wu, Jie
TI  - Voice Liveness Detection for Voice Assistants Through Ear Canal Pressure Monitoring
PY  - 2022
AB  - NA
SP  - 1225
EP  - 1234
JF  - IEEE Transactions on Network Science and Engineering
VL  - 9
IS  - 3
PB  - 
DO  - 10.1109/tnse.2021.3138699
ER  - 

TY  - NA
AU  - Bondareva, Erika; Hauksdóttir, Elín Rós; Mascolo, Cecilia
TI  - UbiComp/ISWC Adjunct - Earables for Detection of Bruxism: a Feasibility Study
PY  - 2021
AB  - Bruxism is a disorder characterised by teeth grinding and clenching, and many bruxism sufferers are not aware of this disorder until their dental health professional notices permanent teeth wear. Stress and anxiety are often listed among contributing factors impacting bruxism exacerbation, which may explain why the COVID-19 pandemic gave rise to a bruxism epidemic. It is essential to develop tools allowing for the early diagnosis of bruxism in an unobtrusive manner. This work explores the feasibility of detecting bruxism-related events using earables in a mimicked in-the-wild setting. Using inertial measurement unit for data collection, we utilise traditional machine learning for teeth grinding and clenching detection. We observe superior performance of models based on gyroscope data, achieving an 88% and 66% accuracy on grinding and clenching activities, respectively, in a controlled environment, and 76% and 73% on grinding and clenching, respectively, in an in-the-wild environment.
SP  - 146
EP  - 151
JF  - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460418.3479327
ER  - 

TY  - NA
AU  - Pezent, Evan; Israr, Ali; Samad, Majed; Robinson, Shea; Agarwal, Priyanshu; Benko, Hrvoje; Colonnese, Nick
TI  - WHC - Tasbi: Multisensory Squeeze and Vibrotactile Wrist Haptics for Augmented and Virtual Reality
PY  - 2019
AB  - Augmented and virtual reality are poised to deliver the next generation of computing interfaces. To fully immerse users, it will become increasingly important to couple visual information with tactile feedback for interactions with the virtual world. Small wearable devices which approximate or substitute for sensations in the hands offer an attractive path forward. In this work, we present Tasbi, a multisensory haptic wristband capable of delivering squeeze and vibrotactile feedback. The device features a novel mechanism for generating evenly distributed and purely normal squeeze forces around the wrist. Our approach ensures that Tasbi’s six radially spaced vibrotactors maintain position and exhibit consistent skin coupling. In addition to experimental device characterization, we present early explorations into Tasbi’s utility as a sensory substitution device for hand interactions, employing squeeze, vibration, and pseudo-haptic effects to render a highly believable virtual button.
SP  - 1
EP  - 6
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816098
ER  - 

TY  - NA
AU  - Cheng, Yifei; Yan, Yukang; Yi, Xin; Shi, Yuanchun; Lindlbauer, David
TI  - UIST - SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections
PY  - 2021
AB  - We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated for MR systems to be beneficial for end users. We contribute an approach that formulates this challenge as a combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. To achieve this, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous layouts. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic associations into account, our approach decreased the number of manual interface adaptations by 33%.
SP  - 282
EP  - 297
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474750
ER  - 

TY  - NA
AU  - Kim, Yoonji; Lee, Hyein; Prasad, Ramkrishna; Je, Seungwoo; Choi, Youngkyung; Ashbrook, Daniel; Oakley, Ian; Bianchi, Andrea
TI  - UIST - SchemaBoard: Supporting Correct Assembly of Schematic Circuits using Dynamic In-Situ Visualization
PY  - 2020
AB  - Assembling circuits on breadboards using reference designs is a common activity among makers. While tools like Fritzing offer a simplified visualization of how components and wires are connected, such pictorial depictions of circuits are rare in formal educational materials and the vast bulk of online technical documentation. Electronic schematics are more common but are perceived as challenging and confusing by novice makers. To improve access to schematics, we propose SchemaBoard, a system for assisting makers in assembling and inspecting circuits on breadboards from schematic source materials. SchemaBoard uses an LED matrix integrated underneath a working breadboard to visualize via light patterns where and how components should be placed, or to highlight elements of circuit topology such as electrical nets and connected pins. This paper presents a formative study with 16 makers, the SchemaBoard system, and a summative evaluation with an additional 16 users. Results indicate that SchemaBoard is effective in reducing both the time and the number of errors associated with building a circuit based on a reference schematic, and for inspecting the circuit for correctness after its assembly.
SP  - 987
EP  - 998
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415887
ER  - 

TY  - JOUR
AU  - Suguitan, Michael; Hoffman, Guy
TI  - What is it like to be a bot? Variable perspective embodied telepresence for crowdsourcing robot movements.
PY  - 2022
AB  - Movement and embodiment are communicative affordances central to social robotics, but designing embodied movements for robots often requires extensive knowledge of both robotics and movement theory. More accessible methods such as learning from demonstration often rely on physical access to the robot which is usually limited to research settings. Machine learning (ML) algorithms can complement hand-crafted or learned movements by generating new behaviors, but this requires large and diverse training datasets, which are hard to come by. In this work, we propose an embodied telepresence system for remotely crowdsourcing emotive robot movement samples that can serve as ML training data. Remote users control the robot through the internet using the motion sensors in their smartphones and view the movement either from a first-person or a third-person perspective. We evaluated the system in an online study where users created emotive movements for the robot and rated their experience. We then utilized the user-crafted movements as inputs to a neural network to generate new movements. We found that users strongly preferred the third-person perspective and that the ML-generated movements are largely comparable to the user-crafted movements. This work supports the usability of telepresence robots as a movement crowdsourcing platform.
SP  - 1
EP  - NA
JF  - Personal and ubiquitous computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s00779-022-01684-y
ER  - 

TY  - NA
AU  - Matthews, Brandon J.; Thomas, Bruce H.; Von Itzstein, G. Stewart; Smith, Ross T.
TI  - Shape Aware Haptic Retargeting for Accurate Hand Interactions
PY  - 2022
AB  - This paper presents Shape Aware Haptic Retargeting, an extension of "state-of-the-art" haptic retargeting that is the first to support retargeted interaction between any part of the user&#x2019;s hand and any part of the target object. In previous haptic retargeting algorithms, the maximum retargeting is applied only when the hand position aligns with the target position. Shape Aware Haptic Retargeting generalizes the distance computation process to instead consider the hand and target geometry. The shortest hand-target distance is then used to calculate the applied retargeting offset. This ensures the full amount of haptic retargeting is applied at the point of contact with the passive haptic regardless of contact position on the hand or target. We leverage existing geometry algorithms to implement three distance computation methods: Multi-Point, Primitive and Mesh Geometry, in addition to conventional single position approaches. These are evaluated through a set of simulated interactions instead of the single position representation used in previous haptic retargeting systems. The evaluation demonstrated all three approaches can provide improved interaction accuracy over a Point distance computation method, with Mesh Geometry being the most accurate and Primitive being the preferred method for combined performance and interaction accuracy.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00083
ER  - 

TY  - NA
AU  - Frich, Jonas; Vermeulen, Lindsay MacDonald; Remy, Christian; Biskjaer, Michael Mose; Dalsgaard, Peter
TI  - CHI - Mapping the Landscape of Creativity Support Tools in HCI
PY  - 2019
AB  - Creativity Support Tools (CSTs) play a fundamental role in the study of creativity in Human-Computer Interaction (HCI). Even so, there is no consensus definition of the term 'CST' in HCI, and in most studies, CSTs have been construed as one-off exploratory prototypes, typically built by the researchers themselves. This makes it difficult to clearly demarcate CST research, but also to compare findings across studies, which impedes advancement in digital creativity as a growing field of research. Based on a literature review of 143 papers from the ACM Digital Library (1999-2018), we contribute a first overview of the key characteristics of CSTs developed by the HCI community. Moreover, we propose a tentative definition of a CST to help strengthen knowledge sharing across CST studies. We end by discussing our study's implications for future HCI research on CSTs and digital creativity.
SP  - 389
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300619
ER  - 

TY  - NA
AU  - Sharma, Adwait; Hedderich, Michael A.; Bhardwaj, Divyanshu; Fruchard, Bruno; McIntosh, Jess; Nittala, Aditya Shekhar; Klakow, Dietrich; Ashbrook, Daniel; Steimle, Jürgen
TI  - CHI - SoloFinger: Robust Microgestures while Grasping Everyday Objects
PY  - 2021
AB  - Using microgestures, prior work has successfully enabled gestural interactions while holding objects. Yet, these existing methods are prone to false activations caused by natural finger movements while holding or manipulating the object. We address this issue with SoloFinger, a novel concept that allows design of microgestures that are robust against movements that naturally occur during primary activities. Using a data-driven approach, we establish that single-finger movements are rare in everyday hand-object actions and infer a single-finger input technique resilient to false activation. We demonstrate this concept’s robustness using a white-box classifier on a pre-existing dataset comprising 36 everyday hand-object actions. Our findings validate that simple SoloFinger gestures can relieve the need for complex finger configurations or delimiting gestures and that SoloFinger is applicable to diverse hand-object actions. Finally, we demonstrate SoloFinger’s high performance on commodity hardware using random forest classifiers.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445197
ER  - 

TY  - NA
AU  - Neshati, Ali; Alallah, Fouad; Rey, Bradley; Sakamoto, Yumiko; Serrano, Marcos; Irani, Pourang
TI  - MobileHCI - SF-LG: Space-Filling Line Graphs for Visualizing Interrelated Time-series Data on Smartwatches
PY  - 2021
AB  - Multiple embedded sensors enable smartwatch apps to amass large amounts of interrelated time-series data simultaneously, such as heart rate, oxygen levels or steps walked. Visualizing multiple interlinked datasets is possible on smartphones but remains challenging on small smartwatch displays. We propose a new technique, the Space-Filling Line Graph (SF-LG), that preserves the key visual properties of time-series graphs while making available space on the display to augment such graphs with additional information. Results from our first study (N=30) suggest that, while SF-LG makes available additional space on the small display, it also enables effective (i.e. quick and accurate) comprehension of key line graph tasks. We next implement a greedy algorithm to embed auxiliary information in the most suitable regions on the display. In a second study (N=27), we find that participants are efficient at locating and linking interrelated content using SF-LG in comparison to two baselines approaches. We conclude with guidelines for smartwatch space maximization for visual displays.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472040
ER  - 

TY  - JOUR
AU  - Foo, Esther; Dunne, Lucy E.; Holschuh, Brad
TI  - User Expectations and Mental Models for Communicating Emotions through Compressive & Warm Affective Garment Actuation
PY  - 2021
AB  - Wearable haptic garments for communicating emotions have great potential in various applications, including supporting social interactions, improving immersive experiences in entertainment, or simply as a research tool. Shape-memory alloys (SMAs) are an emerging and interesting actuation scheme for affective haptic garments since they provide coupled warmth and compressive sensations in a single actuation---potentially acting as a proxy for human touch. However, SMAs are underutilized in current research and there are many unknowns regarding their design/use. The goal of this work is to map the design space for SMA-based garment-mediated emotional communication through warm, compressive actuation (termed 'warm touch'). Two online surveys were deployed to gather user expectations in using varying 'warm touch' parameters (body location, intensity, pattern) to communicate 7 distinct emotions. Further, we also investigated mental models used by participants during the haptic strategy selection process. The findings show 5 major categories of mental models, including representation of body sensations, replication of typical social touch strategies, metaphorical representation of emotions, symbolic representation of physical actions, and mimicry of objects or tasks; the frequency of use of each of these mental frameworks in relation to the selected 'warm touch' parameters in the communication of emotions are presented. These gathered insights can inform more intuitive and consistent haptic garment design approaches for emotional communication.
SP  - 3448097
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 1
PB  - 
DO  - 10.1145/3448097
ER  - 

TY  - NA
AU  - Warner, Jeremy; Lafreniere, Ben; Fitzmaurice, George; Grossman, Tovi
TI  - UIST - ElectroTutor: Test-Driven Physical Computing Tutorials
PY  - 2018
AB  - A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users' success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.
SP  - 435
EP  - 446
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242591
ER  - 

TY  - NA
AU  - Wentzel, Johann; d'Eon, Greg; Vogel, Daniel
TI  - CHI - Improving Virtual Reality Ergonomics Through Reach-Bounded Non-Linear Input Amplification
PY  - 2020
AB  - Input amplification enables easier movement in virtual reality (VR) for users with mobility issues or in confined spaces. However, current techniques either do not focus on maintaining feelings of body ownership, or are not applicable to general VR tasks. We investigate a general purpose non-linear transfer function that keeps the user's reach within reasonable bounds to maintain body ownership. The technique amplifies smaller movements from a user-definable neutral point into the expected larger movements using a configurable Hermite curve. Two experiments evaluate the approach. The first establishes that the technique has comparable performance to the state-of-the-art, increasing physical comfort while maintaining task performance and body ownership. The second explores the characteristics of the technique over a wide range of amplification levels. Using the combined results, design and implementation recommendations are provided with potential applications to related VR transfer functions.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376687
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Huang, Da-Yuan; Wang, Chi; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong; Chen, Bing-Yu
TI  - CHI - Aarnio: Passive Kinesthetic Force Output for Foreground Interactions on an Interactive Chair
PY  - 2019
AB  - We propose a new type of haptic output for foreground interactions on an interactive chair, where input is carried out explicitly in the foreground of the user's consciousness. This type of force output restricts a user's motion by modulating the resistive force when rotating a seat, tilting the backrest, or rolling the chair. These interactions are useful for many applications in a ubiquitous computing environment, ranging from immersive VR games to rapid and private query of information for people who are occupied with other tasks (e.g. in a meeting). We carefully designed and implemented our proposed haptic force output on a standard office chair and determined the recognizability of five force profiles for rotating, tilting, and rolling the chair. We present the result of our studies, as well as a set of novel interaction techniques enabled by this new force output for chairs.
SP  - 672
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300902
ER  - 

TY  - JOUR
AU  - Jun, Heesu; Lee, Minhee; Kim, Hyosu
TI  - WindTrack: Leveraging Sound and Wind to Track Angular Position of Users on Commercial Internet-Connected Fans
PY  - 2021
AB  - A recent trend in electric fans is the support of smarter features based on Internet connectivity, such as remote control. This work opens new opportunities to make these devices more intelligent, particularly by proposing WindTrack, a novel angular positioning technique for electric fans. This would encourage the emergence of a new class of smart fans that use spatial information for automatic wind direction control. Many works have extensively studied angular positioning, but had limitations realizing such smart fans in terms of deployability (due to hardware requirements), accuracy, and usability. WindTrack overcomes these limitations by leveraging the acoustic phenomenon of refraction caused by wind. More specifically, it transmits and receives sound signals by using a smartphone located close to a user, while operating a fan in an oscillating mode. Therefore, WindTrack only requires Internet-connected fans for collaboration with smartphones. It then identifies the user’s angular position based on the relationship between the propagation characteristics of the received signals and the oscillating fan. We further improve the robustness of WindTrack by using multiple speakers and microphones built-in smartphones and listening to wind sounds captured when wind passes across microphones. Extensive experiments on a WindTrack prototype demonstrate that it can achieve an angular positioning accuracy of a few degrees, especially in various environments and with no modification to commercial smartphones and Internet-connected fans.
SP  - 11690
EP  - 11704
JF  - IEEE Internet of Things Journal
VL  - 8
IS  - 14
PB  - 
DO  - 10.1109/jiot.2021.3059870
ER  - 

TY  - NA
AU  - Yamaoka, Junichi
TI  - SIGGRAPH Posters - Rapid and Shape-Changing Digital Fabrication Using Magnetic Thermoplastic Material
PY  - 2020
AB  - In this research, we propose a digital fabrication method that can print reusable objects rapidly using the clay material. Therefore, we developed a magnetic force control device and a magnetic thermoplastic material that deforms dynamically. When the user creates a shape model on the screen, the clay material instantly changes to the shape of the designed model. The user can confirm the shape in a short time, and the material can be reused. In this paper, we describe the design and implementation of magnetic materials and control devices, evaluation, and future work.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2020 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3388770.3407439
ER  - 

TY  - NA
AU  - Hinchet, Ronan; Vechev, Velko; Shea, Herbert; Hilliges, Otmar
TI  - UIST - DextrES: Wearable Haptic Feedback for Grasping in VR via a Thin Form-Factor Electrostatic Brake
PY  - 2018
AB  - We introduce DextrES, a flexible and wearable haptic glove which integrates both kinesthetic and cutaneous feedback in a thin and light form factor (weight is less than 8g). Our approach is based on an electrostatic clutch generating up to 20 N of holding force on each finger by modulating the electrostatic attraction between flexible elastic metal strips to generate an electrically-controlled friction force. We harness the resulting braking force to rapidly render on-demand kinesthetic feedback. The electrostatic brake is mounted onto the the index finger and thumb via modular 3D printed articulated guides which allow the metal strips to glide smoothly. Cutaneous feedback is provided via piezo actuators at the fingertips. We demonstrate that our approach can provide rich haptic feedback under dexterous articulation of the user's hands and provides effective haptic feedback across a variety of different grasps. A controlled experiment indicates that DextrES improves the grasping precision for different types of virtual objects. Finally, we report on results of a psycho-physical study which identifies discrimination thresholds for different levels of holding force.
SP  - 901
EP  - 912
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242657
ER  - 

TY  - NA
AU  - Xu, Zheer; Wong, Pui Chung; Gong, Jun; Wu, Te-Yen; Nittala, Aditya Shekhar; Bi, Xiaojun; Steimle, Jürgen; Fu, Hongbo; Zhu, Kening; Yang, Xing-Dong
TI  - UIST - TipText: Eyes-Free Text Entry on a Fingertip Keyboard
PY  - 2019
AB  - In this paper, we propose and investigate a new text entry technique using micro thumb-tip gestures. Our technique features a miniature QWERTY keyboard residing invisibly on the first segment of the user's index finger. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The keyboard layout was optimized for eyes-free input by utilizing a spatial model reflecting the users' natural spatial awareness of key locations on the index finger. We present our approach of designing and optimizing the keyboard layout through a series of user studies and computer simulated text entry tests over 1,146,484 possibilities in the design space. The outcome is a 2×3 grid with the letters highly confining to the alphabetic and spatial arrangement of QWERTY. Our user evaluation showed that participants achieved an average text entry speed of 11.9 WPM and were able to type as fast as 13.3 WPM towards the end of the experiment.
SP  - 883
EP  - 899
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347865
ER  - 

TY  - NA
AU  - Strasnick, Evan; Follmer, Sean; Agrawala, Maneesh
TI  - CHI - Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation
PY  - 2019
AB  - Difficulties in accessing, isolating, and iterating on the components and connections of a printed circuit board (PCB) create unique challenges in PCB debugging. Manual probing methods are slow and error prone, and even dedicated PCB testing equipment remains limited by its inability to modify the circuit during testing. We present Pinpoint, a tool that facilitates in-circuit PCB debugging through techniques such as programmatically probing signals, dynamically disconnecting components and subcircuits to test in isolation, and splicing in new elements to explore potential modifications. Pinpoint automatically instruments a PCB design and generates designs for a physical jig board that interfaces the user's PCB to our custom testing hardware and to software tools. We evaluate Pinpoint's ability to facilitate the debugging of various PCB issues by instrumenting and testing different classes of boards, as well as by characterizing its technical limitations and by soliciting feedback through a guided exploration with PCB designers.
SP  - 48
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300278
ER  - 

TY  - NA
AU  - Parzer, Patrick; Perteneder, Florian; Probst, Kathrin; Rendl, Christian; Leong, Joanne; Schuetz, Sarah; Vogl, Anita; Schwoediauer, Reinhard; Kaltenbrunner, Martin; Bauer, Siegfried; Haller, Michael J.
TI  - UIST - RESi: A Highly Flexible, Pressure-Sensitive, Imperceptible Textile Interface Based on Resistive Yarns
PY  - 2018
AB  - We present RESi (Resistive tExtile Sensor Interfaces), a novel sensing approach enabling a new kind of yarn-based, resistive pressure sensing. The core of RESi builds on a newly designed yarn, which features conductive and resistive properties. We run a technical study to characterize the behaviour of the yarn and to determine the sensing principle. We demonstrate how the yarn can be used as a pressure sensor and discuss how specific issues, such as connecting the soft textile sensor with the rigid electronics can be solved. In addition, we present a platform-independent API that allows rapid prototyping. To show its versatility, we present applications developed with different textile manufacturing techniques, including hand sewing, machine sewing, and weaving. RESi is a novel technology, enabling textile pressure sensing to augment everyday objects with interactive capabilities.
SP  - 745
EP  - 756
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242664
ER  - 

TY  - JOUR
AU  - Iqbal, Hasan; Latif, Seemab; Yan, Yukang; Yu, Chun; Shi, Yuanchun
TI  - Reducing Arm Fatigue in Virtual Reality by Introducing 3D-Spatial Offset
PY  - 2021
AB  - Arm fatigue is an important factor affecting user experience in Virtual Reality (VR). In this work, we have proposed ProxyHand and StickHand, virtual hand techniques to address this issue. Using ProxyHand or StickHand, users can flexibly adjust the 3D-spatial offset between the physical hand and its virtual representation. This will allow users to keep their arms in a comfortable posture (vertically down) even when they have to manipulate objects in locations that require lifting of arms using the default interaction method. Proposed ProxyHand and StickHand have a similar Underlying concept that is to introduce a 3D-spatial offset between the physical hand and its virtual representation in VR. However, they respond differently to the user’s hand movements because of different working mechanisms. Question arises whether the 3D-spatial offset will negatively impact the hand control ability as the directness of interaction is being violated. To investigate this, we conducted user studies where users were asked to perform object translation, rotation and hybrid tasks. ProxyHand and StickHand are used in combination in some scenarios to maximize positive impact on the user experience in VR. This raises the question to find the best possible combination of these virtual hands to reduce arm fatigue. Firstly, for this purpose, we combined both virtual hands by manually allowing users to switch between ProxyHand and StickHand. Secondly, we used machine learning to automatically switch between both the virtual hands. Results showed that introduction of a 3D-spatial offset largely reduced the arm fatigue while offering equal performance to the default interaction method for all these tasks; translation, rotation and hybrid task. Users preferred using ProxyHand and StickHand to interact in the VR environment for longer periods of time.
SP  - 64085
EP  - 64104
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3075769
ER  - 

TY  - NA
AU  - Kubo, Yuki; Koguchi, Yuto; Shizuki, Buntarou; Takahashi, Shin; Hilliges, Otmar
TI  - MobileHCI - AudioTouch: Minimally Invasive Sensing of Micro-Gestures via Active Bio-Acoustic Sensing
PY  - 2019
AB  - We present AudioTouch, a minimally invasive approach for sensing micro-gestures using active bio-acoustic sensing. It only requires attaching two piezo-electric elements, acting as a surface mounted speaker and microphone, on the back of the hand. It does not require any instrumentation on the palm or fingers; therefore, it does not encumber interactions with physical objects. The signal is rich enough to detect small differences in micro-gestures with standard machine-learning classifiers. This approach also allows for the discrimination of different levels of touch-force, further expanding the interaction vocabulary. We conducted four experiments to evaluate the performances of AudioTouch: a user study for measuring the gesture recognition accuracy, a follow-up study investigating the ability to discriminate different levels of touch-force, an experiment assessing the cross-session robustness, and, a systematic evaluation assessing the effect of sensor placement on the back of the hand.
SP  - 36
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340147
ER  - 

TY  - NA
AU  - Groeger, Daniel; Feick, Martin; Withana, Anusha; Steimle, Jürgen
TI  - UIST - Tactlets: Adding Tactile Feedback to 3D Objects Using Custom Printed Controls
PY  - 2019
AB  - Rapid prototyping of haptic output on 3D objects promises to enable a more widespread use of the tactile channel for ubiquitous, tangible, and wearable computing. Existing prototyping approaches, however, have limited tactile output capabilities, require advanced skills for design and fabrication, or are incompatible with curved object geometries. In this paper, we present a novel digital fabrication approach for printing custom, high-resolution controls for electro-tactile output with integrated touch sensing on interactive objects. It supports curved geometries of everyday objects. We contribute a design tool for modeling, testing, and refining tactile input and output at a high level of abstraction, based on parameterized electro-tactile controls. We further contribute an inventory of 10 parametric Tactlet controls that integrate sensing of user input with real-time electro-tactile feedback. We present two approaches for printing Tactlets on 3D objects, using conductive inkjet printing or FDM 3D printing. Empirical results from a psychophysical study and findings from two practical application cases confirm the functionality and practical feasibility of the Tactlets approach.
SP  - 923
EP  - 936
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347937
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Chiang, Chi-huan; Hsu, Shuo-wen; Yang, Chih-Yun; Huang, Da-Yuan; Chen, Bing-Yu
TI  - SUI - TanGo: Exploring Expressive Tangible Interactions on Head-Mounted Displays
PY  - 2020
AB  - We present TanGo, an always-available input modality on VR headset, which can be complementary to current VR accessories. TanGO is an active mechanical structure symmetrically equipped on Head-Mounted Display, enabling 3-dimensional bimanual sliding input with each degree of freedom furnished a brake system driven by micro servo generating totally 6 passive resistive force profiles. TanGo is an all-in-one structure that possess rich input and output while keeping compactness with the trade-offs between size, weight and usability. Users can actively gesture like pushing, shearing, or squeezing with specific output provided while allowing hands to rest in stationary experiences. TanGo also renders users flexibility to switch seamlessly between virtual and real usage in Augmented Reality without additional efforts and instruments. We demonstrate three applications to show the interaction space of TanGo and then discuss its limitation and show future possibilities based on preliminary user feedback.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418457
ER  - 

TY  - NA
AU  - Boem, Alberto; Troiano, Giovanni Maria
TI  - Conference on Designing Interactive Systems - Non-Rigid HCI: A Review of Deformable Interfaces and Input
PY  - 2019
AB  - Deformable interfaces are emerging in HCI and prototypes show potential for non-rigid interactions. Previous reviews looked at deformation as a material property of shape-changing interfaces and concentrated on output. As such,deformable input was under-discussed. We distinguish deformable from shape-changing interfaces to concentrate on input. We survey 131 papers on deformable interfaces and review their key design elements (e.g., shape, material) based on how they support input. Our survey shows that deformable input was often used to augment or replace rigid input, particularly on elastic and flexible displays. However, when shapes and materials guide interactions, deformable input was used to explore new HCI paradigms, where gestures are potentially endless, and input become analogy to sculpting, metaphor to non-verbal communication, and expressive controls are enhanced. Our review provides designers and practitioners with a baseline for designing deformable interfaces and input methodically. We conclude by highlighting under-explored areas and identify research goals to tackle in future work with deformable interfaces.
SP  - 885
EP  - 906
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322347
ER  - 

TY  - NA
AU  - Davis, Josh Urban; Gong, Jun; Sun, Yunxin; Chilana, Parmit K.; Yang, Xing-Dong
TI  - UIST - CircuitStyle: A System for Peripherally Reinforcing Best Practices in Hardware Computing
PY  - 2019
AB  - Instructors of hardware computing face many challenges including maintaining awareness of student progress, allocating their time adequately between lecturing and helping individual students, and keeping students engaged even while debugging problems. Based on formative interviews with 5 electronics instructors, we found that many circuit style behaviors could help novice users prevent or efficiently debug common problems. Drawing inspiration from the software engineering practice of coding style, these circuit style behaviors consist of best-practices and guidelines for implementing circuit prototypes that do not interfere with the functionality of the circuit, but help a circuit be more readable, less error-prone, and easier to debug. To examine if these circuit style behaviors could be peripherally enforced, aid an in-person instructor's ability to facilitate a workshop, and not monopolize instructor's attention, we developed CircuitStyle, a teaching aid for in-person hardware computing workshops. To evaluate the effectiveness of our tool, we deployed our system in an in-person maker-space workshop. The instructor appreciated CircuitStyle's ability to provide a broad understanding of the workshop's progress and the potential for our system to help instructors of various backgrounds better engage and understand the needs of their classroom.
SP  - 109
EP  - 120
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347920
ER  - 

TY  - NA
AU  - Huang, Kunpeng; Sun, Ruojia; Zhang, Ximeng; Molla, Tahmidul Islam; Dunne, Margaret; Guimbretière, François; Kao, Cindy Hsin-Liu
TI  - Conference on Designing Interactive Systems - WovenProbe: Probing Possibilities for Weaving Fully-Integrated On-Skin Systems Deployable in the Field
PY  - 2021
AB  - On-skin interfaces demonstrate great potential given their direct skin contact; however, conducting field studies of these devices outside of laboratories and in real settings remains a challenge. We conduct a research-through-design investigation using an extended woven practice for fabricating fully-integrated and untethered multi-sensor on-skin systems that are resilient, versatile, and capable of field deployment. We designed, implemented, and deployed a woven on-skin index-finger and thumb-based inertial measurement unit (IMU) sensing system for multi-hour use as a technology probe to understand the social, technical, and design facets towards moving integrated on-skin systems into a wearer’s daily life. Further, we integrate a woven NFC coil into the IMU on-skin system, which is wirelessly powered by a smartwatch substitute, signifying the potential of our woven approach for developing wirelessly powered on-skin systems for longer-term continuous wear. Our investigation and the lessons learned shed light on the opportunities for designing on-skin systems for everyday wear.
SP  - 1143
EP  - 1158
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462105
ER  - 

TY  - JOUR
AU  - Roozendaal, Tjark; Verwaal, Martin; Buso, Alice; Scharff, Rob B N; Song, Yu; Vink, Peter
TI  - Development of a Soft Robotics Module for Active Control of Sitting Comfort.
PY  - 2022
AB  - Sitting comfort is an important factor for passengers in selecting cars, airlines, etc. This paper proposes a soft robotic module that can be integrated into the seat cushion to provide better comfort experiences to passengers. Building on rapid manufacturing technologies and a data-driven approach, the module can be controlled to sense the applied force and the displacement of the top surface and actuate according to four designed modes. A total of 2 modules were prototyped and integrated into a seat cushion, and 16 subjects were invited to test the module's effectiveness. Experiments proved the principle by showing significant differences regarding (dis)comfort. It was concluded that the proposed soft robotics module could provide passengers with better comfort experiences by adjusting the pressure distribution of the seat as well as introducing a variation of postures relevant for prolonged sitting.
SP  - 477
EP  - 477
JF  - Micromachines
VL  - 13
IS  - 3
PB  - 
DO  - 10.3390/mi13030477
ER  - 

TY  - CHAP
AU  - Fritzsche, Albrecht
TI  - The Artefact on Stage – Object Theatre and Philosophy of Engineering and Technology
PY  - 2021
AB  - Philosophical approaches to engineering tend to use the technical artefact as a starting point. From the artefact onwards, they look at its design and its importance in human life. All this, however, only seems possible if the artefact is somehow put in an exposed position, where it becomes available as a reference for further investigation. A similar kind of exposure seems to take place in theatre when something is put on stage. This chapter therefore investigates how the study of theatre may provide a valuable contribution to philosophy of technology. The investigation focusses on object theatre, where the treatment of artefacts plays a particularly important role. After a conceptual clarification of the approach, the paper discusses two recent productions concerned with engineering and technology in more detail.
SP  - 309
EP  - 321
JF  - Engineering and Philosophy
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-70099-7_16
ER  - 

TY  - NA
AU  - Oney, Steve; Lundgard, Alan; Krosnick, Rebecca; Nebeling, Michael; Lasecki, Walter S.
TI  - UIST - Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture
PY  - 2018
AB  - Many web pages developed today require navigation by visual interaction-seeing, hovering, pointing, clicking, and dragging with the mouse over dynamic page content. These forms of interaction are increasingly popular as developer trends have moved from static, logically structured pages to dynamic, interactive pages. However, they are also often inaccessible to blind web users who tend to rely on keyboard-based screen readers to navigate the web. Despite existing web accessibility standards, engineering web pages to be equally accessible via both keyboard and visuomotor mouse-based interactions is often not a priority for developers. Improving access to this kind of visual and interactive web content has been a long-standing goal of HCI researchers, but the barriers have proven to be too varied and unpredictable to be overcome by some of the proposed solutions: promoting guidelines and best practices, automatically generating accessible versions of pre-exisiting web pages, or developing human-assisted solutions, such as screen and cursor-sharing, which tend to diminish an end user's agency. In this paper we present a real-time, collaborative approach to helping blind web users overcome inaccessible parts of existing web pages. We introduce *Arboretum*, a new architecture that enables any web user to seamlessly hand off controlled parts of their browsing session to remote users, while maintaining control over the interface via a "propose and accept/reject" mechanism. We illustrate the benefit of Arboretum by using it to implement *Arbility*, a browser that allows blind users to hand off targeted visual interaction tasks to remote crowd workers. We evaluate the entire system in a study with 9 blind web users, showing that Arbility allows them to interact with web content that was previously difficult to access via a screen reader alone.
SP  - 937
EP  - 949
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242649
ER  - 

TY  - BOOK
AU  - Qi, Qiukai; Yoshida, Shogo; Kakihana, Genki; Torii, Takuma; Van Anh Ho, NA; Xie, Haoran
TI  - RoboSoft - BPActuators: Lightweight and Low-Cost Soft Actuators by Balloons and Plastics
PY  - 2021
AB  - To increase the awareness and impact, soft robotics needs to go beyond the lab environment and should be readily accessible to those even with no robotic expertise. However, most prevailing manufacturing technologies require either professional equipment or materials which usually are not available to common people, thereby constraining the accessibility. In this communication, we propose a lightweight and low-cost soft bending actuator, called "BPActuator", that can be easily manufactured with Balloons and Plastics. For characterization, we fabricated a range of actuators with different morphology and tested in terms of the capability of deformation and load-bearing. We demonstrated that they can bend up to 35 degrees and exert force at the tip around 0.070±0.015N, which is over 5 times higher than their average gravity. We further implemented a gripper of three fingers using BPActuators, and found that the gripper can realize humanlike grasp of a range of daily objects. The gripper can lift objects at least 8 times heavier than its own weight. Moreover, BPActuators are cost effective and each costs about 0.22USD. Given all the advantages, we believe that the BPActuators will significantly improve the accessibility of soft robotics.
SP  - 559
EP  - 562
JF  - 2021 IEEE 4th International Conference on Soft Robotics (RoboSoft)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/robosoft51838.2021.9479188
ER  - 

TY  - NA
AU  - Li, Zhen; Fan, Mingming; Han, Ying; Truong, Khai N.
TI  - HUMA @ ACM Multimedia - iWink: Exploring Eyelid Gestures on Mobile Devices
PY  - 2020
AB  - Although gaze has been widely studied for mobile interactions, eyelid-based gestures are relatively understudied and limited to few basic gestures (e.g., blink). In this work, we propose a gesture grammar to construct both basic and compound eyelid gestures. We present an algorithm to detect nine eyelid gestures in real-time on mobile devices and evaluate its performance with 12 participants. Results show that our algorithm is able to recognize nine eyelid gestures with 83% and 78% average accuracy using user-dependent and user-independent models respectively. Further, we design a gesture mapping scheme to allow for navigating between and within mobile apps only using eyelid gestures. Moreover, we show how eyelid gestures can be used to enable cross-application and sensitive interactions. Finally, we highlight future research directions.
SP  - 83
EP  - 89
JF  - Proceedings of the 1st International Workshop on Human-centric Multimedia Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3422852.3423479
ER  - 

TY  - JOUR
AU  - Vyas, Preeti; Taha, Feras Al; Blum, Jeffrey R.; Weill-Duflos, Antoine; Cooperstock, Jeremy R.
TI  - Ten Little Fingers, Ten Little Toes: Can Toes Match Fingers for Haptic Discrimination?
PY  - 2020
AB  - In comparison with fingers, toes are relatively unexplored candidates for multi-site haptic rendering. This is likely due to their reported susceptibility to erroneous perception of haptic stimuli, owing to their anatomical structure. We hypothesize that this shortcoming can be mitigated by careful design of the tactile encoding to account for the idiosyncrasies of toe perception. Our efforts to design such an encoding achieved an improved perceptual accuracy of 18% for poking and 16% for vibrotactile stimuli. As we demonstrate, in this article, the resulting perceptual accuracy achieved by the proposed tactile encoding approaches that of the fingers, allowing for consideration of the toes as a practical location to render multi-site haptic stimuli.
SP  - 130
EP  - 136
JF  - IEEE transactions on haptics
VL  - 13
IS  - 1
PB  - 
DO  - 10.1109/toh.2020.2966969
ER  - 

TY  - NA
AU  - Ogura, Ayumu; Ito, Kodai; Itoh, Yuichi
TI  - Transtiff: A Stick Interface with Various Stiffness by Artificial Muscle Mechanism
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558777
ER  - 

TY  - NA
AU  - Ye, Yuan-Syun; Chen, Hsin-Yu; Chan, Liwei
TI  - UIST - Pull-Ups: Enhancing Suspension Activities in Virtual Reality with Body-Scale Kinesthetic Force Feedback
PY  - 2019
AB  - We present Pull-Ups, a suspension kit that can suggest a range of body postures and thus enables various exercise styles of users perceiving the kinesthetic force feedback by suspending their weight with arm exertion during the interaction. Pull-Ups actuates the user's body to move up to 15 cm by pulling his or her hands using a pair of pneumatic artificial muscle groups. Our studies informed the discernible kinesthetic force feedbacks that were then exploited for the design of kinesthetic force feedback in three physical activities: kitesurfing, paragliding, and space invader. Our final study on user experiences suggested that a passive suspension kit alone added substantially to users' perceptions of realism and enjoyment (all above neutral) with passive physical support, while sufficient active feedback can further level them up. In addition, we found that both passive and active feedback of the suspension kit significantly reduced motion sickness in simulated kitesurfing and paragliding compared to when no suspension kit (thus no feedback) was provided. This work suggests that a passive suspension kit is cost-effective as a home exercise kit, while active feedback can further level up user experience, though at the cost of the installation (e.g., an air compressor in our prototype).
SP  - 791
EP  - 801
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347874
ER  - 

TY  - NA
AU  - Zhao, Zhenjie; Han, Feng; Ma, Xiaojuan
TI  - AIVR - A Live Storytelling Virtual Reality System with Programmable Cartoon-Style Emotion Embodiment
PY  - 2019
AB  - Virtual reality (VR) is a promising new medium for immersive storytelling. While previous research works on VR narrative have tried to engage audiences through nice scenes and interactivity, the emerging live streaming shows the role of a presenter, especially the conveyance of emotion, for promoting audience involvement and enjoyment. In this paper, to lower the requirement of emotion embodiment, we borrow experience from cartoon animation and comics, and propose a novel cartoon-style hybrid emotion embodiment model to increase a storyteller's presence during live performance, which contains an avatar with six basic emotions and auxiliary multimodal display to enhance emotion expressing. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. In particular, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our lightweight web-based implementation also makes the application very easy to use. We conduct two preliminary qualitative studies to explore the potential of the hybrid model and the storytelling system, including interviews with three experts and a workshop study with local secondary school students. Results show the potential of the VR storytelling system for education.
SP  - 102
EP  - 109
JF  - 2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr46125.2019.00024
ER  - 

TY  - NA
AU  - Zenner, André; Kriegler, Hannah Maria; Krüger, Antonio
TI  - CHI Extended Abstracts - HaRT - The Virtual Reality Hand Redirection Toolkit
PY  - 2021
AB  - Past research has proposed various hand redirection techniques for virtual reality (VR). Such techniques modify a user’s hand movements and have been successfully used to enhance haptics and 3D user interfaces. Up to now, however, no unified framework exists that implements previously proposed techniques such as body warping, world warping, and hybrid methods. In this work, we present the Virtual Reality Hand Redirection Toolkit (HaRT), an open-source framework developed for the Unity engine. The toolkit aims to support both novice and expert VR researchers and practitioners in implementing and evaluating hand redirection techniques. It provides implementations of popular redirection algorithms and exposes a modular class hierarchy for easy integration of new approaches. Moreover, simulation, logging, and visualization features allow users of the toolkit to analyze hand redirection setups with minimal technical effort. We present the architecture of the toolkit along with the results of a qualitative expert study.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451814
ER  - 

TY  - NA
AU  - Gesslein, Travis; Biener, Verena; Gagel, Philipp; Schneider, Daniel; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - ISMAR - Pen-based Interaction with Spreadsheets in Mobile Virtual Reality
PY  - 2020
AB  - Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet’s cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.
SP  - 361
EP  - 373
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00063
ER  - 

TY  - NA
AU  - Bergström, Joanna; Dalsgaard, Tor-Salve; Alexander, Jason; Hornbæk, Kasper
TI  - CHI - How to Evaluate Object Selection and Manipulation in VR? Guidelines from 20 Years of Studies
PY  - 2021
AB  - The VR community has introduced many object selection and manipulation techniques during the past two decades. Typically, they are empirically studied to establish their benefits over the state-of-the-art. However, the literature contains few guidelines on how to conduct such studies; standards developed for evaluating 2D interaction often do not apply. This lack of guidelines makes it hard to compare techniques across studies, to report evaluations consistently, and therefore to accumulate or replicate findings. To build such guidelines, we review 20 years of studies on VR object selection and manipulation. Based on the review, we propose recommendations for designing studies and a checklist for reporting them. We also identify research directions for improving evaluation methods and offer ideas for how to make studies more ecologically valid and rigorous.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445193
ER  - 

TY  - NA
AU  - Muthukumarana, Sachith; Messerschmidt, Moritz Alexander; Matthies, Denys J.C.; Steimle, Jürgen; Scholl, Philipp M.; Nanayakkara, Suranga
TI  - CHI - ClothTiles: A Prototyping Platform to Fabricate Customized Actuators on Clothing using 3D Printing and Shape-Memory Alloys
PY  - 2021
AB  - Emerging research has demonstrated the viability of on-textile actuation mechanisms, however, an easily customizable and versatile on-cloth actuation mechanism is yet to be explored. In this paper, we present ClothTiles along with its rapid fabrication technique that enables actuation of clothes. ClothTiles leverage flexible 3D-printing and Shape-Memory Alloys (SMAs) alongside new parametric actuation designs. We validate the concept of fabric actuation using a base element, and then systematically explore methods of aggregating, scaling, and orienting prospects for extended actuation in garments. A user study demonstrated that our technique enables multiple actuation types applied across a variety of clothes. Users identified both aesthetic and functional applications of ClothTiles. We conclude with a number of insights for the Do-It-Yourself community on how to employ 3D-printing with SMAs to enable actuation on clothes.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445613
ER  - 

TY  - JOUR
AU  - Liang, Chen; Yu, Chun; Qin, Yue; Wang, Yuntao; Shi, Yuanchun
TI  - DualRing: Enabling Subtle and Expressive Hand Interaction with Dual IMU Rings
PY  - 2021
AB  - We present DualRing, a novel ring-form input device that can capture the state and movement of the user's hand and fingers. With two IMU rings attached to the user's thumb and index finger, DualRing can sense not only the absolute hand gesture relative to the ground but also the relative pose and movement among hand segments. To enable natural thumb-to-finger interaction, we develop a high-frequency AC circuit for on-body contact detection. Based on the sensing information of DualRing, we outline the interaction space and divide it into three sub-spaces: within-hand interaction, hand-to-surface interaction, and hand-to-object interaction. By analyzing the accuracy and performance of our system, we demonstrate the informational advantage of DualRing in sensing comprehensive hand gestures compared with single-ring-based solutions. Through the user study, we discovered the interaction space enabled by DualRing is favored by users for its usability, efficiency, and novelty.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478114
ER  - 

TY  - NA
AU  - Guinness, Darren; Muehlbradt, Annika; Szafir, Daniel; Kane, Shaun K.
TI  - ASSETS - RoboGraphics: Dynamic Tactile Graphics Powered by Mobile Robots
PY  - 2019
AB  - Tactile graphics are a common way to present information to people with vision impairments. Tactile graphics can be used to explore a broad range of static visual content but aren't well suited to representing animation or interactivity. We introduce a new approach to creating dynamic tactile graphics that combines a touch screen tablet, static tactile overlays, and small mobile robots. We introduce a prototype system called RoboGraphics and several proof-of-concept applications. We evaluated our prototype with seven participants with varying levels of vision, comparing the RoboGraphics approach to a flat screen, audio-tactile interface. Our results show that dynamic tactile graphics can help visually impaired participants explore data quickly and accurately.
SP  - 318
EP  - 328
JF  - The 21st International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3308561.3353804
ER  - 

TY  - NA
AU  - Lin, Richard; Ramesh, Rohit; Iannopollo, Antonio; Vincentelli, Alberto Sangiovanni; Dutta, Prabal; Alon, Elad; Hartmann, Björn
TI  - CHI - Beyond Schematic Capture: Meaningful Abstractions for Better Electronics Design Tools
PY  - 2019
AB  - Printed Circuit Board (PCB) design tools are critical in helping users build non-trivial electronics devices. While recent work recognizes deficiencies with current tools and explores novel methods, little has been done to understand modern designers and their needs. To gain better insight into their practices, we interview fifteen electronics designers of a variety of backgrounds. Our open-ended, semi-structured interviews examine both overarching design flows and details of individual steps. One major finding was that most creative engineering work happens during system architecture, yet current tools operate at lower abstraction levels and create significant tedious work for designers. From that insight, we conceptualize abstractions and primitives for higher-level tools and elicit feedback from our participants on clickthrough mockups of design flows through an example project. We close with our observation on opportunities for improving board design tools and discuss generalizability of our findings beyond the electronics domain.
SP  - 283
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300513
ER  - 

TY  - NA
AU  - Gheran, Bogdan-Florin; Vanderdonckt, Jean; Vatavu, Radu-Daniel
TI  - Conference on Designing Interactive Systems - Gestures for Smart Rings: Empirical Results, Insights, and Design Implications
PY  - 2018
AB  - We present empirical results about users' gesture preferences for smart rings by analyzing 672 gestures from 24 participants. We report an overall low consensus (mean .112, maximum .225 on the unit scale) between participants' gesture proposals, and we point to the challenges of designing highly-generalizable ring gestures across users. We also contribute to the practice of gesture elicitation studies by discussing how a priori conditions (e.g., participants' traits, such as creativity and motor skills), commitment and behavior during the experiment (e.g., their thinking times), but also a posteriori aspects (the experimenter's choice of criteria to group gestures into categories) affect agreement. We offer design guidelines for ring gestures informed by our empirical observations, and present a collection of gestures reflective of our participants' mental models for effecting commands using smart rings.
SP  - 623
EP  - 635
JF  - Proceedings of the 2018 Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3196709.3196741
ER  - 

TY  - JOUR
AU  - Giri, Gowri Shankar; Maddahi, Yaser; Zareinia, Kourosh
TI  - An Application-Based Review of Haptics Technology
PY  - 2021
AB  - Recent technological development has led to the invention of different designs of haptic devices, electromechanical devices that mediate communication between the user and the computer and allow users to manipulate objects in a virtual environment while receiving tactile feedback. The main criteria behind providing an interactive interface are to generate kinesthetic feedback and relay information actively from the haptic device. Sensors and feedback control apparatus are of paramount importance in designing and manufacturing a haptic device. In general, haptic technology can be implemented in different applications such as gaming, teleoperation, medical surgeries, augmented reality (AR), and virtual reality (VR) devices. This paper classifies the application of haptic devices based on the construction and functionality in various fields, followed by addressing major limitations related to haptics technology and discussing prospects of this technology.
SP  - 29
EP  - NA
JF  - Robotics
VL  - 10
IS  - 1
PB  - 
DO  - 10.3390/robotics10010029
ER  - 

TY  - NA
AU  - Lee, Hyein; Kim, Yoonji; Bianchi, Andrea
TI  - SIGGRAPH ASIA Emerging Technologies - MAScreen: Augmenting Speech with Visual Cues of Lip Motions, Facial Expressions, and Text Using a Wearable Display
PY  - 2020
AB  - Personal protective equipment, particularly face masks, have become increasingly common with the rise of global health issues, such as fine-dust storms and pandemics. Face masks, however, also degrade speech intelligibility by effectively occluding visual cues, such as lip motions and facial expressions. In this paper, we propose MAScreen, a wearable LED display in the shape of a mask, which is capable of sensing lip motion and speech and provides real-time visual feedback of the mouth behind the mask.
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2020 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3415255.3422886
ER  - 

TY  - NA
AU  - Shi, Lei; Lawson, Holly; Zhang, Zhuohao; Azenkot, Shiri
TI  - CHI - Designing Interactive 3D Printed Models with Teachers of the Visually Impaired
PY  - 2019
AB  - Students with visual impairments struggle to learn various concepts in the academic curriculum because diagrams, images, and other visual are not accessible to them. To address this, researchers have design interactive 3D printed models (I3Ms) that provide audio descriptions when a user touches components of a model. In prior work, I3Ms were designed on an ad hoc basis, and it is currently unknown what general guidelines produce effective I3M designs. To address this gap, we conducted two studies with Teachers of the Visually Impaired (TVIs). First, we led two design workshops with 35 TVIs, who modified sample models and added interactive elements to them. Second, we worked with three TVIs to design three I3Ms in an iterative instructional design process. At the end of this process, the TVIs used the I3Ms we designed to teach their students. We conclude that I3Ms should (1) have effective tactile features (e.g., distinctive patterns between components), (2) contain both auditory and visual content (e.g., explanatory animations), and (3) consider pedagogical methods (e.g., overview before details).
SP  - 197
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300427
ER  - 

TY  - CHAP
AU  - , 
TI  - Paths Forward: Aspirations for TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544577
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Koelle, Marion; Mjaku, Fjolla; Strohmeier, Paul; Steimle, Jürgen
TI  - Print-A-Sketch: A Handheld Printer for Physical Sketching of Circuits and Sensors on Everyday Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502074
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Rekimoto, Jun; Chen, Bing-Yu
TI  - CHI - ElasticVR: Providing Multilevel Continuously-Changing Resistive Force and Instant Impact Using Elasticity for VR
PY  - 2019
AB  - Resistive force (e.g., due to object elasticity) and impact (e.g., due to recoil) are common effects in our daily life. However, resistive force continuously changes due to users' movements while impact instantly occurs when an event triggers it. These feedback are still not realistically provided by current VR haptic methods. In this paper, a wearable device, ElasticVR, which consists of an elastic band, servo motors and mechanical brakes, is proposed to provide the continuously-changing resistive force and instantly-occurring impact upon the user's hand to enhance VR realism. By changing two physical properties, length and extension distance, of the elastic band, ElasticVR provides multilevel resistive force with no delay and impact with little delay, respectively, for realistic and versatile VR applications. A force perception study was performed to observe users' force distinguishability of the resistive force and impact, and the prototype was built based on its results. A VR experience study further proves that the resistive force and impact from ElasticVR both outperform those from current approaches in realism. Applications using ElasticVR are also demonstrated.
SP  - 220
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300450
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Wang, Bryan; Lee, Jiun-Yu; Shen, Hao-Ping; Wu, Yu-Chian; Chen, Yu-An; Ku, Pin-Sung; Hsu, Ming-Wei; Lin, Yu-Chih; Chen, Mike Y.
TI  - UIST - CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.
PY  - 2017
AB  - The rise of Maker communities and open-source electronic prototyping platforms have made electronic circuit projects increasingly popular around the world. Although there are software tools that support the debugging and sharing of circuits, they require users to manually create the virtual circuits in software, which can be time-consuming and error-prone. We present CircuitSense, a system that automatically recognizes the wires and electronic components placed on breadboards. It uses a combination of passive sensing and active probing to detect and generate the corresponding circuit representation in software in real-time. CircuitSense bridges the gap between the physical and virtual representations of circuits. It enables users to interactively construct and experiment with physical circuits while gaining the benefits of using software tools. It also dramatically simplifies the sharing of circuit designs with online communities.
SP  - 311
EP  - 319
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126634
ER  - 

TY  - CHAP
AU  - Ambu, Rita; Oliveri, Salvatore Massimo; Calì, Michele
TI  - Lecture Notes in Mechanical Engineering - A Bespoke Neck Orthosis for Additive Manufacturing with Improved Design Method and Sustainable Material
PY  - 2021
AB  - NA
SP  - 50
EP  - 58
JF  - Lecture Notes in Mechanical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-91234-5_5
ER  - 

TY  - NA
AU  - Ueda, Kentaro; Terada, Tsutomu; Tsukamoto, Masahiko
TI  - MoMM - Haptic Feedback Method using Deformation of Clothing
PY  - 2019
AB  - Clothing that we wear every day remains in contact with our skin almost at all times. Various haptic sensations to our skin are produced when our clothes deform. We propose a haptic feedback method using the deformation of clothing. Our method deforms the clothing using the contraction of an actuator consisting of shape memory alloys and springs in order to generate haptic sensations. We implemented a prototype long-sleeve T-shirt placing four actuators around the circumference of the forearm, upper arm, and shoulder. We conducted three evaluations to compare the feedback positions. First, the pressure applied to the skin when an actuator contracted varied depending on the locations of the actuation. Second, as compared to the shoulder, the forearm and upper arm achieved a lower stimulation level at which the participants could detect the deformation of clothing taking place. Finally, participants recognized contraction patterns most accurately on the forearm; however, the accuracy of recognition in the case of the shoulder was the worst. Based on the obtained results, we discuss the feedback performance for each position and present the most effective feedback position and the set of contraction patterns that the user could most easily distinguish.
SP  - 84
EP  - 93
JF  - Proceedings of the 17th International Conference on Advances in Mobile Computing & Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365921.3365933
ER  - 

TY  - JOUR
AU  - Iyer, Vikram; Chan, Justin; Gollakota, Shyamnath
TI  - 3D printing wireless connected objects
PY  - 2017
AB  - Our goal is to 3D print wireless sensors, input widgets and objects that can communicate with smartphones and other Wi-Fi devices, without the need for batteries or electronics. To this end, we present a novel toolkit for wireless connectivity that can be integrated with 3D digital models and fabricated using commodity desktop 3D printers and commercially available plastic filament materials. Specifically, we introduce the first computational designs that 1) send data to commercial RF receivers including Wi-Fi, enabling 3D printed wireless sensors and input widgets, and 2) embed data within objects using magnetic fields and decode the data using magnetometers on commodity smartphones. To demonstrate the potential of our techniques, we design the first fully 3D printed wireless sensors including a weight scale, flow sensor and anemometer that can transmit sensor data. Furthermore, we 3D print eyeglass frames, armbands as well as artistic models with embedded magnetic data. Finally, we present various 3D printed application prototypes including buttons, smart sliders and physical knobs that wirelessly control music volume and lights as well as smart bottles that can sense liquid flow and send data to nearby RF devices, without batteries or electronics.
SP  - 242
EP  - 13
JF  - ACM Transactions on Graphics
VL  - 36
IS  - 6
PB  - 
DO  - 10.1145/3130800.3130822
ER  - 

TY  - JOUR
AU  - Cornelio, Patricia; Haggard, Patrick; Hornbaek, Kasper; Georgiou, Orestis; Bergström, Joanna; Subramanian, Sriram; Obrist, Marianna
TI  - The sense of agency in emerging technologies for human-computer integration: A review.
PY  - 2022
AB  - Human-computer integration is an emerging area in which the boundary between humans and technology is blurred as users and computers work collaboratively and share agency to execute tasks. The sense of agency (SoA) is an experience that arises by a combination of a voluntary motor action and sensory evidence whether the corresponding body movements have somehow influenced the course of external events. The SoA is not only a key part of our experiences in daily life but also in our interaction with technology as it gives us the feeling of "I did that" as opposed to "the system did that," thus supporting a feeling of being in control. This feeling becomes critical with human-computer integration, wherein emerging technology directly influences people's body, their actions, and the resulting outcomes. In this review, we analyse and classify current integration technologies based on what we currently know about agency in the literature, and propose a distinction between body augmentation, action augmentation, and outcome augmentation. For each category, we describe agency considerations and markers of differentiation that illustrate a relationship between assistance level (low, high), agency delegation (human, technology), and integration type (fusion, symbiosis). We conclude with a reflection on the opportunities and challenges of integrating humans with computers, and finalise with an expanded definition of human-computer integration including agency aspects which we consider to be particularly relevant. The aim this review is to provide researchers and practitioners with guidelines to situate their work within the integration research agenda and consider the implications of any technologies on SoA, and thus overall user experience when designing future technology.
SP  - 949138
EP  - NA
JF  - Frontiers in neuroscience
VL  - 16
IS  - NA
PB  - 
DO  - 10.3389/fnins.2022.949138
ER  - 

TY  - JOUR
AU  - Li, Ke; Zhang, Ruidong; Liang, Bo; Guimbretière, François; Zhang, Cheng
TI  - EarIO
PY  - 2022
AB  - <jats:p>This paper presents EarIO, an AI-powered acoustic sensing technology that allows an earable (e.g., earphone) to continuously track facial expressions using two pairs of microphone and speaker (one on each side), which are widely available in commodity earphones. It emits acoustic signals from a speaker on an earable towards the face. Depending on facial expressions, the muscles, tissues, and skin around the ear would deform differently, resulting in unique echo profiles in the reflected signals captured by an on-device microphone. These received acoustic signals are processed and learned by a customized deep learning pipeline to continuously infer the full facial expressions represented by 52 parameters captured using a TruthDepth camera. Compared to similar technologies, it has significantly lower power consumption, as it can sample at 86 Hz with a power signature of 154 mW. A user study with 16 participants under three different scenarios, showed that EarIO can reliably estimate the detailed facial movements when the participants were sitting, walking or after remounting the device. Based on the encouraging results, we further discuss the potential opportunities and challenges on applying EarIO on future ear-mounted wearables.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534621
ER  - 

TY  - NA
AU  - Abler, Aline; Zarate, Juan Jose; Langerak, Thomas; Vechev, Velko; Hilliges, Otmar
TI  - WHC - Hedgehog: Handheld Spherical Pin Array based on a Central Electromagnetic Actuator
PY  - 2021
AB  - We present Hedgehog, a single-actuator spherical pin-array device that produces cutaneous haptic sensations to the user’s palms. Hedgehog can enrich digital experiences by providing dynamic haptic patterns over a spherical surface using a simple, hand-held device. The key to our design is that it uses a single central actuator, a spherical omnidirectional electromagnet, to control the extension of all the 86 movable pins. This keeps our design simple to fabricate and scalable. A core challenge with this type of design is that the pins in the array, made out of permanent magnets, need to have a stable position when retracted. We present a method to compute such an arrays’ spatial stability, evaluate our hardware implementation in terms of its output force and pin’s extension and compare it against our method’s predictions. We also report our findings from three user studies investigating the perceived force and speed of traveling patterns. Finally, we present insights on the possible applications of Hedgehog.
SP  - 133
EP  - 138
JF  - 2021 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc49131.2021.9517197
ER  - 

TY  - NA
AU  - Lee, Woojin; Prasad, Ramkrishna; Je, Seungwoo; Kim, Yoonji; Oakley, Ian; Ashbrook, Daniel; Bianchi, Andrea
TI  - TEI - VirtualWire: Supporting Rapid Prototyping with Instant Reconfigurations of Wires in Breadboarded Circuits
PY  - 2021
AB  - Assembling circuits is a challenging and time consuming activity for novice makers, frequently resulting in incorrect placements of wires and components into breadboards. This results in errors that are difficult to identify and debug, and delays that hinder creating, exploring or reconfiguring circuit layouts. This paper presents VirtualWire, a tool that allows users to rapidly design and modify circuits in software and have these changes instantiated in real-time as electrical connections on a physical breadboard. To achieve this, VirtualWire dynamically translates circuit design files into physical connections inside a hardware switching matrix, which handles wiring across breadboard rows and to/from an embedded Arduino. The user can interactively test, tune, and share different circuit layouts for an Arduino shield, and once satisfied, can fabricate the circuit on a permanent substrate. Quantitative and qualitative user studies demonstrate that VirtualWire significantly reduces the time taken for (by 37%), and the number of errors made during (by 53%) circuit assembly, while also supporting users in creating readable, space-efficient and flexible layouts.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440623
ER  - 

TY  - JOUR
AU  - Yau, Yui-Pan; Lee, Lik Hang; Li, Zheng; Braud, Tristan; Ho, Yi-Hsuan; Hui, Pan
TI  - How Subtle Can It Get?: A Trimodal Study of Ring-sized Interfaces for One-Handed Drone Control
PY  - 2020
AB  - Flying drones have become common objects in our daily lives, serving a multitude of purposes. Many of these purposes involve outdoor scenarios where the user combines drone control with another activity. Traditional interaction methods rely on physical or virtual joysticks that occupy both hands, thus restricting drone usability. In this paper, we investigate one-handed human-to-drone-interaction by leveraging three modalities: force, touch, and IMU. After prototyping three different combinations of these modalities on a smartphone, we evaluate them against the current commercial standard through two user experiments. These experiments help us to find the combination of modalities that strikes a compromise between user performance, perceived task load, wrist rotation, and interaction area size. Accordingly, we select a method that achieves faster task completion times than the two-handed commercial baseline by 16.54% with the merits of subtle user behaviours inside a small-size ring-form device and implements this method within the ring-form device. The last experiment involving 12 participants shows that thanks to its small size and weight, the ring device displays better performance than the same method implemented on a mobile phone. Furthermore, users unanimously found the device useful for controlling a drone in mobile scenarios (AVG = 3.92/5), easy to use (AVG = 3.58/5) and easy to learn (AVG = 3.58/5). Our findings give significant design clues in search of subtle and effective interaction through finger augmentation devices with drone control. The users with our prototypical system and a multi-modal on-finger device can control a drone with subtle wrist rotation (pitch gestures: 43.24° amplitude and roll gestures: 46.35° amplitude) and unnoticeable thumb presses within a miniature-sized area of (1.08 * 0.61 cm2).
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397319
ER  - 

TY  - NA
AU  - Jingu, Arata; Kamigaki, Takaaki; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - UIST - LipNotif: Use of Lips as a Non-Contact Tactile Notification Interface Based on Ultrasonic Tactile Presentation
PY  - 2021
AB  - We propose LipNotif, a non-contact tactile notification system that uses airborne ultrasound tactile presentation to lips. Lips are suitable for non-contact tactile notifications because they have high tactile sensitivity comparable to the palms, are less occupied in daily life, and are constantly exposed outward. LipNotif uses tactile patterns to intuitively convey information to users, allowing them to receive notifications using only their lips, without sight, hearing, or hands. We developed a prototype system that automatically recognizes the position of the lips and presents non-contact tactile sensations. Two experiments were conducted to evaluate the feasibility of LipNotif. In the first experiment, we found that directional information can be notified to the lips with an average accuracy of ± 11.1° in the 120° horizontal range. In the second experiment, we could elicit significantly different affective responses by changing the stimulus intensity. The experimental results indicated that LipNotif is practical for conveying directions, emotions, and combinations of them. LipNotif can be applied for various purposes, such as notifications during work, calling in the waiting room, and tactile feedback in automotive user interfaces.
SP  - 13
EP  - 23
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474732
ER  - 

TY  - NA
AU  - Röddiger, Tobias; Clarke, Christopher; Wolffram, Daniel; Budde, Matthias; Beigl, Michael
TI  - CHI - EarRumble: Discreet Hands- and Eyes-Free Input by Voluntary Tensor Tympani Muscle Contraction
PY  - 2021
AB  - We explore how discreet input can be provided using the tensor tympani - a small muscle in the middle ear that some people can voluntarily contract to induce a dull rumbling sound. We investigate the prevalence and ability to control the muscle through an online questionnaire (N=192) in which 43.2% of respondents reported the ability to “ear rumble”. Data collected from participants (N=16) shows how in-ear barometry can be used to detect voluntary tensor tympani contraction in the sealed ear canal. This data was used to train a classifier based on three simple ear rumble “gestures” which achieved 95% accuracy. Finally, we evaluate the use of ear rumbling for interaction, grounded in three manual, dual-task application scenarios (N=8). This highlights the applicability of EarRumble as a low-effort and discreet eyes- and hands-free interaction technique that users found “magical” and “almost telepathic”.
SP  - 743
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445205
ER  - 

TY  - NA
AU  - Zenner, André; Heqitz, Kora Persephone; Krüger, Antonio
TI  - VR - Blink-Suppressed Hand Redirection
PY  - 2021
AB  - Many interaction techniques in virtual reality break with the 1-to-1 mapping from real to virtual space. Instead, specialized techniques for 3D interaction and haptic retargeting leverage hand redirection, offsetting the virtual hand rendering from the real hand position. To achieve unnoticeable hand redirection, however, the utilization of change blindness phenomena has not been systematically explored. Inspired by recent advances in the domain of redirected walking, we present the first hand redirection technique that makes use of blink-induced visual suppression and corresponding change blindness. We introduce Blink-Suppressed Hand Redirection (BSHR) to study the feasibility and detectability of hand redirection based on blink suppression. Our technique is based on Cheng et al.'s (2017) [9] body warping algorithm and instantaneously shifts the virtual hand when the user's vision is suppressed during a blink. Additionally, it can be configured to continuously increment hand offsets when the user's eyes are opened, limited to an extent below detection thresholds. In a psychophysical experiment, we verify that unnoticeable blink-suppressed hand redirection is possible even in worst -case scenarios, and derive the corresponding conservative detection thresholds (CDTs). Moreover, our results show that the range of unnoticeable redirection can be increased by combining continuous warping and blink-suppressed instantaneous shifts. As an additional contribution, we derive the CDTs for Cheng et al.'s (2017) [9] redirection technique that does not leverage blinks.
SP  - 75
EP  - 84
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00028
ER  - 

TY  - BOOK
AU  - Matulic, Fabrice; Vogel, Brian; Kimura, Naoki; Vogel, Daniel
TI  - ISS - Eliciting Pen-Holding Postures for General Input with Suitability for EMG Armband Detection
PY  - 2019
AB  - We conduct a two-part study to better understand pen grip postures for general input like mode switching and com-mand invocation. The first part of the study asks participants what variations of their normal pen grip posture they might use, without any specific consideration for sensing capabilities. The second part evaluates three of their sug-gested postures with an additional set of six postures designed for the sensing capabilities of a consumer EMG armband. Results show that grips considered normal and mature, such as the dynamic tripod and the dynamic quadrupod, are the best candidates for pen-grip based interaction, followed by finger-on-pen postures and grips using pen tilt. A convolutional neural network trained on EMG data gathered during the study yields above 70% within-participant recognition accuracy for common sets of five postures and above 80% for three-posture subsets. Based on the results, we propose design guidelines for pen interaction using variations of grip postures.
SP  - 89
EP  - 100
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359720
ER  - 

TY  - JOUR
AU  - Shi, Lei
TI  - Designing interactive 3D printed models for blind students
PY  - 2018
AB  - Recent advances in 3D printing technology have made tactile models for a variety of concepts more available. Researchers have explored methods to add interactivity to 3D printed models so that the model can speak a description of a model component when it is touched. Using these interactive 3D printed models (I3Ms), blind students can understand abstract concepts (e.g., the structure of molecules) better. However, the current model creation methods are too complicated for laypersons, and the researchers designed I3Ms without fully considering blind users' needs. As a result, I3MS are still limited in terms of their quantity, quality, and functionalities. I aim to make interactive 3D printed models (I3Ms) more available and useful to blind students. Specifically, I explored different methods to create I3Ms, conducted studies to understand blind users' needs and preferences, and developed tools that make I3Ms easy to create and easy to access.
SP  - 32
EP  - 35
JF  - ACM SIGACCESS Accessibility and Computing
VL  - 120
IS  - 120
PB  - 
DO  - 10.1145/3178412.3178419
ER  - 

TY  - NA
AU  - Katsuragawa, Keiko; Wang, Ju; Shan, Ziyang; Ouyang, Ningshan; Abari, Omid; Vogel, Daniel
TI  - UIST - Tip-Tap: Battery-free Discrete 2D Fingertip Input
PY  - 2019
AB  - We describe Tip-Tap, a wearable input technique that can be implemented without batteries using a custom RFID tag. It recognizes 2-dimensional discrete touch events by sensing the intersection between two arrays of contact points: one array along the index fingertip and the other along the thumb tip. A formative study identifies locations on the index finger that are reachable by different parts of the thumb tip, and the results determine the pattern of contacts points used for the technique. Using a reconfigurable 3x3 evaluation device, a second study shows eyes-free accuracy is 86% after a very short period, and adding bumpy or magnetic passive haptic feedback to contacts is not necessary. Finally, two battery-free prototypes using a new RFID tag design demonstrates how Tip-Tap can be implemented in a glove or tattoo form factor.
SP  - 1045
EP  - 1057
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347907
ER  - 

TY  - NA
AU  - Huang, Gaoping; Qian, Xun; Wang, Tianyi; Patel, Fagun; Sreeram, Maitreya; Cao, Yuanzhi; Ramani, Karthik; Quinn, Alexander J.
TI  - CHI - AdapTutAR: An Adaptive Tutoring System for Machine Tasks in Augmented Reality
PY  - 2021
AB  - Modern manufacturing processes are in a state of flux, as they adapt to increasing demand for flexible and self-configuring production. This poses challenges for training workers to rapidly master new machine operations and processes, i.e. machine tasks. Conventional in-person training is effective but requires time and effort of experts for each worker trained and not scalable. Recorded tutorials, such as video-based or augmented reality (AR), permit more efficient scaling. However, unlike in-person tutoring, existing recorded tutorials lack the ability to adapt to workers’ diverse experiences and learning behaviors. We present AdapTutAR, an adaptive task tutoring system that enables experts to record machine task tutorials via embodied demonstration and train learners with different AR tutoring contents adapting to each user’s characteristics. The adaptation is achieved by continually monitoring learners’ tutorial-following status and adjusting the tutoring content on-the-fly and in-situ. The results of our user study evaluation have demonstrated that our adaptive system is more effective and preferable than the non-adaptive one.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445283
ER  - 

TY  - NA
AU  - Watanabe, Hiroki; Terada, Tsutomu
TI  - AHs - Manipulatable Auditory Perception in Wearable Computing
PY  - 2020
AB  - We proposed a framework to manipulate auditory perception. Since auditory perception is passive sense, we often do not notice important information and acquire unimportant information. In this study, we focused on earphone-type wearable computers (hearable devices) that not only have speakers but also microphones. In a hearable computing environment, we always attach microphones and speakers to the ears. Therefore, we can manipulate our auditory perception using a hearable device. We manipulated the frequency of the input sound from the microphones and transmitted the converted sound from the speakers. Thus, we could acquire the sound that is not heard with our normal auditory perception and eliminate the unwanted sound according to the user's requirements. We devised five types of frequency-manipulating techniques and implemented a prototype device. Moreover, we proposed seven assumed applications that can be realized by the proposed method.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384792
ER  - 

TY  - NA
AU  - Goguey, Alix; Sahoo, Deepak Ranjan; Robinson, Simon; Pearson, Jennifer; Jones, Matt
TI  - CHI - Pulp Friction: Exploring the Finger Pad Periphery for Subtle Haptic Feedback
PY  - 2019
AB  - Current haptic feedback techniques on handheld devices are applied to the finger pad or the palm of the user. These state-of-the-art approaches are coarse-grained and tend to be intrusive, rather than subtle. In contrast, we present a new feedback technique that applies stimuli around the periphery of the finger pulp, demonstrating how this can provide rich, nuanced haptic information. We use a reconfigurable haptic device employing a ferromagnetic marble for back-of-the device handheld use, which, for the first time, probes, without instrumenting the user, the periphery of the distal phalanx with localised stimulation. We present the design-space afforded by this new technique and evaluate the human-factors of finger-peripheral touch interaction in a controlled user-study. We report results with marbles of different diameters, speeds and a combination of poking, lateral vibration and patterns; present the resulting design guidelines for finger-periphery haptic feedback; and, illustrate its potential with use case scenarios.
SP  - 641
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300871
ER  - 

TY  - NA
AU  - Torres, César I.; Chang, Jessica; Patel, Advaita; Paulos, Eric
TI  - Conference on Designing Interactive Systems - Phosphenes: Crafting Resistive Heaters within Thermoreactive Composites
PY  - 2019
AB  - Hybrid practices are emerging that integrate creative materials like paint, clay, and cloth with intangible immaterials like computation, electricity, and heat. This work aims to expand the design potential of immaterial elements by transforming them into manipulatable, observable and intuitive materials. We explore one such immaterial, electric heat, and develop a maker-friendly fabrication pipeline and crafting support tool that allows users to experientially compose resistive heaters that generate heat spatially and temporally. These heaters are then used to couple heat and thermoreactive materials in a class of artifacts we term Thermoreactive Composites (TrCs). In a formal user study, we observe how designing fabrication workflows along dimensions of composability and perceivability better matches the working styles of material practitioners without domain knowledge of electronics. Through exemplar artifacts, we demonstrate the potential of heat as a creative material and discuss implications for immaterials used within creative practices.
SP  - 907
EP  - 919
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322375
ER  - 

TY  - JOUR
AU  - Du, Chuan; Zhang, Lei; Sun, Xiping; Wang, Junxu; Sheng, Jialian
TI  - Enhanced Multi-Channel Feature Synthesis for Hand Gesture Recognition Based on CNN With a Channel and Spatial Attention Mechanism
PY  - 2020
AB  - Millimeter-wave (MMW) radar hand gesture recognition technology is becoming important in many electronic device control applications. Currently, most existing approaches utilize the radical and micro-Doppler features from single-channel MMW radar, which ignores the different importance of the information contained in the micro-Doppler feature background or target areas. In this paper, we propose an algorithm for hand gesture recognition jointly using multi-channel signatures. The algorithm blends the information of both micro-Doppler features and instantaneous angles (azimuth and elevation) to accomplish hand gesture recognition performed with the convolutional neural network (CNN). To have a better features fusion and make CNN focus on the most important target signal regions and suppress the unnecessary noise areas, we apply the channel and spatial attention-based feature refinement modules. We also employ gesture movement mechanism-based data augmentation for more effective training to alleviate potential overfitting. Extensive experiments demonstrate the effectiveness and superiorities of the proposed algorithm. This method achieves a correct classification rate of 96.61%, approximately 5% higher than that of the single-channel-based recognition strategy as measured based on MMW radar datasets.
SP  - 144610
EP  - 144620
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.3010063
ER  - 

TY  - JOUR
AU  - Liu, Yilin; Jiang, Fengyang; Gowda, Mahanth
TI  - Finger Gesture Tracking for Interactive Applications: A Pilot Study with Sign Languages
PY  - 2020
AB  - This paper presents FinGTrAC, a system that shows the feasibility of fine grained finger gesture tracking using low intrusive wearable sensor platform (smart-ring worn on the index finger and a smart-watch worn on the wrist). The key contribution is in scaling up gesture recognition to hundreds of gestures while using only a sparse wearable sensor set where prior works have been able to only detect tens of hand gestures. Such sparse sensors are convenient to wear but cannot track all fingers and hence provide under-constrained information. However application specific context can fill the gap in sparse sensing and improve the accuracy of gesture classification. Rich context exists in a number of applications such as user-interfaces, sports analytics, medical rehabilitation, sign language translation etc. This paper shows the feasibility of exploiting such context in an application of American Sign Language (ASL) translation. Noisy sensor data, variations in gesture performance across users and the inability to capture data from all fingers introduce non-trivial challenges. FinGTrAC exploits a number of opportunities in data preprocessing, filtering, pattern matching, context of an ASL sentence to systematically fuse the available sensory information into a Bayesian filtering framework. Culminating into the design of a Hidden Markov Model, a Viterbi decoding scheme is designed to detect finger gestures and the corresponding ASL sentences in real time. Extensive evaluation on 10 users shows a recognition accuracy of 94.2% for 100 most frequently used ASL finger gestures over different sentences. When the size of the dictionary is extended to 200 words, the accuracy is degrades gracefully to 90% thus indicating the robustness and scalability of the multi-stage optimization framework.
SP  - 112
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1145/3414117
ER  - 

TY  - JOUR
AU  - Li, Jianyou; Tanaka, Hiroya
TI  - Rapid customization system for 3D-printed splint using programmable modeling technique - a practical approach.
PY  - 2018
AB  - Traditional splinting processes are skill dependent and irreversible, and patient satisfaction levels during rehabilitation are invariably lowered by the heavy structure and poor ventilation of splints. To overcome this drawback, use of the 3D-printing technology has been proposed in recent years, and there has been an increase in public awareness. However, application of 3D-printing technologies is limited by the low CAD proficiency of clinicians as well as unforeseen scan flaws within anatomic models. A programmable modeling tool has been employed to develop a semi-automatic design system for generating a printable splint model. The modeling process was divided into five stages, and detailed steps involved in construction of the proposed system as well as automatic thickness calculation, the lattice structure, and assembly method have been thoroughly described. The proposed approach allows clinicians to verify the state of the splint model at every stage, thereby facilitating adjustment of input content and/or other parameters to help solve possible modeling issues. A finite element analysis simulation was performed to evaluate the structural strength of generated models. A fit investigation was applied on fabricated splints and volunteers to assess the wearing experience. Manual modeling steps involved in complex splint designs have been programed into the proposed automatic system. Clinicians define the splinting region by drawing two curves, thereby obtaining the final model within minutes. The proposed system is capable of automatically patching up minor flaws within the limb model as well as calculating the thickness and lattice density of various splints. Large splints could be divided into three parts for simultaneous multiple printing. This study highlights the advantages, limitations, and possible strategies concerning application of programmable modeling tools in clinical processes, thereby aiding clinicians with lower CAD proficiencies to become adept with splint design process, thus improving the overall design efficiency of 3D-printed splints.
SP  - 5
EP  - 5
JF  - 3D printing in medicine
VL  - 4
IS  - 1
PB  - 
DO  - 10.1186/s41205-018-0027-6
ER  - 

TY  - JOUR
AU  - Phutane, Mahika; Wright, Julie; Castro, Brenda Veronica; Shi, Lei; Stern, Simone R.; Lawson, Holly M.; Azenkot, Shiri
TI  - Tactile Materials in Practice: Understanding the Experiences of Teachers of the Visually Impaired
PY  - 2022
AB  - <jats:p> <jats:bold>Teachers of the visually impaired (TVIs)</jats:bold> regularly present tactile materials (tactile graphics, 3D models, and real objects) to students with vision impairments. Researchers have been increasingly interested in designing tools to support the use of tactile materials, but we still lack an in-depth understanding of how tactile materials are created and used in practice today. To address this gap, we conducted interviews with 21 TVIs and a 3-week diary study with eight of them. We found that tactile materials were regularly used for academic as well as non-academic concepts like tactile literacy, motor ability, and spatial awareness. Real objects and 3D models served as “stepping stones” to tactile graphics and our participants preferred to teach with 3D models, despite finding them difficult to create, obtain, and modify. Use of certain materials also carried social implications; participants selected materials that fostered student independence and allow classroom inclusion. We contribute design considerations, encouraging future work on tactile materials to enable student and TVI co-creation, facilitate rapid prototyping, and promote movement and spatial awareness. To support future research in this area, our paper provides a fundamental understanding of current practices. We bridge these practices to established pedagogical approaches and highlight opportunities for growth regarding this important genre of educational materials. </jats:p>
SP  - 1
EP  - 34
JF  - ACM Transactions on Accessible Computing
VL  - 15
IS  - 3
PB  - 
DO  - 10.1145/3508364
ER  - 

TY  - NA
AU  - Karchemsky, Mitchell; Zamfirescu-Pereira, J.D.; Wu, Kuan-Ju; Guimbretière, François; Hartmann, Bjoern
TI  - CHI - Heimdall: A Remotely Controlled Inspection Workbench For Debugging Microcontroller Projects
PY  - 2019
AB  - Students and hobbyists build embedded systems that combine sensing, actuation and microcontrollers on solderless breadboards. To help students debug such circuits, experienced teachers apply visual inspection, targeted measurements, and circuit modifications to diagnose and localize the problem(s). However, experienced helpers may not always be available to review student projects in person. To enable remote debugging of circuit problems, we introduce Heimdall, a remote electronics workbench that allows experts to visually inspect a student's circuit; perform measurements; and to re-wire and inject test signals. These interactions are enabled by an actuated inspection camera; an augmented breadboard that enables flexible configuration of row connectivity and measurement/injection lines; and a web-based UI that teachers can use to perform measurements through interaction with the captured images. We demonstrate that common issues arising in embedded electronics classes can be successfully diagnosed remotely and report on preliminary user feedback from teaching assistants who frequently debug circuits.
SP  - 498
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300728
ER  - 

TY  - CHAP
AU  - , 
TI  - Mediating Technologies
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544574
ER  - 

TY  - JOUR
AU  - Oulasvirta, Antti; Dayama, Niraj Ramesh; Shiripour, Morteza; John, Maximilian; Karrenbauer, Andreas
TI  - Combinatorial Optimization of Graphical User Interface Designs
PY  - 2020
AB  - The graphical user interface (GUI) has become the prime means for interacting with computing systems. It leverages human perceptual and motor capabilities for elementary tasks such as command exploration and invocation, information search, and multitasking. For designing a GUI, numerous interconnected decisions must be made such that the outcome strikes a balance between human factors and technical objectives. Normally, design choices are specified manually and coded within the software by professional designers and developers. This article surveys combinatorial optimization as a flexible and powerful tool for computational generation and adaptation of GUIs. As recently as 15 years ago, applications were limited to keyboards and widget layouts. The obstacle has been the mathematical definition of design tasks, on the one hand, and the lack of objective functions that capture essential aspects of human behavior, on the other. This article presents definitions of layout design problems as integer programming tasks, a coherent formalism that permits identification of problem types, analysis of their complexity, and exploitation of known algorithmic solutions. It then surveys advances in formulating evaluative functions for common design-goal foci such as user performance and experience. The convergence of these two advances has expanded the range of solvable problems. Approaches to practical deployment are outlined with a wide spectrum of applications. This article concludes by discussing the position of this application area within optimization and human–computer interaction research and outlines challenges for future work.
SP  - 434
EP  - 464
JF  - Proceedings of the IEEE
VL  - 108
IS  - 3
PB  - 
DO  - 10.1109/jproc.2020.2969687
ER  - 

TY  - NA
AU  - Shim, Youngbo Aram; Lee, Jaeyeon; Lee, Geehyuk
TI  - CHI Extended Abstracts - Exploring Multimodal Watch-back Tactile Display using Wind and Vibration
PY  - 2018
AB  - A tactile display on the back of a smartwatch is an attractive output option; however, its channel capacity is limited owing to the small contact area. In order to expand the channel capacity, we considered using two perceptually distinct types of stimuli, wind and vibration, together on the same skin area. The result is a multimodal tactile display that combines wind and vibration to create "colored" tactile sensations on the wrist. As a first step toward this goal, we conducted in this study four user experiments with a wind-vibration tactile display to examine different ways of combining wind and vibration: Individual, Sequential, and Simultaneous. The results revealed the sequential combination of wind and vibration to exhibit the highest potential, with an information transfer capacity of 3.29 bits. In particular, the transition of tactile modality was perceived at an accuracy of 98.52%. The current results confirm the feasibility and potential of a multimodal tactile display combining wind and vibration.
SP  - 132
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186488
ER  - 

TY  - NA
AU  - Xie, Haoran; Torii, Takuma; Chiba, Aoshi; Qi, Qiukai
TI  - xBalloon: Animated Objects with Balloon Plastic Actuator
PY  - 2021
AB  - Shape-changing interfaces are promising for users to change the physical properties of common objects. However, prevailing approaches of actuation devices require either professional equipment or materials that are not commonly accessible to non-professional users. In this work, we focus on the controllable soft actuators with inflatable structures because they are soft thus safe for human computer interaction. We propose a soft actuator design, called xBalloon, that is workable, inexpensive and easy-to-fabricate. It consists of daily materials including balloons and plastics and can realize bending actuation very effectively. For characterization, we fabricated xBalloon samples with different geometrical parameters and tested them regarding the bending performance and found the analytical model describing the relationship between the shape and the bending width. We then used xBalloons to animate a series of common objects and all can work satisfactorily. We further verified the user experience about the the fabrication and found that even those with no prior robotic knowledge can fabricate xBalloons with ease and confidence. Given all these advantages, we believe that xBalloon is an ideal platform for interaction design and entertainment applications.
SP  - NA
EP  - NA
JF  - 12th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460881.3460933
ER  - 

TY  - NA
AU  - Visschedijk, Aaron; Kim, Hyunyoung; Tejada, Carlos; Ashbrook, Daniel
TI  - ClipWidgets: 3D-printed Modular Tangible UI Extensions for Smartphones
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501314
ER  - 

TY  - NA
AU  - Fender, Andreas; Herholz, Philipp; Alexa, Marc; Müller, Jörg
TI  - CHI - OptiSpace: Automated Placement of Interactive 3D Projection Mapping Content
PY  - 2018
AB  - We present OptiSpace, a system for the automated placement of perspectively corrected projection mapping content. We analyze the geometry of physical surfaces and the viewing behavior of users over time using depth cameras. Our system measures user view behavior and simulates a virtual projection mapping scene users would see if content were placed in a particular way. OptiSpace evaluates the simulated scene according to perceptual criteria, including visibility and visual quality of virtual content. Finally, based on these evaluations, it optimizes content placement, using a two-phase procedure involving adaptive sampling and the covariance matrix adaptation algorithm. With our proposed architecture, projection mapping applications are developed without any knowledge of the physical layouts of the target environments. Applications can be deployed in different uncontrolled environments, such as living rooms and office spaces.
SP  - 269
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173843
ER  - 

TY  - BOOK
AU  - Gupta, Aakar; Pietrzak, Thomas; Yau, Cleon; Roussel, Nicolas; Balakrishnan, Ravin
TI  - ISS - Summon and Select: Rapid Interaction with Interface Controls in Mid-air
PY  - 2017
AB  - Current freehand interactions with large displays rely on point & select as the dominant paradigm. However, constant hand movement in air for pointer navigation leads to hand fatigue quickly. We introduce summon & select, a new model for freehand interaction where, instead of navigating to the control, the user summons it into focus and then manipulates it. Summon & select solves the problems of constant pointer navigation, need for precise selection, and out-of-bounds gestures that plague point & select. We describe the design and conduct two studies to evaluate the design and compare it against point & select in a multi-button selection study. The results show that summon & select is significantly faster and has less physical and mental demand than point & select.
SP  - 52
EP  - 61
JF  - Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132272.3134120
ER  - 

TY  - NA
AU  - Feuchtner, Tiare; Müller, Jörg
TI  - UIST - Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift
PY  - 2018
AB  - We present Ownershift, an interaction technique for easing overhead manipulation in virtual reality, while preserving the illusion that the virtual hand is the user's own hand. In contrast to previous approaches, this technique does not alter the mapping of the virtual hand position for initial reaching movements towards the target. Instead, the virtual hand space is only shifted gradually if interaction with the overhead target requires an extended amount of time. While users perceive their virtual hand as operating overhead, their physical hand moves gradually to a less strained position at waist level. We evaluated the technique in a user study and show that Ownershift significantly reduces the physical strain of overhead interactions, while only slightly reducing task performance and the sense of body ownership of the virtual hand.
SP  - 31
EP  - 43
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242594
ER  - 

TY  - NA
AU  - Bhatia, Arpit; Kundu, Dhruv; Agarwal, Suyash; Kairon, Varnika; Parnami, Aman
TI  - CHI - Soma-noti: Delivering Notifications Through Under-clothing Wearables
PY  - 2021
AB  - Different form factors of wearable technology provide unique opportunities for output based on how they are connected to the human body. In this work, we investigate the idea of delivering notifications through devices worn on the underside of a user’s clothing. A wearable worn in such a manner is in direct contact with the user’s skin. We leverage this proximity to test the performance of 10 on-skin sensations (Press, Poke, Pinch, Heat, Cool, Blow, Suck, Vibrate, Moisture and Brush) as methods of notification delivery. We developed prototypes for each stimulus and conducted a user study to evaluate them across 6 locations commonly covered by upper body clothing. Results indicate significant differences in reaction time, error rates and comfort which may influence the design of future under-clothing wearables.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445123
ER  - 

TY  - NA
AU  - Lu, Qiuyu; Liu, Yejun; Mi, Haipeng
TI  - MotionFlow: Time-axis-based Multiple Robots Expressive Motion Programming
PY  - 2020
AB  - Robots have been gradually weaving into the fabric of many areas, including children's education and new media arts. Such combination promotes children's creativity and provides new artistic expression paradigms for artists. However, for robot systems that contain many degrees of freedom, the motion programming can be very complicated. This paper presents a time-axis-based computer aided design tool for multiple robots expressive motion programming, MotionFlow. It allows users who have no prior coding experience to easily add and edit motion clips modules on the time axis. In the meanwhile, it provides a rendered animation preview of the robots' motions, which enables users to evaluate the programming result without repeatedly testing the program on the hardware. In this way, users can perform rapid optimization and adjustment of the motion design. According to the user study, MotionFlow is very easy to use and very efficient for motion editing. It can greatly lower the threshold of multiple robots motion programming and allows users to focus more on creative work.
SP  - NA
EP  - NA
JF  - Proceedings of the 2020 3rd International Conference on Computer Science and Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3403746.3403919
ER  - 

TY  - CHAP
AU  - , 
TI  - Authors Cited
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544580
ER  - 

TY  - NA
AU  - Yang, Munseok; Yamaoka, Junichi
TI  - MultiJam: Fabricating Jamming User Interface using Multi-material 3D Printing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505565
ER  - 

TY  - NA
AU  - Satriadi, Kadek Ananta; Ens, Barrett; Czauderna, Tobias; Cordeil, Maxime; Jenny, Bernhard
TI  - CHI - Quantitative Data Visualisation on Virtual Globes
PY  - 2021
AB  - Geographic data visualisation on virtual globes is intuitive and widespread, but has not been thoroughly investigated. We explore two main design factors for quantitative data visualisation on virtual globes: i) commonly used primitives (2D bar, 3D bar, circle) and ii) the orientation of these primitives (tangential, normal, billboarded). We evaluate five distinctive visualisation idioms in a user study with 50 participants. The results show that aligning primitives tangentially on the globe’s surface decreases the accuracy of area-proportional circle visualisations, while the orientation does not have a significant effect on the accuracy of length-proportional bar visualisations. We also find that tangential primitives induce higher perceived mental load than other orientations. Guided by these results we design a novel globe visualisation idiom, Geoburst, that combines a virtual globe and a radial bar chart. A preliminary evaluation reports potential benefits and drawbacks of the Geoburst visualisation.
SP  - 460
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445152
ER  - 

TY  - NA
AU  - Wang, Sitong; Chilton, Lydia B.
TI  - PopNet: a Pop Culture Knowledge Association Network for Supporting Creative Connections.
PY  - 2021
AB  - Pop culture is a pervasive and important aspect of communication and self-expression. When people wish to communicate using pop culture references, they need to find connections between their message and the things, people, location and actions of a movie, tv series, or other pop culture domain. However, finding an appropriate match from memory is challenging and search engines are not specific enough to the task. Often domain-specific knowledge graphs provide the structure, specificity and search capabilities that people need. We introduce PopNet - a Pop Culture Knowledge Association Network automatically created from plain text using state-of-the art NLP methods to extract entities and actions from text summaries of movies and tv shows. The interface allows people to browse and search the entries to find connections. We conduct a study showing that this system is accurate and helpful for finding multiple connections between a message and a pop culture domain.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Popovici, Irina; Schipor, Ovidiu Andrei; Vatavu, Radu-Daniel
TI  - Hover: Exploring cognitive maps and mid-air pointing for television control
PY  - 2019
AB  - NA
SP  - 95
EP  - 107
JF  - International Journal of Human-Computer Studies
VL  - 129
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.03.012
ER  - 

TY  - JOUR
AU  - Sabyrov, Nurbol; Sotsial, Zhuldyz; Abilgaziyev, Anuar; Adair, Desmond; Ali, Hazrat
TI  - Design of a flexible neck orthosis on Fused Deposition Modeling printer for rehabilitation on regular usage
PY  - 2021
AB  - Abstract The usage area of Additive Manufacturing (AM) already spread into the medicine and rehabilitation sphere. The advantages of AM become a driving force for fabricating prostheses, human organs, and implants. The recent studies in AM indicate excellent manufacture of limbs that possesses characteristics of market versions and, at the same time, outperform them in comfortability. Although there is a vast amount of investigation on orthosis development, only a few applications connected with neck orthosis. This paper proposes customized cervical orthosis designed through 3d scanner device and produced by Fused Deposition modeling. TPE (thermoplastic elastomer) FLEX filament used to provide the model with flexible features on par with the lightweight. FEA analysis assessment confirmed the durability of the prototype. Furthermore, the specific construction of orthosis allows patients to comfortably dress and utilize it in daily life, whereas the hole pattern of frame addresses ventilation problems. Obtained results indicate the capability of using TPE (flex) material and show that the FDM printed model able to compete with market analogs.
SP  - 63
EP  - 71
JF  - Procedia Computer Science
VL  - 179
IS  - NA
PB  - 
DO  - 10.1016/j.procs.2020.12.009
ER  - 

TY  - NA
AU  - Fang, Hao
TI  - Building A User-Centric and Content-Driven Socialbot.
PY  - 2020
AB  - To build Sounding Board, we develop a system architecture that is capable of accommodating dialog strategies that we designed for socialbot conversations. The architecture consists of a multi-dimensional language understanding module for analyzing user utterances, a hierarchical dialog management framework for dialog context tracking and complex dialog control, and a language generation process that realizes the response plan and makes adjustments for speech synthesis. Additionally, we construct a new knowledge base to power the socialbot by collecting social chat content from a variety of sources. An important contribution of the system is the synergy between the knowledge base and the dialog management, i.e., the use of a graph structure to organize the knowledge base that makes dialog control very efficient in bringing related content to the discussion. Using the data collected from Sounding Board during the competition, we carry out in-depth analyses of socialbot conversations and user ratings which provide valuable insights in evaluation methods for socialbots. We additionally investigate a new approach for system evaluation and diagnosis that allows scoring individual dialog segments in the conversation. Finally, observing that socialbots suffer from the issue of shallow conversations about topics associated with unstructured data, we study the problem of enabling extended socialbot conversations grounded on a document. To bring together machine reading and dialog control techniques, a graph-based document representation is proposed, together with methods for automatically constructing the graph. Using the graph-based representation, dialog control can be carried out by retrieving nodes or moving along edges in the graph. To illustrate the usage, a mixed-initiative dialog strategy is designed for socialbot conversations on news articles.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Valkeneers, Tom; Leen, Danny; Ashbrook, Daniel; Ramakers, Raf
TI  - UIST - StackMold: Rapid Prototyping of Functional Multi-Material Objects with Selective Levels of Surface Details
PY  - 2019
AB  - We present StackMold, a DIY molding technique to prototype multi-material and multi-colored objects with embedded electronics. The key concept of our approach is a novel multi-stage mold buildup in which casting operations are interleaved with the assembly of the mold to form independent compartments for casting different materials. To build multi-stage molds, we contribute novel algorithms that computationally design and optimize the mold and casting procedure. By default, the multi-stage mold is fabricated in slices using a laser cutter. For regions that require more surface detail, a high-fidelity 3D-printed mold subsection can be incorporated. StackMold is an integrated end-to-end system, supporting all stages of the process: it provides a UI to specify material and detail regions of a 3D~object; it generates fabrication files for the molds; and it produces a step-by-step casting instruction manual.
SP  - 687
EP  - 699
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347915
ER  - 

TY  - JOUR
AU  - Sang, Yu; Shi, Laixi; Liu, Yimin
TI  - Micro Hand Gesture Recognition System Using Ultrasonic Active Sensing
PY  - 2018
AB  - In this paper, we propose a micro hand gesture recognition system and methods using ultrasonic active sensing. This system uses micro dynamic hand gestures for recognition to achieve human–computer interaction (HCI). The implemented system, called hand-ultrasonic gesture (HUG), consists of ultrasonic active sensing, pulsed radar signal processing, and time-sequence pattern recognition by machine learning. We adopt lower frequency (300 kHz) ultrasonic active sensing to obtain high resolution range-Doppler image features. Using high quality sequential range-Doppler features, we propose a state-transition-based hidden Markov model for gesture classification. This method achieves a recognition accuracy of nearly 90% by using symbolized range-Doppler features and significantly reduces the computational complexity and power consumption. Furthermore, to achieve higher classification accuracy, we utilize an end-to-end neural network model and obtain a recognition accuracy of 96.32%. In addition to offline analysis, a real-time prototype is released to verify our method’s potential for application in the real world.
SP  - 49339
EP  - 49347
JF  - IEEE Access
VL  - 6
IS  - NA
PB  - 
DO  - 10.1109/access.2018.2868268
ER  - 

TY  - CHAP
AU  - Turner, Phil
TI  - Imaginary Use
PY  - 2020
AB  - NA
SP  - 103
EP  - 120
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-37348-1_5
ER  - 

TY  - JOUR
AU  - Jeong, Yunwoo; Kim, Han-Jong; Cho, Hyungjun; Nam, Tek-Jin
TI  - M.Integrator: a maker’s tool for integrating kinetic mechanisms and sensors
PY  - 2019
AB  - As digital fabrication and physical computing continue to emerge, prototyping of the interactive kinetic artifacts has become common for makers. However, novice makers have difficulties in integrating components: physical mechanisms, actuators, sensors, and software code. We present a prototyping tool, M.Integrator, which supports a synthetic approach to building interactive kinetic artifacts. It helps makers consider prototypes as a whole from the early stage of the fabrication process. The sensor-based movement of the interactive kinetic artifact can be automatically converted to software code using the visual authoring interface. A maker can interactively test the interactive kinetic artifact by combining virtual and physical components. Instant physical testing is carried out by a customized prototyping board. In the user study with twelve makers, the tool allowed makers to complete the task in a short evaluation session. The visual authoring interface enabled them to generate the sensor-based movement after only a brief explanation. We found that the tool supported synthetic and iterative prototyping. Combinations of virtual and physical components supported the interactive simulation throughout the session. We conclude by discussing the implications for future digital fabrication of the interactive kinetic artifact.
SP  - 271
EP  - 283
JF  - International Journal on Interactive Design and Manufacturing (IJIDeM)
VL  - 14
IS  - 1
PB  - 
DO  - 10.1007/s12008-019-00639-7
ER  - 

TY  - NA
AU  - Kim, Yoonji; Zhu, Junyi; Trivedi, Mihir; Turakhia, Dishita; Wu, Ngai Hang; Ko, Donghyeon; Wessely, Michael; Mueller, Stefanie
TI  - SensorViz: Visualizing Sensor Data Across Different Stages of Prototyping Interactive Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533481
ER  - 

TY  - JOUR
AU  - Manukyan, Vahram K.; Манукян, В К
TI  - Software applications in modeling of physical processes in radio engineering and electronics in the context of distance learning
PY  - 2021
AB  - This article examines the existing software applications for electronic circuit prototyping that can help evaluate the operation of the entire circuit based on specified parameters and find the best options. Using the TAC application and the Arduino library components, projects and instructions for creating and testing electronic circuits were developed. The resulting application was tested for capabilities and efficiency. The interface design and settings were also assessed to determine whether they simulate real conditions. The functionality of TAC application made it possible to generate alternate circuits. The results of software testing showed that using the application speeds up the learning process significantly when compared to traditional methods and helps eliminate problems that may emerge when designing and assembling circuits. The application may be useful in designing complex electronic circuits and developing teaching aids for students in technical fields.
SP  - 89
EP  - 97
JF  - Physics of Wave Processes and Radio Systems
VL  - 24
IS  - 1
PB  - 
DO  - 10.18469/1810-3189.2021.24.1.89-97
ER  - 

TY  - JOUR
AU  - Satriadi, Kadek Ananta; Ens, Barrett; Cordeil, Maxime; Czauderna, Tobias; Jenny, Bernhard
TI  - Maps Around Me: 3D Multiview Layouts in Immersive Spaces
PY  - 2020
AB  - Visual exploration of maps often requires a contextual understanding at multiple scales and locations. Multiview map layouts, which present a hierarchy of multiple views to reveal detail at various scales and locations, have been shown to support better performance than traditional single-view exploration on desktop displays. This paper investigates the extension of such layouts of 2D maps into 3D immersive spaces, which are not limited by the real-estate barrier of physical screens and support sensemaking through spatial interaction. Based on our initial implementation of immersive multiview maps, we conduct an exploratory study with 16 participants aimed at understanding how people place and view such maps in immersive space. We observe the layouts produced by users performing map exploration search, comparison and route-planning tasks. Our qualitative analysis identifies patterns in layoutgeometry (spherical, spherical cap, planar),overview-detail relationship (central window, occluding, coordinated) andinteraction strategy. Based on these observations, along with qualitative feedback from a user walkthrough session, we identify implications and recommend features for immersive multiview map systems. Our main findings are that participants tend to prefer and arrange multiview maps in a spherical cap layout around them and that they often rearrange the views during tasks.
SP  - 201
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427329
ER  - 

TY  - NA
AU  - Xu, Wenge; Liang, Hai-Ning; Zhao, Yuxuan; Yu, Difeng; Monteiro, Diego
TI  - CHI - DMove: Directional Motion-based Interaction for Augmented Reality Head-Mounted Displays
PY  - 2019
AB  - We present DMove, directional motion-based interaction for Augmented Reality (AR) Head-Mounted Displays (HMDs) that is both hands- and device-free. It uses directional walk-ing as a way to interact with virtual objects. To use DMove, a user needs to perform directional motions such as mov-ing one foot forward or backward. In this research, we first investigate the recognition accuracy of the motion direc-tions of our method and the social acceptance of this type of interactions together with users' comfort rating for each direction. We then optimize its design and conduct a sec-ond study to compare DMove in task performance and user preferences (workload, motion sickness, user experience), with two approaches-Hand interaction (Meta 2-like) and Head+Hand interaction (HoloLens-like) for menu selection tasks. Based on the results of these two studies, we provide a set of guidelines for DMove and further demonstrate two applications that utilize directional motions.
SP  - 444
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300674
ER  - 

TY  - NA
AU  - Strasnick, Evan; Agrawala, Maneesh; Follmer, Sean
TI  - CHI - Coupling Simulation and Hardware for Interactive Circuit Debugging
PY  - 2021
AB  - Simulation offers many advantages when designing analog circuits. Designers can explore alternatives quickly, without added cost or risk of hardware faults. However, it is challenging to use simulation as an aid during interactive debugging of physical circuits, due to difficulties in comparing simulated analyses with hardware measurements. Designers must continually configure simulations to match the state of the physical circuit (e.g. capturing sensor inputs), and must manually rework the hardware to replicate changes or analyses performed in simulation. We propose techniques leveraging instrumentation and programmable test hardware to create a tight coupling between a physical circuit and its simulated model. Bridging these representations helps designers to compare simulated and measured behaviors, and to quickly perform analytical techniques on hardware (e.g. parameter-response analysis) that are typically cumbersome outside of simulation. We implement these techniques in a prototype and show how it aids in efficiently debugging a variety of analog circuits.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445422
ER  - 

TY  - JOUR
AU  - Stadlbauer, Pascal; Mlakar, Daniel; Seidel, Hans-Peter; Steinberger, Markus; Zayer, Rhaleb
TI  - Interactive Modeling of Cellular Structures on Surfaces with Application to Additive Manufacturing
PY  - 2020
AB  - NA
SP  - 277
EP  - 289
JF  - Computer Graphics Forum
VL  - 39
IS  - 2
PB  - 
DO  - 10.1111/cgf.13929
ER  - 

TY  - CHAP
AU  - Matsumoto, Nagisa; Suzuki, Chihiro; Fujita, Koji; Sugiura, Yuta
TI  - HCI (17) - A Training System for Swallowing Ability by Visualizing the Throat Position
PY  - 2019
AB  - Our ability to swallow tends to decrease as we grow older. One reason for this is dysphagia, a condition that makes it difficult to swallow food well and causes malnutrition or food aspiration in severe cases. Therefore, preventing dysphagia is important in terms of maintaining quality of life. One way to prevent dysphagia is by using a throat raising exercise whereby we train our own throat muscles by consciously moving the throat. However, throat raising can be difficult because it is hard to see the motion of our own throat. In this work, we developed a system that helps users to move their own throat consciously. We designed a wearable device to visualize larynx position and a game to enhance user motivation. The proposed device measures the distance between skin surface and photo-reflective sensors and estimates the position of the larynx by means of a support vector machine. Experimental results showed that our proposed system can accurately estimate throat motion and that four out of five participants improved their ability to keep their throat in a high position for a long time by playing our proposed game. In future work, we will consider a system that can improve not only the muscle strength of the throat but also the comprehensive swallowing function.
SP  - 501
EP  - 511
JF  - Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management. Healthcare Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-22219-2_37
ER  - 

TY  - NA
AU  - Shi, Lei; Zhao, Yuhang; Penuela, Ricardo Gonzalez; Kupferstein, Elizabeth; Azenkot, Shiri
TI  - CHI - Molder: An Accessible Design Tool for Tactile Maps
PY  - 2020
AB  - Tactile materials are powerful teaching aids for students with visual impairments (VIs). To design these materials, designers must use modeling applications, which have high learning curves and rely on visual feedback. Today, Orientation and Mobility (O&M) specialists and teachers are often responsible for designing these materials. However, most of them do not have professional modeling skills, and many are visually impaired themselves. To address this issue, we designed Molder, an accessible design tool for interactive tactile maps, an important type of tactile materials that can help students learn O&M skills. A designer uses Molder to design a map using tangible input techniques, and Molder provides auditory feedback and high-contrast visual feedback. We evaluated Molder with 12 participants (8 with VIs, 4 sighted). After a 30-minute training session, the participants were all able to use Molder to design maps with customized tactile and interactive information.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376431
ER  - 

TY  - NA
AU  - Ro, Hyocheol; Park, Yoon Jung; Byun, Jung-Hyun; Han, Tack-Don
TI  - SIGGRAPH Posters - Display methods of projection augmented reality based on deep learning pose estimation
PY  - 2019
AB  - In this paper, we propose three display methods for projection-based augmented reality. In spatial augmented reality (SAR), determining where information, objects, or contents are to be displayed is a difficult and important issue. We use deep learning models to estimate user pose and suggest ways to solve the issue based on this data. Finally, each method can be appropriately applied according to various the applications and scenarios.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2019 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3306214.3338608
ER  - 

TY  - NA
AU  - Yu, Li; Abuella, Hisham; Islam, Zobaer; O'Hara, John F.; Crick, Christopher; Ekin, Sabit
TI  - Gesture Recognition using Reflected Visible and Infrared Light Wave Signals.
PY  - 2020
AB  - In this paper, we demonstrate the ability to recognize hand gestures in a non-contact, wireless fashion using only incoherent light signals reflected from a human subject. Fundamentally distinguished from radar, lidar and camera-based sensing systems, this sensing modality uses only a low-cost light source (e.g., LED) and sensor (e.g., photodetector). The light-wave-based gesture recognition system identifies different gestures from the variations in light intensity reflected from the subject's hand within a short (20-35 cm) range. As users perform different gestures, scattered light forms unique, statistically repeatable, time-domain signatures. These signatures can be learned by repeated sampling to obtain the training model against which unknown gesture signals are tested and categorized. Performance evaluations have been conducted with eight gestures, five subjects, different distances and lighting conditions, and with visible and infrared light sources. The results demonstrate the best hand gesture recognition performance of infrared sensing at 20 cm with an average of 96% accuracy. The developed gesture recognition system is low-cost, effective and non-contact technology for numerous Human-computer Interaction (HCI) applications.
SP  - NA
EP  - NA
JF  - arXiv: Signal Processing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Guo, Anhong; McVea, Saige; Wang, Xu; Clary, Patrick; Goldman, Ken; Li, Yang; Zhong, Yu; Bigham, Jeffrey P.
TI  - ASSETS - Investigating Cursor-based Interactions to Support Non-Visual Exploration in the Real World
PY  - 2018
AB  - The human visual system processes complex scenes to focus attention on relevant items. However, blind people cannot visually skim for an area of interest. Instead, they use a combination of contextual information, knowledge of the spatial layout of their environment, and interactive scanning to find and attend to specific items. In this paper, we define and compare three cursor-based interactions to help blind people attend to items in a complex visual scene: window cursor (move their phone to scan), finger cursor (point their finger to read), and touch cursor (drag their finger on the touchscreen to explore). We conducted a user study with 12 participants to evaluate the three techniques on four tasks, and found that: window cursor worked well for locating objects on large surfaces, finger cursor worked well for accessing control panels, and touch cursor worked well for helping users understand spatial layouts. A combination of multiple techniques will likely be best for supporting a variety of everyday tasks for blind users.
SP  - 3
EP  - 14
JF  - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3234695.3236339
ER  - 

TY  - NA
AU  - Lee, Jong-In; Asente, Paul; Kim, Byungmoon; Kim, Yeojin; Stuerzlinger, Wolfgang
TI  - VRST - Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments
PY  - 2020
AB  - Virtual environments with a wide range of scales are becoming commonplace in Virtual Reality applications. Methods to control locomotion parameters can help users explore such environments more easily. For multi-scale virtual environments, point-and-teleport locomotion with a well-designed distance control method can enable mid-air teleportation, which makes it competitive to flying interfaces. Yet, automatic distance control for point-and-teleport has not been studied in such environments. We present a new method to automatically control the distance for point-and-teleport. In our first user study, we used a solar system environment to compare three methods: automatic distance control for point-and-teleport, manual distance control for point-and-teleport, and automatic speed control for flying. Results showed that automatic control significantly reduces overshoot compared with manual control for point-and-teleport, but the discontinuous nature of teleportation made users prefer flying with automatic speed control. We conducted a second study to compare automatic-speed-controlled flying and two versions of our teleportation method with automatic distance control, one incorporating optical flow cues. We found that point-and-teleport with optical flow cues and automatic distance control was more accurate than flying with automatic speed control, and both were equally preferred to point-and-teleport without the cues.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418961
ER  - 

TY  - CHAP
AU  - , 
TI  - TEI in the Wild
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544571
ER  - 

TY  - JOUR
AU  - Zhang, Yunbo; Kwok, Tsz-Ho
TI  - Customization and topology optimization of compression casts/braces on two-manifold surfaces
PY  - 2019
AB  - Abstract This paper applies the topology optimization (TO) technique to the design of custom compression casts/braces on two-manifold mesh surfaces. Conventional braces or casts, usually made of plaster or fiberglass, have the drawbacks of being heavy and unventilated to wear. To reduce the weight and improve the performance of a custom brace, TO methods are adopted to optimize the geometry of the brace in the three-dimensional (3D) space, but they are computationally expensive. Based on our observation that the brace has a much smaller thickness compared to other dimensions and the applied loads are normal forces, this paper presents a novel TO method based on thin plate elements on the two-dimensional manifold (2-manifold) surfaces instead of 3D solid elements. Our working pipeline starts from a 3D scan of a human body represented by a 2-manifold mesh surface, which is the base design domain for the custom brace. Similar to the concept of isoparametric representation, the 3D design domain is mapped onto a two-dimensional (2D) parametric domain. An Finite Element Analysis (FEA) with bending moments is performed on the parameterized 2D design domain, and the Solid Isotropic Material with Penalization (SIMP) method is applied to optimize the pattern in the parametric domain. After the optimized cast/brace is obtained on the 2-manifold mesh surface, a solid model is generated by our design interface and then sent to a 3D printer for fabrication. Compared with the optimization method with solid elements, our method is more efficient and controllable due to the high efficiency of solving FEA in the 2D domain.
SP  - 113
EP  - 122
JF  - Computer-Aided Design
VL  - 111
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2019.02.005
ER  - 

TY  - NA
AU  - Zhao, Yiwei; Follmer, Sean
TI  - CHI - A Functional Optimization Based Approach for Continuous 3D Retargeted Touch of Arbitrary, Complex Boundaries in Haptic Virtual Reality
PY  - 2018
AB  - Passive or actuated physical props can provide haptic feedback, leading to a satisfying sense of presence and realism in virtual reality. However, the mismatch between the physical and virtual surfaces (boundaries) can diminish user experience. Haptic retargeting can overcome this limitation by utilizing visio-haptic effects. Previous investigations in haptic retargeting have focused on methods for point based position retargeting and techniques for remapping 2D shapes or simple 3D shape changes. Our approach extends haptic retargeting to complex, arbitrary shapes that provide a continuous mapping across all points on a boundary. This new approach also allows for multi-finger interaction. We describe a functional optimization to find the ideal spatial warping function with different goals: a maximum mapping smoothness, a minimum mismatch between the real and virtual world, or the combination of the two. We report on a preliminary user study of different optimization goals and elaborate potential applications through a set of demonstrations.
SP  - 544
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174118
ER  - 

TY  - NA
AU  - Sun, Junwei; Foley, Margaret; Xu, Qiang; Li, Chenhe; Li, Jun; Irani, Pourang; Li, Wei
TI  - PenShaft: Enabling Pen Shaft Detection and Interaction for Touchscreens
PY  - 2021
AB  - PenShaft is a battery-free, easy to implement solution for augmenting the shaft of a capacitive pen with interactive capabilities. By applying conductive materials in a specific pattern on the pen’s shaft, we can detect when it is laid on a capacitive touchscreen. This enables on-pen interactions, such as button clicks or swiping, and whole-shaft interactions, such as rotating and dragging the stylus. PenShaft supports at least six interactions, freeing the pen from having to interact with the layers or menus of a conventional user interface. We validate a device’s capability to detect these six interactions with all but two interactions achieving a success rate above 95%. We then present a series of applications to demonstrate the flexibility and utility of these interactions when using the pen.
SP  - NA
EP  - NA
JF  - 12th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460881.3460934
ER  - 

TY  - NA
AU  - Günther, Sebastian; Rasch, Julian; Schön, Dominik; Müller, Florian; Schmitz, Martin; Riemann, Jan; Matviienko, Andrii; Mühlhäuser, Max
TI  - Smooth as Steel Wool: Effects of Visual Stimuli on the Haptic Perception of Roughness in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517454
ER  - 

TY  - NA
AU  - Cao, Yetong; Chen, Huijie; Li, Fan; Wang, Yu
TI  - INFOCOM - CanalScan: Tongue-Jaw Movement Recognition via Ear Canal Deformation Sensing
PY  - 2021
AB  - Human-machine interface based on tongue-jaw movements has recently become one of the major technological trends. However, existing schemes have several limitations, such as requiring dedicated hardware and are usually uncomfortable to wear. This paper presents CanalScan, a nonintrusive system for tongue-jaw movement recognition using only commodity speaker and microphone mounted on ubiquitous off-the-shelf devices (e.g., smartphones). The basic idea is to send an acoustic signal, then captures its reflections and derive unique patterns of ear canal deformation caused by tongue-jaw movements. A dynamic segmentation method with Support Vector Domain Description is used to segment tongue-jaw movements. To combat sensor position-sensitive deficiency and ear-canal-shape-sensitive deficiency in multi-path reflections, we first design algorithms to assist users in adjusting the acoustic sensors to the same valid zone. Then we propose a data transformation mechanism to reduce the impacts of diversities in ear canal shapes and relative positions between sensors and the ear canal. CanalScan explores twelve unique and consistent features and applies a Random Forest classifier to distinguish tongue-jaw movements. Extensive experiments with twenty participants demonstrate that CanalScan achieves promising recognition for six tongue-jaw movements, is robust against various usage scenarios, and can be generalized to new users without retraining and adaptation.
SP  - 1
EP  - 10
JF  - IEEE INFOCOM 2021 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom42981.2021.9488852
ER  - 

TY  - JOUR
AU  - Li, Zhipeng; Jiang, Yu; Zhu, Yihao; Chen, Ruijia; Wang, Ruolin; Wang, Yuntao; Yan, Yukang; Shi, Yuanchun
TI  - Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention
PY  - 2022
AB  - <jats:p>An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534590
ER  - 

TY  - NA
AU  - Ma, Dong; Ferlini, Andrea; Mascolo, Cecilia
TI  - MobiSys - OESense: employing occlusion effect for in-ear human sensing
PY  - 2021
AB  - Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3% step counting recall, 98.3% recognition recall for 5 activities, and 97.0% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds' fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.
SP  - 175
EP  - 187
JF  - Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458864.3467680
ER  - 

TY  - NA
AU  - Han, Changyo; Takahashi, Ryo; Yahagi, Yuchi; Naemura, Takeshi
TI  - CHI Extended Abstracts - 3D Printing Firm Inflatables with Internal Tethers
PY  - 2021
AB  - This paper presents a technique for 3D printing firm inflatables with consumer-grade fused-deposition modeling (FDM) 3D printers and flexible filaments. By printing bridges inside the inflatable to tie its walls, internal tethers can retain the shape of the surfaces when inflated. This internal structure gives extra stiffness to the inflatables while retaining them lightweight and portable; the inflatables can be squished down to reduce the volume and inflated back to a sturdy state. Compared to conventional drop-stitch fabrics, the length of internal tethers can be easily varied owing to 3D printing, allowing us to fabricate angled surfaces as well as parallel surfaces. We evaluate the physical properties of the 3D-printed inflatables with internal tethers made with diverse printing parameters. Finally, we demonstrate the feasibility of our technique in custom inflatable design with example applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451613
ER  - 

TY  - JOUR
AU  - Ye, Hui; Kwan, Kin Chung; Su, Wanchao; Fu, Hongbo
TI  - ARAnimator: in-situ character animation in mobile AR with user-defined motion gestures
PY  - 2020
AB  - Creating animated virtual AR characters closely interacting with real environments is interesting but difficult. Existing systems adopt video see-through approaches to indirectly control a virtual character in mobile AR, making close interaction with real environments not intuitive. In this work we use an AR-enabled mobile device to directly control the position and motion of a virtual character situated in a real environment. We conduct two guessability studies to elicit user-defined motions of a virtual character interacting with real environments, and a set of user-defined motion gestures describing specific character motions. We found that an SVM-based learning approach achieves reasonably high accuracy for gesture classification from the motion data of a mobile device. We present ARAnimator, which allows novice and casual animation users to directly represent a virtual character by an AR-enabled mobile phone and control its animation in AR scenes using motion gestures of the device, followed by animation preview and interactive editing through a video see-through interface. Our experimental results show that with ARAnimator, users are able to easily create in-situ character animations closely interacting with different real environments.
SP  - 83
EP  - NA
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 4
PB  - 
DO  - 10.1145/3386569.3392404
ER  - 

TY  - JOUR
AU  - Wen, Zhen; Zeng, Wei; Weng, Luoxuan; Liu, Yihan; Xu, Mingliang; Chen, Wei
TI  - Effects of View Layout On Situated Analytics for Multiple-View Representations in Immersive Visualization.
PY  - 2022
AB  - Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209475
ER  - 

TY  - NA
AU  - Büschel, Wolfgang; Lehmann, Anke; Dachselt, Raimund
TI  - CHI - MIRIA: A Mixed Reality Toolkit for the In-Situ Visualization and Analysis of Spatio-Temporal Interaction Data
PY  - 2021
AB  - In this paper, we present MIRIA, a Mixed Reality Interaction Analysis toolkit designed to support the in-situ visual analysis of user interaction in mixed reality and multi-display environments. So far, there are few options to effectively explore and analyze interaction patterns in such novel computing systems. With MIRIA, we address this gap by supporting the analysis of user movement, spatial interaction, and event data by multiple, co-located users directly in the original environment. Based on our own experiences and an analysis of the typical data, tasks, and visualizations used in existing approaches, we identify requirements for our system. We report on the design and prototypical implementation of MIRIA, which is informed by these requirements and offers various visualizations such as 3D movement trajectories, position heatmaps, and scatterplots. To demonstrate the value of MIRIA for real-world analysis tasks, we conducted expert feedback sessions using several use cases with authentic study data.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445651
ER  - 

TY  - NA
AU  - Torres, César I.
TI  - Hybrid Aesthetics: Bridging Material Practices and Digital Fabrication through Computational Crafting Proxies
PY  - 2019
AB  - Author(s): Torres, Cesar Armando | Advisor(s): Paulos, Eric | Abstract: Creative technologies like digital fabrication led to the rise of the Maker movement, engendering grassroots innovation in education, manufacturing, and healthcare. Today, these creative technologies stand at a crossroads – despite a significant rise in participation, a deeper engagement with design and material is absent from traditional computer-aided design workflows. In this thesis, I will motivate the need for creative technologies to support the morphogenetic model of making, a thinking and working style characteristic of how practitioners work with physical materials but difficult to access in digital design tools. To communicate my findings, I introduce the concept of a Crafting Proxy, an intermediary between a practitioner and a material that can be used to facilitate the interpretation, manipulation, and evaluation of a material as a part of a creative process. In these works, I employ a Research through Design (RtD) methodology to construct intermediate-level knowledge around the design, implementation, and evaluation of Crafting Proxies. I’ll demonstrate how Crafting Proxies can be enacted within physical materials, physical tools, and physical practices to support morphogenetic workflows in domains such as light and heater design, and metalworking. As a result, this work contributes a design method for creating crafting proxies and a set of design principles that inform how new materials and digital fabrication technologies can foreground the existing knowledge and practices of material practitioners and generate new forms and aesthetics that can alter the trajectory of the Maker movement towards a New Making Renaissance.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Thévin, Lauren; Briant, Carine; Brock, Anke
TI  - X-Road: Virtual Reality Glasses for Orientation and Mobility Training of People with Visual Impairments
PY  - 2020
AB  - Orientation and Mobility (O8M) classes teach people with visual impairments how to navigate the world; for instance, how to cross a road. Yet, this training can be difficult and dangerous due to conditions such as traffic and weather. Virtual Reality (VR) can overcome these challenges by providing interactive controlled environments. However, most existing VR tools rely on visual feedback, which limits their use with students with visual impairment. In a collaborative design approach with O8M instructors, we designed an affordable and accessible VR system for O8M classes, called X-Road. Using a smartphone and a Bespoke headmount, X-Road provides both visual and audio feedback and allows users to move in space as in the real world. In a study with 13 students with visual impairments, X-Road proved to be an effective alternative to teaching and learning classical O8M tasks, and both students and instructors were enthusiastic about this technology. We conclude with design recommendations for inclusive VR systems.
SP  - 1
EP  - 47
JF  - ACM Transactions on Accessible Computing
VL  - 13
IS  - 2
PB  - 
DO  - 10.1145/3377879
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong
TI  - UIST - Proxino: Enabling Prototyping of Virtual Circuits with Physical Proxies
PY  - 2019
AB  - We propose blending the virtual and physical worlds for prototyping circuits using physical proxies. With physical proxies, real-world components (e.g. a motor, or light sensor) can be used with a virtual counterpart for a circuit designed in software. We demonstrate this concept in Proxino, and elucidate the new scenarios it enables for makers, such as remote collaboration with physically distributed electronics components. We compared our hybrid system and its output with designs of real circuits to determine the difference through a system evaluation and observed minimal differences. We then present the results of an informal study with 9 users, where we gathered feedback on the effectiveness of our system in different working conditions (with a desktop, using a mobile, and with a remote collaborator). We conclude by sharing our lessons learned from our system and discuss directions for future research that blend physical and virtual prototyping for electronic circuits.
SP  - 121
EP  - 132
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347938
ER  - 

TY  - NA
AU  - Guerrero, Alfrancis
TI  - The Influence of Flexural Stiffness on the Performance and Preference of Bendable Stylus Interfaces
PY  - 2020
AB  - Flexible sensing styluses deliver additional degrees of input for pen-based interaction, yet no research on bendable styluses has looked at the influence of stiffness on performance or its integration with creative digital applications. We conducted two experiments that evaluated the influence of flexural stiffness on users' performance. To address this, we designed HyperBrush a flexible sensing stylus with interchangeable flexible components. We assessed the performance of different flexibilities on a bend menu technique. While bend input in styluses can perform similarly to pressure input, researchers only have measured stationary input. We furthered this discussion by evaluating performance of simultaneous bend and X-Y pen movement. We conducted a third experiment that investigated how HyperBrush can be a beneficial tool to support users' creativity for digital drawing. We concluded that different flexibilities can pose their own unique advantages analogous to an artist's assortment of paintbrushes.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Parizi, Farshid Salemi; Whitmire, Eric; Patel, Shwetak
TI  - AuraRing
PY  - 2022
AB  - <jats:p>Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. It has a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.</jats:p>
SP  - 85
EP  - 92
JF  - Communications of the ACM
VL  - 65
IS  - 10
PB  - 
DO  - 10.1145/3556639
ER  - 

TY  - JOUR
AU  - Parizi, Farshid Salemi; Whitmire, Eric; Patel, Shwetak N.
TI  - AuraRing: Precise Electromagnetic Finger Tracking
PY  - 2019
AB  - Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. We develop two different approaches to pose reconstruction---a first-principles iterative approach and a closed-form neural network approach. Notably, AuraRing requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 4
PB  - 
DO  - 10.1145/3369831
ER  - 

TY  - JOUR
AU  - Ma, Dong; Ferlini, Andrea; Mascolo, Cecilia
TI  - Innovative Human Motion Sensing With Earbuds
PY  - 2022
AB  - <jats:p>Earbuds, ear-worn wearables, have attracted growing attention from both industry and academia. This trend has witnessed manufacturers embedding multiple sensors on earbuds to enrich their functionalities. For example, Apple AirPods, Sony WF-1000XM3, and Bose QuietControl 30, have been equipped with accelerometers for tapping interaction or multiple microphones for noise cancellation. On the other hand, the research community regards earbuds as a powerful personal-scale human sensing and computing platform. By integrating sensors like PPG, barometer, and ultrasonic sensors, researchers have been devising a plethora of earable sensing applications, such as blood pressure monitoring [1], facial expression recognition [2], and authentication [3].</jats:p>
SP  - 24
EP  - 29
JF  - GetMobile: Mobile Computing and Communications
VL  - 25
IS  - 4
PB  - 
DO  - 10.1145/3529706.3529713
ER  - 

TY  - NA
AU  - Hanagata, Shin; Kakehi, Yasuaki
TI  - AH - Paralogue: A Remote Conversation System Using a Hand Avatar which Postures are Controlled with Electrical Muscle Stimulation
PY  - 2018
AB  - In this paper, we propose a new method of communication with a remote partner. The system, which we call "Paralogue" (parasitic + dialogue), utilizes the user's arm and a hand as an avatar that represents the remote conversation partner. This method is realized by controlling the movement of the user's arm with electrical muscle stimulations(EMS) and making the user's arm behave as if the remote conversation partner were speaking to the user. Therefore, it is possible to add the physical presence of the remote conversation partner to the situation. This can also be a telepresence system using real human arms. However, unlike many telepresence systems, external actuators and large-sized devices are not required for this system and can be completed by only utilizing physical interaction. In this paper, we present its design and implementation.
SP  - 35
EP  - NA
JF  - Proceedings of the 9th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3174910.3174951
ER  - 

TY  - JOUR
AU  - Nejati, Javad; Balasubramanian, Aruna
TI  - WProfX: A Fine-grained Visualization Tool for Web Page Loads
PY  - 2020
AB  - Web page performance is crucial in today's Internet ecosystem, and Web developers use various developer tools to analyze their page load performance. However, existing tools cannot be used to identify the critical bottlenecks during the page load process. In this work, we design an online tool called WProfX that allows Web developers to visually identify bottlenecks in their page structure. The key to WProfX is that unlike existing Web performance tools, WProfX not only visualizes the page load activity timings, but also extracts the dependencies between the activities. Using the dependency structure, WProfX identifies the critical bottleneck activities. This lets a developer quickly identify why their page is loading slow and conduct what-if analyses to study the effect of different optimizations. WProfX uses low-level tracing information exposed by most major browsers to extract the relationship between page load activities. The result is that WProfX works with most major browsers and newer browser versions. WProfX visualizes the page load process as a dependency graph of semantically meaningful Web activities and identifies the critical bottlenecks. We evaluate WProfX with 14 Web developers who perform three what-if analysis tasks involving identifying the page load bottleneck and evaluating the effect of a page optimization. All the participants were able to complete the tasks with WProfX, compared to less than 60% when using the popular developer tools available today. WProfX is currently being used by Web developers in a large telecom and at a Silicon Valley startup.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - EICS
PB  - 
DO  - 10.1145/3394975
ER  - 

TY  - JOUR
AU  - Miyahara, Yuki; Kato, Ryu
TI  - Development of Thin Vibration Sheets Using a Shape Memory Alloy Actuator for the Tactile Feedback of Myoelectric Prosthetic hands.
PY  - 2021
AB  - Because the current myoelectric prosthetic hand does not have a tactile function, the user must always check the condition of the prosthetic hand. Various studies on sensory feedback have been conducted to address this problem, but several devices used in them cannot be integrated with artificial limbs, and wearing the devices is a burden on the user. To solve this problem, we developed thin vibration stimulation sheets using shape memory alloy (SMA) actuators. We then conducted an experiment on the effect of the change in shape at the contact part between the sheet and the skin on perception and confirmed that it would be easier to perceive vibration when the skin was deformed in a wider range. In addition, we investigated the number of distinguishable stimulus intensity levels and identification of stimulus positions. According to the results, the stimulus presented by the developed vibration sheet could be identified in three stages without learning about the stimulus, and the stimulation position by the vibration sheet could be identified with the same or higher accuracy as that of the disk-type vibration motor used in the existing research, although the accuracy decreased when vibrations were presented simultaneously.
SP  - 6255
EP  - NA
JF  - Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference
VL  - 2021
IS  - NA
PB  - 
DO  - 10.1109/embc46164.2021.9630855
ER  - 

TY  - NA
AU  - Ku, Pin-Sung; Huang, Kunpeng; Kao, Cindy Hsin-Liu
TI  - Patch-O: Deformable Woven Patches for On-body Actuation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517633
ER  - 

TY  - NA
AU  - Gonzalez, Eric J.; Abtahi, Parastoo; Follmer, Sean
TI  - UIST (Adjunct Volume) - Evaluating the Minimum Jerk Motion Model for Redirected Reach in Virtual Reality
PY  - 2019
AB  - Reach redirection in virtual reality uses spatial distortion to augment interaction with passive props as well as active haptic devices. For such dynamic physical systems, motion modeling is needed to update the interface based on users' predicted targets & trajectories. However, it remains unclear how well standard predictive models hold under redirection. In this work we evaluate one such commonly used model, the Minimum-Jerk (MJ) model, during redirected reach at various lateral offsets up to 16cm. Results show that larger redirection significantly worsens MJ model fit, suggesting that models should be adjusted for reaches with considerable redirection. Predicted arrival times, based on fitting an MJ model on the first half of reach data, led to an average error of -0.29s for redirected reach, compared to -0.03s for normal reach.
SP  - 4
EP  - 6
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357096
ER  - 

TY  - NA
AU  - Shi, Lei; Zhao, Yuhang; Azenkot, Shiri
TI  - ASSETS - Designing Interactions for 3D Printed Models with Blind People
PY  - 2017
AB  - Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.
SP  - 200
EP  - 209
JF  - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3132525.3132549
ER  - 

TY  - NA
AU  - Bloom, Rens; Zuniga, Marco; Wang, Qing; Giustiniano, Domenico
TI  - INFOCOM - Tweeting with Sunlight: Encoding Data on Mobile Objects
PY  - 2019
AB  - We analyze and optimize the performance of a new type of channel that exploits sunlight for wireless communication. Recent advances on visible light backscatter have shown that if mobile objects attach distinctive reflective patterns to their surfaces, simple photosensors deployed in our environments can decode the reflected light signals. Although the vision is promising, only initial feasibility studies have been performed so far. There is no analysis on how much information this channel can transmit or how reliable the links are. Achieving this vision is a complex endeavour because we have no control over (i) the sun or clouds, which determine the amount and direction of light intensity, and (ii) the mobile object, which determines the modulated reflection of sunlight. We investigate the impact of the surrounding light intensity and physical properties of the object (reflective materials, size and speed) to design a communication system that optimizes the encoding and decoding of information with sunlight. Our experimental evaluation, performed with a car moving on a regular street, shows that our analysis leads to significant improvements across many dimensions. Compared to the state of the art, we can encode seven times more information, and decode this information reliably from an object moving three times faster (53km/h) at a range that is four times longer (4m) and with three times lower light intensity (cloudy day).
SP  - 1324
EP  - 1332
JF  - IEEE INFOCOM 2019 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom.2019.8737410
ER  - 

TY  - NA
AU  - Yoon, Sang Ho; Ma, Siyuan; Lee, Woo Suk; Yadunath, Thakurdesai Shantanu; Sun, Di; Ribeiro, Flavio Protasio; Holbery, James David
TI  - UIST - HapSense: A Soft Haptic I/O Device with Uninterrupted Dual Functionalities of Force Sensing and Vibrotactile Actuation
PY  - 2019
AB  - We present HapSense, a single-volume soft haptic I/O device with uninterrupted dual functionalities of force sensing and vibrotactile actuation. To achieve both input and output functionalities, we employ a ferroelectric electroactive polymer as core functional material with a multilayer structure design. We introduce a haptic I/O hardware that supports tunable high driving voltage waveform for vibrotactile actuation while insitu sensing a change in capacitance from contact force. With mechanically soft nature of fabricated structure, HapSense can be embedded onto various object surfaces including but not limited to furniture, garments, and the human body. Through a series of experiments and evaluations, we characterized physical properties of HapSense and validated the feasibility of using soft haptic I/O with real users. We demonstrated a variety of interaction scenarios using HapSense.
SP  - 949
EP  - 961
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347888
ER  - 

TY  - JOUR
AU  - Zenner, André; Ullmann, Kristin; Krüger, Antonio
TI  - Combining Dynamic Passive Haptics and Haptic Retargeting for Enhanced Haptic Feedback in Virtual Reality
PY  - 2021
AB  - To provide immersive haptic experiences, proxy-based haptic feedback systems for virtual reality (VR) face two central challenges: (1) similarity, and (2) colocation. While to solve challenge (1), physical proxy objects need to be sufficiently similar to their virtual counterparts in terms of haptic properties, for challenge (2), proxies and virtual counterparts need to be sufficiently colocated to allow for seamless interactions. To solve these challenges, past research introduced, among others, two successful techniques: (a) Dynamic Passive Haptic Feedback (DPHF), a hardware-based technique that leverages actuated props adapting their physical state during the VR experience, and (b) Haptic Retargeting, a software-based technique leveraging hand redirection to bridge spatial offsets between real and virtual objects. Both concepts have, up to now, not ever been studied in combination. This paper proposes to combine both techniques and reports on the results of a perceptual and a psychophysical experiment situated in a proof-of-concept scenario focused on the perception of virtual weight distribution. We show that users in VR overestimate weight shifts and that, when DPHF and HR are combined, significantly greater shifts can be rendered, compared to using only a weight-shifting prop or unnoticeable hand redirection. Moreover, we find the combination of DPHF and HR to let significantly larger spatial dislocations of proxy and virtual counterpart go unnoticed by users. Our investigation is the first to show the value of combining DPHF and HR in practice, validating that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.
SP  - 2627
EP  - 2637
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067777
ER  - 

TY  - NA
AU  - Kim, Jin Hee (Heather); Patil, Shreyas Dilip; Matson, Sarina; Conroy, Melissa; Kao, Cindy Hsin-Liu
TI  - KnitSkin: Machine-Knitted Scaled Skin for Locomotion
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502142
ER  - 

TY  - NA
AU  - Matthews, Brandon J.; Thomas, Bruce H.; Von Itzstein, Stewart; Smith, Ross T.
TI  - VR - Remapped Physical-Virtual Interfaces with Bimanual Haptic Retargeting
PY  - 2019
AB  - This paper proposes a novel interface for virtual reality in which physical interface components are mapped to multiple virtual counterparts using haptic retargeting illusions. This gives virtual reality interfaces the ability to have correct haptic sensations for many virtual buttons although in the physical space there is only one. This is a generic system that can be applied to areas including design, interaction tasks, product prototype development and interactive games in virtual reality. The system presented extends existing retargeting algorithms to support asymmetric bimanual interactions. A new warp technique, called interface warp, was developed to support remapped virtual reality user interfaces. Through an experimental user study, we explore the effects of bimanual retargeting and the interface warp technique on task response time, errors, presence, perceived manipulation compared to unimanual (single handed) retargeting and other existing warp techniques. The results demonstrated faster task response time and less errors for the interface warp technique and shows no significant effect of bimanual interactions.
SP  - 19
EP  - 27
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797974
ER  - 

TY  - NA
AU  - Bondareva, Erika; Hauksdóttir, Elín Rós; Mascolo, Cecilia
TI  - Earables for Detection of Bruxism: a Feasibility Study
PY  - 2021
AB  - Bruxism is a disorder characterised by teeth grinding and clenching, and many bruxism sufferers are not aware of this disorder until their dental health professional notices permanent teeth wear. Stress and anxiety are often listed among contributing factors impacting bruxism exacerbation, which may explain why the COVID-19 pandemic gave rise to a bruxism epidemic. It is essential to develop tools allowing for the early diagnosis of bruxism in an unobtrusive manner. This work explores the feasibility of detecting bruxism-related events using earables in a mimicked in-the-wild setting. Using inertial measurement unit for data collection, we utilise traditional machine learning for teeth grinding and clenching detection. We observe superior performance of models based on gyroscope data, achieving an 88% and 66% accuracy on grinding and clenching activities, respectively, in a controlled environment, and 76% and 73% on grinding and clenching, respectively, in an in-the-wild environment.
SP  - NA
EP  - NA
JF  - arXiv: Learning
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhu, Mengjia; Memar, Amirhossein H.; Gupta, Aakar; Samad, Majed; Agarwal, Priyanshu; Visell, Yon; Keller, Sean Jason; Colonnese, Nicholas
TI  - CHI - PneuSleeve: In-fabric Multimodal Actuation and Sensing in a Soft, Compact, and Expressive Haptic Sleeve
PY  - 2020
AB  - Integration of soft haptic devices into garments can improve their usability and wearability for daily computing interactions. In this paper, we introduce PneuSleeve, a fabric-based, compact, and highly expressive forearm sleeve which can render a broad range of haptic stimuli including compression, skin stretch, and vibration. The haptic stimuli are generated by controlling pneumatic pressure inside embroidered stretchable tubes. The actuation configuration includes two compression actuators on the proximal and distal forearm, and four uniformly distributed linear actuators around and tangent to the forearm. Further, to ensure a suitable grip force, two soft mutual capacitance sensors are fabricated and integrated into the compression actuators, and a closed-loop force controller is implemented. We physically characterize the static and dynamic behavior of the actuators, as well as the performance of closed-loop control. We quantitatively evaluate the psychophysical characteristics of the six actuators in a set of user studies. Finally, we show the expressiveness of PneuSleeve by evaluating combined haptic stimuli using subjective assessments.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376333
ER  - 

TY  - CHAP
AU  - Khakurel, Jayden; Knutas, Antti; Melkas, Helinä; Penzenstadler, Birgit; Fu, Bo; Porras, Jari
TI  - HCI (7) - Categorization Framework for Usability Issues of Smartwatches and Pedometers for the Older Adults
PY  - 2018
AB  - In recent years various usability issues related to device characteristics of quantified-self wearables such as smartwatches and pedometers have been identified which appear likely to impact device adoption among the older adults. However, an overall framework has not yet been developed to provide a comprehensive set of usability issues related to smartwatches and pedometers. This study used a two-stage research approach with 33 older participants, applying contextual action theory and usability evaluation methods both to determine perceived usability issues and to formulate a usability categorization framework based on identified issues. Additionally, we prioritized the predominant usability issues of smartwatches and pedometers that warrant immediate attention from technology designers, the research community, and application developers. Results revealed predominant usability issues related to the following device characteristics of smartwatches: user interface (font size, interaction techniques such as notification, button location) and hardware (screen size); and of pedometers: user interface (font size, interaction techniques such as notification, button location, and tap detection) and hardware (screen size).
SP  - 91
EP  - 106
JF  - Universal Access in Human-Computer Interaction. Methods, Technologies, and Users
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-92049-8_7
ER  - 

TY  - NA
AU  - Gong, Jun; Gupta, Aakar; Benko, Hrvoje
TI  - UIST - Acustico: Surface Tap Detection and Localization using Wrist-based Acoustic TDOA Sensing
PY  - 2020
AB  - In this paper, we present Acustico, a passive acoustic sensing approach that enables tap detection and 2D tap localization on uninstrumented surfaces using a wrist-worn device. Our technique uses a novel application of acoustic time differences of arrival (TDOA) analysis. We adopt a sensor fusion approach by taking both 'surface waves' (i.e., vibrations through surface) and 'sound waves' (i.e., vibrations through air) into analysis to improve sensing resolution. We carefully design a sensor configuration to meet the constraints of a wristband form factor. We built a wristband prototype with four acoustic sensors, two accelerometers and two microphones. Through a 20-participant study, we evaluated the performance of our proposed sensing technique for tap detection and localization. Results show that our system reliably detects taps with an F1-score of 0.9987 across different environmental noises and yields high localization accuracies with root-mean-square-errors of 7.6mm (X-axis) and 4.6mm (Y-axis) across different surfaces and tapping techniques.
SP  - 406
EP  - 419
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415901
ER  - 

TY  - BOOK
AU  - Reinholt, Kyle; Guinness, Darren; Kane, Shaun K.
TI  - ISS - EyeDescribe: Combining Eye Gaze and Speech to Automatically Create Accessible Touch Screen Artwork
PY  - 2019
AB  - Many images on the Web, including photographs and artistic images, feature spatial relationships between objects that are inaccessible to someone who is blind or visually impaired even when a text description is provided. While some tools exist to manually create accessible image descriptions, this work is time consuming and requires specialized tools. We introduce an approach that automatically creates spatially registered image labels based on how a sighted person naturally interacts with the image. Our system collects behavioral data from sighted viewers of an image, specifically eye gaze data and spoken descriptions, and uses them to generate a spatially indexed accessible image that can then be explored using an audio-based touch screen application. We describe our approach to assigning text labels to locations in an image based on eye gaze. We then report on two formative studies with blind users testing EyeDescribe. Our approach resulted in correct labels for all objects in our image set. Participants were able to better recall the location of objects when given both object labels and spatial locations. This approach provides a new method for creating accessible images with minimum required effort.
SP  - 101
EP  - 112
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359722
ER  - 

TY  - JOUR
AU  - Song, Jean Y.; Fok, Raymond; Kim, Juho; Lasecki, Walter S.
TI  - FourEyes: Leveraging Tool Diversity as a Means to Improve Aggregate Accuracy in Crowdsourcing
PY  - 2019
AB  - Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging, because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this article, we introduce a novel crowdsourcing approach that leverages tool diversity as a means of improving aggregate crowd performance. Our idea is that given a diverse set of tools, answer aggregation done across tools can help improve the collective performance by offsetting systematic biases induced by the individual tools themselves. To demonstrate the effectiveness of the proposed approach, we design four different tools and present FourEyes, a crowd-powered image segmentation system that uses aggregation across different tools. We then conduct a series of studies that evaluate different aggregation conditions and show that using multiple tools can significantly improve aggregate accuracy. Furthermore, we investigate the idea of applying post-processing for multi-tool aggregation in terms of correction mechanism. We introduce a novel region-based method for synthesizing more accurate bounds for image segmentation tasks through averaging surrounding annotations. In addition, we explore the effect of adjusting the threshold parameter of an EM-based aggregation method. Our results suggest that not only the individual tool’s design, but also the correction mechanism, can affect the performance of multi-tool aggregation. This article extends a work presented at ACM IUI 2018 [46] by providing a novel region-based error-correction method and additional in-depth evaluation of the proposed approach.
SP  - 1
EP  - 30
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 10
IS  - 1
PB  - 
DO  - 10.1145/3237188
ER  - 

TY  - JOUR
AU  - Umezawa, Akino; Takegawa, Yoshinari; Hirata, Keiji; Sugiura, Yuta
TI  - e2-Mask：顔の外見を変える仮面型ディスプレイを用いた緊張緩和に関する効果の検証
PY  - 2021
AB  - NA
SP  - 682
EP  - 690
JF  - The Journal of The Institute of Image Information and Television Engineers
VL  - 75
IS  - 5
PB  - 
DO  - 10.3169/itej.75.682
ER  - 

TY  - NA
AU  - Mazursky, Alex; Teng, Shan-Yuan; Nith, Romain; Lopes, Pedro
TI  - CHI - MagnetIO: Passive yet Interactive Soft Haptic Patches Anywhere
PY  - 2021
AB  - We propose a new type of haptic actuator, which we call MagnetIO, that is comprised of two parts: one battery-powered voice-coil worn on the user's fingernail and any number of interactive soft patches that can be attached onto any surface (everyday objects, user's body, appliances, etc.). When the user's finger wearing our voice-coil contacts any of the interactive patches it detects its magnetic signature via magnetometer and vibrates the patch, adding haptic feedback to otherwise input-only interactions. To allow these passive patches to vibrate, we make them from silicone with regions doped with polarized neodymium powder, resulting in soft and stretchable magnets. This stretchable form-factor allows them to be wrapped to the user's body or everyday objects of various shapes. We demonstrate how these add haptic output to many situations, such as adding haptic buttons to the walls of one's home. In our technical evaluation, we demonstrate that our interactive patches can be excited across a wide range of frequencies (0-500 Hz) and can be tuned to resonate at specific frequencies based on the patch's geometry. Furthermore, we demonstrate that MagnetIO's vibration intensity is as powerful as a typical linear resonant actuator (LRA); yet, unlike these rigid actuators, our passive patches operate as springs with multiple modes of vibration, which enables a wider band around its resonant frequency than an equivalent LRA.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445543
ER  - 

TY  - NA
AU  - Zhao, Haibin; Röddiger, Tobias; Beigl, Michael
TI  - UbiComp/ISWC Adjunct - AirCase: Earable Charging Case with Air Quality Monitoring and Soundscape Sonification
PY  - 2021
AB  - Bad air quality and insufficient ventilation can have severe impacts on personal health. We present AirCase, a smart earable charging case that measures CO2, volatile organic compounds, humidity, air pressure, temperature, and light intensity. The case powers both the air quality system and the earables. We also propose a model-driven air quality soundscape sonification strategy based on the audio capabilities of the earables. AirCase detects conditions unsuitable for measuring air quality (e.g., in pocket) in an office environment at 98.2 % accuracy with a simple classifier based on a single feature. We identified light intensity as the primary indicator to recognize occlusion. In contrast, the speed of the micro ventilator used to increase airflow inside the case did not offer any predictive value. In the future, we hope to see more researchers explore the hidden potential of the new platform.
SP  - 180
EP  - 184
JF  - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460418.3479329
ER  - 

TY  - JOUR
AU  - Zhu, Mengjia; Biswas, Shantonu; Dinulescu, Stejara Iulia; Kastor, Nikolas; Hawkes, Elliot Wright; Visell, Yon
TI  - Soft, Wearable Robotics and Haptics: Technologies, Trends, and Emerging Applications
PY  - 2022
AB  - Recent advances in the rapidly growing field of soft robotics highlight the potential for innovations in wearable soft robotics to meet challenges and opportunities affecting individuals, society, and the economy. Some of the most promising application areas include wearable haptic interfaces, assistive robotics, and biomedical devices. Several attributes of soft robotic systems make them well-suited for use in human-wearable applications. Such systems can be designed to accommodate the complex morphology and movements of the human body, can afford sufficient compliance to ensure safe operation in intimate proximity with humans, and can provide context-appropriate haptic feedback or assistance to their wearers. Many soft robotic systems have been designed to resemble garments or wearables that are already widely used today. Such systems could one day become seamlessly integrated into a myriad of human activities and environments. Here, we review emerging advances in wearable soft robotic technologies and systems, including numerous examples from prior research. We discuss important considerations for the design of such systems based on functional concerns, wearability, and ergonomics. We describe an array of design strategies that have been adopted in prior research. We review wearable soft robotics applications in diverse domains, survey sensing and actuation technologies, materials, and fabrication methods. We conclude by discussing frontiers, challenges, and future prospects for soft, wearable robotics.
SP  - 246
EP  - 272
JF  - Proceedings of the IEEE
VL  - 110
IS  - 2
PB  - 
DO  - 10.1109/jproc.2021.3140049
ER  - 

TY  - NA
AU  - Vechev, Velko; Hinchet, Ronan; Coros, Stelian; Thomaszewski, Bernhard; Hilliges, Otmar
TI  - Computational Design of Active Kinesthetic Garments
PY  - 2022
AB  - Garments with the ability to provide kinesthetic force-feedback on-demand can augment human capabilities in a non-obtrusive way, enabling numerous applications in VR haptics, motion assistance, and robotic control. However, designing such garments is a complex, and often manual task, particularly when the goal is to resist multiple motions with a single design. In this work, we propose a computational pipeline for designing connecting structures between active components - one of the central challenges in this context. We focus on electrostatic (ES) clutches that are compliant in their passive state while strongly resisting elongation when activated. Our method automatically computes optimized connecting structures that efficiently resist a range of pre-defined body motions on demand. We propose a novel dual-objective optimization approach to simultaneously maximize the resistance to motion when clutches are active, while minimizing resistance when inactive. We demonstrate our method on a set of problems involving different body sites and a range of motions. We further fabricate and evaluate a subset of our automatically created designs against manually created baselines using mechanical testing and in a VR pointing study.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545674
ER  - 

TY  - CHAP
AU  - , 
TI  - Vignettes
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544565
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Wang, Wen-Ping; Chiang, Chi-huan; Wu, Te-Yen; Xu, Zheer; Luo, Justin; Chen, Bing-Yu; Yang, Xing-Dong
TI  - CHI - AccessibleCircuits: Adaptive Add-On Circuit Components for People with Blindness or Low Vision
PY  - 2021
AB  - In this paper, we propose the designs for low cost and 3D-printable add-on components to adapt existing breadboards, circuit components and electronics tools for blind or low vision (BLV) users. Through an initial user study, we identified several barriers to entry for beginners with BLV in electronics and circuit prototyping. These barriers guided the design and development of our add-on components. We focused on developing adaptations that provide additional information about the specific component pins and breadboard holes, modify tools to make them easier to use for users with BLV, and expand non-visual feedback (e.g., audio, tactile) for tasks that require vision. Through a second user study, we demonstrated that our adaptations can effectively overcome the accessibility barriers in breadboard circuit prototyping for users with BLV.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445690
ER  - 

TY  - JOUR
AU  - Freire, Maria L. Montoya; Oulasvirta, Antti; Di Francesco, Mario
TI  - Inverse Foraging: Inferring Users' Interest in Pervasive Displays
PY  - 2021
AB  - Users' engagement with pervasive displays has been extensively studied, however, determining how their content is interesting remains an open problem. Tracking of body postures and gaze has been explored as an indication of attention; still, existing works have not been able to estimate the interest of passers-by from readily available data, such as the display viewing time. This article presents a simple yet accurate method of estimating users' interest in multiple content items shown at the same time on displays. The proposed approach builds on the information foraging theory, which assumes that users optimally decide on the content they consume. Through inverse foraging, the parameters of a foraging model are fitted to the values of viewing times observed in practice, to yield estimates of user interest. Different foraging models are evaluated by using synthetic data and with a controlled user study. The results demonstrate that inverse foraging accurately estimates interest, achieving an R2 above 70% in comparison to self-reported interest. As a consequence, the proposed solution allows to dynamically adapt the content shown on pervasive displays, based on viewing data that can be easily obtained in field deployments.
SP  - 1
EP  - 18
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478103
ER  - 

TY  - NA
AU  - Seyed, Teddy; Tang, Anthony
TI  - Conference on Designing Interactive Systems - Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways
PY  - 2019
AB  - Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.
SP  - 317
EP  - 329
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322305
ER  - 

TY  - JOUR
AU  - Yan, Zihan; Zhou, Jiayi; Wu, Yufei; Liu, Guanhong; Luo, Danli; Zhou, Zihong; Mi, Haipeng; Sun, Lingyun; Chen, Xiang 'Anthony'; Tao, Ye; Zhang, Yang; Wang, Guanyun
TI  - Shoes++
PY  - 2022
AB  - <jats:p>Feet are the foundation of our bodies that not only perform locomotion but also participate in intent and emotion expression. Thus, foot gestures are an intuitive and natural form of expression for interpersonal interaction. Recent studies have mostly introduced smart shoes as personal gadgets, while foot gestures used in multi-person foot interactions in social scenarios remain largely unexplored. We present Shoes++, which includes an inertial measurement unit (IMU)-mounted sole and an input vocabulary of social foot-to-foot gestures to support foot-based interaction. The gesture vocabulary is derived and condensed by a set of gestures elicited from a participatory design session with 12 users. We implement a machine learning model in Shoes++ which can recognize two-person and three-person social foot-to-foot gestures with 94.3% and 96.6% accuracies (N=18). In addition, the sole is designed to easily attach to and detach from various daily shoes to support comfortable social foot interaction without taking off the shoes. Based on users' qualitative feedback, we also found that Shoes++ can support team collaboration and enhance emotion expression, thus making social interactions or interpersonal dynamics more engaging in an expanded design space.</jats:p>
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534620
ER  - 

TY  - JOUR
AU  - Chen, Yu; Yang, Yiduo; Li, Mengjiao; Chen, Erdong; Mu, Weilei; Fisher, Rosie; Yin, Rong
TI  - Wearable Actuators: An Overview
PY  - 2021
AB  - The booming wearable market and recent advances in material science has led to the rapid development of the various wearable sensors, actuators, and devices that can be worn, embedded in fabric, accessorized, or tattooed directly onto the skin. Wearable actuators, a subcategory of wearable technology, have attracted enormous interest from researchers in various disciplines and many wearable actuators and devices have been developed in the past few decades to assist and improve people’s everyday lives. In this paper, we review the actuation mechanisms, structures, applications, and limitations of recently developed wearable actuators including pneumatic and hydraulic actuators, shape memory alloys and polymers, thermal and hygroscopic materials, dielectric elastomers, ionic and conducting polymers, piezoelectric actuators, electromagnetic actuators, liquid crystal elastomers, etc. Examples of recent applications such as wearable soft robots, haptic devices, and personal thermal regulation textiles are highlighted. Finally, we point out the current bottleneck and suggest the prospective future research directions for wearable actuators.
SP  - 283
EP  - 321
JF  - Textiles
VL  - 1
IS  - 2
PB  - 
DO  - 10.3390/textiles1020015
ER  - 

TY  - CHAP
AU  - , 
TI  - Aesthetics of TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544575
ER  - 

TY  - NA
AU  - Nguyen, Phuc; Bui, Nam; Nguyen, Anh; Truong, Hoang; Suresh, Abhijit; Whitlock, Matt; Pham, Duy Cong; Dinh, Thang N.; Vu, Tam
TI  - MobiSys - TYTH-Typing On Your Teeth: Tongue-Teeth Localization for Human-Computer Interface
PY  - 2018
AB  - This paper explores a new wearable system, called TYTH, that enables a novel form of human computer interaction based on the relative location and interaction between the user's tongue and teeth. TYTH allows its user to interact with a computing system by tapping on their teeth. This form of interaction is analogous to using a finger to type on a keypad except that the tongue substitutes for the finger and the teeth for the keyboard. We study the neurological and anatomical structures of the tongue to design TYTH so that the obtrusiveness and social awkwardness caused by the wearable is minimized while maximizing its accuracy and sensing sensitivity. From behind the user's ears, TYTH senses the brain signals and muscle signals that control tongue movement sent from the brain and captures the miniature skin surface deformation caused by tongue movement. We model the relationship between tongue movement and the signals recorded, from which a tongue localization technique and tongue-teeth tapping detection technique are derived. Through a prototyping implementation and an evaluation with 15 subjects, we show that TYTH can be used as a form of hands-free human computer interaction with 88.61% detection rate and promising adoption rate by users.
SP  - 269
EP  - 282
JF  - Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210240.3210322
ER  - 

TY  - NA
AU  - Mikalauskas, Claire; Wun, Tiffany; Ta, Kevin; Horacsek, Joshua; Oehlberg, Lora
TI  - Conference on Designing Interactive Systems - Improvising with an Audience-Controlled Robot Performer
PY  - 2018
AB  - In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.
SP  - 657
EP  - 666
JF  - Proceedings of the 2018 Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3196709.3196757
ER  - 

TY  - CHAP
AU  - , 
TI  - Authors' Biographies
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544583
ER  - 

TY  - NA
AU  - Hodges, Steve
TI  - UIST - Democratizing the Production of Interactive Hardware
PY  - 2020
AB  - The development of new hardware can be split into two phases: prototyping and production. A wide variety of tools and techniques have empowered people to build prototypes during the first phase, but the transition to production is still complex, costly and prone to failure. This means the second phase often requires an up-front commitment to large volume production in order to be viable. I believe that new tools and techniques can democratize hardware production. Imagine "DevOps for hardware" - everything from circuit simulation tools to re-usable hardware test jig designs; and from test-driven development for hardware to telepresence for remote factory visits. Supporting low volume production and organic scaling in this way would spur innovation and increase consumer choice. I encourage the UIST community to join me in pursuit of this vision.
SP  - 5
EP  - 6
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3422877
ER  - 

TY  - JOUR
AU  - Wang, Yu; Wu, Yuanjie; Jung, Sungchul; Hoermann, Simon; Yao, Shouwen; Lindeman, Robert W.
TI  - Enlarging the Usable Hand Tracking Area by Using Multiple Leap Motion Controllers in VR
PY  - 2021
AB  - Leap Motion Controller (LMC) is a widely-used 3D user-interface device for virtual reality (VR) in hand tracking applications. However, the tracking area of a single LMC is not sufficient to cover the complete range of hand motion typically used in virtual reality applications, which can cause inconvenience and unnatural behavior of bare-hand interaction in a cooperated virtual environment. In this paper, we propose fusing the data from multiple LMCs to enlarge the tracking area. We present our shared-view calibration method based on a Least-squares Fitting algorithm. To track two hands in the enlarged tracking area, we propose a multi-targets tracking algorithm based on a Clustering-based Labeled Probability Hypothesis Density filter implemented by Gaussian mixture approach. A hand-recognition confidence is proposed to improve the tracking performance when hands are incorrectly recognized. The performance of the proposed algorithm was evaluated by three tests based on a five-LMCs system used on an Oculus Rift S. Results show that our system can track two hands stably in the range of 202.16 degrees horizontally and 164.43 degrees vertically, and the proposed algorithm shows superiority in tracking robustness under hand-recognition errors. The contribution of this paper is to provide a detailed guide for designing an enlarged hand-tracking system using sensor fusion.
SP  - 17947
EP  - 17961
JF  - IEEE Sensors Journal
VL  - 21
IS  - 16
PB  - 
DO  - 10.1109/jsen.2021.3082988
ER  - 

TY  - BOOK
AU  - Ullmer, Brygg; Shaer, Orit; Mazalek, Ali; Hummels, Caroline
TI  - Weaving Fire into Form
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564
ER  - 

TY  - NA
AU  - Nisser, Martin; Makaram, Yashaswini; Covarrubias, Lucian; Bah, Amadou Yaye; Faruqi, Faraz; Suzuki, Ryo; Mueller, Stefanie
TI  - Mixels: Fabricating Interfaces using Programmable Magnetic Pixels
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545698
ER  - 

TY  - NA
AU  - Lo, Jo-Yu; Huang, Da-Yuan; Kuo, Tzu-Sheng; Sun, Chen-Kuo; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong; Chen, Bing-Yu
TI  - CHI - AutoFritz: Autocomplete for Prototyping Virtual Breadboard Circuits
PY  - 2019
AB  - We propose autocomplete for the design and development of virtual breadboard circuits using software prototyping tools. With our system, a user inserts a component into the virtual breadboard, and it automatically provides a user with a list of suggested components. These suggestions complete or ex- tend the electronic functionality of the inserted component to save the user's time and reduce circuit error. To demon- strate the effectiveness of autocomplete, we implemented our system on Fritzing, a popular open source breadboard circuit prototyping software, used by novice makers. Our autocomplete suggestions were implemented based upon schematics from datasheets for standard components, as well as how components are used together from over 4000 circuit projects from the Fritzing community. We report the results of a controlled study with 16 participants, evaluating the effectiveness of autocomplete in the creation of virtual breadboard circuits, and conclude by sharing insights and directions for future research.
SP  - 403
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300633
ER  - 

TY  - NA
AU  - Liu, Jiazhou; Prouzeau, Arnaud; Ens, Barrett; Dwyer, Tim
TI  - VR - Design and Evaluation of Interactive Small Multiples Data Visualisation in Immersive Spaces
PY  - 2020
AB  - We explore the adaptation of 2D small-multiples visualisation on flat screens to 3D immersive spaces. We use a "shelves" metaphor for layout of small multiples and consider a design space across a number of layout and interaction dimensions. We demonstrate the applicability of a prototype system informed by this design space to data sets from different domains. We perform two user studies comparing the effect of the shelf curvature dimension from our design space on users’ ability to perform comparison and trend analysis tasks. Our results suggest that, with fewer multiples, a flat layout is more performant despite the need for participants to walk further. With an increase in the number of multiples, this performance difference disappears due to the time participants had to spend walking. In the latter case, users prefer a semi-circular layout over either a fully surrounding or a flat arrangement.
SP  - 588
EP  - 597
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581122519414
ER  - 

TY  - JOUR
AU  - Reski, Nico; Alissandrakis, Aris
TI  - Open data exploration in virtual reality: a comparative study of input technology
PY  - 2019
AB  - In this article, we compare three different input technologies (gamepad, vision-based motion controls, room-scale) for an interactive virtual reality (VR) environment. The overall system is able to visualize (open) data from multiple online sources in a unified interface, enabling the user to browse and explore displayed information in an immersive VR setting. We conducted a user interaction study ($$n=24$$; $$n=8$$ per input technology, between-group design) to investigate experienced workload and perceived flow of interaction. Log files and observations allowed further insights and comparison of each condition. We have identified trends that indicate user preference of a visual (virtual) representation, but no clear trends regarding the application of physical controllers (over vision-based controls), in a scenario that encouraged exploration with no time limitations.
SP  - 1
EP  - 22
JF  - Virtual Reality
VL  - 24
IS  - 1
PB  - 
DO  - 10.1007/s10055-019-00378-w
ER  - 

TY  - NA
AU  - Martinez, Patricia Ivette Cornelio; Maggioni, Emanuela; Hornbæk, Kasper; Obrist, Marianna; Subramanian, Sriram
TI  - CHI - Beyond the Libet Clock: Modality Variants for Agency Measurements
PY  - 2018
AB  - The Sense of Agency (SoA) refers to our capability to control our own actions and influence the world around us. Recent research in HCI has been investigating SoA to provide users an instinctive sense of "I did that" as opposed to "the system did that". However, current agency measurements are limited. The Intentional Binding (IB) paradigm provides an implicit measure of the SoA, however, it is constrained by requiring high visual attention to a "Libet clock" on-screen. In this paper, we extended the timing stimuli through auditory and tactile cues. Our results demonstrate that audio timing through voice commands and haptic timing through tactile cues on the hand, are an effective alternative measure of the SoA using the IB paradigm. They both address current limitations of the traditional method such as visual attention overload and lack of engagement. We discuss how our results can be applied to measure SoA in tasks involving different interactive scenarios such as in Mixed/Virtual Reality.
SP  - 541
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174115
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Dachselt, Raimund; Steimle, Jürgen
TI  - CHI - Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes
PY  - 2020
AB  - Rapid prototyping of interactive textiles is still challenging, since manual skills, several processing steps, and expert knowledge are involved. We present Rapid Iron-On User Interfaces, a novel fabrication approach for empowering designers and makers to enhance fabrics with interactive functionalities. It builds on heat-activated adhesive materials consisting of smart textiles and printed electronics, which can be flexibly ironed onto the fabric to create custom interface functionality. To support rapid fabrication in a sketching-like fashion, we developed a handheld dispenser tool for directly applying continuous functional tapes of desired length as well as discrete patches. We introduce versatile compositions techniques that allow for creating complex circuits, utilizing commodity textile accessories and sketching custom-shaped I/O modules. We further contribute a comprehensive library of components for input, output, wiring and computing. Three example applications, results from technical experiments and expert reviews demonstrate the functionality, versatility and potential of this approach.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376220
ER  - 

TY  - NA
AU  - Endow, Shreyosi; Moradi, Hedieh; Srivastava, Anvay; Noya, Esau G; Torres, César I.
TI  - Conference on Designing Interactive Systems - Compressables: A Haptic Prototyping Toolkit for Wearable Compression-based Interfaces
PY  - 2021
AB  - Compression-based haptic feedback has been used in wearables to issue notifications, provide therapeutic effects, and create immersive storytelling environments. Such worn devices are well studied on the wrists, arms, and head, however, many unconventional yet context-rich areas of the body remain underexplored. Current haptic prototyping techniques have large instrumentation costs, requiring the design of bespoke embedded devices that do not have the flexibility to be applied to other body sites. In this work, we introduce an open-source prototyping toolkit for designing, fabricating, and programming wearable compression-based interfaces, or compressables. Our approach uses a lost-PVA technique for making custom inflatable silicone bladders, an off-the-shelf pneumatics controller, and a mobile app to explore and tune haptic interactions through sketch gestures. We validate the toolkit’s open-endedness through a user study and heuristic evaluation. We use exemplar artifacts to annotate the design space of compressables and discuss opportunities to further expand haptic expression on the body.
SP  - 1101
EP  - 1114
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462057
ER  - 

TY  - NA
AU  - Fang, Likun; Zhu, Ting; Pescara, Erik; Huang, Yiran; Zhou, Yexu; Beigl, Michael
TI  - DragTapVib: An On-Skin Electromagnetic Drag, Tap, and Vibration Actuator for Wearable Computing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519395
ER  - 

TY  - BOOK
AU  - Thévin, Lauren; Jouffrais, Christophe; Rodier, Nicolas; Palard, Nicolas; Hachet, Martin; Brock, Anke
TI  - ISS - Creating Accessible Interactive Audio-Tactile Drawings using Spatial Augmented Reality
PY  - 2019
AB  - Interactive tactile graphics have shown a true potential for people with visual impairments, for instance for acquiring spatial knowledge. Until today, however, they are not well adopted in real-life settings (e.g. special education schools). One obstacle consists in the creation of these media, which requires specific skills, such as the use of vector-graphic software for drawing and inserting interactive zones, which is challenging for stakeholders (social workers, teachers, families of people with visual impairments, etc.). We explored how a Spatial Augmented Reality approach can enhance the creation of interactive tactile graphics by sighted users. We developed the system using a participatory design method. A user study showed that the augmented reality device allowed stakeholders (N=28) to create interactive tactile graphics more efficiently than with a regular vector-drawing software (baseline), independently of their technical background.
SP  - 17
EP  - 28
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359711
ER  - 

TY  - JOUR
AU  - Röddiger, Tobias; Clarke, Christopher; Breitling, Paula; Schneegans, Tim; Zhao, Haibin; Gellersen, Hans; Beigl, Michael
TI  - Sensing with Earables
PY  - 2022
AB  - <jats:p>Earables have emerged as a unique platform for ubiquitous computing by augmenting ear-worn devices with state-of-the-art sensing. This new platform has spurred a wealth of new research exploring what can be detected on a wearable, small form factor. As a sensing platform, the ears are less susceptible to motion artifacts and are located in close proximity to a number of important anatomical structures including the brain, blood vessels, and facial muscles which reveal a wealth of information. They can be easily reached by the hands and the ear canal itself is affected by mouth, face, and head movements. We have conducted a systematic literature review of 271 earable publications from the ACM and IEEE libraries. These were synthesized into an open-ended taxonomy of 47 different phenomena that can be sensed in, on, or around the ear. Through analysis, we identify 13 fundamental phenomena from which all other phenomena can be derived, and discuss the different sensors and sensing principles used to detect them. We comprehensively review the phenomena in four main areas of (i) physiological monitoring and health, (ii) movement and activity, (iii) interaction, and (iv) authentication and identification. This breadth highlights the potential that earables have to offer as a ubiquitous, general-purpose platform.</jats:p>
SP  - 1
EP  - 57
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550314
ER  - 

TY  - NA
AU  - Umair, Muhammad; Sas, Corina; Latif, Muhammad Hamza
TI  - Conference on Designing Interactive Systems - Towards Affective Chronometry: Exploring Smart Materials and Actuators for Real-time Representations of Changes in Arousal
PY  - 2019
AB  - Increasing HCI work on affective interfaces aimed to capture and communicate users' emotions in order to support self-understanding. While most such interfaces employ traditional screen-based displays, more novel approaches have started to investigate smart materials and actuators-based prototypes. In this paper, we describe our exploration of smart materials and actuators leveraging their temporal qualities as well as common metaphors for real-time representation of changes in arousal through visual and haptic modalities. This exploration provided rationale for the design and implementation of six novel wrist-worn prototypes evaluated with 12 users who wore them over 2 days. Our findings describe how people use them in daily life, and how their material-driven qualities such as responsiveness, duration, rhythm, inertia, aliveness and range shape people's emotion identification, attribution, and regulation. Our findings led to four design implications including support for affective chronometry for both raise and decay time of emotional response, design for slowness, and for expressiveness.
SP  - 1479
EP  - 1494
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322367
ER  - 

TY  - NA
AU  - Shi, Lei; Zhang, Zhuohao; Azenkot, Shiri
TI  - ASSETS - A Demo of Talkit++: Interacting with 3D Printed Models Using an iOS Device
PY  - 2018
AB  - Tactile models are important learning materials for visually impaired students. With the adoption of 3D printing technologies, visually impaired students and teachers will have more access to 3D printed tactile models. We designed Talkit++, an iOS application that plays audio and visual content as a user touches parts of a 3D print. With Talkit++, a visually impaired student can explore a printed model tactilely, and use finger gestures and speech commands to get more information about certain elements in the model. Talkit++ detects the model and finger gestures using computer vision algorithms, simple accessories like paper stickers and printable trackers, and the built-in RGB camera on an iOS device. Based on the model's position and the user's input, Talkit++ speaks textual information, plays audio recordings, and displays visual animations.
SP  - 429
EP  - 431
JF  - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3234695.3241004
ER  - 

TY  - CHAP
AU  - , 
TI  - Tangible and Embodied Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544570
ER  - 

TY  - NA
AU  - Qin, Fang; Cheng, Huai-Yu; Sneeringer, Rachel; Vlachostergiou, Maria; Acharya, Sampada; Liu, Haolin; Majidi, Carmel; Islam, Mohammad; Yao, Lining
TI  - CHI Extended Abstracts - ExoForm: Shape Memory and Self-Fusing Semi-Rigid Wearables
PY  - 2021
AB  - Semi-rigid and rigid structures have been utilized in many on-body applications including musculoskeletal support (e.g., braces and splints). However, most of these support structures are not very compliant, so effortless custom fitting becomes a unique design challenge. Furthermore, the weight and space needed to transport these structures impede adoption in mobile environments. Here, we introduce ExoForm, a compact, customizable, and semi-rigid wearable material system with self-fusing edges that can semi-autonomously assemble on-demand while providing integrated sensing, control, and mobility. We present a comprehensive and holistic engineering strategy that includes optimized material composition, computationally-guided design and fabrication, semi-autonomous self-morphing assembly and fusing steps, heating control, and sensing for our easy-to-wear ExoForm. Finally, we fabricate wearable braces using the ExoForm method as a demonstration along with preliminary evaluation of ExoForm's performance.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451818
ER  - 

TY  - JOUR
AU  - Rao, Cong; Tian, Lihao; Yan, Dong-Ming; Shenghui, Liao; Deussen, Oliver; Lu, Lin
TI  - Consistently fitting orthopedic casts
PY  - 2019
AB  - NA
SP  - 130
EP  - 141
JF  - Computer Aided Geometric Design
VL  - 71
IS  - NA
PB  - 
DO  - 10.1016/j.cagd.2019.04.018
ER  - 

TY  - JOUR
AU  - Youn, Jung-Hwan; Mun, Heeju; Kyung, Ki-Uk
TI  - A Wearable Soft Tactile Actuator With High Output Force for Fingertip Interaction
PY  - 2021
AB  - This paper reports a soft fingertip-mountable tactile actuator based on a Dielectric Elastomer Actuator (DEA), which exhibits high output force over a wide frequency range with a lightweight and soft structure. DEA is a soft actuator characterized by its large area strain, fast response speed, and high specific energy density. The proposed soft tactile actuator is constructed of a multi-layered conical DEA structure. This design has safety benefits because it isolates the high voltage components from the contact point. In this paper, the resonance frequency of the tactile actuator was designed to be at 250 Hz to maximize vibrotactile stimulation. In addition, the geometric design parameters of the soft tactile actuator were optimized by conducting the simulations and the experiments. Based on these efforts, the proposed actuator produces a high output force of 8.48 N at the resonance frequency, with a maximum displacement of 0.46 mm. Our wearable prototype was an entirely soft haptic system, which exhibits high output force, as well as flexibility and conformity with a total weight of 2.6 g.
SP  - 30206
EP  - 30215
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3058979
ER  - 

TY  - NA
AU  - Kimura, Naoki; Kono, Michinari; Rekimoto, Jun
TI  - CHI - SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using Deep Neural Networks
PY  - 2019
AB  - The availability of digital devices operated by voice is expanding rapidly. However, the applications of voice interfaces are still restricted. For example, speaking in public places becomes an annoyance to the surrounding people, and secret information should not be uttered. Environmental noise may reduce the accuracy of speech recognition. To address these limitations, a system to detect a user's unvoiced utterance is proposed. From internal information observed by an ultrasonic imaging sensor attached to the underside of the jaw, our proposed system recognizes the utterance contents without the user's uttering voice. Our proposed deep neural network model is used to obtain acoustic features from a sequence of ultrasound images. We confirmed that audio signals generated by our system can control the existing smart speakers. We also observed that a user can adjust their oral movement to learn and improve the accuracy of their voice recognition.
SP  - 146
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300376
ER  - 

TY  - JOUR
AU  - Wenqiang, Chen; Lin, Chen; Ma, Meiyi; Parizi, Farshid Salemi; Patel, Shwetak N.; Stankovic, John A.
TI  - ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch
PY  - 2021
AB  - Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 1
PB  - 
DO  - 10.1145/3448119
ER  - 

TY  - NA
AU  - Tseng, Wen-Jie; Huron, Samuel; Lecolinet, Eric; Gugenheimer, Jan
TI  - CHI Extended Abstracts - FingerMapper: Enabling Arm Interaction in Confined Spaces for Virtual Reality through Finger Mappings
PY  - 2021
AB  - As Virtual Reality (VR) headsets become more mobile, people can interact in public spaces with applications often requiring large arm movements. However, using these open gestures is often uncomfortable and sometimes impossible in confined and public spaces (e.g., commuting in a bus). We present FingerMapper, a mapping technique that maps small and energy-efficient finger motions onto virtual arms so that users have less physical motions while maintaining presence and partially virtual body ownership. FingerMapper works as an alternative function while the environment is not allowed for full arm interaction and enables users to interact inside a small physical, but larger virtual space. We present one example application, FingerSaber that allows the user to perform the large arm swinging movement using FingerMapper.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451573
ER  - 

TY  - NA
AU  - Chatterjee, Ishan; Pforte, Tadeusz; Tng, Aspen; Salemi Parizi, Farshid; Chen, Chaoran; Patel, Shwetak
TI  - ARDW: An Augmented Reality Workbench for Printed Circuit Board Debugging
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545684
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Yamaoka, Junichi; Leithinger, Daniel; Yeh, Tom; Gross, Mark D.; Kawahara, Yoshihiro; Kakehi, Yasuaki
TI  - UIST - Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation
PY  - 2018
AB  - This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.
SP  - 99
EP  - 111
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242659
ER  - 

TY  - NA
AU  - Chatterjee, Ishan; Khvan, Olga; Pforte, Tadeusz; Li, Richard; Patel, Shwetak N.
TI  - Conference on Designing Interactive Systems - Augmented Silkscreen: Designing AR Interactions for Debugging Printed Circuit Boards
PY  - 2021
AB  - Debugging printed circuit boards (PCBs) requires frequent context switching and spatial pattern matching between software design files and physical boards. To reduce this overhead, we conduct a series of interviews with electrical engineers to understand their workflows, around which we design a set of AR interaction techniques, we call Augmented Silkscreen, to streamline identification, localization, annotation, and measurement tasks. We then run a set of remote user studies with illustrative video sketches and simulated PCB tasks to compare our interactions with current practices, finding that our techniques reduce completion times. Based on these quantitative results, as well as qualitative feedback from our participants, we offer design recommendations for the implementation of these interactions on a future, deployable AR system.
SP  - 220
EP  - 233
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462091
ER  - 

TY  - NA
AU  - Ahuja, Karan; Harrison, Chris; Goel, Mayank; Xiao, Robert
TI  - UIST - MeCap: Whole-Body Digitization for Low-Cost VR/AR Headsets
PY  - 2019
AB  - Low-cost, smartphone-powered VR/AR headsets are becoming more popular. These basic devices - little more than plastic or cardboard shells - lack advanced features, such as controllers for the hands, limiting their interactive capability. Moreover, even high-end consumer headsets lack the ability to track the body and face. For this reason, interactive experiences like social VR are underdeveloped. We introduce MeCap, which enables commodity VR headsets to be augmented with powerful motion capture ("MoCap") and user-sensing capabilities at very low cost (under $5). Using only a pair of hemi-spherical mirrors and the existing rear-facing camera of a smartphone, MeCap provides real-time estimates of a wearer's 3D body pose, hand pose, facial expression, physical appearance and surrounding environment - capabilities which are either absent in contemporary VR/AR systems or which require specialized hardware and controllers. We evaluate the accuracy of each of our tracking features, the results of which show imminent feasibility.
SP  - 453
EP  - 462
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347889
ER  - 

TY  - NA
AU  - Wun, Tiffany; Mikalauskas, Claire; Ta, Kevin; Horacsek, Joshua; Oehlberg, Lora
TI  - Conference on Designing Interactive Systems (Companion Volume) - RIPT: Improvising with an Audience-Sourced Performance Robot
PY  - 2018
AB  - Improvisational theatre (improv) actors develop narratives in real-time as they perform unscripted scenes together. Audience suggestions build engagement and introduce randomness into the scene; however, actors have difficulty mediating or responding to audience suggestions at scale. We present Robot Improv Puppet Theatre (RIPT), a system for short-form improv where a performance robot (Pokey) performs gestures and dialogue provided by audience members via a mobile interface. Improvisers' narratives are shaped in response to the robot's actions, guiding narratives into unexpected directions.
SP  - 323
EP  - 326
JF  - Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3197391.3205397
ER  - 

TY  - NA
AU  - Shim, Youngbo Aram; Lee, Jaeyeon; Lee, Geehyuk
TI  - Exploring Multimodal Watch-back Tactile Display using Wind and Vibration
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173706
ER  - 

TY  - NA
AU  - Shahmiri, Fereshteh; Dietz, Paul Henry
TI  - CHI - ShArc: A Geometric Technique for Multi-Bend/Shape Sensing
PY  - 2020
AB  - We present ShArc, a precision, geometric measurement technique for building multi-bend/shape sensors. ShArc sensors are made from flexible strips that can be dynamically formed into complex curves in a plane. They measure local curvature by noting the relative shift between the inner and outer layers of the sensor at many points and model shape as a series of connected arcs. Unlike jointed systems where angular errors sum with each joint measured, ShArc sensors do not accumulate angular error as more measurement points are added. This allows for inexpensive, robust sensors that can accurately model curves with multiple bends. To demonstrate the efficacy of this technique, we developed a capacitive ShArc sensor and evaluated its performance. We conclude with examples of how ShArc sensors can be employed in applications like gesture input devices, user interface controllers, human motion tracking and angular measurement of free-form objects.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376269
ER  - 

TY  - NA
AU  - Chang, Hong-Yu; Tseng, Wen-Jie; Tsai, Chia-En; Chen, Hsin-Yu; Peiris, Roshan Lalintha; Chan, Liwei
TI  - UIST - FacePush: Introducing Normal Force on Face with Head-Mounted Displays
PY  - 2018
AB  - This paper presents FacePush, a Head-Mounted Display (HMD) integrated with a pulley system to generate normal forces on a user's face in virtual reality (VR). The mechanism of FacePush is obtained by shifting torques provided by two motors that press upon a user's face via utilization of a pulley system. FacePush can generate normal forces of varying strengths and apply those to the surface of the face. To inform our design of FacePush for noticeable and discernible normal forces in VR applications, we conducted two studies to iden- tify the absolute detection threshold and the discrimination threshold for users' perception. After further consideration in regard to user comfort, we determined that two levels of force, 2.7 kPa and 3.375 kPa, are ideal for the development of the FacePush experience via implementation with three applications which demonstrate use of discrete and continuous normal force for the actions of boxing, diving, and 360 guidance in virtual reality. In addition, with regards to a virtual boxing application, we conducted a user study evaluating the user experience in terms of enjoyment and realism and collected the user's feedback.
SP  - 927
EP  - 935
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242588
ER  - 

TY  - JOUR
AU  - Torabi, Ali; Nazari, Ali A.; Conrad-Baldwin, Everly; Zareinia, Kourosh; Tavakoli, NA
TI  - Kinematic design of linkage-based haptic interfaces for medical applications: a review
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>A haptic interface recreates haptic feedback from virtual environments or haptic teleoperation systems that engages the user’s sense of touch. High-fidelity haptic feedback is critical to the safety and success of any interaction with human beings. Such interactions can be seen in haptic systems utilized in medical fields, such as for surgical training, robotic tele-surgery, and tele-rehabilitation, which require appropriate haptic interface design and control. In order to recreate high-fidelity soft and stiff contact experiences for the user in the intended application, different designs strike different trade-offs between the desirable characteristics of an interface, such as back-drivability, low apparent inertia and low friction for the best perception of small reflected forces, large intrinsic stiffness and force feedback capability for the best perception of large reflected forces, a large-enough workspace for exploring the remote or virtual environment, and the uniformity of haptic feedback and its adequate sensitivity over the workspace. Meeting all of the requirements simultaneously is impossible, and different application-driven compromises need to be made. This paper reviews how various kinematic designs have helped address these trade-offs in desired specifications. First, we investigate the required characteristics of linkage-based haptic interfaces and inevitable trade-offs between them. Then, we study the state of the art in the kinematic design of haptic interfaces and their advantages and limitations. In all sections, we consider the applications of the intended haptic interfaces in medical scenarios. Non-linkage-based haptic interfaces are also shortly discussed to show the broad range of haptic technologies in the area. The potentials of kinematic redundancy to address the design trade-offs are introduced. Current challenges and future directions of haptic interface designs for medical applications are shortly discussed, which is finally followed by the conclusion.</jats:p>
SP  - 022005
EP  - NA
JF  - Progress in Biomedical Engineering
VL  - 3
IS  - 2
PB  - 
DO  - 10.1088/2516-1091/abee66
ER  - 

TY  - NA
AU  - Sarwar, Saquib; Wilson, David
TI  - Systematic Literature Review on Making and Accessibility
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 24th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517428.3550377
ER  - 

TY  - NA
AU  - Yamato, Yuki; Suzuki, Yutaro; Sekimori, Kodai; Shizuki, Buntarou; Takahashi, Shin
TI  - AVI - Hand Gesture Interaction with a Low-Resolution Infrared Image Sensor on an Inner Wrist
PY  - 2020
AB  - We propose a hand gesture interaction method using a low-resolution infrared image sensor on an inner wrist. We attach the sensor to the strap of a wrist-worn device, on the palmar side, and apply machine-learning techniques to recognize the gestures made by the opposite hand. As the sensor is placed on the inner wrist, the user can naturally control its direction to reduce privacy invasion. Our method can recognize four types of hand gestures: static hand poses, dynamic hand gestures, finger motion, and the relative hand position. We developed a prototype that does not invade surrounding people's privacy using an 8 x 8 low-resolution infrared image sensor. Then we conducted experiments to validate our prototype, and our results imply that the low-resolution sensor has sufficient capabilities for recognizing a rich array of hand gestures. In this paper, we introduce an implementation of a mapping application that can be controlled by our specified hand gestures, including gestures that use both hands.
SP  - NA
EP  - NA
JF  - Proceedings of the International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3399715.3399858
ER  - 

TY  - JOUR
AU  - Sugrim, Shridatt; Liu, Can; Lindqvist, Janne
TI  - Recruit Until It Fails: Exploring Performance Limits for Identification Systems
PY  - 2019
AB  - Distinguishing identities is useful for several applications such as automated grocery or personalized recommendations. Unfortunately, several recent proposals for identification systems are evaluated using poor recruitment practices. We discovered that 23 out of 30 surveyed systems used datasets with 20 participants or less. Those studies achieved an average classification accuracy of 93%. We show that the classifier performance is misleading when the participant count is small. This is because the finite precision of measurements creates upper limits on the number of users that can be distinguished. To demonstrate why classifier performance is misleading, we used publicly available datasets. The data was collected from human subjects. We created five systems with at least 20 participants each. In three cases we achieved accuracies greater than 90% by merely applying readily available machine learning software packages, often with default parameters. For datasets where we had sufficient participants, we evaluated how the performance degrades as the number of participants increases. One of the systems built suffered a drop in accuracy that was over 35% as the participant count increased from 20 to 250. We argue that data from small participant count datasets do not adequately explore variations. Systems trained on such limited data are likely to incorrectly identify users when the user base increases beyond what was tested. We conclude by explaining generalizable reasons for this issue and provide insights on how to conduct more robust system analysis and design.
SP  - 104
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3351262
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Braud, Tristan; Hosio, Simo; Hui, Pan
TI  - Towards Augmented Reality-driven Human-City Interaction: Current Research on Mobile Headsets and Future Challenges
PY  - 2020
AB  - Interaction design for Augmented Reality (AR) is gaining increasing attention from both academia and industry. This survey discusses 260 articles (68.8% of articles published between 2015 - 2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by a review of the latest trends of information visualization, constrained interfaces, and embodied interaction for AR headsets. We highlight under-explored issues in interface design and input techniques that warrant further research, and conjecture that AR with complementary Conversational User Interfaces (CUIs) is a key enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of AR in Human-City Interaction.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cao, Yetong; Chen, Huijie; Li, Fan; Yang, Song; Wang, Yu
TI  - INFOCOM - AWash: Handwashing Assistance for the Elderly with Dementia via Wearables
PY  - 2021
AB  - Hand hygiene has a significant impact on human health. Proper handwashing, having a crucial effect on reducing bacteria, serves as the cornerstone of hand hygiene. For the elder with dementia, they suffer from a gradual loss of memory and difficulty in coordinating steps in the execution of handwashing. Proper assistance should be provided to them to ensure their hand hygiene adherence. Toward this end, we propose AWash, leveraging only commodity IMU sensor mounted on most wrist-worn devices (e.g., smartwatches) to characterize hand motions and provide assistance accordingly. To handle particular interference of senile dementia patients in IMU sensor readings, we design a number of effective techniques to segment handwashing actions, transform sensory input to body coordinate system, and extract sensor-body inclination angles. A hybrid neural network model is used to enable AWash to generalize to new users without retraining or adaptation, avoiding the trouble of collecting behavior information of every user. To meet the diverse needs of users with various executive functioning, we use a state machine to make prompt decisions, which supports customized assistance. Extensive experiments on a prototype with eight older participants demonstrate that AWash can increase the user’s independence in the execution of handwashing.
SP  - 1
EP  - 10
JF  - IEEE INFOCOM 2021 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom42981.2021.9488688
ER  - 

TY  - JOUR
AU  - Chen, Taizhou; Li, Tianpei; Yang, Xingyu; Zhu, Kening
TI  - EFRing
PY  - 2022
AB  - <jats:p>We present EFRing, an index-finger-worn ring-form device for detecting thumb-to-index-finger (T2I) microgestures through the approach of electric-field (EF) sensing. Based on the signal change induced by the T2I motions, we proposed two machine-learning-based data-processing pipelines: one for recognizing/classifying discrete T2I microgestures, and the other for tracking continuous 1D T2I movements. Our experiments on the EFRing microgesture classification showed an average within-user accuracy of 89.5% and an average cross-user accuracy of 85.2%, for 9 discrete T2I microgestures. For the continuous tracking of 1D T2I movements, our method can achieve the mean-square error of 3.5% for the generic model and 2.3% for the personalized model. Our 1D-Fitts'-Law target-selection study shows that the proposed tracking method with EFRing is intuitive and accurate for real-time usage. Lastly, we proposed and discussed the potential applications for EFRing.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569478
ER  - 

TY  - NA
AU  - Ahsan, Amm Nazmul
TI  - Form and Functionality of Additively Manufactured Parts with Internal Structure
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Qi, Shutong; Chen, Junchi; Shang, MuJie; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong
TI  - CHI - Fabriccio: Touchless Gestural Input on Interactive Fabrics
PY  - 2020
AB  - We present Fabriccio, a touchless gesture sensing technique developed for interactive fabrics using Doppler motion sensing. Our prototype was developed using a pair of loop antennas (one for transmitting and the other for receiving), made of conductive thread that was sewn onto a fabric substrate. The antenna type, configuration, transmission lines, and operating frequency were carefully chosen to balance the complexity of the fabrication process and the sensitivity of our system for touchless hand gestures, performed at a 10 cm distance. Through a ten-participant study, we evaluated the performance of our proposed sensing technique across 11 touchless gestures as well as 1 touch gesture. The study result yielded a 92.8% cross-validation accuracy and 85.2% leave-one-session-out accuracy. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique on soft objects.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376681
ER  - 

TY  - JOUR
AU  - Kim, Yoonji; Choi, Youngkyung; Kang, Daye; Lee, Minkyeong; Nam, Tek-Jin; Bianchi, Andrea
TI  - HeyTeddy: Conversational Test-Driven Development for Physical Computing
PY  - 2019
AB  - Physical computing is a complex activity that consists of different but tightly coupled tasks: programming and assembling hardware for circuits. Prior work clearly shows that this coupling is the main source of mistakes that unfruitfully take a large portion of novices' debugging time. While past work presented systems that simplify prototyping or introduce novel debugging functionalities, these tools either limit what users can accomplish or are too complex for beginners. In this paper, we propose a general-purpose prototyping tool based on conversation. HeyTeddy guides users during hardware assembly by providing additional information on requests or by interactively presenting the assembly steps to build a circuit. Furthermore, the user can program and execute code in real-time on their Arduino platform without having to write any code, but instead by using commands triggered by voice or text via chat. Finally, the system also presents a set of test capabilities for enhancing debugging with custom and proactive unit tests. We codesigned the system with 10 users over 6 months and tested it with realistic physical computing tasks. With the result of two user studies, we show that conversational programming is feasible and that voice is a suitable alternative for programming simple logic and encouraging exploration. We also demonstrate that conversational programming with unit tests is effective in reducing development time and overall debugging problems while increasing users' confidence. Finally, we highlight limitations and future avenues of research.
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 4
PB  - 
DO  - 10.1145/3369838
ER  - 

TY  - CONF
AU  - Xie, Haoran; Torii, Takuma; Chiba, Aoshi; Qi, Qiukai
TI  - xBalloon: Animated Objects with Balloon Plastic Actuator
PY  - 2021
AB  - Shape-changing interfaces are promising for users to change the physical properties of common objects. However, prevailing approaches of actuation devices require either professional equipment or materials that are not commonly accessible to non-professional users. In this work, we focus on the controllable soft actuators with inflatable structures because they are soft thus safe for human computer interaction. We propose a soft actuator design, called xBalloon, that is workable, inexpensive and easy-to-fabricate. It consists of daily materials including balloons and plastics and can realize bending actuation very effectively. For characterization, we fabricated xBalloon samples with different geometrical parameters and tested them regarding the bending performance and found the analytical model describing the relationship between the shape and the bending width. We then used xBalloons to animate a series of common objects and all can work satisfactorily. We further verified the user experience about the the fabrication and found that even those with no prior robotic knowledge can fabricate xBalloons with ease and confidence. Given all these advantages, we believe that xBalloon is an ideal platform for interaction design and entertainment applications.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Colley, Ashley; Inget, Virve; Rantala, Inka; Häkkilä, Jonna
TI  - MUM - Investigating interaction with a ring form factor
PY  - 2017
AB  - In this note, we study interaction with a finger worn ring, focusing on interaction enabled by the ring form factor. We report on 2 user studies, the first (n=13) investigating preferences for different interactions, whilst the second (n=7) explores usage contexts and applications. Twelve different ways of interacting with a ring were evaluated, including e.g. changing the placement of the ring on the fingers and moving the ring along or around a finger. Based the study results, the practical usability and concerns with each of the ring interactions is discussed. Whereas the concept was generally well received, the main concerns related to false positives, losing the ring e.g. when changing finger, and limitations from the rigid size of the form factor.
SP  - 107
EP  - 111
JF  - Proceedings of the 16th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3152832.3152870
ER  - 

TY  - NA
AU  - Kishida, Shoki; Sakurai, Sho; Hirota, Koichi; Nojima, Takuya
TI  - UIST (Adjunct Volume) - Ambre: Augmented Metaphor Based Representation System for Electricity
PY  - 2020
AB  - With the rise of the Maker Movement and STEM education, toolkit research for beginners in electronics has become popular. Most of them are focused on the efficient creation and debugging of electrical circuits. Therefore, users were implicitly required to understanding of the nature of electricity and some prerequisites knowledge. In this paper, we propose tangible interface to express electrical properties such as voltage and resistance. This system aims to bring a deep interest and understanding of the nature of electricity to the beginners explicitly. In this research, we discuss the interaction between the laws of electricity and another several physical laws, present an example of a prototype implementation, and describe the results of a preliminary user questionnaire.
SP  - 16
EP  - 18
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416171
ER  - 

TY  - CHAP
AU  - , 
TI  - Framing TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544572
ER  - 

TY  - JOUR
AU  - Vechev, V.; Zarate, J.; Thomaszewski, B.; Hilliges, O.
TI  - Computational Design of Kinesthetic Garments
PY  - 2022
AB  - Kinesthetic garments provide physical feedback on body posture and motion through tailored distributions of reinforced material. Their ability to selectively stiffen a garment's response to specific motions makes them appealing for rehabilitation, sports, robotics, and many other application fields. However, finding designs that distribute a given amount of reinforcement material to maximally stiffen the response to specified motions is a challenging problem. In this work, we propose an optimization-driven approach for automated design of reinforcement patterns for kinesthetic garments. Our main contribution is to cast this design task as an on-body topology optimization problem. Our method allows designers to explore a continuous range of designs corresponding to various amounts of reinforcement coverage. Our model captures both tight contact and lift-off separation between cloth and body. We demonstrate our method on a variety of reinforcement design problems for different body sites and motions. Optimal designs lead to a two- to threefold improvement in performance in terms of energy density. A set of manufactured designs were consistently rated as providing more resistance than baselines in a comparative user study.
SP  - 535
EP  - 546
JF  - Computer Graphics Forum
VL  - 41
IS  - 2
PB  - 
DO  - 10.1111/cgf.14492
ER  - 

TY  - NA
AU  - Zhao, Zhenjie
TI  - Live Emoji: Semantic Emotional Expressiveness of 2D Live Animation.
PY  - 2019
AB  - Live animation of 2D characters has recently become a popular way for storytelling, and has potential application scenarios like tele-present agents or robots. As an extension of human-human communication, there is a need for augmenting the emotional communication experience of live animation. In this paper, we explore the emotional expressiveness issue of 2D live animation. In particular, we propose a descriptive emotion command model to bind a triggering action, the semantic meaning, psychology measurements, and behaviors of an emotional expression. Based on the model, we designed and implemented a proof-of-concept 2D live animation system, where a novel visual programming tool for editing the behaviors of 2D digital characters, and an emotion command recommendation algorithm are proposed. Through a user evaluation, we showcase the usability of our system and its potential for boosting creativity and enhancing the emotional communication experience.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhang, Zhong-Yi; Chen, Hong-Xian; Wang, Shih-Hao; Tsai, Hsin-Ruey
TI  - ELAXO : Rendering Versatile Resistive Force Feedback for Fingers Grasping and Twisting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545677
ER  - 

TY  - NA
AU  - Hoffard, Jana; Zhang, Xuan; Wu, Erwin; Nakamura, Takuto; Koike, Hideki
TI  - SkiSim: A comprehensive Study on Full Body Motion Capture and Real-Time Feedback in VR Ski Training
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519400
ER  - 

TY  - JOUR
AU  - Yu, Li; Abuella, Hisham; Islam, Zobaer; O'Hara, John F.; Crick, Christopher; Ekin, Sabit
TI  - Gesture Recognition Using Reflected Visible and Infrared Lightwave Signals
PY  - 2021
AB  - In this article, we demonstrate the ability to recognize hand gestures in a noncontact wireless fashion using only incoherent light signals reflected from a human subject. Fundamentally distinguished from radar, lidar, and camera-based sensing systems, this sensing modality uses only a low-cost light source (e.g., LED) and a sensor (e.g., photodetector). The lightwave-based gesture recognition system identifies different gestures from the variations in light intensity reflected from the subject's hand within a short (20–35 cm) range. As users perform different gestures, scattered light forms unique, statistically repeatable, time-domain signatures. These signatures can be learned by repeated sampling to obtain the training model against which unknown gesture signals are tested and categorized. These time-domain variations of the lightwave signals reflected from hand are denoised, standardized, and then classified by using machine learning classification tools such as $K$ -nearest neighbors and support vector machine. Performance evaluations have been conducted with eight gestures, five subjects, different distances and lighting conditions, and visible and infrared light sources. The results demonstrate the best hand gesture recognition performance of infrared sensing at 20 cm with an average of 96% accuracy. The developed gesture recognition system is low-cost, effective, and noncontact technology for numerous human–computer interaction applications.
SP  - 44
EP  - 55
JF  - IEEE Transactions on Human-Machine Systems
VL  - 51
IS  - 1
PB  - 
DO  - 10.1109/thms.2020.3043302
ER  - 

TY  - JOUR
AU  - Han, Amy Kyungwon; Ji, Sheng; Wang, Dangxiao; Cutkosky, Mark R.
TI  - Haptic Surface Display based on Miniature Dielectric Fluid Transducers
PY  - 2020
AB  - We present a lightweight, low power, and compliant miniature dielectric fluid transducer intended for haptic surface display. The actuator has a large strain and fast response without an external compressor. It consists of a thin oil-filled pouch with a 1.5 mm diameter opening covered with a silicone membrane. The application of voltage causes the pouch to squeeze the oil and form a bump by stretching the silicone membrane. The actuator produces 1.45 mm bump height at 3 kV and 13 mN at 3.5 kV using ≈10 μl of oil. The power consumption is <; 3 mW. Though the largest bump height has a bandwidth near 5 Hz, the device achieves perceivable vibration at 200 Hz with a bump height of 200 μm. The actuators can be packed closely and controlled individually to create dynamic texture displays, suitable for active surface exploration with the fingertips. The simulation results show the width of the actuator can be reduced without affecting the performance. Tests with human subjects show that users differentiated simple bump patterns with a 98.8% success rate.
SP  - 4021
EP  - 4027
JF  - IEEE Robotics and Automation Letters
VL  - 5
IS  - 3
PB  - 
DO  - 10.1109/lra.2020.2985624
ER  - 

TY  - NA
AU  - Gesslein, Travis; Biener, Verena; Gagel, Philipp; Schneider, Daniel; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Pen-based Interaction with Spreadsheets in Mobile Virtual Reality
PY  - 2020
AB  - Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet's cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zeinullin, Maralbek; Hersh, Marion
TI  - Tactile Audio Responsive Intelligent System
PY  - 2022
AB  - NA
SP  - 122074
EP  - 122091
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3223099
ER  - 

TY  - JOUR
AU  - Verma, Dhruv; Bhalla, Sejal; Sahnan, Dhruv; Shukla, Jainendra; Parnami, Aman
TI  - ExpressEar: Sensing Fine-Grained Facial Expressions with Earables
PY  - 2021
AB  - Continuous and unobtrusive monitoring of facial expressions holds tremendous potential to enable compelling applications in a multitude of domains ranging from healthcare and education to interactive systems. Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our approach and found that ExpressEar can detect and distinguish between 32 Facial AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9% for any given user. We further quantify the performance across different mobile scenarios in presence of additional face-related activities. Our results demonstrate ExpressEar's applicability in the real world and open up research opportunities to advance its practical adoption.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478085
ER  - 

TY  - BOOK
AU  - Tsuji, Amato; Ushida, Keita
TI  - VR Workshops - A Telepresence System using Toy Robots the Users can Assemble and Manipulate with Finger Plays and Hand Shadow
PY  - 2021
AB  - The authors report a telepresence system using toy robots. The robots can be assembled and set up for telepresence by the user easily. The robots are manipulated with hand gestures of finger play and hand shadow without instructtion, since the users already know these gestures by their experience. Through the experiment, the system was found to be easy to use, and the users felt familiar with the robot and didn’t feel nervous.
SP  - 661
EP  - 662
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00213
ER  - 

TY  - NA
AU  - Han, Teng
TI  - UIST (Adjunct Volume) - Designing Inherent Interactions on Wearable Devices
PY  - 2018
AB  - Wearable devices are becoming important computing devices to personal users. They have shown promising applications in multiple domains. However, designing interactions on smartwears remains challenging as the miniature sized formfactors limit both its input and output space. My thesis research proposes a new paradigm of Inherent Interaction on smartwears, with the idea of seeking interaction opportunities from users daily activities. This is to help bridging the gap between novel smartwear interactions and real-life experiences shared among users. This report introduces the concept of Inherent Interaction with my previous and current explorations in the category.
SP  - 220
EP  - 223
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266130
ER  - 

TY  - JOUR
AU  - Messerschmidt, Moritz Alexander; Muthukumarana, Sachith; Hamdan, Nur Al-Huda; Wagner, Adrian; Zhang, Haimo; Borchers, Jan; Nanayakkara, Suranga Chandima
TI  - ANISMA: A Prototyping Toolkit to Explore Haptic Skin Deformation Applications Using Shape-Memory Alloys
PY  - 2022
AB  - <jats:p>We present ANISMA, a software and hardware toolkit to prototype on-skin haptic devices that generate skin deformation stimuli like pressure, stretch, and motion using shape-memory alloys (SMAs). Our toolkit embeds expert knowledge that makes SMA spring actuators more accessible to human–computer interaction (HCI) researchers. Using our software tool, users can design different actuator layouts, program their spatio-temporal actuation and preview the resulting deformation behavior to verify a design at an early stage. Our toolkit allows exporting the actuator layout and 3D printing it directly on skin adhesive. To test different actuation sequences on the skin, a user can connect the SMA actuators to our customized driver board and reprogram them using our visual programming interface. We report a technical analysis, verify the perceptibility of essential ANISMA skin deformation devices with 8 participants, and evaluate ANISMA regarding its usability and supported creativity with 12 HCI researchers in a creative design task.</jats:p>
SP  - 1
EP  - 34
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3490497
ER  - 

TY  - NA
AU  - Abtahi, Parastoo; Hough, Sidney Q.; Landay, James A.; Follmer, Sean
TI  - Beyond Being Real: A Sensorimotor Control Perspective on Interactions in Virtual Reality
PY  - 2022
AB  - We can create Virtual Reality (VR) interactions that have no equivalent in the real world by remapping spacetime or altering users' body representation, such as stretching the user's virtual arm for manipulation of distant objects or scaling up the user's avatar to enable rapid locomotion. Prior research has leveraged such approaches, what we call beyond-real techniques, to make interactions in VR more practical, efficient, ergonomic, and accessible. We present a survey categorizing prior movement-based VR interaction literature as reality-based, illusory, or beyond-real interactions. We survey relevant conferences (CHI, IEEE VR, VRST, UIST, and DIS) while focusing on selection, manipulation, locomotion, and navigation in VR. For beyond-real interactions, we describe the transformations that have been used by prior works to create novel remappings. We discuss open research questions through the lens of the human sensorimotor control system and highlight challenges that need to be addressed for effective utilization of beyond-real interactions in future VR applications, including plausibility, control, long-term adaptation, and individual differences.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517706
ER  - 

TY  - NA
AU  - Albouys-Perrois, Jérémy; Laviole, Jeremy; Briant, Carine; Brock, Anke
TI  - CHI - Towards a Multisensory Augmented Reality Map for Blind and Low Vision People: a Participatory Design Approach
PY  - 2018
AB  - Current low-tech Orientation & Mobility (O&M) tools for visually impaired people, e.g. tactile maps, possess limitations. Interactive accessible maps have been developed to overcome these. However, most of them are limited to exploration of existing maps, and have remained in laboratories. Using a participatory design approach, we have worked closely with 15 visually impaired students and 3 O&M instructors over 6 months. We iteratively designed and developed an augmented reality map destined at use in O&M classes in special education centers. This prototype combines projection, audio output and use of tactile tokens, and thus allows both map exploration and construction by low vision and blind people. Our user study demonstrated that all students were able to successfully use the prototype, and showed a high user satisfaction. A second phase with 22 international special education teachers allowed us to gain more qualitative insights. This work shows that augmented reality has potential for improving the access to education for visually impaired people.
SP  - 629
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174203
ER  - 

TY  - JOUR
AU  - Jiang, Chutian; Chen, Yanjun; Fan, Mingming; Wang, Liuping; Shen, Luyao; Li, Nianlong; Sun, Wei; Zhang, Yu; Tian, Feng; Han, Teng
TI  - Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality
PY  - 2021
AB  - The imitation of pain sensation in Virtual Reality is considered valuable for safety education and training but has been seldom studied. This paper presents Douleur, a wearable haptic device that renders intensity-adjustable pain sensations with chemical stimulants. Different from mechanical, thermal, or electric stimulation, chemical-induced pain is more close to burning sensations and long-lasting. Douleur consists of a microfluidic platform that precisely emits capsaicin onto the skin and a microneedling component to help the stimulant penetrate the epidermis layer to activate the trigeminal nerve efficiently. Moreover, it embeds a Peltier module to apply the heating or cooling stimulus to the affected area to adjust the level of pain on the skin. To better understand how people would react to the chemical stimulant, we conducted a first study to quantify the enhancement of the sensation by changing the capsaicin concentration, skin temperature, and time and to determine suitable capsaicin concentration levels. In the second study, we demonstrated that Douleur could render a variety of pain sensations in corresponding virtual reality applications. In sum, Douleur is the first wearable prototype that leverages a combination of capsaicin and Peltier to induce rich pain sensations and opens up a wide range of applications for safety education and more.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463527
ER  - 

TY  - NA
AU  - Xu, Zheer; Chen, Weihao; Zhao, Dongyang; Luo, Jiehui; Wu, Te-Yen; Gong, Jun; Yin, Sicheng; Zhai, Jialun; Yang, Xing-Dong
TI  - CHI - BiTipText: Bimanual Eyes-Free Text Entry on a Fingertip Keyboard
PY  - 2020
AB  - We present a bimanual text input method on a miniature fingertip keyboard, that invisibly resides on the first segment of a user's index finger on both hands. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The design of our keyboard layout followed an iterative process, where we first conducted a study to understand the natural expectation of the handedness of the keys in a QWERTY layout for users. Among a choice of 67,108,864 design variations, we identified 1295 candidates offering a good satisfaction for user expectations. Based on these results, we computed an optimized bimanual keyboard layout, while considering the joint optimization problems of word ambiguity and movement time. Our user evaluation revealed that participants achieved an average text entry speed of 23.4 WPM.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376306
ER  - 

TY  - NA
AU  - Song, Xingzhe; Huang, Kai; Gao, Wei
TI  - FaceListener: Recognizing Human Facial Expressions via Acoustic Sensing on Commodity Headphones
PY  - 2022
AB  - Facial expressions are important indicators of user needs that can be used in many interactive computing applications to adapt the system behaviors and settings. Current computing approaches to recognizing human facial expressions, however, either rely on con-tinuous camera recordings that are energy consuming, or require custom sensing hardware that are expensive and difficult to use on commodity systems. In this paper, we present FaceListener, a new sensing system that recognizes human facial expressions by only using commodity headphones. The basic idea of FaceListener is to transform the commodity headphone into an acoustic sensing device, which captures the face skin deformations caused by fa-cial muscle movements with different facial expressions. To ensure the recognition accuracy, FaceListener leverages the knowledge distillation technique to learn the subtle correlation between face skin deformation and the acoustic signal changes. Experiment re-sults over multiple human beings demonstrate that FaceListener can accurately recognize more than 80&#x0025; of different facial expressions. FaceListener is highly energy efficient, and can well adapt to different headphone models, host systems and user activities.
SP  - NA
EP  - NA
JF  - 2022 21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ipsn54338.2022.00019
ER  - 

TY  - JOUR
AU  - Piovarči, Michal; Levin, David I. W.; Kaufman, Danny M.; Didyk, Piotr
TI  - Perception-aware modeling and fabrication of digital drawing tools
PY  - 2018
AB  - Digital drawing is becoming a favorite technique for many artists. It allows for quick swaps between different materials, reverting changes, and applying selective modifications to finished artwork. These features enable artists to be more efficient and creative. A significant disadvantage of digital drawing is poor haptic feedback. Artists are usually limited to one surface and a few different stylus nibs, and while they try to find a combination that suits their needs, this is typically challenging. In this work, we address this problem and propose a method for designing, evaluating, and optimizing different stylus designs. We begin with collecting a representative set of traditional drawing tools. We measure their physical properties and conduct a user experiment to build a perceptual space that encodes perceptually-relevant attributes of drawing materials. The space is optimized to both explain our experimental data and correlate it with measurable physical properties. To embed new drawing tool designs into the space without conducting additional experiments and measurements, we propose a new, data-driven simulation technique for characterizing stylus-surface interaction. We finally leverage the perceptual space, our simulation, and recent advancements in multi-material 3D printing to demonstrate the application of our system in the design of new digital drawing tools that mimic traditional drawing materials.
SP  - 1
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201322
ER  - 

TY  - NA
AU  - Anderson, Fraser; Grossman, Tovi; Fitzmaurice, George
TI  - Trigger-Action-Circuits
PY  - 2017
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126637
ER  - 

TY  - CHAP
AU  - , 
TI  - Acknowledgments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544569
ER  - 

TY  - NA
AU  - Lundgard, Alan; Yang, Yiwei; Foster, Maya L.; Lasecki, Walter S.
TI  - CHI - Bolt: Instantaneous Crowdsourcing via Just-in-Time Training
PY  - 2018
AB  - Real-time crowdsourcing has made it possible to solve problems that are beyond the scope of artificial intelligence (AI) within a matter of seconds, rather than hours or days with traditional crowdsourcing techniques. While this has led to an increase in the potential application domains of crowdsourcing and human computation, problems that require machine-level speeds---on the order of milliseconds, not seconds---have remained out of reach because of the fundamental bounds of human perception and response time. In this paper, we demonstrate that it is possible to exceed these bounds by combining human and machine intelligence. We introduce the look-ahead approach, a hybrid intelligence workflow that enables instantaneous crowdsourcing systems (i.e., those that can return crowd responses within mere milliseconds). The look-ahead approach works by exploring possible future states that may be encountered within a short time horizon (e.g., a few seconds into the future) and prefetching crowd worker responses to these states. We validate the efficacy and explore the limitations of our approach on the Bolt system, which consists of an arcade-style game (Lightning Dodger) that we formally model as a Markov Decision Process (MDP). When the MDP reward function is unspecified---as in many real-world tasks---the look-ahead approach enables just-in-time (JIT) training of the agent's policy function. Through a series of crowd worker experiments, we demonstrate that the look-ahead approach can outperform the fastest individual worker by approximately two orders of magnitude. Our work opens new avenues for hybrid intelligence systems that are as smart as people, but also far faster than humanly possible.
SP  - 467
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174041
ER  - 

TY  - JOUR
AU  - Liu, Xiaokang; Li, Chenran; Lu, Lin; Deussen, Oliver; Tu, Changhe
TI  - Fabricable Multi‐Scale Wang Tiles
PY  - 2022
AB  - NA
SP  - 149
EP  - 159
JF  - Computer Graphics Forum
VL  - 41
IS  - 5
PB  - 
DO  - 10.1111/cgf.14610
ER  - 

TY  - CHAP
AU  - Tsuichihara, Satoki; Watanabe, Yuto; Hara, Kaito; Takahashi, Yasutake
TI  - Effect Analysis of Each Reach Zone in Ergonomics During Putting and Pulling Movement of Shelf on VR Experiment
PY  - 2022
AB  - NA
SP  - 336
EP  - 347
JF  - HCI International 2022 – Late Breaking Papers: Ergonomics and Product Design
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-21704-3_21
ER  - 

TY  - NA
AU  - Davis, Josh Urban; Wu, Te-Yen; Shi, Bo; Lu, Hanyi; Panotopoulou, Athina; Whiting, Emily; Yang, Xing-Dong
TI  - CHI - TangibleCircuits: An Interactive 3D Printed Circuit Education Tool for People with Visual Impairments
PY  - 2020
AB  - We present a novel haptic and audio feedback device that allows blind and visually impaired (BVI) users to understand circuit diagrams. TangibleCircuits allows users to interact with a 3D printed tangible model of a circuit which provides audio tutorial directions while being touched. Our system comprises an automated parsing algorithm which extracts 3D printable models as well as an audio interfaces from a Fritzing diagram. To better understand the requirements of designing technology to assist BVI users in learning hardware computing, we conducted a series of formative inquiries into the accessibility limitations of current circuit tutorial technologies. In addition, we derived insights and design considerations gleaned from conducting a formal comparative user study to understand the effectiveness of TangibleCircuits as a tutorial system. We found that BVI users were better able to understand the geometric, spatial and structural circuit information using TangibleCircuits, as well as enjoyed learning with our tool.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376513
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Botner, Alexander; Rukzio, Enrico; Gugenheimer, Jan
TI  - Ephemeral Fabrication: Exploring a Ubiquitous Fabrication Scenario of Low-Effort, In-Situ Creation of Short-Lived Physical Artifacts
PY  - 2022
AB  - Personal fabrication empowers users to create objects increasingly easier and faster. This continuous decrease in effort evokes a speculative scenario of Ephemeral Fabrication (EF), enabled and amplified by emerging paradigms of mobile, wearable, or even body-integrated fabrication. EF yields fast, temporary, in-situ solutions for everyday problems (e.g., creating a protective skin, affixing a phone). Users solely create those, since the required effort is negligible. We present and critically reflect on the EF scenario, by exploring current trends in research and building a body-worn fabrication device. EF is a plausible extrapolation of current developments, entailing both positive (e.g., accessibility) and negative implications (e.g., unsustainability). Using speculative design methodology to question the trajectory of personal fabrication, we argue that to avert the aftermath of such futures, topics like sustainability can not remain an afterthought, but rather be situated in interactions themselves: through embedded constraints, conscious material choice, and constructive embedding of ephemerality.
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501331
ER  - 

TY  - JOUR
AU  - Chen, Yu-Chun; Liao, Chia-Ying; Hsu, Shuo-wen; Huang, Da-Yuan; Chen, Bing-Yu
TI  - Exploring User Defined Gestures for Ear-Based Interactions
PY  - 2020
AB  - The human ear is highly sensitive and accessible, making it especially suitable for being used as an interface for interacting with smart earpieces or augmented glasses. However, previous works on ear-based input mainly address gesture sensing technology and researcher-designed gestures. This paper aims to bring more understandings of gesture design. Thus, for a user elicitation study, we recruited 28 participants, each of whom designed gestures for 31 smart device-related tasks. This resulted in a total of 868 gestures generated. Upon the basis of these gestures, we compiled a taxonomy and concluded the considerations underlying the participants' designs that also offer insights into their design rationales and preferences. Thereafter, based on these study results, we propose a set of user-defined gestures and share interesting findings. We hope this work can shed some light on not only sensing technologies of ear-based input, but also the interface design of future wearable interfaces.
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427314
ER  - 

TY  - NA
AU  - Fang, Likun; Röddiger, Tobias; Schmid, Felix; Beigl, Michael
TI  - AHs - EarRecorder: A Multi-Device Earable Data Collection Toolkit
PY  - 2021
AB  - Earables are a hot topic in wearable research. An increasing number of off-the-shelf devices support collecting sensor data for different use cases. Following this trend, we introduce EarRecorder – a unifying, open-source app that connects to multiple earables (eSense, Cosinuss°, regular earphones). The app can collect data from multiple sensor streams at the same time and lets users label the data during recording. Two user studies allowed us to optimize the usability of the app further and to validate its functionalities. In the future, we hope to include new earables with open APIs and establish a community that advances the app further.
SP  - 286
EP  - 288
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3459005
ER  - 

TY  - NA
AU  - Masai, Katsutoshi; Kunze, Kai; Sakamoto, Daisuke; Sugiura, Yuta; Sugimoto, Maki
TI  - ISMAR - Face Commands - User-Defined Facial Gestures for Smart Glasses
PY  - 2020
AB  - We propose the use of face-related gestures involving the movement of the face, eyes, and head for augmented reality (AR). This technique allows us to use computer systems via hands-free, discreet interactions. In this paper, we present an elicitation study to explore the proper use of facial gestures for daily tasks in the context of a smart home. We used Amazon Mechanical Turk to conduct this study (N=37). Based on the proposed gestures, we report usage scenarios and complexity, proposed associations between gestures/tasks, a user-defined gesture set, and insights from the participants. We also conducted a technical feasibility study (N=13) with participants using smart eyewear to consider their uses in daily life. The device has 16 optical sensors and an inertial measurement unit (IMU). We can potentially integrate the system into optical see-through displays or other smart glasses. The results demonstrate that the device can detect eight temporal face-related gestures with a mean F1 score of 0.911 using a convolutional neural network (CNN). We also report the results of user-independent training and a one-hour recording of the experimenter testing two of the gestures.
SP  - 374
EP  - 386
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00064
ER  - 

TY  - JOUR
AU  - Lee, Lik Hang; Braud, Tristan; Hosio, Simo; Hui, Pan
TI  - Towards Augmented Reality Driven Human-City Interaction: Current Research on Mobile Headsets and Future Challenges
PY  - 2021
AB  - Interaction design for Augmented Reality (AR) is gaining attention from both academia and industry. This survey discusses 260 articles (68.8% of articles published between 2015–2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by reviewing the latest trends of information visualization, constrained interfaces, and embodied interaction for AR headsets. We highlight under-explored issues in interface design and input techniques that warrant further research and conjecture that AR with complementary Conversational User Interfaces (CUIs) is a crucial enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of AR in Human-City Interaction.
SP  - 1
EP  - 38
JF  - ACM Computing Surveys
VL  - 54
IS  - 8
PB  - 
DO  - 10.1145/3467963
ER  - 

TY  - NA
AU  - Lin, Richard; Ramesh, Rohit; Chi, Connie; Jain, Nikhil; Nuqui, Ryan; Dutta, Prabal; Hartmann, Björn
TI  - UIST - Polymorphic Blocks: Unifying High-level Specification and Low-level Control for Circuit Board Design
PY  - 2020
AB  - Mainstream board-level circuit design tools work at the lowest level of design --- schematics and individual components. While novel tools experiment with higher levels of design, abstraction often comes at the expense of the fine-grained control afforded by low-level tools. In this work, we propose a hardware description language (HDL) approach that supports users at multiple levels of abstraction from broad system architecture to subcircuits and component selection. We extend the familiar hierarchical block diagram with polymorphism to include abstract-typed blocks (e.g., generic resistor supertype) and electronics modeling (i.e., currents and voltages). Such an approach brings the advantages of reusability and encapsulation from object-oriented programming, while addressing the unique needs of electronics designers such as physical correctness verification. We discuss the system design, including fundamental abstractions, the block diagram construction HDL, and user interfaces to inspect and fine-tune the design; demonstrate example designs built with our system; and present feedback from intermediate-level engineers who have worked with our system.
SP  - 529
EP  - 540
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415860
ER  - 

TY  - JOUR
AU  - Brocker, Anke; Schäfer, René; Remy, Christian; Voelker, Simon; Borchers, Jan
TI  - Flowboard: How Seamless, Live, Flow-Based Programming Impacts Learning to Code for Embedded Electronics
PY  - 2022
AB  - <jats:p> Toolkits like the Arduino system have brought embedded programming to STEM education. However, learning embedded programming is still hard, requiring an understanding of coding, electronics, and how both sides interact. To investigate the opportunities of using a different programming paradigm than the imperative approach to learning embedded coding, we developed <jats:italic>Flowboard</jats:italic> . Students code in a visual iPad editor using <jats:italic>flow-based programming</jats:italic> , which is conceptually closer to circuit diagrams than imperative code. Two breadboards with I/O pins mirrored on the iPad connect electronics and program graph more <jats:italic>seamlessly</jats:italic> than existing IDEs. Program changes take effect immediately. This <jats:italic>liveness</jats:italic> reflects circuit behavior better than edit-compile-run loops. A first study confirmed that students can solve basic embedded programming tasks with Flowboard while highlighting important differences to a typical imperative IDE, Ardublock. A second, in-depth study provided qualitative insights into Flowboard’s impact on students’ conceptual models of electronics and embedded programming and exploring those. </jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3533015
ER  - 

TY  - JOUR
AU  - del Río, Iván Gamino; Hellín, Agustín Martínez; Polo, Óscar R.; Arribas, Miguel Jiménez; Parra, Pablo; da Silva, Antonio; Sánchez, Jonatan; Sánchez, Sebastián
TI  - A RISC-V Processor Design for Transparent Tracing
PY  - 2020
AB  - Code instrumentation enables the observability of an embedded software system during its execution. A usage example of code instrumentation is the estimation of “worst-case execution time” using hybrid analysis. This analysis combines static code analysis with measurements of the execution time on the deployment platform. Static analysis of source code determines where to insert the tracing instructions, so that later, the execution time can be captured using a logic analyser. The main drawback of this technique is the overhead introduced by the execution of trace instructions. This paper proposes a modification of the architecture of a RISC pipelined processor that eliminates the execution time overhead introduced by the code instrumentation. In this way, it allows the tracing to be non-intrusive, since the sequence and execution times of the program under analysis are not modified by the introduction of traces. As a use case of the proposed solution, a processor, based on RISC-V architecture, was implemented using VHDL language. The processor, synthesized on a FPGA, was used to execute and evaluate a set of examples of instrumented code generated by a “worst-case execution time” estimation tool. The results validate that the proposed architecture executes the instrumented code without overhead.
SP  - 1873
EP  - NA
JF  - Electronics
VL  - 9
IS  - 11
PB  - 
DO  - 10.3390/electronics9111873
ER  - 

TY  - JOUR
AU  - Kent, Lee; Snider, Chris; Gopsill, James; Hicks, Ben
TI  - Mixed Reality in design prototyping: A systematic review
PY  - 2021
AB  - NA
SP  - 101046
EP  - NA
JF  - Design Studies
VL  - 77
IS  - NA
PB  - 
DO  - 10.1016/j.destud.2021.101046
ER  - 

TY  - NA
AU  - Li, Jiasheng; Yan, Zeyu; Jarjue, Ebrima Haddy; Shetty, Ashrith; Peng, Huaishu
TI  - TangibleGrid: Tangible Web Layout Design for Blind Users
PY  - 2022
AB  - We present TangibleGrid, a novel device that allows blind users to understand and design the layout of a web page with real-time tangible feedback. We conducted semi-structured interviews and a series of co-design sessions with blind users to elicit insights that guided the design of TangibleGrid. Our final prototype contains shape-changing brackets representing the web elements and a baseboard representing the web page canvas. Blind users can design a web page layout through creating and editing web elements by snapping or adjusting tangible brackets on top of the baseboard. The baseboard senses the brackets' type, size, and location, verbalizes the information, and renders the web page on the client browser. Through a formative user study, we found that blind users could understand a web page layout through TangibleGrid. They were also able to design a new web layout from scratch without the help of sighted people.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545627
ER  - 

TY  - JOUR
AU  - Simons, Melanie F.; Digumarti, Krishna Manaswi; Le, Nguyen Hao; Chen, Hsing-Yu; Carreira, Sara Correia; Zaghloul, Nouf S.S.; Diteesawat, Richard Suphapol; Garrad, Martin; Conn, Andrew T.; Kent, Christopher; Rossiter, Jonathan
TI  - B:Ionic Glove: A Soft Smart Wearable Sensory Feedback Device for Upper Limb Robotic Prostheses
PY  - 2021
AB  - Upper limb robotic prosthetic devices currently lack adequate sensory feedback, contributing to a high rejection rate. Incorporating affective sensory feedback into these devices reduces phantom limb pain and increases control and acceptance. To address the lack of sensory feedback we present the B:Ionic glove, wearable over a robotic hand which contains sensing, computation and actuation on board. It uses shape memory alloy (SMA) actuators integrated into an armband to gently squeeze the user's arm when pressure is sensed in novel electro-fluidic fingertip sensors and decoded through soft matter logic. We found that a circular electro-fluidic sensor cavity generated the most sensitive fingertip sensor and considered a computational configuration to convey different information from robot to user. A user study was conducted to characterise the tactile interaction capabilities of the device. No significant difference was found between the skin sensitivity threshold of participants’ lower and upper arm. They found it easier to distinguish stimulation locations than strengths. Finally, we demonstrate a proof-of-concept of the complete device, illustrating how it could be used to grip an object, solely from the affective tactile feedback provided by the B:Ionic glove. The B:Ionic glove is a step towards the integration of natural, soft sensory feedback into robotic prosthetic devices.
SP  - 3311
EP  - 3316
JF  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 2
PB  - 
DO  - 10.1109/lra.2021.3064269
ER  - 

TY  - NA
AU  - Cheema, Noshaba; Frey-Law, Laura; Naderi, Kourosh; Lehtinen, Jaakko; Slusallek, Philipp; Hämäläinen, Perttu
TI  - CHI - Predicting Mid-Air Interaction Movements and Fatigue Using Deep Reinforcement Learning
PY  - 2020
AB  - A common problem of mid-air interaction is excessive arm fatigue, known as the "Gorilla arm" effect. To predict and prevent such problems at a low cost, we investigate user testing of mid-air interaction without real users, utilizing biomechanically simulated AI agents trained using deep Reinforcement Learning (RL). We implement this in a pointing task and four experimental conditions, demonstrating that the simulated fatigue data matches human fatigue data. We also compare two effort models: 1) instantaneous joint torques commonly used in computer animation and robotics, and 2) the recent Three Compartment Controller (3CC-) model from biomechanical literature. 3CC- yields movements that are both more efficient and relaxed, whereas with instantaneous joint torques, the RL agent can easily generate movements that are quickly tiring or only reach the targets slowly and inaccurately. Our work demonstrates that deep RL combined with the 3CC- provides a viable tool for predicting both interaction movements and user experiencein silico, without users.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376701
ER  - 

TY  - JOUR
AU  - Sakashita, Mose; Ricci, E. Andy; Arora, Jatin; Guimbretière, François
TI  - RemoteCoDe: Robotic Embodiment for Enhancing Peripheral Awareness in Remote Collaboration Tasks
PY  - 2022
AB  - <jats:p>Collaborative design activities are often centered around physical artifacts. Depending on the design activity, this can be the model of a building, paper crafts, carving artwork, or a new circuit to be debugged and evaluated. In a typical setting, collaborators are seated around a table and divide their attention between the design artifact under review, at least one laptop supporting measurements and information foraging, and of course their collaborators. Although these activities involve complex sets of tools and configurations, people can easily work together when they are present in the same space. This is because the physical presence of a partner affords peripheral awareness to inform where the partner's attention is and what they are doing. This peripheral awareness allows collaborators to coordinate actions and manage coupling to achieve a shared task. For example, it is quite easy to know when your partner switches their focus from a breadboard to you as a request to start a face to face discussion.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512910
ER  - 

TY  - NA
AU  - Fender, Andreas Rene; Holz, Christian
TI  - Causality-preserving Asynchronous Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501836
ER  - 

TY  - JOUR
AU  - Mankoff, Jennifer; Hofmann, Megan; Chen, Xiang 'Anthony'; Hudson, Scott E.; Hurst, Amy; Kim, Jeeeun
TI  - Consumer-grade fabrication and its potential to revolutionize accessibility
PY  - 2019
AB  - <jats:p>Digital fabrication technologies open new doors---and challenges---for real-world support.</jats:p>
SP  - 64
EP  - 75
JF  - Communications of the ACM
VL  - 62
IS  - 10
PB  - 
DO  - 10.1145/3339824
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Braud, Tristan; Hosio, Simo; Hui, Pan
TI  - Towards Augmented Reality-driven Human-City Interaction: Current Research and Future Challenges.
PY  - 2020
AB  - Interaction design for Augmented Reality (AR) is gaining increasing attention from both academia and industry. This survey discusses 205 articles (75% of articles published between 2015 - 2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by a review of the latest trends of information visualization, constrained interfaces, and embodied interaction for AR headsets. We highlight under-explored issues in interface design and input techniques that warrant further research, and conjecture that AR with complementary Conversational User Interfaces (CUIs) is a key enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of AR in Human-City Interaction.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kaimoto, Hiroki; Yamaoka, Junichi; Nakamaru, Satoshi; Kawahara, Yoshihiro; Kakehi, Yasuaki
TI  - Tangible and Embedded Interaction - ExpandFab: Fabricating Objects Expanding and Changing Shape with Heat
PY  - 2020
AB  - ExpandFab is a fabrication method for creating expanding objects using foam materials. The printed objects change their shape and volume, which is advantageous for reducing the printing time and transportation costs. For the fabrication of expanding objects, we investigated a basic principle of the expansion rate and developed materials by mixing a foam powder and elastic adhesive. Furthermore, we developed a fabrication method using the foam materials. A user can design expanded objects using our design software and sets the expansion areas on the surface. The software simulates and exports the 3d model into a three-dimensional (3D) printer. The 3D printer prints the expandable object by curing with ultraviolet light. Finally, the user heats the printed objects, and the objects expand to maximum approximately 2.7 times of their original size. ExpandFab allows users to prototype products that expand and morph into various shapes, such as objects changing from one shape to various shapes, and functional prototype with electronic components. In this paper, we describe the basic principle of this technique, implementation of the software and hardware, application examples, limitations and discussions, and future works.
SP  - 153
EP  - 164
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374949
ER  - 

TY  - NA
AU  - Je, Seungwoo; Lee, Minkyeong; Kim, Yoonji; Chan, Liwei; Yang, Xing-Dong; Bianchi, Andrea
TI  - CHI - PokeRing: Notifications by Poking Around the Finger
PY  - 2018
AB  - Smart-rings are ideal for subtle and always-available haptic notifications due to their direct contact with the skin. Previous researchers have highlighted the feasibility of haptic technology in smart-rings and their promise in delivering noticeable stimulations by poking a limited set of planar locations on the finger. However, the full potential of poking as a mechanism to deliver richer and more expressive information on the finger is overlooked. With three studies and a total of 76 participants, we informed the design of PokeRing, a smart-ring capable of delivering information via stimulating eight different locations around the index finger's proximal phalanx. We report our evaluation of the performance of PokeRing in semi-realistic wearable conditions, (standing and walking), and its effective usage for information transfer with twenty-one spatio-temporal patterns designed by six interaction designers in a workshop. Finally, we present three applications that exploit PokeRing's notification usages.
SP  - 542
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174116
ER  - 

TY  - NA
AU  - Echeverri, Daniel
TI  - The Non-myth of the Noble Red: Exploring Brechtian Principles of Storytelling and Performance in the Authoring of a Tangible Narrative
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3535207
ER  - 

TY  - CHAP
AU  - Muthukumarana, Sachith; Elvitigala, Don Samitha; Wu, Qin; Pai, Yun Suen; Nanayakkara, Suranga
TI  - INTERACT (5) - Jammify: Interactive Multi-sensory System for Digital Art Jamming
PY  - 2021
AB  - As social distancing is becoming the new normal, technology holds the potential to bridge this societal gap through novel interaction modalities that allow multiple users to collaborate and create content together. We present Jammify, an interactive multi-sensory system that focuses on providing a unique digital art-jamming experience with a visual display and a wearable arm-sleeve. The ‘jamming-canvas’ visual display is a two-sided LED light wall (2 m \(\times \) 6 m) where users can draw free-hand gestures on either side and switch between two view modes: own-view and shared-view. The arm-sleeve uses shape-memory-alloy integrated fabric to sense and re-create a subtle and natural touch sensation on each other’s hands. We describe the details of the design and interaction possibilities based on the diverse combinations of both input and output modalities of the system, as well as findings from a user study with ten participants.
SP  - 23
EP  - 41
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85607-6_2
ER  - 

TY  - NA
AU  - Zhang, Chi; Sahoo, Deepak Ranjan; Pearson, Jennifer; Robinson, Simon; Holton, Mark D.; Hopkins, Philip; Jones, Matt
TI  - MobileHCI - Active PinScreen: Exploring Spatio-Temporal Tactile Feedbackfor Multi-Finger Interaction
PY  - 2020
AB  - Multiple fingers are often used for efficient interaction with handheld computing devices. Currently, any tactile feedback provided is felt on the finger pad or the palm with coarse granularity. In contrast, we present a new tactile feedback technique, Active PinScreen, that applies localised stimuli on multiple fingers with fine spatial and temporal resolution. The tactile screen uses an array of solenoid-actuated magnetic pins with millimetre scale form-factor which could be deployed for back-of-device handheld use without instrumenting the user. As well as presenting a detailed description of the prototype, we provide the potential design configurations and the applications of the Active PinScreen and evaluate the human factors of tactile interaction with multiple fingers in a controlled user evaluation. The results of our study show a high recognition rate for directional and patterned stimulation across different grip orientations as well as within- and between- fingers. We end the paper with a discussion of our main findings, limitations in the current design and directions for future work.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403531
ER  - 

TY  - NA
AU  - Peiris, Roshan Lalitha; Feng, Yuan-Ling; Chan, Liwei; Minamizawa, Kouta
TI  - CHI - ThermalBracelet: Exploring Thermal Haptic Feedback Around the Wrist
PY  - 2019
AB  - Smartwatches enable the wrist to be used as an ideal location to provide always-available haptic notifications as they are constantly worn with direct contact with the skin. With the wrist straps, the haptic feedback can be extended to the full space around the wrist to provide more spatial and enriched feedback. With ThermalBracelet, we investigate thermal feedback as a haptic feedback modality around the wrist. We present three studies that lead to the development of a smartwatch-integratable thermal bracelet that stimulates six locations around the wrist. Our initial evaluation reports on the selection of the thermal module configurations. Secondly, with the selected six-module configuration, we explore its usability in a real-world scenarios such as walking and reading. Thirdly, we investigate its capability of providing spatio temporal feedback while engaged in distracting tasks. Finally we present application scenarios that demonstrates its usability.
SP  - 170
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300400
ER  - 

TY  - NA
AU  - Ma, Dong; Ferlini, Andrea; Mascolo, Cecilia
TI  - OESense: Employing Occlusion Effect for In-ear Human Sensing
PY  - 2021
AB  - Smart earbuds are recognized as a new wearable platform for personal-scale human motion sensing. However, due to the interference from head movement or background noise, commonly-used modalities (e.g. accelerometer and microphone) fail to reliably detect both intense and light motions. To obviate this, we propose OESense, an acoustic-based in-ear system for general human motion sensing. The core idea behind OESense is the joint use of the occlusion effect (i.e., the enhancement of low-frequency components of bone-conducted sounds in an occluded ear canal) and inward-facing microphone, which naturally boosts the sensing signal and suppresses external interference. We prototype OESense as an earbud and evaluate its performance on three representative applications, i.e., step counting, activity recognition, and hand-to-face gesture interaction. With data collected from 31 subjects, we show that OESense achieves 99.3% step counting recall, 98.3% recognition recall for 5 activities, and 97.0% recall for five tapping gestures on human face, respectively. We also demonstrate that OESense is compatible with earbuds' fundamental functionalities (e.g. music playback and phone calls). In terms of energy, OESense consumes 746 mW during data recording and recognition and it has a response latency of 40.85 ms for gesture recognition. Our analysis indicates such overhead is acceptable and OESense is potential to be integrated into future earbuds.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Tymms, Chelsea; Wang, Siqi; Zorin, Denis
TI  - Appearance-preserving tactile optimization
PY  - 2020
AB  - Textures are encountered often on various common objects and surfaces. Many textures combine visual and tactile aspects, each serving important purposes; most obviously, a texture alters the object's appearance or tactile feeling as well as serving for visual or tactile identification and improving usability. The tactile feel and visual appearance of objects are often linked, but they may interact in unpredictable ways. Advances in high-resolution 3D printing enable highly flexible control of geometry to permit manipulation of both visual appearance and tactile properties. In this paper, we propose an optimization method to independently control the tactile properties and visual appearance of a texture. Our optimization is enabled by neural network-based models, and allows the creation of textures with a desired tactile feeling while preserving a desired visual appearance at a relatively low computational cost, for use in a variety of applications.
SP  - 212
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 6
PB  - 
DO  - 10.1145/3414685.3417857
ER  - 

TY  - CHAP
AU  - Riveni, Mirela; Hillen, Christiaan; Dustdar, Schahram
TI  - Privacy in Human Computation: User Awareness Study, Implications for Existing Platforms, Recommendations, and Research Directions
PY  - 2018
AB  - Research and industry have made great advancements in human computation and today we can see multiple forms of it reflected in growing numbers and diversification of platforms, from crowdsourcing ones, social computing platforms (in terms of collaborative task execution), and online labor/expert markets to collective adaptive systems (CAS) with humans-in-the-loop. Despite the advancements in various mechanisms to support effective provisioning of human computation, there is still one topic that seems to be close to neglected both in research and the current design and development of human computation systems, namely privacy. In this work, we investigate this problem. Starting from the fact that user awareness is crucial for enforcing privacy-respecting mechanisms, we conducted an online survey study to assess user privacy awareness in human computation systems and in this paper provide the results of it. Lastly, we provide recommendations for developers for designing privacy-preserving human computation platforms as well as research directions.
SP  - 247
EP  - 267
JF  - Lecture Notes in Social Networks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-94105-9_10
ER  - 

TY  - JOUR
AU  - Huang, Cheng-Yi; Yang, Gang; Huang, Pei; Hu, Jin-Ming; Tang, Zhen-Hua; Li, Yuan-Qing; Fu, Shao-Yun
TI  - Flexible Pressure Sensor with an Excellent Linear Response in a Broad Detection Range for Human Motion Monitoring.
PY  - 2023
AB  - Pressure sensing is highly demanding in wearable devices, robotics, and artificial intelligence, whereas it is still a big challenge to develop a pressure sensor with an excellent linear response in a broad detection range. Herein, a flexible and porous carbon nanotube (CNT)/carbon black (CB)/carbonyl iron powder (CIP)/silicone composite is proposed by a simple strategy of mixing, curing, and washing. Due to the porous structure induced by the sacrifice of sugar particles, an excellent linear response (<i>R</i><sup>2</sup> = 0.999) is achieved for the composite sensor by manipulating the contributions of contact resistance and tunnel resistance to the sensing performance via the alternation of CB and CNT contents. Moreover, the porous structure donates the composite sensor a low compressive modulus at a low pressure level, while the CIPs introduced lead to a high compressive modulus at a high pressure level with the assistance of an external magnetic field. As a result, the sensor produced has a wide linear response range of 80 Pa to 220 kPa, much wider than most of the linear response pressure sensors reported previously. The wide detection range is demonstrated by cyclic pressure tests in the frequency range of 0.1-5 Hz, durability tests, and monitoring human or robot motions including breathing, walking, lifting, and boxing, etc. Taking the advantages of low cost, high sensitivity, and excellent linear response in a wide pressure range, the current composite sensor is promising for precise monitoring of human motions and delicate controlling of robots.
SP  - 3476
EP  - 3485
JF  - ACS applied materials & interfaces
VL  - 15
IS  - 2
PB  - 
DO  - 10.1021/acsami.2c19465
ER  - 

TY  - CHAP
AU  - Khakurel, Jayden; Porras, Jari; Melkas, Helinä; Fu, Bo
TI  - A Comprehensive Framework of Usability Issues Related to the Wearable Devices
PY  - 2020
AB  - Wearable devices have the potential to be used for monitoring, augmenting, assisting, delivering content, and tracking in both individual and organizational contexts. Despite this potential, previous studies indicate that the abandonment rate is quite high relative to the usage rate due to usability factors. This chapter provides a comprehensive systematic literature review on the usability issues related to wearable devices, as well as recommendations for overcoming the identified problems. It also investigates and presents a survey of the existing usability evaluation methods used to identify and evaluate the usability of wearable devices, including their strengths and limitations. As such, we present a categorization framework that gives an overview of the overall usability issues that act as the barriers to user adoption and a summary of which types of usability issues are associated with which type of device category. The chapter has the potential to inform and assist researchers, practitioners, and application developers as they work toward developing, implementing, and evaluating wearable devices and their associated interfaces, and this, in turn, may assist with sustained engagement among users.
SP  - 21
EP  - 66
JF  - EAI/Springer Innovations in Communication and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-41368-2_2
ER  - 

TY  - JOUR
AU  - Srivastava, Tanmay; Khanna, Prerna; Pan, Shijia; Nguyen, Phuc; Jain, Shubham
TI  - MuteIt
PY  - 2022
AB  - <jats:p>In this paper, we present MuteIt, an ear-worn system for recognizing unvoiced human commands. MuteIt presents an intuitive alternative to voice-based interactions that can be unreliable in noisy environments, disruptive to those around us, and compromise our privacy. We propose a twin-IMU set up to track the user's jaw motion and cancel motion artifacts caused by head and body movements. MuteIt processes jaw motion during word articulation to break each word signal into its constituent syllables, and further each syllable into phonemes (vowels, visemes, and plosives). Recognizing unvoiced commands by only tracking jaw motion is challenging. As a secondary articulator, jaw motion is not distinctive enough for unvoiced speech recognition. MuteIt combines IMU data with the anatomy of jaw movement as well as principles from linguistics, to model the task of word recognition as an estimation problem. Rather than employing machine learning to train a word classifier, we reconstruct each word as a sequence of phonemes using a bi-directional particle filter, enabling the system to be easily scaled to a large set of words. We validate MuteIt for 20 subjects with diverse speech accents to recognize 100 common command words. MuteIt achieves a mean word recognition accuracy of 94.8% in noise-free conditions. When compared with common voice assistants, MuteIt outperforms them in noisy acoustic environments, achieving higher than 90% recognition accuracy. Even in the presence of motion artifacts, such as head movement, walking, and riding in a moving vehicle, MuteIt achieves mean word recognition accuracy of 91% over all scenarios.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550281
ER  - 

TY  - NA
AU  - Zhang, Qian; Cao, Yetong; Chen, Huijie; Li, Fan; Yang, Song; Wang, Yu; Yang, Zheng; Liu, Yunhao
TI  - ICDCS - airFinger: Micro Finger Gesture Recognition via NIR Light Sensing for Smart Devices
PY  - 2020
AB  - Micro finger gesture recognition is an emerging approach to realize more friendly interaction between human and smart devices, especially for small wearable devices, such as smartwatches and virtual reality glasses. This paper proposes airFinger, a novel solution utilizing NIR light sensing to realize both real-time gesture recognition and finger tracking aiming at micro finger gestures. Using a custom NIR-based sensor with novel algorithms to capture subtle finger movements, airFinger enables to detect a rich set of micro finger gestures and track finger movements in terms of scrolling direction, velocity, and displacement. Besides, airFinger is capable of effective noise mitigation, gesture segmentation, and reducing false recognition due to the unintentional actions of users. Extensive experimental results demonstrate that airFinger has robustness against individual diversity, gesture inconsistency, and many other impacts. The overall performance reaches an average accuracy as high as 98.72% over a set of 8 micro finger gestures among 10, 000 gesture samples collected from 10 volunteers.
SP  - 552
EP  - 562
JF  - 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icdcs47774.2020.00073
ER  - 

TY  - NA
AU  - Veras, Rafael; Singh, Gaganpreet; Farhadi-Niaki, Farzin; Udhani, Ritesh; Patekar, Parth Pradeep; Zhou, Wei; Irani, Pourang; Li, Wei
TI  - CHI - Elbow-Anchored Interaction: Designing Restful Mid-Air Input
PY  - 2021
AB  - We designed a mid-air input space for restful interactions on the couch. We observed people gesturing in various postures on a couch and found that posture affects the choice of arm motions when no constraints are imposed by a system. Study participants that sat with the arm rested were more likely to use the forearm and wrist, as opposed to the whole arm. We investigate how a spherical input space, where forearm angles are mapped to screen coordinates, can facilitate restful mid-air input in multiple postures. We present two controlled studies. In the first, we examine how a spherical space compares with a planar space in an elbow-anchored setup, with a shoulder-level input space as baseline. In the second, we examine the performance of a spherical input space in four common couch postures that set unique constraints to the arm. We observe that a spherical model that captures forearm movement facilitates comfortable input across different seated postures.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445546
ER  - 

TY  - NA
AU  - Quinn, Philip
TI  - CHI - Estimating Touch Force with Barometric Pressure Sensors
PY  - 2019
AB  - Finger pressure offers a new dimension for touch interaction, where input is defined by its spatial position and orthogonal force. However, the limited availability and complexity of integrated force-sensing hardware in mobile devices is a barrier to exploring this design space. This paper presents a synthesis of two features in recent mobile devices - a barometric sensor (pressure altimeter) and ingress protection - to sense a user's touch force. When a user applies force to a device's display, it flexes inward and causes an increase in atmospheric pressure within the sealed chassis. This increase in pressure can be sensed by the device's internal barometer. However, this change is uncontrolled and requires a calibration model to map atmospheric pressure to touch force. This paper derives such a model and demonstrates its viability on four commercially-available devices (including two with dedicated force sensors). The results show this method is sensitive to forces of less than 1 N, and is comparable to dedicated force sensors.
SP  - 689
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300919
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Hung, Ching-Wen; Wu, Tzu-Chun; Chen, Bing-Yu
TI  - CHI - ElastOscillation: 3D Multilevel Force Feedback for Damped Oscillation on VR Controllers
PY  - 2020
AB  - Force feedback from damped oscillation is a common effect in our daily lives, especially when shaking an elastic object, an object hanging or containing other stuff, or a container with liquid, e.g., casting with a fishing pole or wine-swirling. Such a force, affected by complex physical variations and collisions, is difficult to properly simulate using current force feedback methods. Therefore, we propose ElastOscillation on a virtual reality (VR) controller to provide 3D multilevel force feedback for damped oscillation to enhance VR experiences. ElastOscillation consists of a proxy, six elastic bands and DC motors. It leverages the motors to control the bands' elasticity to restrain the movement of the proxy, which is connected with the bands. Therefore, when users shake the ElastOscillation device, the proxy shakes or moves in corresponding ranges of movement. The users then perceive the force from oscillation at different levels. In addition, elastic force from the bands further reinforces the oscillation force feedback. We conducted a force perception study to understand users' distinguishability for perceiving oscillation forces in 1D and 2D movement, respectively. Based on the results, we performed a VR experience study to show that the force feedback provided by ElastOscillation enhances VR realism.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376408
ER  - 

TY  - NA
AU  - Montano-Murillo, Roberto A.; Cornelio-Martinez, Patricia I.; Subramanian, Sriram; Martinez-Plasencia, Diego
TI  - UIST - Drift-Correction Techniques for Scale-Adaptive VR Navigation
PY  - 2019
AB  - Scale adaptive techniques for VR navigation enable users to navigate spaces larger than the real space available, while allowing precise interaction when required. However, due to these techniques gradually scaling displacements as the user moves (changing user's speed), they introduce a Drift effect. That is, a user returning to the same point in VR will not return to the same point in the real space. This mismatch between the real/virtual spaces can grow over time, and turn the techniques unusable (i.e., users cannot reach their target locations). In this paper, we characterise and analyse the effects of Drift, highlighting its potential detrimental effects. We then propose two techniques to correct Drift effects and use a data driven approach (using navigation data from real users with a specific scale adaptive technique) to tune them, compare their performance and chose an optimum correction technique and configuration. Our user study, applying our technique in a different environment and with two different scale adaptive navigation techniques, shows that our correction technique can significantly reduce Drift effects and extend the life-span of the navigation techniques (i.e., time that they can be used before Drift draws targets unreachable), while not hindering users' experience.
SP  - 1123
EP  - 1135
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347914
ER  - 

TY  - NA
AU  - Suguitan, Michael; Hoffman, Guy
TI  - CHI Extended Abstracts - You Are (Not) The Robot: Variable Perspective Motion Control of a Social Telepresence Robot
PY  - 2021
AB  - COVID-19 has dramatically limited opportunities for in-person human-robot interaction research and shifted focus towards remote technologies such as telepresence robots. Telepresence robots enable rich communication and agency through their physical presence and controllability, but their screen-oriented designs and button-centric controls abstract users away from their own physicality. In this demonstration, we present a telepresence system for remotely controlling a social robot using a smartphone’s motion sensors. Users can select between a first-person perspective from the robot’s internal camera or a third-person perspective showing the robot’s whole body. Users can also record their movements for later playback. This system has applications as an embodied remote communication platform and for crowdsourcing demonstrations of user-crafted robot movements.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451571
ER  - 

TY  - NA
AU  - Gonzalez, Eric J; Chase, Elyse D. Z.; Kotipalli, Pramod; Follmer, Sean
TI  - A Model Predictive Control Approach for Reach Redirection in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501907
ER  - 

TY  - NA
AU  - Kruijff, Ernst; Biswas, Saugata; Trepkowski, Christina; Maiero, Jens; Ghinea, George; Stuerzlinger, Wolfgang
TI  - CHI - Multilayer Haptic Feedback for Pen-Based Tablet Interaction
PY  - 2019
AB  - We present a novel, multilayer interaction approach that enables state transitions between spatially above-screen and 2D on-screen feedback layers. This approach supports the exploration of haptic features that are hard to simulate using rigid 2D screens. We accomplish this by adding a haptic layer above the screen that can be actuated and interacted with (pressed on) while the user interacts with on-screen content using pen input. The haptic layer provides variable firmness and contour feedback, while its membrane functionality affords additional tactile cues like texture feedback. Through two user studies, we look at how users can use the layer in haptic exploration tasks, showing that users can discriminate well between different firmness levels, and can perceive object contour characteristics. Demonstrated also through an art application, the results show the potential of multilayer feedback to extend on-screen feedback with additional widget, tool and surface properties, and for user guidance.
SP  - 143
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300373
ER  - 

TY  - NA
AU  - Chen, Yan; Herskovitz, Jaylin; Lasecki, Walter S.; Oney, Steve
TI  - VL/HCC - Bashon: A Hybrid Crowd-Machine Workflow for Shell Command Synthesis
PY  - 2020
AB  - Despite advances in machine learning, there has been little progress towards creating automated systems that can reliably solve general purpose tasks, such as programming or scripting. In this paper, we propose techniques for increasing the reliability of automated systems for program synthesis tasks via a hybrid workflow that augments the system with input from crowds of human workers. Unlike previous hybrid workflow systems, which have been focused on less complex tasks that crowd workers can do in their entirety (e.g., image labeling), our proposed workflow handles tasks that untrained crowd workers cannot do alone (i.e., scripting). We evaluate our approach by creating BashOn, a system that increases the performance of an automated program that generates Bash shell commands from natural language descriptions by ~30%. Our approach can not only help people make program synthesis tools more robust, reliable, and trustworthy for end-users to use, but also help lower the cost of downstream data collection for program synthesis when a preliminary model exists.
SP  - 1
EP  - 8
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127248
ER  - 

TY  - NA
AU  - Zhu, Jingwen; Kao, Hsin-Liu (Cindy)
TI  - Scaling E-Textile Production: Understanding the Challenges of Soft Wearable Production for Individual Creators
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544794.3558475
ER  - 

TY  - CHAP
AU  - , 
TI  - Theories of Embodiment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544573
ER  - 

TY  - NA
AU  - Didehkhorshid, Seyed Amir Ahmad
TI  - Performance Evaluation of Warped Virtual Surfaces in Virtual Reality
PY  - 2020
AB  - This thesis proposes a novel surface warping (scaling) technique similar to applying CD gain to the traditional mouse cursor on a screen with a 1:1 input device in Virtual Reality (VR). We call the technique Warped Virtual Surfaces (WVS). WVS solution is a promising way of providing large tactile surfaces in VR while using small physical surfaces with little impact on user performance. We utilized a stylus with VR head-mounted displays (HMDs), enabling users to interact with arbitrarily large virtual panels in VR while their real physical movement is within a fixed-sized real panel area.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Sanches, Emilia Christie Picelli; Bueno, Juliana; Okimoto, Maria Lucia Leite Ribeiro
TI  - HCI (7) - Designing 3D Printed Audio-Tactile Graphics: Recommendations from Prior Research
PY  - 2021
AB  - 3D printed audio-tactile graphics are interactive materials aimed at the accessibility of people with visual impairment. There is not yet a consensus on how to design 3D printed audio-tactile graphics and most publications on this topic follow an empirical approach, experimenting first and learning with the process. However, these research publications show meaningful knowledge that can serve as the base for a set of compiled design recommendations. The goal of this paper is to present recommendations that were analyzed and extracted from 32 publications found in a systematic literature review. In total, 57 recommendations compose the set.
SP  - 461
EP  - 472
JF  - Universal Access in Human-Computer Interaction. Design Methods and User Experience
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78092-0_31
ER  - 

TY  - JOUR
AU  - Biener, Verena; Gesslein, Travis; Schneider, Daniel; Kawala, Felix; Otte, Alexander; Kristensson, Per Ola; Pahud, Michel; Ofek, Eyal; Campos, Cuauhtli; Kljun, Matjaz; Pucihar, Klen Copic; Grubert, Jens
TI  - PoVRPoint: Authoring Presentations in Mobile Virtual Reality.
PY  - 2022
AB  - Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint-a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded shapes or objects. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.
SP  - 2069
EP  - 2079
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2022.3150474
ER  - 

TY  - NA
AU  - Demoe, Matthew; Uribe-Quevedo, Alvaro; de Lima Salgado, André; Mimura, Hidenori; Kanev, Kamen; Hung, Patrick C. K.
TI  - SeGAH - Exploring Data Glove and Robotics Hand Exergaming: Lessons Learned
PY  - 2020
AB  - In this preliminary study, we explore the use of a high-end data glove as a consumer-level hand exergame human interface device. Disorders affecting the musculoskeletal apparatus account for approximately 43 % of all workplace related injuries, leading to increasing claim costs and work absenteeism. Treatment includes unsupervised stretching and exercising with low adherence due to its monotonous and repetitive nature. Exergames, that is the use of games to elicit physical activity, provide engaging experiences that can help motive patients or workers into performing the exercises. Previous works using consumer-level technology have focused on image-based and open electronics 3D printed gloves that have shown the potential of exergames and motion capture as a tool to add immersion. In this paper, we present exergame that employs the Yamaha Data Glove (YDG) integrated to a computer- and robot-based exergame. The data glove allows controlling a virtual arcade crane in addition to interactive sessions with a social robot called ASUS Zenbo Junior. The preliminary quantitative and qualitative data suggest that motion capture data requires further processing and customization to tailor the experience to each user to improve usability and cognitive load affected by suitable tracking hand gestures. The exergame also requires additional cues to ease the experience and maintain users within a state flow.
SP  - 1
EP  - 8
JF  - 2020 IEEE 8th International Conference on Serious Games and Applications for Health (SeGAH)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/segah49190.2020.9201747
ER  - 

TY  - NA
AU  - Hamdan, Nur Al-huda; Wagner, Adrian; Voelker, Simon; Steimle, Jürgen; Borchers, Jan
TI  - CHI - Springlets: Expressive, Flexible and Silent On-Skin Tactile Interfaces
PY  - 2019
AB  - We introduce Springlets, expressive, non-vibrating mechanotactile interfaces on the skin. Embedded with shape memory alloy springs, we implement Springlets as thin and flexible stickers to be worn on various body locations, thanks to their silent operation even on the neck and head. We present a technically simple and rapid technique for fabricating a wide range of Springlet interfaces and computer-generated tactile patterns. We developed Springlets for six tactile primitives: pinching, directional stretching, pressing, pulling, dragging, and expanding. A study placing Springlets on the arm and near the head demonstrates Springlets' effectiveness and wearability in both stationary and mobile situations. We explore new interactive experiences in tactile social communication, physical guidance, health interfaces, navigation, and virtual reality gaming, enabled by Springlets' unique and scalable form factor.
SP  - 488
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300718
ER  - 

TY  - NA
AU  - Ogawa, Ryoma; Futami, Kyosuke; Murao, Kazuya
TI  - NasalBreathInput: A Hands-Free Input Method by Nasal Breath Gestures using a Glasses Type Device
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - The 23rd International Conference on Information Integration and Web Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3487664.3487750
ER  - 

TY  - NA
AU  - Li, Yichen; Li, Tianxing; Patel, Ruchir A.; Yang, Xing-Dong; Zhou, Xia
TI  - Self-Powered Gesture Recognition with Ambient Light
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242635
ER  - 

TY  - NA
AU  - Cheng, Yi Fei; Luong, Tiffany; Fender, Andreas Rene; Streli, Paul; Holz, Christian
TI  - ComforTable User Interfaces: Surfaces Reduce Input Error, Time, and Exertion for Tabletop and Mid-air User Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00029
ER  - 

TY  - JOUR
AU  - Xing, Yu; Zhou, Yu; Yan, Xin; Zhao, Haisen; Wenqiang, Liu; Jiang, Jingbo; Lu, Lin
TI  - Shell thickening for extrusion-based ceramics printing
PY  - 2021
AB  - NA
SP  - 160
EP  - 169
JF  - Computers & Graphics
VL  - 97
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.04.031
ER  - 

TY  - NA
AU  - Belo, João; Feit, Anna Maria; Feuchtner, Tiare; Grønbæk, Kaj
TI  - CHI - XRgonomics: Facilitating the Creation of Ergonomic 3D Interfaces
PY  - 2021
AB  - Arm discomfort is a common issue in Cross Reality applications involving prolonged mid-air interaction. Solving this problem is difficult because of the lack of tools and guidelines for 3D user interface design. Therefore, we propose a method to make existing ergonomic metrics available to creators during design by estimating the interaction cost at each reachable position in the user’s environment. We present XRgonomics, a toolkit to visualize the interaction cost and make it available at runtime, allowing creators to identify UI positions that optimize users’ comfort. Two scenarios show how the toolkit can support 3D UI design and dynamic adaptation of UIs based on spatial constraints. We present results from a walkthrough demonstration, which highlight the potential of XRgonomics to make ergonomics metrics accessible during the design and development of 3D UIs. Finally, we discuss how the toolkit may address design goals beyond ergonomics.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445349
ER  - 

TY  - NA
AU  - Vatavu, Radu-Daniel; Bilius, Laura-Bianca
TI  - UIST - GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices
PY  - 2021
AB  - Despite an exciting area with many promises for innovations in wearable interactive systems, research on interaction techniques for smart rings lacks structured knowledge and readily-available resources for designers to systematically attain such innovations. In this work, we conduct a systematic literature review of ring-based gesture input, from which we extract key results and a large set of gesture commands for ring, ring-like, and ring-ready devices. We use these findings to deliver GestuRING, our web-based tool to support design of ring-based gesture input. GestuRING features a searchable gesture-to-function dictionary of 579 records with downloadable numerical data files and an associated YouTube video library. These resources are meant to assist the community in attaining further innovations in ring-based gesture input for interactive systems.
SP  - 710
EP  - 723
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474780
ER  - 

TY  - JOUR
AU  - Adilkhanov, Adilzhan; Rubagotti, Matteo; Kappassov, Zhanat
TI  - Haptic Devices: Wearability-Based Taxonomy and Literature Review
PY  - 2022
AB  - In the last decade, several new haptic devices have been developed, contributing to the definition of more realistic virtual environments. An overview on this topic requires a description of the various technologies employed in building such devices, and of their application domains. This survey describes the current technology underlying haptic devices, based on the concept of &#x201C;wearability level&#x201D;. More than 90 devices, newly developed and described in scientific papers published in the period 2010-2021, are reviewed, which provide either haptic illusions or novel haptic feedback for teleoperation, entertainment, training, education, guidance and notification. As a result, the analyzed systems are divided into grounded, hand-held and wearable devices; the latter are further split into exoskeletons and gloves, finger-worn devices, and arm-worn devices. For the systems in each of these categories, descriptions and tables are provided that analyze their structure, including device mass and employed actuators, their applications, and other characteristics such as type of haptic feedback and tactile illusions. The paper also provides an overview of devices worn in parts of the human body other than arms and hands, and precisely haptic vests, jackets and belts, and haptic devices for head, legs and feet. Based on this analysis, the survey also provides a discussion on research gaps and challenges, and potential future directions.
SP  - 91923
EP  - 91947
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3202986
ER  - 

TY  - NA
AU  - Butkow, Kayla-Jade; Dang, Ting; Ferlini, Andrea; Ma, Dong; Mascolo, Cecilia
TI  - Motion-resilient Heart Rate Monitoring with In-ear Microphones.
PY  - 2021
AB  - With the soaring adoption of in-ear wearables, the research community has started investigating suitable in-ear Heart Rate (HR) detection systems. HR is a key physiological marker of cardiovascular health and physical fitness. Continuous and reliable HR monitoring with wearable devices has therefore gained increasing attention in recent years. Existing HR detection systems in wearables mainly rely on Photoplethysmography (PPG) sensors, however these are notorious for poor performance in the presence of human motion. In this work, leveraging the sound enhancing properties of the occlusion effect, which can be generated by sealing the entrance of the ear canal (something that some existing earphones already do to improve noise cancellation), we investigate for the first time \textit{in-ear audio-based motion-resilient} HR monitoring. This is done by measuring HR-induced sound in the human ear canal with in-ear microphones. Concretely, we develop a novel motion artefact (MA) removal technique based on wavelet transforms, followed by an HR estimation algorithm to extract HR from in-ear audio signals compounded with other activities (e.g., walking, running, and speaking). Unlike existing works, we present a systematic evaluation of our technique under a set of different motion artifacts and while speaking. With data collected from 15 subjects over four activities, we demonstrate that our approach achieves a mean absolute error (MAE) of 0.88$\pm$0.27 BPM, 8.11$\pm$3.89 BPM, 13.79$\pm$5.61 BPM and 7.49$\pm$3.23 BPM for stationary, walking, running and speaking, respectively, opening the door to a new non-invasive and affordable HR monitoring with usable performance for daily activities.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cami, Drini; Matulic, Fabrice; Calland, Richard G.; Vogel, Brian; Vogel, Daniel
TI  - UIST - Unimanual Pen+Touch Input Using Variations of Precision Grip Postures
PY  - 2018
AB  - We introduce a new pen input space by forming postures with the same hand that also grips the pen while writing, drawing, or selecting. The postures contact the multitouch surface around the pen to enable detection without special sensors. A formative study investigates the effectiveness, accuracy, and comfort of 33 candidate postures in controlled tasks. The results indicate a useful subset of postures. Using raw capacitive sensor data captured in the study, a convolutional neural network is trained to recognize 10 postures in real time. This recognizer is used to create application demonstrations for pen-based document annotation and vector drawing. A small usability study shows the approach is feasible.
SP  - 825
EP  - 837
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242652
ER  - 

TY  - NA
AU  - Chen, Yu-Wen; Lin, Wei-Ju; Chen, Yi; Cheng, Lung-Pan
TI  - UIST - PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables
PY  - 2021
AB  - We present PneuSeries, a series of modularized inflatables where their inflation and deflation are propagated in-between stage by stage to form various shapes. The key component of PneuSeries is the bidirectional check valve that passively regulates the air flowing in/out from/to adjacent inflatables, allowing each of the inflatables to be inflated/deflated one by one through serial propagation. The form of the inflatable series thus is programmed by the sequential operations of a pump that push/pull the air in/out. In this paper, we explored the design of PneuSeries and implemented working prototypes as a proof of concept. In particular, we built PneuSeries with (1) modularized cubical, cuboidal, tetrahedral, prismatic, and custom inflatables to examine their shape forming, (2) fast assembly connectors to allow quick reconfiguration of the series, and (3) folding mechanism to reduce irregularity of the shrunken inflatables. We also evaluated the inflating and deflating time and the flow rate of the valve for simulating the inflating and deflating process and display the steps and time required to transform in our software. Finally, we demonstrate example objects that show the capability of PneuSeries and its potential applications.
SP  - 431
EP  - 440
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474760
ER  - 

TY  - JOUR
AU  - Ramasamy, Priyanka; Calderon-Sastre, Enrique; Renganathan, Gunarajulu; Das, Swagata; Kurita, Yuichi
TI  - Soft actuators-based skill training wearables: a review on the interaction modes, feedback types, VR scenarios, sensors utilization and applications
PY  - 2023
AB  - <jats:title>Abstract</jats:title><jats:p>Dexterity training helps improve our motor skills while engaging in precision tasks such as surgery in the medical field and playing musical instruments. In addition, post-stroke recovery also requires extensive dexterity training to recover the original motor skills associated with the affected portion of the body. Recent years have seen a rise in the usage of soft-type actuators to perform such training, giving higher levels of comfort, compliance, portability, and adaptability. Their capabilities of performing high dexterity and safety enhancement make them specific biomedical applications and serve as a sensitive tools for physical interaction. The scope of this article discusses the soft actuator types, characterization, sensing, and control based on the interaction modes and the 5 most relevant articles that touch upon the skill improvement models and interfacing nature of the task and the precision it demands. This review attempts to report the latest developments that prioritize soft materials over hard interfaces for dexterity training and prospects of end-user satisfaction.</jats:p>
SP  - NA
EP  - NA
JF  - ROBOMECH Journal
VL  - 10
IS  - 1
PB  - 
DO  - 10.1186/s40648-023-00239-x
ER  - 

TY  - JOUR
AU  - Hasler, Jennifer
TI  - Large-Scale Field-Programmable Analog Arrays
PY  - 2020
AB  - Large-scale field-programmable analog array (FPAA) devices could enable ubiquitous analog or mixed-signal low-power sensor to processing devices similar to the ubiquitous implementation of the existing field-programmable gate array (FPGA) devices. Design tools enable high-level synthesis to gate/transistor design targeting today’s FPGA devices and the opportunity for analog or mixed-signal applications with FPAA devices. This discussion will illustrate the FPAA concepts and FPAA history. The development of FPAAs enables the development of multiple potential metrics, and these metrics illustrate future FPAA device directions. The system-on-chip (SoC) FPAA devices illustrate the IC capabilities, computation, tools, and resulting hardware infrastructure. SoC FPAA device generation has enabled analog computing with levels of abstraction for application design.
SP  - 1283
EP  - 1302
JF  - Proceedings of the IEEE
VL  - 108
IS  - 8
PB  - 
DO  - 10.1109/jproc.2019.2950173
ER  - 

TY  - NA
AU  - Hung, Ching-Wen; Chang, Ruei-Che; Chen, Hong-Sheng; Liang, Chung Han; Chan, Liwei; Chen, Bing-Yu
TI  - Puppeteer: Exploring Intuitive Hand Gestures and Upper-Body Postures for Manipulating Human Avatar Actions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565609
ER  - 

TY  - CHAP
AU  - Matulic, Fabrice; Vogel, Daniel
TI  - Deep Learning-Based Hand Posture Recognition for Pen Interaction Enhancement
PY  - 2021
AB  - This chapter examines how digital pen interaction can be expanded by detecting different hand postures formed primarily by the hand while it grips the pen. Three systems using different types of sensors are considered: an EMG armband, the raw capacitive image of the touchscreen, and a pen-top fisheye camera. In each case, deep neural networks are used to perform classification or regression to detect hand postures and gestures. Additional analyses are provided to demonstrate the benefit of deep learning over conventional machine-learning methods, as well as explore the impact on model accuracy resulting from the number of postures to be recognised, user-dependent versus user-independent models, and the amount of training data. Examples of posture-based pen interaction in applications are discussed and a number of usability aspects resulting from user evaluations are identified. The chapter concludes with perspectives on the recognition and design of posture-based pen interaction for future systems.
SP  - 193
EP  - 225
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_7
ER  - 

TY  - JOUR
AU  - Hoppe, Adrian Heinrich; van de Camp, Florian; Stiefelhagen, Rainer
TI  - ShiSha: Enabling Shared Perspective With Face-to-Face Collaboration Using Redirected Avatars in Virtual Reality
PY  - 2021
AB  - The importance of remote collaboration grows in an interconnected world as the reasons to avoid travel increase. The spatial rendering and collaboration capabilities of virtual and augmented reality systems are well suited for tasks such as support or training. Users can take a shared perspective to build a common understanding. Also, users may engage in face-to-face cooperation to support interpersonal communication. However, a shared perspective and face-to-face collaboration are both desirable but naturally exclude each other. We place all users at the same location to provide a shared perspective. To avoid overlapping body parts, the avatars of the other connected users are shifted to the side. A redirected body pose modification corrects the resulting inconsistencies. The implemented system is compared to a baseline of two users standing in the same location and working with overlapping avatars. The results of a user study show that the proposed modifications provide an easy to use, efficient collaboration and yield higher co-presence and the feeling of teamwork. Applying redirection techniques to other users opens up novel ways to increase social presence for local or remote collaboration.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW3
PB  - 
DO  - 10.1145/3432950
ER  - 

TY  - CHAP
AU  - , 
TI  - Appendices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544579
ER  - 

TY  - CHAP
AU  - , 
TI  - Remarks
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544567
ER  - 

TY  - CHAP
AU  - , 
TI  - Bibliography
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544582
ER  - 

TY  - NA
AU  - Tejada, Carlos E.; Ramakers, Raf; Boring, Sebastian; Ashbrook, Daniel
TI  - CHI - AirTouch: 3D-printed Touch-Sensitive Objects Using Pneumatic Sensing
PY  - 2020
AB  - 3D printing technology can be used to rapidly prototype the look and feel of 3D objects. However, the objects produced are passive. There has been increasing interest in making these objects interactive, yet they often require assembling components or complex calibration. In this paper, we contribute AirTouch, a technique that enables designers to fabricate touch-sensitive objects with minimal assembly and calibration using pneumatic sensing. AirTouch-enabled objects are 3D printed as a single structure using a consumer-level 3D printer. AirTouch uses pre-trained machine learning models to identify interactions with fabricated objects, meaning that there is no calibration required once the object has completed printing. We evaluate our technique using fabricated objects with various geometries and touch sensitive locations, obtaining accuracies of at least 90% with 12 interactive locations.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376136
ER  - 

TY  - BOOK
AU  - Fang, Xinrui; Xia, Chengshuo; Sugiura, Yuta
TI  - AsianCHI@CHI - FacialPen: Using Facial Detection to Augment Pen-Based Interaction
PY  - 2021
AB  - Pen-based interactions have been ubiquitously adopted on mobile and stationary devices, but the usability can be further augmented through the use of advanced techniques. In this work, we propose FacialPen, a prototype that uses facial gestures to trigger commands for pen-based manipulation. In our prototype, a fisheye camera is mounted to the end of a stylus that provides a broad view from which to capture the human face. We facilitated an elicitation study to identify natural and user-defined gestures for interactions with facial expressions. Different gestures can be further discerned via face detection and a classification pipeline. We designed a sketching demonstration application to explore usage scenarios and evaluated the effectiveness of FacialPen through a qualitative study. The user study posits that FacialPen supports efficiency by reducing screen widgets, enabling the continuity of creation work and liberating the user’s stylus holding postures when switching sketch functions.
SP  - 1
EP  - 8
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3467672
ER  - 

TY  - NA
AU  - McGrath, William; Warner, Jeremy; Karchemsky, Mitchell; Head, Andrew; Drew, Daniel S.; Hartmann, Bjoern
TI  - UIST - WiFröst: Bridging the Information Gap for Debugging of Networked Embedded Systems
PY  - 2018
AB  - The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFrost, a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFrost automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.
SP  - 447
EP  - 455
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242668
ER  - 

TY  - JOUR
AU  - Kraemer, Christopher; Guo, Amy; Ahmed, Saad; Hester, Josiah
TI  - Battery-free MakeCode
PY  - 2022
AB  - <jats:p>Hands-on computing has emerged as an exciting and accessible way to learn about computing and engineering in the physical world for students and makers of all ages. Current end-to-end approaches like Microsoft MakeCode require tethered or battery-powered devices like a micro:bit, limiting usefulness and applicability, as well as abdicating responsibility for teaching sustainable practices. Unfortunately, energy harvesting computing devices are usually only programmable by experts and require significant supporting toolchains and knowledge across multiple engineering and computing disciplines to work effectively. This paper bridges the gap between sustainable computing efforts, the maker movement, and novice-focused programming environments with MakeCode-Iceberg, a set of compiler extensions to Microsoft's open-source MakeCode project. The extensions automatically and invisibly transform user code in any language supported (Blocks, JavaScript, Python)into a version that can safely and correctly execute across intermittent power failures caused by unreliable energy harvesting. Determining where, when, and what to save in a checkpoint on limited energy, time, and hardware budget is challenging. We leverage the unique intermediate representation of the MakeCode source-to-source compiler to design and deploy various checkpointing techniques. Our approach allows us to provide, for the first time, a fully web-based and toolchain-free environment to program intermittent computing devices, making battery-free operation accessible to all. We demonstrate new use cases with multiple energy harvesters, peripherals, and application domains: including a Smart Terrarium, Step Counter, and Combination Lock. MakeCode-Iceberg provides sustainable hands-on computing opportunities to a broad audience of makers and learners, democratizing access to energy harvesting and battery-free embedded systems.</jats:p>
SP  - 1
EP  - 35
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 1
PB  - 
DO  - 10.1145/3517236
ER  - 

TY  - NA
AU  - Ikkala, Aleksi; Fischer, Florian; Klar, Markus; Bachinski, Miroslav; Fleig, Arthur; Howes, Andrew; Hämäläinen, Perttu; Müller, Jörg; Murray-Smith, Roderick; Oulasvirta, Antti
TI  - Breathing Life Into Biomechanical User Models
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545689
ER  - 

TY  - NA
AU  - Alghofaili, Rawan; Solah, Michael S; Huang, Haikun
TI  - VR - Optimizing Visual Element Placement via Visual Attention Analysis
PY  - 2019
AB  - Eye-tracking enables researchers to conduct complex analysis on human behavior. With the recent introduction of eye-tracking into consumer-grade virtual reality headsets, the barrier of entry to visual attention analysis in virtual environments has been lowered significantly. Whether for arranging artwork in a virtual museum, posting banners for virtual events or placing advertisements in virtual worlds, analyzing visual attention patterns provides a powerful means for guiding visual element placement. In this work, we propose a novel data-driven optimization approach for automatically analyzing visual attention and placing visual elements in 3D virtual environments. Using an eye-tracking virtual reality headset, we collect eye-tracking data which we use to train a regression model for predicting gaze duration. We then use the predicted gaze duration output of our regressors to optimize the placement of visual elements with respect to certain visual attention and design goals. Through experiments in several virtual environments, we demonstrate the effectiveness of our optimization approach for predicting gaze duration and for placing visual elements in different practical scenarios. Our approach is implemented as a useful plug-in that level designers can use to automatically populate visual elements in 3D virtual environments.
SP  - 464
EP  - 473
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797816
ER  - 

TY  - JOUR
AU  - Liu, Haoyan; Panahi, Atiyehsadat; Andrews, David; Nelson, Alexander
TI  - An FPGA-Based Upper-Limb Rehabilitation Device for Gesture Recognition and Motion Evaluation Using Multi-Task Recurrent Neural Networks
PY  - 2022
AB  - Upper-Extremity motor impairment affects millions of Americans due to cerebrovascular incidents, spinal cord injuries, or brain trauma. Current therapy practices used to assist these individuals in regaining motor functionality often require extensive time at rehabilitation facilities with potentially prohibitive travel or financial costs. This work presents a mobile low-cost field programmable gate array (FPGA)-smart rehabilitation system that can be used in home environments. The prototype is a rehabilitation table instrumented with a capacitive sensor array (CSA) to track upper-extremity motions of the user through proximity or touch. In addition, inertial measurement units (IMUs) are placed on the affected upper limb and combined with the CSA data with our sensor fusion signal processing architecture. Motions are classified and evaluated using multi-task convolutional recurrent neural networks with three additional motion quality output classes to personalize recognition based on the particular motor skills of each patient. The prototype achieves above 99&#x0025; accuracy with 32-bit fixed-point format implementation for recognizing dynamic motions and identifying unnatural characteristics (i.e., tremor or limited flexion and extension) in upper limb motions based on sensor values. The convolutional recurrent neural network (C-RNN) fusion classification network is implemented on a 200 MHz Zynq ZCU104 FPGA using an HLS-based design optimized with pipelining and parallelism techniques and achieves 5.4x speedup compared to ARM&#x00AE; Cortex-A53 implementation running at an operating frequency of 1.3 GHz. The prototype is also demonstrated to perform the machine learning classification in real-time.
SP  - 3605
EP  - 3615
JF  - IEEE Sensors Journal
VL  - 22
IS  - 4
PB  - 
DO  - 10.1109/jsen.2022.3141659
ER  - 

TY  - NA
AU  - Chen, Tuochao; Steeper, Benjamin; Alsheikh, Kinan; Tao, Songyun; Guimbretière, François; Zhang, Cheng
TI  - UIST - C-Face: Continuously Reconstructing Facial Expressions by Deep Learning Contours of the Face with Ear-mounted Miniature Cameras
PY  - 2020
AB  - C-Face (Contour-Face) is an ear-mounted wearable sensing technology that uses two miniature cameras to continuously reconstruct facial expressions by deep learning contours of the face. When facial muscles move, the contours of the face change from the point of view of the ear-mounted cameras. These subtle changes are fed into a deep learning model which continuously outputs 42 facial feature points representing the shapes and positions of the mouth, eyes and eyebrows. To evaluate C-Face, we embedded our technology into headphones and earphones. We conducted a user study with nine participants. In this study, we compared the output of our system to the feature points outputted by a state of the art computer vision library (Dlib) from a font facing camera. We found that the mean error of all 42 feature points was 0.77 mm for earphones and 0.74 mm for headphones. The mean error for 20 major feature points capturing the most active areas of the face was 1.43 mm for earphones and 1.39 mm for headphones. The ability to continuously reconstruct facial expressions introduces new opportunities in a variety of applications. As a demonstration, we implemented and evaluated C-Face for two applications: facial expression detection (outputting emojis) and silent speech recognition. We further discuss the opportunities and challenges of deploying C-Face in real-world applications.
SP  - 112
EP  - 125
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415879
ER  - 

TY  - NA
AU  - Chen, Yan
TI  - On-Demand Collaboration in Programming
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Pezent, Evan; Agarwal, Priyanshu; Hartcher-OrBrien, Jessica; Colonnese, Nicholas; O'Malley, Marcia K.
TI  - Design, Control, and Psychophysics of Tasbi: A Force-Controlled Multimodal Haptic Bracelet
PY  - 2022
AB  - Haptic feedback is known to enhance the realism of an individual’s interactions with objects in virtual environments. Wearable haptic devices, such as vibrotactile sleeves or armbands, can provide haptic feedback in a smaller and more lightweight form factor than haptic gloves that can be bulky and cumbersome to the wearer. In this article, we present tactile and squeeze bracelet interface (Tasbi), a multimodal haptic wristband that can provide radial squeeze forces around the wrist along with vibrotactile feedback at six discrete locations around the band. Tasbi implements a squeezing mechanism that minimizes tangential forces between the band’s points of contact with the skin, instead of focusing the motor actuation to predominantly normal forces. Force sensing capacitors enable closed-loop control of the squeeze force, while vibration is achieved with linear resonant actuators. A detailed description of the design and experimental results demonstrating closed-loop control of squeeze cues provided by Tasbi is presented. Additionally, we present the results of psychophysical experiments that quantify user perception of the vibration and squeeze cues, including vibrotactile identification accuracy in the presence of varying squeeze forces, discrimination thresholds for the squeeze force, and an analysis of user preferences for squeeze actuation magnitudes.
SP  - 2962
EP  - 2978
JF  - IEEE Transactions on Robotics
VL  - 38
IS  - 5
PB  - 
DO  - 10.1109/tro.2022.3164840
ER  - 

TY  - NA
AU  - Chen, Victor; Xu, Xuhai; Li, Richard; Shi, Yuanchun; Patel, Shwetak N.; Wang, Yuntao
TI  - Understanding the Design Space of Mouth Microgestures
PY  - 2021
AB  - As wearable devices move toward the face (i.e. smart earbuds, glasses), there is an increasing need to facilitate intuitive interactions with these devices. Current sensing techniques can already detect many mouth-based gestures; however, users' preferences of these gestures are not fully understood. In this paper, we investigate the design space and usability of mouth-based microgestures. We first conducted brainstorming sessions (N=16) and compiled an extensive set of 86 user-defined gestures. Then, with an online survey (N=50), we assessed the physical and mental demand of our gesture set and identified a subset of 14 gestures that can be performed easily and naturally. Finally, we conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to various daily smartphone operations under a sitting and walking context. From these studies, we develop a taxonomy for mouth gestures, finalize a practical gesture set for common applications, and provide design guidelines for future mouth-based gesture interactions.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Park, Soya; Zhang, Amy X.; Murray, Luke; Karger, David R.
TI  - CHI - Opportunities for Automating Email Processing: A Need-Finding Study
PY  - 2019
AB  - Email management consumes significant effort from senders and recipients. Some of this work might be automatable. We performed a mixed-methods need-finding study to learn: (i) what sort of automatic email handling users want, and (ii) what kinds of information and computation are needed to support that automation. Our investigation included a design workshop to identify categories of needs, a survey to better understand those categories, and a classification of existing email automation software to determine which needs have been addressed. Our results highlight the need for: a richer data model for rules, more ways to manage attention, leveraging internal and external email context, complex processing such as response aggregation, and affordances for senders. To further investigate our findings, we developed a platform for authoring small scripts over a user's inbox. Of the automations found in our studies, half are impossible in popular email clients, motivating new design directions.
SP  - 374
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300604
ER  - 

TY  - NA
AU  - Gupta, Aakar
TI  - Extended Hand Attributes for Touch Input, Touch Output and Touchless Interaction
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Futami, Kyosuke; Oyama, Kohei; Murao, Kazuya
TI  - A Method to Recognize Facial Gesture using Infrared Distance Sensor Array on Ear Accessories
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - The 23rd International Conference on Information Integration and Web Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3487664.3487761
ER  - 

TY  - CHAP
AU  - , 
TI  - Index
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544584
ER  - 

TY  - NA
AU  - Tseng, Wen-Jie; Bonnail, Elise; McGill, Mark; Khamis, Mohamed; Lecolinet, Eric; Huron, Samuel; Gugenheimer, Jan
TI  - The Dark Side of Perceptual Manipulations in Virtual Reality
PY  - 2022
AB  - "Virtual-Physical Perceptual Manipulations" (VPPMs) such as redirected walking and haptics expand the user's capacity to interact with Virtual Reality (VR) beyond what would ordinarily physically be possible. VPPMs leverage knowledge of the limits of human perception to effect changes in the user's physical movements, becoming able to (perceptibly and imperceptibly) nudge their physical actions to enhance interactivity in VR. We explore the risks posed by the malicious use of VPPMs. First, we define, conceptualize and demonstrate the existence of VPPMs. Next, using speculative design workshops, we explore and characterize the threats/risks posed, proposing mitigations and preventative recommendations against the malicious use of VPPMs. Finally, we implement two sample applications to demonstrate how existing VPPMs could be trivially subverted to create the potential for physical harm. This paper aims to raise awareness that the current way we apply and publish VPPMs can lead to malicious exploits of our perceptual vulnerabilities.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517728
ER  - 

TY  - NA
AU  - Matulic, Fabrice; Arakawa, Riku; Vogel, Brian; Vogel, Daniel
TI  - CHI - PenSight: Enhanced Interaction with a Pen-Top Camera
PY  - 2020
AB  - We propose mounting a downward-facing camera above the top end of a digital tablet pen. This creates a unique and practical viewing angle for capturing the pen-holding hand and the immediate surroundings which can include the other hand. The fabrication of a prototype device is described and the enabled interaction design space is explored, including dominant and non-dominant hand pose recognition, tablet grip detection, hand gestures, capturing physical content in the environment, and detecting users and pens. A deep learning computer vision pipeline is developed for classification, regression, and keypoint detection to enable these interactions. Example applications demonstrate usage scenarios and a qualitative user evaluation confirms the potential of the approach.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376147
ER  - 

TY  - NA
AU  - Kandler, Alexandra
TI  - Interactive Technology for People with Colour Vision Deficiency – Translating Colours into Haptic Feedback
PY  - 2018
AB  - Ten percent of all males live with a colour vision deficiency, meaning their visual perception of colour is limited or absent, causing challenges in their daily lives. To find out how, and to which ...
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kim, Daehwa; Harrison, Chris
TI  - EtherPose: Continuous Hand Pose Tracking with Wrist-Worn Antenna Impedance Characteristic Sensing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545665
ER  - 

TY  - NA
AU  - Ban, Reigo; Matsumoto, Keigo; Narumi, Takuji; Kuzuoka, Hideaki
TI  - Wormholes in VR: Teleporting Hands for Flexible Passive Haptics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00093
ER  - 

TY  - NA
AU  - Franceschini, Stefano; Ambrosanio, Michele; Vitale, S.; Baselice, Fabio; Gifuni, Angelo; Grassini, G.; Pascazio, Vito
TI  - Hand Gesture Recognition via Radar Sensors and Convolutional Neural Networks
PY  - 2020
AB  - In this communication, a low-cost radar-sensor-based apparatus for contactless hand gesture recognition via Doppler signature analysis is proposed. The raw reflected signal, after some pre-processing, is analysed via its time-frequency representation, known as spectrogram. This information is then exploited to train a convolutional neural network (CNN) to perform the classification step. The whole procedure was tested on an in-house experimental data set composed of four different hand gestures, showing good performance and reaching an accuracy of approximately 97%. Finally, the classification performance was tested also in a cluttered environment which includes the presence of a strong echo close to the target.
SP  - NA
EP  - NA
JF  - 2020 IEEE Radar Conference (RadarConf20)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/radarconf2043947.2020.9266565
ER  - 

TY  - CHAP
AU  - , 
TI  - Evaluating TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544576
ER  - 

TY  - JOUR
AU  - Sharma, Adwait; Salchow-Hömmen, Christina; Mollyn, Vimal Suresh; Nittala, Aditya Shekhar; Hedderich, Michael A.; Koelle, Marion; Seel, Thomas; Steimle, Jürgen
TI  - SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures
PY  - 2022
AB  - <jats:p> Gestural interaction with freehands and while grasping an everyday object enables <jats:italic>always-available input</jats:italic> . To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present <jats:italic>SparseIMU</jats:italic> , a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios. </jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569894
ER  - 

TY  - NA
AU  - Yasu, Kentaro
TI  - MagneShape: A Non-electrical Pin-Based Shape-Changing Display
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545645
ER  - 

TY  - JOUR
AU  - Wei, Haowen; Li, Ziheng; Galvan, Alexander D.; Su, Zhuoran; Zhang, Xiao; Pahlavan, Kaveh; Solovey, Erin T.
TI  - IndexPen
PY  - 2022
AB  - <jats:p>In this paper, we introduce IndexPen, a novel interaction technique for text input through two-finger in-air micro-gestures, enabling touch-free, effortless, tracking-based interaction, designed to mirror real-world writing. Our system is based on millimeter-wave radar sensing, and does not require instrumentation on the user. IndexPen can successfully identify 30 distinct gestures, representing the letters A-Z, as well as Space, Backspace, Enter, and a special Activation gesture to prevent unintentional input. Additionally, we include a noise class to differentiate gesture and non-gesture noise. We present our system design, including the radio frequency (RF) processing pipeline, classification model, and real-time detection algorithms. We further demonstrate our proof-of-concept system with data collected over ten days with five participants yielding 95.89% cross-validation accuracy on 31 classes (including noise). Moreover, we explore the learnability and adaptability of our system for real-world text input with 16 participants who are first-time users to IndexPen over five sessions. After each session, the pre-trained model from the previous five-user study is calibrated on the data collected so far for a new user through transfer learning. The F-1 score showed an average increase of 9.14% per session with the calibration, reaching an average of 88.3% on the last session across the 16 users. Meanwhile, we show that the users can type sentences with IndexPen at 86.2% accuracy, measured by string similarity. This work builds a foundation and vision for future interaction interfaces that could be enabled with this paradigm.</jats:p>
SP  - 1
EP  - 39
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534601
ER  - 

TY  - CHAP
AU  - , 
TI  - Introduction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544568
ER  - 

TY  - CHAP
AU  - , 
TI  - Foreword
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544566
ER  - 

TY  - JOUR
AU  - Lacanlale, Jonathan; Isayan, Paruyr; Mkrtchyan, Katya; Nahapetian, Ani
TI  - Sensoring the Neck: Classifying Movements and Actions with a Neck-Mounted Wearable Device.
PY  - 2022
AB  - Sensor technology that captures information from a user's neck region can enable a range of new possibilities, including less intrusive mobile software interfaces. In this work, we investigate the feasibility of using a single inexpensive flex sensor mounted at the neck to capture information about head gestures, about mouth movements, and about the presence of audible speech. Different sensor sizes and various sensor positions on the neck are experimentally evaluated. With data collected from experiments performed on the finalized prototype, a classification accuracy of 91% in differentiating common head gestures, a classification accuracy of 63% in differentiating mouth movements, and a classification accuracy of 83% in speech detection are achieved.
SP  - 4313
EP  - 4313
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 12
PB  - 
DO  - 10.3390/s22124313
ER  - 

TY  - CHAP
AU  - Onorati, Teresa; Muñoz, Clara Blanco; Díaz, Paloma; Aedo, Ignacio
TI  - Exploring the Affordances of Immersive Visualization Spaces: A Use Case About COVID-19
PY  - 2022
AB  - NA
SP  - 252
EP  - 263
JF  - Proceedings of the International Conference on Ubiquitous Computing & Ambient Intelligence (UCAmI 2022)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-21333-5_25
ER  - 

TY  - JOUR
AU  - Wei, Dong; Huang, Bidan; Li, Qiang
TI  - Multi-View Merging for Robot Teleoperation With Virtual Reality
PY  - 2021
AB  - In robotic teleoperation, an operator usually needs to be trained for long hours. One of the factors leading to this steep learning curve is the quality of telepresence. This letter proposes a novel and user-friendly telepresence interface for manipulation. Visual information from different views is merged and presented to the operator in an intuitive way to facilitate the task based on state-of-the-art virtual reality technology. Besides rendering the scene, a virtual robot is also rendered in the immersive view so that the robot is visible even when it is occluded. We performed a series of user studies to evaluate this interface. The results show that the proposed interface can achieve a better task performance compared to a standard approach in both the manipulation efficiency and the users’ preferences.
SP  - 8537
EP  - 8544
JF  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 4
PB  - 
DO  - 10.1109/lra.2021.3109348
ER  - 

TY  - JOUR
AU  - Danyluk, Kurtis Thorvald; Ulusoy, Teoman Tomo; Wei, Wei; Willett, Wesley
TI  - Touch and Beyond:Comparing Physical and Virtual Reality Visualizations.
PY  - 2022
AB  - We compare physical and virtual reality (VR) versions of simple data visualizations. We also explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examined differences in how viewers interact with physical hand-scale, virtual hand-scale,and virtual table-scale visualizations and the impact that the different forms had on viewer's problem solving behavior. A second study examined how interactive annotation and filtering tools might sup-port new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 4
PB  - 
DO  - 10.1109/tvcg.2020.3023336
ER  - 

TY  - JOUR
AU  - Yang, Yalong; Dwyer, Tim; Marriott, Kim; Jenny, Bernhard; Goodwin, Sarah
TI  - Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and Bar Chart in Immersive Environments
PY  - 2021
AB  - We introduce Tilt Map , a novel interaction technique for intuitively transitioning between 2D and 3D map visualisations in immersive environments. Our focus is visualising data associated with areal features on maps, for example, population density by state. Tilt Map transitions from 2D choropleth maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our article includes two user studies. The first study compares subjects’ task performance interpreting population density data using 2D choropleth maps and 3D prism maps in virtual reality (VR). We observed greater task accuracy with prism maps, but faster response times with choropleth maps. The complementarity of these views inspired our hybrid Tilt Map design. Our second study compares Tilt Map to: a side-by-side arrangement of the various views; and interactive toggling between views. The results indicate benefits for Tilt Map in user preference; and accuracy (versus side-by-side) and time (versus toggle).
SP  - 4507
EP  - 4519
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2020.3004137
ER  - 

TY  - NA
AU  - Zhang, Peiying; Li, Chenhui; Wang, Changbo
TI  - ICME - Smarttext: Learning To Generate Harmonious Textual Layout Over Natural Image
PY  - 2020
AB  - Automatic typography is important because it helps designers avoid highly repetitive tasks and amateur users achieve high-quality textual layout designs. However, there are often many parameters that need to be adjusted in automatic typography work. In this paper, we propose an efficient content-aware learning-based framework to generate harmonious textual layout over natural image. Our method incorporates both semantic features and visual perception principles. First, we combine a semantic visual saliency detection network with diffusion equations and a text-region proposal algorithm to generate candidate text anchors with various positions and sizes. Second, we develop a deep scoring network to assess the aesthetic quality of the candidate results. We design multiple evaluations to compare our method with several baselines and a commercial poster design tool. The results demonstrate that our method can generate harmonious textual layout in various actual scenarios with better performance.
SP  - 1
EP  - 6
JF  - 2020 IEEE International Conference on Multimedia and Expo (ICME)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icme46284.2020.9102780
ER  - 

TY  - JOUR
AU  - Kaluarachchi, Tharindu; Reis, Andrew; Nanayakkara, Suranga
TI  - A Review of Recent Deep Learning Approaches in Human-Centered Machine Learning.
PY  - 2021
AB  - After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning (HCML). We present this review paper as an overview and analysis of existing work in HCML related to DL. Firstly, we collaborated with field domain experts to develop a working definition for HCML. Secondly, through a systematic literature review, we analyze and classify 162 publications that fall within HCML. Our classification is based on aspects including contribution type, application area, and focused human categories. Finally, we analyze the topology of the HCML landscape by identifying research gaps, highlighting conflicting interpretations, addressing current challenges, and presenting future HCML research opportunities.
SP  - 2514
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 21
IS  - 7
PB  - 
DO  - 10.3390/s21072514
ER  - 

TY  - NA
AU  - Madan, Spandan; Bylinskii, Zoya; Nobre, Carolina; Tancik, Matthew; Recasens, Adrià; Zhong, Kimberli; Alsheikh, Sami; Oliva, Aude; Durand, Frédo; Pfister, Hanspeter
TI  - PacificVis - Parsing and Summarizing Infographics with Synthetically Trained Icon Detection
PY  - 2021
AB  - Widely used in news, business, and educational media, infographics are handcrafted to effectively communicate messages about complex and often abstract topics including ‘ways to conserve the environment’ and ‘coronavirus prevention’. The computational understanding of infographics required for future applications like automatic captioning, summarization, search, and question-answering, will depend on being able to parse the visual and textual elements contained within. However, being composed of stylistically and semantically diverse visual and textual elements, infographics pose challenges for current A.I. systems. While automatic text extraction works reasonably well on infographics, standard object detection algorithms fail to identify the stand-alone visual elements in infographics that we refer to as ‘icons’. In this paper, we propose a novel approach to train an object detector using synthetically-generated data, and show that it succeeds at generalizing to detecting icons within in-the-wild infographics. We further pair our icon detection approach with an icon classifier and a state-of-the-art text detector to demonstrate three demo applications: topic prediction, multi-modal summarization, and multi-modal search. Parsing the visual and textual elements within infographics provides us with the first steps towards automatic infographic understanding.
SP  - 31
EP  - 40
JF  - 2021 IEEE 14th Pacific Visualization Symposium (PacificVis)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/pacificvis52677.2021.00012
ER  - 

TY  - NA
AU  - Troost, Ivar; Tanhaei, Ghazaleh; Hardman, Lynda; Hürst, Wolfgang
TI  - Conference on Designing Interactive Systems - Exploring Relations in Neuroscientific Literature using Augmented Reality: A Design Study
PY  - 2021
AB  - To support scientists in maintaining an overview of disciplinary concepts and their interrelations, we investigate whether Augmented Reality can serve as a platform to make automated methods more accessible and integrated into current literature exploration practices. Building on insights from text and immersive analytics, we identify information and design requirements. We embody these in DatAR, a system design and implementation focussed on analysis of co-occurrences in neuroscientific text collections. We conducted a scenario-based video survey with a sample of neuroscientists and other domain experts, focusing on participants’ willingness to adopt such an AR system in their regular literature review practices. The AR-tailored epistemic and representational designs of our system were generally perceived as suitable for performing complex analytics. We also discuss several fundamental issues with our chosen 3D visualisations, making steps towards understanding in which ways AR is a suitable medium for high-level conceptual literature exploration.
SP  - 266
EP  - 274
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462053
ER  - 

TY  - NA
AU  - Satriadi, Kadek Ananta; Smiley, Jim; Ens, Barrett; Cordeil, Maxime; Czauderna, Tobias; Lee, Benjamin; Yang, Ying; Dwyer, Tim; Jenny, Bernhard
TI  - Tangible Globes for Data Visualisation in Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517715
ER  - 

TY  - NA
AU  - Qian, Jing; Young-Ng, Meredith; Li, Xiangyu; Cheung, Angel; Yang, Fumeng; Huang, Jeff
TI  - CHI Extended Abstracts - Portalware: A Smartphone-Wearable Dual-Display System for Expanding the Free-Hand Interaction Region in Augmented Reality
PY  - 2020
AB  - Free-hand manipulation in smartphone augmented reality (AR) enables users to directly interact with virtual contents using their hands. However, human hands can ergonomically move in a broader range than a smartphone's field of view (FOV) can capture, requiring users to be aware of the limited usable interaction and viewing regions at all times. We present Portalware, a smartphone-wearable dual-display system that expands the usable interaction region for free-hand manipulation and enables users to receive visual feedback outside the smartphone's view. The wearable is a lightweight, low-cost display that shares the same AR environment in real-time with the smartphone.This setup empowers AR applications such as mid-air drawing and object manipulation by providing a 180-degree horizontal interaction region in front of the user. Other potential applications include wearing the smartphone like a pendant while using Portalware to continue interacting with AR objects. Without having to hold their phone up with their hand, users can benefit from resting their arms as needed. Finally, we discuss usability explorations, potential interactions, and future plans for empirical studies.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383079
ER  - 

TY  - JOUR
AU  - Bryan, Chris; Mishra, Aditi; Shidara, Hidekazu; Ma, Kwan-Liu
TI  - Analyzing gaze behavior for text-embellished narrative visualizations under different task scenarios
PY  - 2020
AB  - Abstract We conduct an eye tracking study to investigate perception text-embellished narrative visualizations under different task conditions. Study stimuli are data visualizations embellished with text-based elements: annotations, captions, labels, and descriptive text. We consider three common viewing tasks that occur when these types of graphics are viewed: (1) simple observation, (2) active search to answer a query, and (3) information memorization for later recall. The overarching goal is to understand, at a perceptual level, if and how task affects how these visualizations are interacted with. By analyzing collected gaze data and conducting advanced semantic scanpath analysis, we find, at a high level, diverse patterns of gaze behavior: simple observation and information memorization lead to similar optical viewing strategies, while active search significantly diverges, both in regards to which areas of the visualization are focused upon and how often embellishments are interacted with. We discuss study outcomes in the context of embellishing visualizations with text for various usage scenarios.
SP  - 41
EP  - 50
JF  - Visual Informatics
VL  - 4
IS  - 3
PB  - 
DO  - 10.1016/j.visinf.2020.08.001
ER  - 

TY  - JOUR
AU  - He, Qiang; Wu, Yufen; Feng, Zhiping; Sun, Chenchen; Fan, Wenjing; Zhou, Zhihao; Meng, Keyu; Fan, Endong; Yang, Jin
TI  - Triboelectric vibration sensor for a human-machine interface built on ubiquitous surfaces
PY  - 2019
AB  - NA
SP  - 689
EP  - 696
JF  - Nano Energy
VL  - 59
IS  - NA
PB  - 
DO  - 10.1016/j.nanoen.2019.03.005
ER  - 

TY  - JOUR
AU  - Pérez-Medina, Jorge Luis; Villarreal, Santiago; Vanderdonckt, Jean
TI  - A Gesture Elicitation Study of Nose-Based Gestures.
PY  - 2020
AB  - Presently, miniaturized sensors can be embedded in any small-size wearable to recognize movements on some parts of the human body. For example, an electrooculography-based sensor in smart glasses recognizes finger movements on the nose. To explore the interaction capabilities, this paper conducts a gesture elicitation study as a between-subjects experiment involving one group of 12 females and one group of 12 males, expressing their preferred nose-based gestures on 19 Internet-of-Things tasks. Based on classification criteria, the 912 elicited gestures are clustered into 53 unique gestures resulting in 23 categories, to form a taxonomy and a consensus set of 38 final gestures, providing researchers and practitioners with a larger base with six design guidelines. To test whether the measurement method impacts these results, the agreement scores and rates, computed for determining the most agreed gestures upon participants, are compared with the Condorcet and the de Borda count methods to observe that the results remain consistent, sometimes with a slightly different order. To test whether the results are sensitive to gender, inferential statistics suggest that no significant difference exists between males and females for agreement scores and rates.
SP  - 7118
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 24
PB  - 
DO  - 10.3390/s20247118
ER  - 

TY  - JOUR
AU  - Franconeri, Steven L; Padilla, Lace M; Shah, Priti; Zacks, Jeffrey M; Hullman, Jessica
TI  - The Science of Visual Data Communication: What Works.
PY  - 2021
AB  - Effectively designed data visualizations allow viewers to use their powerful visual systems to understand patterns in data across science, education, health, and public policy. But ineffectively designed visualizations can cause confusion, misunderstanding, or even distrust-especially among viewers with low graphical literacy. We review research-backed guidelines for creating effective and intuitive visualizations oriented toward communicating data to students, coworkers, and the general public. We describe how the visual system can quickly extract broad statistics from a display, whereas poorly designed displays can lead to misperceptions and illusions. Extracting global statistics is fast, but comparing between subsets of values is slow. Effective graphics avoid taxing working memory, guide attention, and respect familiar conventions. Data visualizations can play a critical role in teaching and communication, provided that designers tailor those visualizations to their audience.
SP  - 110
EP  - 161
JF  - Psychological science in the public interest : a journal of the American Psychological Society
VL  - 22
IS  - 3
PB  - 
DO  - 10.1177/15291006211051956
ER  - 

TY  - NA
AU  - Laput, Gierad; Harrison, Chris
TI  - CHI - SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences
PY  - 2019
AB  - IoT appliances are gaining consumer traction, from smart thermostats to smart speakers. These devices generally have limited user interfaces, most often small buttons and touchscreens, or rely on voice control. Further, these devices know little about their surroundings unaware of objects, people and activities happening around them. Consequently, interactions with these "smart" devices can be cumbersome and limited. We describe SurfaceSight, an approach that enriches IoT experiences with rich touch and object sensing, offering a complementary input channel and increased contextual awareness. For sensing, we incorporate LIDAR into the base of IoT devices, providing an expansive, ad hoc plane of sensing just above the surface on which devices rest. We can recognize and track a wide array of objects, including finger input and hand gestures. We can also track people and estimate which way they are facing. We evaluate the accuracy of these new capabilities and illustrate how they can be used to power novel and contextually-aware interactive experiences.
SP  - 329
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300559
ER  - 

TY  - JOUR
AU  - Patnaik, Biswaksen; Batch, Andrea; Elmqvist, Niklas
TI  - Information Olfactation: Harnessing Scent to Convey Data
PY  - 2018
AB  - Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present vi S cent : A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.
SP  - 726
EP  - 736
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865237
ER  - 

TY  - JOUR
AU  - Li, Jianan; Yang, Jimei; Hertzmann, Aaron; Zhang, Jianming; Xu, Tingfa
TI  - LayoutGAN: Synthesizing Graphic Layouts With Vector-Wireframe Adversarial Networks
PY  - 2021
AB  - Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements, represented by vectors and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We, thus, propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation, tangram graphic design, mobile app layout design, and webpage layout optimization from hand-drawn sketches.
SP  - 2388
EP  - 2399
JF  - IEEE transactions on pattern analysis and machine intelligence
VL  - 43
IS  - 7
PB  - 
DO  - 10.1109/tpami.2019.2963663
ER  - 

TY  - NA
AU  - Matsumoto, Keigo; Langbehn, Eike; Narumi, Takuji; Steinicke, Frank
TI  - VR - Detection Thresholds for Vertical Gains in VR and Drone-based Telepresence Systems
PY  - 2020
AB  - Several redirected walking techniques have been introduced and analyzed in recent years, while the main focus was on manipulations in horizontal directions, in particular, by means of curvature, rotation, and translation gains. However, less research has been conducted on the manipulation of vertical movements and its possible use as a redirection technique. Actually, vertical movements are fundamentally important, e.g., for remotely steering a drone using a virtual reality headset.In this paper, we explored vertical gains, a novel redirection technique, which enables us to purposefully manipulate the mapping of the user’s physical vertical movements to movements in the virtual space and the remote space. This approach allows natural and more active physical control of a real drone. To demonstrate the usability of vertical gains, we implemented a telepresence drone and vertical redirection techniques for stretching and crouching actions using common VR devices. We conducted two user studies to investigate the effective manipulation ranges and its usability: one study using a virtual environment (VE), and one using a camera stream from a telepresence drone. The results revealed that our technique could manipulate a users vertical movement without her/his noticing.
SP  - 101
EP  - 107
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581262503135
ER  - 

TY  - JOUR
AU  - Kovacs, Balazs; O'Donovan, Peter; Bala, Kavita; Hertzmann, Aaron
TI  - Context-Aware Asset Search for Graphic Design
PY  - 2018
AB  - Graphic design tools provide powerful controls for expert-level design creation, but the options can often be overwhelming for novices. This paper proposes Context-Aware Asset Search tools that take the current state of the user's design into account, thereby providing search and selections that are compatible with the current design and better fit the user's needs. In particular, we focus on image search and color selection, two tasks that are central to design. We learn a model for compatibility of images and colors within a design, using crowdsourced data. We then use the learned model to rank image search results or color suggestions during design. We found counterintuitive behavior using conventional training with pairwise comparisons for image search, where models with and without compatibility performed similarly. We describe a data collection procedure that alleviates this problem. We show that our method outperforms baseline approaches in quantitative evaluation, and we also evaluate a prototype interactive design tool.
SP  - 2419
EP  - 2429
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2018.2842734
ER  - 

TY  - NA
AU  - Miyagawa, Shoki; Koyama, Yuki; Kato, Jun; Goto, Masataka; Morishima, Shigeo
TI  - Conference on Designing Interactive Systems (Companion Volume) - Placing Music in Space: A Study on Music Appreciation with Spatial Mapping
PY  - 2018
AB  - We investigate the potential of music appreciation using spatial mapping techniques, which allow us to "place" audio sources in various locations within a physical space. We consider possible ways of this new appreciation style and list some design variables, such as how to define coordinate systems, how to show visually, and how to place the sound sources. We conducted an exploratory user study to examine how these design variables affect users' music listening experiences. Based on our findings from the study, we discuss how we should develop systems that incorporate these design variables for music appreciation in the future.
SP  - 39
EP  - 43
JF  - Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3197391.3205409
ER  - 

TY  - NA
AU  - Swearngin, Amanda; Li, Yang
TI  - CHI - Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning
PY  - 2019
AB  - Tapping is an immensely important gesture in mobile touchscreen interfaces, yet people still frequently are required to learn which elements are tappable through trial and error. Predicting human behavior for this everyday gesture can help mobile app designers understand an important aspect of the usability of their apps without having to run a user study. In this paper, we present an approach for modeling tappability of mobile interfaces at scale. We conducted large-scale data collection of interface tappability over a rich set of mobile apps using crowdsourcing and computationally investigated a variety of signifiers that people use to distinguish tappable versus not-tappable elements. Based on the dataset, we developed and trained a deep neural network that predicts how likely a user will perceive an interface element as tappable versus not tappable. Using the trained tappability model, we developed TapShoe, a tool that automatically diagnoses mismatches between the tappability of each element as perceived by a human user---predicted by our model, and the intended or actual tappable state of the element specified by the developer or designer. Our model achieved reasonable accuracy: mean precision 90.2% and recall 87.0%, in matching human perception on identifying tappable UI elements. The tappability model and TapShoe were well received by designers via an informal evaluation with 7 professional interaction designers.
SP  - 75
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300305
ER  - 

TY  - JOUR
AU  - Kim, Nam Wook; Bylinskii, Zoya; Borkin, Michelle A.; Gajos, Krzysztof Z.; Oliva, Aude; Durand, Frédo; Pfister, Hanspeter
TI  - BubbleView: an interface for crowdsourcing image importance maps and tracking visual attention
PY  - 2017
AB  - In this paper, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal "bubbles" - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both (i) successfully approximate eye fixations on different images, and (ii) be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types.
SP  - 36
EP  - 40
JF  - ACM Transactions on Computer-Human Interaction
VL  - 24
IS  - 5
PB  - 
DO  - 10.1145/3131275
ER  - 

TY  - NA
AU  - Fosco, Camilo; Casser, Vincent; Bedi, Amish Kumar; O'Donovan, Peter; Hertzmann, Aaron; Bylinskii, Zoya
TI  - UIST - Predicting Visual Importance Across Graphic Design Types
PY  - 2020
AB  - This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance.
SP  - 249
EP  - 260
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415825
ER  - 

TY  - CHAP
AU  - Manandhar, Dipu; Ruta, Dan; Collomosse, John
TI  - ECCV (22) - Learning Structural Similarity of User Interface Layouts Using Graph Networks.
PY  - 2020
AB  - We propose a novel representation learning technique for measuring the similarity of user interface designs. A triplet network is used to learn a search embedding for layout similarity, with a hybrid encoder-decoder backbone comprising a graph convolutional network (GCN) and convolutional decoder (CNN). The properties of interface components and their spatial relationships are encoded via a graph which also models the containment (nesting) relationships of interface components. We supervise the training of a dual reconstruction and pair-wise loss using an auxiliary measure of layout similarity based on intersection over union (IoU) distance. The resulting embedding is shown to exceed state of the art performance for visual search of user interface layouts over the public Rico dataset, and an auto-annotated dataset of interface layouts collected from the web. We release the codes and dataset (https://github.com/dips4717/gcn-cnn.)
SP  - 730
EP  - 746
JF  - Computer Vision – ECCV 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58542-6_44
ER  - 

TY  - NA
AU  - Liu, Sean J.; Agrawala, Maneesh; DiVerdi, Stephen; Hertzmann, Aaron
TI  - UIST - View-Dependent Video Textures for 360° Video
PY  - 2019
AB  - A major concern for filmmakers creating 360° video is ensuring that the viewer does not miss important narrative elements because they are looking in the wrong direction. This paper introduces gated clips which do not play the video past a gate time until a filmmaker-defined viewer gaze condition is met, such as looking at a specific region of interest (ROI). Until the condition is met, we seamlessly loop video playback using view-dependent video textures, a new variant of standard video textures that adapt the looping behavior to the portion of the scene that is within the viewer's field of view. We use our desktop GUI to edit live action and computer animated 360° videos. In a user study with casual viewers, participants prefer our looping videos over the standard versions and are able to successfully see all of the looping videos' ROIs without fear of missing important narrative content.
SP  - 249
EP  - 262
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347887
ER  - 

TY  - NA
AU  - Feick, Martin; Bateman, Scott; Tang, Anthony; Miede, André; Marquardt, Nicolai
TI  - ISMAR - Tangi: Tangible Proxies For Embodied Object Exploration And Manipulation In Virtual Reality
PY  - 2020
AB  - Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi’s Manipulators and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi’s proxies outperform traditional controllers and were generally favored by participants.
SP  - 195
EP  - 206
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00042
ER  - 

TY  - NA
AU  - Parsoya, Arihant; Rajamanickam, Venkatesh
TI  - KeySlide: using directional gestures on keyboard for cursor positioning
PY  - 2019
AB  - KeySlide is a novel interaction technique to position the text cursor in text editors by refashioning standard physical keyboard used in desktop and laptop devices. The technique enables users to position the cursor at the desired location on text documents using natural and intuitive finger-sliding gestures over the keys of the keyboard. The natural mapping of the gesture to the action eliminates the learnability of the action, and minimises the disruption caused due to switching between the keyboard and the mouse. The design, implementation and evaluation of KeySlide is presented. While the theoretical simulation study found that KeySlide is 52% faster than standard keyboard shortcuts for cursor positioning, a comparison user study consisting of 18 participants, found that text editing performance using KeySlide is 19% slower than using keyboard shortcuts. The discrepancy is discussed.
SP  - NA
EP  - NA
JF  - Proceedings of the 10th Indian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3364183.3364186
ER  - 

TY  - JOUR
AU  - Yang, Yalong
TI  - Visualising Geographically-Embedded Origin-Destination Flows: in 2D and immersive environments
PY  - NA
AB  - This thesis develops and evaluates effective techniques for visualisation of flows (e.g. of people, trade, knowledge) between places on geographic maps. This geographically-embedded flow data contains information about geographic locations, and flows from origin locations to destination locations. This thesis explores the design space of OD flow visualisation in both 2D and immersive environments. We do so by creating novel OD flow visualisations in both environments, and then conducting controlled user studies to evaluate different designs.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.26180/5c087e9980d8a
ER  - 

TY  - NA
AU  - Daniel, Maxime; Rivière, Guillaume; Couture, Nadine
TI  - Tangible and Embedded Interaction - CairnFORM: a Shape-Changing Ring Chart Notifying Renewable Energy Availability in Peripheral Locations
PY  - 2019
AB  - We present CairnFORM, a shape-changing cylindrical display that physicalizes forecasts of renewable energy availability. CairnFORM aims at creating and encouraging new socially-shared practices by displaying energy data in collective and public spaces, such as public places and workplaces. It is 360°-readable, and as a dynamic physical ring chart, it can change its cylindrical symmetry with quiet motion. We conducted two user studies. The first study clearly revealed the attractiveness of CairnFORM in a public place and its usability for a range task and for a compare task. Consequently, this makes CairnFORM useful to analyze renewable energy availability. The second study revealed that a non-constant motion speed is the better visualization stimulus at a workplace.
SP  - 275
EP  - 286
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295634
ER  - 

TY  - NA
AU  - Herdel, Viviane; Yamin, Lee J.; Cauchard, Jessica R.
TI  - Above and Beyond: A Scoping Review of Domains and Applications for Human-Drone Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501881
ER  - 

TY  - NA
AU  - Lin, David Chuan-En; Martelaro, Nikolas
TI  - Conference on Designing Interactive Systems - Learning Personal Style from Few Examples
PY  - 2021
AB  - A key task in design work is grasping the client’s implicit tastes. Designers often do this based on a set of examples from the client. However, recognizing a common pattern among many intertwining variables such as color, texture, and layout and synthesizing them into a composite preference can be challenging. In this paper, we leverage the pattern recognition capability of computational models to aid in this task. We offer a set of principles for computationally learning personal style. The principles are manifested in PseudoClient, a deep learning framework that learns a computational model for personal graphic design style from only a handful of examples. In several experiments, we found that PseudoClient achieves a 79.40% accuracy with only five positive and negative examples, outperforming several alternative methods. Finally, we discuss how PseudoClient can be utilized as a building block to support the development of future design applications.
SP  - 1566
EP  - 1578
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462115
ER  - 

TY  - NA
AU  - Ulusoy, Teoman Tomo; Danyluk, Kurtis Thorvald; Willett, Wesley
TI  - Beyond the Physical: Examining Scale and Annotation in Virtual Reality Visualizations
PY  - NA
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.11575/prism/33138
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Lee, Juyoung; Kim, Hyung-il; Gupta, Aakar; Bianchi, Andrea; Vogel, Daniel; Koike, Hideki; Woo, Woontack; Quigley, Aaron
TI  - MobileHCI - WRIST: Watch-Ring Interaction and Sensing Technique for Wrist Gestures and Macro-Micro Pointing
PY  - 2019
AB  - To better explore the incorporation of pointing and gesturing into ubiquitous computing, we introduce WRIST, an interaction and sensing technique that leverages the dexterity of human wrist motion. WRIST employs a sensor fusion approach which combines inertial measurement unit (IMU) data from a smartwatch and a smart ring. The relative orientation difference of the two devices is measured as the wrist rotation that is independent from arm rotation, which is also position and orientation invariant. Employing our test hardware, we demonstrate that WRIST affords and enables a number of novel yet simplistic interaction techniques, such as (i) macro-micro pointing without explicit mode switching and (ii) wrist gesture recognition when the hand is held in different orientations (e.g., raised or lowered). We report on two studies to evaluate the proposed techniques and we present a set of applications that demonstrate the benefits of WRIST. We conclude with a discussion of the limitations and highlight possible future pathways for research in pointing and gesturing with wearable devices.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340130
ER  - 

TY  - NA
AU  - Elahi, Ehsan; Iglesias, Ana; Morato, Jorge
TI  - Web Images Relevance and Quality: User Evaluation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 5th International Conference on Computer Science and Software Engineering (CSSE 2022)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569966.3569984
ER  - 

TY  - NA
AU  - Leen, Danny; Veuskens, Tom; Luyten, Kris; Ramakers, Raf
TI  - CHI - JigFab: Computational Fabrication of Constraints to Facilitate Woodworking with Power Tools
PY  - 2019
AB  - We present JigFab, an integrated end-to-end system that supports casual makers in designing and fabricating constructions with power tools. Starting from a digital version of the construction, JigFab achieves this by generating various types of constraints that configure and physically aid the movement of a power tool. Constraints are generated for every operation and are custom to the work piece. Constraints are laser cut and assembled together with predefined parts to reduce waste. JigFab's constraints are used according to an interactive step-by-step manual. JigFab internalizes all the required domain knowledge for designing and building intricate structures, consisting of various types of finger joints, tenon & mortise joints, grooves, and dowels. Building such structures is normally reserved for artisans or automated with advanced CNC machinery.
SP  - 156
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300386
ER  - 

TY  - JOUR
AU  - Ahmad, Imtiaz; Farzan, Rosta; Kapadia, Apu; Lee, Adam J.
TI  - Tangible Privacy: Towards User-Centric Sensor Designs for Bystander Privacy
PY  - 2020
AB  - Sensor-enabled computers in the form of 'IoT' devices such as home security cameras and voice assistants are increasingly becoming pervasive in our environment. With the embedded cameras and microphones in these devices, this 'invasion' of our everyday spaces can pose significant threats to the privacy of bystanders. Because of their complex functionality, even when people attempt privacy measures (such as asking the owner to "turn the camera off"), these devices may still record information because of the lack of a 'real' off button. With the ambiguities of current designs, a bystander's perceived privacy can diverge from their actual privacy. Indeed, being able to assess one's actual privacy is a key aspect in managing one's privacy according to Altman's theory of boundary regulation, and current designs fall short in assuring people of their privacy. To understand how people as bystanders manage their privacy with IoT devices, we conducted an interview study about people's perceptions of and behaviors around current IoT devices. We find that although participants' behaviors line up with Altman's theory of boundary regulation, in the face of uncertainty about their privacy, they desire or engage in various 'tangible' workarounds. Based on our findings, we identify and introduce the concept of 'tangible privacy' as being essential to boundary regulation with IoT devices. We argue that IoT devices should be designed in a way that clearly and unambiguously conveys sensor states to people around them and make actionable design recommendations to provide strong privacy assurances to bystanders.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW2
PB  - 
DO  - 10.1145/3415187
ER  - 

TY  - JOUR
AU  - Galati, Alexia; Schoppa, Riley; Lu, Aidong
TI  - Exploring the SenseMaking Process through Interactions and fNIRS in Immersive Visualization
PY  - 2021
AB  - Theories of cognition inform our decisions when designing human-computer interfaces, and immersive systems enable us to examine these theories. This work explores the sensemaking process in an immersive environment through studying both internal and external user behaviors with a classical visualization problem: a visual comparison and clustering task. We developed an immersive system to perform a user study, collecting user behavior data from different channels: AR HMD for capturing external user interactions, functional near-infrared spectroscopy (fNIRS) for capturing internal neural sequences, and video for references. To examine sensemaking, we assessed how the layout of the interface (planar 2D vs. cylindrical 3D layout) and the challenge level of the task (low vs. high cognitive load) influenced the users' interactions, how these interactions changed over time, and how they influenced task performance. We also developed a visualization system to explore joint patterns among all the data channels. We found that increased interactions and cerebral hemodynamic responses were associated with more accurate performance, especially on cognitively demanding trials. The layout types did not reliably influence interactions or task performance. We discuss how these findings inform the design and evaluation of immersive systems, predict user performance and interaction, and offer theoretical insights about sensemaking from the perspective of embodied and distributed cognition.
SP  - 2714
EP  - 2724
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067693
ER  - 

TY  - JOUR
AU  - Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim
TI  - Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment
PY  - 2021
AB  - Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA : the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.
SP  - 1171
EP  - 1181
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030450
ER  - 

TY  - NA
AU  - Lu, Min; Wang, Chufeng; Lanir, Joel; Zhao, Nanxuan; Pfister, Hanspeter; Cohen-Or, Daniel; Huang, Hui
TI  - CHI - Exploring Visual Information Flows in Infographics
PY  - 2020
AB  - Infographics are engaging visual representations that tell an informative story using a fusion of data and graphical elements. The large variety of infographic design poses a challenge for their high-level analysis. We use the concept of Visual Information Flow (VIF), which is the underlying semantic structure that links graphical elements to convey the information and story to the user. To explore VIF, we collected a repository of over 13K infographics. We use a deep neural network to identify visual elements related to information, agnostic to their various artistic appearances. We construct the VIF by automatically chaining these visual elements together based on Gestalt principles. Using this analysis, we characterize the VIF design space by a taxonomy of 12 different design patterns. Exploring in a real-world infographic dataset, we discuss the design space and potentials of VIF in light of this taxonomy.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376263
ER  - 

TY  - NA
AU  - Yoshida, Shigeo; Sun, Yuqian; Kuzuoka, Hideaki
TI  - CHI - PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality
PY  - 2020
AB  - We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376358
ER  - 

TY  - NA
AU  - Babic, Teo; Perteneder, Florian; Reiterer, Harald; Haller, Michael J.
TI  - CHI Extended Abstracts - Simo: Interactions with Distant Displays by Smartphones with Simultaneous Face and World Tracking
PY  - 2020
AB  - The interaction with distant displays often demands complex, multi-modal inputs which need to be achieved with a very simple hardware solution so that users can perform rich inputs wherever they encounter a distant display. We present Simo, a novel approach, that transforms a regular smartphone into a highly-expressive user motion tracking device and controller for distant displays. Both the front and back cameras of the smartphone are used simultaneously to track the user's hand as well as the head, and body movements in real-world space and scale. In this work, we first define the possibilities for simultaneous face- and world-tracking using current off-the-shelf smartphones. Next, we present the implementation of a smartphone app enabling hand, head, and body motion tracking. Finally, we present a technical analysis outlining the possible tracking range.
SP  - 1
EP  - 12
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382962
ER  - 

TY  - NA
AU  - Neshati, Ali; Rey, Bradley; Faleel, Ahmed Shariff Mohommed; Bardot, Sandra; Latulipe, Celine; Irani, Pourang
TI  - CHI - BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion
PY  - 2021
AB  - We present BezelGlide, a novel suite of bezel interaction techniques, designed to minimize screen occlusion and ‘fat finger’ effects, when interacting with common graphs on smartwatches. To explore the design of BezelGlide, we conducted two user studies. First, we quantified the amount of screen occlusion experienced when interacting with the smartwatch bezel. Next, we designed two techniques that involve gliding the finger along the smartwatch bezel for graph interaction. Full BezelGlide (FBG) and Partial BezelGlide (PBG), use the full or a portion of the bezel, respectively, to reduce screen occlusion while scanning a line chart for data. In the common value detection task, we find that PBG outperforms FBG and Shift, a touchscreen occlusion-free technique, both quantitatively and subjectively, also while mobile. We finally illustrate the generzability potential of PBG to interact with common graph types making it a valuable interaction technique for smartwatch users.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445201
ER  - 

TY  - NA
AU  - Villarreal-Narvaez, Santiago; Vanderdonckt, Jean; Vatavu, Radu-Daniel; Wobbrock, Jacob O.
TI  - Conference on Designing Interactive Systems - A Systematic Review of Gesture Elicitation Studies: What Can We Learn from 216 Studies?
PY  - 2020
AB  - Gesture elicitation studies represent a popular and resourceful method in HCI to inform the design of intuitive gesture commands, reflective of end-users' behavior, for controlling all kinds of interactive devices, applications, and systems. In the last ten years, an impressive body of work has been published on this topic, disseminating useful design knowledge regarding users' preferences for finger, hand, wrist, arm, head, leg, foot, and whole-body gestures. In this paper, we deliver a systematic literature review of this large body of work by summarizing the characteristics and findings ofN=216gesture elicitation studies subsuming 5,458 participants, 3,625 referents, and 148,340 elicited gestures. We highlight the descriptive, comparative, and generative virtues of our examination to provide practitioners with an effective method to (i) understand how new gesture elicitation studies position in the literature; (ii) compare studies from different authors; and (iii) identify opportunities for new research. We make our large corpus of papers accessible online as a Zotero group library at https://www.zotero.org/groups/2132650/gesture_elicitation_studies.
SP  - 855
EP  - 872
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395511
ER  - 

TY  - NA
AU  - Jin, Chuhao; Xu, Hongteng; Song, Ruihua; Lu, Zhiwu
TI  - Text2Poster: Laying Out Stylized Texts on Retrieved Images
PY  - 2022
AB  - Poster generation is a significant task for a wide range of applications, which is often time-consuming and requires lots of manual editing and artistic experience. In this paper, we propose a novel data-driven framework, called Text2Poster, to automatically generate visually-effective posters from textual information. Imitating the process of manual poster editing, our framework leverages a large-scale pretrained visual-textual model to retrieve background images from given texts, lays out the texts on the images iteratively by cascaded autoencoders, and finally, stylizes the texts by a matching-based method. We learn the modules of the framework by weakly-and self-supervised learning strategies, mitigating the demand for labeled data. Both objective and subjective experiments demonstrate that our Text2Poster outperforms state-of-the-art methods, including academic research and commercial software, on the quality of generated posters.
SP  - NA
EP  - NA
JF  - ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icassp43922.2022.9747465
ER  - 

TY  - JOUR
AU  - Nsugbe, Ejay; Phillips, Carol; Fraser, Mike; McIntosh, Jess
TI  - Gesture recognition for transhumeral prosthesis control using EMG and NIR
PY  - 2020
AB  - A key challenge associated with myoelectric prosthesis limbs is the acquisition of a good quality gesture intent signal from the residual anatomy of an amputee. In this study, the authors aim to overcome this limitation by observing the classification accuracy of the fusion of wearable electromyography (EMG) and near-infrared (NIR) to classify eight hand gesture motions across 12 able-bodied participants. As part of the study, they investigate the classification accuracy across a multi-layer perceptron neural network, linear discriminant analysis and quadratic discriminant analysis for different sensing configurations, i.e. EMG-only, NIR-only and EMG-NIR. A separate offline ultrasound scan was conducted as part of the study and served as a ground truth and contrastive basis for the results picked up from the wearable sensors, and allowed for a closer study of the anatomy along the humerus during gesture motion. Results and findings from the work suggest that it could be possible to further develop transhumeral prosthesis using affordable, ergonomic and wearable EMG and NIR sensing, without the need for invasive neuromuscular sensors or further hardware complexity.
SP  - 122
EP  - 131
JF  - IET Cyber-Systems and Robotics
VL  - 2
IS  - 3
PB  - 
DO  - 10.1049/iet-csr.2020.0008
ER  - 

TY  - NA
AU  - Mazumdar, Amrita; Haynes, Brandon; Balazinska, Magda; Ceze, Luis; Cheung, Alvin; Oskin, Mark
TI  - SoCC - Perceptual Compression for Video Storage and Processing Systems
PY  - 2019
AB  - Compressed videos constitute 70% of Internet traffic, and video upload growth rates far outpace compute and storage improvement trends. Past work in leveraging perceptual cues like saliency, i.e., regions where viewers focus their perceptual attention, reduces compressed video size while maintaining perceptual quality, but requires significant changes to video codecs and ignores the data management of this perceptual information. In this paper, we propose Vignette, a compression technique and storage manager for perception-based video compression in the cloud. Vignette complements off-the-shelf compression software and hardware codec implementations. Vignette's compression technique uses a neural network to predict saliency information used during transcoding, and its storage manager integrates perceptual information into the video storage system. Our results demonstrate the benefit of embedding information about the human visual system into the architecture of cloud video storage systems.
SP  - 179
EP  - 192
JF  - Proceedings of the ACM Symposium on Cloud Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357223.3362725
ER  - 

TY  - CHAP
AU  - Tangemann, Matthias; Kümmerer, Matthias; Wallis, Thomas S. A.; Bethge, Matthias
TI  - ECCV (28) - Measuring the Importance of Temporal Features in Video Saliency
PY  - 2020
AB  - Where people look when watching videos is believed to be heavily influenced by temporal patterns. In this work, we test this assumption by quantifying to which extent gaze on recent video saliency benchmarks can be predicted by a static baseline model. On the recent LEDOV dataset, we find that at least 75% of the explainable information as defined by a gold standard model can be explained using static features. Our baseline model “DeepGaze MR” even outperforms state-of-the-art video saliency models, despite deliberately ignoring all temporal patterns. Visual inspection of our static baseline’s failure cases shows that clear temporal effects on human gaze placement exist, but are both rare in the dataset and not captured by any of the recent video saliency models. To focus the development of video saliency models on better capturing temporal effects we construct a meta-dataset consisting of those examples requiring temporal information.
SP  - 667
EP  - 684
JF  - Computer Vision – ECCV 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58604-1_40
ER  - 

TY  - JOUR
AU  - Jiang, Shuo; Kang, Peiqi; Song, Xinyu; Lo, Benny; Shull, Peter B.
TI  - Emerging Wearable Interfaces and Algorithms for Hand Gesture Recognition: A Survey.
PY  - 2022
AB  - Hands are vital in a wide range of fundamental daily activities, and neurological diseases that impede hand function can significantly affect quality of life. Wearable hand gesture interfaces hold promise to restore and assist hand function and to enhance human-human and human-computer communication. The purpose of this review is to synthesize current novel sensing interfaces and algorithms for hand gesture recognition, and the scope of applications covers rehabilitation, prosthesis control, sign language recognition, and human-computer interaction. Results showed that electrical, dynamic, acoustical/vibratory, and optical sensing were the primary input modalities in gesture recognition interfaces. Two categories of algorithms were identified: 1) classification algorithms for predefined, fixed hand poses and 2) regression algorithms for continuous finger and wrist joint angles. Conventional machine learning algorithms, including linear discriminant analysis, support vector machines, random forests, and non-negative matrix factorization, have been widely used for a variety of gesture recognition applications, and deep learning algorithms have more recently been applied to further facilitate the complex relationship between sensor signals and multi-articulated hand postures. Future research should focus on increasing recognition accuracy with larger hand gesture datasets, improving reliability and robustness for daily use outside of the laboratory, and developing softer, less obtrusive interfaces.
SP  - 1
EP  - 1
JF  - IEEE reviews in biomedical engineering
VL  - 15
IS  - NA
PB  - 
DO  - 10.1109/rbme.2021.3078190
ER  - 

TY  - CONF
AU  - Watson, Diane; Fitzmaurice, George; Matejka, Justin
TI  - How Tall is that Bar Chart? Virtual Reality, Distance Compression and Visualizations
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Furumoto, Takuro; Ito, Mitsuru; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki; Kamigaki, Takaaki
TI  - SIGGRAPH ASIA Emerging Technologies - Three-dimensional Interaction Technique Using an Acoustically Manipulated Balloon
PY  - 2019
AB  - We propose a system that uses an acoustically manipulated balloon as a visual and tangible interface for the representation of a mid-air virtual object in a full-body augmented reality environment. In this system, airborne ultrasound phased-array transducers on the ceiling actuate a spherical balloon inflated with a mixture of helium and air. This configuration permits (1) a full-body workspace with lateral scalability, (2) a long flight time, (3) good visibility, and (4) easily tangible access to the balloon. A projector-camera system projects a 2D or 3D perspective-correct image onto the balloon. The user can manipulate the corresponding virtual object by physically manipulating the balloon.
SP  - 51
EP  - 52
JF  - SIGGRAPH Asia 2019 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3355049.3360536
ER  - 

TY  - NA
AU  - Ahuja, Karan; Streli, Paul; Holz, Christian
TI  - UIST - TouchPose: Hand Pose Prediction, Depth Estimation, and Touch Classification from Capacitive Images
PY  - 2021
AB  - Today’s touchscreen devices commonly detect the coordinates of user input through capacitive sensing. Yet, these coordinates are the mere 2D manifestations of the more complex 3D configuration of the whole hand—a sensation that touchscreen devices so far remain oblivious to. In this work, we introduce the problem of reconstructing a 3D hand skeleton from capacitive images, which encode the sparse observations captured by touch sensors. These low-resolution images represent intensity mappings that are proportional to the distance to the user’s fingers and hands. We present the first dataset of capacitive images with corresponding depth maps and 3D hand pose coordinates, comprising 65,374 aligned records from 10 participants. We introduce our supervised method TouchPose, which learns a 3D hand model and a corresponding depth map using a cross-modal trained embedding from capacitive images in our dataset. We quantitatively evaluate TouchPose’s accuracy in touch classification, depth estimation, and 3D joint reconstruction, showing that our model generalizes to hand poses it has never seen during training and can infer joints that lie outside the touch sensor’s volume. Enabled by TouchPose, we demonstrate a series of interactive apps and novel interactions on multitouch devices. These applications show TouchPose’s versatile capability to serve as a general-purpose model, operating independent of use-case, and establishing 3D hand pose as an integral part of the input dictionary for application designers and developers. We also release our dataset, code, and model to enable future work in this domain.
SP  - 997
EP  - 1009
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474801
ER  - 

TY  - JOUR
AU  - Reipschläger, Patrick; Flemisch, Tamara; Dachselt, Raimund
TI  - Personal Augmented Reality for Information Visualization on Large Interactive Displays
PY  - 2021
AB  - In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.
SP  - 1182
EP  - 1192
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030460
ER  - 

TY  - NA
AU  - Kluge, Sven; Gladisch, Stefan; von Lukas, Uwe Freiherr; Staadt, Oliver G.; Tominski, Christian
TI  - Virtual Lenses as Embodied Tools for Immersive Analytics
PY  - NA
AB  - Interactive lenses are useful tools for supporting the analysis of data in different ways. Most existing lenses are designed for 2D visualization and are operated using standard mouse and keyboard interaction. On the other hand, research on virtual lenses for novel 3D immersive visualization environments is scarce. Our work aims to narrow this gap in the literature. We focus particularly on the interaction with lenses. Inspired by natural interaction with magnifying glasses in the real world, our lenses are designed as graspable tools that can be created and removed as needed, manipulated and parameterized depending on the task, and even combined to flexibly create new views on the data. We implemented our ideas in a system for the visual analysis of 3D sonar data. Informal user feedback from more than a hundred people suggests that the designed lens interaction is easy to use for the task of finding a hidden wreck in sonar data.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - 10.18420/vrar2020_8
ER  - 

TY  - JOUR
AU  - Bok, Jinwook; Kim, Bohyoung; Seo, Jinwook
TI  - Augmenting Parallel Coordinates Plots With Color-Coded Stacked Histograms.
PY  - 2022
AB  - We introduce Parallel Histogram Plot (PHP), a technique that overcomes the innate limitations of parallel coordinates plot (PCP) by attaching stacked-bar histograms with discrete color schemes to PCP. The color-coded histograms enable users to see an overview of the whole data without cluttering or scalability issues. Each rectangle in the PHP histograms is color coded according to the data ranking by a selected attribute. This color-coding scheme allows users to visually examine relationships between attributes, even between those that are displayed far apart, without repositioning or reordering axes. We adopt the Visual Information Seeking Mantra so that the polylines of the original PCP can be used to show details of a small number of selected items when the cluttering problem subsides. We also design interactions, such as a focus+context technique, to help users investigate small regions of interest in a space-efficient manner. We provide a real-world example in which PHP is effectively utilized compared with other visualizations, and we perform a controlled user study to evaluate the performance of PHP in helping users estimate the correlation between attributes. The results demonstrate that the performance of PHP was consistent in the estimation of correlations between two attributes regardless of the distance between them.
SP  - 2563
EP  - 2576
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2020.3038446
ER  - 

TY  - NA
AU  - Taniguchi, Keiichiro; Hashida, Tomoko
TI  - SIGGRAPH ASIA Posters - Rapid prototyping system using transformable and adherable PCL blocks
PY  - 2018
AB  - In this paper, we propose a rapid prototyping system with a combination of blocks that change shape and firmly adhere to each other. We focused on PCL (polycaprolactone), which is a plastic characterized by a low melting point, as a block material. PCL blocks can be transformed and bonded many times by melting them with hot water. In this research, we implemented a system that transforms external data into a block diagram and creates blocks. It enables rapid prototyping with more flexibility and stronger adhesion than ordinary block assembly.
SP  - 3283308
EP  - NA
JF  - SIGGRAPH Asia 2018 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3283289.3283308
ER  - 

TY  - JOUR
AU  - Wagner, Jorge; Stuerzlinger, Wolfgang; Nedel, Luciana
TI  - The Effect of Exploration Mode and Frame of Reference in Immersive Analytics
PY  - 2022
AB  - The design space for user interfaces for Immersive Analytics applications is vast. Designers can combine navigation and manipulation to enable data exploration with ego- or exocentric views, have the user operate at different scales, or use different forms of navigation with varying levels of physical movement. This freedom results in a multitude of different viable approaches. Yet, there is no clear understanding of the advantages and disadvantages of each choice. Our goal is to investigate the affordances of several major design choices, to enable both application designers and users to make better decisions. In this work, we assess two main factors, exploration mode and frame of reference, consequently also varying visualization scale and physical movement demand. To isolate each factor, we implemented nine different conditions in a Space-Time Cube visualization use case and asked 36 participants to perform multiple tasks. We analyzed the results in terms of performance and qualitative measures and correlated them with participants' spatial abilities. While egocentric room-scale exploration significantly reduced mental workload, exocentric exploration improved performance in some tasks. Combining navigation and manipulation made tasks easier by reducing workload, temporal demand, and physical effort.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 9
PB  - 
DO  - 10.1109/tvcg.2021.3060666
ER  - 

TY  - JOUR
AU  - Kang, Kyoungkook; Cho, Sunghyun
TI  - Interactive and automatic navigation for 360° video playback
PY  - 2019
AB  - A common way to view a 360° video on a 2D display is to crop and render a part of the video as a normal field-of-view (NFoV) video. While users can enjoy natural-looking NFoV videos using this approach, they need to constantly make manual adjustment of the viewing direction not to miss interesting events in the video. In this paper, we propose an interactive and automatic navigation system for comfortable 360° video playback. Our system finds a virtual camera path that shows the most salient areas through the video, generates a NFoV video based on the path, and plays it in an online manner. A user can interactively change the viewing direction while watching a video, and the system instantly updates the path reflecting the intention of the user. To enable online processing, we design our system consisting of an offline pre-processing step, and an online 360° video navigation step. The pre-processing step computes optical flow and saliency scores for an input video. Based on these, the online video navigation step computes an optimal camera path reflecting user interaction, and plays a NFoV video in an online manner. For improved user experience, we also introduce optical flow-based camera path planning, saliency-aware path update, and adaptive control of the temporal window size. Our experimental results including user studies show that our system provides more pleasant experience of watching 360° videos than existing approaches.
SP  - 108
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 4
PB  - 
DO  - 10.1145/3306346.3323046
ER  - 

TY  - NA
AU  - Drogemuller, Adam; Cunningham, Andrew; Walsh, James A.; Cordeil, Maxime; Ross, William; Thomas, Bruce H.
TI  - BDVA - Evaluating Navigation Techniques for 3D Graph Visualizations in Virtual Reality
PY  - 2018
AB  - Research into how virtual reality (VR) can be a beneficial technology for new and emerging large, complex data visualizations for data scientists is ongoing. In this paper, we evaluate three-dimensional VR navigation technique for data visualizations and test their effectiveness with a large graph visualization. We evaluate two prominent navigation techniques employed in VR (Teleportation and One-Handed Flying) against two less common methods (Two-Handed Flying and Worlds In Miniature) and evaluate their performance and effectiveness through a series of tasks. We found Steering Patterns (One-Handed Flying and Two-Handed Flying) to be faster and preferred by participants for completing searching tasks in comparision to Teleportation. Worlds-In-Miniature was the least physically demanding of the navigations, and was preferred by participants for tasks that required an overview of the graph such as triangle counting.
SP  - 50
EP  - 59
JF  - 2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bdva.2018.8533895
ER  - 

TY  - NA
AU  - Ens, Barrett; Bach, Benjamin; Cordeil, Maxime; Engelke, Ulrich; Serrano, Marcos; Willett, Wesley; Prouzeau, Arnaud; Anthes, Christoph; Büschel, Wolfgang; Dunne, Cody; Dwyer, Tim; Grubert, Jens; Haga, Jason H.; Kirshenbaum, Nurit; Kobayashi, Dylan; Lin, Tica; Olaosebikan, Monsurat; Pointecker, Fabian; Saffo, David; Saquib, Nazmus; Schmalstieg, Dieter; Szafir, Danielle Albers; Whitlock, Matt; Yang, Yalong
TI  - CHI - Grand Challenges in Immersive Analytics
PY  - 2021
AB  - Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.
SP  - 459
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3446866
ER  - 

TY  - NA
AU  - Kumagai, Kota; Miura, Shun; Hayasaki, Yoshio
TI  - Laser-excited volumetric display using aerial re-imaging by parabolic mirrors
PY  - 2021
AB  - A volumetric display forms three-dimensional (3D) graphics in real space by generating light emission or scattering points as volume pixels (voxels). In order to realize this display, we have proposed several types of system using femtosecond-laser-excited voxel and holographic beam control with computer-generated hologram. In our systems, a gas-state type which employs air as a screen shows touch interaction applications between user and graphics, and also demonstrated augmented reality in real 3D space. However, this system still has a challenge to display color graphics because the color of the aerial voxel is monochromatic bluish white. In this research, we try to form the graphics with multi-color by selectively re-imaging light from voxels with only arbitrary colors using two parabolic mirrors including variable color filters.
SP  - 99
EP  - 103
JF  - Digital Optical Technologies 2021
VL  - 11788
IS  - NA
PB  - 
DO  - 10.1117/12.2593770
ER  - 

TY  - BOOK
AU  - Chen, Guojun; Weiner, Noah; Zhong, Lin
TI  - DroNet@MobiSys - POD: A Smartphone That Flies
PY  - 2021
AB  - We present POD, a smartphone that flies, as a new way to achieve hands-free, eyes-up mobile computing. Unlike existing drone-carried user interfaces, POD features a smartphone-sized display and the computing and sensing power of a modern smartphone. We share our experience in prototyping POD, discuss the technical challenges facing it, and describe early results toward addressing them.
SP  - 7
EP  - 12
JF  - Proceedings of the 7th Workshop on Micro Aerial Vehicle Networks, Systems, and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3469259.3470490
ER  - 

TY  - JOUR
AU  - Wang, Huxi; Zuo, Siming; Cerezo-Sánchez, María; Arekhloo, Negin Ghahremani; Nazarpour, Kianoush; Heidari, Hadi
TI  - Wearable super-resolution muscle-machine interfacing.
PY  - 2022
AB  - Muscles are the actuators of all human actions, from daily work and life to communication and expression of emotions. Myography records the signals from muscle activities as an interface between machine hardware and human wetware, granting direct and natural control of our electronic peripherals. Regardless of the significant progression as of late, the conventional myographic sensors are still incapable of achieving the desired high-resolution and non-invasive recording. This paper presents a critical review of state-of-the-art wearable sensing technologies that measure deeper muscle activity with high spatial resolution, so-called super-resolution. This paper classifies these myographic sensors according to the different signal types (i.e., biomechanical, biochemical, and bioelectrical) they record during measuring muscle activity. By describing the characteristics and current developments with advantages and limitations of each myographic sensor, their capabilities are investigated as a super-resolution myography technique, including: (i) non-invasive and high-density designs of the sensing units and their vulnerability to interferences, (ii) limit-of-detection to register the activity of deep muscles. Finally, this paper concludes with new opportunities in this fast-growing super-resolution myography field and proposes promising future research directions. These advances will enable next-generation muscle-machine interfaces to meet the practical design needs in real-life for healthcare technologies, assistive/rehabilitation robotics, and human augmentation with extended reality.
SP  - 1020546
EP  - NA
JF  - Frontiers in neuroscience
VL  - 16
IS  - NA
PB  - 
DO  - 10.3389/fnins.2022.1020546
ER  - 

TY  - NA
AU  - Asahina, Ray; Nomoto, Takashi; Yoshida, Takatoshi; Watanabe, Yoshihiro
TI  - VR - Realistic 3D Swept-Volume Display with Hidden-Surface Removal Using Physical Materials
PY  - 2021
AB  - Conventional swept-volume displays can provide accurate physical cues for depth perception. However, the corresponding texture reproduction does not have high quality because such displays employ high-speed projectors with low bit-depth and low resolution. In this study, to address the limitation of swept-volume displays while retaining their advantages, a novel swept-volume three-dimensional (3D) display is proposed by incorporating physical materials as screens. Physical materials such as wool, felt, and so on are directly used for reproducing textures on a displayed 3D surface. Furthermore, we introduce the adaptive pattern generation based on real-time viewpoint tracking to perform the hidden-surface removal. Our algorithm leverages the ray-tracing concept and can run at high speed on GPU.
SP  - 113
EP  - 121
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00032
ER  - 

TY  - NA
AU  - Fosco, Camilo; Newman, Anelise; Sukhum, Pat; Bin Zhang, Yun; Zhao, Nanxuan; Oliva, Aude; Bylinskii, Zoya
TI  - CVPR - How Much Time Do You Have? Modeling Multi-Duration Saliency
PY  - 2020
AB  - What jumps out in a single glance of an image is different than what you might notice after closer inspection. Yet conventional models of visual saliency produce predictions at an arbitrary, fixed viewing duration, offering a limited view of the rich interactions between image content and gaze location. In this paper we propose to capture gaze as a series of snapshots, by generating population-level saliency heatmaps for multiple viewing durations. We collect the CodeCharts1K dataset, which contains multiple distinct heatmaps per image corresponding to 0.5, 3, and 5 seconds of free-viewing. We develop an LSTM-based model of saliency that simultaneously trains on data from multiple viewing durations. Our Multi-Duration Saliency Excited Model (MD-SEM) achieves competitive performance on the LSUN 2017 Challenge with 57% fewer parameters than comparable architectures. It is the first model that produces heatmaps at multiple viewing durations, enabling applications where multi-duration saliency can be used to prioritize visual content to keep, transmit, and render.
SP  - 4473
EP  - 4482
JF  - 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr42600.2020.00453
ER  - 

TY  - NA
AU  - Surale, Hemant Bhaskar; Matulic, Fabrice; Vogel, Daniel
TI  - CHI - Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality
PY  - 2019
AB  - We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard "subtraction method" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.
SP  - 196
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300426
ER  - 

TY  - JOUR
AU  - Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers
PY  - 2020
AB  - Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.
SP  - 3490
EP  - 3502
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - 26
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2020.3023567
ER  - 

TY  - NA
AU  - Merino, Leonel; Sotomayor-Gómez, Boris; Yu, Xingyao; Salgado, Ronie; Bergel, Alexandre; Sedlmair, Michael; Weiskopf, Daniel
TI  - CHI Extended Abstracts - Toward Agile Situated Visualization: An Exploratory User Study
PY  - 2020
AB  - We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383017
ER  - 

TY  - JOUR
AU  - MinXiongkuo, NA; GuKe, NA; ZhaiGuangtao, NA; YangXiaokang, NA; ZhangWenjun, NA; CalletPatrick, Le; Wen, ChenChang
TI  - Screen Content Quality Assessment: Overview, Benchmark, and Beyond
PY  - 2021
AB  - Screen content, which is often computer-generated, has many characteristics distinctly different from conventional camera-captured natural scene content. Such characteristic differences impose majo...
SP  - 1
EP  - 36
JF  - ACM Computing Surveys
VL  - 54
IS  - 9
PB  - 
DO  - 10.1145/3470970
ER  - 

TY  - NA
AU  - Lee, Benjamin; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim
TI  - A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501859
ER  - 

TY  - JOUR
AU  - Polatsek, Patrik; Waldner, Manuela; Viola, Ivan; Kapec, Peter; Benesova, Wanda
TI  - Exploring visual attention and saliency modeling for task-based visual analysis
PY  - 2018
AB  - NA
SP  - 26
EP  - 38
JF  - Computers & Graphics
VL  - 72
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2018.01.010
ER  - 

TY  - JOUR
AU  - Saktheeswaran, Ayshwarya; Srinivasan, Arjun; Stasko, John
TI  - Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction for Visual Network Exploration and Analysis
PY  - 2020
AB  - Interaction plays a vital role during visual network exploration as users need to engage with both elements in the view (e.g., nodes, links) and interface controls (e.g., sliders, dropdown menus). Particularly as the size and complexity of a network grow, interactive displays supporting multimodal input (e.g., touch, speech, pen, gaze) exhibit the potential to facilitate fluid interaction during visual network exploration and analysis. While multimodal interaction with network visualization seems like a promising idea, many open questions remain. For instance, do users actually prefer multimodal input over unimodal input, and if so, why? Does it enable them to interact more naturally, or does having multiple modes of input confuse users? To answer such questions, we conducted a qualitative user study in the context of a network visualization tool, comparing speech- and touch-based unimodal interfaces to a multimodal interface combining the two. Our results confirm that participants strongly prefer multimodal input over unimodal input attributing their preference to: 1) the freedom of expression, 2) the complementary nature of speech and touch, and 3) integrated interactions afforded by the combination of the two modalities. We also describe the interaction patterns participants employed to perform common network visualization operations and highlight themes for future multimodal network visualization systems to consider.
SP  - 2168
EP  - 2179
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 6
PB  - 
DO  - 10.1109/tvcg.2020.2970512
ER  - 

TY  - NA
AU  - Chidambaram, Subramanian; Zhang, Yunbo; Sundararajan, Venkatraghavan; Elmqvist, Niklas; Ramani, Karthik
TI  - CHI - Shape Structuralizer: Design, Fabrication, and User-driven Iterative Refinement of 3D Mesh Models
PY  - 2019
AB  - Current Computer-Aided Design (CAD) tools lack proper support for guiding novice users towards designs ready for fabrication. We propose Shape Structuralizer (SS), an interactive design support system that repurposes surface models into structural constructions using rods and custom 3D-printed joints. Shape Structuralizer embeds a recommendation system that computationally supports the user during design ideation by providing design suggestions on local refinements of the design. This strategy enables novice users to choose designs that both satisfy stress constraints as well as their personal design intent. The interactive guidance enables users to repurpose existing surface mesh models, analyze them in-situ for stress and displacement constraints, add movable joints to increase functionality, and attach a customized appearance. This also empowers novices to fabricate even complex constructs while ensuring structural soundness. We validate the Shape Structuralizer tool with a qualitative user study where we observed that even novice users were able to generate a large number of structurally safe designs for fabrication.
SP  - 663
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300893
ER  - 

TY  - CHAP
AU  - Zhu, Liyu; Cao, Xueni; Fang, Ying; Zhang, Liqun; Li, Xiaodong
TI  - HCI (14) - Application of Visual Saliency in the Background Image Cutting for Layout Design
PY  - 2020
AB  - In many poster designs, an image usually will be used as a back-ground image, and text and picture will be carried out on the background image later. For intelligent layout design, cropping a suitable background image should be the first problem to be solved. In this paper, through eye movement experiments, ground truth saliency maps of the posters are obtained. Then, the characteristics of the saliency maps of background images are summarized. The characteristics are mainly the rules of the location and size of the salient areas in the background image. The research found that the salient areas of the poster background images are more concentrated in the upper and middle of the poster image, and they are distributed in an inverted triangle. These rules can cut a more suitable background image for typesetting.
SP  - 168
EP  - 183
JF  - Social Computing and Social Media. Design, Ethics, User Behavior, and Social Network Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49570-1_12
ER  - 

TY  - NA
AU  - Schoop, Eldon; Zhou, Xin; Li, Gang; Chen, Zhourong; Hartmann, Bjoern; Li, Yang
TI  - Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis
PY  - 2022
AB  - We use a deep learning based approach to predict whether a selected element in a mobile UI screenshot will be perceived by users as tappable, based on pixels only instead of view hierarchies required by previous work. To help designers better understand model predictions and to provide more actionable design feedback than predictions alone, we additionally use ML interpretability techniques to help explain the output of our model. We use XRAI to highlight areas in the input screenshot that most strongly influence the tappability prediction for the selected region, and use k-Nearest Neighbors to present the most similar mobile UIs from the dataset with opposing influences on tappability perception.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517497
ER  - 

TY  - JOUR
AU  - Okada, Kaya; Yoshida, Mitsuo; Itoh, Takayuki; Czauderna, Tobias; Stephens, Kingsley
TI  - VR system for spatio-temporal visualization of tweet data and support of map exploration
PY  - 2019
AB  - Social media analysis is helpful to understand the behavior of people. Human behavior in social media is related to time and location, which is often difficult to find the characteristics appropriately and quickly. We chose to apply virtual reality (VR) technologies to present the spatio-temporal social media data. This makes us easier to develop interactive and intuitive user interfaces and explore the data as we want. This paper proposes a VR system featuring two visualization techniques. One of the techniques is a three-dimensional temporal visualization of tweets of microblogs with location information. It consists of the two-dimensional map and a time axis. In particular, we aggregate the number of tweets of each coordinate and time step and depict them as piled cubes. We highlight only specific cubes so that users can understand the overall tendency of datasets. The other technique provides a route recommendation based on tweets of microblogs. Our technique supports users to explore attractive events and places by selecting effective tweets and suggesting routes. We also developed user interfaces for operating these objects in a VR space which indicate details of tweets.
SP  - 32849
EP  - 32868
JF  - Multimedia Tools and Applications
VL  - 78
IS  - 23
PB  - 
DO  - 10.1007/s11042-019-08016-y
ER  - 

TY  - NA
AU  - Newman, Anelise; McNamara, Barry A.; Fosco, Camilo; Bin Zhang, Yun; Sukhum, Pat; Tancik, Matthew; Kim, Nam Wook; Bylinskii, Zoya
TI  - CHI - TurkEyes: A Web-Based Toolbox for Crowdsourcing Attention Data
PY  - 2020
AB  - Eye movements provide insight into what parts of an image a viewer finds most salient, interesting, or relevant to the task at hand. Unfortunately, eye tracking data, a commonly-used proxy for attention, is cumbersome to collect. Here we explore an alternative: a comprehensive web-based toolbox for crowdsourcing visual attention. We draw from four main classes of attention-capturing methodologies in the literature. ZoomMaps is a novel zoom-based interface that captures viewing on a mobile phone. CodeCharts is a self-reporting methodology that records points of interest at precise viewing durations. ImportAnnots is an "annotation" tool for selecting important image regions, and cursor-based BubbleView lets viewers click to deblur a small area. We compare these methodologies using a common analysis framework in order to develop appropriate use cases for each interface. This toolbox and our analyses provide a blueprint for how to gather attention data at scale without an eye tracker.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376799
ER  - 

TY  - NA
AU  - Miyafuji, Shio; Perteneder, Florian; Sato, Toshiki; Koike, Hideki; Klinker, Gudrun
TI  - VRCAI - A Bowl-Shaped Display for Controlling Remote Vehicles
PY  - 2019
AB  - This paper proposes a bowl-shaped hemispherical display to observe omnidirectional images. This display type has many advantages over conventional, flat 2D displays, in particular when it is used for controlling remote vehicles. First, it allows users to observe an azimuthal equidistant view of omnidirectional images by looking from above. Second, it provides a first-person view by looking into the inside of the hemispherical surface from diagonally above. Third, it provides a pseudo–third-person view as if we watched the remote vehicle from its back, by observing both the inside and outside at the same time from obliquely above. These characteristics solve the issues of blind angles around the remote vehicle. We conduct a VR-based user study to compare the bowl-shaped display to an equirectangular projection on a 2D display and a first-person view used in head-mounted displays. Based on the insights gained in the study, we present a real-world implementation and describe the uniqueness, advantages but also shortcomings of our method.
SP  - NA
EP  - NA
JF  - Proceedings of the 17th International Conference on Virtual-Reality Continuum and its Applications in Industry
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359997.3365706
ER  - 

TY  - NA
AU  - Tanhaei, Ghazaleh; Hardman, Lynda; Huerst, Wolfgang
TI  - AIVR - DatAR: Your Brain, Your Data, On Your Desk - A Research Proposal
PY  - 2019
AB  - We present a research proposal that investigates the use of 3D representations in Augmented Reality (AR) to allow neuroscientists to explore literature they wish to understand for their own scientific purposes. Neuroscientists need to identify potential real-life experiments they wish to perform that provide the most information for their field with the minimum use of limited resources. This requires understanding both the already known relationships among concepts and those that have not yet been discovered. Our assumption is that by providing overviews of the correlations among concepts through the use of linked data, these will allow neuroscientists to better understand the gaps in their own literature and more quickly identify the most suitable experiments to carry out. We will identify candidate visualizations and improve upon these for a specific information need. We describe our planned prototype 3D AR implementation and directions we intend to explore.
SP  - 138
EP  - 1385
JF  - 2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr46125.2019.00029
ER  - 

TY  - NA
AU  - Fonnet, Adrien; Vigier, Toinon; Prié, Yannick; Cliquet, Gregoire; Picarougne, Fabien
TI  - BDVA - Axes and Coordinate Systems Representations for Immersive Analytics of Multi-Dimensional Data
PY  - 2018
AB  - Axes are the main components of coordinate systems representations. They play a critical role for the visual analysis of multi-dimensional data. However their representation seems to have always be considered self evident, with oriented lines crossing at an origin, completed with labels such as ticks and names. Such classical representation show limits when it comes 3D visualization and immersive analytic (IA), mainly because orthogonal projection of points on linear axes is hard in a 3d environment, and because the user can move therefore the axes can get out of his field of view. In this paper we propose a task-based definition of axes and coordinate systems representation, as well as a tentative design space for coordinates systems representation in immersion. We also present an exploratory user study we carried out to compare three grid-based representations of coordinate systems for multidimensional data analysis with 3D scatterplots.
SP  - 1
EP  - 10
JF  - 2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bdva.2018.8533892
ER  - 

TY  - JOUR
AU  - Ivanov, Alexander; Danyluk, Kurtis Thorvald; Jacob, Christian; Willett, Wesley
TI  - A Walk Among the Data
PY  - 2019
AB  - We examine the potential for immersive unit visualizations—interactive virtual environments populated with objects representing individual items in a dataset. Our virtual reality prototype highlights how immersive unit visualizations can allow viewers to examine data at multiple scales, support immersive exploration, and create affective personal experiences with data.
SP  - 19
EP  - 28
JF  - IEEE computer graphics and applications
VL  - 39
IS  - 3
PB  - 
DO  - 10.1109/mcg.2019.2898941
ER  - 

TY  - NA
AU  - Huang, Hao-Juan; Shen, I-Chao; Chan, Liwei
TI  - MobileHCI - Director-360: Introducing Camera Handling to 360 Cameras
PY  - 2020
AB  - This work introduces the concept of camera handling for a 360 camera and proposes Director-360, a 360 camera enhanced with two novel handling techniques. Pointer and field-of-view (FoV) are designed to explicitly and implicitly capture the 360 photographer’s subject of interest within the 360 media at capture time. Pointer lets users specify a subject of interest about the scene by directly pointing the 360 camera as if using the camera as a flashlight, while FoV captures the user’s subject of interest within the 360 scene by mapping the user’s face direction to the 360 media. We described an implementation using deep-learning algorithms. We also presented the Director-360 Editor, which incorporates the handling data to streamline the post-editing process. To understand how Director-360 helps to compose 360 media, a pilot study was carried out to create video storytelling in three target scenarios. The results and user feedback from a pilot study were reported.
SP  - 3403550
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403550
ER  - 

TY  - JOUR
AU  - Lu, Yiqin; Huang, Bingjian; Yu, Chun; Liu, Guahong; Shi, Yuanchun
TI  - Designing and Evaluating Hand-to-Hand Gestures with Dual Commodity Wrist-Worn Devices
PY  - 2020
AB  - Hand gestures provide a natural and easy-to-use way to input commands. However, few works have studied the design space of bimanual hand gestures or attempted to infer gestures that involve devices on both hands. We explore the design space of hand-to-hand gestures, a group of gestures that are performed by touching one hand with the other hand. Hand-to-hand gestures are easy to perform and provide haptic feedback on both hands. Moreover, hand-to-hand gestures generate simultaneous vibration on two hands that can be sensed by dual off-the-shelf wrist-worn devices. In this work, we derive a hand-to-hand gesture vocabulary with subjective ratings from users and select gesture sets for real-life scenarios. We also take advantage of devices on both wrists to demonstrate their gesture-sensing capability. Our results show that the recognition accuracy for fourteen gestures is 94.6% when the user is stationary, and the accuracy for five gestures is 98.4% or 96.3% when the user is walking or running, respectively. This is significantly more accurate than a single device worn on either wrist. Our further evaluation also validates that users can easily remember hand-to-hand gestures and use our technique to invoke commands in real-life contexts.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3380984
ER  - 

TY  - CHAP
AU  - Zheng, Quanlong; Jiao, Jianbo; Cao, Ying; Lau, Rynson W. H.
TI  - ECCV (14) - Task-driven Webpage Saliency
PY  - 2018
AB  - In this paper, we present an end-to-end learning framework for predicting task-driven visual saliency on webpages. Given a webpage, we propose a convolutional neural network to predict where people look at it under different task conditions. Inspired by the observation that given a specific task, human attention is strongly correlated with certain semantic components on a webpage (e.g., images, buttons and input boxes), our network explicitly disentangles saliency prediction into two independent sub-tasks: task-specific attention shift prediction and task-free saliency prediction. The task-specific branch estimates task-driven attention shift over a webpage from its semantic components, while the task-free branch infers visual saliency induced by visual features of the webpage. The outputs of the two branches are combined to produce the final prediction. Such a task decomposition framework allows us to efficiently learn our model from a small-scale task-driven saliency dataset with sparse labels (captured under a single task condition). Experimental results show that our method outperforms the baselines and prior works, achieving state-of-the-art performance on a newly collected benchmark dataset for task-driven webpage saliency detection.
SP  - 300
EP  - 316
JF  - Computer Vision – ECCV 2018
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01264-9_18
ER  - 

TY  - NA
AU  - Brudy, Frederik; Holz, Christian; Rädle, Roman; Wu, Chi-Jui; Houben, Steven; Klokmose, Clemens Nylandsted; Marquardt, Nicolai
TI  - CHI - Cross-Device Taxonomy: Survey, Opportunities and Challenges of Interactions Spanning Across Multiple Devices
PY  - 2019
AB  - Designing interfaces or applications that move beyond the bounds of a single device screen enables new ways to engage with digital content. Research addressing the opportunities and challenges of interactions with multiple devices in concert is of continued focus in HCI research. To inform the future research agenda of this field, we contribute an analysis and taxonomy of a corpus of 510 papers in the cross-device computing domain. For both new and experienced researchers in the field we provide: an overview, historic trends and unified terminology of cross-device research; discussion of major and under-explored application areas; mapping of enabling technologies; synthesis of key interaction techniques spanning across multiple devices; and review of common evaluation strategies. We close with a discussion of open issues. Our taxonomy aims to create a unified terminology and common understanding for researchers in order to facilitate and stimulate future cross-device research.
SP  - 562
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300792
ER  - 

TY  - NA
AU  - Chen, Guojun; Weiner, Noah; Zhong, Lin
TI  - POD: A Smartphone That Flies
PY  - 2021
AB  - We present POD, a smartphone that flies, as a new way to achieve hands-free, eyes-up mobile computing. Unlike existing drone-carried user interfaces, POD features a smartphone-sized display and the computing and sensing power of a modern smartphone. We share our experience in building a prototype of POD, discuss the technical challenges facing it, and describe early results toward addressing them.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Shahmoradi, Javad; Martinez-Ponce, Joseph; Cross, Richard; Olivas, Micaela; Malburg, Alexander; Roghanchi, Pedram; Hassanalian, Mostafa
TI  - Design and Optimization of an Encased Drone for Underground Mining Applications
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - AIAA Scitech 2021 Forum
VL  - NA
IS  - NA
PB  - 
DO  - 10.2514/6.2021-0696
ER  - 

TY  - NA
AU  - Batch, Andrea; Patnaik, Biswaksen; Akazue, Moses; Elmqvist, Niklas
TI  - CHI - Scents and Sensibility: Evaluating Information Olfactation
PY  - 2020
AB  - Olfaction---the sense of smell---is one of the least explored of the human senses for conveying abstract information. In this paper, we conduct a comprehensive perceptual experiment on information olfactation: the use of olfactory and cross-modal sensory marks and channels to convey data. More specifically, following the example from graphical perception studies, we design an experiment that studies the perceptual accuracy of four cross-modal sensory channels---scent type, scent intensity, airflow, and temperature---for conveying three different types of data---nominal, ordinal, and quantitative. We also present details of a 24-scent multi-sensory display and its software framework that we designed in order to run this experiment. Our results yield a ranking of olfactory and cross-modal sensory channels that follows similar principles as classic rankings for visual channels.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376733
ER  - 

TY  - NA
AU  - Butcher, Peter; John, Nigel W.; Ritsos, Panagiotis D.
TI  - Towards a Framework for Immersive Analytics on the Web
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Goetschel, Eric; Sekaran, Janane; Ren, Weihang; He, Mingyi; Ogbonnaya, Nnenne; Nkereuwem, Michael; Mapfunde, Irene; Martin, Chloe; Cogburn, Courtney D.; Feiner, Steven
TI  - VR Workshops - COVIZ: Visualization of Effects of COVID-19 on New York City Through Socially Impactful Virtual Reality
PY  - 2021
AB  - This work is the product of a collaboration between students studying computer science and social work to visualize the impacts and effects of COVID-19 in New York City in a virtual environment (VE). As a proof of concept, the team chose two datasets from NYC Open Data; COVID-19 infection cases and rates per zip code and vehicular traffic rates within the five boroughs of New York City. To foster unexplored insights into the relationship between these data, we developed a virtual reality application that provides a stronger sense of embodiment and ownership of urban visualization analysis when manipulating 3D virtual maps for comparison in a VE.
SP  - 703
EP  - 704
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00234
ER  - 

TY  - NA
AU  - Ikegawa, Koshi; Shizuki, Buntarou
TI  - OZCHI - Tesla blocks: magnetism-based tangible 3D modeling system using block-shaped objects
PY  - 2018
AB  - We herein demonstrate Tesla Blocks, a magnetism-based tangible 3D modeling system using block-shaped objects. The system recognizes the structure assembled by the user and draws the 3D model in real time. Each block of the system has a simple structure; we embed only a permanent magnet in a block. Because the electronic circuit used for recognizing the structure exists outside the blocks, the system is simple. Furthermore, occlusion by the user's hand does not occur in recognizing the structure.
SP  - 411
EP  - 415
JF  - Proceedings of the 30th Australian Conference on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3292147.3292221
ER  - 

TY  - NA
AU  - Wessely, Michael; Tsandilas, Theophanis; Mackay, Wendy E.
TI  - UIST - Shape-Aware Material: Interactive Fabrication with ShapeMe
PY  - 2018
AB  - Makers often create both physical and digital prototypes to explore a design, taking advantage of the subtle feel of physical materials and the precision and power of digital models. We introduce ShapeMe, a novel smart material that captures its own geometry as it is physically cut by an artist or designer. ShapeMe includes a software toolkit that lets its users generate customized, embeddable sensors that can accommodate various object shapes. As the designer works on a physical prototype, the toolkit streams the artist's physical changes to its digital counterpart in a 3D CAD environment. We use a rapid, inexpensive and simple-to-manufacture inkjet printing technique to create embedded sensors. We successfully created a linear predictive model of the sensors' lengths, and our empirical tests of ShapeMe show an average accuracy of 2 to 3 mm. We present two application scenarios for modeling multi-object constructions, such as architectural models, and 3D models consisting of multiple layers stacked one on top of each other. ShapeMe demonstrates a novel technique for integrating digital and physical modeling, and suggests new possibilities for creating shape-aware materials.
SP  - 127
EP  - 139
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - 18
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242619
ER  - 

TY  - NA
AU  - Reipschläger, Patrick; Brudy, Frederik; Dachselt, Raimund; Matejka, Justin; Fitzmaurice, George; Anderson, Fraser
TI  - AvatAR: An Immersive Analysis Environment for Human Motion Data Combining Interactive 3D Avatars and Trajectories
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517676
ER  - 

TY  - NA
AU  - Miyafuji, Shio; Toyohara, Soichiro; Sato, Toshiki; Koike, Hideki
TI  - UIST (Adjunct Volume) - DisplayBowl: A Bowl-Shaped Display for Omnidirectional Videos
PY  - 2018
AB  - We introduce DisplayBowl which is a concept of a bowl shaped hemispherical display for showing omnidirectional images. This display provides three-way observation for omnidirectional images. DisplayBowl allows users to observe an omnidirectional image by looking the image from above. In addition, users can see it with a first-person-viewpoint, by looking into the inside of the hemispherical surface from diagonally above. Furthermore, by observing both the inside and the outside of the hemispherical surface at the same time from obliquely above, it is possible to observe it by a pseudo third-person-viewpoint, like watching the drone obliquely from behind. These ways of viewing solve the problem of inability of pilots controlling a remote vehicle such as a drone to notice what happens behind them, which happen with conventional displays such as flat displays and head mounted displays.
SP  - 99
EP  - 101
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266114
ER  - 

TY  - NA
AU  - Menzner, Tim; Gesslein, Travis; Otte, Alexander; Grubert, Jens
TI  - VR - Above Surface Interaction for Multiscale Navigation in Mobile Virtual Reality
PY  - 2020
AB  - Virtual Reality enables the exploration of large information spaces. In physically constrained spaces such as airplanes or buses, controller-based or mid-air interaction in mobile Virtual Reality can be challenging. Instead, the input space on and above touchscreen enabled devices such as smartphones or tablets could be employed for Virtual Reality interaction in those spaces.In this context, we compared an above surface interaction technique with traditional 2D on-surface input for navigating large planar information spaces such as maps in a controlled user study (n = 20). We find that our proposed above surface interaction technique results in significantly better performance and user preference compared to pinch-to-zoom and drag-to-pan when navigating planar information spaces.
SP  - 372
EP  - 381
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581107639032
ER  - 

TY  - JOUR
AU  - Li, Chenhui; Zhang, Peiying; Wang, Changbo
TI  - Harmonious Textual Layout Generation Over Natural Images via Deep Aesthetics Learning
PY  - 2022
AB  - NA
SP  - 3416
EP  - 3428
JF  - IEEE Transactions on Multimedia
VL  - 24
IS  - NA
PB  - 
DO  - 10.1109/tmm.2021.3097900
ER  - 

TY  - NA
AU  - Chen, Zhenfang; Byrne, Daragh; EL-Zanfaly, Dina
TI  - Google Home, Listen: Building Helper Intelligences for Non-Verbal Sound
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3535202
ER  - 

TY  - NA
AU  - Kikuchi, Kotaro; Simo-Serra, Edgar; Otani, Mayu; Yamaguchi, Kota
TI  - Constrained Graphic Layout Generation via Latent Optimization
PY  - 2021
AB  - It is common in graphic design humans visually arrange various elements according to their design intent and semantics. For example, a title text almost always appears on top of other elements in a document. In this work, we generate graphic layouts that can flexibly incorporate such design semantics, either specified implicitly or explicitly by a user. We optimize using the latent space of an off-the-shelf layout generation model, allowing our approach to be complementary to and used with existing layout generation models. Our approach builds on a generative layout model based on a Transformer architecture, and formulates the layout generation as a constrained optimization problem where design constraints are used for element alignment, overlap avoidance, or any other user-specified relationship. We show in the experiments that our approach is capable of generating realistic layouts in both constrained and unconstrained generation tasks with a single model. The code is available at this https URL .
SP  - 88
EP  - 96
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3475497
ER  - 

TY  - JOUR
AU  - Chu, Wei-Ta; Motomura, Hideto; Tsumura, Norimichi; Yamasaki, Toshihiko
TI  - [Invited papers] A Survey on Multimedia Artworks Analysis and Attractiveness Computing in Multimedia
PY  - 2019
AB  - NA
SP  - 60
EP  - 67
JF  - ITE Transactions on Media Technology and Applications
VL  - 7
IS  - 2
PB  - 
DO  - 10.3169/mta.7.60
ER  - 

TY  - CHAP
AU  - Quijano-Chavez, Carlos; Nedel, Luciana; Freitas, Carla M. D. S.
TI  - INTERACT (3) - An Immersive Approach Based on Two Levels of Interaction for Exploring Multiple Coordinated 3D Views
PY  - 2021
AB  - Multiple coordinated views have often been used for visual analytics purposes over the last years. In this context, if the exploration of 2D visualizations is not an obstacle, adding an extra dimension can be an issue. The interaction with multiple 3D visualizations in 2D conventional displays lacks usability and does not guarantee the usefulness the extra dimension would provide. Immersive visualization techniques can potentially fulfill these gaps by providing 3D visualizations and novel 3D interactions simultaneously. In this paper, we propose a new approach for interacting with composite and multiple coordinated visualizations in immersive virtual environments. We use a 3D-WIMP-like concept, i.e., virtual cubes (Spaces), for encapsulating views, which the user can freely control in the virtual environment. Moreover, operations like “cloning” and “coordinated interactions” features provide a way for performing composed tasks. We compared our approach with a desktop version to evaluate its performance when dealing with composed tasks. A user study with 19 participants was conducted, and the results show that the immersive approach has advantages over the corresponding desktop version regarding interaction with multiple coordinated 3D views.
SP  - 493
EP  - 513
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85613-7_33
ER  - 

TY  - BOOK
AU  - Marriott, Kim; Chen, Jian; Hlawatsch, Marcel; Itoh, Takayuki; Nacenta, Miguel A.; Reina, Guido; Stuerzlinger, Wolfgang
TI  - Immersive Analytics - Immersive Analytics: Time to Reconsider the Value of 3D for Information Visualisation
PY  - 2018
AB  - Modern virtual reality display technologies engender spatial immersion by using a variety of depth cues such as perspective and head-tracked binocular presentation to create visually realistic 3D worlds. While 3D visualisations are common in scientific visualisation, they are much less common in information visualisation. In this chapter we explore whether immersive analytic applications should continue to use traditional 2D information visualisations or whether there are situations when 3D may offer benefits. We identify a number of potential applications of 3D depth cues for abstract data visualisation: using depth to show an additional data dimension, such as in 2.5D network layouts, views on non-flat surfaces and egocentric views in which the data is placed around the viewer, and visualising abstract data with a spatial embedding. Another important potential benefit is the ability to arrange multiple views in the 3D space around the user and to attach abstract visualisations to objects in the real world.
SP  - 25
EP  - 55
JF  - Immersive Analytics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01388-2_2
ER  - 

TY  - JOUR
AU  - Fadaei Jouybari, Atena; Jeanmonod, Kenny; Kannape, Olivier A.; Potheegadoo, Jevita; Bleuler, Hannes; Hara, Masayuki; Blanke, Olaf
TI  - Cogno-Vest: A Torso-Worn, Force Display to Experimentally Induce Specific Hallucinations and Related Bodily Sensations
PY  - 2022
AB  - NA
SP  - 497
EP  - 506
JF  - IEEE Transactions on Cognitive and Developmental Systems
VL  - 14
IS  - 2
PB  - 
DO  - 10.1109/tcds.2021.3051395
ER  - 

TY  - NA
AU  - Bach, Benjamin; Cordeil, Maxime; Engelke, Ulrich; Ens, Barrett; Serrano, Marcos; Willett, Wesley
TI  - CHI Extended Abstracts - Interaction Design & Prototyping for Immersive Analytics
PY  - 2019
AB  - Immersive Analytics is concerned with the design and evaluation of interactive next-generation interfaces that support human understanding, data analysis, and decision making. New immersive technologies present many opportunities for enhancing humans' experiences with data interaction, but also present many challenges, a subset of which are specific to the analytics domain. This workshop is centered around a set of group prototyping sessions, aimed at identifying new approaches to existing design challenges. In addition to giving perspective on opportunities and difficulties faced by future designers, these exercises will also explore new prototyping methods and tools for the design of interactive data-centric interfaces. This part-day workshop aims to build new ties between the existing immersive analytics community with researchers across many disciplines of the CHI community.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3299019
ER  - 

TY  - BOOK
AU  - Cordeil, Maxime; Dwyer, Tim
TI  - ISS - Introduction to IATK: An Immersive Visual Analytics toolkit
PY  - 2019
AB  - Immersive Analytics is an emerging interdisciplinary research area that investigates the use of nontraditional display and input technology to immerse users in their data. A prominent aspect this research investigates is "immersive data visualization", which uses augmented and virtual reality technology to visually immerse users in their data to facilitate collaboration, exploration and understanding. Currently, there is a lack of simple tools to build interactive data visualizations in immersive environments. The de facto approach is to use off-the-shelf game engines because they allow easy prototyping of 3D user interfaces in these immersive environments. However, game engines do not consider the specialized requirements of data visualization, such as the type and structure of datasets, the breadth of data, especially in the age of "big data", and typical information visualization tasks. IATK: Immersive Analytics Toolkit is an open source visualization toolkit for the Unity game engine that fills this gap. Specifically, IATK: supports an infovis pipeline for virtual and augmented reality environments; visualizes large (up to 1 million) data points at an optimal framerate for immersive applications; and, provides a technology-agnostic model for user interactions with immersive visualizations. This tutorial will introduce participants to the Unity game engine and teach practical skills for implementing immersive data visualisations using IATK.
SP  - 431
EP  - 435
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3361927
ER  - 

TY  - NA
AU  - Varga, Virag; Wyss, Marc; Vakulya, Gergely; Sample, Alanson P.; Gross, Thomas R.
TI  - UIST - Designing Groundless Body Channel Communication Systems: Performance and Implications
PY  - 2018
AB  - Novel interactions that capacitively couple electromagnetic (EM) fields between devices and the human body are gaining more attention in the human-computer interaction community. One class of these techniques is Body Channel Communication (BCC), a method that overlays physical touch with digital information. Despite the number of published capacitive sensing and communication prototypes, there exists no guideline on how to design such hardware or what are the application limitations and possibilities. Specifically, wearable (groundless) BCC has been proven in the past to be extremely challenging to implement. Additionally, the exact behavior of the human body as an EM-field medium is still not fully understood today. Consequently, the application domain of BCC technology could not be fully explored. This paper addresses this problem. Based on a recently published general purpose wearable BCC system, we first present a thorough evaluation of the impact of various technical parameter choices and an exhaustive channel characterization of the human body as a host for BCC. Second, we discuss the implications of these results for the application design space and present guidelines for future wearable BCC systems and their applications. Third, we point out an important observation of the measurements, namely that BCC can employ the whole body as user interface (and not just hands or feet). We sketch several applications with these novel interaction modalities.
SP  - 683
EP  - 695
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242622
ER  - 

TY  - NA
AU  - Mäkelä, Ville; Keskinen, Tuuli; Mäkelä, John; Kallioniemi, Pekka; Karhu, Jussi; Ronkainen, Kimmo; Burova, Alisa; Hakulinen, Jaakko; Turunen, Markku
TI  - TVX - What Are Others Looking at? Exploring 360° Videos on HMDs with Visual Cues about Other Viewers
PY  - 2019
AB  - Viewing 360° videos on a head-mounted display (HMD) can be an immersive experience. However, viewers must often be guided, as the freedom to rotate the view may make them miss things. We explore a unique, automatic approach to this problem with dynamic guidance methods called social indicators. They use the viewers’ gaze data to recognize popular areas in 360° videos, which are then visualized to subsequent viewers. We developed and evaluated two different social indicators in a 30-participant user study. Although the indicators show great potential in subtly guiding users and improving the experience, finding the balance between guidance and self-exploration is vital. Also, users had varying interest towards indicators that represented a larger audience but reported a clear desire to use the indicators with their friends. We also present guidelines for providing dynamic guidance for 360° videos.
SP  - 13
EP  - 24
JF  - Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3317697.3323351
ER  - 

TY  - JOUR
AU  - 최, 준영; 정, 해진; 정, 원기
TI  - 가젯암: 확장현실을 위한 손 제스처 기반 대화형 데이터 시각화 시스템
PY  - 2019
AB  - ?????? ??? ??????????????? ?????? ????????????(XR: Extended Reality)??? ????????? ????????? ????????? ??? ????????? ?????? ????????? ???????????? ????????? ??????. ?????????????????? ???????????? 3?????? ?????? ????????? ???????????? ????????? ??? ?????? ???????????? ??????????????? ?????? ????????? ??? ???????????? ?????? ???????????? ????????? ????????? ????????????. ????????? ??????????????? ????????? ????????? ???????????? ?????? ???????????? ???????????? ?????? ????????? ?????????????????? ???????????? ????????? ????????? ????????? ?????????. ??? ??????????????? ??? ?????????????????? XR??? ?????? ????????? ??? ????????? ????????? ???, ?????? 3?????? ????????? ????????? ??? ?????? ?????????(Gadget Arms) ???????????? ????????????. ??? ??????????????? ???????????? ???????????? ?????? ???????????? ????????? ?????????????????? ????????? ?????? ?????? XR ????????? ????????? ???????????? ??????????????? ?????? ????????? ????????? ???????????? ??? ?????????, ????????? ?????? ????????? 3?????? ????????? ??????????????? ???????????? ??? ?????? ??????????????? ?????? ????????? 3?????? ????????? ??????????????? ????????? ??? ??????. ????????? ????????? ????????? ?????? ????????? ???????????? ????????? ????????? ?????? ????????? ???????????? ?????? ?????????????????? ?????? ???????????????
SP  - 31
EP  - 41
JF  - Journal of the Korea Computer Graphics Society
VL  - 25
IS  - 2
PB  - 
DO  - 10.15701/kcgs.2019.25.2.31
ER  - 

TY  - JOUR
AU  - Newbury, Rhys; Satriadi, Kadek Ananta; Bolton, Jesse; Liu, Jiazhou; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard
TI  - Embodied gesture interaction for immersive maps
PY  - 2021
AB  - With the increasing availability of head-mounted displays for virtual reality and augmented reality, we can create immersive maps in which the user is closer to the data. Embodiment is a key concep...
SP  - 417
EP  - 431
JF  - Cartography and Geographic Information Science
VL  - 48
IS  - 5
PB  - 
DO  - 10.1080/15230406.2021.1929492
ER  - 

TY  - JOUR
AU  - Chu, Xiangtong; Xie, Xiao; Ye, Shuainan; Lu, Haolin; Xiao, Hongguang; Yuan, Zeqing; Chen, Zhutian; Zhang, Hui; Wu, Yingcai
TI  - TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations.
PY  - 2021
AB  - Tactic analysis is a major issue in badminton as the effective usage of tactics is the key to win. The tactic in badminton is defined as a sequence of consecutive strokes. Most existing methods use statistical models to find sequential patterns of strokes and apply 2D visualizations such as glyphs and statistical charts to explore and analyze the discovered patterns. However, in badminton, spatial information like the shuttle trajectory, which is inherently 3D, is the core of a tactic. The lack of sufficient spatial awareness in 2D visualizations largely limited the tactic analysis of badminton. In this work, we collaborate with domain experts to study the tactic analysis of badminton in a 3D environment and propose an immersive visual analytics system, TIVEE, to assist users in exploring and explaining badminton tactics from multi-levels. Users can first explore various tactics from the third-person perspective using an unfolded visual presentation of stroke sequences. By selecting a tactic of interest, users can turn to the first-person perspective to perceive the detailed kinematic characteristics and explain its effects on the game result. The effectiveness and usefulness of TIVEE are demonstrated by case studies and an expert interview.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2021.3114861
ER  - 

TY  - NA
AU  - Wataru, Yamada; Manabe, Hiroyuki; Ikeda, Daizo
TI  - CHI - ZeRONE: Safety Drone with Blade-Free Propulsion
PY  - 2019
AB  - We present ZeRONE, a new indoor drone that does not use rotating blades for propulsion. The proposed device is a helium blimp type drone that uses the wind generated by the ultrasonic vibration of piezo elements for propulsion. Compared to normal drones with rotating propellers, the drone is much safer because its only moving parts are the piezo elements whose surfaces vibrate at the order of micrometers. The drone can float for a few weeks and the ultrasonic propulsion system is quiet. We implement a prototype of the drone and evaluate its performance and unique characteristics in experiments. Moreover, application scenarios in which ZeRONE coexists with people are also discussed.
SP  - 365
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300595
ER  - 

TY  - CHAP
AU  - Suzuki, Yutaro; Sekimori, Kodai; Yamato, Yuki; Yamasaki, Yusuke; Shizuki, Buntarou; Takahashi, Shin
TI  - HCI (2) - A Mouth Gesture Interface Featuring a Mutual-Capacitance Sensor Embedded in a Surgical Mask
PY  - 2020
AB  - We developed a mouth gesture interface featuring a mutual-capacitance sensor embedded in a surgical mask. This wearable hands-free interface recognizes non-verbal mouth gestures; others cannot eavesdrop on anything the user does with the user’s device. The mouth is hidden by the mask; others do not know what the user is doing. We confirm the feasibility of our approach and demonstrate the accuracy of mouth shape recognition. We present two applications. Mouth shape can be used to zoom in or out, or to select an application from a menu.
SP  - 154
EP  - 165
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_10
ER  - 

TY  - BOOK
AU  - Razi, Septian; Gardner, Henry; Adcock, Matt
TI  - VR Workshops - Immersive Pedigree Graph Visualisations
PY  - 2021
AB  - We describe a study of six visualisation methods for hierarchical pedigree data in virtual reality. We model six different pedigree graph layouts: planar, cylinder, floor, sphere, cone (force directed) and vase layouts. Measurements of task accuracy and task completion time showed a statistically significant pairwise comparison on two task completion times between the sphere (better) and floor (worse) conditions. Likert ratings of user sentiments showed a statistically significant main effect of graph condition ratings of the understandability of the data with the sphere and vase graph layouts to be generally higher and the floor layout to be generally lower.
SP  - 659
EP  - 660
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00212
ER  - 

TY  - JOUR
AU  - Higuchi, Shino; Oku, Hiromasa
TI  - Wide angular range dynamic projection mapping method applied to drone-based avatar robot
PY  - 2021
AB  - In this study, we propose a method for dynamic projection mapping on a target moving at high speed in a wide area around the projection equipment, using a high-speed gaze control system called Sacc...
SP  - 675
EP  - 684
JF  - Advanced Robotics
VL  - 35
IS  - 11
PB  - 
DO  - 10.1080/01691864.2021.1928550
ER  - 

TY  - NA
AU  - Becker, Vincent; Fessler, Linus; Sörös, Gábor
TI  - UbiComp - GestEar: combining audio and motion sensing for gesture recognition on smartwatches
PY  - 2019
AB  - We present GestEar, a gesture recognition method for sound-emitting gestures, such as snapping, knocking, or clapping, using only a simple smartwatch. Besides the motion information from the built-in accelerometer and gyroscope, we exploit audio data recorded by the smartwatch microphone as input. We propose a lightweight convolutional neural network architecture for gesture recognition, specifically designed to run locally on resource-constrained devices, which achieves a user-independent recognition accuracy of 97.2% for nine distinct gestures. We further show how to incorporate gesture detection and gesture classification in the same network, compare different network designs, and showcase a number of different applications built with our method. We find that the audio input drastically reduces the false positive rate in continuous recognition compared to using only motion.
SP  - 10
EP  - 19
JF  - Proceedings of the 23rd International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341163.3347735
ER  - 

TY  - NA
AU  - Agarwal, Mallika; Srinivasan, Arjun; Stasko, John
TI  - IEEE VIS (Short Papers) - VisWall: Visual Data Exploration Using Direct Combination on Large Touch Displays
PY  - 2019
AB  - An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.
SP  - 26
EP  - 30
JF  - 2019 IEEE Visualization Conference (VIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/visual.2019.8933673
ER  - 

TY  - NA
AU  - Köppel, Thomas; Gröller, M. Eduard; Wu, Hsiang-Yun
TI  - PacificVis - Context-Responsive Labeling in Augmented Reality
PY  - 2021
AB  - Route planning and navigation are common tasks that often require additional information on points of interest. Augmented Reality (AR) enables mobile users to utilize text labels, in order to provide a composite view associated with additional information in a real-world environment. Nonetheless, displaying all labels for points of interest on a mobile device will lead to unwanted overlaps between information, and thus a context-responsive strategy to properly arrange labels is expected. The technique should remove overlaps, show the right level-of-detail, and maintain label coherence. This is necessary as the viewing angle in an AR system may change rapidly due to users’ behaviors. Coherence plays an essential role in retaining user experience and knowledge, as well as avoiding motion sickness. In this paper, we develop an approach that systematically manages label visibility and levels-of-detail, as well as eliminates unexpected incoherent movement. We introduce three label management strategies, including (1) occlusion management, (2) level-of-detail management, and (3) coherence management by balancing the usage of the mobile phone screen. A greedy approach is developed for fast occlusion handling in AR. A level-of-detail scheme is adopted to arrange various types of labels. A 3D scene manipulation is then built to simultaneously suppress the incoherent behaviors induced by viewing angle changes. Finally, we present the feasibility and applicability of our approach through one synthetic and two real-world scenarios, followed by a qualitative user study.
SP  - 91
EP  - 100
JF  - 2021 IEEE 14th Pacific Visualization Symposium (PacificVis)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/pacificvis52677.2021.00020
ER  - 

TY  - BOOK
AU  - Lisle, Lee; Chen, Xiaoyu; Gitre, J.K. Edward; North, Chris; Bowman, Doug A.
TI  - VR Workshops - Evaluating the Benefits of the Immersive Space to Think
PY  - 2020
AB  - Sensemaking with large multimedia dataset is a cognitively intensive task that requires analysts to understand the underlying stories that the dataset tells. Often, analysts use tools in order to offload cognition as well as convey their new understanding of the dataset; however, existing tools are limited by their underlying technologies. We have proposed a novel virtual reality tool to support sensemaking called the Immersive Space to Think (IST). IST can aid the process of analyzing multimedia data, but it remains unproven whether IST improves sensemaking performance over a traditional desktop setting. In a study performed over six weeks, one participant used both IST and traditional methods of sensemaking with a dataset of 100 text documents of transcribed survey responses from World War 2 soldiers to perform historical analysis. The participant was asked guided questions that produced three essays with their understanding of the data. After conducting a blind evaluation of the participant’s interpretation of the data, a team of three experts in historical analysis concluded that the essays written with IST displayed a better understanding of the dataset. Furthermore, the participant gave positive feedback on IST, and also suggested possible improvements.
SP  - 331
EP  - 337
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00073
ER  - 

TY  - NA
AU  - Ueno, Michihiko; Satoh, Shin'ichi
TI  - IUI - Continuous and Gradual Style Changes of Graphic Designs with Generative Model
PY  - 2021
AB  - Creating a high-quality layout design from scratch is difficult for novices. Therefore, novices often consult the works of other skilled designers for ideas regarding layout designs. Researchers have previously investigated methods to support the layout design process; these works mainly focused on retrieval methods for similar layout designs, or refinement of existing layouts. To enhance user creativity in designing layouts, assistance is needed for exploring various designs. Herein, we propose a novel deep generative model that enables the generation of various layout designs and guarantees continuous and gradual changes in layouts, for effectively exploring graphic designs. Accordingly, we present an adversarial training method with dual critic networks; we trained our model by a public graphic design dataset. We developed another interaction method that allows the user to change the graphic designs between two different layout styles and categories parametrically. We demonstrated the efficacy of the proposed method in generating rich layout variations with representation of latent space by comparing the layout designs generated by our model with by an existing model.
SP  - 280
EP  - 289
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450666
ER  - 

TY  - NA
AU  - Petford, Julian; Carson, Iain; Nacenta, Miguel A.; Gutwin, Carl
TI  - CHI - A Comparison of Notification Techniques for Out-of-View Objects in Full-Coverage Displays
PY  - 2019
AB  - Full-coverage displays can place visual content anywhere on the interior surfaces of a room (e.g., a weather display near the coat stand). In these settings, digital artefacts can be located behind the user and out of their field of view - meaning that it can be difficult to notify the user when these artefacts need attention. Although much research has been carried out on notification, little is known about how best to direct people to the necessary location in room environments. We designed five diverse attention-guiding techniques for full-coverage display rooms, and evaluated them in a study where participants completed search tasks guided by the different techniques. Our study provides new results about notification in full-coverage displays: we showed benefits of persistent visualisations that could be followed all the way to the target and that indicate distance-to-target. Our findings provide useful information for improving the usability of interactive full-coverage environments.
SP  - 58
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300288
ER  - 

TY  - CHAP
AU  - Cardoso, Bruno; Cohn, Neil; Truyen, Frederik; Brosens, Koenraad
TI  - INTERACT (3) - Explore Data, Enjoy Yourself - KUbism, A Playful Approach to Data Exploration
PY  - 2021
AB  - The increasing relevance of information in today’s world has grown the challenge of accessible data exploration. Most existing approaches are primarily utilitarian, where the motivation for exploration remains mostly extrinsic, correlated to the value of the outcome. In light of our natural drives for exploration, we argue that data exploration can be made more rewarding and enjoyable via interfaces that leverage the intrinsic motivations of users. To that end, we present KUbism, a prototype emphasizing the hedonic value of data exploration through a playful interface built with elements of Minecraft gameplay. Implemented as a Terasology module, KUbism invites users to explore data in voxel worlds populated with data blocks. We conducted a user study (n = 41) to validate our approach in both hedonic and utilitarian dimensions. Our results demonstrate that KUbism allows effective data exploration while arousing curiosity and joy, thereby reinforcing the extrinsic value of data exploration with intrinsic rewards.
SP  - 43
EP  - 64
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85613-7_4
ER  - 

TY  - CHAP
AU  - Drap, Pierre; Papini, Odile; Merad, Djamal; Pasquet, Jérôme; Royer, Jean-Philip; Nawaf, Mohamad Motasem; Saccone, Mauro; Ellefi, Mohamed Ben; Chemisky, Bertrand; Seinturier, Julien; Sourisseau, Jean-Christophe; Gambin, Timmy; Castro, Filipe
TI  - Deepwater Archaeological Survey: An Interdisciplinary and Complex Process
PY  - 2019
AB  - This chapter introduces several state of the art techniques that could help to make deep underwater archaeological photogrammetric surveys easier, faster, more accurate, and to provide more visually appealing representations in 2D and 3D for both experts and public. We detail how the 3D captured data is analysed and then represented using ontologies, and how this facilitates interdisciplinary interpretation and cooperation. Towards more automation, we present a new method that adopts a deep learning approach for the detection and the recognition of objects of interest, amphorae for example. In order to provide more readable, direct and clearer illustrations, we describe several techniques that generate different styles of sketches out of orthophotos developed using neural networks. In the same direction, we present the Non-Photorealistic Rendering (NPR) technique, which converts a 3D model into a more readable 2D representation that is more useful to communicate and simplifies the identification of objects of interest. Regarding public dissemination, we demonstrate how recent advances in virtual reality to provide an accurate, high resolution, amusing and appropriate visualization tool that offers the public the possibility to ‘visit’ an unreachable archaeological site. Finally, we conclude by introducing the plenoptic approach, a new promising technology that can change the future of the photogrammetry by making it easier and less time consuming and that allows a user to create a 3D model using only one camera shot. Here, we introduce the concepts, the developing process, and some results, which we obtained with underwater imaging.
SP  - 135
EP  - 153
JF  - 3D Recording and Interpretation for Maritime Archaeology
VL  - 31
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-03635-5_9
ER  - 

TY  - NA
AU  - Prouzeau, Arnaud; Dharshini, M B; Balasubramaniam, Manivannan; Henry, Joshua; Hoang, Ngoc; Dwyer, Tim
TI  - BDVA - Visual Analytics for Energy Monitoring in the Context of Building Management
PY  - 2018
AB  - Building management systems (BMS) provide monitoring and control of most large-building assets (heating, ventilation, air conditioning, lighting, security systems, and so on). With the recent advancement of the Internet of Things and data management systems, BMS must gather and manage increasingly detailed data coming from a greater number and diversity of sources. The availability of such data should help building managers optimise the energy consumption of buildings. However, current BMS don't allow efficient visualisation of such data, which means that even if the data is available, it is not used to its full potential. In this paper, we describe a prototype BMS interface providing interactive visualisations of traditional building data (temperature, energy consumption), as well as more novel data (comfort feedback from occupants and live occupancy). We evaluate this prototype by first showing how it could be used to plan a long- term energy saving strategy, and then in a feedback session involving facility managers at a university.
SP  - 130
EP  - 138
JF  - 2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bdva.2018.8534026
ER  - 

TY  - NA
AU  - Baytas, Mehmet Aydin; Çay, Damla; Zhang, Yuchong; Obaid, Mohammad; Yantac, Asim Evren; Fjeld, Morten
TI  - CHI - The Design of Social Drones: A Review of Studies on Autonomous Flyers in Inhabited Environments
PY  - 2019
AB  - The design space of social drones, where autonomous flyers operate in close proximity to human users or bystanders, is distinct from use cases involving a remote human operator and/or an uninhabited environment; and warrants foregrounding human-centered design concerns. Recently, research on social drones has followed a trend of rapid growth. This paper consolidates the current state of the art in human-centered design knowledge about social drones through a review of relevant studies, scaffolded by a descriptive framework of design knowledge creation. Our analysis identified three high-level themes that sketch out knowledge clusters in the literature, and twelve design concerns which unpack how various dimensions of drone aesthetics and behavior relate to pertinent human responses. These results have the potential to inform and expedite future research and practice, by supporting readers in defining and situating their future contributions. The materials and results of our analysis are also published in an open online repository that intends to serve as a living hub for a community of researchers and designers working with social drones.
SP  - 250
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300480
ER  - 

TY  - JOUR
AU  - Wang, Yun; Gao, Yi; Huang, Ray; Cui, Weiwei; Zhang, Haidong; Zhang, Dongmei
TI  - Animated Presentation of Static Infographics with InfoMotion
PY  - 2021
AB  - NA
SP  - 507
EP  - 518
JF  - Computer Graphics Forum
VL  - 40
IS  - 3
PB  - 
DO  - 10.1111/cgf.14325
ER  - 

TY  - NA
AU  - Miyafuji, Shio; Toyohara, Soichiro; Sato, Toshiki; Koike, Hideki
TI  - SIGGRAPH Posters - Remote control experiment with displaybowl and 360-degree video
PY  - 2019
AB  - DisplayBowl is a bowl-shaped hemispherical display for showing omnidirectional images with direction data. It provides users with a novel way of observing 360-degree video streams, which improves the awareness of the surroundings when operating a remote-controlled vehicle compared to conventional flat displays and HMDs. In this paper, we present a user study, in which we asked participants to control a remote drone using an omnidirectional video streaming, to compare the uniqueness and advantages of three displays: a flat panel display, a head-mounted display and DisplayBowl.
SP  - 62
EP  - NA
JF  - ACM SIGGRAPH 2019 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3306214.3338568
ER  - 

TY  - NA
AU  - Lindlbauer, David; Wilson, Andrew D.
TI  - CHI - Remixed Reality: Manipulating Space and Time in Augmented Reality
PY  - 2018
AB  - We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.
SP  - 129
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173703
ER  - 

TY  - NA
AU  - Chang, Chunlei; Dwyer, Tim; Marriott, Kim
TI  - PacificVis - An Evaluation of Perceptually Complementary Views for Multivariate Data
PY  - 2018
AB  - We evaluate the relative merits of three techniques for visualising multivariate data: parallel coordinates; scatterplot matrix; and a side-by-side, coordinated combination of these views. In particular, we report on: (1) the most effective visual encoding of multivariate data for each of the six common tasks considered; (2) common strategies that our participants used when the two views were combined based on eye-tracking data analysis; (3) the finding that these views are perceptually complementary in the sense that they both show the same information, but with different and complementary support for different types of analysis. For the combined view, our studies show that there is a perceptually complementary effect in terms of significantly improved accuracy for certain tasks, but that there is a small cost in terms of slightly longer completion time than the faster of the two techniques alone. Eye-movement data shows that for many tasks participants were able to swiftly switch their strategies after trying both in the training phase.
SP  - 195
EP  - 204
JF  - 2018 IEEE Pacific Visualization Symposium (PacificVis)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/pacificvis.2018.00033
ER  - 

TY  - NA
AU  - Vyas, Preeti; Taha, Feras Al; Blum, Jeffrey R.; Cooperstock, Jeremy R.
TI  - HAPTICS - HapToes: Vibrotactile Numeric Information Delivery via Tactile Toe Display
PY  - 2020
AB  - Tactile rendering of numeric information via a single actuator has been considered for such purposes as fitness progress tracking. However, multi-actuator designs, leveraging spatial mapping, may offer superior performance. Motivated to explore this approach without requiring hardware on the fingers or wrist, we designed HapToes, a novel ten-digit spatial mapping of numeric information to the toes, which overcomes inter-toe discrimination ambiguity. Compared to ActiVibe, a single-actuator wrist-based numeric rendering technique, under similar distraction conditions, HapToes demonstrates equivalent performance for single-value identification, and improved accuracy, response time, and cognitive load when conveying three values sequentially in a single message.
SP  - 61
EP  - 67
JF  - 2020 IEEE Haptics Symposium (HAPTICS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/haptics45997.2020.ras.hap20.34.8ad689d4
ER  - 

TY  - NA
AU  - Mejjati, Youssef Alami; Gomez, Celso F.; Kim, Kwang In; Shechtman, Eli; Bylinskii, Zoya
TI  - Look here! A parametric learning based approach to redirect visual attention
PY  - 2020
AB  - Across photography, marketing, and website design, being able to direct the viewer's attention is a powerful tool. Motivated by professional workflows, we introduce an automatic method to make an image region more attention-capturing via subtle image edits that maintain realism and fidelity to the original. From an input image and a user-provided mask, our GazeShiftNet model predicts a distinct set of global parametric transformations to be applied to the foreground and background image regions separately. We present the results of quantitative and qualitative experiments that demonstrate improvements over prior state-of-the-art. In contrast to existing attention shifting algorithms, our global parametric approach better preserves image semantics and avoids typical generative artifacts. Our edits enable inference at interactive rates on any image size, and easily generalize to videos. Extensions of our model allow for multi-style edits and the ability to both increase and attenuate attention in an image region. Furthermore, users can customize the edited images by dialing the edits up or down via interpolations in parameter space. This paper presents a practical tool that can simplify future image editing pipelines.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Liao, Yi-Chi; Kim, Sunjun; Lee, Byungjoo; Oulasvirta, Antti
TI  - Button Simulation and Design via FDVV Models.
PY  - 2020
AB  - Designing a push-button with desired sensation and performance is challenging because the mechanical construction must have the right response characteristics. Physical simulation of a button's force-displacement (FD) response has been studied to facilitate prototyping; however, the simulations' scope and realism have been limited. In this paper, we extend FD modeling to include vibration (V) and velocity-dependence characteristics (V). The resulting FDVV models better capture tactility characteristics of buttons, including snap. They increase the range of simulated buttons and the perceived realism relative to FD models. The paper also demonstrates methods for obtaining these models, editing them, and simulating accordingly. This end-to-end approach enables the analysis, prototyping, and optimization of buttons, and supports exploring designs that would be hard to implement mechanically.
SP  - 3376262
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376262
ER  - 

TY  - NA
AU  - Mirri, Silvia; Prandi, Catia; Salomoni, Paola
TI  - MAGESys@SIGCOMM - Human-Drone Interaction: state of the art, open issues and challenges
PY  - 2019
AB  - In the evolution of the Human-Computer Interaction discipline, it is interesting to evaluate the users' experience within the interaction between users and a specific category of robots, which are characterized by peculiar features: the unmanned aerial vehicles (UAVs). Drones are becoming more and more diffused, being used with different purposes. Hence, the interaction with these devices is getting common and here we aim at investigating how they can be exploited by means of different users' interfaces and with different interaction mechanisms.In this paper, we present a review of the state of the art in the context of the human-drone interaction, so as to study and discuss the main open issues and challenges currently highlighted and reported in research projects and papers available in the current literature.
SP  - 43
EP  - 48
JF  - Proceedings of the ACM SIGCOMM 2019 Workshop on Mobile AirGround Edge Computing, Systems, Networks, and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341568.3342111
ER  - 

TY  - NA
AU  - Su, Yu-Chuan; Grauman, Kristen
TI  - CVPR Workshops - Kernel Transformer Networks for Compact Spherical Convolution
PY  - 2019
AB  - Ideally, 360° imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. We present the Kernel Transformer Network (KTN) to efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360° images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360° image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN’s accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint.
SP  - 9442
EP  - 9451
JF  - 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr.2019.00967
ER  - 

TY  - NA
AU  - Tigwell, Garreth W.; Gorman, Benjamin M.; Menzies, Rachel
TI  - CHI - Emoji Accessibility for Visually Impaired People
PY  - 2020
AB  - Emoji are graphical symbols that appear in many aspects of our lives. Worldwide, around 36 million people are blind and 217 million have a moderate to severe visual impairment. This portion of the population may use and encounter emoji, yet it is unclear what accessibility challenges emoji introduce. We first conducted an online survey with 58 visually impaired participants to understand how they use and encounter emoji online, and the challenges they experience. We then conducted 11 interviews with screen reader users to understand more about the challenges reported in our survey findings. Our interview findings demonstrate that technology is both an enabler and a barrier, emoji descriptors can hinder communication, and therefore the use of emoji impacts social interaction. Using our findings from both studies, we propose best practice when using emoji and recommendations to improve the future accessibility of emoji for visually impaired people.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376267
ER  - 

TY  - BOOK
AU  - Prouzeau, Arnaud; Lhuillier, Antoine; Ens, Barrett; Weiskopf, Daniel; Dwyer, Tim
TI  - ISS - Visual Link Routing in Immersive Visualisations
PY  - 2019
AB  - In immersive display environments, such as virtual or augmented reality, we can make explicit the connections between data points in visualisations and their context in the world, or in other visualisations. This paper considers the requirements and design space for drawing such links in order to minimise occlusion and clutter. A novel possibility in immersive environments is to optimise the link layout with respect to a particular point of view. In collaborative scenarios there is the need to do this for multiple points of view. We present an algorithm to achieve such link layouts and demonstrate its applicability in a variety of practical use cases.
SP  - 241
EP  - 253
JF  - Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343055.3359709
ER  - 

TY  - JOUR
AU  - Wu, Aoyu; Wang, Yun; Zhou, Mengyu; He, Xinyi; Zhang, Haidong; Qu, Huamin; Zhang, Dongmei
TI  - MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation.
PY  - 2021
AB  - We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2021.3114826
ER  - 

TY  - NA
AU  - Do, Youngwook; Hoang, Linh Thai; Park, Jung Wook; Abowd, Gregory D.; Das, Sauvik
TI  - Conference on Designing Interactive Systems - Spidey Sense: Designing Wrist-Mounted Affective Haptics for Communicating Cybersecurity Warnings
PY  - 2021
AB  - Improving end-users’ awareness of cybersecurity warnings (e.g., phishing and malware alerts) remains a longstanding problem in usable security. Prior work suggests two key weaknesses with existing warnings: they are primarily communicated via saturated communication channels (e.g., visual, auditory, and vibrotactile); and, they are communicated rationally, not viscerally. We hypothesized that wrist-based affective haptics should address both of these weaknesses in a form-factor that is practically deployable: i.e., as a replaceable wristband compatible with modern smartwatches like the Apple Watch. To that end, we designed and implemented Spidey Sense, a wristband that produces customizable squeezing sensations to alert users to urgent cybersecurity warnings. To evaluate Spidey Sense, we applied a three-phased ‘Gen-Rank-Verify’ study methodology with 48 participants. We found evidence that, relative to vibrotactile alerts, Spidey Sense was considered more appropriate for the task of alerting people to cybersecurity warnings.
SP  - 125
EP  - 137
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462027
ER  - 

TY  - NA
AU  - Mikawa, Yuri; Sueishi, Tomohiro; Watanabe, Yoshihiro; Ishikawa, Masatoshi
TI  - ICME - Projection Mapping System To A Widely Dynamic Sphere With Circumferential Markers
PY  - 2020
AB  - Image projection on spheres and their surroundings have been researched for all-around display and motion visualization. However, wide-range projection onto a dynamic sphere can suffer from tracking errors and projection latency. We propose a high-speed projection mapping system for dynamic spheres, and circumferential markers for sphere posture estimation. Marker detection based on ellipse appearance can easily start tracking, and is robust against interactive occlusion; therefore, the system enables projection mapping and rotational visualization for a dynamically moving sphere. We experimentally confirmed sufficient pose estimation accuracy of the circumferential markers compared with initialized dot markers, and sufficient tracking ability against initialization, occlusion, and depth-direction movement. Demonstrations showed accurate, rotation-visualized projection onto a dynamic sphere, which is useful for sports practice applications.
SP  - 1
EP  - 6
JF  - 2020 IEEE International Conference on Multimedia and Expo (ICME)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icme46284.2020.9102813
ER  - 

TY  - JOUR
AU  - Dwyer, Tim; Cordeil, Maxime; Czauderna, Tobias; Haghighi, Pari Delir; Ens, Barrett; Goodwin, Sarah; Jenny, Bernhard; Marriott, Kim; Wybrow, Michael
TI  - The Data Visualisation and Immersive Analytics Research Lab at Monash University
PY  - 2020
AB  - Abstract This article reviews two decades of research in topics in Information Visualisation emerging from the Data Visualisation and Immersive Analytics Lab at Monash University Australia (Monash IA Lab). The lab has been influential with contributions in algorithms, interaction techniques and experimental results in Network Visualisation, Interactive Optimisation and Geographic and Cartographic visualisation. It has also been a leader in the emerging topic of Immersive Analytics, which explores natural interactions and immersive display technologies in support of data analytics. We reflect on advances in these areas but also sketch our vision for future research and developments in data visualisation more broadly.
SP  - 41
EP  - 49
JF  - Visual Informatics
VL  - 4
IS  - 4
PB  - 
DO  - 10.1016/j.visinf.2020.11.001
ER  - 

TY  - NA
AU  - Furumoto, Takuro; Kasai, Takumi; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - UIST - Midair Balloon Interface: A Soft and Lightweight Midair Object for Proximate Interactions
PY  - 2021
AB  - This paper introduces a midair balloon interface, a fast and soft interactive object in mid-air. Our approach tackles the trade-off between safety and speed by controlling a soft helium-filled balloon with external actuators and sensors. We developed a prototype system that uses airborne ultrasound phased arrays to propel a balloon and high-speed stereo cameras to track its motion. This configuration realizes both a high thrust/weight ratio and such a soft body that is safe-to-collide. We describe a sight-based interaction and a touch-based interaction that leverage the safety and speed of a midair balloon interface. A sight-based interaction allows the user to keep the object inside her/his view within reach by controlling a balloon to follow the direction of the user’s face. A touch-based interaction allows the user to manipulate the object directly with his/her hand, issue a command by moving his/her finger on the surface, and receive vibrotactile feedback produced by vibrating the balloon with amplitude-modulated ultrasound. We describe the implementation and evaluation of the prototype and explore the application scenarios.
SP  - 783
EP  - 795
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474786
ER  - 

TY  - NA
AU  - Saffo, David; Di Bartolomeo, Sara; Yildirim, Caglar; Dunne, Cody
TI  - Two Dimensions for Organizing Immersive Analytics: Toward a Taxonomy for Facet and Position.
PY  - 2020
AB  - As immersive analytics continues to grow as a discipline, so too should its underlying methodological support. Taxonomies play an important role for information visualization and human computer interaction. They provide an organization of the techniques used in a particular domain that better enable researchers to describe their work, discover existing methods, and identify gaps in the literature. Existing taxonomies in related fields do not capture or describe the unique paradigms employed in immersive analytics. We conceptualize a taxonomy that organizes immersive analytics according to two dimensions: spatial and visual presentation. Each intersection of this taxonomy represents a unique design paradigm which, when thoroughly explored, can aid in the design and research of new immersive analytic applications.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhu, Kening; Chen, Taizhou; Han, Feng; Wu, Yi-Shiun
TI  - CHI - HapTwist: Creating Interactive Haptic Proxies in Virtual Reality Using Low-cost Twistable Artefacts
PY  - 2019
AB  - In this paper, we present a series of studies on using Rubik's Twist, a type of low-cost twistable artefact, to create haptic proxies for various hand-graspable VR objects. Our pilot studies validated the feasibility and effectiveness of Rubik's-Twist-based haptic proxies. The pilot results also revealed user challenges in the physical shape creation, motivating the development of the HapTwist toolkit. The toolkit consists of the shape-generation algorithm, the software interface for shape-construction guidance and interaction authoring, and the hardware modules for constructing interactive haptic proxies. The user studies showed that HapTwist was easy to learn and use, and it significantly improved user performance in creating interactive haptic proxies with Rubik's Twist. Furthermore, HapTwist-generated haptic proxies achieved similar VR experience as the real objects.
SP  - 693
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300923
ER  - 

TY  - JOUR
AU  - Sathya, Anup; Li, Jiasheng; Rahman, Tauhidur; Gao, Ge; Peng, Huaishu
TI  - Calico
PY  - 2022
AB  - <jats:p>We explore Calico, a miniature relocatable wearable system with fast and precise locomotion for on-body interaction, actuation and sensing. Calico consists of a two-wheel robot and an on-cloth track mechanism or "railway," on which the robot travels. The robot is self-contained, small in size, and has additional sensor expansion options. The track system allows the robot to move along the user's body and reach any predetermined location. It also includes rotational switches to enable complex routing options when diverging tracks are presented. We report the design and implementation of Calico with a series of technical evaluations for system performance. We then present a few application scenarios, and user studies to understand the potential of Calico as a dance trainer and also explore the qualitative perception of our scenarios to inform future research in this space.</jats:p>
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550323
ER  - 

TY  - NA
AU  - Wu, Aoyu; Xie, Liwenhan; Lee, Bongshin; Wang, Yun; Cui, Weiwei; Qu, Huamin
TI  - Learning to Automate Chart Layout Configurations Using Crowdsourced Paired Comparison
PY  - 2021
AB  - We contribute a method to automate parameter configurations for chart layouts by learning from human preferences. Existing charting tools usually determine the layout parameters using predefined heuristics, producing sub-optimal layouts. People can repeatedly adjust multiple parameters (e.g., chart size, gap) to achieve visually appealing layouts. However, this trial-and-error process is unsystematic and time-consuming, without a guarantee of improvement. To address this issue, we develop Layout Quality Quantifier (LQ2), a machine learning model that learns to score chart layouts from pairwise crowdsourcing data. Combined with optimization techniques, LQ2 recommends layout parameters that improve the charts' layout quality. We apply LQ2 on bar charts and conduct user studies to evaluate its effectiveness by examining the quality of layouts it produces. Results show that LQ2 can generate more visually appealing layouts than both laypeople and baselines. This work demonstrates the feasibility and usages of quantifying human preferences and aesthetics for chart layouts.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Smith, Ross T.; Clarke, Thomas J.; Mayer, Wolfgang; Cunningham, Andrew; Matthews, Brandon J.; Zucco, Joanne E.
TI  - Mixed Reality Interaction and Presentation Techniques for Medical Visualisations.
PY  - 2020
AB  - Mixed, Augmented and Virtual reality technologies are burgeoning with new applications and use cases appearing rapidly. This chapter provides a brief overview of the fundamental display presentation methods; head-worn, hand-held and projector-based displays. We present a summary of visualisation methods that employ these technologies in the medical domain with a diverse range of examples presented including diagnostic and exploration, intervention and clinical, interaction and gestures, and education.
SP  - 123
EP  - 139
JF  - Advances in experimental medicine and biology
VL  - 1260
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-47483-6_7
ER  - 

TY  - NA
AU  - Seraji, Mohammad Rajabi; Stuerzlinger, Wolfgang
TI  - XVCollab: An Immersive Analytics Tool for Asymmetric Collaboration across the Virtuality Spectrum
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00035
ER  - 

TY  - CHAP
AU  - Belk, Marios; Fidas, Christos; Katsi, Eleni; Constantinides, Argyris; Pitsillides, Andreas
TI  - INTERACT (4) - An Empirical Study of Picture Password Composition on Smartwatches.
PY  - 2021
AB  - Recent research works suggest that human cognitive differences affect security and usability of picture passwords within a variety of interaction contexts, such as conventional desktops, smartphones, and extended reality. However, the interplay of human cognition towards users’ interaction behavior and security of picture passwords on smartwatch devices has not been investigated so far. In this paper, we report on such a research attempt that embraced a between-subjects in-lab user study (n = 50) in which users were classified according to their cognitive processing characteristics (i.e., Field Dependence-Independence cognitive differences), and further composed a picture password on a smartwatch device. Analysis of results reveal that already known effects of human cognition towards interaction behavior and security of picture passwords within conventional interaction contexts, do not necessarily replicate when these are deployed on smartwatch devices. Findings point towards the need to design for diversity and device-aware picture password schemes.
SP  - 655
EP  - 664
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85610-6_37
ER  - 

TY  - JOUR
AU  - Barrera-Leon, Luisa; Corno, Fulvio; De Russis, Luigi
TI  - How the Preattentive Process is Exploited in Practical Information Visualization Design: A Review
PY  - 2022
AB  - NA
SP  - 707
EP  - 720
JF  - International Journal of Human–Computer Interaction
VL  - 39
IS  - 4
PB  - 
DO  - 10.1080/10447318.2022.2049137
ER  - 

TY  - NA
AU  - Endo, Isamu; Takashima, Kazuki; Inoue, Maakito; Fujita, Kazuyuki; Kiyokawa, Kiyoshi; Kitamura, Yoshifumi
TI  - UIST - ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World
PY  - 2021
AB  - We propose ModularHMD, a new mobile head-mounted display concept, which adopts a modular mechanism and allows a user to perform ad-hoc peripheral interaction with real-world devices or people during VR experiences. ModularHMD is comprised of a central HMD and three removable module devices installed in the periphery of the HMD cowl. Each module has four main states: occluding, extended VR view, video see-through (VST), and removed/reused. Among different combinations of module states, a user can quickly setup the necessary HMD forms, functions, and real-world visions for ad-hoc peripheral interactions without removing the headset. For instance, an HMD user can see her surroundings by switching a module into the VST mode. She can also physically remove a module to obtain direct peripheral visions of the real world. The removed module can be reused as an instant interaction device (e.g., touch keyboards) for subsequent peripheral interactions. Users can end the peripheral interaction and revert to a full VR experience by re-mounting the module. We design ModularHMD’s configuration and peripheral interactions with real-world objects and people. We also implement a proof-of-concept prototype of ModularHMD to validate its interactions capabilities through a user study. Results show that ModularHMD is an effective solution that enables both immersive VR and ad-hoc peripheral interactions.
SP  - 100
EP  - 117
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474738
ER  - 

TY  - JOUR
AU  - Sun, Maoyuan; Namburi, Akhil; Koop, David; Zhao, Jian; Li, Tianyi; Chung, Haeyong
TI  - Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships.
PY  - 2022
AB  - Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this article, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.
SP  - 4741
EP  - 4756
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3102966
ER  - 

TY  - JOUR
AU  - Shin, Sungbok; Chung, Sunghyo; Hong, Sanghyun; Elmqvist, Niklas
TI  - A Scanner Deeply: Predicting Gaze Heatmaps On Visualizations Using Crowdsourced Eye Movement Data.
PY  - 2022
AB  - Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a SCANNER DEEPLY, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper's contribution.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209472
ER  - 

TY  - CONF
AU  - Cui, Wenzhe; Zhu, Suwen; Li, Zhi; Xu, Zheer; Yang, Xing-Dong; Ramakrishnan, I. V.; Bi, Xiaojun
TI  - CHI - BackSwipe: Back-of-device Word-Gesture Interaction on Smartphones
PY  - 2021
AB  - Back-of-device interaction is a promising approach to interacting on smartphones. In this paper, we create a back-of-device command and text input technique called BackSwipe, which allows a user to hold a smartphone with one hand, and use the index finger of the same hand to draw a word-gesture anywhere at the back of the smartphone to enter commands and text. To support BackSwipe, we propose a back-of-device word-gesture decoding algorithm which infers the keyboard location from back-of-device gestures, and adjusts the keyboard size to suit the gesture scales; the inferred keyboard is then fed back into the system for decoding. Our user study shows BackSwipe is feasible and a promising input method, especially for command input in the one-hand holding posture: users can enter commands at an average accuracy of 92% with a speed of 5.32 seconds/command. The text entry performance varies across users. The average speed is 9.58 WPM with some users at 18.83 WPM; the average word error rate is 11.04% with some users at 2.85%. Overall, BackSwipe complements the extant smartphone interaction by leveraging the back of the device as a gestural input surface.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wilhelm, Mathias; Krakowczyk, Daniel; Albayrak, Sahin
TI  - PeriSense: Ring-Based Multi-Finger Gesture Interaction Utilizing Capacitive Proximity Sensing.
PY  - 2020
AB  - Rings are widely accepted wearables for gesture interaction. However, most rings can sense only the motion of one finger or the whole hand. We present PeriSense, a ring-shaped interaction device enabling multi-finger gesture interaction. Gestures of the finger wearing ring and its adjacent fingers are sensed by measuring capacitive proximity between electrodes and human skin. Our main contribution is the determination of PeriSense's interaction space involving the evaluation of capabilities and limitations. We introduce a prototype named PeriSense, analyze the sensor resolution at different distances, and evaluate finger gestures and unistroke gestures based on gesture sets allowing the determination of the strengths and limitations. We show that PeriSense is able to sense the change of conductive objects reliably up to 2.5 cm. Furthermore, we show that this capability enables different interaction techniques such as multi-finger gesture recognition or two-handed unistroke input.
SP  - 3990
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 14
PB  - 
DO  - 10.3390/s20143990
ER  - 

TY  - NA
AU  - E, Jane L.; Fried, Ohad; Lu, Jingwan; Zhang, Jianming; Mech, Radomir; Echevarria, Jose; Hanrahan, Pat; Landay, James A.
TI  - CHI - Adaptive Photographic Composition Guidance
PY  - 2020
AB  - Photographic composition is often taught as alignment with composition grids-most commonly, the rule of thirds. Professional photographers use more complex grids, like the harmonic armature, to achieve more diverse dynamic compositions. We are interested in understanding whether these complex grids are helpful to amateurs. In a formative study, we found that overlaying the harmonic armature in the camera can help less experienced photographers discover and achieve different compositions, but it can also be overwhelming due to the large number of lines. Photographers actually use subsets of lines from the armature to explain different aspects of composition. However, this occurs mainly offline to analyze existing images. We propose bringing this mental model into the camera-by adaptively highlighting relevant lines to the current scene and point of view. We describe a saliency-based algorithm for selecting these lines and present an evaluation of the system that shows that photographers found the proposed adaptive armatures helpful for capturing more well-composed images.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376635
ER  - 

TY  - JOUR
AU  - Wagner, Jorge; Stuerzlinger, Wolfgang; Nedel, Luciana
TI  - Comparing and Combining Virtual Hand and Virtual Ray Pointer Interactions for Data Manipulation in Immersive Analytics
PY  - 2021
AB  - In this work, we evaluate two standard interaction techniques for Immersive Analytics environments: virtual hands, with actions such as grabbing and stretching, and virtual ray pointers, with actions assigned to controller buttons. We also consider a third option: seamlessly integrating both modes and allowing the user to alternate between them without explicit mode switches. Easy-to-use interaction with data visualizations in Virtual Reality enables analysts to intuitively query or filter the data, in addition to the benefit of multiple perspectives and stereoscopic 3D display. While many VR-based Immersive Analytics systems employ one of the studied interaction modes, the effect of this choice is unknown. Considering that each has different advantages, we compared the three conditions through a controlled user study in the spatio-temporal data domain. We did not find significant differences between hands and ray-casting in task performance, workload, or interactivity patterns. Yet, 60% of the participants preferred the mixed mode and benefited from it by choosing the best alternative for each low-level task. This mode significantly reduced completion times by 23% for the most demanding task, at the cost of a 5% decrease in overall success rates.
SP  - 2513
EP  - 2523
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067759
ER  - 

TY  - NA
AU  - Yamaguchi, Shoma; Ogawa, Nami; Narumi, Takuji
TI  - ISMAR - Now I’m Not Afraid: Reducing Fear of Missing Out in 360° Videos on a Head-Mounted Display using a Panoramic Thumbnail
PY  - 2021
AB  - Cinematic virtual reality, or 360° video, provides viewers with an immersive experience, allowing them to enjoy a video while moving their head to watch in any direction. However, there is an inevitable problem of feeling fear of missing out (FOMO) when viewing a 360° video, as only a part of the video is visible to the viewer at any given time. To solve this problem, we developed a technique to present a panoramic thumbnail of a full 360° video to users through a head-mounted display. With this technique, the user can grasp the overall view of the video as needed. We conducted an experiment to evaluate the FOMO, presence, and quality of viewing experience while using this technique compared to normal viewing without it. The results of the experiment show that the proposed technique relieved FOMO, the quality of viewing experience was improved, and there was no difference in presence. We also investigated how users interacted with this new interface based on eye tracking and head tracking data during viewing, which suggested that users used the panoramic thumbnail to actively explore outside their field of view.
SP  - 176
EP  - 183
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00032
ER  - 

TY  - NA
AU  - Su, Yu-Chuan; Grauman, Kristen
TI  - Kernel Transformer Networks for Compact Spherical Convolution.
PY  - 2018
AB  - Ideally, 360° imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. In this work, we present the Kernel Transformer Network (KTN). KTNs efficiently transfer convolution kernels from perspective images to the equirectangular projection of 360° images. Given a source CNN for perspective images as input, the KTN produces a function parameterized by a polar angle and kernel as output. Given a novel 360° image, that function in turn can compute convolutions for arbitrary layers and kernels as would the source CNN on the corresponding tangent plane projections. Distinct from all existing methods, KTNs allow model transfer: the same model can be applied to different source CNNs with the same base architecture. This enables application to multiple recognition tasks without re-training the KTN. Validating our approach with multiple source CNNs and datasets, we show that KTNs improve the state of the art for spherical convolution. KTNs successfully preserve the source CNN's accuracy, while offering transferability, scalability to typical image resolutions, and, in many cases, a substantially lower memory footprint.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Fujino, Keita; Inoue, Sozo; Shibata, Tomohiro
TI  - MobiCASE - Machine Learning of User Attentions in Sensor Data Visualization
PY  - 2018
AB  - In this paper, we propose a method for automatically estimating important points of large sensor data by collecting attention points of the user when visualized, and applying a supervised machine-learning algorithm. For large-scale sensor data, it is difficult to find important points simply through visualization, because such points are buried in a large scope of visualization. We also provide the results of an estimation, the accuracy of which was over 80% for multiple visualizations. In addition, the method has the advantage that the trained model can be reused to any other visualization from the same type of the sensors. We show the results of such reusability for the new type of visualization, which achieved an accuracy rate of 70–80%.
SP  - 125
EP  - 143
JF  - Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-90740-6_8
ER  - 

TY  - NA
AU  - Oyama, Shinji; Yamasaki, Toshihiko
TI  - BigMM - Visual Clarity Analysis and Improvement Support for Presentation Slides
PY  - 2019
AB  - Presentation slides offer effective ways to deliver information in various fields. It has become easier to create slides owing to advanced presentation software such as PowerPoint. However, novices still face difficulty in designing slides that are easily comprehensible, as few slide evaluation methods exist that can objectively judge the quality of slides. In this paper, we analyze the features extracted from slides and tackle a simple classification problem, i.e., whether the input single-page slide is easy to understand. For evaluation, we created a new dataset of 1,000 PowerPoint slides with visual clarity label by using a crowdsourcing service. Using the 30% of the slides with high/low clarity rating, we achieved an accurate classification rate of 90.3%. We further proposed a feedback system that supports the improvement of slide designs. User study demonstrates that our system, which uses feedback on visual clarity scores and areas that should be modified, effectively supports slide improvement.
SP  - 421
EP  - 428
JF  - 2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bigmm.2019.00025
ER  - 

TY  - NA
AU  - Goguey, Alix; Steer, Cameron; Lucero, Andrés; Nigay, Laurence; Sahoo, Deepak Ranjan; Coutrix, Céline; Roudaut, Anne; Subramanian, Sriram; Tokuda, Yutaka; Neate, Timothy; Pearson, Jennifer; Robinson, Simon; Jones, Matt
TI  - CHI - PickCells: A Physically Reconfigurable Cell-composed Touchscreen
PY  - 2019
AB  - Touchscreens are the predominant medium for interactions with digital services; however, their current fixed form factor narrows the scope for rich physical interactions by limiting interaction possibilities to a single, planar surface. In this paper we introduce the concept of PickCells, a fully re-configurable device concept composed of cells, that breaks the mould of rigid screens and explores a modular system that affords rich sets of tangible interactions and novel across-device relationships. Through a series of co-design activities -- involving HCI experts and potential end-users of such systems -- we synthesised a design space aimed at inspiring future research, giving researchers and designers a framework in which to explore modular screen interactions. The design space we propose unifies existing works on modular touch surfaces under a general framework and broadens horizons by opening up unexplored spaces providing new interaction possibilities. In this paper, we present the PickCells concept, a design space of modular touch surfaces, and propose a toolkit for quick scenario prototyping.
SP  - 273
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300503
ER  - 

TY  - JOUR
AU  - Nsugbe, Ejay
TI  - A pilot exploration on the use of NIR monitored haemodynamics in gesture recognition for transradial prosthesis control
PY  - 2021
AB  - Abstract The application of near infrared (NIR) sensing for gesture recognition carries a high appeal as it presents a haemodynamic monitoring method, which provides an alternative to the conventional electromyography sensing method for prosthesis control, which is limited by muscle fatigue and is said to be expensive depending on the specifications. Using an affordable (£25), low sampling and high resolution wearable NIR sensor, a feature selection exercise was conducted where 23 signal features were extracted from the acquired signal and downselected to an optimal 11, which allow for gesture recognition while enhancing recognition accuracy. The selected features were tested and compared with a reduced feature set with four different classifiers, namely multilayer perceptron neural network (MLPNN), Bayesian classifier (BC), linear and quadratic discriminant analysis (LDA and QDA) across four different gestures in five non-amputated participants. The best improvement in classification was produced with the MLPNN and QDA, owing to their overall model complexity and ability to separate data clusters using nonlinear decision boundaries, therein validating the candidate list of features which could be used to characterise and extract more information from haemodynamic based signals. Further work in this area can include an exercise to determine the optimal spacing of the NIR emitters and receivers to allow for maximal penetrative depth and therein an increased amount of physiological information acquired in the signal, and an investigation to observe the extent which anatomical contraction force can be differentiated using NIR.
SP  - 200045
EP  - NA
JF  - Intelligent Systems with Applications
VL  - 9
IS  - NA
PB  - 
DO  - 10.1016/j.iswa.2021.200045
ER  - 

TY  - JOUR
AU  - Kumagai, Kota; Miura, Shun; Hayasaki, Yoshio
TI  - Colour volumetric display based on holographic-laser-excited graphics using drawing space separation.
PY  - 2021
AB  - A volumetric display generates a graphics that can be viewed from 360 $$^{\circ }$$ by representing the 3D information of an object as voxels in physical space. However, the natural properties of physical objects, such as 3D information and colors, and the seamless relationships between graphics and humans make it difficult to implement such displays. Here, we introduce a novel system that combines the spatial generation of femtosecond-laser-excited emission points using computer-generated holograms and beam scanning with the drawing space separation method. We demonstrate the drawing of volumetric graphics that can be color-expressed in voxel units in the air. This system enables the drawing of volumetric graphics in the air, accurate color representations, and robust graphics that are not destroyed by contact with users or objects. It also lays the foundation for the implementation of future volumetric displays.
SP  - 22728
EP  - NA
JF  - Scientific reports
VL  - 11
IS  - 1
PB  - 
DO  - 10.1038/s41598-021-02107-3
ER  - 

TY  - NA
AU  - Wang, Xi; Bylinskii, Zoya; Castelhano, Monica S.; Hillis, James M.; Duchowski, Andrew T.
TI  - CHI Extended Abstracts - EMICS'20: Eye Movements as an Interface to Cognitive State
PY  - 2020
AB  - Eye movement recording has been extensively used in HCI and offers the possibility to understand how information is perceived and processed by users. Hardware developments provide the ubiquitous accessibility of eye recording, allowing eye movements to enter common usage as a control modality. Recent A.I. developments provide powerful computational means to make predictions about the user. However, the connection between eye movements and cognitive state has been largely under-exploited in HCI. Despite the rich literature in psychology, a deeper understanding of its usability in practice is still required. This EMICS SIG will provide an opportunity to discuss possible application scenarios and HCI interfaces to infer users' mental state from eye movements. It will bring together researchers across disciplines with the goal of expanding shared knowledge, discussing innovative research directions and methods, fostering future collaborations around the use of eye movements as an interface to cognitive state, and providing a solid foundation for an EMICS workshop at CHI 2021.
SP  - 1
EP  - 4
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3381062
ER  - 

TY  - NA
AU  - Yeo, Hui-Shyong; Wu, Erwin; Lee, Juyoung; Quigley, Aaron; Koike, Hideki
TI  - UIST - Opisthenar: Hand Poses and Finger Tapping Recognition by Observing Back of Hand Using Embedded Wrist Camera
PY  - 2019
AB  - We introduce a vision-based technique to recognize static hand poses and dynamic finger tapping gestures. Our approach employs a camera on the wrist, with a view of the opisthenar (back of the hand) area. We envisage such cameras being included in a wrist-worn device such as a smartwatch, fitness tracker or wristband. Indeed, selected off-the-shelf smartwatches now incorporate a built-in camera on the side for photography purposes. However, in this configuration, the fingers are occluded from the view of the camera. The oblique angle and placement of the camera make typical vision-based techniques difficult to adopt. Our alternative approach observes small movements and changes in the shape, tendons, skin and bones on the opisthenar area. We train deep neural networks to recognize both hand poses and dynamic finger tapping gestures. While this is a challenging configuration for sensing, we tested the recognition with a real-time user test and achieved a high recognition rate of 89.4% (static poses) and 67.5% (dynamic gestures). Our results further demonstrate that our approach can generalize across sessions and to new users. Namely, users can remove and replace the wrist-worn device while new users can employ a previously trained system, to a certain degree. We conclude by demonstrating three applications and suggest future avenues of work based on sensing the back of the hand.
SP  - 963
EP  - 971
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347867
ER  - 

TY  - JOUR
AU  - Lhachemi, Hugo; Malik, Ammar; Shorten, Robert
TI  - Augmented Reality, Cyber-Physical Systems, and Feedback Control for Additive Manufacturing: A Review
PY  - 2019
AB  - Our objective in this paper is to review the application of feedback ideas in the area of additive manufacturing. Both the application of feedback control to the 3D printing process and the application of feedback theory to enable users to interact better with machines are reviewed. Where appropriate, opportunities for future work are highlighted.
SP  - 50119
EP  - 50135
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2907287
ER  - 

TY  - NA
AU  - Cui, Wenzhe; Zheng, Jingjie; Lewis, Blaine; Vogel, Daniel; Bi, Xiaojun
TI  - CHI - HotStrokes: Word-Gesture Shortcuts on a Trackpad
PY  - 2019
AB  - Expert interaction techniques like hotkeys are efficient, but poorly adopted because they are hard to learn. HotStrokes removes the need for learning arbitrary mappings of commands to hotkeys. A user enters a HotStroke by holding a modifier key, then gesture typing a command name on a laptop trackpad as if on an imaginary virtual keyboard. The gestures are recognized using an adaptation of the SHARK2 algorithm with a new spatial model and a refined method for dynamic suggestions. A controlled experiment shows HotStrokes effectively augments the existing "menu and hotkey" command activation paradigm. Results show the method is efficient by reducing command activation time by 43% compared to linear menus. The method is also easy to learn with a high adoption rate, replacing 91% of linear menu usage. Finally, combining linear menus, hotkeys, and HotStrokes leads to 24% faster command activation overall.
SP  - 165
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300395
ER  - 

TY  - NA
AU  - Ens, Barrett; Cordeil, Maxime; North, Chris; Dwyer, Tim; Besançon, Lonni; Prouzeau, Arnaud; Liu, Jiazhou; Cunningham, Andrew; Drogemuller, Adam; Satriadi, Kadek Ananta; Thomas, Bruce H
TI  - Immersive Analytics 2.0: Spatial and Embodied Sensemaking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3503726
ER  - 

TY  - JOUR
AU  - Wallace, Shaun; Le, Brendan; Leiva, Luis A.; Haq, Aman; Kintisch, Ari; Bufrem, Gabrielle; Chang, Linda; Huang, Jeff
TI  - Sketchy: Drawing Inspiration from the Crowd
PY  - 2020
AB  - In-person user studies show that designers draw inspiration by looking at their peers' work while sketching. To recreate this behavior in a virtual environment, we developed Sketchy, a web-based drawing application where users sketch in virtual rooms and use the "Peek'' functionality to gain ideas from their peers' sketches in real-time. To assess if "Peek'' supports individual creativity through finding inspiration, students from a Human-Computer Interaction class sketched user interface design tasks in two studies. Study 1 compares creativity measures with and without Peek between two groups of students, where self-reports reveal Peek increases satisfaction with their final sketch and better supports individual creativity. Study 2 took place in a large classroom, where 90 students, all with Peek enabled, completed different design tasks. Peeking led students to report an intention to change their sketch 18% of the time in Study 1 and 17% of the time in Study 2. Student designers were influenced by sketches that seem closer to completion, contain more details, and are carefully drawn. They were also about three times more likely to clear their canvas and start over if they found a sketch inspirational. Furthermore, sketches created by students with more sketching and design experience influence less experienced student designers. This work explores the directions and benefits of incorporating digital peeking to support individual creativity within a student designer's classroom experience to create more satisfactory final sketches.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW2
PB  - 
DO  - 10.1145/3415243
ER  - 

TY  - JOUR
AU  - Zhang, Peiying; Li, Chenhui; Wang, Changbo
TI  - VisCode: Embedding Information in Visualization Images using Encoder-Decoder Network
PY  - 2021
AB  - We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.
SP  - 326
EP  - 336
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030343
ER  - 

TY  - NA
AU  - Wang, Song; Dong, Zhu; Yu, Hao; Wu, Yadong
TI  - PacificVis - Immersive WYSIWYG (What You See is What You Get) Volume Visualization
PY  - 2020
AB  - Extended to immersive environment, volume visualization has the analytical superiority in spatial immersion, user engagement, multidimensional awareness and other aspects. But in a highly immersive virtual environment, traditional single-channel precise interactive methods cannot be applied directly to the immersive environment. Inspired by how users typically interact with everyday objects, a novel non-contact gesture interaction method base on What You See is What You Get（WYSIWYG）for volume rendering results is proposed in this paper. Just likes grab interaction in real scene, a full set of tools have been developed to enable direct volume rendering manipulation of color, saturation, contrast, brightness, and other optical properties by gestural motions in our method. Simultaneously, in order to improve the interactive experience in immersive environment, the evaluation model of motion comfort is introduced to design the interactive hand gestures, the cursor model is defined to estimating the gesture state combined with context gestural motions. Finally, the test platform is established with Oculus Rift + Leap Motion to verify the functionality and effectiveness of our method in improving the visual cognitive ability for volume visualization.
SP  - 166
EP  - 170
JF  - 2020 IEEE Pacific Visualization Symposium (PacificVis)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/pacificvis48177.2020.1001
ER  - 

TY  - NA
AU  - Guo, Kaiwen; Zhou, Hao; Tian, Ye; Zhou, Wangqiu; Ji, Yusheng; Li, Xiang-Yang
TI  - Mudra: A Multi-Modal Smartwatch Interactive System with Hand Gesture Recognition and User Identification
PY  - 2022
AB  - The great popularity of smartwatches leads to a growing demand for smarter interactive systems. Hand gesture is suitable for interaction due to its unique features. However, the existing single-modal gesture interactive systems have different biases in diverse scenarios, which makes it intractable to be applied in real life. In this paper, we propose a multi-modal smartwatch interactive system named Mudra, which fuses vision and Inertial Measurement Unit (IMU) signals to recognize and identify hand gestures for convenient and robust interaction. We carefully design a parallel attention multi-task model for different modals, and fuse classification results at the decision level with an adaptive weight adjustment algorithm. We implement a prototype of Mudra and collect data from 25 volunteers to evaluate its effectiveness. Extensive experiments demonstrate that Mudra can achieve 95.4% and 92.3% F1-scores on recognition and identification tasks, respectively. Meanwhile, Mudra can maintain stability and robustness under different experimental settings.
SP  - NA
EP  - NA
JF  - IEEE INFOCOM 2022 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom48880.2022.9796879
ER  - 

TY  - BOOK
AU  - Poblete, Brian Michael; Mendoza, Emir Christopher; De Castro, Julian Paolo; Deja, Jordan Aiko; Nodalo, Giselle
TI  - CHIuXiD - A Research through Design (Rtd) Approach in the Design of a 360-Video Platform Interface
PY  - 2019
AB  - Many video interfaces enable multiple sources of input video in displaying and streaming vital information. Most of these setups can be seen in deployed security systems and observer footage that are usually used for surveillance and crisis monitoring. The motivations of this study includes the use of multiple videos of a single event taken from varying sources in the investigation of a crime. In this study, we consider a crowd-sourced approach to multiple sources of video and aim to design an interface towards multiple possible use-cases. In designing this interface, we performed field studies and on site surveying along with initial user tests to validate our ideas. Research through design was added into the methodology to consider multiple point of views considering varying sources of perspective. Specifically, we catered the design of an initial interface in helping multiple users understand several views from various cameras, angles, and positions. The participants chosen for this study are students who have at least the basic technological ability of using a smartphone and taking a video with it. The results of this study could add to the use cases for 360 videos and video live streams. We intend to extend this study by validating the 360-view and designing an algorithm towards stitching one final view crowd-sourced from multiple cameras and streamers.
SP  - 166
EP  - 171
JF  - Proceedings of the 5th International ACM In-Cooperation HCI and UX Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3328243.3328265
ER  - 

TY  - JOUR
AU  - Fennedy, Katherine; Hartmann, Jeremy; Roy, Quentin; Perrault, Simon T.; Vogel, Daniel
TI  - OctoPocus in VR: Using a Dynamic Guide for 3D Mid-Air Gestures in Virtual Reality
PY  - 2021
AB  - Bau and Mackays OctoPocus dynamic guide helps novices learn, execute, and remember 2D surface gestures. We adapt OctoPocus to 3D mid-air gestures in Virtual Reality (VR) using an optimization-based recognizer, and by introducing an optional exploration mode to help visualize the spatial complexity of guides in a 3D gesture set. A replication of the original experiment protocol is used to compare OctoPocus in VR with a VR implementation of a crib-sheet. Results show that despite requiring 0.9s more reaction time than crib-sheet, OctoPocus enables participants to execute gestures 1.8s faster with 13.8 percent more accuracy during training, while remembering a comparable number of gestures. Subjective ratings support these results, 75 percent of participants found OctoPocus easier to learn and 83 percent found it more accurate. We contribute an implementation and empirical evidence demonstrating that an adaptation of the OctoPocus guide to VR is feasible and beneficial.
SP  - 4425
EP  - 4438
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3101854
ER  - 

TY  - JOUR
AU  - Pena, Alyssa M.; Ragan, Eric D.; Harrison, Lane
TI  - Memorability of Enhanced Informational Graphics: The effects of design relevance and chart type on recall
PY  - 2020
AB  - Design enhancements are often added to charts, signage, and infographics to help garner attention or communicate a message. Though it is argued that they may also detract user’s focus on the underlying information, prior studies have contributed evidence that visual design enhancements and even simple decorations can improve memory of visual displays. However, there is limited empirical knowledge about how the type of aesthetic enhancements infuences memory, and what informational and data elements are remembered from a visualization. We conducted a user study testing chart types (line, pie, and bar), the presence of color, and whether added enhancements were contextually related to the data topic presented in each chart. We found that enhancements relevant to the data helped in the recall of title and thematic elements, but enhancements did not signifcantly affect recall of specifc data values. This suggests that using relevant enhancements can have a positive effect on memorability of some chart content, but only if the design styles are chosen well to match the information topic. Recall of chart topics for unrelated embellishments was worse than plain, un-enhanced charts, which suggests that visual enhancement can distract or interfere with memorability if the viewer does not understand a meaningful connection between informational topic and design modifcations.
SP  - NA
EP  - NA
JF  - Interdisciplinary Journal of Signage and Wayfinding
VL  - 4
IS  - 1
PB  - 
DO  - 10.15763/issn.2470-9670.2020.v4.i1.a54
ER  - 

TY  - CHAP
AU  - Buchem, Ilona
TI  - HCI (26) - Design Principles for Wearable Enhanced Embodied Learning of Movement.
PY  - 2019
AB  - Human Computer Interaction (HCI) has seen increased interest in designing embodied experiences and interactions. This also includes the field of Wearable Enhanced Learning which marks the transition from the desktop age through the mobile age to the age of wearable, ubiquitous computing. Wearable enhanced learning relates to learning in a state of physical mobility supported by body-worn devices and sensors. While computer-mediated communication has been observed to enhance conscious experiences without self-reference leading to the sense of disembodiment, wearable technologies have the potential to enhance embodied experience with a strong self-reference. In fact, the affordances of wearable technologies to support embodied learning make wearable enhanced learning unique compared to other technology enhanced learning approaches. The concept of embodiment is based on the assumption that thoughts, feelings, and behaviours are grounded in movement and bodily interactions. Wearable Enhanced Embodied Learning is enabled by transmitting bodily information gathered by wearable sensors onto dynamic displays and making bodily information accessible to learners, in this way extending the learning experience. This paper draws on literature review in HCI and Embodiment research and collates a set of principles to inform the design of wearable enhanced embodied learning of movement.
SP  - 13
EP  - 25
JF  - Learning and Collaboration Technologies. Ubiquitous and Virtual Environments for Learning and Collaboration
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-21817-1_2
ER  - 

TY  - NA
AU  - Li, Jiannan; Lyu, Jiahe; Sousa, Maurício; Balakrishnan, Ravin; Tang, Anthony; Grossman, Tovi
TI  - UIST - Route Tapestries: Navigating 360° Virtual Tour Videos Using Slit-Scan Visualizations
PY  - 2021
AB  - An increasingly popular way of experiencing remote places is by viewing 360° virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360° video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating 360°virtual tour videos.
SP  - 223
EP  - 238
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474746
ER  - 

TY  - NA
AU  - Vanderdonckt, Jean; Khaddam, Iyad; Vatavu, Radu-Daniel
TI  - EICS - The foldinterface editor: a visual tool for designing user interfaces for foldable displays
PY  - 2020
AB  - Designing foldable user interfaces ("foldinterfaces") is complex and time-consuming because of the multiple technologies involved in the process, from the hardware details for folding pixels to design requirements regarding efficiency and ease of use. To assist this process, we introduce a visual editor for designers to specify foldinterfaces by implementing an extension of the Yoshizawa-Randlett diagramming system for origami and Event-Condition-Action rules from event-driven software architecture. The outcome is rendered as a 3-D foldable surface in a virtual environment.
SP  - NA
EP  - NA
JF  - Companion Proceedings of the 12th ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3393672.3398490
ER  - 

TY  - JOUR
AU  - Ojuroye, Olivia; Torah, Russel; Beeby, Steve
TI  - Modified PDMS packaging of sensory e-textile circuit microsystems for improved robustness with washing
PY  - 2019
AB  - <jats:title>Abstract</jats:title><jats:p>Electronic Textiles (e-textiles) should ideally be handled and cleaned like traditional textiles. Therefore, we can expect e-textiles to be machine washed or hand washed. As e-textiles enhance traditional fabrics with electronic functionality, any embedded microsystem i.e., flexible electronic circuits, will be expected to survive and show functionality after the e-textile has been washed multiple times to ensure the garment is practical. Therefore, the choice of encapsulation material for microsystems in a textile must be hydrophobic and offer minimal expansion when washed and ensure the electronics are undetectable when the textile is handled or cleaned. This paper evaluates five different base/curing agent mixing ratios—5:1, 7:1, 10:1, 15:1, and 20:1—of commercial polydimethylsiloxane (PDMS) as an electronic packaging encapsulation. Contact angle and aqueous permeability experiments were conducted to tailor the PDMS mixture specifically for washable e-textile applications. The experimental results show that 20:1 PDMS is the most suitable as it is sufficiently hydrophobic with minimal swelling in commercial washing machine trials. Following this, a 40.3 µm-thick 20:1 conformal encapsulation of PDMS upon an touch and proximity flexible circuit that can be integrated into textiles via knitting and/or weaving, was examined. Results show the washing spin speed is a crucial factor with washing cycle duration having minimal impact when determining circuit functionality survival. Overall, the e-textiles in this work survived between 10 and 15 washes with microscopic inspection of the circuits revealing failure of the external wires but not the PDMS encapsulation—suggesting its sufficient robustness and durability as a suitable encapsulation material for washable electronic textiles.</jats:p>
SP  - 1467
EP  - 1484
JF  - Microsystem Technologies
VL  - 28
IS  - 6
PB  - 
DO  - 10.1007/s00542-019-04455-7
ER  - 

TY  - NA
AU  - Kim, Daehwa; Park, Keunwoo; Lee, Geehyuk
TI  - CHI - AtaTouch: Robust Finger Pinch Detection for a VR Controller Using RF Return Loss
PY  - 2021
AB  - Handheld controllers are an essential part of VR systems. Modern sensing techniques enable them to track users’ finger movements to support natural interaction using hands. The sensing techniques, however, often fail to precisely determine whether two fingertips touch each other, which is important for the robust detection of a pinch gesture. To address this problem, we propose AtaTouch, which is a novel, robust sensing technique for detecting the closure of a finger pinch. It utilizes a change in the coupled impedance of an antenna and human fingers when the thumb and finger form a loop. We implemented a prototype controller in which AtaTouch detects the finger pinch of the grabbing hand. A user test with the prototype showed a finger-touch detection accuracy of 96.4%. Another user test with the scenarios of moving virtual blocks demonstrated low object-drop rate (2.75%) and false-pinch rate (4.40%). The results and feedback from the participants support the robustness and sensitivity of AtaTouch.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445442
ER  - 

TY  - CHAP
AU  - Chakraborty, Supratim; Stuerzlinger, Wolfgang
TI  - INTERACT (3) - VizInteract: Rapid Data Exploration Through Multi-touch Interaction with Multi-dimensional Visualizations
PY  - 2021
AB  - Creating and editing multi-dimensional data visualizations with current tools typically involves complex interactions. We present VizInteract, an interactive data visualization tool for touch-enabled displays. VizInteract supports efficient multi-touch data exploration through rapid construction of and interaction with multi-dimensional data visualizations. Building on primitive visualization idioms like histograms, VizInteract addresses the need for easy data exploration by facilitating the construction of multi-dimensional visualizations, such as scatter plots, parallel coordinate plots, radar charts, and scatter plot matrices, through simple multi-touch actions. Touch-based brushing-and-linking as well as attribute-based filter bubbles support “diving into the data” during analysis. We present the results of two explorative studies, one on a tablet and another on a large touchscreen and analyze the usage patterns that emerge while participants conducted visual analytics data exploration tasks in both conditions.
SP  - 610
EP  - 632
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85613-7_39
ER  - 

TY  - NA
AU  - Bala, Paulo; Masu, Raul; Nisi, Valentina; Nunes, Nuno Jardim
TI  - CHI - "When the Elephant Trumps": A Comparative Study on Spatial Audio for Orientation in 360º Videos
PY  - 2019
AB  - Orientation is an emerging issue in cinematic Virtual Reality (VR), as viewers may fail in locating points of interest. Recent strategies to tackle this research problem have investigated the role of cues, specifically diegetic sound effects. In this paper, we examine the use of sound spatialization for orientation purposes, namely by studying different spatialization conditions ("none", "partial", and "full" spatial manipulation) of multitrack soundtracks. We performed a between-subject mixed-methods study with 36 participants, aided by Cue Control, a tool we developed for dynamic spatial sound editing and data collection/analysis. Based on existing literature on orientation cues in 360o and theories on human listening, we discuss situations in which the spatialization was more effective (namely, "full" spatial manipulation both when using only music and when combining music and diegetic effects), and how this can be used by creators of 360o videos.
SP  - 695
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300925
ER  - 

TY  - NA
AU  - Li, Zhengqing; Teo, Theophilus; Chan, Liwei; Lee, Gun A.; Adcock, Matt; Billinghurst, Mark; Koike, Hideki
TI  - Conference on Designing Interactive Systems - OmniGlobeVR: A Collaborative 360-Degree Communication System for VR
PY  - 2020
AB  - In this paper, we present a novel collaboration tool, OmniGlobeVR, which is an asymmetric system that supports communication and collaboration between a VR user (occupant) and multiple non-VR users (designers) across the virtual and physical platform. OmniGlobeVR allows designer(s) to explore the VR space from any point of view using two view modes: a 360° first-person mode and a third-person mode. In addition, a shared gaze awareness cue is provided to further enhance communication between the occupant and the designer(s). Finally, the system has a face window feature that allows designer(s) to share their facial expressions and upper body view with the occupant for exchanging and expressing information using nonverbal cues. We conducted a user study to evaluate the OmniGlobeVR, comparing three conditions: (1) first-person mode with the face window, (2) first-person mode with a solid window, and (3) third-person mode with the face window. We found that the first-person mode with the face window required significantly less mental effort, and provided better spatial presence, usability, and understanding of the partner's focus. We discuss the design implications of these results and directions for future research.
SP  - 615
EP  - 625
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395429
ER  - 

TY  - JOUR
AU  - Sicat, Ronell; Li, Jiabao; Choi, Junyoung; Cordeil, Maxime; Jeong, Won-Ki; Bach, Benjamin; Pfister, Hanspeter
TI  - DXR: A Toolkit for Building Immersive Data Visualizations
PY  - 2018
AB  - This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.
SP  - 715
EP  - 725
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865152
ER  - 

TY  - NA
AU  - Chen, Linfeng; Takashima, Kazuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - CHI - PinpointFly: An Egocentric Position-control Drone Interface using Mobile AR
PY  - 2021
AB  - Accurate drone positioning is challenging because pilots only have a limited position and direction perception of a flying drone from their perspective. This makes conventional joystick-based speed control inaccurate and more complicated and significantly degrades piloting performance. We propose PinpointFly, an egocentric drone interface that allows pilots to arbitrarily position and rotate a drone using position-control direct interactions on a see-through mobile AR where the drone position and direction are visualized with a virtual cast shadow (i.e., the drone’s orthogonal projection onto the floor). Pilots can point to the next position or draw the drone’s flight trajectory by manipulating the virtual cast shadow and the direction/height slider bar on the touchscreen. We design and implement a prototype of PinpointFly for indoor and visual line of sight scenarios, which are comprised of real-time and predefined motion-control techniques. We conduct two user studies with simple positioning and inspection tasks. Our results demonstrate that PinpointFly makes the drone positioning and inspection operations faster, more accurate, simpler and fewer workload than a conventional joystick interface with a speed-control method.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445110
ER  - 

TY  - JOUR
AU  - Butcher, Peter; John, Nigel W.; Ritsos, Panagiotis D.
TI  - VRIA: A Web-Based Framework for Creating Immersive Analytics Experiences
PY  - 2021
AB  - We present VRIA, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. VRIA is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML Document Object Model (DOM). This makes VRIA ubiquitous and platform-independent. Moreover, by using WebVR's progressive enhancement, the experiences VRIA creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on open-standards Web technologies, present the VRIA creation workflow and detail the underlying mechanics of our framework. We also report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability implications of our framework, and present a series of use case applications to demonstrate the various features of VRIA. Finally, we discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.
SP  - 3213
EP  - 3225
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2020.2965109
ER  - 

TY  - NA
AU  - Li, Jianan; Yang, Jimei; Hertzmann, Aaron; Zhang, Jianming; Xu, Tingfa
TI  - LayoutGAN: Generating Graphic Layouts with Wireframe Discriminators
PY  - 2019
AB  - Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Srinivasan, Arjun; Lee, Bongshin; Stasko, John
TI  - Interweaving Multimodal Interaction With Flexible Unit Visualizations for Data Exploration
PY  - 2021
AB  - Multimodal interfaces that combine direct manipulation and natural language have shown great promise for data visualization. Such multimodal interfaces allow people to stay in the flow of their visual exploration by leveraging the strengths of one modality to complement the weaknesses of others. In this article, we introduce an approach that interweaves multimodal interaction combining direct manipulation and natural language with flexible unit visualizations. We employ the proposed approach in a proof-of-concept system, DataBreeze. Coupling pen, touch, and speech-based multimodal interaction with flexible unit visualizations, DataBreeze allows people to create and interact with both systematically bound (e.g., scatterplots, unit column charts) and manually customized views, enabling a novel visual data exploration experience. We describe our design process along with DataBreeze's interface and interactions, delineating specific aspects of the design that empower the synergistic use of multiple modalities. We also present a preliminary user study with DataBreeze, highlighting the data exploration patterns that participants employed. Finally, reflecting on our design process and preliminary user study, we discuss future research directions.
SP  - 3519
EP  - 3533
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 8
PB  - 
DO  - 10.1109/tvcg.2020.2978050
ER  - 

TY  - JOUR
AU  - Kono, Michinari; Takahashi, Takumi; Nakamura, Hiromi; Miyaki, Takashi; Rekimoto, Jun
TI  - Design Guideline for Developing Safe Systems that Apply Electricity to the Human Body
PY  - 2018
AB  - The human body has unique electrical characteristics. These characteristics have been investigated in various studies in human-computer interaction (HCI) and related research fields. Such studies include applications for using the body as a conductive lead for transmission or electric field sensing and activating human muscles or organs. However, electricity is not completely safe for the human body; therefore, to avoid harming users, careful consideration is essential when developing such devices. The knowledge required for such consideration is spread throughout a large number research fields, and it can be difficult for researchers in the HCI field to comprehend all of them. The purpose of this article is to support researchers in developing systems that apply electricity to the human body and to serve as a basis for further research. This article reviews previous research pertaining to HCI in which users come into contact with electricity. In addition, considerations of how and where this type of research can be expanded, along with guidelines grounded in other fields for designing systems safely and addressing ethical concerns, are presented. An understanding of the field and of the related safety issues will enhance the understanding of limitations and potential and can clarify the design space.
SP  - 19
EP  - 36
JF  - ACM Transactions on Computer-Human Interaction
VL  - 25
IS  - 3
PB  - 
DO  - 10.1145/3184743
ER  - 

TY  - NA
AU  - Lee, Chunggi; Kim, Sang-Hoon; Han, Dongyun; Yang, Hongjun; Park, Young-Woo; Kwon, Bum Chul; Ko, Sungahn
TI  - GUIComp: A GUI Design Assistant with Real-Time, Multi-Faceted Feedback
PY  - 2020
AB  - Users may face challenges while designing graphical user interfaces, due to a lack of relevant experience and guidance. This paper aims to investigate the issues that users with no experience face during the design process, and how to resolve them. To this end, we conducted semi-structured interviews, based on which we built a GUI prototyping assistance tool called GUIComp. This tool can be connected to GUI design software as an extension, and it provides real-time, multi-faceted feedback on a user's current design. Additionally, we conducted two user studies, in which we asked participants to create mobile GUIs with or without GUIComp, and requested online workers to assess the created GUIs. The experimental results show that GUIComp facilitated iterative design and the participants with GUIComp had better a user experience and produced more acceptable designs than those who did not.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Ojuroye, Olivia; Torah, Russel; Beeby, Steve
TI  - Modified PDMS packaging of sensory e-textile circuit microsystems for improved robustness with washing
PY  - 2019
AB  - Electronic Textiles (e-textiles) should ideally be handled and cleaned like traditional textiles. Therefore, we can expect e-textiles to be machine washed or hand washed. As e-textiles enhance traditional fabrics with electronic functionality, any embedded microsystem i.e., flexible electronic circuits, will be expected to survive and show functionality after the e-textile has been washed multiple times to ensure the garment is practical. Therefore, the choice of encapsulation material for microsystems in a textile must be hydrophobic and offer minimal expansion when washed and ensure the electronics are undetectable when the textile is handled or cleaned. This paper evaluates five different base/curing agent mixing ratios—5:1, 7:1, 10:1, 15:1, and 20:1—of commercial polydimethylsiloxane (PDMS) as an electronic packaging encapsulation. Contact angle and aqueous permeability experiments were conducted to tailor the PDMS mixture specifically for washable e-textile applications. The experimental results show that 20:1 PDMS is the most suitable as it is sufficiently hydrophobic with minimal swelling in commercial washing machine trials. Following this, a 40.3 µm-thick 20:1 conformal encapsulation of PDMS upon an touch and proximity flexible circuit that can be integrated into textiles via knitting and/or weaving, was examined. Results show the washing spin speed is a crucial factor with washing cycle duration having minimal impact when determining circuit functionality survival. Overall, the e-textiles in this work survived between 10 and 15 washes with microscopic inspection of the circuits revealing failure of the external wires but not the PDMS encapsulation—suggesting its sufficient robustness and durability as a suitable encapsulation material for washable electronic textiles.
SP  - 1
EP  - 18
JF  - Microsystem Technologies-micro-and Nanosystems-information Storage and Processing Systems
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Khurana, Rushil; Goel, Mayank
TI  - CHI - Eyes on the Road: Detecting Phone Usage by Drivers Using On-Device Cameras
PY  - 2020
AB  - Using a phone while driving is distracting and dangerous. It increases the accident chances by 400%. Several techniques have been proposed in the past to detect driver distraction due to phone usage. However, such techniques usually require instrumenting the user or the car with custom hardware. While detecting phone usage in the car can be done by using the phone's GPS, it is harder to identify whether the phone is used by the driver or one of the passengers. In this paper, we present a lightweight, software-only solution that uses the phone's camera to observe the car's interior geometry to distinguish phone position and orientation. We then use this information to distinguish between driver and passenger phone use. We collected data in 16 different cars with 33 different users and achieved an overall accuracy of 94% when the phone is held in hand and 92.2% when the phone is docked (1 sec. delay). With just a software upgrade, this work can enable smartphones to proactively adapt to the user's context in the car and and substantially reduce distracted driving incidents.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376822
ER  - 

TY  - NA
AU  - Gupta, Prakhar; Gupta, Shubh; Jayagopal, Ajaykrishnan; Pal, Sourav; Sinha, Ritwik
TI  - Saliency Prediction for Mobile User Interfaces
PY  - 2017
AB  - We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons, text, etc. in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied area. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Fonnet, Adrien; Prié, Yannick
TI  - Survey of Immersive Analytics
PY  - 2021
AB  - Immersive analytics (IA) is a new term referring to the use of immersive technologies for data analysis. Yet such applications are not new, and numerous contributions have been made in the last three decades. However, no survey reviewing all these contributions is available. Here we propose a survey of IA from the early nineties until the present day, describing how rendering technologies, data, sensory mapping, and interaction means have been used to build IA systems, as well as how these systems have been evaluated. The conclusions that emerge from our analysis are that: multi-sensory aspects of IA are under-exploited, the 3DUI and VR community knowledge regarding immersive interaction is not sufficiently utilised, the IA community should focus on converging towards best practices, as well as aim for real life IA systems.
SP  - 2101
EP  - 2122
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 3
PB  - 
DO  - 10.1109/tvcg.2019.2929033
ER  - 

TY  - NA
AU  - Bardot, Sandra; Rempel, Sawyer; Rey, Bradley; Neshati, Ali; Sakamoto, Yumiko; Menon, Carlo; Irani, Pourang
TI  - AH - Eyes-free graph legibility: using skin-dragging to provide a tactile graph visualization on the arm
PY  - 2020
AB  - Recent technological advances have enabled novel tactile displays which have mainly focused on providing shorter sensations for notifications and/or simple messages. These have been primarily been used to enhance the user experience. In contrast, conveying information via data charts, such as a line graph, remains largely unexplored. To address this gap, we developed a tactile display prototype. Our prototype uses skin-dragging, a method to produce longer tactile perceptions from dragging a tip on the skin, as the primary means to convey the data. We postulate that if such an approach is successful, it could convey the data in eyes-free scenarios, an element common for on-the-go computing. In an experiment (n=12), we compare the recognition performance of graphs with two different skin-dragging properties, Full-Drag and Dot. The results show that participants performed both techniques equally well, but our Full-Drag technique was greatly preferred. We conclude with design guidelines for tactile displays that focus on graph representations.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3396339.3396344
ER  - 

TY  - NA
AU  - Wataru, Yamada; Manabe, Hiroyuki; Ikeda, Daizo; Rekimoto, Jun
TI  - SUI - RayGraphy: Aerial Volumetric Graphics Rendered Using Lasers in Fog
PY  - 2020
AB  - We present RayGraphy display technology that renders volumetric graphics by superimposing the trajectories of lights in indoor space filled with fog. Since the traditional FogScreen approach requires the shaping of a thin layer of fog, it can only show two-dimensional images in a narrow range that is close to the fog-emitting nozzle. Although a method that renders volumetric graphics with plasma generated using high-power laser was also proposed, its operation in a public space is considered quite dangerous. The proposed system mainly comprises dozens of laser projectors circularly arranged in a fog-filled space, and renders volumetric graphics in a fog by superimposing weak laser beams from the projectors. Compared to the conventional methods, this system employing weak laser beams and the non-shaped innocuous fog is more scalable and safer. We aim to construct a new spatial augmented reality platform where computer-generated images can be drawn directly in the real world. We implement a prototype that consists of 32 laser projectors and a fog machine. Moreover, we evaluate and discuss the system performance and characteristics in experiments.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418446
ER  - 

TY  - JOUR
AU  - Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Anderson, Fraser; Wang, Florence Ying; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim
TI  - Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics
PY  - 2021
AB  - Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative’ scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use’ interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.
SP  - 1193
EP  - 1203
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030334
ER  - 

TY  - NA
AU  - Kumagai, Kota; Chiba, Taisei; Miura, Shun; Hayasaki, Yoshio
TI  - Volumetric graphics of microbubbles
PY  - 2020
AB  - A volumetric display generates three-dimensional (3D) graphics consisting of voxels in real space. It is a promising scheme of the volumetric displays to draw the voxels by 3D scanning of laser focusing points. The laser drawing method supports a wide viewing angle because these is no physical wiring between drawing space and the graphics forming system. The graphics size was ~ 1 cm3 in our previous research. In order to enlarge the volumetric bubble displays, the scan range should be increase with an objective lens with a long focal length. However, the increase gives an increase of the focusing diameter that requires an increase of the excitation energy. It is demonstrated that the use of gold nanoparticles was to decrease the excitation energy of the microbubbles.
SP  - 1155116
EP  - NA
JF  - Holography, Diffractive Optics, and Applications X
VL  - 11551
IS  - NA
PB  - 
DO  - 10.1117/12.2574778
ER  - 

TY  - NA
AU  - Ikeda, Bryce; Szafir, Daniel
TI  - Advancing the Design of Visual Debugging Tools for Roboticists
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/hri53351.2022.9889392
ER  - 

TY  - NA
AU  - Henschke, Martin
TI  - User Behaviour with Unguided Touchless Gestural Interfaces
PY  - NA
AB  - Touchless gestural interfaces provide users with the ability to use body movements and language gestures as a modality of input. Unlike other input modes, gestures must be interpreted by a recognition system, providing users with a degree of flexibility in how they are performed. Such interfaces are often suggested as a 'natural interface' alternative, allowing the utilisation of pre-learned communication skills for interaction. However, touchless interfaces are still governed by constraints, which can be opaque to users due to the complexity of the recognition system involved. This thesis explores the intuitive behaviour users exhibit when using touchless gestural interfaces. It describes four separate studies where users specified the constraints of the touchless interfaces, to allow the system to respond to their intuitive method of gesturing. The studies provided users with a series of tasks they performed with either deictic (pointing) gestures or dynamic free-hand gestures. I recorded variations between user performances and classified individual gestures to determine if similarities existed between the way users approached such systems. A considerable degree of variation was observed between the way users approached defining their gestures initially. Individual actions often had many possible definitions, with no significant inclination towards a common definition from the tested sample. Over time, participants typically did not make significant changes to their gesture performances, but when they did it was in response to recognition errors, fatigue, or curiosity and enjoyment. The studies also found that certain elements of the user performance could be manipulated by changing factors in the environment, such as providing a differently shaped device for performing deictic gestures. The findings suggest that there is no pattern of common behaviour that users exhibit when trying to use a touchless interface. Additionally, there are many different potential interpretations of how such systems work, and this interpretation can significantly impact the way users perform gestures. Guiding this interpretation could be a powerful tool to interaction designers to produce intuitive behaviours conducive to a positive user experience
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.25911/5f857473ae849
ER  - 

TY  - NA
AU  - Tian, Rundong; Saran, Vedant; Kritzler, Mareike; Michahelles, Florian; Paulos, Eric
TI  - UIST - Turn-by-Wire: Computationally Mediated Physical Fabrication
PY  - 2019
AB  - Advances in digital fabrication have simultaneously created new capabilities while reinforcing outdated workflows that constrain how, and by whom, these fabrication tools are used. In this paper, we investigate how a new class of hybrid-controlled machines can collaborate with novice and expert users alike to yield a more lucid making experience. We demonstrate these ideas through our system, Turn-by-Wire. By combining the capabilities of a traditional lathe with haptic input controllers that modulate both position and force, we detail a series of novel interaction metaphors that invite a more fluid making process spanning digital, model-centric, computer control, and embodied, adaptive, human control. We evaluate our system through a user study and discuss how these concepts generalize to other fabrication tools.
SP  - 713
EP  - 725
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347918
ER  - 

TY  - NA
AU  - Franz, Rachel L.; Junuzovic, Sasa; Mott, Martez E.
TI  - ASSETS - Nearmi: A Framework for Designing Point of Interest Techniques for VR Users with Limited Mobility
PY  - 2021
AB  - We propose Nearmi, a framework that enables designers to create customizable and accessible point-of-interest (POI) techniques in virtual reality (VR) for people with limited mobility. Designers can use Nearmi by creating and combining instances of its four components—representation, display, selection, and transition. These components enable users to gain awareness of POIs in virtual environments, and automatically re-orient the virtual camera toward a selected POI. We conducted a video elicitation study where 17 participants with limited mobility provided feedback on different Nearmi implementations. Although participants generally weighed the same design considerations when discussing their preferences, their choices reflected tradeoffs in accessibility, realism, spatial awareness, comfort, and familiarity with the interaction. Our findings highlight the need for accessible and customizable VR interaction techniques, as well as design considerations for building and evaluating these techniques.
SP  - NA
EP  - NA
JF  - The 23rd International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441852.3471230
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Chen, Yu; Yang, Yue; Yu, Zhi; Luo, Danli; Gu, Jianzhe; Yao, Lining; Tao, Ye; Wang, Guanyun
TI  - CHI - FlexTruss: A Computational Threading Method for Multi-material, Multi-form and Multi-use Prototyping
PY  - 2021
AB  - 3D printing, as a rapid prototyping technique, usually fabricates objects that are difficult to modify physically. This paper presents FlexTruss, a design and construction pipeline based on the assembly of modularized truss-shaped objects fabricated with conventional 3D printers and assembled by threading. To create an end-to-end system, a parametric design tool with an optimal Euler path calculation method is developed, which can support both inverse and forward design workflow and multi-material construction of modular parts. In addition, the assembly of truss modules by threading is evaluated with a series of application cases to demonstrate the affordance of FlexTruss. We believe that FlexTruss extends the design space of 3D printing beyond typically hard and fixed forms, and it will provide new capabilities for designers and researchers to explore the use of such flexible truss structures in human-object interaction.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445311
ER  - 

TY  - JOUR
AU  - Valliappan, Nachiappan; Dai, Na; Steinberg, Ethan; He, Junfeng; Rogers, Kantwon; Ramachandran, Venky; Xu, Pingmei; Shojaeizadeh, Mina; Guo, Li; Kohlhoff, Kai; Navalpakkam, Vidhya
TI  - Accelerating eye movement research via accurate and affordable smartphone eye tracking.
PY  - 2020
AB  - Eye tracking has been widely used for decades in vision research, language and usability. However, most prior research has focused on large desktop displays using specialized eye trackers that are expensive and cannot scale. Little is known about eye movement behavior on phones, despite their pervasiveness and large amount of time spent. We leverage machine learning to demonstrate accurate smartphone-based eye tracking without any additional hardware. We show that the accuracy of our method is comparable to state-of-the-art mobile eye trackers that are 100x more expensive. Using data from over 100 opted-in users, we replicate key findings from previous eye movement research on oculomotor tasks and saliency analyses during natural image viewing. In addition, we demonstrate the utility of smartphone-based gaze for detecting reading comprehension difficulty. Our results show the potential for scaling eye movement research by orders-of-magnitude to thousands of participants (with explicit consent), enabling advances in vision research, accessibility and healthcare.
SP  - 4553
EP  - 4553
JF  - Nature communications
VL  - 11
IS  - 1
PB  - 
DO  - 10.1038/s41467-020-18360-5
ER  - 

TY  - NA
AU  - Mahmood, Tahir; Fulmer, Willis; Mungoli, Neelesh; Huang, Jian; Lu, Aidong
TI  - ISMAR - Improving Information Sharing and Collaborative Analysis for Remote GeoSpatial Visualization Using Mixed Reality
PY  - 2019
AB  - Remote collaboration systems allow users at different sites to perform joint tasks, which are required by many real-life applications. For example, environmental pollution is a complex problem requiring many kinds of expertise to fully understand, as pollutants disperse not only locally but also regionally or even globally. This paper presents a remote collaborative visualization system through providing co-presence, information sharing, and collaborative analysis functions based on mixed reality techniques. We start with developing an immersive visualization approach for analyzing multi-attribute and geo-spatial data with intuitive multi-model interactions, simulating co-located collaboration effects. We then go beyond by designing a set of information sharing and collaborative analysis functions to support different users to share and analyze their sensemaking processes collaboratively. We provide example results and usage scenario to demonstrate that our system enables users to perform a variety of immersive and collaborative analytics tasks effectively. Through two small user studies focusing on evaluating our design of information sharing and system usability, the evaluation results confirm the effectiveness of comprehensive sharing among user, data, physical, and interaction spaces for improving remote collaborative analysis experience.
SP  - 236
EP  - 247
JF  - 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar.2019.00021
ER  - 

TY  - CHAP
AU  - Çöltekin, Arzu; Griffin, Amy L.; Slingsby, Aidan; Robinson, Anthony C.; Christophe, Sidonie; Rautenbach, Victoria; Chen, Min; Pettit, Christopher; Klippel, Alexander
TI  - Geospatial Information Visualization and Extended Reality Displays
PY  - 2019
AB  - In this chapter, we review and summarize the current state of the art in geovisualization and extended reality (i.e., virtual, augmented and mixed reality), covering a wide range of approaches to these subjects in domains that are related to geographic information science. We introduce the relationship between geovisualization, extended reality and Digital Earth, provide some fundamental definitions of related terms, and discuss the introduced topics from a human-centric perspective. We describe related research areas including geovisual analytics and movement visualization, both of which have attracted wide interest from multidisciplinary communities in recent years. The last few sections describe the current progress in the use of immersive technologies and introduce the spectrum of terminology on virtual, augmented and mixed reality, as well as proposed research concepts in geographic information science and beyond. We finish with an overview of “dashboards”, which are used in visual analytics as well as in various immersive technologies. We believe the chapter covers important aspects of visualizing and interacting with current and future Digital Earth applications.
SP  - 229
EP  - 277
JF  - Manual of Digital Earth
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-32-9915-3_7
ER  - 

TY  - NA
AU  - Vaddamanu, Praneetha; Aggarwal, Vinay; Guda, Bhanu Prakash Reddy; Srinivasan, Balaji Vasan; Chhaya, Niyati
TI  - Harmonized Banner Creation from Multimodal Design Assets
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519610
ER  - 

TY  - NA
AU  - Lin, Stephen Shiao-ru; Gamage, Nisal Menuka; Herath, Kithmini; Withana, Anusha
TI  - MyoSpring: 3D Printing Mechanomyographic Sensors for Subtle Finger Gesture Recognition
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501321
ER  - 

TY  - NA
AU  - Veras, Rafael; Collins, Christopher
TI  - CHI - Saliency Deficit and Motion Outlier Detection in Animated Scatterplots
PY  - 2019
AB  - We report the results of a crowdsourced experiment that measured the accuracy of motion outlier detection in multivariate, animated scatterplots. The targets were outliers either in speed or direction of motion, and were presented with varying levels of saliency in dimensions that are irrelevant to the task of motion outlier detection (e.g., color, size, position). We found that participants had trouble finding the outlier when it lacked irrelevant salient features and that visual channels contribute unevenly to the odds of an outlier being correctly detected. Direction of motion contributes the most to accurate detection of speed outliers, and position contributes the most to accurate detection of direction outliers. We introduce the concept of saliency deficit in which item importance in the data space is not reflected in the visualization due to a lack of saliency. We conclude that motion outlier detection is not well supported in multivariate animated scatterplots.
SP  - 541
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300771
ER  - 

TY  - NA
AU  - Zhang, Peiying; Li, Chenhui; Wang, Changbo
TI  - VisCode: Embedding Information in Visualization Images using Encoder-Decoder Network
PY  - 2020
AB  - We present an approach called VisCode for embedding information into visualization images. This technology can implicitly embed data information specified by the user into a visualization while ensuring that the encoded visualization image is not distorted. The VisCode framework is based on a deep neural network. We propose to use visualization images and QR codes data as training data and design a robust deep encoder-decoder network. The designed model considers the salient features of visualization images to reduce the explicit visual loss caused by encoding. To further support large-scale encoding and decoding, we consider the characteristics of information visualization and propose a saliency-based QR code layout algorithm. We present a variety of practical applications of VisCode in the context of information visualization and conduct a comprehensive evaluation of the perceptual quality of encoding, decoding success rate, anti-attack capability, time performance, etc. The evaluation results demonstrate the effectiveness of VisCode.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gong, Jun; Yang, Xin; Seyed, Teddy; Davis, Josh Urban; Yang, Xing-Dong
TI  - UIST - Indutivo: Contact-Based, Object-Driven Interactions with Inductive Sensing
PY  - 2018
AB  - We present Indutivo, a contact-based inductive sensing technique for contextual interactions. Our technique recognizes conductive objects (metallic primarily) that are commonly found in households and daily environments, as well as their individual movements when placed against the sensor. These movements include sliding, hinging, and rotation. We describe our sensing principle and how we designed the size, shape, and layout of our sensor coils to optimize sensitivity, sensing range, recognition and tracking accuracy. Through several studies, we also demonstrated the performance of our proposed sensing technique in environments with varying levels of noise and interference conditions. We conclude by presenting demo applications on a smartwatch, as well as insights and lessons we learned from our experience.
SP  - 321
EP  - 333
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242662
ER  - 

TY  - JOUR
AU  - Wu, Aoyu; Wang, Yun; Shu, Xinhuan; Moritz, Dominik; Cui, Weiwei; Zhang, Haidong; Zhang, Dongmei; Qu, Huamin
TI  - AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization.
PY  - 2022
AB  - Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at ai4vis.github.io.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3099002
ER  - 

TY  - JOUR
AU  - Youn, Eunhye; Lee, Sangyoon; Kim, Sunbum; Shim, Youngbo Aram; Chan, Liwei; Lee, Geehyuk
TI  - WristDial: An Eyes-Free Integer-Value Input Method by Quantizing the Wrist Rotation
PY  - 2021
AB  - Smartwatches are a convenient substitute for smartphones in hand-busy situations, but there is limited interactivity to one-handed use of wrist-worn devices. In this paper, we propose WristDial, a ...
SP  - 1607
EP  - 1624
JF  - International Journal of Human–Computer Interaction
VL  - 37
IS  - 17
PB  - 
DO  - 10.1080/10447318.2021.1898848
ER  - 

TY  - NA
AU  - Lao, Cheryl; Xia, Haijun; Wigdor, Daniel; Chevalier, Fanny
TI  - SUI - Attribute Spaces: Supporting Design Space Exploration in Virtual Reality
PY  - 2021
AB  - Exploring the design space of configurations for objects in virtual scenes is a challenge within virtual reality authoring tools due to the lack of visualization capabilities, non-destructive operations, suggestions, and flexibility. This work introduces Attribute Spaces, tools for visualizing and manipulating object attributes in virtual reality during 3D content generation. Attribute Spaces enable designers to systematically explore design spaces by supporting rapid comparisons between design alternatives and offering design suggestions. Custom combinations of attributes can be grouped and manipulated simultaneously for several objects. The grouping supports the creation of custom operation combinations that can be used as tools to edit multiple attribute, as well as snapshots of promising design decisions for later review. In an evaluation of Attribute Spaces by 3D design experts, our approach was found to enhance users’ understanding of their design space exploration progress and showed promise for integration into existing 3D workflows.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485290
ER  - 

TY  - BOOK
AU  - Okada, Kaya; Yoshida, Mitsuo; Itoh, Takayuki; Czauderna, Tobias; Stephens, Kingsley
TI  - IV - VR System for Spatio-Temporal Visualization of Tweet Data
PY  - 2018
AB  - Social media analysis is helpful to understand the behavior of people. Human behavior in social media is related to time and location, which is often difficult to understand the characteristics appropriately and quickly. We chose to apply virtual reality technologies to visualize the spatio-temporal social media data. This makes us easier to develop interactive and intuitive user interfaces and explore the data as we want. This paper presents our visualization of tweets of microblogs with location information. Our system features a three-dimensional temporal visualization which consists of the two-dimensional map and a time axis. In particular, we aggregate the number of tweets of each coordinate and time step, calculate scores and display them as piled cubes. We highlight only specific cubes so that users can understand the overall tendency of datasets. We also developed user interfaces for operating these cubes and panels which indicate details of tweets.
SP  - 91
EP  - 95
JF  - 2018 22nd International Conference Information Visualisation (IV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iv.2018.00026
ER  - 

TY  - JOUR
AU  - Feng, Wenjie; Liu, Sheng Hua; Faloutsos, Christos; Hooi, Bryan; Shen, Huawei; Cheng, Xueqi
TI  - EagleMine: Vision-guided Micro-clusters recognition and collective anomaly detection
PY  - 2021
AB  - NA
SP  - 236
EP  - 250
JF  - Future Generation Computer Systems
VL  - 115
IS  - NA
PB  - 
DO  - 10.1016/j.future.2020.08.033
ER  - 

TY  - JOUR
AU  - Takami, Rei; Shibata, Hiroki; Takama, Yasufumi
TI  - A Visual Analytics Interface for Formulating Evaluation Metrics of Multi-Dimensional Time-Series Data
PY  - 2021
AB  - A visual analytics (VA) interface for formulating evaluation metrics of multi-dimensional time-series data is proposed. Evaluation metrics such as key performance indicators (KPI) are expected to play an important role in quantitatively evaluating current situations and the quality of target objects. However, it is difficult for even domain experts to formulate metrics, especially for data with complexity related to dimensionality and temporal characteristics. The proposed interface is designed by extending the concept of semantic interaction to consider the temporal characteristics of target data. It represents metrics as a linear combination of data attributes and provides a means for adjusting it through interactive VA. On an animated scatter plot, an analyst can directly manipulate several visualized objects, i.e., a node, a trajectory, and a convex hull, as the group of nodes and trajectories. The result of manipulating the objects is reflected in the linear combination of attributes, which corresponds to an axis of the scatter plot. Using the axes as the output of the analysis, analysts can formulate a metric. The effectiveness of the proposed interface is demonstrated through an example and evaluated by two user experiments on the basis of hypotheses obtained from the example.
SP  - 102783
EP  - 102800
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3098621
ER  - 

TY  - NA
AU  - Menzner, Tim; Gesslein, Travis; Otte, Alexander; Grubert, Jens
TI  - Above Surface Interaction for Multiscale Navigation in Mobile Virtual Reality
PY  - 2020
AB  - Virtual Reality enables the exploration of large information spaces. In physically constrained spaces such as airplanes or buses, controller-based or mid-air interaction in mobile Virtual Reality can be challenging. Instead, the input space on and above touch-screen enabled devices such as smartphones or tablets could be employed for Virtual Reality interaction in those spaces. In this context, we compared an above surface interaction technique with traditional 2D on-surface input for navigating large planar information spaces such as maps in a controlled user study (n = 20). We find that our proposed above surface interaction technique results in significantly better performance and user preference compared to pinch-to-zoom and drag-to-pan when navigating planar information spaces.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Bhowmick, Shimmila
TI  - VR Workshops - Exploring Body Gestures for Small Object Selection in Dense Environment in HMD VR for Data Visualization Applications
PY  - 2021
AB  - Recent years have seen incredible growth of Virtual Reality (VR) interfaces. In the area of data visualization and analytics, VR applications offers immense opportunities to interact, modify and explore 3D data in an interactive manner, which help in establishing new trends and patterns. In this regard, object selection is of primary importance. It is the fundamental and initial task in any immersive VR. However, the current literature is limited in investigating the effectiveness of object selection techniques in different VEs including dense and occluded dense VE, varied object sizes, proximity, and distances in the area of Immersive Analytics. In this paper, I present my ongoing PhD research to explore controller-less gestures for nail-size object selection on HMD-VR interface for dense and occluded dense VE. I describe the experiments, the findings and my future studies. I believe the outcome of these experiments in the form of guidelines or framework will enable researchers to design controller-less body gestures for object selection task.
SP  - 713
EP  - 714
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00239
ER  - 

TY  - NA
AU  - Lambrichts, Mannu; Tijerina, Jose Maria; Ramakers, Raf
TI  - Tangible and Embedded Interaction - SoftMod: A Soft Modular Plug-and-Play Kit for Prototyping Electronic Systems
PY  - 2020
AB  - We present SoftMod, a novel modular electronics kit consisting of soft and flexible modules that snap together. Unlike existing modular kits, SoftMod tracks the topology of interconnected modules and supports basic plug-and-play behavior as well as advanced user-specified behavior. As such, the shape of a SoftMod assembly does not depend on the desired behavior and various 2D and 3D electronic systems can be realized. While the plug-and-play nature of our modules stimulates play, the advanced features for specifying behavior and for making a variety of soft and flexible shapes, offer a high-ceiling when experimenting with novel types of interfaces, such as wearables, and interactive skin and textiles.
SP  - 287
EP  - 298
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374950
ER  - 

TY  - JOUR
AU  - Martin, Daniel; Malpica, Sandra; Gutierrez, Diego; Masia, Belen; Serrano, Ana
TI  - Multimodality in VR: A Survey
PY  - 2022
AB  - <jats:p>Virtual reality (VR) is rapidly growing, with the potential to change the way we create and consume content. In VR, users integrate multimodal sensory information they receive to create a unified perception of the virtual world. In this survey, we review the body of work addressing multimodality in VR and its role and benefits in user experience, together with different applications that leverage multimodality in many disciplines. These works thus encompass several fields of research and demonstrate that multimodality plays a fundamental role in VR, enhancing the experience, improving overall performance, and yielding unprecedented abilities in skill and knowledge transfer.</jats:p>
SP  - 1
EP  - 36
JF  - ACM Computing Surveys
VL  - 54
IS  - 10s
PB  - 
DO  - 10.1145/3508361
ER  - 

TY  - CHAP
AU  - Shah, Syed Hammad Hussain; Lee, Jong Weon
TI  - Authoring Tool for Generating Multiple Experiences of 360° Virtual Reality
PY  - 2019
AB  - 360° video is a special form of digital content which provides visual details of 360° world around the viewer. While watching such video using head mounted display (HMD), it immerses the viewer with experience of a different place. This paper introduces a novel authoring tool which enables the viewers to create multiple interesting experiences of a 360° video. Viewers can read descriptions of experiences provided by their creators and choose anyone to watch. This tool assists users to efficiently follow the selected experiences while watching 360° video. Major challenge in existing authoring tools is that they are difficult to learn. We have made authoring easy for user to create experiences of 360° video by simply watching in virtual reality by wearing an HMD. Another problem in previous authoring tools is that it is difficult to author multiple video experiences of a 360° video. This authoring tool has made it easy.
SP  - 73
EP  - 78
JF  - Advances in Computer Science and Ubiquitous Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-13-9341-9_13
ER  - 

TY  - NA
AU  - Satkowski, Marc; Dachselt, Raimund
TI  - CHI - Investigating the Impact of Real-World Environments on the Perception of 2D Visualizations in Augmented Reality
PY  - 2021
AB  - In this work we report on two comprehensive user studies investigating the perception of Augmented Reality (AR) visualizations influenced by real-world backgrounds. Since AR is an emerging technology, it is important to also consider productive use cases, which is why we chose an exemplary and challenging industry 4.0 environment. Our basic perceptual research focuses on both the visual complexity of backgrounds as well as the influence of a secondary task. In contrast to our expectation, data of our 34 study participants indicate that the background has far less influence on the perception of AR visualizations. Moreover, we observed a mismatch between measured and subjectively reported performance. We discuss the importance of the background and recommendations for visual real-world augmentations. Overall, our results suggest that AR can be used in many visually challenging environments without losing the ability to productively work with the visualizations shown.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445330
ER  - 

TY  - NA
AU  - Ramprasad, Brian; Chen, Hongkai; da Silva Veith, Alexandre; Truong, Khai N.; de Lara, Eyal
TI  - MobiSys - Pain-o-vision, effortless pain management
PY  - 2021
AB  - Chronic pain is often an ongoing challenge for patients to track and collect data. Pain-O-Vision is a smartwatch enabled pain management system that uses computer vision to capture the details of painful events from the user. A natural reaction to pain is to clench ones fist. The embedded camera is used to capture different types of fist clenching, to represent different levels of pain. An initial prototype was built on an Android smartwatch that uses a cloud-based classification service to detect the fist clench gestures. Our results show that it is possible to map a fist clench to different levels of pain which allows the patient to record the intensity of a painful event without carrying a specialized pain management device.
SP  - 483
EP  - 484
JF  - Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458864.3466907
ER  - 

TY  - JOUR
AU  - Liu, Yang; Lin, Chengdong; Li, Zhenjiang
TI  - WR-Hand: Wearable Armband Can Track User's Hand
PY  - 2021
AB  - This paper presents WR-Hand, a wearable-based system tracking 3D hand pose of 14 hand skeleton points over time using Electromyography (EMG) and gyroscope sensor data from commercial armband. This system provides a significant leap in wearable sensing and enables new application potentials in medical care, human-computer interaction, etc. A challenge is the armband EMG sensors inevitably collect mixed EMG signals from multiple forearm muscles because of the fixed sensor positions on the device, while prior bio-medical models for hand pose tracking are built on isolated EMG signal inputs from isolated forearm spots for different muscles. In this paper, we leverage the recent success of neural networks to enhance the existing bio-medical model using the armband's EMG data and visualize our design to understand why our solution is effective. Moreover, we propose solutions to place the constructed hand pose reliably in a global coordinate system, and address two practical issues by providing a general plug-and-play version for new users without training and compensating for the position difference in how users wear their armbands. We implement a prototype using different commercial armbands, which is lightweight to execute on user's phone in real-time. Extensive evaluation shows the efficacy of the WR-Hand design.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478112
ER  - 

TY  - NA
AU  - Shi, Yilei; Zhang, Haimo; Rajapakse, Hasitha; Perera, Nuwan Tharaka; Gálvez, Tomás Vega; Nanayakkara, Suranga
TI  - CHI - GestAKey: Touch Interaction on Individual Keycaps
PY  - 2018
AB  - Conventionally, keys on a physical keyboard have only two states: "released'' and "pressed''. As such, various techniques, such as hotkeys, are designed to enhance the keyboard expressiveness. Realizing that user inevitably perform touch actions during keystrokes, we propose GestAKey, leveraging location and motion of the touch on individual keycaps to augment the functionalities of existing keystrokes. With a log study, we collected touch data for both normal usage (typing and hotkeys) and while performing touch gestures (location and motion), which are analyzed to assess the viability of augmenting keystrokes with simultaneous gestures. A controlled experiment was conducted to compare GestAKey with existing keyboard interaction techniques, in terms of efficiency and learnability. The results show that GestAKey has comparable performance with hotkey. We further discuss the insights of integrating such touch modality into existing keyboard interaction, and demonstrate several usage scenarios.
SP  - 596
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174170
ER  - 

TY  - NA
AU  - Truong, Hoang; Zhang, Shuo; Muncuk, Ufuk; Nguyen, Phuc; Bui, Nam; Nguyen, Anh; Lv, Qin; Chowdhury, Kaushik R.; Dinh, Thang N.; Vu, Tam
TI  - SenSys - CapBand: Battery-free Successive Capacitance Sensing Wristband for Hand Gesture Recognition
PY  - 2018
AB  - We present CapBand, a battery-free hand gesture recognition wearable in the form of a wristband. The key challenges in creating such a system are (1) to sense useful hand gestures at ultra-low power so that the device can be powered by the limited energy harvestable from the surrounding environment and (2) to make the system work reliably without requiring training every time a user puts on the wristband. We present successive capacitance sensing, an ultra-low power sensing technique, to capture small skin deformations due to muscle and tendon movements on the user's wrist, which corresponds to specific groups of wrist muscles representing the gestures being performed. We build a wrist muscles-to-gesture model, based on which we develop a hand gesture classification method using both motion and static features. To eliminate the need for per-usage training, we propose a kernel-based on-wrist localization technique to detect the CapBand's position on the user's wrist. We prototype CapBand with a custom-designed capacitance sensor array on two flexible circuits driven by a custom-built electronic board, a heterogeneous material-made, deformable silicone band, and a custom-built energy harvesting and management module. Evaluations on 20 subjects show 95.0% accuracy of gesture recognition when recognizing 15 different hand gestures and 95.3% accuracy of on-wrist localization.
SP  - 54
EP  - 67
JF  - Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3274783.3274854
ER  - 

TY  - NA
AU  - Fulmer, Willis; Mahmood, Tahir; Li, Zhongyu; Zhang, Shaoting; Huang, Jian; Lu, Aidong
TI  - IUI - ImWeb: cross-platform immersive web browsing for online 3D neuron database exploration
PY  - 2019
AB  - Web services have become one major way for people to obtain and explore information nowadays. However, web browsers currently only offer limited data analysis capabilities, especially for large-scale 3D datasets. This project presents a method of immersive web browsing (ImWeb) to enable effective exploration of multiple datasets over the web with augmented reality (AR) techniques. The ImWeb system allows inputs from both the web browser and AR and provides a set of immersive analytics methods for enhanced web browsing, exploration, comparison, and summary tasks. We have also integrated 3D neuron mining and abstraction approaches to support efficient analysis functions. The architecture of ImWeb system flexibly separates the tasks on web browser and AR and supports smooth networking among the system, so that ImWeb can be adopted by different platforms, such as desktops, large displays, and tablets. We use an online 3D neuron database to demonstrate that ImWeb enables new experiences of exploring 3D datasets over the web. We expect that our approach can be applied to various other online databases and become one useful addition to future web services.
SP  - 367
EP  - 378
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302319
ER  - 

TY  - JOUR
AU  - Haehn, Daniel; Tompkin, James; Pfister, Hanspeter
TI  - Evaluating ‘Graphical Perception’ with CNNs
PY  - 2018
AB  - Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.
SP  - 641
EP  - 650
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865138
ER  - 

TY  - NA
AU  - Lisle, Lee; Davidson, Kylie; Gitre, Edward J. K.; North, Chris; Bowman, Doug A.
TI  - VR - Sensemaking Strategies with Immersive Space to Think
PY  - 2021
AB  - The process of sensemaking involves foraging through and extracting information from large sets of documents, and it can be a cognitively intensive task. A recent approach, the Immersive Space to Think (IST), allows analysts to browse, read, mark up documents, and use immersive 3D space to organize and label collections of documents. In this study, we observed seventeen novice analysts perform a historical analysis task in order to understand how users utilize the features of IST to extract meaning from large text-based datasets. We found three different layout strategies they employed to create meaning with the documents we provided. We further found patterns of interaction and organization that can inform future improvements to the IST approach.
SP  - 529
EP  - 537
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00077
ER  - 

TY  - JOUR
AU  - Su, Yu-Chuan; Grauman, Kristen
TI  - Learning Spherical Convolution for 360 Recognition.
PY  - 2022
AB  - While 360 cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make visual recognition non-trivial. Ideally, 360 imagery could inherit the convolutional neural networks (CNNs) trained with great success on perspective projection images. However, existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. We propose to learn a Spherical Convolution Network (SphConv) that translates a planar CNN to the equirectangular projection of 360 images. Given a source CNN for perspective images, SphConv learns to reproduce the flat filter outputs on 360 data. The key benefits are 1) efficient and accurate recognition for 360 images, and 2) the ability to leverage pre-trained networks for perspective images. We propose two instantiations of SphConv---Spherical Kernel, which learns location dependent kernels on the sphere, and Kernel Transformer Network, which learns a functional transformation that generates SphConv from the source CNN. Validating our approach with multiple source CNNs and datasets, we show that it successfully preserves the source CNN's accuracy, while offering efficiency, transferability, and scalability to typical image resolutions. We further introduce a spherical Faster R-CNN based on SphConv and show that we can learn a spherical object detector without any object annotations in 360 images.
SP  - 1
EP  - 1
JF  - IEEE transactions on pattern analysis and machine intelligence
VL  - 44
IS  - 01
PB  - 
DO  - 10.1109/tpami.2021.3113612
ER  - 

TY  - NA
AU  - Schmitz, Anastasia; MacQuarrie, Andrew; Julier, Simon; Binetti, Nicola; Steed, Anthony
TI  - VR - Directing versus Attracting Attention: Exploring the Effectiveness of Central and Peripheral Cues in Panoramic Videos
PY  - 2020
AB  - Filmmakers of panoramic videos frequently struggle to guide attention to Regions of Interest (ROIs) due to consumers’ freedom to explore. Some researchers hypothesize that peripheral cues attract reflexive/involuntary attention whereas cues within central vision engage and direct voluntary attention. This mixed-methods study evaluated the effectiveness of using central arrows and peripheral flickers to guide and focus attention in panoramic videos. Twenty-five adults wore a head-mounted display with an eye tracker and were guided to 14 ROIs in two panoramic videos. No significant differences emerged in regard to the number of followed cues, the time taken to reach and observe ROIs, ROI-related memory and user engagement. However, participants’ gaze travelled a significantly greater distance toward ROIs within the first 500 ms after flicker-onsets compared to arrow-onsets. Nevertheless, most users preferred the arrow and perceived it as significantly more rewarding than the flicker. The findings imply that traditional attention paradigms are not entirely applicable to panoramic videos, as peripheral cues appear to engage both involuntary and voluntary attention. Theoretical and practical implications as well as limitations are discussed.
SP  - 63
EP  - 72
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581102716289
ER  - 

TY  - NA
AU  - Hayatpur, Devamardeep; Xia, Haijun; Wigdor, Daniel
TI  - UIST - DataHop: Spatial Data Exploration in Virtual Reality
PY  - 2020
AB  - Virtual reality has recently been adopted for use within the domain of visual analytics because it can provide users with an endless workspace within which they can be actively engaged and use their spatial reasoning skills for data analysis. However, virtual worlds need to utilize layouts and organizational schemes that are meaningful to the user and beneficial for data analysis. This paper presents DataHop, a novel visualization system that enables users to lay out their data analysis steps in a virtual environment. With a Filter, a user can specify the modification they wish to perform on one or more input data panels (i.e., containers of points), along with where output data panels should be placed in the virtual environment. Using this simple tool, highly intricate and useful visualizations may be generated and traversed by harnessing a user's spatial abilities. An exploratory study conducted with six virtual reality users evaluated the usability, affordances, and performance of DataHop for data analysis tasks, and found that spatially mapping one's workflow can be beneficial when exploring multidimensional datasets.
SP  - 818
EP  - 828
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415878
ER  - 

TY  - NA
AU  - Lee, Hsin-Ying; Jiang, Lu; Essa, Irfan; Le, Phuong B.; Gong, Haifeng; Yang, Ming-Hsuan; Yang, Weilong
TI  - Neural Design Network: Graphic Layout Generation with Constraints
PY  - 2019
AB  - Graphic design is essential for visual communication with layouts being fundamental to composing attractive designs. Layout generation differs from pixel-level image synthesis and is unique in terms of the requirement of mutual relations among the desired components. We propose a method for design layout generation that can satisfy user-specified constraints. The proposed neural design network (NDN) consists of three modules. The first module predicts a graph with complete relations from a graph with user-specified relations. The second module generates a layout from the predicted graph. Finally, the third module fine-tunes the predicted layout. Quantitative and qualitative experiments demonstrate that the generated layouts are visually similar to real design layouts. We also construct real designs based on predicted layouts for a better understanding of the visual quality. Finally, we demonstrate a practical application on layout recommendation.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Urakami, Jacqueline; Matulis, Henrique; Miyafuji, Shio; Li, Zhengqing; Koike, Hideki; Chignell, Mark
TI  - Comparing immersiveness and perceptibility of spherical and curved displays.
PY  - 2020
AB  - NA
SP  - 103271
EP  - NA
JF  - Applied ergonomics
VL  - 90
IS  - NA
PB  - 
DO  - 10.1016/j.apergo.2020.103271
ER  - 

TY  - JOUR
AU  - Abu Al-Haija, Qasem; Al Badawi, Ahmad
TI  - High-performance intrusion detection system for networked UAVs via deep learning
PY  - 2022
AB  - NA
SP  - 10885
EP  - 10900
JF  - Neural Computing and Applications
VL  - 34
IS  - 13
PB  - 
DO  - 10.1007/s00521-022-07015-9
ER  - 

TY  - JOUR
AU  - Zhican, Yang; Yu, Chun; Yi, Xin; Shi, Yuanchun
TI  - Investigating Gesture Typing for Indirect Touch
PY  - 2019
AB  - With the development of ubiquitous computing, entering text on HMDs and smart TVs using handheld touchscreen devices (e.g., smartphone and controller) is becoming more and more attractive. In these indirect touch scenarios, the touch input surface is decoupled from the visual display. Compared with direct touch input, entering text using a keyboard in indirect touch is more challenging because before the finger touch, no visual feedback is available for locating the touch finger. Aiming at this problem, in this paper, we investigate the feasibility of gesture typing for indirect touch since keeping the finger in touch with the screen during typing makes it possible to provide continuous visual feedback, which is beneficial for increasing the input performance. We first examine users' gesture typing ability in terms of the appropriate keyboard size and location in motor space and then compare the typing performance in direct and indirect touch mode. We then propose an improved design to address the uncertainty and inaccuracy of the first touch. Our evaluation result shows that users can quickly acquire indirect gesture typing, and type 22.3 words per minute after 30 phases, which significantly outperforms previous numbers in literature. Our work provides the empirical support for leveraging gesture typing for indirect touch.
SP  - 117
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3351275
ER  - 

TY  - BOOK
AU  - Wang, Tzu-Yang; Noaki, Yuki; Kuzuoka, Hideaki
TI  - AsianCHI@CHI - Exploring How to Display Referential Action to Support Remote Group Discussion
PY  - 2021
AB  - Nowadays, remote conferences are widely seen in many situations. Although there is little problem to conduct one-on-one remote conferences, remote group discussion still contains many challenges. One of the main issue is that the remote participants often unnotice the referential action performed by the local participants. To solve such problem, we proposed to develop a function that automatically detects the presence of referential action and displays to the remote participant. One of the issues for this function is about what kind of information of the referential action should be provided and how the information should be displayed. In this research, we conducted a lab study to compare two displaying methods: Picture-in-Picture (PiP) and auto-pilot and compare two displaying contents: object being referred and person performing the referential action. The result shows that the PiP method had higher usability and remote participant had higher opportunity to join the conversation with the PiP method. On the other hand, displaying object being referred had higher usability than displaying the person performing the referential action.
SP  - 89
EP  - 96
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3468188
ER  - 

TY  - JOUR
AU  - Jacobson, Alec
TI  - RodSteward: A Design‐to‐Assembly System for Fabrication using 3D‐Printed Joints and Precision‐Cut Rods
PY  - 2019
AB  - We present RodSteward, a design-to-assembly system for creating furniture-scale structures composed of 3D printed joints and precision-cut rods. The RodSteward systems consists of: RSDesigner, a fabrication-aware design interface that visualizes accurate geometries during edits and identifies infeasible designs; physical fabrication of parts via novel fully automatic construction of solid 3D-printable joint geometries and automatically generated cutting plans for rods; and RSAssembler, a guided-assembly interface that prompts the user to place parts in order while showing a focus+context visualization of the assembly in progress. We demonstrate the effectiveness of our tools with a number of example constructions of varying complexity, style and parameter choices.
SP  - 765
EP  - 774
JF  - Computer Graphics Forum
VL  - 38
IS  - 7
PB  - 
DO  - 10.1111/cgf.13878
ER  - 

TY  - NA
AU  - Ens, Barrett; Bach, Benjamin; Cordeil, Maxime; Engelke, Ulrich; Serrano, Marcos; Willett, Wesley
TI  - CHI Extended Abstracts - Envisioning Future Productivity for Immersive Analytics
PY  - 2020
AB  - Immersive technologies have opened many opportunities for the visual analytics and visualisation community. However, despite an emerging focus on immersive analytics research, there is currently a lack of coherent and broadly applicable visualisation and interaction design in order to make immersive analytics systems productive and usable in real world scenarios. In this workshop, we propose to identify a road-map towards productive user interfaces for immersive analytics. In particular we aim to understand how we can effectively support specific visualisation and analytics tasks in VR/AR, and identify the interactions needed to support these. Our goal is to catalyse the research community and lead toward a reference paper to inform future research.
SP  - 3375145
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375145
ER  - 

TY  - BOOK
AU  - Büschel, Wolfgang; Chen, Jian; Dachselt, Raimund; Drucker, Steven M.; Dwyer, Tim; Görg, Carsten; Isenberg, Tobias; Kerren, Andreas; North, Chris; Stuerzlinger, Wolfgang
TI  - Immersive Analytics - Interaction for immersive analytics
PY  - 2018
AB  - In this chapter, we briefly review the development of natural user interfaces and discuss their role in providing human-computer interaction that is immersive in various ways. Then we examine some opportunities for how these technologies might be used to better support data analysis tasks. Specifically, we review and suggest some interaction design guidelines for immersive analytics. We also review some hardware setups for data visualization that are already archetypal. Finally, we look at some emerging system designs that suggest future directions.
SP  - 95
EP  - 138
JF  - Immersive Analytics
VL  - 11190
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01388-2_4
ER  - 

TY  - NA
AU  - Hubenschmid, Sebastian; Zagermann, Johannes; Butscher, Simon; Reiterer, Harald
TI  - CHI - STREAM: Exploring the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics
PY  - 2021
AB  - Recent research in the area of immersive analytics demonstrated the utility of head-mounted augmented reality devices for visual data analysis. However, it can be challenging to use the by default supported mid-air gestures to interact with visualizations in augmented reality (e.g. due to limited precision). Touch-based interaction (e.g. via mobile devices) can compensate for these drawbacks, but is limited to two-dimensional input. In this work we present STREAM: Spatially-aware Tablets combined with Augmented Reality Head-Mounted Displays for the multimodal interaction with 3D visualizations. We developed a novel eyes-free interaction concept for the seamless transition between the tablet and the augmented reality environment. A user study reveals that participants appreciated the novel interaction concept, indicating the potential for spatially-aware tablets in augmented reality. Based on our findings, we provide design insights to foster the application of spatially-aware touch devices in augmented reality and research implications indicating areas that need further investigation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445298
ER  - 

TY  - JOUR
AU  - Fröhler, B.; Anthes, C.; Pointecker, F.; Friedl, J.; Schwajda, D.; Riegler, A.; Tripathi, S.; Holzmann, C.; Brunner, M.; Jodlbauer, H.; Jetter, H.‐C.; Heinzl, C.
TI  - A Survey on Cross‐Virtuality Analytics
PY  - 2022
AB  - Cross-virtuality analytics (XVA) is a novel field of research within immersive analytics and visual analytics. A broad range of heterogeneous devices across the reality–virtuality continuum, along with respective visual metaphors and analysis techniques, are currently becoming available. The goal of XVA is to enable visual analytics that use transitional and collaborative interfaces to seamlessly integrate different devices and support multiple users. In this work, we take a closer look at XVA and analyse the existing body of work for an overview of its current state. We classify the related literature regarding ways of establishing cross-virtuality by interconnecting different stages in the reality–virtuality continuum, as well as techniques for transitioning and collaborating between the different stages. We provide insights into visualization and interaction techniques employed in current XVA systems. We report on ways of evaluating such systems, and analyse the domains where such systems are becoming available. Finally, we discuss open challenges in XVA, giving directions for future research.
SP  - 465
EP  - 494
JF  - Computer Graphics Forum
VL  - 41
IS  - 1
PB  - 
DO  - 10.1111/cgf.14447
ER  - 

TY  - NA
AU  - Fennedy, Katherine; Malacria, Sylvain; Lee, Hyowon; Perrault, Simon T.
TI  - Investigating Performance and Usage of Input Methods for Soft Keyboard Hotkeys
PY  - 2020
AB  - Touch-based devices, despite their mainstream availability, do not support a unified and efficient command selection mechanism, available on every platform and application. We advocate that hotkeys, conventionally used as a shortcut mechanism on desktop computers, could be generalized as a command selection mechanism for touch-based devices, even for keyboard-less applications. In this paper, we investigate the performance and usage of soft keyboard shortcuts or hotkeys (abbreviated SoftCuts) through two studies comparing different input methods across sitting, standing and walking conditions. Our results suggest that SoftCuts not only are appreciated by participants but also support rapid command selection with different devices and hand configurations. We also did not find evidence that walking deters their performance when using the Once input method.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403552
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Karim, Adnan; Xia, Tian; Hedayati, Hooman; Marquardt, Nicolai
TI  - Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces
PY  - 2022
AB  - This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517719
ER  - 

TY  - NA
AU  - Leen, Danny; Peek, Nadya; Ramakers, Raf
TI  - UIST - LamiFold: Fabricating Objects with Integrated Mechanisms Using a Laser cutter Lamination Workflow
PY  - 2020
AB  - We present LamiFold, a novel design and fabrication workflow for making functional mechanical objects using a laser cutter. Objects fabricated with LamiFold embed advanced rotary, linear, and chained mechanisms, including linkages that support fine-tuning and locking position. Laser cutting such mechanisms without LamiFold requires designing for and embedding off-the-shelf parts such as springs, bolts, and axles for gears. The key to laser cutting our functional mechanisms is the selective cutting and gluing of stacks of sheet material. Designing mechanisms for this workflow is non-trivial, therefore we contribute a set of mechanical primitives that are compatible with our lamination workflow and can be combined to realize advanced mechanical systems. Our software design environment facilitates the process of inserting and composing our mechanical primitives and realizing functional laser-cut objects.
SP  - 304
EP  - 316
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415885
ER  - 

TY  - NA
AU  - Seraji, Mohammad Rajabi; Stuerzlinger, Wolfgang
TI  - HybridAxes: An Immersive Analytics Tool With Interoperability Between 2D and Immersive Reality Modes
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00036
ER  - 

TY  - CHAP
AU  - Ye, Xinhui; Frens, Joep; Hu, Jun
TI  - Exploring Future Tools for Supporting Remote Collaborative Prototyping Process
PY  - 2022
AB  - NA
SP  - 2806
EP  - 2820
JF  - [ ] With Design: Reinventing Design Modes
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-19-4472-7_181
ER  - 

TY  - JOUR
AU  - Lin, Yanna; Zeng, Wei; Ye, Yu; Qu, Huamin
TI  - Saliency-aware color harmony models for outdoor signboard
PY  - 2022
AB  - NA
SP  - 25
EP  - 35
JF  - Computers & Graphics
VL  - 105
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2022.04.012
ER  - 

TY  - JOUR
AU  - Shin, Hyungsub; Chang, Seokhee; Yu, Namgyenong; Jeong, Chaeeun; Xi, Wen; Bae, Jihyun
TI  - Development of a Backpack-Based Wearable Proximity Detection System
PY  - 2022
AB  - NA
SP  - 647
EP  - 654
JF  - Fashion & Textile Research Journal
VL  - 24
IS  - 5
PB  - 
DO  - 10.5805/sfti.2022.24.5.647
ER  - 

TY  - NA
AU  - Bernard, Corentin; Monnoyer, Jocelyn; Ystad, Sølvi; Wiertlewski, Michael
TI  - Eyes-Off Your Fingers: Gradual Surface Haptic Feedback Improves Eyes-Free Touchscreen Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501872
ER  - 

TY  - NA
AU  - Tamai, Yura; Oki, Maho; Tsukada, Koji
TI  - AHs - POV Display and Interaction Methods extending Smartphone
PY  - 2021
AB  - Persistence of Vision (POV) display uses a technique that presents an after-image that occurs when rhythmic blinking LEDs are moved swiftly. It is a cost-effective and simple method used to turn a large surface into a display. In addition, a characteristic visual expression, such as one floating in the air, can be produced. This technique is widely used because of its advantages, however, it has the limitation of poor interactivity. In this study, a system in which a smartphone can be used as a POV display by mounting a rotation mechanism and installing original applications is suggested. Furthermore, examples of the application and the possibility of interaction methods using this system are presented, including the concept, implementation, experimental results, and implications.
SP  - 183
EP  - 191
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458992
ER  - 

TY  - JOUR
AU  - Bobek, Szymon; Tadeja, Sławomir K.; Struski, Łukasz; Stachura, Przemysław; Kipouros, Timoleon; Tabor, Jacek; Nalepa, Grzegorz J.; Kristensson, Per Ola
TI  - Virtual Reality-Based Parallel Coordinates Plots Enhanced with Explainable AI and Data-Science Analytics for Decision-Making Processes
PY  - 2021
AB  - <jats:p>We present a refinement of the Immersive Parallel Coordinates Plots (IPCP) system for Virtual Reality (VR). The evolved system provides data-science analytics built around a well-known method for visualization of multidimensional datasets in VR. The data-science analytics enhancements consist of importance analysis and a number of clustering algorithms including a novel SuMC (Subspace Memory Clustering) solution. These analytical methods were applied to both the main visualizations and supporting cross-dimensional scatter plots. They automate part of the analytical work that in the previous version of IPCP had to be done by an expert. We test the refined system with two sample datasets that represent the optimum solutions of two different multi-objective optimization studies in turbomachinery. The first one describes 54 data items with 29 dimensions (DS1), and the second 166 data items with 39 dimensions (DS2). We include the details of these methods as well as the reasoning behind selecting some methods over others.</jats:p>
SP  - 331
EP  - 331
JF  - Applied Sciences
VL  - 12
IS  - 1
PB  - 
DO  - 10.3390/app12010331
ER  - 

TY  - NA
AU  - Devrio, Nathan; Harrison, Chris
TI  - DiscoBand: Multiview Depth-Sensing Smartwatch Strap for Hand, Body and Environment Tracking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545634
ER  - 

TY  - JOUR
AU  - Garcia, Davide Astiaso; Groppi, Daniele; Tavakoli, Siamak
TI  - Developing and testing a new tool to foster wind energy sector industrial skills.
PY  - 2020
AB  - The wind energy sector has seen an increasing growth in the last decade and this is foreseen to continue in the next years. This has posed several challenges in terms of skilled and prepared professionals that have always to be up to date in an industry that is constantly changing. Thus, teaching tools have gained an increasing interest. The present research reviewed the state of the art in terms of digital interactive training tools pinpointing that the existing options do not feature the user involvement in the development of the training material. Hence, the main aim of this paper is to develop and test an innovative method based on gamification to increase wind energy sector industrial skills, providing a digital interactive environment in the form of a new user-friendly software that can allow its users to train and contribute to the teaching and learning contents. The first methodological step deals with the associated background studies that were required at strategy implementation and development stages, including market analysis and technology trade-offs, as well as the general structure and the implementation steps of the software design. Obtained results pinpointed that with minimal use of web-based database and network connectivity, a mobile phone application could work in the form of a time-scored quiz application that remotely located staff at wind energy farms could benefit from. The technological innovation brought by this research will substantially improve the service of training, allowing a more dynamic formative management contributing to an improvement in the competitiveness and a step towards excellence for the whole sector.
SP  - 124549
EP  - 124549
JF  - Journal of cleaner production
VL  - 282
IS  - NA
PB  - 
DO  - 10.1016/j.jclepro.2020.124549
ER  - 

TY  - JOUR
AU  - Reina, Guido; Childs, Hank; Matkovic, Kresimir; Bühler, Katja; Waldner, Manuela; Pugmire, David; Kozlíková, Barbora; Ropinski, Timo; Ljung, Patric; Itoh, Takayuki; Gröller, M. Eduard; Krone, Michael
TI  - The moving target of visualization software for an increasingly complex world
PY  - 2020
AB  - Abstract Visualization has evolved into a mature scientific field and it has also become widely accepted as a standard approach in diverse fields, including physics, life sciences, and business intelligence. However, despite its successful development, there are still many open research questions that require customized implementations in order to explore and establish concepts, and to perform experiments and take measurements. Many methods and tools have been developed and published but most are stand-alone prototypes and have not reached a mature state that can be used in a reliable manner by collaborating domain scientists or a wider audience. In this study, we discuss the challenges, solutions, and open research questions that affect the development of sophisticated, relevant, and novel scientific visualization solutions with minimum overheads. We summarize and discuss the results of a recent National Institute of Informatics Shonan seminar on these topics.
SP  - 12
EP  - 29
JF  - Computers & Graphics
VL  - 87
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2020.01.005
ER  - 

TY  - JOUR
AU  - Hurter, Christophe; Riche, Nathalie Henry; Drucker, Steven M.; Cordeil, Maxime; Alligier, Richard; Vuillemot, Romain
TI  - FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights
PY  - 2018
AB  - Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.
SP  - 704
EP  - 714
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865191
ER  - 

TY  - NA
AU  - Wang, Xi; Bylinskii, Zoya; Castelhano, Monica S.; Hillis, James M.; Duchowski, Andrew T.
TI  - CHI Extended Abstracts - EMICS’21: Eye Movements as an Interface to Cognitive State
PY  - 2021
AB  - Eye movement recording has been extensively used in HCI and offers the possibility to understand how information is perceived and processed by users. Hardware developments provide the ubiquitous accessibility of eye recording, allowing eye movements to enter common usage as a control modality. Recent A.I. developments provide powerful computational means to make predictions about the user. However, the connection between eye movements and cognitive state has been largely under-exploited in HCI. Despite the rich literature in psychology, a deeper understanding of its usability in practice is still required. This virtual EMICS workshop will provide an opportunity to discuss possible application scenarios and HCI interfaces to infer users’ mental state from eye movements. It will bring together researchers across disciplines with the goal of expanding shared knowledge, discussing innovative research directions and methods, fostering future collaborations around the use of eye movements as an interface to cognitive state.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3441357
ER  - 

TY  - NA
AU  - Varga, Virag; Vakulya, Gergely; Buergisser, Benjamin; Riopelle, Nathan; Zünd, Fabio; Sumner, Robert W.; Gross, Thomas R.; Sample, Alanson P.
TI  - TEI - Real-Time Capture of Holistic Tangible Interactions
PY  - 2021
AB  - When digital applications aim to blend virtual and real worlds, understanding the actual physical actions of users becomes an important task; the precise timing of these tangible interaction events is needed, along with the identity, and possibly location and history, of all involved actors/objects. With multiple actors or objects, it is difficult to identify who touches which object and when. Instrumenting objects for Body Channel Communication (BCC) allows message exchange around the human body between instrumented objects and the user themselves. In this paper we show how BCC can be utilized to perform under real-time conditions so that we can directly notice touch events (and the identity of actors). TangibleID is a framework that unifies tangible interaction capture for objects and users based on wearable BCC. TangibleID provides identification and communication with tagged objects/users in less than 120 ms and supports a variety of tangible interactions, without the need to restrict user (hand) movements or to maintain line-of-sight connection to cameras. When an AR application is combined with TangibleID, a new tangible mixed reality experience is achieved, as demonstrated in the “Haunted Castle” showcase. The paper presents an end-to-end technical evaluation including trade-offs regarding robustness and speed of touch recognition, outlines the breadth of interaction modalities, and reports on an initial user assessment.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440658
ER  - 

TY  - JOUR
AU  - Smiley, Jim; Lee, Benjamin; Tandon, Siddhant; Cordeil, Maxime; Besançon, Lonni; Knibbe, Jarrod; Jenny, Bernhard; Dwyer, Tim
TI  - The MADE-Axis: A Modular Actuated Device to Embody the Axis of a Data Dimension
PY  - 2021
AB  - Tangible controls-especially sliders and rotary knobs-have been explored in a wide range of interactive applications for desktop and immersive environments. Studies have shown that they support greater precision and provide proprioceptive benefits, such as support for eyes-free interaction. However, such controls tend to be expressly designed for specific applications. We draw inspiration from a bespoke controller for immersive data visualisation, but decompose this design into a simple, wireless, composable unit featuring two actuated sliders and a rotary encoder. Through these controller units, we explore the interaction opportunities around actuated sliders; supporting precise selection, infinite scrolling, adaptive data representations, and rich haptic feedback; all within a mode-less interaction space. We demonstrate the controllers' use for simple, ad hoc desktop interaction,before moving on to more complex, multi-dimensional interactions in VR and AR. We show that the flexibility and composability of these actuated controllers provides an emergent design space which covers the range of interactive dynamics for visual analysis. In a user study involving pairs performing collaborative visual analysis tasks in mixed-reality, our participants were able to easily compose rich visualisations, make insights and discuss their findings.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3488546
ER  - 

TY  - NA
AU  - Terenti, Mihail; Vatavu, Radu-Daniel
TI  - Measuring the User Experience of Vibrotactile Feedback on the Finger, Wrist, and Forearm for Touch Input on Large Displays
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519704
ER  - 

TY  - NA
AU  - Toyohara, Soichiro; Sato, Toshiki; Koike, Hideki
TI  - VRST - Balloonygen: extended tabletop display embedded with balloon-like deformable spherical screen
PY  - 2018
AB  - Balloonygen, an extended tabletop display embedded with a balloon-like deformable spherical screen, is a display that can seamlessly expose a spherical screen for three-dimensional contents, such as omnidirectional images, in a conventional flat display. By continuously morphing between a two-dimensional shape called tabletop and a three-dimensional shape called sphere, we render the benefits of a flat display and a spherical display to coexist and propose a smoother approach for information sharing. Balloonygen dynamically provides an optimal way to display the contents by inflating the rubber membrane installed at the center of a tabletop display and morphing between the two- and three-dimensional shapes. In this study, by prototyping and designing the application scenario, we discuss the advantages and disadvantages of this display and possible interactions involved.
SP  - 7
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281532
ER  - 

TY  - JOUR
AU  - Saraee, Elham; Jalal, Mona; Betke, Margrit
TI  - Visual complexity analysis using deep intermediate-layer features
PY  - 2020
AB  - Abstract In this paper, we focus on visual complexity, an image attribute that humans can subjectively evaluate based on the level of details in the image. We explore unsupervised information extraction from intermediate convolutional layers of deep neural networks to measure visual complexity. We derive an activation energy metric that combines convolutional layer activations to quantify visual complexity. To show the effectiveness of our proposed metric for various applications, we introduce Savoias , a visual complexity dataset that compromises of more than 1,400 images from seven diverse image categories (e.g., advertisement and interior design). We demonstrate high correlations of our deep neural network-based measure of visual complexity with human-curated ground-truth (GT) scores on various widely used network architectures, e.g., VGG16, ResNet-v2-152, and EfficientNet, and in networks trained on two classification tasks (object and scene classification). This result reveals that intermediate convolutional layers of deep neural networks carry information about the complexity of images that is meaningful to people. Furthermore, we show that our method of measuring visual complexity outperforms traditional methods on Savoias and two other state-of-the-art benchmark datasets. Moreover, we perform extensive analysis on the performance difference between our unsupervised method and a supervised method trained on the feature map, and show that by supervision, we can improve the prediction. Finally, we demonstrate that, within the context of a category, visually more complex images are also more memorable to human observers.
SP  - 102949
EP  - NA
JF  - Computer Vision and Image Understanding
VL  - 195
IS  - NA
PB  - 
DO  - 10.1016/j.cviu.2020.102949
ER  - 

TY  - JOUR
AU  - Vanderdonckt, Jean; Vatavu, Radu-Daniel
TI  - Extensible, Extendable, Expandable, Extractable: The 4E Design Approach for Reconfigurable Displays
PY  - 2021
AB  - We introduce “4E,” a new design approach of reconfigurable displays that can change their form factors by capitalizing on four quality properties inspired by applied material: extensibility, extend...
SP  - 1720
EP  - 1736
JF  - International Journal of Human–Computer Interaction
VL  - 37
IS  - 18
PB  - 
DO  - 10.1080/10447318.2021.1908666
ER  - 

TY  - CONF
AU  - Leiva, Luis A.; Xue, Yunfei; Bansal, Avya; Tavakoli, Hamed R.; Köroðlu, Tuðçe; Du, Jingzhou; Dayama, Niraj Ramesh; Oulasvirta, Antti
TI  - MobileHCI - Understanding Visual Saliency in Mobile User Interfaces
PY  - 2020
AB  - For graphical user interface (UI) design, it is important to understand what attracts visual attention. While previous work on saliency has focused on desktop and web-based UIs, mobile app UIs differ from these in several respects. We present findings from a controlled study with 30 participants and 193 mobile UIs. The results speak to a role of expectations in guiding where users look at. Strong bias toward the top-left corner of the display, text, and images was evident, while bottom-up features such as color or size affected saliency less. Classic, parameter-free saliency models showed a weak fit with the data, and data-driven models improved significantly when trained specifically on this dataset (e.g., NSS rose from 0.66 to 0.84). We also release the first annotated dataset for investigating visual saliency in mobile UIs.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Yang, Yalong; Dwyer, Tim; Jenny, Bernhard; Marriott, Kim; Cordeil, Maxime; Chen, Haohui
TI  - Origin-Destination Flow Maps in Immersive Environments
PY  - 2018
AB  - Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call MapsLink , involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that careful use of the third spatial dimension can resolve visual clutter in complex flow maps.
SP  - 693
EP  - 703
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865192
ER  - 

TY  - JOUR
AU  - Tong, Wai; Chen, Zhutian; Xia, Meng; Lo, Leo Yu-Ho; Yuan, Linping; Bach, Benjamin; Qu, Huamin
TI  - Exploring Interactions with Printed Data Visualizations in Augmented Reality.
PY  - 2022
AB  - This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops (N=20) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter)action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study (N=12, HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement "point" for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209386
ER  - 

TY  - NA
AU  - Alvina, Jessalyn; Bunt, Andrea; Chilana, Parmit K.; Malacria, Sylvain; McGrenere, Joanna
TI  - Conference on Designing Interactive Systems - Where is that Feature?: Designing for Cross-Device Software Learnability
PY  - 2020
AB  - People increasingly access cross-device applications from their smartphones while on the go. Yet, they do not fully use the mobile versions for complex tasks, preferring the desktop version of the same application. We conducted a survey (N=77) to identify challenges when switching back and forth between devices. We discovered significant cross-device learnability issues, including that users often find exploring the mobile version frustrating, which leads to prematurely giving up on using the mobile version. Based on the findings, we created four design concepts as video prototypes to explore how to support cross-device learnability. The concepts vary in four key dimensions: the device involved, automation, temporality, and learning approach. Interviews (N=20) probing the design concepts identified individual differences affecting cross-device learning preferences, and that users are more motivated to use cross-device applications when offered the right cross-device learnability support. We conclude with future design directions for supporting seamless cross-device learnability.
SP  - 1103
EP  - 1115
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395506
ER  - 

TY  - JOUR
AU  - Yang, Yalong; Cordeil, Maxime; Beyer, Johanna; Dwyer, Tim; Marriott, Kim; Pfister, Hanspeter
TI  - Embodied Navigation in Immersive Abstract Data Visualization: Is Overview+Detail or Zooming Better for 3D Scatterplots?
PY  - 2021
AB  - Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale. Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied. We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview. We find significant differences in participants' response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.
SP  - 1214
EP  - 1224
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030427
ER  - 

TY  - NA
AU  - Sun, Ke; Yu, Chun; Shi, Weinan; Liu, Lan; Shi, Yuanchun
TI  - UIST - Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands
PY  - 2018
AB  - We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech. Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize the issued commands with an end-to-end deep learning model. Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps). We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efficiency, and comparing with voiced commands with regards to personal privacy and social norms. We demonstrate that Lip-Interact can help users access functionality efficiently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more fluent.
SP  - 581
EP  - 593
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242599
ER  - 

TY  - NA
AU  - Jannat, Marium-E-; Vo, Thuan T; Hasan, Khalad
TI  - Face-Centered Spatial User Interfaces on Smartwatches
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519720
ER  - 

TY  - JOUR
AU  - Batch, Andrea; Cunningham, Andrew; Cordeil, Maxime; Elmqvist, Niklas; Dwyer, Tim; Thomas, Bruce H.; Marriott, Kim
TI  - There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics
PY  - 2019
AB  - Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.
SP  - 536
EP  - 546
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2019.2934803
ER  - 

TY  - JOUR
AU  - Xie, Yujia; Wang, Meizhen; Liu, Xuejun; Wang, Xing; Wu, Yiguang; Wang, Feiyue; Wang, Xiaozhi
TI  - Multi-camera video synopsis of a geographic scene based on optimal virtual viewpoint
PY  - 2021
AB  - NA
SP  - 1221
EP  - 1239
JF  - Transactions in GIS
VL  - 26
IS  - 3
PB  - 
DO  - 10.1111/tgis.12862
ER  - 

TY  - NA
AU  - Vekemans, Verindi; Leenders, Ward; Zhu, Sijie; Liang, Rong-Hao
TI  - ISWC - MOTUS: Rendering Emotions with a Wrist-worn Tactile Display
PY  - 2021
AB  - This paper investigates whether tactile texture patterns on the wrists can be interpreted as particular emotions. A prototype watch-back tactile display, MOTUS, was implemented to press different texture patterns in various frequencies onto a wrist to convey emotions. We conducted a preliminary guessability study with the prototype. The result reveals the wearers’ agreement in interpreting the emotional states from the tactile texture patterns.
SP  - 159
EP  - 161
JF  - 2021 International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460421.3480390
ER  - 

TY  - NA
AU  - Müller, Philipp; Staal, Sander; Bâce, Mihai; Bulling, Andreas
TI  - Designing for Noticeability: Understanding the Impact of Visual Importance on Desktop Notifications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501954
ER  - 

TY  - NA
AU  - Wu, Aoyu; Xie, Liwenhan; Lee, Bongshin; Wang, Yun; Cui, Weiwei; Qu, Huamin
TI  - CHI - Learning to Automate Chart Layout Configurations Using Crowdsourced Paired Comparison
PY  - 2021
AB  - We contribute a method to automate parameter configurations for chart layouts by learning from human preferences. Existing charting tools usually determine the layout parameters using predefined heuristics, producing sub-optimal layouts. People can repeatedly adjust multiple parameters (e.g., chart size, gap) to achieve visually appealing layouts. However, this trial-and-error process is unsystematic and time-consuming, without a guarantee of improvement. To address this issue, we develop Layout Quality Quantifier (LQ2), a machine learning model that learns to score chart layouts from paired crowdsourcing data. Combined with optimization techniques, LQ2 recommends layout parameters that improve the charts’ layout quality. We apply LQ2 on bar charts and conduct user studies to evaluate its effectiveness by examining the quality of layouts it produces. Results show that LQ2 can generate more visually appealing layouts than both laypeople and baselines. This work demonstrates the feasibility and usages of quantifying human preferences and aesthetics for chart layouts.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445179
ER  - 

TY  - JOUR
AU  - Sun, Maoyuan; Shaikh, Abdul Rahman; Alhoori, Hamed; Zhao, Jian
TI  - SightBi: Exploring Cross-View Data Relationships with Biclusters.
PY  - 2021
AB  - Multiple-view visualization (MV) has been heavily used in visual analysis tools for sensemaking of data in various domains (e.g., bioinformatics, cybersecurity and text analytics). One common task of visual analysis with multiple views is to relate data across different views. For example, to identify threats, an intelligence analyst needs to link people from a social network graph with locations on a crime-map, and then search for and read relevant documents. Currently, exploring cross-view data relationships heavily relies on view-coordination techniques (e.g., brushing and linking), which may require significant user effort on many trial-and-error attempts, such as repetitiously selecting elements in one view, and then observing and following elements highlighted in other views. To address this, we present SightBi, a visual analytics approach for supporting cross-view data relationship explorations. We discuss the design rationale of SightBi in detail, with identified user tasks regarding the use of cross-view data relationships. SightBi formalizes cross-view data relationships as biclusters, computes them from a dataset, and uses a bi-context design that highlights creating stand-alone relationship-views. This helps preserve existing views and offers an overview of cross-view data relationships to guide user exploration. Moreover, SightBi allows users to interactively manage the layout of multiple views by using newly created relationship-views. With a usage scenario, we demonstrate the usefulness of SightBi for sensemaking of cross-view data relationships.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2021.3114801
ER  - 

TY  - NA
AU  - Reski, Nico; Alissandrakis, Aris; Tyrkkö, Jukka; Kerren, Andreas
TI  - NordiCHI - “Oh, that’s where you are!” – Towards a Hybrid Asymmetric Collaborative Immersive Analytics System
PY  - 2020
AB  - We present a hybrid Immersive Analytics system to support asymmetrical collaboration between a pair of users during synchronous data exploration. The system consists of an immersive Virtual Reality application, a non-immersive web application, and a real-time communication interface connecting both applications to provide features to facilitate the collaborators’ mutual understanding and their ability to make (spatial) references. We conducted a real world case study with pairs of language students, encouraging them to use the developed system to investigate a large multivariate Twitter dataset from a sociolinguistic perspective within an explorative analysis scenario. Based on the results of usability scores, log file analyses, observations, and interviews, we were able to validate the approach in general, and gain insights into the users’ collaboration with respect to awareness, deixis, and group dynamics.
SP  - 1
EP  - 12
JF  - Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3419249.3420102
ER  - 

TY  - JOUR
AU  - Chen, Zhutian; Wang, Yun; Wang, Qianwen; Wang, Yong; Qu, Huamin
TI  - Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline
PY  - 2019
AB  - Designers need to consider not only perceptual effectiveness but also visual styles when creating an infographic. This process can be difficult and time consuming for professional designers, not to mention non-expert users, leading to the demand for automated infographics design . As a first step, we focus on timeline infographics, which have been widely used for centuries. We contribute an end-to-end approach that automatically extracts an extensible timeline template from a bitmap image. Our approach adopts a deconstruction and reconstruction paradigm. At the deconstruction stage, we propose a multi-task deep neural network that simultaneously parses two kinds of information from a bitmap timeline: 1) the global information, i.e. , the representation, scale, layout , and orientation of the timeline, and 2) the local information, i.e. , the location, category , and pixels of each visual element on the timeline. At the reconstruction stage, we propose a pipeline with three techniques, i.e. , Non-Maximum Merging, Redundancy Recover , and DL GrabCut , to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate the effectiveness of our approach, we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images) from the Internet. We first report quantitative evaluation results of our approach over the two datasets. Then, we present examples of automatically extracted templates and timelines automatically generated based on these templates to qualitatively demonstrate the performance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics.
SP  - 917
EP  - 926
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2019.2934810
ER  - 

TY  - JOUR
AU  - Pester, Britta; Russig, Benjamin; Winke, Oliver; Ligges, Carolin; Dachselt, Raimund; Gumhold, Stefan
TI  - Understanding multi-modal brain network data: An immersive 3D visualization approach
PY  - 2022
AB  - Understanding the human brain requires the incorporation of functional interaction patterns that depend on a variety of features like experimental setup, strength of directed connectedness or variability between several individuals or groups. In addition to these external factors, there are internal properties of the brain network as for example temporal propagation of connections, or connectivity patterns that only occur in a distinct frequency range of the signal. The visualization of detected networks covering all necessary information poses a substantial problem which is mainly due to the high number of features that have to be integrated within the same view in a natural spatial context. To address this problem, we propose a new tool that transfers the network into an anatomically arranged origin–destination view in a virtual visual analysis lab. This offers the user an opportunity to assess the temporal evolution of connectivity patterns and provides an intuitive and motivating way of exploring the corresponding features via navigation and interaction in virtual reality (VR). The approach was evaluated in a user study including participants with neuroscientific background as well as people working in the field of computer science. As a first proof of concept trial we used functional brain networks derived from time series of electroencephalography recordings evoked by visual stimuli. All participants gave a positive general feedback, notably they saw a benefit in using the VR view instead of the compared 2D desktop variant. This suggests that our application successfully fills a gap in the visualization of high-dimensional brain networks and that it is worthwhile to further follow and enhance the proposed representation method. • We developed a novel concept for 3D VR visualization of multi-mode EEG brain connectivity networks comprising multiple features within one view. • The anatomical arrangement of network nodes, together with color-coded temporal evolution along directed links between the nodes offers an intuitive understanding of network patterns. • Users can interactively explore brain networks in VR, leading to a higher motivation and offering the possibility of a non-hypothesis-driven research. • According to a standard system usability scale questionnaire, experts from neuroscience, as well as from the field of computer science are convinced by the system usability.
SP  - 88
EP  - 97
JF  - Computers & Graphics
VL  - 106
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2022.05.024
ER  - 

TY  - JOUR
AU  - Nogales, Ruben; Benalcazar, Marco E.
TI  - Hand gesture recognition using machine learning and infrared information: a systematic literature review
PY  - 2021
AB  - Currently, gesture recognition is like a problem of feature extraction and pattern recognition, in which a movement is labeling as belonging to a given class. A gesture recognition system’s response could solve different problems in various fields, such as medicine, robotics, sign language, human–computer interfaces, virtual reality, augmented reality, and security. In this context, this work proposes a systematic literature review of hand gesture recognition based on infrared information and machine learning algorithms. This systematic literature review is an extended version of the work presented at the 2019 ICSE conference. To develop this systematic literature review, we used the Kitchenham methodology. This systematic literature review retrieves information about the models’ architectures, the implemented techniques in each module, the type of learning used (supervised, unsupervised, semi-supervised, and reinforcement learning), and recognition accuracy classification, and the processing time. Also, it will identify literature gaps for future research.
SP  - 2859
EP  - 2886
JF  - International Journal of Machine Learning and Cybernetics
VL  - 12
IS  - 10
PB  - 
DO  - 10.1007/s13042-021-01372-y
ER  - 

TY  - NA
AU  - Powley, Benjamin Thomas
TI  - VL/HCC - Exploring Immersive and Non-Immersive Techniques for Geographic Data Visualization
PY  - 2020
AB  - Analyzing multi-dimensional geospatial data is difficult and immersive analytics systems are used to visualize geospatial data and models. There is little previous work evaluating when immersive and non-immersive visualizations are the most suitable for data analysis and more research is needed.
SP  - 1
EP  - 2
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127197
ER  - 

TY  - NA
AU  - Leiva, Luis A.; Xue, Yunfei; Bansal, Avya; Tavakoli, Hamed R.; Köroğlu, Tuğçe; Dayama, Niraj Ramesh; Oulasvirta, Antti
TI  - Understanding Visual Saliency in Mobile User Interfaces
PY  - 2020
AB  - For graphical user interface (UI) design, it is important to understand what attracts visual attention. While previous work on saliency has focused on desktop and web-based UIs, mobile app UIs differ from these in several respects. We present findings from a controlled study with 30 participants and 193 mobile UIs. The results speak to a role of expectations in guiding where users look at. Strong bias toward the top-left corner of the display, text, and images was evident, while bottom-up features such as color or size affected saliency less. Classic, parameter-free saliency models showed a weak fit with the data, and data-driven models improved significantly when trained specifically on this dataset (e.g., NSS rose from 0.66 to 0.84). We also release the first annotated dataset for investigating visual saliency in mobile UIs.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403557
ER  - 

TY  - NA
AU  - Peng, Huaishu; Briggs, Jimmy; Wang, Cheng Yao; Guo, Kevin; Kider, Joseph T.; Mueller, Stefanie; Baudisch, Patrick; Guimbretière, François
TI  - CHI - RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer
PY  - 2018
AB  - We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience. As a designer creates a new model using RoMA AR CAD editor, features are constructed concurrently by a 3D printing robotic arm sharing the same design volume. The partially printed physical model then serves as a tangible reference for the designer as she adds new elements to her design. RoMA's proxemics-inspired handshake mechanism between the designer and the 3D printing robotic arm allows the designer to quickly interrupt printing to access a printed area or to indicate that the robot can take full control of the model to finish printing. RoMA lets users integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts or to extend existing objects. We conclude by presenting the strengths and limitations of our current design.
SP  - 579
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174153
ER  - 

TY  - NA
AU  - Whitlock, Matt; Smart, Stephen; Szafir, Danielle Albers
TI  - VR - Graphical Perception for Immersive Analytics
PY  - 2020
AB  - Immersive Analytics (IA) uses immersive virtual and augmented reality displays for data visualization and visual analytics. Designers rely on studies of how accurately people interpret data in different visualizations to make effective visualization choices. However, these studies focus on data analysis in traditional desktop environments. We lack empirical grounding for how to best visualize data in immersive environments. This study explores how people interpret data visualizations across different display types by measuring how quickly and accurately people conduct three analysis tasks over five visual channels: color, size, height, orientation, and depth. We identify key quantitative differences in performance and user behavior, indicating that stereo viewing resolves some of the challenges of visualizations in 3D space. We also find that while AR displays encourage increased navigation, they decrease performance with color-based visualizations. Our results provide guidelines on how to tailor visualizations to different displays in order to better leverage the affordances of IA modalities.
SP  - 616
EP  - 625
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1582298687237
ER  - 

TY  - NA
AU  - Bala, Paulo; Oakley, Ian; Nisi, Valentina; Nunes, Nuno Jardim
TI  - CHI - Dynamic Field of View Restriction in 360° Video: Aligning Optical Flow and Visual SLAM to Mitigate VIMS
PY  - 2021
AB  - Head-Mounted Display based Virtual Reality is proliferating. However, Visually Induced Motion Sickness (VIMS), which prevents many from using VR without discomfort, bars widespread adoption. Prior work has shown that limiting the Field of View (FoV) can reduce VIMS at a cost of also reducing presence. Systems that dynamically adjust a user’s FoV may be able to balance these concerns. To explore this idea, we present a technique for standard 360° video that shrinks FoVs only during VIMS inducing scenes. It uses Visual Simultaneous Localization and Mapping and peripheral optical flow to compute camera movements and reduces FoV during rapid motion or optical flow. A user study (N=23) comparing 360° video with unrestricted-FoVs (90°), reduced fixed-FoVs (40°) and dynamic-FoVs (40°-90°) revealed that dynamic-FoVs mitigate VIMS while maintaining presence. We close by discussing the user experience of dynamic-FoVs and recommendations for how they can help make VR comfortable and immersive for all.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445499
ER  - 

TY  - NA
AU  - Ross, Sam H.; Ozmen, Ecenaz Jen; Kogan, Maria V.; Chilton, Lydia B.
TI  - IUI - WordBlender: principles and tools for generating word blends
PY  - 2020
AB  - Combining text and images is a powerful strategy in graphic design because images convey meaning faster, but text conveys more precise meaning. Word blends are a technique to combine both elements in a succinct yet expressive image. In a word blend, a letter is replaced by a symbol relevant to the message. This is difficult because the replacement must look blended enough to be readable, yet different enough to recognize the symbol. Currently, there are no known design principles to find aesthetically pleasing word blends. To establish these principles, we run two experiments and find that to be readable, the object should have a similar shape as the letter. However, to be aesthetically pleasing, the font should match some of the secondary features of the image: color, style, and thickness. We present WordBlender, an AI-powered design tool to quickly and easily create word blends based on these visual design principles. WordBlender automatically generates shape-based matches and allows users to explore combinations of color, style, and font that improve the design of blends.
SP  - 38
EP  - 42
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377527
ER  - 

TY  - NA
AU  - Fosco, Camilo; Casser, Vincent; Bedi, Amish Kumar; O'Donovan, Peter; Hertzmann, Aaron; Bylinskii, Zoya
TI  - Predicting Visual Importance Across Graphic Design Types
PY  - 2020
AB  - This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance. The model, code, and importance dataset are available at this https URL .
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cordeil, Maxime; Cunningham, Andrew; Bach, Benjamin; Hurter, Christophe; Thomas, Bruce H.; Marriott, Kim; Dwyer, Tim
TI  - VR - IATK: An Immersive Analytics Toolkit
PY  - 2019
AB  - We introduce IATK, the Immersive Analytics Toolkit, a software package for Unity that allows interactive authoring and exploration of data visualisation in immersive environments. The design of IATK was informed by interdisciplinary expert-collaborations as well as visual analytics applications and iterative refinement over several years. IATK allows for easy assembly of visualisations through a grammar of graphics that a user can configure in a GUI-in addition to a dedicated visualisation API that supports the creation of novel immersive visualisation designs and interactions. IATK is designed with scalability in mind, allowing visualisation and fluid responsive interactions in the order of several million points at a usable frame rate. This paper outlines our design requirements, IATK's framework design and technical features, its user interface, as well as application examples.
SP  - 200
EP  - 209
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797978
ER  - 

TY  - JOUR
AU  - Drogemuller, Adam; Cunningham, Andrew; Walsh, James A.; Thomas, Bruce H.; Cordeil, Maxime; Ross, William
TI  - Examining virtual reality navigation techniques for 3D network visualisations
PY  - 2020
AB  - NA
SP  - 100937
EP  - NA
JF  - Journal of Computer Languages
VL  - 56
IS  - NA
PB  - 
DO  - 10.1016/j.cola.2019.100937
ER  - 

TY  - NA
AU  - Kienzle, Wolf; Whitmire, Eric; Rittaler, Chris; Benko, Hrvoje
TI  - CHI - ElectroRing: Subtle Pinch and Touch Detection with a Ring
PY  - 2021
AB  - We present ElectroRing, a wearable ring-based input device that reliably detects both onset and release of a subtle finger pinch, and more generally, contact of the fingertip with the user’s skin. ElectroRing addresses a common problem in ubiquitous touch interfaces, where subtle touch gestures with little movement or force are not detected by a wearable camera or IMU. ElectroRing’s active electrical sensing approach provides a step-function-like change in the raw signal, for both touch and release events, which can be easily detected using only basic signal processing techniques. Notably, ElectroRing requires no second point of instrumentation, but only the ring itself, which sets it apart from existing electrical touch detection methods. We built three demo applications to highlight the effectiveness of our approach when combined with a simple IMU-based 2D tracking system.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445094
ER  - 

TY  - JOUR
AU  - Cheng, Shiwei; Fan, Jing; Hu, Yilin
TI  - Visual saliency model based on crowdsourcing eye tracking data and its application in visual design
PY  - 2020
AB  - The visual saliency models based on low-level features of an image have the problem of low accuracy and scalability, while the visual saliency models based on deep neural networks can effectively improve the prediction performance, but require a large amount of training data, e.g., eye tracking data, to achieve good results. However, the traditional eye tracking method is limited by high equipment and time cost, complex operation process, low user experience, etc. Therefore, this paper proposed a visual saliency model based on crowdsourcing eye tracking data, which was collected by gaze recall with self-reporting from crowd workers. Parameter optimization on our crowdsourcing method was explored, and it came out that the accuracy of gaze data reached 1° of visual angle, which was 3.6% higher than other existed crowdsourcing methods. On this basis, we collected a webpage dataset of crowdsourcing gaze data and constructed a visual saliency model based on a fully convolutional neural network (FCN). The evaluation results showed that after trained by crowdsourcing gaze data, the model performed better, such as prediction accuracy increased by 44.8%. Also, our model outperformed the existing visual saliency models. We also applied our model to help webpage designers evaluate and revise their visual designs, and the experimental results showed that the revised design obtained improved ratings by 8.2% compared to the initial design.
SP  - 1
EP  - 18
JF  - Personal and Ubiquitous Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s00779-020-01463-7
ER  - 

TY  - JOUR
AU  - Santos-Torres, Andrés; Zarraonandia, Telmo; Díaz, Paloma; Onorati, Teresa; Aedo, Ignacio
TI  - An empirical comparison of interaction styles for map interfaces in immersive virtual environments
PY  - 2020
AB  - Geographical Information Systems (GIS) can be visualized using immersive technologies like Virtual Reality (VR). Before using this kind of technologies it is required to explore which interactions are affordable, efficient and satisfactory from the users’ point of view. The purpose of this work is to provide insight on how to design efficient and natural interaction on GIS VR interfaces. This study presents a within-subjects comparative study that assesses the usability and performance of two popular interaction strategies: body-based interaction and device based interaction. In body-based interaction, participants use their hands and head orientation to control the VR map. In the second case, users interact with the Oculus Touch controller. Thirty two users participated in an experiment whose results suggest that interacting with the controller improves performance of the selection task, in terms of time spent and error rate. Also, the results show a preference of users for the controller in terms of perceived usability.
SP  - 35717
EP  - 35738
JF  - Multimedia Tools and Applications
VL  - 79
IS  - 47
PB  - 
DO  - 10.1007/s11042-020-08709-9
ER  - 

TY  - JOUR
AU  - Mikawa, Yuri; Sueishi, Tomohiro; Watanabe, Yoshihiro; Ishikawa, Masatoshi
TI  - Dynamic Projection Mapping for Robust Sphere Posture Tracking Using Uniform/Biased Circumferential Markers.
PY  - 2022
AB  - In spatial augmented reality, a widely dynamic projection mapping system has been developed as a novel approach to graphics presentation for widely moving objects in dynamic situations. However, this method necessitates a novel tracking marker design that is resistant to random and complex occlusion and out-of-focus blurring, which conventional markers have not achieved. This article presents a uniform circumferential marker that becomes an ellipse in perspective projection and expresses geometric information. It can track the relative posture of a dynamically moving sphere with high speed, high accuracy, and robustness owing to continuous contour lines, thereby supporting both wide-range movement in the depth direction and human interaction. Moreover, a biased circumferential marker is proposed to embed unique coding, where the absolute posture is decoded with a novel recognition algorithm. Moreover, rough initialization using the geometry of multiple ellipses is proposed for both markers to start the automatic and precise tracking. Real-time rotation visualization onto the surface of a moving sphere is made possible with the high-speed, widely dynamic projection mapping system. The tracking performance is demonstrated to exhibit sufficient basic tracking performance as well as robustness against blurring and occlusion compared to conventional dot-based markers.
SP  - 4016
EP  - 4031
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3111085
ER  - 

TY  - NA
AU  - Li, Nianlong; Zhang, Zhengquan; Liu, Can; Yang, Zengyao; Fu, Yinan; Tian, Feng; Han, Teng; Fan, Mingming
TI  - CHI - vMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors
PY  - 2021
AB  - Interacting with out of reach or occluded VR objects can be cumbersome. Although users can change their position and orientation, such as via teleporting, to help observe and select, doing so frequently may cause loss of spatial orientation or motion sickness. We present vMirror, an interactive widget leveraging reflection of mirrors to observe and select distant or occluded objects. We first designed interaction techniques for placing mirrors and interacting with objects through mirrors. We then conducted a formative study to explore a semi-automated mirror placement method with manual adjustments. Next, we conducted a target-selection experiment to measure the effect of the mirror’s orientation on users’ performance. Results showed that vMirror can be as efficient as direct target selection for most mirror orientations. We further compared vMirror with teleport technique in a virtual treasure hunt game and measured participants’ task performance and subjective experiences. Finally, we discuss vMirorr user experience and present future directions.
SP  - 132
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445537
ER  - 

TY  - NA
AU  - Szafir, Daniel; Szafir, Danielle Albers
TI  - HRI - Connecting Human-Robot Interaction and Data Visualization
PY  - 2021
AB  - Human-robot interaction (HRI) research frequently explores how to design interfaces that enable humans to effectively teleoperate and supervise robots. One of the principle goals of such systems is to support data collection, analysis, and human decision making, which requires representing robot data in ways that support fast and accurate analyses by humans. However, the interfaces for these systems do not always use best-practice principles for effectively visualizing data. We present a new framework to scaffold reasoning about robot interface design that emphasizes the need to consider data visualization for supporting analysis and decision making processes, detail several data visualization best practices relevant to HRI, identify a set of core data tasks that commonly occur in HRI, and highlight several promising opportunities for further synergistic activities at the intersection of these two research areas.
SP  - 281
EP  - 292
JF  - Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3434073.3444683
ER  - 

TY  - JOUR
AU  - Mishra, Prerna; Kumar, Santosh; Chaube, Mithilesh Kumar
TI  - Graph Interpretation, Summarization and Visualization Techniques: A Review and Open Research Issues
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11042-021-11582-9
ER  - 

TY  - JOUR
AU  - Yuan, Linping; Zhou, Ziqi; Zhao, Jian; Guo, Yiqiu; Du, Fan; Qu, Huamin
TI  - InfoColorizer: Interactive Recommendation of Color Palettes for Infographics.
PY  - 2022
AB  - When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements? spatial arrangement. We propose a data-driven method that provides flexibility by considering users? preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3085327
ER  - 

TY  - NA
AU  - Darbar, Rajkumar; Roo, Joan Sol; Lainé, Thibault; Hachet, Martin
TI  - MUM - DroneSAR: extending physical spaces in spatial augmented reality using projection on a drone
PY  - 2019
AB  - Spatial Augmented Reality (SAR) transforms real-world objects into interactive displays by projecting digital content using video projectors. SAR enables co-located collaboration immediately between multiple viewers without the need to wear any special glasses. Unfortunately, one major limitation of SAR is that visual content can only be projected onto its physical supports. As a result, displaying User Interfaces (UI) widgets such as menus and pop-up windows in SAR is very challenging. We are trying to address this limitation by extending SAR space in mid-air. In this paper, we propose Drone-SAR, which extends the physical space of SAR by projecting digital information dynamically on the tracked panels mounted on a drone. DroneSAR is a proof of concept of novel SAR User Interface (UI), which provides support for 2D widgets (i.e., label, menu, interactive tools, etc.) to enrich SAR interactive experience. We also describe the implementation details of our proposed approach.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365610.3365631
ER  - 

TY  - NA
AU  - Butcher, Peter; John, Nigel W.; Ritsos, Panagiotis D.
TI  - CHI Extended Abstracts - VRIA - A Framework for Immersive Analytics on the Web
PY  - 2019
AB  - We report on the design, implementation and evaluation of , a framework for building immersive analytics (IA) solutions in Web-based Virtual Reality (VR), built upon WebVR, A-Frame, React and D3. The recent emergence of affordable VR interfaces have reignited the interest of researchers and developers in exploring new, immersive ways to visualize data. In particular, the use of open-standards web-based technologies for implementing VR in a browser facilitates the ubiquitous and platform-independent adoption of IA systems. Moreover, such technologies work in synergy with established visualization libraries, through the HTML document object model (DOM). We discuss high-level features of and present a preliminary user experience evaluation of one of our use cases.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312798
ER  - 

TY  - NA
AU  - Lee, Chunggi; Kim, Sang-Hoon; Han, Dongyun; Yang, Hongjun; Park, Young-Woo; Kwon, Bum Chul; Ko, Sungahn
TI  - CHI - GUIComp: A GUI Design Assistant with Real-Time, Multi-Faceted Feedback
PY  - 2020
AB  - Users may face challenges while designing graphical user interfaces, due to a lack of relevant experience and guidance. This paper aims to investigate the issues users face during the design process, and how to resolve them. To this end, we conducted semi-structured interviews, based on which we built a GUI prototyping assistance tool called GUIComp. This tool can be connected to GUI design software as an extension, and it provides real-time, multi-faceted feedback on a user's current design. Additionally, we conducted two user studies, in which we asked participants to create mobile GUIs with or without GUIComp, and requested online workers to assess the created GUIs. The experimental results show that GUIComp facilitated iterative designs and the participants with GUIComp had better a user experience and produced more acceptable designs than those who did not use it.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376327
ER  - 

TY  - NA
AU  - Wu, Aoyu; Wang, Yun; Zhou, Mengyu; He, Xinyi; Zhang, Haidong; Qu, Huamin; Zhang, Dongmei
TI  - MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation
PY  - 2021
AB  - We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the need of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Tadeja, Slawomir Konrad; Kipouros, Timoleon; Kristensson, Per Ola
TI  - CHI Extended Abstracts - Exploring Parallel Coordinates Plots in Virtual Reality
PY  - 2019
AB  - Parallel Coordinates Plots (PCP) are a widely used approach to interactively visualize and analyze multidimensional scientific data in a 2D environment. In this paper, we explore the use of Parallel Coordinates in an immersive Virtual Reality (VR) 3D visualization environment as a means to support the decision-making process in engineering design processes. We evaluate the potential of VR PCP using a formative qualitative study with seven participants. In a task involving 54 points with 29 dimensions per point, we found that participants were able to detect patterns in the dataset compared with a previously published study with two expert users using traditional 2D PCP, which acts as the gold standard for the dataset. The dataset describes the Pareto front for a three-objective aerodynamic design optimization study in turbomachinery.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3313068
ER  - 

TY  - NA
AU  - O'Leary, Jasper Tran; Nandi, Chandrakana; Lee, Khang; Peek, Nadya
TI  - UIST - Taxon: a Language for Formal Reasoning with Digital Fabrication Machines
PY  - 2021
AB  - Digital fabrication machines for makers have expanded access to manufacturing processes such as 3D printing, laser cutting, and milling. While digital models encode the data necessary for a machine to manufacture an object, understanding the trade-offs and limitations of the machines themselves is crucial for successful production. Yet, this knowledge is not codified and must be gained through experience, which limits both adoption of and creative exploration with digital fabrication tools. To formally represent machines, we present Taxon, a language that encodes a machine’s high-level characteristics, physical composition, and performable actions. With this programmatic foundation, makers can develop rules of thumb that filter for appropriate machines for a given job and verify that actions are feasible and safe. We integrate the language with a browser-based system for simulating and experimenting with machine workflows. The system lets makers engage with rules of thumb and enrich their understanding of machines. We evaluate Taxon by representing several machines from both common practice and digital fabrication research. We find that while Taxon does not exhaustively describe all machines, it provides a starting point for makers and HCI researchers to develop tools for reasoning about and making decisions with machines.
SP  - 691
EP  - 709
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474779
ER  - 

TY  - JOUR
AU  - Zhao, Maozheng; Cui, Wenzhe; Ramakrishnan, I. V.; Zhai, Shumin; Bi, Xiaojun
TI  - UIST - Voice and Touch Based Error-tolerant Multimodal Text Editing and Correction for Smartphones
PY  - 2021
AB  - Editing operations such as cut, copy, paste, and correcting errors in typed text are often tedious and challenging to perform on smartphones. In this paper, we present VT, a voice and touch-based multi-modal text editing and correction method for smartphones. To edit text with VT, the user glides over a text fragment with a finger and dictates a command, such as ”bold” to change the format of the fragment, or the user can tap inside a text area and speak a command such as ”highlight this paragraph” to edit the text. For text correcting, the user taps approximately at the area of erroneous text fragment and dictates the new content for substitution or insertion. VT combines touch and voice inputs with language context such as language model and phrase similarity to infer a user’s editing intention, which can handle ambiguities and noisy input signals. It is a great advantage over the existing error correction methods (e.g., iOS’s Voice Control) which require precise cursor control or text selection. Our evaluation shows that VT significantly improves the efficiency of text editing and text correcting on smartphones over the touch-only method and the iOS’s Voice Control method. Our user studies showed that VT reduced the text editing time by 30.80%, and text correcting time by 29.97% over the touch-only method. VT reduced the text editing time by 30.81%, and text correcting time by 47.96% over the iOS’s Voice Control method.
SP  - 162
EP  - 178
JF  - Proceedings of the ACM Symposium on User Interface Software and Technology. ACM Symposium on User Interface Software and Technology
VL  - 2021
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474742
ER  - 

TY  - NA
AU  - Homps, Francois; Beugin, Yohan; Vuillemot, Romain
TI  - VR - ReViVD: Exploration and Filtering of Trajectories in an Immersive Environment using 3D Shapes
PY  - 2020
AB  - We present ReViVD, a tool for exploring and filtering large trajectory-based datasets using virtual reality. ReViVD’s novelty lies in using simple 3D shapes—such as cuboids, spheres and cylinders—as queries for users to select and filter groups of trajectories. Building on this simple paradigm, more complex queries can be created by combining previously made selection groups through a system of user-created Boolean operations. We demonstrate the use of ReViVD in different application domains, from GPS position tracking to simulated data (e. g., turbulent particle flows and traffic simulation). Our results show the ease of use and expressiveness of the 3D geometric shapes in a broad range of exploratory tasks. Re- ViVD was found to be particularly useful for progressively refining selections to isolate outlying behaviors. It also acts as a powerful communication tool for conveying the structure of normally abstract datasets to an audience.
SP  - 729
EP  - 737
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581269207852
ER  - 

TY  - NA
AU  - Rosono, Satoshi; Miyake, Tamon; Tamaki, Emi
TI  - PondusHand: Estimation Method of Fingertips Force by User's Forearm Muscle Deformation based on Calibration with Mobile Phone's Touch Screen
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/biorob52689.2022.9925517
ER  - 

TY  - NA
AU  - Hosono, Satoshi; Nishimura, Shoji; Iwasaki, Ken; Tamaki, Emi
TI  - SSIP - A Method for Estimating the Load on Muscles Using a Wearable IR Sensor Array Device
PY  - 2019
AB  - The purpose of this research is to propose a system that allows users to objectively check the quantified load on their muscles in the exercise training, especially in rehabilitation and strength training. This is because it is difficult to accurately recognize the quantified load on the muscle. And there is a problem that an appropriate effect cannot be obtained from exercise training due to excessive load applied on muscles when exercise training. The proposed method is to estimate the load on muscles from the device using the IR sensor array and gauge sensor, using SVR (Support Vecotr Regression) which is a machine learning algorithm of regression. This is because the IR sensor device used in this research is a wearable device that is not bulky and is robust against noise such as sweat electromagnetic noise. As a conclusion, it can be said that the system we suggested is not bulky. In addition, it can be said that by using a sensor using infrared light, it is possible to estimate the muscle load using a sensor that is robust against noise. And it is found that there is a correlation between muscle bulge and the load on the hands and arms. Since the MAE is 2.88N and the standard deviation is 1.13N, the force applied to the user's muscles is confirmed during rehabilitation and training using exercise with isometric contraction that does not produce more force than his own power It can be said that this is an error that does not bother user.
SP  - 77
EP  - 81
JF  - Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365245.3365258
ER  - 

TY  - NA
AU  - Huo, Ke; Cao, Yuanzhi; Yoon, Sang Ho; Xu, Zhuangying; Chen, Guiming; Ramani, Karthik
TI  - CHI - Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes
PY  - 2018
AB  - The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.
SP  - 219
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173793
ER  - 

TY  - JOUR
AU  - Cofer, Savannah; Chen, Tyler N.; Yang, Jackie; Follmer, Sean
TI  - Detecting Touch and Grasp Gestures Using a Wrist-Worn Optical and Inertial Sensing Network
PY  - 2022
AB  - Freehand gesture-based interaction promises to enable rich interaction in applications such as augmented reality, virtual reality, human-robot interaction, and robotic prosthetic devices. However, current sensing approaches are limited to mid-air whole hand gestures and fail to identify small-scale tactile interactions with unsensorized environments. Detecting tactile interactions may unlock potential new applications in augmented reality and human-robot interaction in which unsensorized surfaces are used as touch input devices. This work presents a novel wrist-worn sensing device that combines near-infrared and inertial measurement unit sensing to enable high-accuracy detection of surface touch and grasp interactions. Two convolutional neural networks were used to map device inputs to detect touch events, and classify them by gesture type or direction. We evaluated the accuracy and temporal precision of our system for event detection and classification. Results from an in-lab user study of 12 participants showed an average of 97% touch detection accuracy and 98% grasp detection accuracy. In our study, we found that near-infrared and inertial sensing are complementary and can be used in tandem to effectively address both touch event detection and directionality classification.
SP  - 10842
EP  - 10849
JF  - IEEE Robotics and Automation Letters
VL  - 7
IS  - 4
PB  - 
DO  - 10.1109/lra.2022.3191173
ER  - 

TY  - JOUR
AU  - Li, Jianan; Yang, Jimei; Zhang, Jianming; Liu, Chang; Wang, Christina; Xu, Tingfa
TI  - Attribute-Conditioned Layout GAN for Automatic Graphic Design
PY  - 2021
AB  - Modeling layout is an important first step for graphic design. Recently, methods for generating graphic layouts have progressed, particularly with Generative Adversarial Networks (GANs). However, the problem of specifying the locations and sizes of design elements usually involves constraints with respect to element attributes, such as area, aspect ratio and reading-order. Automating attribute conditional graphic layouts remains a complex and unsolved problem. In this article, we introduce Attribute-conditioned Layout GAN to incorporate the attributes of design elements for graphic layout generation by forcing both the generator and the discriminator to meet attribute conditions. Due to the complexity of graphic designs, we further propose an element dropout method to make the discriminator look at partial lists of elements and learn their local patterns. In addition, we introduce various loss designs following different design principles for layout optimization. We demonstrate that the proposed method can synthesize graphic layouts conditioned on different element attributes. It can also adjust well-designed layouts to new sizes while retaining elements’ original reading-orders. The effectiveness of our method is validated through a user study.
SP  - 4039
EP  - 4048
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 10
PB  - 
DO  - 10.1109/tvcg.2020.2999335
ER  - 

TY  - NA
AU  - Li, Jianan; Yang, Jimei; Zhang, Jianming; Liu, Chang; Wang, Christina; Xu, Tingfa
TI  - Attribute-conditioned Layout GAN for Automatic Graphic Design
PY  - 2020
AB  - Modeling layout is an important first step for graphic design. Recently, methods for generating graphic layouts have progressed, particularly with Generative Adversarial Networks (GANs). However, the problem of specifying the locations and sizes of design elements usually involves constraints with respect to element attributes, such as area, aspect ratio and reading-order. Automating attribute conditional graphic layouts remains a complex and unsolved problem. In this paper, we introduce Attribute-conditioned Layout GAN to incorporate the attributes of design elements for graphic layout generation by forcing both the generator and the discriminator to meet attribute conditions. Due to the complexity of graphic designs, we further propose an element dropout method to make the discriminator look at partial lists of elements and learn their local patterns. In addition, we introduce various loss designs following different design principles for layout optimization. We demonstrate that the proposed method can synthesize graphic layouts conditioned on different element attributes. It can also adjust well-designed layouts to new sizes while retaining elements' original reading-orders. The effectiveness of our method is validated through a user study.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zheng, Xinru; Qiao, Xiaotian; Cao, Ying; Lau, Rynson W. H.
TI  - Content-aware generative modeling of graphic design layouts
PY  - 2019
AB  - Layout is fundamental to graphic designs. For visual attractiveness and efficient communication of messages and ideas, graphic design layouts often have great variation, driven by the contents to be presented. In this paper, we study the problem of content-aware graphic design layout generation. We propose a deep generative model for graphic design layouts that is able to synthesize layout designs based on the visual and textual semantics of user inputs. Unlike previous approaches that are oblivious to the input contents and rely on heuristic criteria, our model captures the effect of visual and textual contents on layouts, and implicitly learns complex layout structure variations from data without the use of any heuristic rules. To train our model, we build a large-scale magazine layout dataset with fine-grained layout annotations and keyword labeling. Experimental results show that our model can synthesize high-quality layouts based on the visual semantics of input images and keyword-based summary of input text. We also demonstrate that our model internally learns powerful features that capture the subtle interaction between contents and layouts, which are useful for layout-aware design retrieval.
SP  - 133
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 4
PB  - 
DO  - 10.1145/3306346.3322971
ER  - 

TY  - JOUR
AU  - Sikora, Mariusz; Paszkiel, Szczepan
TI  - Muscle activity measurement using visible light and infrared
PY  - 2019
AB  - Abstract This article describes possibilities of using optical sensor MAX30105 as a tool for muscle activity detection and measurement. On the beginning, there is a preview to another works, where muscle activity was measured for different purposes and with a number of methods. Own measurement with the usage of visible light as well as near infrared was focused on upper limbs, especially on forearm and shoulder muscles. These data are compared with signals registered with gyroscope and one channel electromyography (EMG). Some measurements were done on several volunteers in order to confirm the effectiveness of this method.
SP  - 329
EP  - 334
JF  - IFAC-PapersOnLine
VL  - 52
IS  - 27
PB  - 
DO  - 10.1016/j.ifacol.2019.12.682
ER  - 

TY  - NA
AU  - Alvina, Jessalyn; Qu, Chengcheng; McGrenere, Joanna; Mackay, Wendy E.
TI  - CHI Extended Abstracts - MojiBoard: Generating Parametric Emojis with Gesture Keyboards
PY  - 2019
AB  - Inserting emojis can be cumbersome when users must swap through panels. From our survey, we learned that users often use a series of consecutive emojis to convey rich, nuanced non-verbal expressions such as emphasis, change of expressions, or micro stories. We introduce MojiBoard, an emoji entry technique that enables users to generate dynamic parametric emojis from a gesture keyboard. With MojiBoard, users can switch seamlessly between typing and parameterizing emojis.
SP  - 1
EP  - 6
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3312771
ER  - 

TY  - NA
AU  - Beltran, Fernando; Geng, Jing
TI  - Building a Distributed XR Immersive Environment for data Visualization
PY  - 2021
AB  - We present a novel way to support business organizations’ decision-making in the context of analysis of network data sets. For such a purpose we built a mixed-reality, multi-user platform, which we named Aroaro and can be deployed on different virtual reality and augmented reality devices. In Aroaro, users can experience data visualization, either individually or as part of a team. With a set of lab tasks designed to lead lab subjects to analyze the network information while immersing into the rich virtual environment, our approach belongs to the nascent area of immersive analytics. We assume a business unit has already procured some network data set and been tasked with problems that need to reveal information about the network's connectivity properties. This paper is concerned with how a business unit achieves problem solving (decisionmaking) when relying on immersive analytics. Our approach combines a high-quality virtual environment such as Aroaro, its data visualization engine and the analytics afforded by concepts of network centrality in a network. We report on preliminary results of having lab subjects experience two visualization environments: a 3D visualization system on a 2D-flat screen and our mixed-reality, multi-user platform for network data visualization.
SP  - NA
EP  - NA
JF  - 2021 ITU Kaleidoscope: Connecting Physical and Virtual Worlds (ITU K)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/ituk53220.2021.9662103
ER  - 

TY  - NA
AU  - Olaosebikan, Monsurat; Aranda Barrios, Claudia; Cowen, Lenore; Shaer, Orit
TI  - Embodied Notes: A Cognitive Support Tool For Remote Scientific Collaboration in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519664
ER  - 

TY  - NA
AU  - Ivanov, Alexander; Danyluk, Kurtis Thorvald; Willett, Wesley
TI  - CHI Extended Abstracts - Exploration & Anthropomorphism in Immersive Unit Visualizations
PY  - 2018
AB  - We report on an initial examination of the potential of immersive unit visualizations in virtual reality, showing how these visualizations can help viewers examine data at multiple scales and support affective, personal experiences with data. We outline unique opportunities for unit visualizations in virtual reality, including support for (1) dynamic scale transitions, (2) immersive exploration, and (3) anthropomorphic interactions. We then demonstrate a prototype system and discuss the potential for virtual reality visualization to support personal interactions with data.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188544
ER  - 

TY  - CHAP
AU  - Mejjati, Youssef Alami; Gomez, Celso F.; Kim, Kwang In; Shechtman, Eli; Bylinskii, Zoya
TI  - ECCV (23) - Look here! A parametric learning based approach to redirect visual attention
PY  - 2020
AB  - Across photography, marketing, and website design, being able to direct the viewer’s attention is a powerful tool. Motivated by professional workflows, we introduce an automatic method to make an image region more attention-capturing via subtle image edits that maintain realism and fidelity to the original. From an input image and a user-provided mask, our GazeShiftNet model predicts a distinct set of global parametric transformations to be applied to the foreground and background image regions separately. We present the results of quantitative and qualitative experiments that demonstrate improvements over prior state-of-the-art. In contrast to existing attention shifting algorithms, our global parametric approach better preserves image semantics and avoids typical generative artifacts. Our edits enable inference at interactive rates on any image size, and easily generalize to videos. Extensions of our model allow for multi-style edits and the ability to both increase and attenuate attention in an image region. Furthermore, users can customize the edited images by dialing the edits up or down via interpolations in parameter space. This paper presents a practical tool that can simplify future image editing pipelines.
SP  - 343
EP  - 361
JF  - Computer Vision – ECCV 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58592-1_21
ER  - 

TY  - NA
AU  - Bolder, Anna; Grünvogel, Stefan M.; Angelescu, Emanuel
TI  - VRST - Comparison of the usability of a car infotainment system in a mixed reality environment and in a real car
PY  - 2018
AB  - Instead of installing new control modes for infotainments systems in a real vehicle for testing, it is an attractive idea (saving time and cost) to evaluate and develop these systems in a mixed reality (MR) environment. The central question of the study is whether the usability evaluation of a car entertainment system within a MR environment provides the same results as the evaluation of the car entertainment system within a real car. For this purpose a prototypical car infotainment system was built and integrated into a real car and into a MR environment. The MR environment represents the interior of the car and uses finger tracking and real haptic control elements of the center console of a car. Two test groups were assigned to the two different test environments. The study shows, that the usability is rated similar in both environments although readability and representation within the infotainment system is problematic.
SP  - 8
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281512
ER  - 

TY  - NA
AU  - Buddhika, Thisum; Zhang, Haimo; Chan, Samantha W. T.; Dissanayake, Vipula; Nanayakkara, Suranga; Zimmermann, Roger
TI  - AH - fSense: Unlocking the Dimension of Force for Gestural Interactions using Smartwatch PPG Sensor
PY  - 2019
AB  - While most existing gestural interfaces focus on the static posture or the dynamic action of the hand, few have investigated the feasibility of using the forces that are exerted while performing gestures. Using the photoplethysmogram (PPG) sensor of off-the-shelf smartwatches, we show that, it is possible to recognize the force of a gesture as an independent channel of input. Based on a user study with 12 participants, we found that users were able to reliably produce two levels of force across several types of common gestures. We demonstrate a few interaction scenarios where the force is either used as a standalone input or to complement existing input modalities.
SP  - 11
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311839
ER  - 

TY  - NA
AU  - Wang, Chiu-Hsuan; Yong, Seraphina; Chen, Hsin-Yu; Ye, Yuan-Syun; Chan, Liwei
TI  - UIST - HMD Light: Sharing In-VR Experience via Head-Mounted Projector for Asymmetric Interaction
PY  - 2020
AB  - We present HMD Light, a proof-of-concept Head-Mounted Display (HMD) implementation that reveals the Virtual Reality (VR) user's experience in the physical environment to facilitate communication between VR and external users in a mobile VR context. While previous work externalized the VR user's experience through an on-HMD display, HMD Light places the display into the physical environment to enable larger display and interaction area. This work explores the interaction design space of HMD Light and presents four applications to demonstrate its versatility. Our exploratory user study observed participant pairs experience applications with HMD Light and evaluated usability, accessibility and social presence between users. From the results, we distill design insights for HMD Light and asymmetric VR collaboration.
SP  - 472
EP  - 486
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415847
ER  - 

TY  - NA
AU  - Graham-Knight, John Brandon; Corbett, Jon; Lasserre, Patricia; Liang, Hai-Ning; Hasan, Khalad
TI  - OZCHI - Exploring Haptic Feedback for Common Message Notification Between Intimate Couples with Smartwatches
PY  - 2020
AB  - In this paper, we explore haptic feedback for smartwatches (i.e., vibrations) as a means to transmit the conversational meaning of short text messages through a non-visual mode of communication between intimate people. The current use of smartwatch vibrations is limited to basic patterns to convey simple information such as notifying users on an incoming phone call or a text message. We envision the use of vibrations to notify commonly exchanged messages between intimate ones by providing discreet feedback on their wrist. This form of communication preserves the flow of users’ primary activities without making them to look at the display, supporting unobtrusive interaction. We start our exploration by examining the common short text messages that intimate people, like couples, exchange in their daily life. We next investigate the vibration properties such as vibration duration and number of vibrations that are suitable to convey the meaning of these messages. We further examine users’ accuracy of detecting and extracting the meaning of messages from vibrations where our results report an accuracy of 95% while perceiving the correct meanings. We conclude with design recommendations for using such vibrational feedback for communicating information with intimate partners.
SP  - 245
EP  - 252
JF  - 32nd Australian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441000.3441012
ER  - 

TY  - JOUR
AU  - Zahrádka, Jaroslav; Machová, Veronika; Kučera, Jiří
TI  - WHAT IS THE PRICE OF OUTDOOR ADVERTISING: A CASE STUDY OF THE CZECH REPUBLIC?
PY  - 2021
AB  - <jats:p>The aim of the paper is to analyze the price of outdoor advertising in various regions of the Czech Republic. The base source of data is the website plakatov.cz, from which the prices of outdoor advertisement in individual regions were obtained. The results show that the prices of outdoor advertisement in the individual regions are the same. The main difference is in how many outdoor advertisements are located in individual regions for the lowest and how many for the highest price. An overview was created, which shows what is the price of outdoor advertisement in each region, how many ads are located in each region and subsequently, the data is displayed using a map. The results show that the cheapest outdoor advertisement is located in the Hradec Králové region. The price of the outdoor advertisement in the Hradec Králové Region is CZK 5,204. On the contrary, the most expensive outdoor advertisement is in Prague. The price of outdoor advertising in Prague is CZK 16,567. Most outdoor advertisements are located in Prague. There are 174 outdoor advertisements in Prague. The lowest number of outdoor advertisements is in the Pardubice and Zlín regions. There are 18 outdoor advertisements in both regions. The difference between outdoor advertisements in the Pardubice and Zlín regions is in their price. While in the Zlín Region outdoor advertising costs CZK 6,466, in the Pardubice Region it costs CZK 12,333. The results are beneficial for people who are interested in outdoor advertising. They are mostly beneficial for outdoor advertising producers to know their standing compared to their competition and other regions.</jats:p>
SP  - 386
EP  - 391
JF  - AD ALTA: 11/01
VL  - 11
IS  - 1
PB  - 
DO  - 10.33543/1101386391
ER  - 

TY  - NA
AU  - Maereg, Andualem Tadesse; Lou, Yang; Secco, Emanuele Lindo; King, Raymond
TI  - VISIGRAPP (2: HUCAPP) - Hand Gesture Recognition based on Near-infrared Sensing Wristband.
PY  - 2020
AB  - Wrist-worn gesture sensing systems can be used as a seamless interface for AR/VR interactions and control of various devices. In this paper, we present a low-cost gesture sensing system that utilizes near Infrared Emitters (600 - 1100 nm) and Photo-Receivers encompassing the wrist to infer hand gestures. The proposed system consists of a wristband comprising Infrared emitters and receivers, data acquisition hardware, data post-processing software, and gesture classification algorithms. During the data acquisition process, 24 near Infrared Emitters are sequentially switched on around the wrist, and twelve Photo-diodes measure the light reflected, refracted, and scattered by the tissues inside the wrist. The acquired data corresponding to different gestures are labeled and input into a machine learning algorithm for gesture classification. To demonstrated the accuracy and speed of the proposed system, real-time gesture sensing user studies were conducted. As a result of this comparison, we obtained an average accuracy of 98.06% with standard deviation of 1.82%. In addition, we evaluated that the system can perform six-eight gestures per second in real time using a desktop computer operating with Core i7-7800X CPU at 3.5GHz and 32 GB RAM.
SP  - 110
EP  - 117
JF  - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.5220/0008909401100117
ER  - 

TY  - JOUR
AU  - Li, Tiemeng; Jin, Yanning; Wu, Songqian; Liu, Shiran
TI  - Multivariate Fence: Using Parallel Coordinates to Locate and Compare Attributes of Adjacency Matrix Nodes in Immersive Environment
PY  - 2022
AB  - <jats:p>Adjacency matrix visualization is a common method for presenting graph data, and the Focus+Context technique can be used to explore the details of the ROI (region of interest). Embedded views and multi-view approaches are usually applied when locating and comparing attributes among multiple nodes. However, the embedded view has an issue of edge occlusion, while the multi-view would cause repeated perspective switching. In this paper, we propose a Multivariate Fence (MVF) model as a focus view of the adjacency matrix to locate and compare attributes among nodes. An additional spatial parallel coordinate is added to the 2D adjacency matrix in an immersive environment so that the attribute information can be shown in a single view without blocking edge information. We also conduct a user study to evaluate the performance of the MVF. The results show that the MVF has better efficiency and accuracy in locating and comparing the multivariate adjacency matrix in the immersive environment against the existing focus model. Moreover, the MVF model is easier to understand and is preferred by users.</jats:p>
SP  - 12182
EP  - 12182
JF  - Applied Sciences
VL  - 12
IS  - 23
PB  - 
DO  - 10.3390/app122312182
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Luo, Danli; Fang, Ziqi; Fan, Yitao; Yu, Zhi; Chen, Yu; Pan, Deying; Yang, Yue; Zhao, Yijun; Gu, Jianzhe; Yao, Lining; Tao, Ye; Wang, Guanyun
TI  - CHI Extended Abstracts - Fashion Design with FlexTruss Approach
PY  - 2021
AB  - This demo presents FlexTruss, a design and construction pipeline based on the assembly of modularized truss-like objects fabricated with conventional 3D printers and assembled by threading. To create an end-to-end system, a parametric design tool with an optimal Euler path calculation method is developed, which can support both inverse and forward design workflow and multi-material construction of modular parts. In addition, the workflow guides the assembly of printed truss modules by threading. Finally, a series of application cases to demonstrate the affordance of FlexTruss are presented. We believe that FlexTruss extends the design space of 3D printing beyond typically hard and fixed forms, and it will provide new capabilities for designers and researchers to explore the use of such flexible truss structures in human-object interaction.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451572
ER  - 

TY  - NA
AU  - Cunningham, Andrew; Hart, Jonathon D.; Engelke, Ulrich; Adcock, Matt; Thomas, Bruce H.
TI  - e-Energy - Towards Embodied Interaction for Geospatial Energy Sector Analytics in Immersive Environments
PY  - 2021
AB  - We present a tool for immersive analysis of spatial energy data. The tool is built on ImAxes, which was designed for immersive analytics of abstract data in virtual environments, and extends it with a geospatial layer that allows for iterative and collaborative sensemaking. We developed new interaction techniques that enable the user to freely choose data variables, combine them into charts, and use filters and data lenses to create interactive visualisations grounded in a map. Fusion between various separate data sources can take place directly in the hands of the humans who need to understand and explore it. In this paper, we describe the system development and present a use case scenario with data from Australia's National Energy Analytics Research Program.
SP  - 396
EP  - 400
JF  - Proceedings of the Twelfth ACM International Conference on Future Energy Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447555.3466602
ER  - 

TY  - NA
AU  - Leiva, Luis A.; Kim, Sunjun; Cui, Wenzhe; Bi, Xiaojun; Oulasvirta, Antti
TI  - MobileHCI - How We Swipe: A Large-scale Shape-writing Dataset and Empirical Findings
PY  - 2021
AB  - Despite the prevalence of shape-writing (gesture typing, swype input, or swiping for short) as a text entry method, there are currently no public datasets available. We report a large-scale dataset that can support efforts in both empirical study of swiping as well as the development of better intelligent text entry techniques. The dataset was collected via a web-based custom virtual keyboard, involving 1,338 users who submitted 11,318 unique English words. We report aggregate-level indices on typing performance, user-related factors, as well as trajectory-level data, such as the gesture path drawn on top of the keyboard or the time lapsed between consecutively swiped keys. We find some well-known effects reported in previous studies, for example that speed and error are affected by age and language skill. We also find surprising relationships such that, on large screens, swipe trajectories are longer but people swipe faster.
SP  - 11
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472059
ER  - 

TY  - NA
AU  - Wu, Erwin; Yuan, Ye; Yeo, Hui-Shyong; Quigley, Aaron; Koike, Hideki; Kitani, Kris M.
TI  - UIST - Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-worn Camera via Dorsum Deformation Network
PY  - 2020
AB  - The automatic recognition of how people use their hands and fingers in natural settings -- without instrumenting the fingers -- can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fingers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress finger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the first vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81 degree for user-specific models and 9.77 degree for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer.
SP  - 1147
EP  - 1160
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415897
ER  - 

TY  - NA
AU  - Newman, Anelise; McNamara, Barry A.; Fosco, Camilo; Bin Zhang, Yun; Sukhum, Pat; Tancik, Matthew; Kim, Nam Wook; Bylinskii, Zoya
TI  - TurkEyes: A Web-Based Toolbox for Crowdsourcing Attention Data
PY  - 2020
AB  - Eye movements provide insight into what parts of an image a viewer finds most salient, interesting, or relevant to the task at hand. Unfortunately, eye tracking data, a commonly-used proxy for attention, is cumbersome to collect. Here we explore an alternative: a comprehensive web-based toolbox for crowdsourcing visual attention. We draw from four main classes of attention-capturing methodologies in the literature. ZoomMaps is a novel "zoom-based" interface that captures viewing on a mobile phone. CodeCharts is a "self-reporting" methodology that records points of interest at precise viewing durations. ImportAnnots is an "annotation" tool for selecting important image regions, and "cursor-based" BubbleView lets viewers click to deblur a small area. We compare these methodologies using a common analysis framework in order to develop appropriate use cases for each interface. This toolbox and our analyses provide a blueprint for how to gather attention data at scale without an eye tracker.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Xu, Kai; Ottley, Alvitta; Walchshofer, Conny; Streit, Marc; Chang, Remco; Wenskovitch, John
TI  - Survey on the Analysis of User Interactions and Visualization Provenance
PY  - 2020
AB  - There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.
SP  - 757
EP  - 783
JF  - Computer Graphics Forum
VL  - 39
IS  - 3
PB  - 
DO  - 10.1111/cgf.14035
ER  - 

TY  - JOUR
AU  - Kamarushi, Manisha Varma; Watson, Stacey L.; Tigwell, Garreth W.; Peiris, Roshan L.
TI  - OneButtonPIN: A Single Button Authentication Method for Blind or Low Vision Users to Improve Accessibility and Prevent Eavesdropping
PY  - 2022
AB  - <jats:p>A Personal Identification Number (PIN) is a widely adopted authentication method used by smartphones, ATMs, etc. PINs offer strong security and can be reset when compromised (unlike biometric authentication). However, PINs can be inaccessible for blind or low vision (BLV) users due to screen readers voicing PINs to bystanders or potential shoulder surfing attack risks---bystanders could watch the PIN being entered without the user noticing. To address this, we present OneButtonPIN, an interface to improve PIN entry accessibility and security for BLV users. Here, a single on-screen button, when pressed and held, triggers a haptic vibration sequence. A digit is entered by counting the vibrations and releasing the button. We explored introducing random timings to the vibration sequence to increase security. A week-long evaluation with 9 BLV participants and a security study with 10 sighted participants acting as shoulder surfers demonstrated OneButtonPIN's usability and resilience against eavesdropping.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546747
ER  - 

TY  - CHAP
AU  - Cantu, Alma; Duval, Thierry; Grisvard, Olivier; Coppin, Gilles
TI  - EuroVR - Expert evaluation of the usability of HeloVis: a 3D Immersive Helical Visualization for SIGINT Analysis
PY  - 2019
AB  - This paper presents an evaluation of HeloVis: a 3D interactive visualization that relies on immersive properties to improve user performance during SIGnal INTelligence (SIGINT) analysis. HeloVis draws on perceptive biases, highlighted by Gestalt laws, and on depth perception to enhance the recurrence properties contained in the data. In this paper, we briefly recall what is SIGINT, the challenges that it brings to visual analytics, and the limitations of state of the art SIGINT tools. Then, we present HeloVis, and we evaluate its efficiency through the results of an evaluation that we have made with civil and military operators who are the expert end-users of SIGINT analysis.
SP  - 181
EP  - 198
JF  - Virtual Reality and Augmented Reality
VL  - 11883
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-31908-3_12
ER  - 

TY  - NA
AU  - Yu, Jin; Sakhardande, Prabodh; Parmar, Ruchita; Oh, HyunJoo
TI  - Strawctures: A Modular Electronic Construction Kit for Human-Scale Interactive Structures
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501322
ER  - 

TY  - JOUR
AU  - Huang, Dongjin; Li, Jinyao; Liu, Chuanman; Liu, Jinhua
TI  - AUPOD: End-to-End Automatic Poster Design by Self-Supervision
PY  - 2022
AB  - The automatic design has become a popular topic in the application field of computer vision technologies. Previous methods for automatic design are mostly saliency-based, relying on an off-the-shelf model for saliency map detection and hand-crafted aesthetic rules for ranking on multiple proposals. We argue that the multi-stage generation and the excessive reliance on saliency map hindered the progress of pursuing better automatic design solutions. In this work, we explore the possibility of a saliency-free solution in a representative scenario, automatic poster design. We propose a novel end-to-end framework to solve the automatic poster design problem, which is divided into the layout prediction and attributes identification sub-tasks. We design a neural network based on multi-modality feature extraction to learn the two sub-tasks jointly. We train the deep neural network in our framework with automatically extracted supervision from semi-structured posters, bypassing a large amount of required manual labor. Both qualitative and quantitative results show the impressive performance of our end-to-end approach after discarding the explicit saliency detection module. Our system learned on self-supervision performs well on the automatic design by learning aesthetic constraints implicitly in the neural networks.
SP  - 47348
EP  - 47360
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3171033
ER  - 

TY  - NA
AU  - Pavel, Amy; Hartmann, Björn; Agrawala, Maneesh
TI  - UIST - Shot Orientation Controls for Interactive Cinematography with 360 Video
PY  - 2017
AB  - Virtual reality filmmakers creating 360-degree video currently rely on cinematography techniques that were developed for traditional narrow field of view film. They typically edit together a sequence of shots so that they appear at a fixed-orientation irrespective of the viewer's field of view. But because viewers set their own camera orientation they may miss important story content while looking in the wrong direction. We present new interactive shot orientation techniques that are designed to help viewers see all of the important content in 360-degree video stories. Our viewpoint-oriented technique reorients the shot at each cut so that the most important content lies in the the viewer's current field of view. Our active reorientation technique, lets the viewer press a button to immediately reorient the shot so that important content lies in their field of view. We present a 360-degree video player which implements these techniques and conduct a user study which finds that users spend 5.2-9.5% more time viewing the important points (manually labelled) of the scene with our techniques compared to the traditional fixed-orientation cuts. In practice, 360-degree video creators may label important content, but we also provide an automatic method for determining important content in existing 360-degree videos.
SP  - 289
EP  - 297
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126636
ER  - 

TY  - CHAP
AU  - Lee, Hsin-Ying; Jiang, Lu; Essa, Irfan; Le, Phuong B.; Gong, Haifeng; Yang, Ming-Hsuan; Yang, Weilong
TI  - ECCV (3) - Neural Design Network: Graphic Layout Generation with Constraints
PY  - 2020
AB  - Graphic design is essential for visual communication with layouts being fundamental to composing attractive designs. Layout generation differs from pixel-level image synthesis and is unique in terms of the requirement of mutual relations among the desired components. We propose a method for design layout generation that can satisfy user-specified constraints. The proposed neural design network (NDN) consists of three modules. The first module predicts a graph with complete relations from a graph with user-specified relations. The second module generates a layout from the predicted graph. Finally, the third module fine-tunes the predicted layout. Quantitative and qualitative experiments demonstrate that the generated layouts are visually similar to real design layouts. We also construct real designs based on predicted layouts for a better understanding of the visual quality. Finally, we demonstrate a practical application on layout recommendation.
SP  - 491
EP  - 506
JF  - Computer Vision – ECCV 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58580-8_29
ER  - 

TY  - NA
AU  - Butscher, Simon; Hubenschmid, Sebastian; Müller, Jens; Fuchs, Johannes; Reiterer, Harald
TI  - CHI - Clusters, Trends, and Outliers: How Immersive Technologies Can Facilitate the Collaborative Analysis of Multidimensional Data
PY  - 2018
AB  - Immersive technologies such as augmented reality devices are opening up a new design space for the visual analysis of data. This paper studies the potential of an augmented reality environment for the purpose of collaborative analysis of multidimensional, abstract data. We present ART, a collaborative analysis tool to visualize multidimensional data in augmented reality using an interactive, 3D parallel coordinates visualization. The visualization is anchored to a touch-sensitive tabletop, benefiting from well-established interaction techniques. The results of group-based, expert walkthroughs show that ART can facilitate immersion in the data, a fluid analysis process, and collaboration. Based on the results, we provide a set of guidelines and discuss future research areas to foster the development of immersive technologies as tools for the collaborative analysis of multidimensional data.
SP  - 90
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173664
ER  - 

TY  - NA
AU  - Nguyen, Viet; Rupavatharam, Siddharth; Liu, Luyang; Howard, Richard; Gruteser, Marco
TI  - SenSys - HandSense: capacitive coupling-based dynamic, micro finger gesture recognition
PY  - 2019
AB  - Head-mounted devices (HMD) for Augmented Reality (AR) are gaining traction thanks to a growing number of applications in the areas of image guided therapy, computer aided design, cargo packing, manufacturing and digital field service. However, providing an always available, intuitive and user friendly input for these devices remains a challenging problem. This paper explores recognizing dynamic, micro finger gestures using capacitive coupling for interacting with a head-mounted device. Electrodes are attached to fingertips of users gloves and capacitive coupling among all pairs of electrodes is measured quickly to infer the real-time spatial relationship between fingers. The system is able to recognize fine, low-effort finger gestures, such as swiping, sliding, tap, double-tap. We evaluated our prototype with 14 gestures executed by 10 subjects and found a 97% accuracy of gesture recognition.
SP  - 285
EP  - 297
JF  - Proceedings of the 17th Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3356250.3360040
ER  - 

TY  - BOOK
AU  - Barrera-Leon, Luisa; Corno, Fulvio; De Russis, Luigi
TI  - IV - Systematic Variation of Preattentive Attributes to Highlight Relevant Data in Information Visualization
PY  - 2020
AB  - In information visualization (InfoVis), the Visualizers (graph designers and creators) have to consider multiple parameters, such as colors and graphic symbols, to obtain a chart that correctly represents a data set. Along with this, visualizers must adequately select the combination of these range of parameters to drive the observers’ attention to the relevant data. When a visualizer drives the attention to relevant aspects of the information, she is providing a starting point to read the graph; this focus point might help the observer to complete the task faster and more efficiently, minimizing distraction from unimportant information. Contemporary tools for InfoVis help visualizers to a certain extent, but most of them currently do not provide insights or suggestions about the modifications needed to drive data attention. This article presents the preliminary results of an exploratory approach to draw the attention to some specific data subset selected by the graph creator, through a systematic variation of some preattentive attributes (i.e., color, texture and orientation). As a first simple method to validate the feasibility of the approach, a set of charts is created from the same source data, with exhaustive variations on preattentive attributes. All generated charts are then automatically evaluated using a salience map algorithm for data analysis images, to identify their focus attention point. After that, the algorithm chooses the chart that best emphasizes the data subset initially specified by the visualizer. To validate our approach, we have implemented a prototype tool, and preliminary results confirm that it is possible to systematically change the attention area of a chart.
SP  - 74
EP  - 79
JF  - 2020 24th International Conference Information Visualisation (IV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iv51561.2020.00022
ER  - 

TY  - NA
AU  - Engelke, Ulrich; Cordeil, Maxime; Cunningham, Andrew; Ens, Barrett
TI  - Immersive analytics
PY  - 2019
AB  - Welcome and OverviewVisualisation and Visual AnalyticsIntroduction to Immersive AnalyticsComputing Beyond the DesktopCollaboration
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2019 Courses
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3355047.3359391
ER  - 

TY  - NA
AU  - Aberman, Kfir; He, Junfeng; Gandelsman, Yossi; Mosseri, Inbar; Jacobs, David E.; Kohlhoff, Kai; Pritch, Yael; Rubinstein, Michael
TI  - Deep Saliency Prior for Reducing Visual Distraction.
PY  - 2021
AB  - Using only a model that was trained to predict where people look at images, and no additional training data, we can produce a range of powerful editing effects for reducing distraction in images. Given an image and a mask specifying the region to edit, we backpropagate through a state-of-the-art saliency model to parameterize a differentiable editing operator, such that the saliency within the masked region is reduced. We demonstrate several operators, including: a recoloring operator, which learns to apply a color transform that camouflages and blends distractors into their surroundings; a warping operator, which warps less salient image regions to cover distractors, gradually collapsing objects into themselves and effectively removing them (an effect akin to inpainting); a GAN operator, which uses a semantic prior to fully replace image regions with plausible, less salient alternatives. The resulting effects are consistent with cognitive research on the human visual system (e.g., since color mismatch is salient, the recoloring operator learns to harmonize objects' colors with their surrounding to reduce their saliency), and, importantly, are all achieved solely through the guidance of the pretrained saliency model, with no additional supervision. We present results on a variety of natural images and conduct a perceptual study to evaluate and validate the changes in viewers' eye-gaze between the original images and our edited results.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Tadeja, Slawomir Konrad; Kipouros, Timoleon; Kristensson, Per Ola
TI  - IPCP: Immersive Parallel Coordinates Plots for Engineering Design Processes
PY  - 2020
AB  - Cambridge European and Trinity Hall; Engineering and Physical Sciences Research Council (EPSRC-1788814)
SP  - 324
EP  - NA
JF  - AIAA Scitech 2020 Forum
VL  - NA
IS  - NA
PB  - 
DO  - 10.2514/6.2020-0324
ER  - 

TY  - JOUR
AU  - Stemasov, Evgeny; Rukzio, Enrico; Gugenheimer, Jan
TI  - The Road to Ubiquitous Personal Fabrication: Modeling-Free Instead of Increasingly Simple
PY  - 2021
AB  - The tools for personal digital fabrication (DF) are on the verge of reaching mass-adoption beyond technology enthusiasts, empowering consumers to fabricate personalized artifacts. We argue that to achieve similar outreach and impact as personal computing, personal fabrication research may have to venture beyond ever-simpler interfaces for creation, toward lowest-effort workflows for remixing. We surveyed novice-friendly DF workflows from the perspective of HCI. Through this survey, we found two distinct approaches for this challenge: 1) simplifying expert modeling tools (AutoCAD →Tinkercad) and 2) enriching tools not involving primitive-based modeling with powerful customization (e.g., Thingiverse). Drawing parallel to content creation domains such as photography, we argue that the bulk of content is created via remixing (2). In this article, we argue that to be able to include the majority of the population in DF, research should embrace omission of workflow steps, shifting toward automation, remixing, and templates, instead of modeling from the ground up.
SP  - 19
EP  - 27
JF  - IEEE Pervasive Computing
VL  - 20
IS  - 1
PB  - 
DO  - 10.1109/mprv.2020.3029650
ER  - 

TY  - NA
AU  - Satriadi, Kadek Ananta; Cunningham, Andrew; Thomas, Bruce H.; Drogemuller, Adam; Odi, Antoine; Patel, Niki; Aston, Cathlyn; Smith, Ross T.
TI  - Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00019
ER  - 

TY  - JOUR
AU  - Devyatkov, V.V.; Tipsin, E.A.
TI  - Assessment of User Interface Usability in Terms of Tile Logic
PY  - 2020
AB  - <jats:p>The paper presents methodological principles for a formal automated assessment of user interface usability. We chose tile logic as the mathematical tool for solving the problems pertaining to this formal check. The paper reviews previous publications in the field under consideration, explains the reasons behind selecting the method of tile logic, and considers the principles governing verification (proof) of user interface usability. We highlight the differences between the procedure developed and those known previously, and its advantages. We use a simple example to show the principles of formal automated assessment of user interface usability in terms of tile logic. We describe the development of the procedure proposed and the prospects of employing it</jats:p>
SP  - 64
EP  - 84
JF  - Herald of the Bauman Moscow State Technical University. Series Instrument Engineering
VL  - NA
IS  - 1 (130)
PB  - 
DO  - 10.18698/0236-3933-2020-1-64-84
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Ting, Chao-Hsien; Hung, Chia-Sheng; Lee, Wan-Chen; Chen, Liang-Jin; Chao, Yu-Tzu; Chen, Bing-Yu; Guo, Anhong
TI  - OmniScribe: Authoring Immersive Audio Descriptions for 360° Videos
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545613
ER  - 

TY  - BOOK
AU  - Marriott, Kim; Chen, Jian; Hlawatsch, Marcel; Itoh, Takayuki; Nacenta, Miguel A.; Reina, Guido; Stuerzlinger, Wolfgang
TI  - Immersive Analytics - Just 5 Questions: Toward a Design Framework for Immersive Analytics
PY  - 2018
AB  - We present an initial design framework for immersive analytics based on Brehmer and Munzner’s “What-Why-How” data visualisation framework. We extend their framework to take into account Who are the people or teams of people who are going to use the system, and Where is the system to be used and what are the available devices and technology. In addition, the How component is extended to cater for collaboration, multisensory presentation, interaction with an underlying computational model, degree of fidelity and organisation of the workspace around the user. By doing so we provide a framework for understanding immersive analytics research and applications as well as clarifying how immersive analytics differs from traditional data visualisation and visual analytics.
SP  - 259
EP  - 288
JF  - Immersive Analytics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01388-2_9
ER  - 

TY  - JOUR
AU  - Jansen, Pascal; Colley, Mark; Rukzio, Enrico
TI  - A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review
PY  - 2022
AB  - <jats:p>Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.</jats:p>
SP  - 1
EP  - 51
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534617
ER  - 

TY  - CONF
AU  - Hubenschmid, Sebastian; Zagermann, Johannes; Butscher, Simon; Reiterer, Harald
TI  - Employing Tangible Visualisations in Augmented Reality with Mobile Devices
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mazumdar, Amrita; Haynes, Brandon; Balazinska, Magdalena; Ceze, Luis; Cheung, Alvin; Oskin, Mark
TI  - Vignette: Perceptual Compression for Video Storage and Processing Systems
PY  - 2019
AB  - Compressed videos constitute 70% of Internet traffic, and video upload growth rates far outpace compute and storage improvement trends. Past work in leveraging perceptual cues like saliency, i.e., regions where viewers focus their perceptual attention, reduces compressed video size while maintaining perceptual quality, but requires significant changes to video codecs and ignores the data management of this perceptual information. In this paper, we propose Vignette, a compression technique and storage manager for perception-based video compression. Vignette complements off-the-shelf compression software and hardware codec implementations. Vignette's compression technique uses a neural network to predict saliency information used during transcoding, and its storage manager integrates perceptual information into the video storage system to support a perceptual compression feedback loop. Vignette's saliency-based optimizations reduce storage by up to 95% with minimal quality loss, and Vignette videos lead to power savings of 50% on mobile phones during video playback. Our results demonstrate the benefit of embedding information about the human visual system into the architecture of video storage systems.
SP  - NA
EP  - NA
JF  - arXiv: Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Kraus, Matthias; Fuchs, Johannes; Sommer, Björn; Klein, Karsten; Engelke, Ulrich; Keim, Daniel; Schreiber, Falk
TI  - Immersive Analytics with Abstract 3D Visualizations: A Survey
PY  - 2021
AB  - After a long period of scepticism, more and more publications describe basic research but also practical approaches to how abstract data can be presented in immersive environments for effective and efficient data understanding. Central aspects of this important research question in immersive analytics research are concerned with the use of 3D for visualization, the embedding in the immersive space, the combination with spatial data, suitable interaction paradigms and the evaluation of use cases. We provide a characterization that facilitates the comparison and categorization of published works and present a survey of publications that gives an overview of the state of the art, current trends, and gaps and challenges in current research.
SP  - 201
EP  - 229
JF  - Computer Graphics Forum
VL  - 41
IS  - 1
PB  - 
DO  - 10.1111/cgf.14430
ER  - 

TY  - NA
AU  - Wang, Qianwen; Chen, Zhutian; Wang, Yong; Qu, Huamin
TI  - A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualization.
PY  - 2021
AB  - Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions:what visualization processes can be assisted by MLandhow ML techniques can be used to solve visualization problemsThis survey reveals seven main processes where the employment of ML techniques can benefit visualizations:Data Processing4VIS, Data-VIS Mapping, InsightCommunication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations.Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io.
SP  - 1
EP  - 1
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - NA
IS  - 01
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Cui, Wenzhe; Xu, Zheer; Zhu, Suwen; Yang, Xing-Dong; Bi, Xiaojun; Li, Zhi; Ramakrishnan, I V
TI  - BackSwipe: Back-of-device Word-Gesture Interaction on Smartphones.
PY  - 2021
AB  - Back-of-device interaction is a promising approach to interacting on smartphones. In this paper, we create a back-of-device command and text input technique called BackSwipe, which allows a user to hold a smartphone with one hand, and use the index finger of the same hand to draw a word-gesture anywhere at the back of the smartphone to enter commands and text. To support BackSwipe, we propose a back-of-device word-gesture decoding algorithm which infers the keyboard location from back-of-device gestures, and adjusts the keyboard size to suit the gesture scales; the inferred keyboard is then fed back into the system for decoding. Our user study shows BackSwipe is feasible and a promising input method, especially for command input in the one-hand holding posture: users can enter commands at an average accuracy of 92% with a speed of 5.32 seconds/command. The text entry performance varies across users. The average speed is 9.58 WPM with some users at 18.83 WPM; the average word error rate is 11.04% with some users at 2.85%. Overall, BackSwipe complements the extant smartphone interaction by leveraging the back of the device as a gestural input surface.
SP  - NA
EP  - NA
JF  - Proceedings of the SIGCHI conference on human factors in computing systems. CHI Conference
VL  - 2021
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445081
ER  - 

TY  - NA
AU  - Aberman, Kfir; He, Junfeng; Gandelsman, Yossi; Mosseri, Inbar; Jacobs, David E.; Kohlhoff, Kai; Pritch, Yael; Rubinstein, Michael
TI  - Deep Saliency Prior for Reducing Visual Distraction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr52688.2022.01923
ER  - 

TY  - CONF
AU  - Fennedy, Katherine; Malacria, Sylvain; Lee, Hyowon; Perrault, Simon T.
TI  - MobileHCI - Investigating Performance and Usage of Input Methods for Soft Keyboard Hotkeys
PY  - 2020
AB  - Touch-based devices, despite their mainstream availability, do not support a unified and efficient command selection mechanism, available on every platform and application. We advocate that hotkeys, conventionally used as a shortcut mechanism on desktop computers, could be generalized as a command selection mechanism for touch-based devices, even for keyboard-less applications. In this paper, we investigate the performance and usage of soft keyboard shortcuts or hotkeys (abbreviated SoftCuts) through two studies comparing different input methods across sitting, standing and walking conditions. Our results suggest that SoftCuts not only are appreciated by participants but also support rapid command selection with different devices and hand configurations. We also did not find evidence that walking deters their performance when using the Once input method.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cheng, Hsien-Tzu; Chao, Chun-Hung; Dong, Jin-Dong; Wen, Hao-Kai; Liu, Tyng-Luh; Sun, Min
TI  - Cube Padding for Weakly-Supervised Saliency Prediction in 360{\deg} Videos
PY  - 2018
AB  - Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Huang, Xincheng; Miller, Keylonnie L.; Sample, Alanson P.; Banovic, Nikola
TI  - StructureSense
PY  - 2022
AB  - <jats:p>Recent advancements in object-tracking technologies can turn mundane constructive assemblies into Tangible User Interfaces (TUI) media. Users rely on instructions or their own creativity to build both permanent and temporary structures out of such objects. However, most existing object-tracking technologies focus on tracking structures as monoliths, making it impossible to infer and track the user's assembly process and the resulting structures. Technologies that can track the assembly process often rely on specially fabricated assemblies, limiting the types of objects and structures they can track. Here, we present StructureSense, a tracking system based on passive UHF-RFID sensing that infers constructive assembly structures from object motion. We illustrated StructureSense in two use cases (as guided instructions and authoring tool) on two different constructive sets (wooden lamp and Jumbo Blocks), and evaluated system performance and usability. Our results showed the feasibility of using StructureSense to track mundane constructive assembly structures.</jats:p>
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3570343
ER  - 

TY  - JOUR
AU  - Shahmoradi, Javad; Talebi, Elaheh; Roghanchi, Pedram; Hassanalian, Mostafa
TI  - A Comprehensive Review of Applications of Drone Technology in the Mining Industry
PY  - 2020
AB  - This paper aims to provide a comprehensive review of the current state of drone technology and its applications in the mining industry. The mining industry has shown increased interest in the use of drones for routine operations. These applications include 3D mapping of the mine environment, ore control, rock discontinuities mapping, postblast rock fragmentation measurements, and tailing stability monitoring, to name a few. The article offers a review of drone types, specifications, and applications of commercially available drones for mining applications. Finally, the research needs for the design and implementation of drones for underground mining applications are discussed.
SP  - 34
EP  - NA
JF  - Drones
VL  - 4
IS  - 3
PB  - 
DO  - 10.3390/drones4030034
ER  - 

TY  - NA
AU  - Kroma, Assem
TI  - The Technical Dilemmas of Creative Design and Rapid Prototyping for Immersive Storytelling
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3533730
ER  - 

TY  - NA
AU  - Lee, Chi-Jung; Tsai, Hsin-Ruey; Chen, Bing-Yu
TI  - CHI - HairTouch: Providing Stiffness, Roughness and Surface Height Differences Using Reconfigurable Brush Hairs on a VR Controller
PY  - 2021
AB  - Tactile feedback is widely used to enhance realism in virtual reality (VR). When touching virtual objects, stiffness and roughness are common and obvious factors perceived by the users. Furthermore, when touching a surface with complicated surface structure, differences from not only stiffness and roughness but also surface height are crucial. To integrate these factors, we propose a pin-based handheld device, HairTouch, to provide stiffness differences, roughness differences, surface height differences and their combinations. HairTouch consists of two pins for the two finger segments close to the index fingertip, respectively. By controlling brush hairs’ length and bending direction to change the hairs’ elasticity and hair tip direction, each pin renders various stiffness and roughness, respectively. By further independently controlling the hairs’ configuration and pins’ height, versatile stiffness, roughness and surface height differences are achieved. We conducted a perception study to realize users’ distinguishability of stiffness and roughness on each of the segments. Based on the results, we performed a VR experience study to verify that the tactile feedback from HairTouch enhances VR realism.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445285
ER  - 

TY  - NA
AU  - Martin, Daniel; Malpica, Sandra; Gutierrez, Diego; Masia, Belen; Serrano, Ana
TI  - Multimodality in VR: A Survey
PY  - 2021
AB  - Virtual reality (VR) is rapidly growing, with the potential to change the way we create and consume content. In VR, users integrate multimodal sensory information they receive, to create a unified perception of the virtual world. In this survey, we review the body of work addressing multimodality in VR, and its role and benefits in user experience, together with different applications that leverage multimodality in many disciplines. These works thus encompass several fields of research, and demonstrate that multimodality plays a fundamental role in VR; enhancing the experience, improving overall performance, and yielding unprecedented abilities in skill and knowledge transfer.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Buchem, Ilona; Klamma, Ralf; Wild, Fridolin
TI  - Introduction to Wearable Enhanced Learning (WELL): Trends, Opportunities, and Challenges
PY  - 2019
AB  - Wearable enhanced learning (WELL) is an emerging area of interest for researchers, practitioners in educational institutions, and companies. Also many grassroots movements are providing new sensors, devices, prototypical concepts, and learning solutions for WELL. Deeply rooted in the traditions of technology enhanced learning (TEL), such as self-regulated learning and mobile learning, WELL has been generating new challenges and opportunities in the field. Fragmentation, scalability, and data aggregation and resulting pedagogical approaches are among the key challenges and opportunities. The authors of this chapter explore drivers and affordances of wearable enhanced learning, outline the development of WELL as part of the evolution of technology enhanced learning, describe the key stakeholders in WELL (business, vocational training, higher education, and maker communities), and inspect some of the key domains in WELL, such as gaming and entertainment, health and sports, business and industries, and some technology trends, such as e-textiles, smart accessories, and head-mounted displays. This chapter broadens current perspectives on learning with wearables and learning about wearables and integrates insights from related fields including philosophy of technology, sociology, and design.
SP  - 3
EP  - 32
JF  - Perspectives on Wearable Enhanced Learning (WELL)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-64301-4_1
ER  - 

TY  - NA
AU  - Li, Yi-Jun; Shi, Jinchuan; Zhang, Fang-Lue; Wang, Miao
TI  - Bullet Comments for 360°Video
PY  - 2022
AB  - Time-anchored on-screen comments, as known as bullet comments, are a popular feature for online video streaming. Bullet comments reflect audiences&#x2019; feelings and opinions at specific video timings, which have been shown to be beneficial to video content understanding and social connection level. In this paper, we for the first time investigate the problem of bullet comment display and insertion for 360&#x00B0; video via head-mounted display and controller. We design four bullet comment display methods and evaluate their effects on 360&#x00B0; video experiences. We further propose two controller-based methods for bullet comment insertion. Combining the display and insertion methods, the user can experience 360&#x00B0; videos with bullet comments, and interactively post new ones by selecting among existing comments. User study results revealed how the factors of display and insertion methods affect 360&#x00B0; video experience. With the experiment findings, we also discuss useful design insights for 360&#x00B0; video bullet comments.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00017
ER  - 

TY  - NA
AU  - Cheng, Hsien-Tzu; Chao, Chun-Hung; Dong, Jin-Dong; Wen, Hao-Kai; Liu, Tyng-Luh; Sun, Min
TI  - CVPR - Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos
PY  - 2018
AB  - Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.
SP  - 1420
EP  - 1429
JF  - 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr.2018.00154
ER  - 

TY  - NA
AU  - Iravantchi, Yasha; Goel, Mayank; Harrison, Chris
TI  - CHI - BeamBand: Hand Gesture Sensing with Ultrasonic Beamforming
PY  - 2019
AB  - BeamBand is a wrist-worn system that uses ultrasonic beamforming for hand gesture sensing. Using an array of small transducers, arranged on the wrist, we can ensem-ble acoustic wavefronts to project acoustic energy at spec-ified angles and focal lengths. This allows us to interro-gate the surface geometry of the hand with inaudible sound in a raster-scan-like manner, from multiple view-points. We use the resulting, characteristic reflections to recognize hand pose at 8 FPS. In our user study, we found that BeamBand supports a six-class hand gesture set at 94.6% accuracy. Even across sessions, when the sensor is removed and reworn later, accuracy remains high: 89.4%. We describe our software and hardware, and future ave-nues for integration into devices such as smartwatches and VR controllers.
SP  - 15
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300245
ER  - 

TY  - JOUR
AU  - Li, Tiemeng; Zhu, Yangyang
TI  - Functional narrative animation as visual feedback for interactions in 3D visualization
PY  - 2022
AB  - Visual feedback can help users understand the function, state, and outcome of a system during the pre-, mid-, and post-interaction phases. Current visual feedback in 3D visualization scenarios takes less account of information transfer in terms of appearance design and dynamic behavior, which results in visual feedback being presented in a more engineered form and conveying simple information. For these issues, we proposed the concept and method of functional narrative animation as visual feedback for interactions in 3D visualization. We also provided a set of Unity-based animation library and a plugin tool for configuring the animations. Finally, through user experiments and interviews, we analyzed the role of functional narrative animation as in the interactive visual feedback for 3D visualizations and make corresponding design recommendations.
SP  - NA
EP  - NA
JF  - Computer Animation and Virtual Worlds
VL  - 33
IS  - 3-4
PB  - 
DO  - 10.1002/cav.2086
ER  - 

TY  - JOUR
AU  - Liu, Richen; Gao, Min; Wang, Lijun; Wang, Xiaohan; Xiang, Yuzhe; Zhang, Aolin; Xia, Jiazhi; Chen, Yi; Chen, Siming
TI  - Interactive Extended Reality Techniques in Information Visualization
PY  - 2022
AB  - Immersive techniques, such as virtual reality, augmented reality, and mixed reality, take immersive displays as carriers to provide immersive experience. A large number of approaches focus on the visualization of scientific data in immersive environments while just a few methods concentrate on interactive information visualization (InfoVis) in an immersive environment, although InfoVis has been extended to the 3-D space for a long time. In the era of data explosion, the traditional 2-D space is unable to convey large amounts of abstract information in an intuitive way. Meanwhile, desktop-based 3-D InfoVis generally leads to visual conflict and confusion owing to limited display size and field of vision. In this survey, we search for the interactive techniques in immersive InfoVis and summarize their commonalities and discuss their differences and potential trends. The data types of abstract information in InfoVis can be categorized into graph/network data, high-dimensional and multivariate data, time-varying data, and text and document data. Besides, the visual presentation of information in immersive environments is also summarized, especially for charts, plots, and diagrams, which are some basic components of InfoVis techniques. We also described the immersive applications of InfoVis techniques, including the tools or frameworks on immersive analytics and infographics. The discussion about the traditional nonimmersive and the immersive methods in data visualizations show that the latter one has the potential to become an alternative to explore massive information in the future.
SP  - 1338
EP  - 1351
JF  - IEEE Transactions on Human-Machine Systems
VL  - 52
IS  - 6
PB  - 
DO  - 10.1109/thms.2022.3211317
ER  - 

TY  - NA
AU  - Furumoto, Takuro; Fujiwara, Masahiro; Makino, Yasutoshi; Shinodas, Hiroyuki
TI  - VR - BaLuna: Floating Balloon Screen Manipulated Using Ultrasound
PY  - 2019
AB  - In this paper, we present BaLuna, a prototype of an externally actuated midair display for indoor use in a room-scale workspace. This system is the first battery-less midair display with a one-meter-cubic workspace. The system projects an image onto a balloon screen whose position is controlled by airborne ultrasound phased array (AUPA) devices. Users can naturally manipulate the screen position by dragging and dropping the screen directly with their hands. We adapted feedback-based acoustic manipulation technology that enables sparsely distributed AUPA devices to control the screen position. This is combined with a depth-image-based tracking and a three-dimensionally calibrated projector.
SP  - 937
EP  - 938
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797860
ER  - 

TY  - NA
AU  - Sassa, Hinako; Cordeil, Maxime; Yoshida, Mitsuo; Itoh, Takayuki
TI  - Brushing Feature Values in Immersive Graph Visualization Environment.
PY  - 2020
AB  - There are a variety of graphs where multidimensional feature values are assigned to the nodes. Visualization of such datasets is not an easy task since they are complex and often huge. Immersive Analytics is a powerful approach to support the interactive exploration of such large and complex data. Many recent studies on graph visualization have applied immersive analytics frameworks. However, there have been few studies on immersive analytics for visualization of multidimensional attributes associated with the input graphs. This paper presents a new immersive analytics system that supports the interactive exploration of multidimensional feature values assigned to the nodes of input graphs. The presented system displays label-axes corresponding to the dimensions of feature values, and label-edges that connect label-axes and corresponding to the nodes. The system supports brushing operations which controls the display of edges that connect a label-axis and nodes of the graph. This paper introduces visualization examples with a graph dataset of Twitter users and reviews by experts on graph data analysis.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Madan, Spandan; Bylinskii, Zoya; Tancik, Matthew; Recasens, Adrià; Zhong, Kimberli; Alsheikh, Sami; Pfister, Hanspeter; Oliva, Aude; Durand, Frédo
TI  - Synthetically Trained Icon Proposals for Parsing and Summarizing Infographics.
PY  - 2018
AB  - Widely used in news, business, and educational media, infographics are handcrafted to effectively communicate messages about complex and often abstract topics including `ways to conserve the environment' and `understanding the financial crisis'. Composed of stylistically and semantically diverse visual and textual elements, infographics pose new challenges for computer vision. While automatic text extraction works well on infographics, computer vision approaches trained on natural images fail to identify the stand-alone visual elements in infographics, or `icons'. To bridge this representation gap, we propose a synthetic data generation strategy: we augment background patches in infographics from our Visually29K dataset with Internet-scraped icons which we use as training data for an icon proposal mechanism. On a test set of 1K annotated infographics, icons are located with 38% precision and 34% recall (the best model trained with natural images achieves 14% precision and 7% recall). Combining our icon proposals with icon classification and text extraction, we present a multi-modal summarization application. Our application takes an infographic as input and automatically produces text tags and visual hashtags that are textually and visually representative of the infographic's topics respectively.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhao, Nanxuan; Kim, Nam Wook; Herman, Laura M.; Pfister, Hanspeter; Lau, Rynson W. H.; Echevarria, Jose; Bylinskii, Zoya
TI  - CHI - ICONATE: Automatic Compound Icon Generation and Ideation
PY  - 2020
AB  - Compound icons are prevalent on signs, webpages, and infographics, effectively conveying complex and abstract concepts, such as "no smoking" and "health insurance", with simple graphical representations. However, designing such icons requires experience and creativity, in order to efficiently navigate the semantics, space, and style features of icons. In this paper, we aim to automate the process of generating icons given compound concepts, to facilitate rapid compound icon creation and ideation. Informed by ethnographic interviews with professional icon designers, we have developed ICONATE, a novel system that automatically generates compound icons based on textual queries and allows users to explore and customize the generated icons. At the core of ICONATE is a computational pipeline that automatically finds commonly used icons for sub-concepts and arranges them according to inferred conventions. To enable the pipeline, we collected a new dataset, Compicon1k, consisting of 1000 compound icons annotated with semantic labels (i.e., concepts). Through user studies, we have demonstrated that our tool is able to automate or accelerate the compound icon design process for both novices and professionals.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376618
ER  - 

TY  - NA
AU  - Wu, Aoyu; Wang, Yun; Shu, Xinhuan; Moritz, Dominik; Cui, Weiwei; Zhang, Haidong; Zhang, Dongmei; Qu, Huamin
TI  - Survey on Artificial Intelligence Approaches for Visualization Data.
PY  - 2021
AB  - Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data. We define visualization data as the digital representations of visualizations in computers and focus on visualizations in information visualization and visual analytic. We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at this http URL.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chilton, Lydia B.; Ozmen, Ecenaz Jen; Ross, Sam H.; Liu, Vivian Yuen Ting
TI  - CHI - VisiFit: Structuring Iterative Improvement for Novice Designers
PY  - 2021
AB  - Visual blends are an advanced graphic design technique to seamlessly integrate two objects into one. Existing tools help novices create prototypes of blends, but it is unclear how they would improve them to be higher fidelity. To help novices, we aim to add structure to the iterative improvement process. We introduce a method for improving prototypes that uses secondary design dimensions to explore a structured design space. This method is grounded in the cognitive principles of human visual object recognition. We present VisiFit – a computational design system that uses this method to enable novice graphic designers to improve blends with computationally generated options they can select, adjust, and chain together. Our evaluation shows novices can substantially improve 76% of blends in under 4 minutes. We discuss how the method can be generalized to other blending problems, and how computational tools can support novices by enabling them to explore a structured design space quickly and efficiently.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445089
ER  - 

TY  - NA
AU  - Sassa, Hinako; Cordeil, Maxime; Yoshida, Mitsuo; Itoh, Takayuki
TI  - VINCI - Visual Linking of Feature Values in Immersive Graph Visualization Environment
PY  - 2021
AB  - There are a variety of graphs where multidimensional feature values are assigned to the nodes. Visualization of such datasets is not an easy task since they are complex and often huge. Immersive Analytics is a powerful approach to support the interactive exploration of such large and complex data. Many recent studies on graph visualization have applied immersive analytics frameworks; however, there have been few studies on immersive analytics for visualization of multidimensional attributes associated with the input graphs. This paper presents a new immersive analytics system that supports the interactive exploration of multidimensional feature values assigned to the nodes of input graphs. The presented system displays ”label-axes” corresponding to the dimensions of feature values, and ”label-edges” that connect label-axes and corresponding to the nodes. The system supports visual linking operations which controls the display of edges that connect a label-axis and nodes of the graph. This paper introduces visualization examples with a graph dataset of Twitter users and reviews by experts on graph data analysis.
SP  - NA
EP  - NA
JF  - The 14th International Symposium on Visual Information Communication and Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3481549.3481555
ER  - 

TY  - NA
AU  - Zhu, Yi; Miao, Chenglin; Zheng, Tianhang; Hajiaghajani, Foad; Su, Lu; Qiao, Chunming
TI  - CCS - Can We Use Arbitrary Objects to Attack LiDAR Perception in Autonomous Driving
PY  - 2021
AB  - As an effective way to acquire accurate information about the driving environment, LiDAR perception has been widely adopted in autonomous driving. The state-of-the-art LiDAR perception systems mainly rely on deep neural networks (DNNs) to achieve good performance. However, DNNs have been demonstrated vulnerable to adversarial attacks. Although there are a few works that study adversarial attacks against LiDAR perception systems, these attacks have some limitations in feasibility, flexibility, and stealthiness when being performed in real-world scenarios. In this paper, we investigate an easier way to perform effective adversarial attacks with high flexibility and good stealthiness against LiDAR perception in autonomous driving. Specifically, we propose a novel attack framework based on which the attacker can identify a few adversarial locations in the physical space. By placing arbitrary objects with reflective surface around these locations, the attacker can easily fool the LiDAR perception systems. Extensive experiments are conducted to evaluate the performance of the proposed attack, and the results show that our proposed attack can achieve more than 90% success rate. In addition, our real-world study demonstrates that the proposed attack can be easily performed using only two commercial drones. To the best of our knowledge, this paper presents the first study on the effect of adversarial locations on LiDAR perception models' behaviors, the first investigation on how to attack LiDAR perception systems using arbitrary objects with reflective surface, and the first attack against LiDAR perception systems using commercial drones in physical world. Potential defense strategies are also discussed to mitigate the proposed attacks.
SP  - 1945
EP  - 1960
JF  - Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460120.3485377
ER  - 

TY  - NA
AU  - Kimura, Naoki; Kono, Michinari; Rekimoto, Jun
TI  - PerDis - Deep dive: deep-neural-network-based video extension for immersive head-mounted display experiences
PY  - 2019
AB  - Immersion is an important factor in video experiences. Therefore, various methods and video viewing systems have been proposed. Head-mounted displays (HMDs) are home-friendly pervasive devices, which can provide an immersive video experience owing to their wide field-of-view (FoV) and separation of users from the outside environment. They are often used for viewing panoramic and stereoscopic recorded videos or virtually generated environments, but the demand for viewing standard plane videos with HMDs has increased. However, the theater mode, which restricts the FoV, is basically used for viewing plane videos. Thus, the advantages of HMDs are not fully utilized. Therefore, we explored a method for viewing plane videos by an HMD, in combination with view augmentation by LED implants to the HMD. We have constructed a system for viewing plane videos using an HMD with a deep neural network (DNN) model optimized for generating and extending images for peripheral vision and wide FoV customization. We found that enlarging the original video and extending the video with our DNN model can improve the user experience. However, our method provided more comfortable viewing by preventing motion sickness in a first-person-view video.
SP  - 22
EP  - NA
JF  - Proceedings of the 8th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3321335.3324932
ER  - 

TY  - NA
AU  - Xu, Chaoqing; Neuroth, Tyson; Fujiwara, Takanori; Liang, Ronghua; Ma, Kwan-Liu
TI  - A Predictive Visual Analytics System for Studying Neurodegenerative Disease based on DTI Fiber Tracts
PY  - 2020
AB  - Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce an intelligent visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system's AI-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space, and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using real data from the research database of Parkinson's Progression Markers Initiative.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gu, Jianzhe; Lin, Yuyu; Cui, Qiang; Li, Xiaoqian; Li, Jiaji; Sun, Lingyun; Yao, Cheng; Ying, Fangtian; Wang, Guanyun; Yao, Lining
TI  - PneuMesh: Pneumatic-driven Truss-based Shape Changing System
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502099
ER  - 

TY  - CHAP
AU  - Garrido, Daniel; Jacob, João; Silva, Daniel Castro
TI  - ICCS (1) - Building a Prototype for Easy to Use Collaborative Immersive Analytics
PY  - 2021
AB  - The increase in the size and complexity of today’s datasets creates a need to develop and experiment with novel data visualization methods. One of these innovations is immersive analytics, in which extended reality technologies such as virtual reality headsets are used to present and study data in virtual worlds. But while the use of immersive analytics dates back to the end of the 20th century, it wasn’t until recently that collaboration in these data visualization environments was taken in consideration. One of the problems currently surrounding this field is the lack of availability of easy to use cooperative data visualization tools that take advantage of the modern, easily attainable head mounted display virtual reality solutions. This work proposes to create an accessible collaborative immersive analytics framework that users with low virtual reality background can master, and share, regardless of platform. With this in mind, a prototype of a visualization platform was developed in Unity3D that allows users to create their own visualizations and collaborate with other users from around the world. Additional features such as avatars, resizable visualizations and data highlighters were implemented to increase immersion and collaborative thinking. The end result shows promising qualities, as it is platform versatile, simple to setup and use and is capable of rapidly enabling groups to meet and analyse data in an immersive environment, even across the world.
SP  - 628
EP  - 641
JF  - Computational Science – ICCS 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77961-0_50
ER  - 

TY  - JOUR
AU  - Zhao, Nanxuan; Cao, Ying; Lau, Rynson W. H.
TI  - What characterizes personalities of graphic designs
PY  - 2018
AB  - Graphic designers often manipulate the overall look and feel of their designs to convey certain personalities (e.g., cute, mysterious and romantic) to impress potential audiences and achieve business goals. However, understanding the factors that determine the personality of a design is challenging, as a graphic design is often a result of thousands of decisions on numerous factors, such as font, color, image, and layout. In this paper, we aim to answer the question of what characterizes the personality of a graphic design. To this end, we propose a deep learning framework for exploring the effects of various design factors on the perceived personalities of graphic designs. Our framework learns a convolutional neural network (called personality scoring network) to estimate the personality scores of graphic designs by ranking the crawled web data. Our personality scoring network automatically learns a visual representation that captures the semantics necessary to predict graphic design personality. With our personality scoring network, we systematically and quantitatively investigate how various design factors (e.g., color, font, and layout) affect design personality across different scales (from pixels, regions to elements). We also demonstrate a number of practical application scenarios of our network, including element-level design suggestion and example-based personality transfer.
SP  - 116
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201355
ER  - 

TY  - CHAP
AU  - Chen, Taizhou; Wu, Yi-Shiun; Zhu, Kening
TI  - INTERACT (2) - DupRobo: Interactive Robotic Autocompletion of Physical Block-Based Repetitive Structure
PY  - 2019
AB  - In this paper, we present DupRobo, an interactive robotic platform for tangible block-based design and construction. DupRobo supported user-customizable exemplar, repetition control, and tangible autocompletion, through the computer-vision and the robotic techniques. With DupRobo, we aim to reduce users’ workload in repetitive block-based construction, yet preserve the direct manipulatability and the intuitiveness in tangible model design, such as product design and architecture design. Through a user study with 12 participants, we found that DupRobo significantly reduced participants’ perceived physical demand, overall efforts, and frustration in the process of block-based structure design and construction, compared to the situation without DupRobo. In addition, the participants rated DupRobo as easy to learn and use.
SP  - 475
EP  - 495
JF  - Human-Computer Interaction – INTERACT 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29384-0_29
ER  - 

TY  - JOUR
AU  - Ghaemi, Zeinab; Engelke, Ulrich; Ens, Barrett; Jenny, Bernhard
TI  - Proxemic maps for immersive visualization
PY  - 2022
AB  - NA
SP  - 205
EP  - 219
JF  - Cartography and Geographic Information Science
VL  - 49
IS  - 3
PB  - 
DO  - 10.1080/15230406.2021.2013946
ER  - 

TY  - NA
AU  - Ochieng, Dennis Miyogi
TI  - Exploring e3-value ontology-based service engineering for participatory processes of community development projects
PY  - 2018
AB  - Submitted in fulfillment of the requirements of the Doctor of Philosophy in Information Technology, Durban University of Technology, Durban, South Africa, 2018.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wu, Aoyu; Wang, Yun; Shu, Xinhuan; Moritz, Dominik; Cui, Weiwei; Zhang, Haidong; Zhang, Dongmei; Qu, Huamin
TI  - AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization.
PY  - 2021
AB  - Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at this http URL.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Li, Zhengqing; Miyafuji, Shio; Wu, Erwin; Kuzuoka, Hideaki; Yamashita, Naomi; Koike, Hideki
TI  - Conference on Designing Interactive Systems - OmniGlobe: An Interactive I/O System For Symmetric 360-Degree Video Communication
PY  - 2019
AB  - Video communication systems have been suffered from the narrow field of view. To solve this limitation, one study proposed symmetric 360° video communication system by combining an omnidirectional camera and a hemispherical display. However, the system still had several issues, e.g., the invisibility of hemisphere which was at the opposite side from a user caused the inconvenience of observing the remote environment. To solve these issues, we introduce OmniGlobe, a novel symmetric full 360° video communication system which incorporates an omnidirectional camera, a full spherical display, and several visual or interactive techniques. Based on an experiment, we could indicate that our system is effective in reducing the inconvenience of observing the remote environment and increased the remote space awareness and user's gaze awareness to support remote collaboration. We also discuss the takeaways, limitations and application areas in our system which help improve the system.
SP  - 1427
EP  - 1438
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322314
ER  - 

TY  - JOUR
AU  - Wu, Aoyu; Tong, Wai; Dwyer, Tim; Lee, Bongshin; Isenberg, Petra; Qu, Huamin
TI  - MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework
PY  - 2021
AB  - We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.
SP  - 464
EP  - 474
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030423
ER  - 

TY  - NA
AU  - Gupta, Prakhar; Gupta, Shubh; Jayagopal, Ajaykrishnan; Pal, Sourav; Sinha, Ritwik
TI  - Saliency Prediction for Mobile User Interfaces
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - 2018 IEEE Winter Conference on Applications of Computer Vision (WACV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wacv.2018.00171
ER  - 

TY  - NA
AU  - Liu, Dawei; Cao, Ying; Lau, Rynson W. H.; Chan, Antoni B.
TI  - ICME - ButtonTips: Design Web Buttons with Suggestions
PY  - 2019
AB  - Buttons are fundamental in web design. An effective button is important for higher click-through and conversion rates. However, designing effective buttons can be challenging for novices. This paper presents a novel interactive method to aid the button design process by making design suggestions. Our method proceeds in three steps: 1) button presence prediction, 2) button layout suggestion and 3) button color selection. We investigate two distinct but complementary interfaces for button design suggestion: 1) region selection interface, where the button will appear in a user-specific region; 2) element selection interface, where the button will be associated with a user-selected element. We compare our method with an existing website building tool, and show that for novice designers, both interfaces require significantly less manual efforts, and produce significantly better button design, as evaluated by professional web designers.
SP  - 466
EP  - 471
JF  - 2019 IEEE International Conference on Multimedia and Expo (ICME)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icme.2019.00087
ER  - 

TY  - CHAP
AU  - Swearngin, Amanda; Li, Yang
TI  - Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning
PY  - 2021
AB  - NA
SP  - 73
EP  - 96
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_3
ER  - 

TY  - NA
AU  - Xu, Xuhai; Gong, Jun; Brum, Carolina; Liang, Lilian; Suh, Bongsoo; Gupta, Shivam Kumar; Agarwal, Yash; Lindsey, Laurence; Kang, Runchang; Shahsavari, Behrooz; Nguyen, Tu; Nieto, Heriberto; Hudson, Scott E; Maalouf, Charlie; Mousavi, Jax Seyed; Laput, Gierad
TI  - Enabling Hand Gesture Customization on Wrist-Worn Devices
PY  - 2022
AB  - We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we first deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7% and a false-positive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3%, 83.1%, and 87.2% on using one, three, or five shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the effectiveness, learnability, and usability of our customization framework. Our approach paves the way for a future where users are no longer bound to pre-existing gestures, freeing them to creatively introduce new gestures tailored to their preferences and abilities.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501904
ER  - 

TY  - NA
AU  - Endo, Isamu; Takashima, Kazuki; Inoue, Maakito; Fujita, Kazuyuki; Kiyokawa, Kiyoshi; Kitamura, Yoshifumi
TI  - CHI Extended Abstracts - A Reconfigurable Mobile Head-Mounted Display Supporting Real World Interactions
PY  - 2021
AB  - We propose a new mobile head-mounted display, ModularHMD, that uses a modular mechanism with a manually reconfigurable structure to enable ad-hoc peripheral interaction with real-world objects and people while maintaining an immersive VR experience. We designed and built a proof-of-concept prototype of ModularHMD using a base commercial HMD and three removable display modules installed in the periphery of the HMD cowl. The user can rapidly switch between different HMD configurations according to their needs. The modules can be removed to ensure peripheral awareness of the real world, used as instant interaction devices (e.g, keyboards), and then returned to their original positions to terminate the peripheral interaction.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451765
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Kuo, Tzu-Sheng; Wang, Chi; Chiang, Chi-huan; Huang, Da-Yuan; Chan, Liwei; Chen, Bing-Yu
TI  - UIST - PuPoP: Pop-up Prop on Palm for Virtual Reality
PY  - 2018
AB  - The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.
SP  - 5
EP  - 17
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242628
ER  - 

TY  - NA
AU  - Das, Sauvik; Lu, David; Lee, Taehoon; Lo, Joanne; Hong, Jason
TI  - UIST - The Memory Palace: Exploring Visual-Spatial Paths for Strong, Memorable, Infrequent Authentication
PY  - 2019
AB  - Many accounts and devices require only infrequent authentication by an individual, and thus authentication secrets should be both secure and memorable without much reinforcement. Inspired by people's strong visual-spatial memory, we introduce a novel system to help address this problem: the Memory Palace. The Memory Palace encodes authentication secrets as paths through a 3D virtual labyrinth navigated in the first-person perspective. We ran two experiments to iteratively design and evaluate the Memory Palace. In the first, we found that visual-spatial secrets are most memorable if navigated in a 3D first-person perspective. In the second, we comparatively evaluated the Memory Palace against Android's 9-dot pattern lock along three dimensions: memorability after one week, resilience to shoulder surfing, and speed. We found that relative to 9-dot, complexity-controlled secrets in the Memory Palace were significantly more memorable after one week, were much harder to break through shoulder surfing, and were not significantly slower to enter.
SP  - 1109
EP  - 1121
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347917
ER  - 

TY  - JOUR
AU  - Ding, Dian; Yang, Lanqing; Chen, Yi-Chao; Xue, Guangtao
TI  - Leakage or Identification
PY  - 2021
AB  - <jats:p>The convenience of laptops brings with it the risk of information leakage, and conventional security systems based on the password or the explicit biometric do little to alleviate this problem. Biometric identification based on anatomical features provides far stronger security; however, a lack of suitable sensors on laptops limits the applicability of this technology. In this paper, we developed a behavior-irrelevant user identification system applicable to laptops with a metal casing. The proposed scheme, referred to as LeakPrint, is based on leakage current, wherein the system uses an earphone to capture current leaking through the body and then transmits the corresponding signal to a server for identification. The user identification is achieved via denoising, dimension reduction, and feature extraction. Compared to other biometric identification methods, the proposed system is less dependent on external hardware and more robust to environmental noise. The experiments in real-world environments demonstrated that LeakPrint can verify user identity with high accuracy (93.6%), while providing effective defense against replay attacks (96.5%) and mimicry attacks (90.9%).</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494984
ER  - 

TY  - CHAP
AU  - Wang, Guan; Gu, Wenying; Suh, Ayoung
TI  - HCI (23) - The Effects of 360-Degree VR Videos on Audience Engagement: Evidence from the New York Times
PY  - 2018
AB  - This study examines the current application of 360-degree VR videos in the news industry. Both the advantages and challenges of 360-degree VR videos in enhancing audience experiences and engagement are discussed. To better understand the effects of immersive technology on audience engagement, this study selects the case of The New York Times (NYT). Data were crawled from 598 videos on the NYT YouTube account for analyses. The results showed that 360-degree VR videos generally performed worse than non-VR videos in enhancing audience engagement. An interaction effect was found between video format (360-degree VR or non-VR) and content genres.
SP  - 217
EP  - 235
JF  - HCI in Business, Government, and Organizations
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-91716-0_17
ER  - 

TY  - NA
AU  - Sun, Bo; Weidner, Benjamin; Fritz, Aleksandr
TI  - A Comparative Performance Study on Immersive Analytics to Support Everyday Use
PY  - 2021
AB  - With the rapid growth of Big Data, Immersive Analytics became an emerging area to help analyze large scale and multidimensional data set. Immersive Analytics relies on immersive data visualization to produce analytical reasoning for data analytics. In this paper, we present two immersive analytics platforms operated on HMD and aim to support everyday use by highlighting their design approaches and differences. We conducted user case studies to further compare their analytical usability and summarized our findings in interaction modalities, physical navigations and visual representations. The research findings provide guidance to future immersive analytics design for everyday use.
SP  - 135
EP  - 143
JF  - 2021 IEEE 7th International Conference on Virtual Reality (ICVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icvr51878.2021.9483861
ER  - 

TY  - JOUR
AU  - Zhu, Lifeng; Jiang, Xudong; Shen, Jiangwei; Zhang, Heng; Mo, Yiting; Song, Aiguo
TI  - TapeTouch: A Handheld Shape-changing Device for Haptic Display of Soft Objects.
PY  - 2022
AB  - Haptic feedback is widely used to enhance realism in virtual reality (VR). Shape and softness are two common factors perceived by the users in the haptic rendering of soft objects. To integrate these factors, we propose a new handheld shape-changing device, TapeTouch, to provide various shapes and softness in real time. TapeTouch is based on a controllable shape-changing tape, which is mainly composed of four motors and a section of brass tape. We design a structure of the components to fit a portable controller and allow to flexibly adjust the shape of the brass tape. After decoding desired shapes into the signals to control the motor, we automatically reproduce varying shapes and levels of softness to the finger or palm touching the shape-changing tape. We conducted two user studies to understand the capability of TapeTouch to render shape and softness, and the results showed that TapeTouch could provide a variety of distinguishable shapes as well as multiple levels of softness. Based on the results, we performed two VR experience studies to verify that the haptic feedback from TapeTouch enhances VR realism.
SP  - 3928
EP  - 3938
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203087
ER  - 

TY  - JOUR
AU  - Shahana Fatima, ; Kashish Wanjari, ; Valiuddin Qureshi, ; Rinku Shivhare, ; Rakib Pathan, 
TI  - Gesture Recognition using Open CV
PY  - 2022
AB  - <jats:p>With advanced technologies in this digital era, there is always scope for development in the field of computing. Hands free computing is in demand as of today it addresses the needs of quadriplegics. This paper presents a Human computer interaction (HCI) system that is of great importance to amputees and those who have issues with using their hands. The system built is an hand gesture-based interface that acts as a computer mouse to translate finger movements towards the mouse cursor actions with this we have implemented eye recognition/ detection. The system in discussion makes use of a simple webcam and its software requirements arespyder(anaconda 3), OpenCV, NumPy and a few other packages which are necessary for gesture recognition. The gesture detector can be built using the HOG (Histogram of oriented Gradients) feature along with a linear classifier, and the sliding window technique. It is hands free, and no external hardware or sensors are required..</jats:p>
SP  - 794
EP  - 797
JF  - International Journal of Advanced Research in Science, Communication and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.48175/ijarsct-4662
ER  - 

TY  - NA
AU  - Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers
PY  - 2020
AB  - Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Vij, Ritvik; Raj, Rohit; Singhal, Madhur; Tanwar, Manish; Bedathur, Srikanta
TI  - VizAI : Selecting Accurate Visualizations of Numerical Data
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3493700.3493717
ER  - 

TY  - NA
AU  - Han, Feilin; Zhong, Ying; Zhou, Minxi
TI  - Evaluating the Effect of Cinematography on the Viewing Experience in Immersive Environment
PY  - 2022
AB  - Cinematic Virtual Reality (CVR) is an increasingly popular digital art production technology that could enhance the sense of presence when a viewer explores immersive environments. There are three important viewing-experience-related aspects, attention, sustainability, and guidance, which can be affected by the cinematography principles. Attention indicates whether the viewer is focusing on the storytelling-related region or not. Sustainability refers to viewers' ability to continuously watch the CVR content, and guidance affects the understanding of the narrative. In this paper, we conducted within-subject repeated-measures experiments on 22 participants in an HMD-based immersive environment, to explore the correlation between viewing experience and comprehensive factors. According to experimental results, we suggest an attention-comfort-understanding analysis paradigm for directing the CVR shot, which could help creators effectively attract viewers' attention, minimize the cybersickness, and deepen their understanding of narratives.
SP  - NA
EP  - NA
JF  - 2022 IEEE International Conference on Multimedia and Expo (ICME)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icme52920.2022.9859864
ER  - 

TY  - JOUR
AU  - Yamada, Wataru
TI  - 6. Spherical Drone Display
PY  - 2018
AB  - NA
SP  - 504
EP  - 507
JF  - The Journal of The Institute of Image Information and Television Engineers
VL  - 72
IS  - 7
PB  - 
DO  - 10.3169/itej.72.504
ER  - 

TY  - NA
AU  - Chen, Yang; Sun, Leyuan; Benallegue, Mehdi; Cisneros-Limon, Rafael; Singh, Rohan P.; Kaneko, Kenji; Tanguy, Arnaud; Caron, Guillaume; Suzuki, Kenji; Kheddar, Abderrahmane; Kanehiro, Fumio
TI  - Enhanced Visual Feedback with Decoupled Viewpoint Control in Immersive Humanoid Robot Teleoperation using SLAM
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/humanoids53995.2022.9999740
ER  - 

TY  - JOUR
AU  - Beaudouin-Lafon, Michel; Bødker, Susanne; Mackay, Wendy E.
TI  - Generative Theories of Interaction
PY  - 2021
AB  - Although Human–Computer Interaction research has developed various theories and frameworks for analyzing new and existing interactive systems, few address the generation of novel technological solu...
SP  - 1
EP  - 54
JF  - ACM Transactions on Computer-Human Interaction
VL  - 28
IS  - 6
PB  - 
DO  - 10.1145/3468505
ER  - 

TY  - JOUR
AU  - Wang, Qianwen; Chen, Zhutian; Wang, Yong; Qu, Huamin
TI  - A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization.
PY  - 2022
AB  - Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VIS is needed. In this article, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: "what visualization processes can be assisted by ML?" and "how ML techniques can be used to solve visualization problems? "This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this article can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io.
SP  - 5134
EP  - 5153
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3106142
ER  - 

TY  - NA
AU  - Tanaka, Yudai; Nishida, Jun; Lopes, Pedro
TI  - Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501910
ER  - 

TY  - NA
AU  - Parizi, Farshid Salemi; Kienzle, Wolf; Whitmire, Eric; Gupta, Aakar; Benko, Hrvoje
TI  - RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband
PY  - 2021
AB  - We introduce RotoWrist, an infrared (IR) light based solution for continuously and reliably tracking 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489886
ER  - 

TY  - NA
AU  - Kim, Hyunyoung
TI  - UIST (Adjunct Volume) - Fostering Design Process of Shape-Changing Interfaces
PY  - 2018
AB  - Shape-changing interfaces match forms and haptics with functions and bring affordances to devices. I believe that shape-changing interfaces will be increasingly available to end-users in the future. To increase acceptance of shape-changing interfaces by end-users, we need to provide designers with design criteria and framework closely grounded on their current skills and needs. Also, we need to provide them with prototyping tools to enable quick assessment of ideas in the physical world. In this paper, I introduce the three threads of my Ph.D. research in the direction of providing the design tools. First, I advance existing shape-changing interface taxonomies to broaden design vocabulary and systemize design framework, based on the classification of everyday objects. Second, I conduct a study with end-users to suggest interaction techniques and design guidelines for shape-changing interfaces from their current practice. Lastly, I develop a physical prototyping tool for shape-changing interfaces to shorten prototyping iterations based on well-known Lego-like bricks.
SP  - 224
EP  - 227
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266131
ER  - 

TY  - NA
AU  - Feick, Martin; Bateman, Scott; Tang, Anthony; Miede, André; Marquardt, Nicolai
TI  - TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation in Virtual Reality
PY  - 2020
AB  - Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit, and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives, and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators, and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi's proxies outperform traditional controllers, and were generally favored by participants.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Park, Joon Sung; Popowski, Lindsay; Cai, Carrie; Morris, Meredith Ringel; Liang, Percy; Bernstein, Michael S.
TI  - Social Simulacra: Creating Populated Prototypes for Social Computing Systems
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545616
ER  - 

TY  - NA
AU  - Griggio, Carla F.; Sato, Arissa J.; Mackay, Wendy E.; Yatani, Koji
TI  - CHI - Mediating Intimacy with DearBoard: a Co-Customizable Keyboard for Everyday Messaging
PY  - 2021
AB  - Co-customizations are collaborative customizations in messaging apps that all conversation members can view and change, e.g. the color of chat bubbles on Facebook Messenger. Co-customizations grant new opportunities for expressing intimacy; however, most apps offer private customizations only. To investigate how people in close relationships integrate co-customizations into their established communication app ecosystems, we built DearBoard: an Android keyboard that allows two people to co-customize its color theme and a toolbar of expression shortcuts (emojis and GIFs). In a 5-week field study with 18 pairs of couples, friends, and relatives, participants expressed their shared interests, history, and knowledge of each other through co-customizations that served as meaningful decorations, interface optimizations, conversation themes, and non-verbal channels for playful, affectionate interactions. The co-ownership of the co-customizations invited participants to negotiate who customizes what and for whom they customize. We discuss how co-customizations mediate intimacy through place-making efforts and suggest design opportunities.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445757
ER  - 

TY  - NA
AU  - Alhakamy, A’aeshah; Trajkova, Milka; Cafaro, Francesco
TI  - Conference on Designing Interactive Systems - Show Me How You Interact, I Will Tell You What You Think: Exploring the Effect of the Interaction Style on Users’ Sensemaking about Correlation and Causation in Data
PY  - 2021
AB  - Findings from embodied cognition suggest that our whole body (not just our eyes) plays an important role in how we make sense of data when we interact with data visualizations. In this paper, we present the results of a study that explores how different designs of the ”interaction” (with a data visualization) alter the way in which people report and discuss correlation and causation in data. We conducted a lab study with two experimental conditions: Full body (participants interacted with a 65” display showing geo-referenced data using gestures and body movements); and, Gamepad (people used a joypad to control the system). Participants tended to agree less with statements that portray correlation and causation in data after using the Gamepad system. Additionally, discourse analysis based on Conceptual Metaphor Theory revealed that users made fewer remarks based on FORCE schemata in Gamepad than in Full-Body.
SP  - 564
EP  - 575
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462083
ER  - 

TY  - NA
AU  - Yu, Jiahui; Li, Min; Zhang, Xinlei; Zhang, Tao; Zhou, Xianzhong
TI  - A Multi-sensor Gesture Interaction System for Human-robot Cooperation
PY  - 2021
AB  - Gestures are considered as a natural expression of the human body and are used to communicate with other people. The gesture-based human-robot interaction is natural, convenient, and applicable, and can be applied to complex interactive scenarios. In this paper, considering the real-time nature of the human-robot cooperation(HRC) system and the variability of the interaction range, we combine Kinect V2.0 (far-range sensor) and Leap Motion (short-range and high precision sensor), and propose a real-time multi-sensor gesture interaction system. Firstly, a reasonable layout of two sensors is discussed to realize far-range perception of natural gesture interaction. Then, nine gestures are defined that are easy for users to remember and operate. At the same time, a gesture interactive mechanism is proposed that can automatically switch two sensors according to the distance of the operator&#x2019;s position. It can better improve the defects such as occlusion and confusion in the process of gesture controlling, and solve the minimum distance constraint of Kinect. Finally, the interactive experiment proves the stability and accuracy of the system.
SP  - NA
EP  - NA
JF  - 2021 IEEE International Conference on Networking, Sensing and Control (ICNSC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icnsc52481.2021.9702166
ER  - 

TY  - JOUR
AU  - Garg, Radhika; Moreno, Christopher
TI  - Understanding Motivators, Constraints, and Practices of Sharing Internet of Things
PY  - 2019
AB  - Smart devices such as mobile phones, tablets, and smart watches are designed under the assumption that they will be used by a single user. In contrast, many other devices, such as smart thermostats and smart speakers, are inherently sharable. This paper presents results from a diary study that we conducted with 20 participants to gain a nuanced understanding of the purposes, motivators, and constraints involved in the sharing of smart devices, which are cumulatively referred to as the Internet of Things. We also report on users' practices of coordinating their shared use with sharees/co-users, the impact of not understanding a smart device's behavior and the context of shared use, the differences between sharing personal and inherently sharable devices in terms of content that is available and accessible, trust between sharees, and measures taken to ensure accountable use. Finally, we discuss the implications of our findings and provide guidelines for the design of future smart devices.
SP  - 44
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 2
PB  - 
DO  - 10.1145/3328915
ER  - 

TY  - NA
AU  - Ayyanchira, Akshay Murari
TI  - CROSS-PLATFORM INDOOR NAVIGATION USING MIXED-REALITY
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Romat, Hugo; Riche, Nathalie Henry; Hurter, Christophe; Drucker, Steven M.; Amini, Fereshteh; Hinckley, Ken
TI  - CHI - Dear Pictograph: Investigating the Role of Personalization and Immersion for Consuming and Enjoying Visualizations
PY  - 2020
AB  - Much of the visualization literature focuses on assessment of visual representations with regard to their effectiveness for understanding data. In the present work, we instead focus on making data visualization experiences more enjoyable, to foster deeper engagement with data. We investigate two strategies to make visualization experiences more enjoyable and engaging: personalization, and immersion. We selected pictographs (composed of multiple data glyphs) as this representation affords creative freedom, allowing people to craft symbolic or whimsical shapes of personal significance to represent data. We present the results of a qualitative study with 12 participants crafting pictographs using a large pen-enabled device and while immersed within a VR environment. Our results indicate that personalization and immersion both have positive impact on making visualizations more enjoyable experiences.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376348
ER  - 

TY  - NA
AU  - Zheng, Qingyuan; Li, Zhuoru; Bargteil, Adam W.
TI  - Learning Aesthetic Layouts via Visual Guidance.
PY  - 2021
AB  - We explore computational approaches for visual guidance to aid in creating aesthetically pleasing art and graphic design. Our work complements and builds on previous work that developed models for how humans look at images. Our approach comprises three steps. First, we collected a dataset of art masterpieces and labeled the visual fixations with state-of-art vision models. Second, we clustered the visual guidance templates of the art masterpieces with unsupervised learning. Third, we developed a pipeline using generative adversarial networks to learn the principles of visual guidance and that can produce aesthetically pleasing layouts. We show that the aesthetic visual guidance principles can be learned and integrated into a high-dimensional model and can be queried by the features of graphic elements. We evaluate our approach by generating layouts on various drawings and graphic designs. Moreover, our model considers the color and structure of graphic elements when generating layouts. Consequently, we believe our tool, which generates multiple aesthetic layout options in seconds, can help artists create beautiful art and graphic designs.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Chu, Shaowei; Tu, Huawei
TI  - A Comparative Evaluation of Mechanical Vibration and Ultrasonic Vibration on Smartphones in Tactile Code Perception
PY  - 2022
AB  - NA
SP  - 41038
EP  - 41046
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3167526
ER  - 

TY  - CHAP
AU  - Lou, Yun; Gao, Weiyue; Chen, Pei; Liu, Xuanhui; Yang, Changyuan; Sun, Lingyun
TI  - DesignEva: A Design-Supported Tool with Multi-faceted Perceptual Evaluation
PY  - 2022
AB  - AbstractPerceptual design evaluation helps designers recognize how others perceive their work and iterate their design process. Organizing user studies to gather human perceptual evaluation is time-consuming. Thus, computational evaluation methods are proposed to provide rapid and reliable feedback for designers. In recent years, the development of deep neural networks has enabled Artificial Intelligence (AI) to conduct perceptual quality evaluation as human beings. This article proposes to utilize AI to provide designers with real-time evaluations of their designs and to facilitate the iterative design. To achieve this, we developed a prototype, DesignEva, a design-supported tool to offer multi-faceted perceptual evaluation on design works, including aesthetics, visual importance, memorability, and sentiment. In addition, based on designers’ current works, DesignEva also searches for similar examples from the material library as references to inspire designers. We conducted a user study to verify the effectiveness of our proposed prototype. The experimental results showed that DesignEva could help designers reflect on their designs from different perspectives in a timely way.KeywordsDesign evaluationPerceptual evaluationDesign-supported tool
SP  - 508
EP  - 519
JF  - Cross-Cultural Design. Interaction Design Across Cultures
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06038-0_38
ER  - 

TY  - NA
AU  - Yuan, Linping; Zhou, Ziqi; Zhao, Jian; Guo, Yiqiu; Du, Fan; Qu, Huamin
TI  - InfoColorizer: Interactive Recommendation of Color Palettes for Infographics
PY  - 2021
AB  - When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements' spatial arrangement. We propose a data-driven method that provides flexibility by considering users' preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Neshati, Ali; Salo, Aaron; Faleel, Shariff Am; Li, Ziming; Liang, Hai-Ning; Latulipe, Celine; Irani, Pourang
TI  - EdgeSelect: Smartwatch Data Interaction with Minimal Screen Occlusion
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556586
ER  - 

TY  - NA
AU  - Rudolph, Julius Cosmo Romeo; Holman, David; De Araujo, Bruno; Jota, Ricardo; Wigdor, Daniel; Savage, Valkyrie
TI  - Sensing Hand Interactions with Everyday Objects by Profiling Wrist Topography
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501320
ER  - 

TY  - NA
AU  - Troost, Ivar; Tanhaei, Ghazaleh; Hardman, Lynda; Hürst, Wolfgang
TI  - AIVR - DatAR: An Immersive Literature Exploration Environment for Neuroscientists
PY  - 2020
AB  - Maintaining an overview of publications in the neuroscientific field is challenging, especially with an eye to finding relations at scale; for example, between brain regions and diseases. This is true for well-studied as well as nascent relationships. To support neuroscientists in this challenge, we developed an Immersive Analytics (IA) prototype for the analysis of relationships in large collections of scientific papers. In our video demonstration we showcase the system’s design and capabilities using a walkthrough and mock user scenario. This companion paper relates our prototype to previous IA work and offers implementation details.
SP  - 55
EP  - 56
JF  - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr50618.2020.00020
ER  - 

TY  - JOUR
AU  - Rateau, Hanae; Lank, Edward; Liu, Zhe
TI  - Leveraging Smartwatch and Earbuds Gesture Capture to Support Wearable Interaction
PY  - 2022
AB  - <jats:p>Due to the proliferation of smart wearables, it is now the case that designers can explore novel ways that devices can be used in combination by end-users. In this paper, we explore the gestural input enabled by the combination of smart earbuds coupled with a proximal smartwatch. We identify a consensus set of gestures and a taxonomy of the types of gestures participants create through an elicitation study. In a follow-on study conducted on Amazon's Mechanical Turk, we explore the social acceptability of gestures enabled by watch+earbud gesture capture. While elicited gestures continue to be simple, discrete, in-context actions, we find that elicited input is frequently abstract, varies in size and duration, and is split almost equally between on-body, proximal, and more distant actions. Together, our results provide guidelines for on-body, near-ear, and in-air input using earbuds and a smartwatch to support gesture capture.</jats:p>
SP  - 31
EP  - 50
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567710
ER  - 

TY  - JOUR
AU  - Babic, Teo; Reiterer, Harald; Haller, Michael
TI  - Understanding and Creating Spatial Interactions with Distant Displays Enabled by Unmodified Off-The-Shelf Smartphones
PY  - 2022
AB  - <jats:p>Over decades, many researchers developed complex in-lab systems with the overall goal to track multiple body parts of the user for a richer and more powerful 2D/3D interaction with a distant display. In this work, we introduce a novel smartphone-based tracking approach that eliminates the need for complex tracking systems. Relying on simultaneous usage of the front and rear smartphone cameras, our solution enables rich spatial interactions with distant displays by combining touch input with hand-gesture input, body and head motion, as well as eye-gaze input. In this paper, we firstly present a taxonomy for classifying distant display interactions, providing an overview of enabling technologies, input modalities, and interaction techniques, spanning from 2D to 3D interactions. Further, we provide more details about our implementation—using off-the-shelf smartphones. Finally, we validate our system in a user study by a variety of 2D and 3D multimodal interaction techniques, including input refinement.</jats:p>
SP  - 94
EP  - 94
JF  - Multimodal Technologies and Interaction
VL  - 6
IS  - 10
PB  - 
DO  - 10.3390/mti6100094
ER  - 

TY  - NA
AU  - Siwach, Gautam; Haridas, Adinarayana; Bunch, Don
TI  - Inferencing Big Data with Artificial Intelligence & Machine Learning Models in Metaverse
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 International Conference on Smart Applications, Communications and Networking (SmartNets)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smartnets55823.2022.9994013
ER  - 

TY  - NA
AU  - Vij, Ritvik; Raj, Rohit; Singhal, Madhur; Tanwar, Manish; Bedathur, Srikanta
TI  - VizAI : Selecting Accurate Visualizations of Numerical Data.
PY  - 2021
AB  - A good data visualization is not only a distortion-free graphical representation of data but also a way to reveal underlying statistical properties of the data. Despite its common use across various stages of data analysis, selecting a good visualization often is a manual process involving many iterations. Recently there has been interest in reducing this effort by developing models that can recommend visualizations, but they are of limited use since they require large training samples (data and visualization pairs) and focus primarily on the design aspects rather than on assessing the effectiveness of the selected visualization. In this paper, we present VizAI, a generative-discriminative framework that first generates various statistical properties of the data from a number of alternative visualizations of the data. It is linked to a discriminative model that selects the visualization that best matches the true statistics of the data being visualized. VizAI can easily be trained with minimal supervision and adapts to settings with varying degrees of supervision easily. Using crowd-sourced judgements and a large repository of publicly available visualizations, we demonstrate that VizAI outperforms the state of the art methods that learn to recommend visualizations.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Avila, Johann Felipe Gonzalez; McClelland, John C.; Teather, Robert J.; Figueroa, Pablo; Girouard, Audrey
TI  - SUI - Adaptic: A Shape Changing Prop with Haptic Retargeting
PY  - 2021
AB  - We present Adaptic, a novel ”hybrid” active/passive haptic device that can change shape to act as a proxy for a range of virtual objects in VR. We use Adaptic with haptic retargeting to redirect the user’s hand to provide haptic feedback for several virtual objects in arm’s reach using only a single prop. To evaluate the effectiveness of Adaptic with haptic retargeting, we conducted a within-subjects experiment employing a docking task to compare Adaptic to non-matching proxy objects (i.e., Styrofoam balls) and matching shape props. In our study, Adaptic sat on a desk in front of the user and changed shapes between grasps, to provide matching tactile feedback for various virtual objects placed in different virtual locations. Results indicate that the illusion was convincing: users felt they were manipulating several virtual objects in different virtual locations with a single Adaptic device. Docking performance (completion time and accuracy) with Adaptic was comparable to props without haptic retargeting.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485293
ER  - 

TY  - JOUR
AU  - Bigelow, Charles
TI  - Typeface features and legibility research.
PY  - 2019
AB  - In the early 20th century, reading researchers expressed optimism that scientific study of reading would improve the legibility of typefaces. Font-making was, however, complex, expensive and impractical for reading research, which was therefore restricted to standard commercial fonts. The adoption of computer typography in legibility studies makes the measurement, modification and creation of experimental fonts easier, while display of text on computer screens facilitates reading studies. These technical advances have spurred innovative research. Some studies continue to test fonts for efficient reading in low vision as well as normal vision, while others use novel fonts to investigate visual mechanisms in reading. Some experimental fonts incorporate color and animation features that were impractical or impossible in traditional typography. While it is not clear that such innovations will achieve the optimistic goals of a century ago, they extend the investigation and understanding of the nature of reading.
SP  - 162
EP  - 172
JF  - Vision research
VL  - 165
IS  - NA
PB  - 
DO  - 10.1016/j.visres.2019.05.003
ER  - 

TY  - NA
AU  - Ren, Donghao; Lee, Bongshin; Höllerer, Tobias
TI  - VRST - XRCreator: interactive construction of immersive data-driven stories
PY  - 2018
AB  - Immersive data-driven storytelling, which uses interactive immersive visualizations to present insights from data, is a compelling use case for VR and AR environments. We present XRCreator, an authoring system to create immersive data-driven stories. The cross-platform nature of our React-inspired system architecture enables the collaboration among VR, AR, and web users, both in authoring and in experiencing immersive data-driven stories.
SP  - 136
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3283400
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles; Haliyo, Sinan; Frey, Pascal
TI  - UIST - CoVR: A Large-Scale Force-Feedback Robotic Interface for Non-Deterministic Scenarios in VR
PY  - 2020
AB  - We present CoVR, a novel robotic interface providing strong kinesthetic feedback (100 N) in a room-scale VR arena. It consists of a physical column mounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of (1) resisting to body-scaled users' actions such as pushing or leaning; (2) acting on the users by pulling or transporting them as well as (3) carrying multiple potentially heavy objects (up to 80kg) that users can freely manipulate or make interact with each other. We describe its implementation and define a trajectory generation algorithm based on a novel user intention model to support non-deterministic scenarios, where the users are free to interact with any virtual object of interest with no regards to the scenarios' progress. A technical evaluation and a user study demonstrate the feasibility and usability of CoVR, as well as the relevance of whole-body interactions involving strong forces, such as being pulled through or transported.
SP  - 209
EP  - 222
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415891
ER  - 

TY  - NA
AU  - Olwal, Alex; Moeller, Jon; Priest-Dorman, Greg E.; Starner, Thad; Carroll, Ben
TI  - UIST (Adjunct Volume) - I/O Braid: Scalable Touch-Sensitive Lighted Cords Using Spiraling, Repeating Sensing Textiles and Fiber Optics
PY  - 2018
AB  - We introduce I/O Braid, an interactive textile cord with embedded sensing and visual feedback. I/O Braid senses proximity, touch, and twist through a spiraling, repeating braiding topology of touch matrices. This sensing topology is uniquely scalable, requiring only a few sensing lines to cover the whole length of a cord. The same topology allows us to embed fiber optic strands to integrate co-located visual feedback. We provide an overview of the enabling braiding techniques, design considerations, and approaches to gesture detection. These allow us to derive a set of interaction techniques, which we demonstrate with different form factors and capabilities. Our applications illustrate how I/O Braid can invisibly augment everyday objects, such as touch-sensitive headphones and interactive drawstrings on garments, while enabling discoverability and feedback through embedded light sources.
SP  - 485
EP  - 497
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242638
ER  - 

TY  - JOUR
AU  - Handelzalts, Shirley; Ballardini, Giulia; Avraham, Chen; Pagano, Mattia; Casadio, Maura; Nisky, Ilana
TI  - Integrating Tactile Feedback Technologies Into Home-Based Telerehabilitation: Opportunities and Challenges in Light of COVID-19 Pandemic.
PY  - 2021
AB  - The COVID-19 pandemic has highlighted the need for advancing the development and implementation of novel means for home-based telerehabilitation in order to enable remote assessment and training for individuals with disabling conditions in need of therapy. While somatosensory input is essential for motor function, to date, most telerehabilitation therapies and technologies focus on assessing and training motor impairments, while the somatosensorial aspect is largely neglected. The integration of tactile devices into home-based rehabilitation practice has the potential to enhance the recovery of sensorimotor impairments and to promote functional gains through practice in an enriched environment with augmented tactile feedback and haptic interactions. In the current review, we outline the clinical approaches for stimulating somatosensation in home-based telerehabilitation and review the existing technologies for conveying mechanical tactile feedback (i.e., vibration, stretch, pressure, and mid-air stimulations). We focus on tactile feedback technologies that can be integrated into home-based practice due to their relatively low cost, compact size, and lightweight. The advantages and opportunities, as well as the long-term challenges and gaps with regards to implementing these technologies into home-based telerehabilitation, are discussed.
SP  - 617636
EP  - 617636
JF  - Frontiers in neurorobotics
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnbot.2021.617636
ER  - 

TY  - JOUR
AU  - Wang, Dangxiao; Ohnishi, Kouhei; Xu, Weiliang
TI  - Multimodal Haptic Display for Virtual Reality: A Survey
PY  - 2020
AB  - Human haptic perception system is complex, involving both cutaneous and kinesthetic receptors. These receptors work together and enable human to perceive the external world. To simulate immersive interaction with virtual objects in virtual reality scenarios haptic devices are desired to reproduce multiproperties of virtual objects, support multigestures of human hands to perform fine manipulation, produce haptic stimuli for simultaneously stimulating multireceptors (including cutaneous and kinesthetic receptors) of human haptic channel, and thus invoke realistic compound haptic sensations. In recent years, such multimodal haptic devices have emerged. In this paper, we survey the latest progress on multimodal haptic devices, identify the gaps, and put forward future research directions on the topic.
SP  - 610
EP  - 623
JF  - IEEE Transactions on Industrial Electronics
VL  - 67
IS  - 1
PB  - 
DO  - 10.1109/tie.2019.2920602
ER  - 

TY  - NA
AU  - Zhong, Mingyuan; Li, Gang; Li, Yang
TI  - CHI - Spacewalker: Rapid UI Design Exploration Using Lightweight Markup Enhancement and Crowd Genetic Programming
PY  - 2021
AB  - User interface design is a complex task that involves designers examining a wide range of options. We present Spacewalker, a tool that allows designers to rapidly search a large design space for an optimal web UI with integrated support. Designers first annotate each attribute they want to explore in a typical HTML page, using a simple markup extension we designed. Spacewalker then parses the annotated HTML specification, and intelligently generates and distributes various configurations of the web UI to crowd workers for evaluation. We enhanced a genetic algorithm to accommodate crowd worker responses from pairwise comparison of UI designs, which is crucial for obtaining reliable feedback. Based on our experiments, Spacewalker allows designers to effectively search a large design space of a UI, using the language they are familiar with, and improve their design rapidly at a minimal cost.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445326
ER  - 

TY  - NA
AU  - Kishishita, Yusuke; Das, Swagata; Ramirez, Antonio Vega; Thakur, Chetan; Tadayon, Ramin; Kurita, Yuichi
TI  - VR - Muscleblazer: Force-Feedback Suit for Immersive Experience
PY  - 2019
AB  - The increasing use of virtual reality (VR) and augmented reality (AR) systems has opened the possibility of providing immersive experiences to the general population around the world. However, most of the existing systems do not provide highly effective force-feedback experiences to the user. To provide such augmented systems in combination with force-feedback, novel ideas must be introduced that can be easily integrated with the existing VR and AR technologies. This work proposes a first-person VR game integrated with a soft exoskeleton that enhances the quality of interaction between the subject and the virtual environment (VE) through an additional force-feedback element. The effect of introducing the force-feedback element on the user is analyzed by using biosensors and questionnaire feedback. The biosensors are used to measure the level of anxiety induced in the subject during interaction with the virtual environment. Conditions in which this interaction occurs with and without force-feedback are compared. The questionnaire analyzes the perceived change in emotions of users as a result of the introduction of the force-feedback element. This game can be played by most individuals, regardless of age and physical fitness.
SP  - 1813
EP  - 1818
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797962
ER  - 

TY  - JOUR
AU  - Tan, Jeanne; Shao, Li; Lam, Ngan Yi Kitty; Toomey, Anne; Ge, Lan
TI  - Intelligent textiles : designing a gesture-controlled illuminated textile based on computer vision
PY  - 2021
AB  - Artificial intelligence (AI) offers the potential for the development of e-textiles that give wearers a smart and intuitive experience. An emerging challenge in intelligent materials design is hand...
SP  - 004051752110342
EP  - 3048
JF  - Textile Research Journal
VL  - 92
IS  - 17-18
PB  - 
DO  - 10.1177/00405175211034245
ER  - 

TY  - JOUR
AU  - Oppenlaender, Jonas; Tiropanis, Thanassis; Hosio, Simo
TI  - CrowdUI: Supporting Web Design with the Crowd
PY  - 2020
AB  - Web design is a complex and challenging task. It involves making many design decisions that materialise preconceived notions of user needs that may or may not be true. In this paper, we investigate supporting the co-design of a website with visual feedback elicited from the website's community of users. Website users can express their needs by re-arranging and modifying the website's layout and design. To explore and validate this idea, we present CrowdUI, a web-based tool that enables members of the community of a website to visually express their design improvement ideas, frustrations and needs, and to send this feedback to the person in charge of designing or maintaining the website. CrowdUI is validated in a study with 45 users of a popular social media and networking website. Second, our qualitative evaluation with 60 experienced web developers shows that CrowdUI is able to elicit diverse and meaningful feedback. Put together, our results suggest that CrowdUI's approach constitutes a productive setting for eliciting visual feedback from the user community as a complement to traditional ways of eliciting feedback and participatory design. Finally, based on our experiences, we discuss a design space for crowdsourced web design and provide design recommendations for similar future tools.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - EICS
PB  - 
DO  - 10.1145/3394978
ER  - 

TY  - NA
AU  - Zhang, Zhenliang; Wang, Cong; Weng, Dongdong; Liu, Yue; Wang, Yongtian
TI  - Symmetrical Reality: Toward a Unified Framework for Physical and Virtual Reality
PY  - 2019
AB  - In this paper, we review the background of physical reality, virtual reality, and some traditional mixed forms of them. Based on the current knowledge, we propose a new unified concept called symmetrical reality to describe the physical and virtual world in a unified perspective. Under the framework of symmetrical reality, the traditional virtual reality, augmented reality, inverse virtual reality, and inverse augmented reality can be interpreted using a unified presentation. We analyze the characteristics of symmetrical reality from two different observation locations (i.e., from the physical world and from the virtual world), where all other forms of physical and virtual reality can be treated as special cases of symmetrical reality.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gentile, Vito; Adjorlu, Ali; Serafin, Stefania; Rocchesso, Davide; Sorce, Salvatore
TI  - PerDis - Touch or touchless?: evaluating usability of interactive displays for persons with autistic spectrum disorders
PY  - 2019
AB  - Interactive public displays have been exploited and studied for engaging interaction in several previous studies. In this context, applications have been focused on supporting learning or entertainment activities, specifically designed for people with special needs. This includes, for example, those with Autism Spectrum Disorders (ASD). In this paper, we present a comparison study aimed at understanding the difference in terms of usability, effectiveness, and enjoyment perceived by users with ASD between two interaction modalities usually supported by interactive displays: touch-based and touchless gestural interaction. We present the outcomes of a within-subject setup involving 8 ASD users (age 18-25 y.o., IQ 40-60), based on the use of two similar user interfaces, differing only by the interaction modality. We show that touch interaction provides higher usability level and results in more effective actions, although touchless interaction is more effective in terms of enjoyment and engagement.
SP  - 1
EP  - 7
JF  - Proceedings of the 8th ACM International Symposium on Pervasive Displays
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3321335.3324946
ER  - 

TY  - NA
AU  - Lee, Sang Won
TI  - UIST (Adjunct Volume) - Hybrid Use of Asynchronous and Synchronous Interaction for Collaborative Creation
PY  - 2017
AB  - My dissertation is aimed at enabling people to collaborate to create complex artifacts: for example, to develop software, sketch GUI prototypes, play music together, or write a novel. Such creative processes are not well defined and can evolve dynamically. We introduce interactive systems that help users collaborate and communicate in the open-ended process. In particular, we investigate the benefits of both integrating asynchronous interactions into real-time collaborations and of having real time components in asynchronous collaborative settings. The systems provide tools that combine the two different types of interaction techniques, and we validate them via user study, participatory performing arts, and the online deployments of systems and crowdsourced tasks. The hybrid methods are designed to help users recover collaborative context, make the process approachable to nonexperts, collaborate online crowds on demand in real-time, and sustain liveness during collaboration. The dissertation will result in cross-domain knowledge in designing collaborative systems and it will help us create a framework for future intelligent systems that will help people solve more complex tasks effectively.
SP  - 95
EP  - 98
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131841
ER  - 

TY  - NA
AU  - Mikkonen, Jussi; Townsend, Riikka
TI  - CHI - Frequency-Based Design of Smart Textiles
PY  - 2019
AB  - Despite the increasing amount of smart textile design practitioners, the methods and tools commonly available have not progressed to the same scale. Most smart textile interaction designs today rely on detecting changes in resistance. The tools and sensors for this are generally limited to DC-voltage-divider based sensors and multimeters. Furthermore, the textiles and the materials used in smart textile design can exhibit behaviour making it difficult to identify even simple interactions using those means. For instance, steel-based textiles exhibit intrinsic semiconductive properties that are difficult to identify with current methods. In this paper, we show an alternative way to measure interaction with smart textiles. By relying on visualisation known as Lissajous-figures and frequency-based signals, we can detect even subtle and varied forms of interaction with smart textiles. We also show an approach to measuring frequency-based signals and present an Arduino-based system called Teksig to support this type of textile practice.
SP  - 294
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300524
ER  - 

TY  - NA
AU  - Clergeaud, Damien; Roo, Joan Sol; Hachet, Martin; Guitton, Pascal
TI  - VRST - Towards seamless interaction between physical and virtual locations for asymmetric collaboration
PY  - 2017
AB  - Virtual Reality allows rapid prototyping and simulation of physical artefacts, which would be difficult and expensive to perform otherwise. On the other hand, when the design process is complex and involves multiple stakeholders, decisions are taken in meetings hosted in the physical world. In the case of aerospace industrial designs, the process is accelerated by having asymmetric collaboration between the two locations: experts discuss the possibilities in a meeting room while a technician immersed in VR tests the selected alternatives. According to experts, the current approach is not without limitations, and in this work, we present prototypes designed to tackle them. The described artefacts were created to address the main issues: awareness of the remote location, remote interaction and manipulation, and navigation between locations. First feedback from experts regarding the prototypes is also presented. The resulting design considerations can be used in other asymmetric collaborative scenarios.
SP  - 17
EP  - NA
JF  - Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3139131.3139165
ER  - 

TY  - NA
AU  - Ichikawa, Shotaro; Endo, Isamu; Onishi, Yuki; Suzuki, Aoi; Hayashi, Daigo; Niwano, Anri; Ebi, Akiyuki; Kitamura, Yoshifumi
TI  - VR - Be Bait!: A Unique Fishing Experience with Hammock-based Underwater Locomotion Method
PY  - 2019
AB  - We present “Be Bait!”, a unique virtual fishing experience that allows the user to become a bait by lying on the hammocks, instead of holding a fishing rod. We implement the hammock-based locomotion method with haptic feedback mechanisms. In our demonstration, users can enjoy exploring a virtual underwater world and fighting with fishes in direct and intuitive ways.
SP  - 1315
EP  - 1316
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798243
ER  - 

TY  - NA
AU  - Choi, Inrak; Ofek, Eyal; Benko, Hrvoje; Sinclair, Mike; Holz, Christian
TI  - CHI - CLAW: A Multifunctional Handheld Haptic Controller for Grasping, Touching, and Triggering in Virtual Reality
PY  - 2018
AB  - CLAW is a handheld virtual reality controller that augments the typical controller functionality with force feedback and actuated movement to the index finger. Our controller enables three distinct interactions (grasping virtual object, touching virtual surfaces, and triggering) and changes its corresponding haptic rendering by sensing the differences in the user's grasp. A servo motor coupled with a force sensor renders controllable forces to the index finger during grasping and touching. Using position tracking, a voice coil actuator at the index fingertip generates vibrations for various textures synchronized with finger movement. CLAW also supports a haptic force feedback in the trigger mode when the user holds a gun. We describe the design considerations for CLAW and evaluate its performance through two user studies. The first study obtained qualitative user feedback on the naturalness, effectiveness, and comfort when using the device. The second study investigated the ease of the transition between grasping and touching when using our device.
SP  - 654
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174228
ER  - 

TY  - JOUR
AU  - Tyler, David; Wood, Jane; Sabir, Tasneem; McDonnell, Chloe; Abu Sadat Muhammad Sayem, NA; Whittaker, Nick
TI  - Wearable electronic textiles
PY  - 2019
AB  - Whilst the bulk of products classified as wearable technologies are watch-like bands that are worn on arms and legs, there is growing interest not only in garments that incorporate sensors and actu...
SP  - 299
EP  - 384
JF  - Textile Progress
VL  - 51
IS  - 4
PB  - 
DO  - 10.1080/00405167.2020.1840151
ER  - 

TY  - NA
AU  - Han, Ping-Hsuan; Chen, Yang-Sheng; Lee, Kong-Chang; Wang, Hao-Cheng; Hsieh, Chiao-En; Hsiao, Jui-Chun; Chou, Chien-Hsing; Hung, Yi-Ping
TI  - VRST - Haptic around: multiple tactile sensations for immersive environment and interaction in virtual reality
PY  - 2018
AB  - In this paper, we present Haptic Around, a hybrid-haptic feedback system, which utilizes fan, hot air blower, mist creator and heat light to recreate multiple tactile sensations in virtual reality for enhancing the immersive environment and interaction. This system consists of a steerable haptic device rigged on the top of the user head and a handheld device also with haptics feedbacks to simultaneously provide tactile sensations to the users in a 2m x 2m space. The steerable haptic device can enhance the immersive environment for providing full body experience, such as heat in the desert or cold in the snow mountain. Additionally, the handheld device can enhance the immersive interaction for providing partial body experience, such as heating the iron or quenching the hot iron. With our system, the users can perceive visual, auditory and haptic when they are moving around in virtual space and interacting with virtual object. In our study, the result has shown the potential of the hybrid-haptic feedback system, which the participants rated the enjoyment, realism, quality, immersion higher than the other.
SP  - 35
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281507
ER  - 

TY  - JOUR
AU  - Pramudwiatmoko, Arif; Gutmann, Gregory; Ueno, Yutaka; Kakugo, Akira; Yamamura, Masayuki; Konagaya, Akihiko
TI  - Tensegrity representation of microtubule objects using unified particle objects and springs
PY  - 2020
AB  - NA
SP  - 19
EP  - 43
JF  - Chem-Bio Informatics Journal
VL  - 20
IS  - 0
PB  - 
DO  - 10.1273/cbij.20.19
ER  - 

TY  - JOUR
AU  - Horst, Robin; Klonowski, Fabio; Rau, Linda; Dörner, Ralf
TI  - The Shared View Paradigm in Asymmetric Virtual Reality Setups
PY  - 2020
AB  - <jats:title>Abstract</jats:title> <jats:p>Asymmetric Virtual Reality (VR) applications are a substantial subclass of multi-user VR that offers not all participants the same interaction possibilities with the virtual scene. While one user might be immersed using a VR head-mounted display (HMD), another user might experience the VR through a common desktop PC. In an educational scenario, for example, learners can use immersive VR technology to inform themselves at different exhibits within a virtual scene. Educators can use a desktop PC setup for following and guiding learners through virtual exhibits and still being able to pay attention to safety aspects in the real world (e. g., avoid learners bumping against a wall). In such scenarios, educators must ensure that learners have explored the entire scene and have been informed about all virtual exhibits in it. According visualization techniques can support educators and facilitate conducting such VR-enhanced lessons. One common technique is to render the view of the learners on the 2D screen available to the educators. We refer to this solution as the <jats:italic>shared view paradigm</jats:italic>. However, this straightforward visualization involves challenges. For example, educators have no control over the scene and the collaboration of the learning scenario can be tedious. In this paper, we differentiate between two classes of visualizations that can help educators in asymmetric VR setups. First, we investigate five techniques that visualize the view direction or field of view of users (<jats:italic>view visualizations</jats:italic>) within virtual environments. Second, we propose three techniques that can support educators to understand what parts of the scene learners already have explored (<jats:italic>exploration visualization</jats:italic>). In a user study, we show that our participants preferred a volume-based rendering and a view-in-view overlay solution for view visualizations. Furthermore, we show that our participants tended to use combinations of different view visualizations.</jats:p>
SP  - 87
EP  - 101
JF  - i-com
VL  - 19
IS  - 2
PB  - 
DO  - 10.1515/icom-2020-0006
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Wu, K. D.; Chen, Jacqueline; Lopes, Pedro
TI  - Prolonging VR Haptic Experiences by Harvesting Kinetic Energy from the User
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545635
ER  - 

TY  - NA
AU  - Daiber, Florian; Degraen, Donald; Zenner, André; Döring, Tanja; Steinicke, Frank; Nunez, Oscar Javier Ariza; Simeone, Adalberto L.
TI  - CHI Extended Abstracts - Everyday Proxy Objects for Virtual Reality
PY  - 2020
AB  - Immersive virtual experiences are becoming ubiquitous in our daily lives. Besides visual and auditory feedback, other senses like haptics, smell and taste can enhance immersion in virtual environments. Most solutions presented in the past require specialized hardware to provide appropriate feedback. To mitigate this need, researchers conceptualized approaches leveraging everyday physical objects as proxies instead. Transferring these approaches to varying physical environments and conditions, however, poses significant challenges to a variety of disciplines such as HCI, VR, haptics, tracking, perceptual science, design, etc. This workshop will explore the integration of everyday items for multi-sensory feedback in virtual experiences and sets course for respective future research endeavors. Since the community still seems to lack a cohesive agenda for advancing this domain, the goal of this workshop is to bring together individuals interested in everyday proxy objects to review past work, build a unifying research agenda, share ongoing work, and encourage collaboration.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375165
ER  - 

TY  - NA
AU  - Preechayasomboon, Pornthep; Israr, Ali; Samad, Majed
TI  - CHI - Chasm: A Screw Based Expressive Compact Haptic Actuator
PY  - 2020
AB  - We present a compact broadband linear actuator, Chasm, that renders expressive haptic feedback on wearable and handheld devices. Unlike typical motor-based haptic devices with integrated gearheads, Chasm utilizes a miniature leadscrew coupled to a motor shaft, thereby directly translating the high-speed rotation of the motor to the linear motion of a nut carriage without an additional transmission. Due to this simplicity, Chasm can render low-frequency skin-stretch and high-frequency vibrations, simultaneously and independently. We present the design of the actuator assembly and validate its electromechanical and perceptual performance. We then explore use cases and show design solutions for embedding Chasm in device prototypes. Finally, we report investigations with Chasm in two VR embodiments, i.e., in a headgear band to induce locomotion cues and in a handheld pointer to enhance dynamic manual interactions. Our explorations show wide use for Chasm in enhancing user interactions and experience in virtual and augmented settings.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376512
ER  - 

TY  - NA
AU  - Chen, Yan
TI  - VL/HCC - Mocking-up Desired UI Behaviors from UI Element-Based Recording
PY  - 2019
AB  - Software developers often ask for support from other developers, but effective communication about programming problems can be challenging. In the context of user interface (UI) development, effective communication about interactive behaviors of a UI is particularly difficult as it often requires a visual demonstration of the UI behaviors as supporting context. My motivational study found that our participants can always correctly understand a request when it includes video demos of the problem UI behavior and desired UI behavior. I summarized that an ideal request regarding a UI interactive behavior problem should include interrelated natural language description, relevant code, and demonstrations of non-desired and desired UI behaviors. I observed that developers often provide only the demonstration of not desired UI behavior and then describing the desired behavior on top of it. Unable to provide desired UI behaviors makes communication about UI behavior ineffective, and I argue this is a limitation of existing techniques. In this work, I would like to propose a solution to address it.
SP  - 231
EP  - 232
JF  - 2019 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2019.8818827
ER  - 

TY  - NA
AU  - Oulasvirta, Antti; Kim, Sunjun; Lee, Byungjoo
TI  - CHI - Neuromechanics of a Button Press
PY  - 2018
AB  - To press a button, a finger must push down and pull up with the right force and timing. How the motor system succeeds in button-pressing, in spite of neural noise and lacking direct access to the mechanism of the button, is poorly understood. This paper investigates a unifying account based on neuromechanics. Mechanics is used to model muscles controlling the finger that contacts the button. Neurocognitive principles are used to model how the motor system learns appropriate muscle activations over repeated strokes though relying on degraded sensory feedback. Neuromechanical simulations yield a rich set of predictions for kinematics, dynamics, and user performance and may aid in understanding and improving input devices. We present a computational implementation and evaluate predictions for common button types.
SP  - 508
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174082
ER  - 

TY  - NA
AU  - Büschel, Wolfgang; Mitschick, Annett; Meyer, Thomas; Dachselt, Raimund
TI  - MobileHCI - Investigating Smartphone-based Pan and Zoom in 3D Data Spaces in Augmented Reality
PY  - 2019
AB  - In this paper, we investigate mobile devices as interactive controllers to support the exploration of 3D data spaces in head-mounted Augmented Reality (AR). In future mobile contexts, applications such as immersive analysis or ubiquitous information retrieval will involve large 3D data sets, which must be visualized in limited physical space. This necessitates efficient interaction techniques for 3D panning and zooming. Smartphones as additional input devices are promising because they are familiar and widely available in mobile usage contexts. They also allow more casual and discreet interaction compared to free-hand gestures or voice input. We introduce smartphone-based pan & zoom techniques for 3D data spaces and present a user study comparing five techniques. Our results show that spatial device gestures can outperform both touch-based techniques and hand gestures in terms of task completion times and user preference. We discuss our findings in detail and suggest suitable techniques for specific AR navigation tasks.
SP  - 2
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340113
ER  - 

TY  - JOUR
AU  - Tanabe, Takeshi; Endo, Hiroshi; Ino, Shuichi
TI  - Effects of Asymmetric Vibration Frequency on Pulling Illusions.
PY  - 2020
AB  - It is known that humans experience a haptic illusion, such as the sensation of being pulled in a particular direction, when asymmetric vibrations are presented. A pulling illusion has been used to provide a force feedback for a virtual reality (VR) system and a pedestrian navigation system, and the asymmetric vibrations can be implemented in any small non-grounded device. However, the design methodology of asymmetric vibration stimuli to induce the pulling illusion has not been fully demonstrated. Although the frequency of the asymmetric vibration is important, findings on the frequency have not been reported. In this study, we clarified the influences of the effects on the pulling illusion based on the investigation of asymmetric vibration frequency differences. Two psychophysical experiments that related to the frequency of asymmetric vibration were performed. Experiment I showed that the illusion occurs for specific vibration waveforms at 40 Hz and 75 Hz. As a result of Experiment II, the threshold was the lowest when the frequency was 40 Hz, and highest when the frequency was 110 Hz. This result supports the previous hypothesis that the Meissner corpuscles and the Ruffini endings contribute to the illusion, while the Pacinian corpuscles do not.
SP  - 7086
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 24
PB  - 
DO  - 10.3390/s20247086
ER  - 

TY  - NA
AU  - Roo, Joan Sol; Basset, Jean; Cinquin, Pierre-Antoine; Hachet, Martin
TI  - CHI - Understanding Users' Capability to Transfer Information between Mixed and Virtual Reality: Position Estimation across Modalities and Perspectives
PY  - 2018
AB  - Mixed Reality systems combine physical and digital worlds, with great potential for the future of HCI. It is possible to design systems that support flexible degrees of virtuality by combining complementary technologies. In order for such systems to succeed, users must be able to create unified mental models out of heterogeneous representations. In this paper, we present two studies focusing on the users' accuracy on heterogeneous systems using Spatial Augmented Reality (SAR) and immersive Virtual Reality (VR) displays, and combining viewpoints (egocentric and exocentric). The results show robust estimation capabilities across conditions and viewpoints.
SP  - 363
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173937
ER  - 

TY  - NA
AU  - Mäkelä, Ville; Winter, Jonas; Schwab, Jasmin; Koch, Michael; Alt, Florian
TI  - Pandemic Displays: Considering Hygiene on Public Touchscreens in the Post-Pandemic Era
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501937
ER  - 

TY  - JOUR
AU  - Jurda, Mikoláš; Urbanová, Petra; Chmelík, Jiří
TI  - Digital restoration of fragmentary human skeletal remains: Testing the feasibility of virtual reality.
PY  - 2019
AB  - Experts in forensic anthropology and medicine have become gradually accustomed to examining components of the human body in the virtual workspace. While the computer-assisted approach offers numerous benefits, the interactions with digital three-dimensional biological objects are often problematic, particularly if conducted with mouse, keyboard and flat-panel screen. The study focusses on feasibility of a virtual reality (VR) system for virtual restoration of fragmentary skeletal remains. The VR system was confronted with three cases of fragmentary remains. The cases were reassembled manually by twenty participants using a HTC Vive headset combined with an in-house application A.R.T. The same task was performed using a CloudCompare software in conjunction with a desktop peripheral. The two systems were compared in terms of time efficiency, the geometric properties of the resulting restorations, and convenience of use. Restoration using the VR system took approximately half the time the desktop set-up did. The VR system also yielded a lower error rate when a severely fragmented skull was reassembled. Ultimately, although the efficiency of the reassembling was shown to be strongly dependent on the operator's experience, the use of the VR system balanced out the uneven levels of proficiency in computer graphics. The current generation of virtual reality headsets has a strong potential to facilitate and improve tasks relating to the virtual restoration of fragmented skeletal remains. A VR system offers an intuitive digital working environment which is less affected by an operator's computer skills and practical understanding of the technology than the desktop systems are.
SP  - 50
EP  - 57
JF  - Journal of forensic and legal medicine
VL  - 66
IS  - NA
PB  - 
DO  - 10.1016/j.jflm.2019.06.005
ER  - 

TY  - NA
AU  - Kwok, Tiffany C.K.; Kiefer, Peter; Schinazi, Victor R.; Adams, Benjamin; Raubal, Martin
TI  - CHI - Gaze-Guided Narratives: Adapting Audio Guide Content to Gaze in Virtual and Real Environments
PY  - 2019
AB  - Exploring a city panorama from a vantage point is a popular tourist activity. Typical audio guides that support this activity are limited by their lack of responsiveness to user behavior and by the difficulty of matching audio descriptions to the panorama. These limitations can inhibit the acquisition of information and negatively affect user experience. This paper proposes Gaze-Guided Narratives as a novel interaction concept that helps tourists find specific features in the panorama (gaze guidance) while adapting the audio content to what has been previously looked at (content adaptation). Results from a controlled study in a virtual environment (n=60) revealed that a system featuring both gaze guidance and content adaptation obtained better user experience, lower cognitive load, and led to better performance in a mapping task compared to a classic audio guide. A second study with tourists situated at a vantage point (n=16) further demonstrated the feasibility of this approach in the real world.
SP  - 491
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300721
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Tan, Lu; Zhang, Yuji; Seyed, Teddy; Yang, Xing-Dong
TI  - UIST - Capacitivo: Contact-Based Object Recognition on Interactive Fabrics using Capacitive Sensing
PY  - 2020
AB  - We present Capacitivo, a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace. To demonstrate our technique, we created a prototype composed of a 12 x 12 grid of electrodes, made from conductive fabric attached to a textile substrate. We designed the size and separation between the electrodes to maximize the sensing area and sensitivity. We then used a 10-person study to evaluate the performance of our sensing technique using 20 different objects, which yielded a 94.5% accuracy rate. We conclude this work by presenting several different application scenarios to demonstrate unique interactions that are enabled by our technique on fabrics.
SP  - 649
EP  - 661
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415829
ER  - 

TY  - NA
AU  - Koskinen, Olli; Rakkolainen, Ismo; Raisamo, Roope
TI  - VRST - Gigapixel virtual reality employing live superzoom cameras
PY  - 2018
AB  - We present a live gigapixel virtual reality system employing a 360° camera, a superzoom camera with a pan-tilt robotic head, and a head-mounted display (HMD). The system is capable of showing on-demand gigapixel-level subregions of 360° videos. Similar systems could be used to have live feed for foveated rendering HMDs.
SP  - 92
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281586
ER  - 

TY  - NA
AU  - Xu, Xuhai; Shi, Haitian; Yi, Xin; Liu, WenJia; Yan, Yukang; Shi, Yuanchun; Mariakakis, Alex; Mankoff, Jennifer; Dey, Anind K.
TI  - CHI - EarBuddy: Enabling On-Face Interaction via Wireless Earbuds
PY  - 2020
AB  - Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376836
ER  - 

TY  - NA
AU  - Cai, Shaoyu; Ke, Pingchuan; Narumi, Takuji; Zhu, Kening
TI  - VR - ThermAirGlove: A Pneumatic Glove for Thermal Perception and Material Identification in Virtual Reality
PY  - 2020
AB  - We present ThermAirGlove (TAGlove), a pneumatic glove which provides thermal feedback for users, to support the haptic experience of grabbing objects of different temperatures and materials in virtual reality (VR). The system consists of a glove with five inflatable airbags on the fingers and the palm, two temperature chambers (one hot and one cold), and the closed-loop pneumatic thermal control system. Our technical experiments showed that the highest temperature-changing speed of TAGlove system was 2.75°C/s for cooling, and the pneumatic-control mechanism could generate the thermal cues of different materials (e.g., foam, glass, copper, etc.). The user-perception experiments showed that the TAGlove system could provide five distinct levels of thermal sensation (ranging from very cool to very warm). The user-perception experiments also showed that the TAGlove could support users’ material identification among foam, glass, and copper with the average accuracy of 87.2%, with no significant difference compared to perceiving the real physical objects. The user studies on VR experience showed that using TAGlove in immersive VR could significantly improve users’ experience of presence compared to the situations without any temperature or material simulation.
SP  - 248
EP  - 257
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1580801081068
ER  - 

TY  - NA
AU  - Wang, Fen; Zhou, Xian
TI  - Research on Cognitive APP Evaluation of Visually Impaired Users
PY  - 2020
AB  - In the evaluation process of visually impaired users' cognitive APP, the subjective and objective combination weighting method based on the analytic hierarchy process (AHP) and the index importance correlation method (CRITIC) is adopted to comprehensively consider the evaluation system from the two dimensions of subjective and objective The weight of each index forms a more optimized combined weighting evaluation method; then a cognitive evaluation method combining Vague fuzzy set theory and TOPSIS is adopted, which uses Vague sets to describe qualitative indexes of language variables The values are converted to Vague values, and TOPSIS is used to rank the decision-making schemes. Finally, a reasonable evaluation result is obtained, which effectively reduces the influence of expert subjectivity on the weighting of index weights. Finally, the proposed evaluation method is analyzed and applied to verify the feasibility and effectiveness of the evaluation system.
SP  - NA
EP  - NA
JF  - 2020 International Conference on Computer Network, Electronic and Automation (ICCNEA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccnea50255.2020.00036
ER  - 

TY  - NA
AU  - Lin, Chuan-en; Cheng, Ta Ying; Ma, Xiaojuan
TI  - CHI - ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop
PY  - 2020
AB  - Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376614
ER  - 

TY  - JOUR
AU  - Jokinen, Jussi P. P.; Silvennoinen, Johanna; Kujala, Tuomo
TI  - Relating Experience Goals With Visual User Interface Design
PY  - 2018
AB  - This article examines the cognitive process of visually experiencing user interfaces. It contributes to a theory- A nd methodology-grounded understanding of how UIs are experienced with regard to various aesthetic criteria. This aids in considering the targeted experience goals in relation to visual design choices-a problem that designers usually have to tackle intuitively. The issue in explicitly relating designs to experiences stems from the complexity of the process in which visual stimuli are processed and turned into experiences. The authors present a cognitive top-down approach to this process, rooted in the appraisal theory and the theory of the predictive brain. Several predictions are derived via this approach, and an eye-tracking experiment with Web sites is presented that provides evidence of them. The experience goals and repeated exposure to stimuli are shown to affect appraisal times and visual scanpaths in Web pages' evaluation; this supports the top-down approach described. Researchers can usethe findings to inform their theoretical and empirical pursuits as they strive to understand what makes design artefacts emotionally evocative, and the methodology outlined can assist designers in locating the visual regions and elements relevant for experiential design goals.Peer reviewe
SP  - 378
EP  - 395
JF  - Interacting with Computers
VL  - 30
IS  - 5
PB  - 
DO  - 10.1093/iwc/iwy016
ER  - 

TY  - JOUR
AU  - Stellmacher, Carolin; Bonfert, Michael; Kruijff, Ernst; Schöning, Johannes
TI  - Triggermuscle: Exploring Weight Perception for Virtual Reality Through Adaptive Trigger Resistance in a Haptic VR Controller
PY  - 2022
AB  - <jats:p>It is challenging to provide users with a haptic weight sensation of virtual objects in VR since current consumer VR controllers and software-based approaches such as pseudo-haptics cannot render appropriate haptic stimuli. To overcome these limitations, we developed a haptic VR controller named <jats:italic>Triggermuscle</jats:italic> that adjusts its trigger resistance according to the weight of a virtual object. Therefore, users need to adapt their index finger force to grab objects of different virtual weights. Dynamic and continuous adjustment is enabled by a spring mechanism inside the casing of an HTC Vive controller. In two user studies, we explored the effect on weight perception and found large differences between participants for sensing change in trigger resistance and thus for discriminating virtual weights. The variations were easily distinguished and associated with weight by some participants while others did not notice them at all. We discuss possible limitations, confounding factors, how to overcome them in future research and the pros and cons of this novel technology.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 2
IS  - NA
PB  - 
DO  - 10.3389/frvir.2021.754511
ER  - 

TY  - NA
AU  - Oppenlaender, Jonas; Kuosmanen, Elina; Lucero, Andrés; Hosio, Simo
TI  - Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer Design Feedback in the Classroom
PY  - 2021
AB  - Feedback is an important aspect of design education, and crowdsourcing has emerged as a convenient way to obtain feedback at scale. In this paper, we investigate how crowdsourced design feedback compares to peer design feedback within a design-oriented HCI class and across two metrics: perceived quality and perceived fairness. We also examine the perceived monetary value of crowdsourced feedback, which provides an interesting contrast to the typical requester-centric view of the value of labor on crowdsourcing platforms. Our results reveal that the students (N=106) perceived the crowdsourced design feedback as inferior to peer design feedback in multiple ways. However, they also identified various positive aspects of the online crowds that peers cannot provide. We discuss the meaning of the findings and provide suggestions for teachers in HCI and other researchers interested in crowd feedback systems on using crowds as a potential complement to peers.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445380
ER  - 

TY  - JOUR
AU  - Hung, Ching-Wen; Tsai, Hsin-Ruey; Su, Chi-Chun; Chiu, Jui-Cheng; Chen, Bing-Yu
TI  - OsciHead
PY  - 2022
AB  - <jats:p>Current haptic devices are usually designed to provide one type of force feedback; however, most VR scenarios require versatile force feedback, which may require the integration of different devices to provide various types of forces. In addition, besides the main haptic effects caused by the forces, multiple types of oscillation may also commonly accompany them, which are crucial for improving VR realism and immersion. Therefore, we simulate versatile force feedback by rendering the corresponding types of oscillation as the effects caused by those forces. We take inertia and impact forces as examples in this paper, and achieve versatility using the proposed device, OsciHead, on a head-mounted display (HMD), instead of integrating different devices. By controlling elastic bands' elasticity and stored power, OsciHead uses two rotatable oscillators on both sides of the HMD, in order to render various multilevel and multidimensional oscillation feedback in 2D translation and 2D rotation directions on a head. In an exploratory study, we explored different scenarios in which multiple types of oscillation could be simulated by OsciHead. We then observed oscillation level distinguishability in two just-noticeable difference (JND) studies, and evaluated the oscillation type recognition rates in a recognition study. Based on the results, we performed a VR study, which verified that the inertia and impact feedback simulated by OsciHead enhances realism and achieves versatility.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546715
ER  - 

TY  - JOUR
AU  - Binbeshr, Farid; Kiah, Laiha Mat; Por, Lip Yee; Zaidan, A. A.
TI  - A systematic review of PIN-entry methods resistant to shoulder-surfing attacks
PY  - 2021
AB  - NA
SP  - 102116
EP  - NA
JF  - Computers & Security
VL  - 101
IS  - NA
PB  - 
DO  - 10.1016/j.cose.2020.102116
ER  - 

TY  - NA
AU  - Harley, Daniel; Verni, Alexander; Willis, Mackenzie; Ng, Ashley; Bozzo, Lucas; Mazalek, Ali
TI  - Tangible and Embedded Interaction - Sensory VR: Smelling, Touching, and Eating Virtual Reality
PY  - 2018
AB  - We present two proof of concept sensory experiences designed for virtual reality (VR). Our experiences bring together smell, sound, taste, touch, and sight, focusing on low-cost, non-digital materials and on passive interactions. We also contribute a design rationale and a review of sensory interactions, particularly those designed for VR. We argue that current sensory experiences designed for VR often lack a broader consideration of the senses, especially in their neglect of the non-digital. We discuss some implications of non-digital design for sensory VR, suggesting that there may be opportunities to expand conceptions of what sensory design in VR can be.
SP  - 386
EP  - 397
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173241
ER  - 

TY  - NA
AU  - Newell, Anthony; George, Abraham; Papakostas, Nikolaos; Lhachemi, Hugo; Malik, Ammar; Shorten, Robert
TI  - ICE/ITMC - On Design for Additive Manufacturing: Review of Challenges and Opportunities utilising Visualisation Technologies
PY  - 2019
AB  - Design for additive manufacturing poses new challenges and opportunities for manufacturers to produce highly customised parts while reducing cost, production time and improving quality. Manufacturing constraints of conventional manufacturing methods, such as geometric complexity limitations and workpiece handling, have shaped the landscape of computer-aided design tools, which are therefore not suitably adapted to design for additive manufacturing. Furthermore, computer-aided design tools require a high level of training to produce appropriate models. Augmented reality and feedback technologies pose an interesting opportunity for design for additive manufacturing, whereby the interaction with 3D models in an augmented or virtual design space can provide intuitive feedback to engineers and designers, providing fast validation of designs, parametric modelling and opportunities for training and use in both professional and amateur designer communities. This paper will explore and review the opportunities this exciting new technology provides.
SP  - 1
EP  - 7
JF  - 2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ice.2019.8792569
ER  - 

TY  - BOOK
AU  - Norberto, Marcus; Gaedicke, Lukas; Bernardino, Maicon; Legramante, Guilherme; Basso, Fábio Paulo; Rodrigues, NA
TI  - SBQS - Performance Testing in Mobile Application: a Systematic Literature Map
PY  - 2019
AB  - Context: The technological evolution of wireless networks and the technological advancement of mobile devices make them ever more present in daily life, becoming almost indispensable solutions. With the popularization of mobile devices, developers need to be committed to building applications that can be reliable, robust, secure and that ensure adequate performance for their end users. A good practice to ensure the performance of mobile applications is through a performance testing approach. Although the literature can be used by specialists and non-specialists for decision-making and selection approaches for performance testing, it is limited in the sense of providing an overview. Goal: Our main objective is to contribute to the performance testing body of knowledge. Method: A protocol was formulated and executed according to the guidelines for performing systematic literature mappings in Software Engineering. Results: This study identifies, through a systematic mapping, the tools, strategies, approaches, methods and processes of performance testing in mobile applications. Providing answers and filling a research gap identified in the literature. Conclusions: It is worth highlighting the results on rating performance metrics and problems reported on performance testing for mobile applications. Therefore, this systematic literature map is a valuable contribution to making decisions about performance testing strategies for mobile applications.
SP  - 99
EP  - 108
JF  - Proceedings of the XVIII Brazilian Symposium on Software Quality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3364641.3364653
ER  - 

TY  - NA
AU  - Feick, Martin; Kleer, Niko; Zenner, André; Tang, Anthony; Krüger, Antonio
TI  - CHI - Visuo-haptic Illusions for Linear Translation and Stretching using Physical Proxies in Virtual Reality
PY  - 2021
AB  - Providing haptic feedback when manipulating virtual objects is an essential part of immersive virtual reality experiences; however, it is challenging to replicate all of an object's properties and characteristics. We propose the use of visuo-haptic illusions alongside physical proxies to enhance the scope of proxy-based interactions with virtual objects. In this work, we focus on two manipulation techniques, linear translation and stretching across different distances, and investigate how much discrepancy between the physical proxy and the virtual object may be introduced without participants noticing. In a study with 24 participants, we found that manipulation technique and travel distance significantly affect the detection thresholds, and that visuo-haptic illusions impact performance and accuracy. We show that this technique can be used to enable functional proxy objects that act as stand-ins for multiple virtual objects, illustrating the technique through a showcase VR-DJ application.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445456
ER  - 

TY  - NA
AU  - Gong, Hebo; Cui, Zhitong; Wang, Yanan; Shen, Chengyi; Zhang, Deyin; Luo, Shijian
TI  - CHI Extended Abstracts - eGlove: Designing Interactive Fabric Sensor for Enhancing Contact-Based Interactions
PY  - 2021
AB  - We present eGlove, a wearable and low-cost fabric sensor for recognizing a rich context of objects by touching them, including daily necessities, fruits, plants, as well as different body parts. Our sensing approach utilizes Swept frequency Capacitive Sensing (SFCS) to provide consistent sensor readings even when the fabric electrode is under varying deformation and stretching degrees. Our work proposes an easy fabrication method and hardware configuration for prototyping the interactive fabric sensor. We evaluated our system’s classification accuracy through per-user training and found a real-time classification of 96.3%. We also demonstrated novel contextual interactions enabled by our technical approach with several applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451824
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Hedayati, Hooman; Zheng, Clement; Bohn, James L.; Szafir, Daniel; Yi-Luen, Ellen; Gross, Mark D.; Leithinger, Daniel
TI  - CHI - RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots
PY  - 2020
AB  - RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376523
ER  - 

TY  - NA
AU  - Zhang, Zhenliang; Weng, Dongdong; Jiang, Haiyan; Liu, Yue; Wang, Yongtian
TI  - ISMAR Adjunct - Inverse Augmented Reality: A Virtual Agent's Perspective
PY  - 2018
AB  - We propose a framework called inverse augmented reality (IAR) which describes the scenario that a virtual agent living in the virtual world can observe both virtual objects and real objects. This is different from the traditional augmented reality. The traditional virtual reality, mixed reality and augmented reality are all generated for humans, i.e., they are human-centered frameworks. On the contrary, the proposed inverse augmented reality is a virtual agent-centered framework, which represents and analyzes the reality from a virtual agent's perspective. In this paper, we elaborate the framework of inverse augmented reality to argue the equivalence of the virtual world and the physical world regarding the whole physical structure.
SP  - 154
EP  - 157
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2018.00056
ER  - 

TY  - NA
AU  - Laviole, Jeremy; Thévin, Lauren; Albouys-Perrois, Jérémy; Brock, Anke
TI  - VRIC - Nectar: Multi-user Spatial Augmented Reality for everyone: Three live demonstrations of educative applications
PY  - 2018
AB  - In this demonstration we showcase a new spatial augmented reality device (interactive projection) with three applications: education and experimentation of color models, map exploration for visually impaired people and scientific vulgarization of machine learning.The first exhibition is an interactive exploration about the nature of light. Visitors can experiment with additive subtractive color models. We engage them with questions, and they have to reply using cards to find out answers. This exhibit is suitable for children.The second exhibition is about map exploration and creation for Visually Impaired Persons (VIP). VIP generally use tactile maps with braille to learn about an unknown environment. However, these maps are not accessible to the 80% of VIP who don't read braille. Our prototype augments raised-line maps with audio output.The third exhibition is destined to be used for scientific outreach. It enables the creation of artificial neural networks (ANN) using tangible interfaces. Neurons are represented by laser-cut diamond shaped tokens, and the data to learn is printed on cards. The ANN learns to differentiate shapes, and the whole learning process is made visible and interactive.These three applications demonstrate the capabilities of our hardware and software development kit in different scenarios. At ReVo, each demonstration will have its own setup and interactive space.
SP  - NA
EP  - NA
JF  - Proceedings of the Virtual Reality International Conference - Laval Virtual
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3234253.3234317
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Oechsner, Carl; Alt, Florian; Bulling, Andreas
TI  - AVI - VRpursuits: interaction in virtual reality using smooth pursuit eye movements
PY  - 2018
AB  - Gaze-based interaction using smooth pursuit eye movements (Pursuits) is attractive given that it is intuitive and overcomes the Midas touch problem. At the same time, eye tracking is becoming increasingly popular for VR applications. While Pursuits was shown to be effective in several interaction contexts, it was never explored in-depth for VR before. In a user study (N=26), we investigated how parameters that are specific to VR settings influence the performance of Pursuits. For example, we found that Pursuits is robust against different sizes of virtual 3D targets. However performance improves when the trajectory size (e.g., radius) is larger, particularly if the user is walking while interacting. While walking, selecting moving targets via Pursuits is generally feasible albeit less accurate than when stationary. Finally, we discuss the implications of these findings and the potential of smooth pursuits for interaction in VR by demonstrating two sample use cases: 1) gaze-based authentication in VR, and 2) a space meteors shooting game.
SP  - 18
EP  - NA
JF  - Proceedings of the 2018 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3206505.3206522
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Schuster, Nora; George, Ceenu; Pfeiffer, Max
TI  - VRST - ElectroCutscenes: Realistic Haptic Feedback in Cutscenes of Virtual Reality Games Using Electric Muscle Stimulation
PY  - 2019
AB  - Cutscenes in Virtual Reality (VR) games enhance story telling by delivering output in the form of visual, auditory, or haptic feedback (e.g., using vibrating handheld controllers). Since they lack interaction in the form of user input, cutscenes would significantly benefit from improved feedback. We introduce the concept and implementation of ElectroCutscenes, where Electric Muscle Stimulation (EMS) is leveraged to elicit physical user movements to different body parts to correspond to those of personal avatars in cutscenes of VR games while the user stays passive. Through a user study (N=22) in which users passively received kinesthetic feedback resulting in involuntarily movements, we show that ElectroCutscenes significantly increases perceived presence and realism compared to controller-based vibrotactile and no haptic feedback. Furthermore, we found preliminary evidence that combining visual and EMS feedback can evoke movements that are not actuated by either of them alone. We discuss how to enhance realism and presence of cutscenes in VR games even when EMS can partially rather than completely actuate the desired body movements.
SP  - NA
EP  - NA
JF  - 25th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359996.3364250
ER  - 

TY  - NA
AU  - Wu, Tony; Fukuhara, Shiho; Gillian, Nicholas; Sundara-Rajan, Kishore; Poupyrev, Ivan
TI  - UIST - ZebraSense: A Double-sided Textile Touch Sensor for Smart Clothing
PY  - 2020
AB  - ZebraSense is a novel dual-sided woven touch sensor that can recognize and differentiate interactions on the top and bottom surfaces of the sensor. ZebraSense is based on an industrial multi-layer textile weaving technique, yet it enables a novel capacitive sensing paradigm, where each sensing element contributes to touch detection on both surfaces of the sensor simultaneously. Unlike the common "sensor sandwich" approach used in previous work, ZebraSense inherently minimizes the number of sensing elements, which drastically simplifies both sensor construction and its integration into soft goods, while preserving maximum sensor resolution. The experimental evaluation confirmed the validity of our approach and demonstrated that ZebraSense is a reliable, efficient, and accurate solution for detecting user gestures in various dual-sided interaction scenarios, allowing for new use cases in smart apparel, home decoration, toys, and other textile objects.
SP  - 662
EP  - 674
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415886
ER  - 

TY  - JOUR
AU  - Ye, Xupeng
TI  - A Survey on Simulation for Weight Perception in Virtual Reality
PY  - 2021
AB  - Virtual reality (VR) technology can provide users with an immersive experience as if they are in the real world, which can be applied in the fields of entertainment, education and scientific research, etc. In order to improve the sense of presence and immersion in VR, the design of multimodal feedback is an important component. In particular, the simulation of weight of virtual objects poses many challenges due to the limitations of hardware and software. Many researchers focused on this issue in various ways. These methods are mainly divided into two categories: device-based simulation and software-based simulation. This paper investigates the focus of software-based simulation, particularly for the virtual feedback methods proposed by researchers in recent years. We introduce the background of these proposed methods, technical implementation principles, application scenarios, the advantages and disadvantages of these simulation methods, and the evaluation criteria. We also propose the future challenges and the development of simulation methods for weight perception of virtual objects in VR.
SP  - 1
EP  - 24
JF  - Journal of Computer and Communications
VL  - 9
IS  - 9
PB  - 
DO  - 10.4236/jcc.2021.99001
ER  - 

TY  - JOUR
AU  - Senarath, Shashimal; Pathirana, Primesh; Meedeniya, Dulani; Jayarathna, Sampath
TI  - Customer Gaze Estimation in Retail Using Deep Learning
PY  - 2022
AB  - At present, intelligent computing applications are widely used in different domains, including retail stores. The analysis of customer behaviour has become crucial for the benefit of both customers and retailers. In this regard, the novel concept of remote gaze estimation using deep learning has shown promising results in analyzing customer behaviour in retail due to its scalability, robustness, low cost, and uninterrupted nature. This study presents a three-stage, three-attention-based deep convolutional neural network for remote gaze estimation in retail using only image data. In the first stage, we design a mechanism to estimate the 3D gaze of the subject using image data and monocular depth estimation. The second stage presents a novel three-attention mechanism to estimate the gaze in the wild from field-of-view, depth range, and object channel attentions. The third stage generates the gaze saliency heatmap from the output attention map of the second stage. We train and evaluate the proposed model on the benchmark GOO-Real dataset and compare the results with baseline models. Further, we adapt our model to real-retail environments by introducing a novel Retail Gaze dataset. Extensive experiments demonstrate that our approach significantly improves remote gaze target estimation performance on GOO-Real and Retail Gaze datasets.
SP  - 64904
EP  - 64919
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3183357
ER  - 

TY  - JOUR
AU  - Kim, Jung-Hwa; Choi, Seung-June; Jeong, Jin-Woo
TI  - Watch & Do: A Smart IoT Interaction System with Object Detection and Gaze Estimation
PY  - 2019
AB  - The Internet of Things (IoT) attempts to help people access Internet-connected devices, applications, and services anytime and anywhere. However, how providing an efficient and intuitive method of interaction between people and IoT devices is still an open challenge. In this paper, we propose a novel interaction system called Watch & Do , where users can control an IoT device by gazing at it and doing simple gestures. The proposed system mainly consists of: 1) object detection module; 2) gaze estimation module; 3) hand gesture recognition module; and 4) IoT controller module. The target device is identified by various deep learning-based gaze estimation and object detection techniques. Afterwards, hand gesture recognition is applied to generate an IoT device control command which is transmitted to the IoT platform. The experimental results and case studies demonstrate the feasibility of the proposed system and imply the future research directions.
SP  - 195
EP  - 204
JF  - IEEE Transactions on Consumer Electronics
VL  - 65
IS  - 2
PB  - 
DO  - 10.1109/tce.2019.2897758
ER  - 

TY  - JOUR
AU  - Pramudwiatmoko, Arif; Tsutoh, Satoru; Gutmann, Gregory; Ueno, Yutaka; Konagaya, Akihiko
TI  - A high-performance haptic rendering system for virtual reality molecular modeling
PY  - 2019
AB  - To provide a virtual reality 3D user interface with comprehensive molecular modeling, we have developed a novel haptic rendering system with a fingertip haptic rendering device and a hand-tracking Leap Motion controller. The system handles virtual molecular objects with real hands motion captured by the Leap Motion controller in a virtual reality environment. The fingertip haptic rendering device attached on each finger and a wrist gives haptic display, when virtual hands manipulating virtual molecular objects. Based on preliminary software development studies using existing 3D graphics toolkit such as CHAI3D and Unity, the fingertip haptic rendering device works with a reasonable performance for a polygon surface model and a ribbon model, but not for an atomic model due to the low rendering performance. On the other hand, the device provides us a grasping feeling of a large molecule represented by an atomic model, when used with the particle simulation system running on graphics library, DirectX 12. The haptic rendering performances, among the three software systems are discussed.
SP  - 542
EP  - 549
JF  - Artificial Life and Robotics
VL  - 24
IS  - 4
PB  - 
DO  - 10.1007/s10015-019-00555-9
ER  - 

TY  - NA
AU  - Knibbe, Jarrod; Freire, Rachel; Koelle, Marion; Strohmeier, Paul
TI  - TEI - Skill-Sleeves: Designing Electrode Garments for Wearability
PY  - 2021
AB  - Many existing explorations of wearables for HCI consider functionality first and wearability second. Typically, as the technologies, designs, and experiential understandings develop, attention can shift towards questions of deployment and wearability. To support this shift of focus we present a case study of the iterative design of electrode sleeves. We consider the design motivations and background that led to the existing, prototype EMS sleeves, and the resultant challenges around their wearability. Through our own design research practice, we seek to reveal design criteria towards the wearability of such a sleeve, and provide designs that optimise for those criteria. We contribute (1) new electrode sleeve designs, which begin to make it practicable to take EMS beyond the lab, (2) new fabrication processes that support rapid production and personalisation, and (3) reflections on criteria for wearability across new eTextile garments.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440652
ER  - 

TY  - BOOK
AU  - Stellmacher, Carolin
TI  - VR Workshops - Haptic-Enabled Buttons Through Adaptive Trigger Resistance
PY  - 2021
AB  - While commercial controllers for virtual reality (VR) offer a variety of components to register user input, their ability to generate meaningful haptic feedback during the interaction lags behind. This prevents users from experiencing the virtual world through their haptic sense. For example, grabbing a light virtual object feels identical to grabbing a heavy virtual object. In this workshop paper, we enrich an established input component available in any commercial VR controller with appropriate haptic rendering capabilities. As a proof of concept, we present our haptic VR controller Triggermuscle and its adaptive trigger to simulate weight in VR. The trigger dynamically adapts its resistance according to the weight of a grabbed virtual object: The heavier the virtual object, the higher the trigger resistance and the more force users need to apply. Our system is built into the casing of an HTC Vive controller and connects the original trigger component to an extension spring for variable resistance. We envision for the future, that VR input devices can evolve more into input-output technologies and provide meaningful and versatile haptic feedback.
SP  - 201
EP  - 204
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00044
ER  - 

TY  - NA
AU  - Aigner, Roland; Pointner, Andreas; Preindl, Thomas; Danner, Rainer; Haller, Michael J.
TI  - CHI - TexYZ: Embroidering Enameled Wires for Three Degree-of-Freedom Mutual Capacitive Sensing
PY  - 2021
AB  - In this paper, we present TexYZ, a method for rapid and effortless manufacturing of textile mutual capacitive sensors using a commodity embroidery machine. We use enameled wire as a bobbin thread to yield textile capacitors with high quality and consistency. As a consequence, we are able to leverage the precision and expressiveness of projected mutual capacitance for textile electronics, even when size is limited. Harnessing the assets of machine embroidery, we implement and analyze five distinct electrode patterns, examine the resulting electrical features with respect to geometrical attributes, and demonstrate the feasibility of two promising candidates for small-scale matrix layouts. The resulting sensor patches are further evaluated in terms of capacitance homogeneity, signal-to-noise ratio, sensing range, and washability. Finally, we demonstrate two use case scenarios, primarily focusing on continuous input with up to three degrees-of-freedom.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445479
ER  - 

TY  - NA
AU  - Xie, Haoran; Mitsuhashi, Kento; Torii, Takuma
TI  - AH - Augmenting Human With a Tail
PY  - 2019
AB  - Human-augmentation devices have been extensively proposed and developed recently and are useful in improving our work efficiency and our quality of life. Inspired by animal tails, this study aims to propose a wearable and functional tail device that combines physical and emotional-augmentation modes. In the physical-augmentation mode, the proposed device can be transformed into a consolidated state to support a user's weight, similar to a kangaroo's tail. In the emotional-augmentation mode, the proposed device can help users express their emotions, which are realized by different tail-motion patterns. For our initial prototype, we developed technical features that can support the weight of an adult, and we performed a perceptional investigation of the relations between the tail movements and the corresponding perceptual impressions. Using the animal-tail analog, the proposed device may be able to help the human user in both physical and emotional ways.
SP  - 35
EP  - NA
JF  - Proceedings of the 10th Augmented Human International Conference 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311823.3311847
ER  - 

TY  - NA
AU  - Strohmeier, Paul; Håkansson, Victor; Honnet, Cedric; Ashbrook, Daniel; Hornbæk, Kasper
TI  - Tangible and Embedded Interaction - Optimizing Pressure Matrices: Interdigitation and Interpolation Methods for Continuous Position Input
PY  - 2019
AB  - This paper provides resources and design recommendations for optimizing position input for pressure sensor matrices, a sensor design often used in eTextiles. Currently applications using pressure matrices for precise continuous position control are rare. One reason designers opt against using these sensors for continuous position control is that when the finger transitions from one sensing electrode to the next, jerky motion, jumps or other non-linear artifacts appear. We demonstrate that interdigitation can improve transition behavior and discuss interpolation algorithms to best leverage such designs. We provide software for reproducing our sensors and experiment, as well as a dataset consisting of 1122 swipe gestures performed on 17 sensors.
SP  - 117
EP  - 126
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295638
ER  - 

TY  - NA
AU  - Hechuan, Zhang; Zhiyong, Chen; Guo, Shihui; Lin, Juncong; Yating, Shi; Xiangyang, Liu; Ma, Yong
TI  - CHI - Sensock: 3D Foot Reconstruction with Flexible Sensors
PY  - 2020
AB  - Capturing 3D foot models is important for applications such as manufacturing customized shoes and creating clubfoot orthotics. In this paper, we propose a novel prototype, Sensock, to offer a fully wearable solution for the task of 3D foot reconstruction. The prototype consists of four soft stretchable sensors, made from silk fibroin yarn. We identify four characteristic foot girths based on the existing knowledge of foot anatomy, and measure their lengths with the resistance value of the stretchable sensors. A learning-based model is trained offline and maps the foot girths to the corresponding 3D foot shapes. We compare our method with existing solutions using red-green-blue (RGB) or RGBD (RGB-depth) cameras, and show the advantages of our method in terms of both efficiency and accuracy. In the user experiment, we find that the relative error of Sensock is lower than 0.55%. It performs consistently across different trials and is considered comfortable and suitable for long-term wearing.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376387
ER  - 

TY  - NA
AU  - He, Zhenyi; Zhu, Fengyuan; Perlin, Ken; Ma, Xiaojuan
TI  - Manifest the Invisible: Design for Situational Awareness of Physical Environments in Virtual Reality.
PY  - 2018
AB  - Virtual Reality (VR) provides immersive experiences in the virtual world, but it may reduce users' awareness of physical surroundings and cause safety concerns and psychological discomfort. Hence, there is a need of an ambient information design to increase users' situational awareness (SA) of physical elements when they are immersed in VR environment. This is challenging, since there is a tradeoff between the awareness in reality and the interference with users' experience in virtuality. In this paper, we design five representations (indexical, symbolic, and iconic with three emotions) based on two dimensions (vividness and emotion) to address the problem. We conduct an empirical study to evaluate participants' SA, perceived breaks in presence (BIPs), and perceived engagement through VR tasks that require movement in space. Results show that designs with higher vividness evoke more SA, designs that are more consistent with the virtual environment can mitigate the BIP issue, and emotion-evoking designs are more engaging.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Sakr, Sophia; Cagneau, Barthelemy; Daunizeau, Thomas; Regnier, Stephane; Haliyo, Sinan
TI  - Haptic Remote Control Interface for Robotic Micro-Assembly at Low Frequency
PY  - 2020
AB  - Microassembly of submillimetric objects is still a manual process in most industries. Manufacture of MOEMS (Micro-Optical-Electrical-Mechanical System) based sensors, or watchmaking and even micro-surgery relies very heavily on the motor skills of an operator handling specialized tools. We propose here a preliminary work on bilateral coupling between a macro-tweezers and a micro-tweezers using a user interface for a microrobotic system which ambitions to combine the precision of the robot with the expertise of the craftsman. It strives to hide the scale-change between the microcomponents and the user’s workbench, by providing a scaled-up tabletop image of the robot’s sample-holder, co-located with a hand-held tool. This active tweezers mimics the traditional tool in form and function, albeit augmented with feedback. Its motions are tracked to drive the microgripper. Users may hence directly pick-up a micro-object from the image, while feeling the grasp of the microgripper on their fingers.
SP  - 1
EP  - 7
JF  - 2020 International Conference on Manipulation, Automation and Robotics at Small Scales (MARSS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/marss49294.2020.9307839
ER  - 

TY  - NA
AU  - Yang, Keng-Ta; Wang, Chiu-Hsuan; Chan, Liwei
TI  - UIST - ShareSpace: Facilitating Shared Use of the Physical Space by both VR Head-Mounted Display and External Users
PY  - 2018
AB  - Currently, "walkable" virtual reality (VR) is achieved by dedicating a room-sized space for VR activities, which is not shared with non-HMD users engaged in their own activities. To achieve the goal of allowing shared use of space for all users while overcoming the obvious difficulty of integrating use with those immersed in a VR experience, we present ShareSpace, a system that allows external users to communicate their needs for physical space to those wearing an HMD and immersed in their VR experience. ShareSpace works by allowing external users to place "shields" in the virtual environment by using a set of physical shield tools. A pad visualizer helps this process by allowing external users to examine the arrangement of virtual shields. We also discuss interaction techniques that minimize the interference between the respective activities of the HMD wearers and the other users of the same physical space. To evaluate our design, a user study was conducted to collect user feedback from participants in four trial scenarios. The results indicate that our ShareSpace system allows users to perform their respective activities with improved engagement and safety. In addition, this study shows that while the HMD users did perceive a considerable degree of interference due to the internal visual indications from the ShareSpace system, they were still more engaged in their VR experience than when interrupted by direct external physical interference initiated by external users.
SP  - 499
EP  - 509
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242630
ER  - 

TY  - NA
AU  - Tsui, Tse; Morimoto, Tania K.
TI  - WHC - Design of a Portable Shape Display for Augmented Reality
PY  - 2021
AB  - We introduce a portable pin-based shape display for augmented reality that enables active environment exploration over a large workspace. As users explore the environment, our custom Unity App displays the augmented reality objects and computes the associated shapes to be rendered on the portable 3x3 pin array. We conducted a user study using a non-mechatronic version of the device to evaluate the effects of important design parameters on the perceived realism of interactions with physical objects. Based on these results, we designed the final device and conducted a second user study to determine the effectiveness of the device at conveying shape features. Without visual feedback, participants were successful in identifying the correct shape with 42.86% accuracy, demonstrating the potential of the device to be used to enhance the haptic experience during active exploration. Finally, two preliminary applications are explored to illustrate possible educational use cases for the overall system.
SP  - 91
EP  - 96
JF  - 2021 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc49131.2021.9517207
ER  - 

TY  - JOUR
AU  - Tanabe, Takeshi; Yano, Hiroaki; Endo, Hiroshi; Ino, Shuichi; Iwata, Hiroo
TI  - Pulling Illusion Based on the Phase Difference of the Frequency Components of Asymmetric Vibrations
PY  - 2021
AB  - When presented with asymmetric vibrations, humans experience an illusory force, similar to the sensation of being pulled in a particular direction. A pulling illusion has also been used in new display elements for a virtual reality content and a pedestrian navigation system. However, the basic design of asymmetric vibration stimuli that can induce this illusion has not yet been determined. In particular, it is unclear as to which part of the vibration waveform should be asymmetric to induce an illusion. To better understand the design of asymmetric vibration stimuli that can induce a pulling illusion, we evaluated the effect of the illusion corresponding to the waveform deformation due to a change in phase difference of asymmetric-vibration frequency components. The results of a psychophysical experiment demonstrate that when the phase differences of the fundamental and second harmonic waves of the asymmetric vibration are close to 0 $^\circ$ or $-$ 180 $^\circ$ , the illusion is more likely to occur. This result implies that the difference in the rate by which the acceleration changes at each polarity contributes to the illusion.
SP  - 203
EP  - 213
JF  - IEEE/ASME Transactions on Mechatronics
VL  - 26
IS  - 1
PB  - 
DO  - 10.1109/tmech.2020.3009384
ER  - 

TY  - BOOK
AU  - Ritchie, Jackie; Bontilao, Joselle; Kennelly, Sarah; Topliss, Jack; Dunn, Jessica; Renaud, Andre; Huber, Tim; de Gast, Barro W; Piumsomboon, Thammathip
TI  - AsianCHI@CHI - COMFlex: An Adaptive Haptic Interface with Shape-Changing and Weight-Shifting Mechanism for Immersive Virtual Reality
PY  - 2021
AB  - This work explores shape-changing and weight-shifting mechanisms for haptic interfaces to simulate various shapes and sizes in Virtual Reality (VR) for the industrial product design process. The COMFlex system offers haptic feedback in the form of a weight distribution changing (COM) and shape-changing (Flex) device while perceiving a visual representation in VR. Any state changes to the virtual representation are reflected by the COMFlex system, allowing live dynamic feedback to the user. We share initial findings from experimenting with COMFlex in several use cases for our follow up design improvements. Finally, future work is discussed, including physical changes to the device and potential applications.
SP  - 210
EP  - 214
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3468214
ER  - 

TY  - NA
AU  - Gonzalez, Eric J.; Ofek, Eyal; Gonzalez-Franco, Mar; Sinclair, Mike
TI  - UIST - X-Rings: A Hand-mounted 360° Shape Display for Grasping in Virtual Reality
PY  - 2021
AB  - X-Rings is a novel hand-mounted 360° shape display for Virtual Reality that renders objects in 3D and responds to user-applied touch and grasping force. Designed as a modular stack of motor-driven expandable rings (5.7-7.7 cm diameter), X-Rings renders radially-symmetric surfaces graspable by the user’s whole hand. The device is strapped to the palm, allowing the fingers to freely make and break contact with the device. Capacitance sensors and motor current sensing provide estimates of finger touch states and gripping force. We present the results of a user study evaluating participants’ ability to associate device-rendered shapes with visually-rendered objects as well as a demo application that allows users to freely interact with a variety of objects in a virtual environment.
SP  - 732
EP  - 742
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474782
ER  - 

TY  - NA
AU  - Gao, Tianyang; Itoh, Yuta
TI  - ISMAR Adjunct - The Kuroko Paradigm: The Implications of Augmenting Physical Interaction with AR Avatars
PY  - 2019
AB  - We propose a concept in this poster paper, the Kuroko Paradigm, which is able to enhance user engagement during interaction with an augmented reality (AR) avatar by adding a physical object to the interaction with the avatar. With the development of AR and VR, interactions between users and AR avatars have been realized with different approaches. However, most of such interactions and experiences are passive, from which users do not expect a high level of engagement. We hypothesize that by introducing a reality actuator, such as a robot or a drone, to handle a physical object triggered by the user without being noticed, and rendering AR avatars as interacting with the physical object at the same time, user engagement during the experience will be enhanced. To prove this concept, we conducted an experiment emulating a classic game of catch. In the experiment, a user will try to throw a ball to an AR avatar, and the ball will be caught by a reality actuator. From the user's perspective, the ball is caught by the AR avatar. In the future, we plan to extend the experiment by adding control groups with differing conditions.
SP  - 26
EP  - 27
JF  - 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2019.00022
ER  - 

TY  - NA
AU  - Lou, Yuze; Uddin, Mahfus; Brown, Nathaniel; Cafarella, Michael
TI  - HILDA@SIGMOD - Knowledge Graph Programming with a Human-in-the-Loop: Preliminary Results
PY  - 2019
AB  - In this paper we introduce knowledge graph programming, a new method for writing extremely succinct programs. This method allows programmers to save work by writing programs that are brief but also underspecified and underconstrained; a human-in-the-loop "data compiler" then automatically fills in missing values without the programmer's explicit help. It uses modern data quality mechanisms such as information extraction, data integration, and crowdsourcing. The language encourages users to mention knowledge graph entities in their programs, thus enabling the data compiler to exploit the extensive factual and type structure present in modern KGs. We describe the knowledge graph programming user experience, explain its conceptual steps and data model, describe our prototype KGP system, and present some preliminary experimental results.
SP  - NA
EP  - NA
JF  - Proceedings of the Workshop on Human-In-the-Loop Data Analytics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3328519.3329132
ER  - 

TY  - JOUR
AU  - Ferri, Josue; Llopis, Raúl Llinares; Martinez, Gabriel; Roger, José Vicente Lidon; Garcia-Breijo, Eduardo
TI  - Comparison of E-Textile Techniques and Materials for 3D Gesture Sensor with Boosted Electrode Design
PY  - 2020
AB  - There is an interest in new wearable solutions that can be directly worn on the curved human body or integrated into daily objects. Textiles offer properties that are suitable to be used as holders for electronics or sensors components. Many sensing technologies have been explored considering textiles substrates in combination with conductive materials in the last years. In this work, a novel solution of a gesture recognition touchless sensor is implemented with satisfactory results. Moreover, three manufacturing techniques have been considered as alternatives: screen-printing with conductive ink, embroidery with conductive thread and thermosealing with conductive fabric. The main critical parameters have been analyzed for each prototype including the sensitivity of the sensor, which is an important and specific parameter of this type of sensor. In addition, user validation has been performed, testing several gestures with different subjects. During the tests carried out, flick gestures obtained detection rates from 79% to 89% on average. Finally, in order to evaluate the stability and strength of the solutions, some tests have been performed to assess environmental variations and washability deteriorations. The obtained results are satisfactory regarding temperature and humidity variations. The washability tests revealed that, except for the screen-printing prototype, the sensors can be washed with minimum degradation.
SP  - 2369
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 8
PB  - 
DO  - 10.3390/s20082369
ER  - 

TY  - JOUR
AU  - Kim, Huhn; Baek, Mi-Seon
TI  - Moment Controller: VR Controller raises Awareness of the Difference in Weight between Virtual Objects
PY  - 2021
AB  - NA
SP  - 133
EP  - 151
JF  - Archives of Design Research
VL  - 34
IS  - 2
PB  - 
DO  - 10.15187/adr.2021.05.34.2.133
ER  - 

TY  - NA
AU  - Çamci, Anil
TI  - VR - Some Considerations on Creativity Support for VR Audio
PY  - 2019
AB  - As the consumer interest in VR grows, the community of content creators working in this domain expands accordingly. Given the intrinsic role of audio in VR experiences, this growth necessitates authoring tools for immersive audio that can cater to a wide range of designers regardless of their expertise in spatial audio. In this article, we discuss some of the modern considerations on designing interactive tools for creativity support in VR audio. We provide examples from existing spatial audio design software, and discuss areas in which new tools can facilitate, for expert and novice users alike, the use of immersive audio in compelling new VR experiences.
SP  - 1500
EP  - 1502
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798210
ER  - 

TY  - NA
AU  - Achberger, Alexander; Arulrajah, Pirathipan; Sedlmair, Michael; Vidackovic, Kresimir
TI  - STROE: An Ungrounded String-Based Weight Simulation Device
PY  - 2022
AB  - We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user&#x2019;s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users&#x2019; perceived realism and immersion of VR scenes.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00029
ER  - 

TY  - NA
AU  - Min, Dae-Hong; Lee, Dong-Yong; Cho, Yong-Hun; Lee, In-Kwon
TI  - VR - Shaking Hands in Virtual Space: Recovery in Redirected Walking for Direct Interaction between Two Users
PY  - 2020
AB  - Various studies have been conducted to realize realistic direct interaction in the virtual environment. In this study, we focus on a situation wherein two users using the same physical space explore the same virtual environment using redirected walking (RDW) technology. For two users to meet each other in a virtual environment to realize realistic direct interaction, they must simultaneously meet each other in physical space. However, if the RDW algorithm is applied to each user independently, the relative positions and orientations of the two users can be significantly different in the virtual and physical spaces. We present a recovery algorithm that adjusts the relative position and orientation such that they become the same in the two spaces. Our recovery algorithm uses either modified subtle RDW techniques or overt recovery techniques in three cases depending on the relative position and orientation of the two users. Once the recovered state is reached, the two users can go forward to meet each other and directly interact in the virtual and physical spaces simultaneously. Based on the experiment results, we can confirm that the application of our recovery technology to the system increases the user’s satisfaction in usability and the presence of coexistence in the virtual environment with other users.
SP  - 164
EP  - 173
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581308731108
ER  - 

TY  - NA
AU  - Yang, Humphrey; Johnson, Tate; Zhong, Ke; Patel, Dinesh; Olson, Gina; Majidi, Carmel; Islam, Mohammad; Yao, Lining
TI  - ReCompFig: Designing Dynamically Reconfigurable Kinematic Devices Using Compliant Mechanisms and Tensioning Cables
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502065
ER  - 

TY  - NA
AU  - Antoine, Axel; Malacria, Sylvain; Casiez, Géry
TI  - CHI - Using High Frequency Accelerometer and Mouse to Compensate for End-to-end Latency in Indirect Interaction
PY  - 2018
AB  - End-to-end latency corresponds to the temporal difference between a user input and the corresponding output from a system. It has been shown to degrade user performance in both direct and indirect interaction. If it can be reduced to some extend, latency can also be compensated through software compensation by trying to predict the future position of the cursor based on previous positions, velocities and accelerations. In this paper, we propose a hybrid hardware and software prediction technique specifically designed for partially compensating end-to-end latency in indirect pointing. We combine a computer mouse with a high frequency accelerometer to predict the future location of the pointer using Euler based equations. Our prediction method results in more accurate prediction than previously introduced prediction algorithms for direct touch. A controlled experiment also revealed that it can improve target acquisition time in pointing tasks.
SP  - 609
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174183
ER  - 

TY  - CHAP
AU  - Ushiyama, Keigo; Takahashi, Akifumi; Kajimoto, Hiroyuki
TI  - Increasing Perceived Weight and Resistance by Applying Vibration to Tendons During Active Arm Movements
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>We proposed to use kinesthetic illusion to achieve wearable/portable haptic devices for kinesthetic feedback in VR experiences. The kinesthetic illusion is the illusion of limb movement typically induced by vibratory stimulation. We investigated how the kinesthetic illusion affected the perceived weight and resistance of the handheld object. We designed vibration patterns that simulate constant gravity and velocity-related resistance. Two experiments were conducted to measure changes in perceiving weight and resistance when wielding cylindrical weights and hand fans. The results of the experiments indicated that the designed kinesthetic illusions enhanced these sensations; the real weight was perceived heavier, and the real resistance was perceived larger. However, we could not find the explicit difference between the two stimulation patterns, and the resistance sensation induced by the illusion differed from the actual sensation of using the hand fans.</jats:p>
SP  - 93
EP  - 100
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06249-0_11
ER  - 

TY  - NA
AU  - Esteves, Augusto; Quintal, Filipe; Caires, Fabio; Baptista, Vitor; Mendes, Pedro
TI  - exp.at - Wattom: Ambient Eco-feedback with Mid-air Input
PY  - 2019
AB  - This paper presents Wattom, a highly interactive ambient eco-feedback smart plug that aims to promote a more sustainable use of electricity in the home. This paper describes our latest implementation of the Wattom plug, and three system applications. The first enables Wattom to power a connected device, and provide real-time feedback on the amount of electricity in the grid from renewable sources. The second enables users to schedule power events from their smart watches. Finally, the third application uses non-intrusive load monitoring (NILM) to provide users with personal consumption information on multiple devices connected to a single Wattom plug. The paper concludes by presenting insights into the development and use of various iterations of the Wattom plug.
SP  - 12
EP  - 15
JF  - 2019 5th Experiment International Conference (exp.at'19)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/expat.2019.8876565
ER  - 

TY  - JOUR
AU  - Wentao, Lyu; Ding, Peng; Zhang, Yingliang; Chen, Anpei; Wu, Minye; Yin, Shu; Yu, Jingyi
TI  - Refocusable Gigapixel Panoramas for Immersive VR Experiences
PY  - 2021
AB  - There have been significant advances in capturing gigapixel panoramas (GPP). However, solutions for viewing GPPs on head-mounted displays (HMDs) are lagging: an immersive experience requires ultra-fast rendering while directly loading a GPP onto the GPU is infeasible due to limited texture memory capacity. In this paper, we present a novel out-of-core rendering technique that supports not only classic panning, tilting, and zooming but also dynamic refocusing for viewing a GPP on HMD. Inspired by the network package transmission mechanisms in distributed visualization, our approach employs hierarchical image tiling and on-demand data updates across the main and the GPU memory. We further present a multi-resolution rendering scheme and a refocused light field rendering technique based on RGBD GPPs with minimal memory overhead. Comprehensive experiments demonstrate that our technique is highly efficient and reliable, able to achieve ultra-high frame rates ( $> 50$ > 50 fps) even on low-end GPUs. With an embedded gaze tracker, our technique enables immersive panorama viewing experiences with unprecedented resolutions, field-of-view, and focus variations while maintaining smooth spatial, angular, and focal transitions.
SP  - 2028
EP  - 2040
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 3
PB  - 
DO  - 10.1109/tvcg.2019.2940444
ER  - 

TY  - NA
AU  - Sun, Wei; Chen, Yanjun; Zhan, Simon; Han, Teng; Tian, Feng; Wang, Hongan; Yang, Xing-Dong
TI  - CHI - RElectrode: A Reconfigurable Electrode For Multi-Purpose Sensing Based on Microfluidics
PY  - 2021
AB  - In this paper, we propose a reconfigurable electrode, RElectrode, using a microfluidic technique that can change the geometry and material properties of the electrode to satisfy the needs for sensing a variety of different types of user input through touch/touchless gestures, pressure, temperature, and distinguish between different types of objects or liquids. Unlike the existing approaches, which depend on the specific-shaped electrode for particular sensing (e.g., coil for inductive sensing), RElectrode enables capacity, inductance, resistance/pressure, temperature, pH sensings all in a single package. We demonstrate the design and fabrication of the microfluidic structure of our RElectrode, evaluate its sensing performance through several studies, and provide some unique applications. RElectrode demonstrates technical feasibility and application values of integrating physical and biochemical properties of microfluidics into novel sensing interfaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445652
ER  - 

TY  - NA
AU  - Honnet, Cedric; Perner-Wilson, Hannah; Teyssier, Marc; Fruchard, Bruno; Steimle, Jürgen; Baptista, Ana Catarina; Strohmeier, Paul
TI  - CHI - PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization
PY  - 2020
AB  - We present a method for enabling arbitrary textiles to sense pressure and deformation: In-situ polymerization supports integration of piezoresistive properties at the material level, preserving a textile's haptic and mechanical characteristics. We demonstrate how to enhance a wide set of fabrics and yarns using only readily available tools. To further support customisation by the designer, we present methods for patterning, as needed to create circuits and sensors, and demonstrate how to combine areas of different conductance in one material. Technical evaluation results demonstrate the performance of sensors created using our method is comparable to off-the-shelf piezoresistive textiles. As application examples, we demonstrate rapid manufacturing of on-body interfaces, tie-dyed motion-capture clothing, and zippers that act as potentiometers.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376841
ER  - 

TY  - NA
AU  - Elvezio, Carmine; Amelot, Pierre; Boyle, Robert; Wes, Catherine Ilona; Feiner, Steven
TI  - ISMAR Adjunct - Hybrid UIs for Music Exploration in AR and VR
PY  - 2018
AB  - We present hybrid user interfaces that facilitate interaction with music content in 3D, using a combination of 2D and 3D input and display devices. Participants will explore an online music library, some wearing AR or VR head-worn displays used alone or in conjunction with touch screens, and others using only touch screens. They will select genres, artists, albums, and songs, interacting through a combination of 3D hand-tracking and 2D multi-touch technologies.
SP  - 411
EP  - 412
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2018.00121
ER  - 

TY  - NA
AU  - Wang, Yifan; Song, Jiarun; Yang, Fuzheng
TI  - Two-Layer 3DoF+ System Based on Ultra-High Resolution Panoramic Video
PY  - 2019
AB  - 3DoF+ supports limited movements of the user to better interact with the video contents. It provides user with more detailed information by utilizing the ultra-high resolution panoramic video. However, such high-resolution video poses a great challenge to network bandwidth and client performance. In this paper, a two-layer based 3DoF+ panoramic video system is proposed utilizing the ultra-high resolution panoramic video. According to the user’s viewpoint, this system will select different layer videos to display to provide users with a real overall zooming viewing experience, while effectively saving bandwidth. Experimental results show that the proposed system saves 85.95% transmission bandwidth than the traditional system in average.
SP  - 1734
EP  - 1738
JF  - 2019 IEEE 5th International Conference on Computer and Communications (ICCC)
VL  - 2019
IS  - NA
PB  - 
DO  - 10.1109/iccc47050.2019.9064317
ER  - 

TY  - JOUR
AU  - Venkatagiri, Sukrit; Thebault-Spieker, Jacob; Kohler, Rachel; Purviance, John; Mansur, Rifat Sabbir; Luther, Kurt
TI  - GroundTruth: Augmenting Expert Image Geolocation with Crowdsourcing and Shared Representations
PY  - 2019
AB  - Expert investigators bring advanced skills and deep experience to analyze visual evidence, but they face limits on their time and attention. In contrast, crowds of novices can be highly scalable and parallelizable, but lack expertise. In this paper, we introduce the concept of shared representations for crowd--augmented expert work, focusing on the complex sensemaking task of image geolocation performed by professional journalists and human rights investigators. We built GroundTruth, an online system that uses three shared representations-a diagram, grid, and heatmap-to allow experts to work with crowds in real time to geolocate images. Our mixed-methods evaluation with 11 experts and 567 crowd workers found that GroundTruth helped experts geolocate images, and revealed challenges and success strategies for expert-crowd interaction. We also discuss designing shared representations for visual search, sensemaking, and beyond.
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359209
ER  - 

TY  - BOOK
AU  - Lohse, Andreas L.; Kjar, Christoffer K.; Hamulic, Ervin; Lima, Ingrid G. A.; Jensen, Tilde H.; Bruni, Luis Emilio; Nilsson, Niels Christian
TI  - WEVR@VR - Leveraging Change Blindness for Haptic Remapping in Virtual Environments
PY  - 2019
AB  - Passive haptic feedback provides an inexpensive and convenient approach to virtual touch. However, this approach requires that all virtual objects represented by physical props. In this paper we present change blindness haptic remapping—a novel approach that leverages change blindness to map two or more virtual objects onto a single physical prop. We describe a preliminary evaluation comparing the proposed approach to a control condition where all virtual objects where mapped to physical props. The study revealed no notable differences in terms of the participants’ experience and less than one fourth of the participants noticed the manipulation. However, the participants did perform interaction errors when exposed to haptic remapping. Based on the findings, we discuss improvements to the proposed approach and potential directions for future work.
SP  - 8809587
EP  - NA
JF  - 2019 IEEE 5th Workshop on Everyday Virtual Reality (WEVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wevr.2019.8809587
ER  - 

TY  - CHAP
AU  - Zhang, Yiran; Ladeveze, Nicolas; Fleury, Cédric; Bourdot, Patrick
TI  - EuroVR - Switch Techniques to Recover Spatial Consistency Between Virtual and Real World for Navigation with Teleportation
PY  - 2019
AB  - In many virtual reality systems, user physical workspace is superposed with a particular area in a virtual environment. The spatial consistency between the real and virtual interactive space allows users to take advantage of physical workspace to walk and to interact intuitively with the real and virtual contents. To maintain such spatial consistency, application designers usually deactivate user virtual navigation capability. This limits user reachable virtual area, and segments the spatial consistency required sub-task from a continuous scenario mixing large scale navigation. In order to provide users with a continuous virtual experience, we introduce two switch techniques to help users to recover the spatial consistency in some predefined virtual areas with teleportation navigation: simple switch and improved switch. We conducted a user study with a box-opening task in a CAVE-like system to evaluate the performance and usability of these techniques under different conditions. The results highlight that assisting the user on switching back to a spatially consistent situation ensures entire workspace accessibility and decreases time and cognitive effort used to complete the sub-task. The simple switch results in less task completion time, less cognitive load, and is globally preferred by users. With additional visual feedback of user switch destination, the improved switch seems to provide the user with a better understanding of the resulting spatial configuration of the switch.
SP  - 3
EP  - 23
JF  - Virtual Reality and Augmented Reality
VL  - 11883
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-31908-3_1
ER  - 

TY  - JOUR
AU  - Choi, Inrak; Gonzalez, Eric J.; Follmer, Sean
TI  - Hybrid Actuation With Unidirectional Clutches for Handheld Haptic Controllers
PY  - 2021
AB  - Realizing high performance force feedback through a handheld haptic interface is challenging due to the limitations on actuator selection imposed by form factor requirements including size, weight, and cost. Here, we introduce a hybrid actuation mechanism composed of a geared motor and two active unidirectional clutches to achieve an increased impedance range and improve performance. One clutch is used to selectively couple a free end-effector to the geared motor only during haptic rendering, eliminating the high inertia of the geared motor in free space. A second clutch is used to ground the end-effector directly to the device body, allowing the device to render large force without stalling the motor. This hybrid mechanism renders free space and solid objects effectively in a passive manner and extends the impedance rendering capability of a haptic interface driven by a small motor while keeping the device lightweight, energy-efficient, safe, and low-cost.
SP  - 4827
EP  - 4834
JF  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 3
PB  - 
DO  - 10.1109/lra.2021.3068700
ER  - 

TY  - NA
AU  - Chen, Yan; Lee, Sang Won; Oney, Steve
TI  - CHI - CoCapture: Effectively Communicating UI Behaviors on Existing Websites by Demonstrating and Remixing
PY  - 2021
AB  - User Interface (UI) mockups are commonly used as shared context during interface development collaboration. In practice, UI designers often use screenshots and sketches to create mockups of desired UI behaviors for communication. However, in the later stages of UI development, interfaces can be arbitrarily complex, making it labor-intensive to sketch, and static screenshots are limited in the types of interactive and dynamic behaviors they can express. We introduce CoCapture, a system that allows designers to easily create UI behavior mockups on existing web interfaces by demonstrating and remixing, and to accurately describe their requests to helpers by referencing the resulting mockups using hypertext. We showed that participants could more accurately describe UI behaviors with CoCapture than with existing sketch and communication tools and that the resulting descriptions were clear and easy to follow. Our approach can help teams develop UIs efficiently by bridging communication gaps with more accurate visual context.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445573
ER  - 

TY  - NA
AU  - Je, Seungwoo; Kim, Myung Jin; Lee, Woojin; Lee, Byungjoo; Yang, Xing-Dong; Lopes, Pedro; Bianchi, Andrea
TI  - UIST - Aero-plane: A Handheld Force-Feedback Device that Renders Weight Motion Illusion on a Virtual 2D Plane
PY  - 2019
AB  - Force feedback is said to be the next frontier in virtual reality (VR). Recently, with consumers pushing forward with untethered VR, researchers turned away from solutions based on bulky hardware (e.g., exoskeletons and robotic arms) and started exploring smaller portable or wearable devices. However, when it comes to rendering inertial forces, such as when moving a heavy object around or when interacting with objects with unique mass properties, current ungrounded force feedback devices are unable to provide quick weight shifting sensations that can realistically simulate weight changes over 2D surfaces. In this paper we introduce Aero-plane, a force-feedback handheld controller based on two miniature jet propellers that can render shifting weights of up to 14 N within 0.3 seconds. Through two user studies we: (1) characterize the users' ability to perceive and correctly recognize different motion paths on a virtual plane while using our device; and, (2) tested the level of realism and immersion of the controller when used in two VR applications (a rolling ball on a plane, and using kitchen tools of different shapes and sizes). Lastly, we present a set of applications that further explore different usage cases and alternative form-factors for our device.
SP  - 763
EP  - 775
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347926
ER  - 

TY  - JOUR
AU  - Xu, Guanghua; Wan, Quan; Deng, Wenwu; Guo, Tao; Cheng, Jingyuan
TI  - Smart-Sleeve: A Wearable Textile Pressure Sensor Array for Human Activity Recognition.
PY  - 2022
AB  - Human activity recognition is becoming increasingly important. As contact with oneself and the environment accompanies almost all human activities, a Smart-Sleeve, made of soft and stretchable textile pressure sensor matrix, is proposed to sense human contact with the surroundings and identify performed activities in this work. Additionally, a dataset including 18 activities, performed by 14 subjects in 10 repetitions, is generated. The Smart-Sleeve is evaluated over six classical machine learning classifiers (support vector machine, k-nearest neighbor, logistic regression, random forest, decision tree and naive Bayes) and a convolutional neural network model. For classical machine learning, a new normalization approach is proposed to overcome signal differences caused by different body sizes and statistical, geometric, and symmetry features are used. All classification techniques are compared in terms of classification accuracy, precision, recall, and F-measure. Average accuracies of 82.02% (support vector machine) and 82.30% (convolutional neural network) can be achieved in 10-fold cross-validation, and 72.66% (support vector machine) and 74.84% (convolutional neural network) in leave-one-subject-out validation, which shows that the Smart-Sleeve and the proposed data processing method are suitable for human activity recognition.
SP  - 1702
EP  - 1702
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 5
PB  - 
DO  - 10.3390/s22051702
ER  - 

TY  - CHAP
AU  - Townsend, Riikka; Bang, Anne Louise; Mikkonen, Jussi
TI  - HCI (23) - Textile Designer Perspective on Haptic Interface Design: A Sensorial Platform for Conversation Between Discipline
PY  - 2020
AB  - Smart textiles have established a foothold in different academic fields, such as in chemistry, engineering, and in human-computer interaction (HCI). Within HCI, smart textiles are present in research in many ways, for example, as context, as means, or as focus. However, interdisciplinary projects tend to leave the implications of and to textile design without notice. How can a project utilise a textile designer’s skills to feed back to textile design from an interdisciplinary project? In this paper, we present a case study, where a textile designer’s role extends beyond the prototype production, and we analyse the project in light of textile design. Our findings show that textile design can augment data collection and analysis. We conclude with a discussion towards inclusion of textile design in HCI.
SP  - 110
EP  - 127
JF  - Distributed, Ambient and Pervasive Interactions
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50344-4_9
ER  - 

TY  - NA
AU  - Leake, Mackenzie; Lai, Frances; Grossman, Tovi; Wigdor, Daniel; Lafreniere, Ben
TI  - CHI - PatchProv: Supporting Improvisational Design Practices for Modern Quilting
PY  - 2021
AB  - The craft of improvisational quilting involves working without the use of a predefined pattern. Design decisions are made “in the fabric,” with design experimentation tightly interleaved with the creation of the final artifact. To investigate how this type of design process can be supported, and to address challenges faced by practitioners, this paper presents PatchProv, a system for supporting improvisational quilt design. Based on a review of popular books on improvisational quilting, a set of design principles and key challenges to improvisational quilt design were identified, and PatchProv was developed to support the unique aspects of this process. An evaluation with a small group of quilters showed enthusiasm for the approach and revealed further possibilities for how computational tools can support improvisational quilting and improvisational design practices more broadly.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445601
ER  - 

TY  - NA
AU  - Krosnick, Rebecca
TI  - VL/HCC - Creating Interactive User Interfaces by Demonstration using Crowdsourcing
PY  - 2018
AB  - People are becoming increasingly interested in creating their own digital content and media. This is evident in the enormous number of blogs, personal websites, and portfolios available online. Website templates and creation/hosting services (e.g., Wix, WordPress, Google Sites) have made it possible for even non-programmers to create websites. However, with these services, non-programmers are limited to templates or basic user interface elements and behaviors, lacking the ability to create truly custom web pages that satisfy their needs. More complex and custom user interfaces like digital games and software are virtually impossible for non-programmers to create; even visual programming (e.g., Blockly, GameMaker Studio 2) and data flow languages that try to make computing more approachable still require an understanding of programming and computing concepts. As simple as it is for the average person to sketch a User Interface (UI) on paper or describe it in words, I believe it should be just as easy for them to create the actual digital UI with all of the desired behaviors. Programming should not be a barrier to creating new things and sharing them with the world.
SP  - 277
EP  - 278
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506507
ER  - 

TY  - NA
AU  - Elkin, Lisa A.; Kay, Matthew; Higgins, James J.; Wobbrock, Jacob O.
TI  - UIST - An Aligned Rank Transform Procedure for Multifactor Contrast Tests
PY  - 2021
AB  - Data from multifactor HCI experiments often violates the assumptions of parametric tests (i.e., nonconforming data). The Aligned Rank Transform (ART) has become a popular nonparametric analysis in HCI that can find main and interaction effects in nonconforming data, but leads to incorrect results when used to conduct post hoc contrast tests. We created a new algorithm called ART-C for conducting contrast tests within the ART paradigm and validated it on 72,000 synthetic data sets. Our results indicate that ART-C does not inflate Type I error rates, unlike contrasts based on ART, and that ART-C has more statistical power than a t-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also extended an open-source tool called ARTool with our ART-C algorithm for both Windows and R. Our validation had some limitations (e.g., only six distribution types, no mixed factorial designs, no random slopes), and data drawn from Cauchy distributions should not be analyzed with ART-C.
SP  - 754
EP  - 768
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474784
ER  - 

TY  - NA
AU  - Yamanaka, Shota; Usuba, Hiroki; Takahashi, Haruki; Miyashita, Homei
TI  - UIST - Servo-Gaussian Model to Predict Success Rates in Manual Tracking: Path Steering and Pursuit of 1D Moving Target
PY  - 2020
AB  - We propose a Servo-Gaussian model to predict success rates in continuous manual tracking tasks. Two tasks were conducted to validate this model: path steering and pursuit of a 1D moving target. We hypothesized that (1) hand movements follow the servo-mechanism model, (2) submovement endpoints form a bivariate Gaussian distribution, thus enabling us to predict the success rate at which a submovement endpoint falls inside the tolerance, and (3) the success rate for a whole trial can be predicted if the number of submovements is known. The cross-validation showed R^2>0.92 and MAE0.95 and MAE
SP  - 844
EP  - 857
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415896
ER  - 

TY  - JOUR
AU  - Wicaksono, Irmandy; Hwang, Peter G; Droubi, Samir; Wu, Franny Xi; Serio, Allison N; Yan, Wei; Paradiso, Joseph A
TI  - 3DKnITS: Three-dimensional Digital Knitting of Intelligent Textile Sensor for Activity Recognition and Biomechanical Monitoring.
PY  - 2022
AB  - We present an approach to develop seamless and scalable piezo-resistive matrix-based intelligent textile using digital flat-bed and circular knitting machines. By combining and customizing functional and common yarns, we can design the aesthetics and architecture and engineer both the electrical and mechanical properties of a sensing textile. By incorporating a melting fiber, we propose a method to shape and personalize three-dimensional piezo-resistive fabric structure that can conform to the human body through thermoforming principles. It results in a robust textile structure and intimate interfacing, suppressing sensor drifts and maximizing accuracy while ensuring comfortability. This paper describes our textile design, fabrication approach, wireless hardware system, deep-learning enabled recognition methods, experimental results, and application scenarios. The digital knitting approach enables the fabrication of 2D to 3D pressure-sensitive textile interiors and wearables, including a 45 x 45 cm intelligent mat with 256 pressure-sensing pixels, and a circularly-knitted, form-fitted shoe with 96 sensing pixels across its 3D surface both with linear piezo-resistive sensitivity of 39.4 for up to 500 N load. Our personalized convolutional neural network models are able to classify 7 basic activities and exercises and 7 yoga poses in-real time with 99.6% and 98.7% accuracy respectively. Further, we demonstrate our technology for a variety of applications ranging from rehabilitation and sport science, to wearables and gaming interfaces.
SP  - 2403
EP  - NA
JF  - Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference
VL  - 2022
IS  - NA
PB  - 
DO  - 10.1109/embc48229.2022.9871651
ER  - 

TY  - NA
AU  - Matsuura, Yuki; Terada, Tsutomu; Aoki, Tomohiro; Sonoda, Susumu; Isoyama, Naoya; Tsukamoto, Masahiko
TI  - Readability and legibility of fonts considering shakiness of head mounted displays
PY  - 2019
AB  - In wearable computing environments, users acquire visual information in various scenes using a head mounted display (HMD). However, this induces problems due to their differences from conventional displays such as smartphone and e-books. In this research, we focused on the problem of vertical shock caused by walking. This problem interferes with seeing information on HMDs. In this paper, we discuss selection of font shapes to minimize the effect of this problem. If we can clarify the characteristics of the readability of fonts in wearable computing environments, the application designers can select fonts that strike a balance between intended design elements and readability. In this paper, we first investigated the characteristics of fonts used in Japan considering the HMD swing that results from walking, from the viewpoints of readability (text readability) and legibility (ease of letter recognition) using six different fonts. From this evaluation, we find that fonts with very thin horizontal lines and with very thin horizontal and vertical lines should not be presented on HMDs.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341163.3347748
ER  - 

TY  - JOUR
AU  - Chen, Huijie; Li, Fan; Du, Wan; Yang, Song; Conn, Matthew; Wang, Yu
TI  - Listen to Your Fingers: User Authentication Based on Geometry Biometrics of Touch Gesture
PY  - 2020
AB  - Inputting a pattern or PIN code on the touch screen is a popular method to prevent unauthorized access to mobile devices. However, these sensitive tokens are highly susceptible to being inferred by various types of side-channel attacks, which can compromise the security of the private data stored in the device. This paper presents a second-factor authentication method, TouchPrint, which relies on the user's hand posture shape traits (dependent on the individual different posture type and unique hand geometry biometrics) when the user inputs PIN or pattern. It is robust against the behavioral variability of inputting a passcode and places no restrictions on input manner (e.g., number of the finger touching the screen, moving speed, or pressure). To capture the spatial characteristic of the user's hand posture shape when input the PIN or pattern, TouchPrint performs active acoustic sensing to scan the user's hand posture when his/her finger remains static at some reference positions on the screen (e.g., turning points for the pattern and the number buttons for the PIN code), and extracts the multipath effect feature from the echo signals reflected by the hand. Then, TouchPrint fuses with the spatial multipath feature-based identification results generated from the multiple reference positions to facilitate a reliable and secure MFA system. We build a prototype on smartphone and then evaluate the performance of TouchPrint comprehensively in a variety of scenarios. The experiment results demonstrate that TouchPrint can effectively defend against the replay attacks and imitate attacks. Moreover, TouchPrint can achieve an authentication accuracy of about 92% with only ten training samples.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1145/3411809
ER  - 

TY  - NA
AU  - Auda, Jonas; Busse, Leon; Pfeuffer, Ken; Gruenefeld, Uwe; Rivu, Radiah; Alt, Florian; Schneegass, Stefan
TI  - SUI - I’m in Control! Transferring Object Ownership Between Remote Users with Haptic Props in Virtual Reality
PY  - 2021
AB  - Virtual Reality (VR) remote collaboration is becoming more and more relevant in a wide range of scenarios, such as remote assistance or group work. A way to enhance the user experience is using haptic props that make virtual objects graspable. But physical objects are only present in one location and cannot be manipulated directly by remote users. We explore different strategies to handle ownership of virtual objects enhanced by haptic props. In particular, two strategies of handling object ownership – SingleOwnership and SharedOwnership. SingleOwnership restricts virtual objects to local haptic props, while SharedOwnership allows collaborators to take over ownership of virtual objects using local haptic props. We study both strategies for a collaborative puzzle task regarding their influence on performance and user behavior. Our findings show that SingleOwnership increases communication and enhanced with virtual instructions, results in higher task completion times. SharedOwnership is less reliant on verbal communication and faster, but there is less social interaction between the collaborators.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485287
ER  - 

TY  - NA
AU  - Horst, Robin; Alberternst, Sebastian; Sutter, Jan; Slusallek, Philipp; Kloos, Uwe; Dörner, Ralf
TI  - VISIGRAPP (2: HUCAPP) - Avatar2Avatar: Augmenting the Mutual Visual Communication between Co-located Real and Virtual Environments
PY  - 2019
AB  - Virtual Reality (VR) technology has the potential to support knowledge communication in several sectors. Still, when educators make use of immersive VR technology in favor of presenting their knowledge, their audience within the same room may not be able to see them anymore due to wearing head-mounted displays (HMDs). In this paper, we propose the Avatar2Avatar system and design, which augments the visual aspect during such a knowledge presentation. Avatar2Avatar enables users to see both a realistic representation of their respective counterpart and the virtual environment at the same time. We point out several design aspects of such a system and address design challenges and possibilities that arose during implementation. We specifically explore opportunities of a system design for integrating 2D video-avatars in existing room-scale VR setups. An additional user study indicates a positive impact concerning spatial presence when using Avatar2Avatar.
SP  - 89
EP  - 96
JF  - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications
VL  - 2
IS  - NA
PB  - 
DO  - 10.5220/0007311800890096
ER  - 

TY  - JOUR
AU  - Park, Jaeyoung; Son, Bukun; Han, Ilhwan; Lee, Woochan
TI  - Effect of Cutaneous Feedback on the Perception of Virtual Object Weight during Manipulation.
PY  - 2020
AB  - Haptic interface technologies for virtual reality applications have been developed to increase the reality and manipulability of a virtual object by creating a diverse tactile sensation. Most evaluation of the haptic technologies, however, have been limited to the haptic perception of the tactile stimuli via static virtual objects. Noting this, we investigated the effect of lateral cutaneous feedback, along with kinesthetic feedback on the perception of virtual object weight during manipulation. We modeled the physical interaction between a participant’s finger avatars and virtual objects. The haptic stimuli were rendered with custom-built haptic feedback systems that can provide kinesthetic and lateral cutaneous feedback to the participant. We conducted two virtual object manipulation experiments, 1. a virtual object manipulation with one finger, and 2. the pull-out and lift-up of a virtual object grasped with a precision grip. The results of Experiment 1 indicate that the participants felt the virtual object rendered with lateral cutaneous feedback significantly heavier than with only kinesthetic feedback (p < 0.05 for mref = 100 and 200 g). Similarly, the participants of Experiment 2 felt the virtual objects significantly heavier when lateral cutaneous feedback was available (p < 0.05 for mref = 100, 200, and 300 g). Therefore, the additional lateral cutaneous feedback to the force feedback led the participants to feel the virtual object heavier than without the cutaneous feedback. The results also indicate that the contact force applied to a virtual object during manipulation can be a function of the perceived object weight (p = 0.005 for Experiment 1 and p = 0.02 for Experiment 2).
SP  - 1357
EP  - 1357
JF  - Scientific reports
VL  - 10
IS  - 1
PB  - 
DO  - 10.1038/s41598-020-58247-5
ER  - 

TY  - NA
AU  - Olwal, Alex; Starner, Thad; Mainini, Gowa
TI  - CHI - E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics
PY  - 2020
AB  - E-textile microinteractions advance cord-based interfaces by enabling the simultaneous use of precise continuous control and casual discrete gestures. We leverage the recently introduced I/O Braid sensing architecture to enable a series of user studies and experiments which help design suitable interactions and a real-time gesture recognition pipeline. Informed by a gesture elicitation study with 36 participants, we developed a user-dependent classifier for eight discrete gestures with 94% accuracy for 12 participants. In a formal evaluation we show that we can enable precise manipulation with the same architecture. Our quantitative targeting experiment suggests that twisting is faster than existing headphone button controls and is comparable in speed to a capacitive touch surface. Qualitative interview feedback indicates a preference for I/O Braid's interaction over that of in-line headphone controls. Our applications demonstrate how continuous and discrete gestures can be combined to form new, integrated e-textile microinteraction techniques for real-time continuous control, discrete actions and mode switching.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376236
ER  - 

TY  - JOUR
AU  - Zhao, Ziqi; Yeo, Minku; Manoharan, Stefan; Ryu, Seok Chang; Park, Hangue
TI  - Electrically-Evoked Proximity Sensation Can Enhance Fine Finger Control in Telerobotic Pinch.
PY  - 2020
AB  - For teleoperation tasks requiring high control accuracy, it is essential to provide teleoperators with information on the interaction between the end effector and the remote environment. Real-time imaging devices have been widely adopted, but it delivers limited information, especially when the end effectors approach the target following the line-of-sight. In such situations, teleoperators rely on the perspective at the screen and can apply high force unintentionally at the initial contact. This research proposes to deliver the distance information at teleoperation to the fingertips of teleoperators, i.e., proximity sensation. Transcutaneous electrical stimulation was applied onto the fingertips of teleoperators, with the pulsing frequency inversely proportional to the distance. The efficacy of the proximity sensation was evaluated by the initial contact force during telerobotic pinch in three sensory conditions: vision only, vision + visual assistance (distance on the screen), and vision + proximity sensation. The experiments were repeated at two viewing angles: 30–60° and line-of-sight, for eleven healthy human subjects. For both cases, the initial contact force could be significantly reduced by either visual assistance (20–30%) or the proximity sensation (60–70%), without additional processing time. The proximity sensation is two-to-three times more effective than visual assistance regarding the amount of force reduction.
SP  - 163
EP  - 163
JF  - Scientific reports
VL  - 10
IS  - 1
PB  - 
DO  - 10.1038/s41598-019-56985-9
ER  - 

TY  - CHAP
AU  - Gueorguiev, David; Javot, Bernard; Spiers, Adam; Kuchenbecker, Katherine J.
TI  - Larger Skin-Surface Contact Through a Fingertip Wearable Improves Roughness Perception
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>With the aim of creating wearable haptic interfaces that allow the performance of everyday tasks, we explore how differently designed fingertip wearables change the sensory threshold for tactile roughness perception. Study participants performed the same two-alternative forced-choice roughness task with a bare finger and wearing three flexible fingertip covers: two with a square opening (64 and 36 mm<jats:inline-formula><jats:alternatives><jats:tex-math>$$^2$$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:msup> <mml:mrow /> <mml:mn>2</mml:mn> </mml:msup> </mml:math></jats:alternatives></jats:inline-formula>, respectively) and the third with no opening. The results showed that adding the large opening improved the 75% JND by a factor of 2 times compared to the fully covered finger: the higher the skin-surface contact area, the better the roughness perception. Overall, the results show that even partial skin-surface contact through a fingertip wearable improves roughness perception, which opens design opportunities for haptic wearables that preserve natural touch.</jats:p>
SP  - 171
EP  - 179
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06249-0_20
ER  - 

TY  - JOUR
AU  - Gardony, Aaron L.; Martis, Shaina B.; Taylor, Holly A.; Brunyé, Tad T.
TI  - Interaction Strategies for Effective Augmented Reality Geo-Visualization: Insights from Spatial Cognition
PY  - 2018
AB  - Standalone augmented reality (AR) systems have great potential for interactive three-dimensional (3D) geo-visualization. Emerging head-worn AR technologies can display rich graphical imagery of lar...
SP  - 107
EP  - 149
JF  - Human–Computer Interaction
VL  - 36
IS  - 2
PB  - 
DO  - 10.1080/07370024.2018.1531001
ER  - 

TY  - CONF
AU  - Steed, Anthony; Friston, Sebastian; Pawar, Vijay; Swapp, David
TI  - VRST - Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations of Grounded and Worn Devices
PY  - 2020
AB  - Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user’s location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Shull, Peter B.; Tan, Tian; Culbertson, Heather; Zhu, Xiangyang; Okamura, Allison M.
TI  - Resonant Frequency Skin Stretch for Wearable Haptics
PY  - 2019
AB  - Resonant frequency skin stretch uses cyclic lateral skin stretches matching the skin's resonant frequency to create highly noticeable stimuli, signifying a new approach for wearable haptic stimulation. Four experiments were performed to explore biomechanical and perceptual aspects of resonant frequency skin stretch. In the first experiment, effective skin resonant frequencies were quantified at the forearm, shank, and foot. In the second experiment, perceived haptic stimuli were characterized for skin stretch actuations across a spectrum of frequencies. In the third experiment, perceived haptic stimuli were characterized by different actuator masses. In the fourth experiment, haptic classification ability was determined as subjects differentiated haptic stimulation cues while sitting, walking, and jogging. Results showed that subjects perceived stimulations at, above, and below the skin's resonant frequency differently: stimulations lower than the skin resonant frequency felt like distinct impacts, stimulations at the skin resonant frequency felt like cyclic skin stretches, and stimulations higher than the skin resonant frequency felt like standard vibrations. Subjects successfully classified stimulations while sitting, walking, and jogging, perceived haptic stimuli was affected by actuator mass, and classification accuracy decreased with increasing speed, especially for stimulations at the shank. This paper could facilitate more widespread use of wearable skin stretch. Potential applications include gaming, medical simulation, and surgical augmentation, and for training to reduce injury risk or improve sports performance.
SP  - 247
EP  - 256
JF  - IEEE transactions on haptics
VL  - 12
IS  - 3
PB  - 
DO  - 10.1109/toh.2019.2917072
ER  - 

TY  - JOUR
AU  - Luo, Yang; Xiao, Xiao; Chen, Jun; Li, Qian; Fu, Hongyan
TI  - Machine-Learning-Assisted Recognition on Bioinspired Soft Sensor Arrays.
PY  - 2022
AB  - Soft interfaces with self-sensing capabilities play an essential role in environment awareness and reaction. The growing overlap between materials and sensory systems has created a myriad of challenges for sensor integration, including the design of a multimodal sensory, simplified system design capable of high spatiotemporal sensing resolution and efficient processing methods. Here we report a bioinspired soft sensor array (BOSSA) that integrates pressure and material sensing capabilities based on the triboelectric effect. Cascaded row + column electrodes embedded in low-modulus porous silicone rubber allow rich information to be captured from the environment and further analyzed by data-driven algorithms (multilayer perceptrons) to extract higher level features. BOSSA demonstrates the ability to identify 10 users (98.9%) and identify the placement or extraction of 10 objects (98.6%). Moreover, its scalable fabrication facilitates large-area sensor arrays with high spatiotemporal resolution and multimodal sensing abilities yet with a less complex system architecture. These features may be promising in the development of immersive sensing networks for intelligent monitoring and stimuli response in smart home/industry applications.
SP  - 6734
EP  - 6743
JF  - ACS nano
VL  - 16
IS  - 4
PB  - 
DO  - 10.1021/acsnano.2c01548
ER  - 

TY  - BOOK
AU  - Çamci, Anil; Çakmak, Cem; Forbes, Angus G.
TI  - New Directions in Music and Human-Computer Interaction - Applying Game Mechanics to Networked Music HCI Systems
PY  - 2019
AB  - We discuss the use of game mechanics as a means to facilitate collaboration in networked music performances. We first look at core concepts of gaming and how these relate to musical creativity and performance. We offer an overview of various perspectives towards game mechanics and rule systems with examples from video games and musical practices. We then describe audiovisual software that we developed in order to study the integration of game-like elements and mechanics into networked music performance. Finally, we offer the results of a preliminary user study conducted using this system in private and public performance contexts. We observe that imposing various game mechanics upon a networked performance affects the nature of the musical collaboration as well as the audience’s attentiveness towards the performance.
SP  - 223
EP  - 241
JF  - New Directions in Music and Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-92069-6_14
ER  - 

TY  - BOOK
AU  - Cabrera, Miguel Altamirano; Tsetserukou, Dzmitry
TI  - AsiaHaptics - LinkGlide: A Wearable Haptic Display with Inverted Five-Bar Linkages for Delivering Multi-contact and Multi-modal Tactile Stimuli
PY  - 2019
AB  - LinkGlide is a novel wearable palm-worn tactile display to deliver multicontact and multimodal stimuli at the user’s palm. The array of inverted five-bar linkages generate three independent contact points to cover the whole palm area. The independent contact points allow to control different patterns in each of the points and provide multimodal tactile feedback, such as slippage, force vector, pressure, temperature, and vibration. With this novel haptic device, we can potentially achieve a highly immersive VR experience and make it more interactive.
SP  - 149
EP  - 154
JF  - Lecture Notes in Electrical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-13-3194-7_33
ER  - 

TY  - NA
AU  - Ali, Abdullah; Morris, Meredith Ringel; Wobbrock, Jacob O.
TI  - CHI - Crowdlicit: A System for Conducting Distributed End-User Elicitation and Identification Studies
PY  - 2019
AB  - End-user elicitation studies are a popular design method. Currently, such studies are usually confined to a lab, limiting the number and diversity of participants, and therefore the representativeness of their results. Furthermore, the quality of the results from such studies generally lacks any formal means of evaluation. In this paper, we address some of the limitations of elicitation studies through the creation of the Crowdlicit system along with the introduction of end-user identification studies, which are the reverse of elicitation studies. Crowdlicit is a new web-based system that enables researchers to conduct online and in-lab elicitation and identification studies. We used Crowdlicit to run a crowd-powered elicitation study based on Morris's "Web on the Wall" study (2012) with 78 participants, arriving at a set of symbols that included six new symbols different from Morris's. We evaluated the effectiveness of 49 symbols (43 from Morris and six from Crowdlicit) by conducting a crowd-powered identification study. We show that the Crowdlicit elicitation study resulted in a set of symbols that was significantly more identifiable than Morris's.
SP  - 255
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300485
ER  - 

TY  - NA
AU  - Nancel, Mathieu; Aranovskiy, Stanislav; Ushirobira, Rosane; Efimov, Denis; Poulmane, Sebastien; Roussel, Nicolas; Casiez, Géry
TI  - UIST - Next-Point Prediction for Direct Touch Using Finite-Time Derivative Estimation
PY  - 2018
AB  - End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.
SP  - 793
EP  - 807
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242646
ER  - 

TY  - NA
AU  - Harley, Daniel; Tarun, Aneesh P.; Elsharawy, Sara; Verni, Alexander; Tibu, Tudor; Bilic, Marko; Bakogeorge, Alexander; Mazalek, Ali
TI  - Conference on Designing Interactive Systems - Mobile Realities: Designing for the Medium of Smartphone-VR
PY  - 2019
AB  - We present two proof of concept experiences for a virtual reality (VR) game that draws on several medium-specific qualities of mobile, location-based, and tangible storytelling. In contemporary smartphone-VR, experiences are limited by short playtimes, limited interactions, and limited movement within a physical space. To address these limitations, we suggest a reconceptualization of smartphone-VR. Rather than design that deems the smartphone the least capable VR platform, we propose design that adds VR to an already rich mobile storytelling platform. We argue that by drawing on otherwise separate storytelling media, designers can circumvent limitations related to smartphone-VR while also extending the range of smartphone-based storytelling. We conclude by reflecting on possible implications of this extended design space.
SP  - 1131
EP  - 1144
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322341
ER  - 

TY  - NA
AU  - Kunihiro, Mio; Ohta, Masaya
TI  - GCCE - A Prototype of Smart Parts Supporting Electrical Experiments at School
PY  - 2020
AB  - Smart Parts (SP) proposed in this study is a novel hardware tool for electrical circuit experiments at primary, middle and high schools. SP can be “transformed” into any electrical part, instrument, or DC/AC power supply. With conventional electrical circuit experiments at schools, it is difficult for the teacher in a large classroom to check whether each student is creating unsafe circuits, such as with a short circuit. In this study, we propose the SP method to solve this problem, implement a prototype, and conduct simple experiments.
SP  - 859
EP  - 860
JF  - 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/gcce50665.2020.9292005
ER  - 

TY  - NA
AU  - Sinclair, Mike; Ofek, Eyal; Gonzalez-Franco, Mar; Holz, Christian
TI  - UIST - CapstanCrunch: A Haptic VR Controller with User-supplied Force Feedback
PY  - 2019
AB  - We introduce CapstanCrunch, a force resisting, palm-grounded haptic controller that renders haptic feedback for touching and grasping both rigid and compliant objects in a VR environment. In contrast to previous controllers, Cap-stan¬Crunch renders human-scale forces without the use of large, high force, electrically power consumptive and ex-pensive actuators. Instead, CapstanCrunch¬ integrates a friction-based capstan-plus-cord variable-resistance brake mechanism that is dynamically controlled by a small inter-nal motor. The capstan mechanism magnifies the motor's force by a factor of around 40 as an output resistive force. Compared to active force control devices, it is low cost, low electrical power, robust, safe, fast and quiet, while providing high force control to user interaction. We describe the de-sign and implementation of CapstanCrunch and demon-strate its use in a series of VR scenarios. Finally, we evalu-ate the performance of CapstanCrunch in two user studies and compare our controller with an active haptic controller with the ability to simulate different levels of convincing object rigidity and/or compliance.
SP  - 815
EP  - 829
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347891
ER  - 

TY  - NA
AU  - Park, Hyojoon; Park, Jung-Min
TI  - IROS - Adaptive Precision-Enhancing Hand Rendering for Wearable Fingertip Tracking Devices
PY  - 2020
AB  - We introduce a 3D hand rendering framework to reconstruct a visually realistic hand from a set of fingertip positions. One of the key limitations of wearable fingertip tracking devices used in VR/AR applications is the lack of detailed measurements and tracking of the hand, making the hand rendering difficult. The motivation for this paper is to develop a general framework to render a visually plausible hand given only the fingertip positions. In addition, our framework adjusts the size of a virtual hand based on the fingertip positions and device's structure, and reduces a mismatch between the pose of the rendered and user's hand by retargeting virtual finger motions. Moreover, we impose a new hinge constraint on the finger model to employ a real-time inverse kinematic solver. We show our framework is helpful for performing virtual grasping tasks more efficiently when only the measurements of fingertip positions are available.
SP  - 11452
EP  - 11457
JF  - 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iros45743.2020.9341459
ER  - 

TY  - NA
AU  - Lo, Jo-Yu; Huang, Da-Yuan; Sun, Chen-Kuo; Hou, Chu-En; Chen, Bing-Yu
TI  - UIST - RollingStone: Using Single Slip Taxel for Enhancing Active Finger Exploration with a Virtual Reality Controller
PY  - 2018
AB  - We propose using a single slip tactile pixel on virtual reality controllers to produce sensations of finger sliding and textures. When a user moves the controller on a virtual surface, we add a slip opposite to the movement, creating an illusion of a finger that is sliding on the surface, while varying the slip feedback changes lateral forces on fingertip. When coupled with hand motion the lateral forces can be used to create perceptions of artificial textures. RollingStone has been implemented as a prototype VR controller consisting of a ball-based slip display positioned under the user's fingertip. Within the slip display, a pair of motors actuates the ball, which is capable of gener- ating both short- and long-term two-degree-of-freedom slip feedback. An exploratory study was conducted to ensure that changing the relative motion between the finger and the ball could alter the perceptions conveying the properties of a tex- ture. The following two perception-based studies examined the minimum changes in speed of slip and angle of slip that are detectable by users. The results help us to design haptic patterns as well as our prototype applications. Finally, our preliminary user evaluation indicated that participants wel- comed RollingStone as a useful addition to the range of VR controllers.
SP  - 839
EP  - 851
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242627
ER  - 

TY  - NA
AU  - Silva, Hugo; Resende, Ricardo Pontes; Breternitz, Mauricio
TI  - Mixed reality application to support infrastructure maintenance
PY  - 2018
AB  - This paper presents a mixed reality (MR) application developed for the head-mounted display Microsoft HoloLens that supports infrastructure maintenance works in buildings with complex infrastructure. The solution is intended to help maintenance workers when they need to track and fix part of the infrastructure by revealing hidden infrastructure, displaying additional information and guiding workers in complex tasks. The application has the potential to improve maintenance worker's tasks as it can help them perform faster and with more accuracy. The work explores the creation of the application and discusses the methodologies used to build an optimal and user-friendly tool. The methodology is based on design science research: an improvement need, and not necessarily a problem, was identified, and from there a solution was conceived. MR has proven to be a major tool for helping in several areas, and this paper can give insights for many future solutions with mixed reality or HoloLens and help them build new and better applications to improve tasks at a job or at home.
SP  - 50
EP  - 54
JF  - 2018 International Young Engineers Forum (YEF-ECE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/yef-ece.2018.8368938
ER  - 

TY  - JOUR
AU  - An, Jaepung; Choi, Gyujin; Chun, Wooyoung; Joo, Yesle; Park, Sanghun; Ihm, Insung
TI  - Accurate and stable alignment of virtual and real spaces using consumer-grade trackers
PY  - 2021
AB  - The world space defined by a traditional virtual reality application is generally regarded as separate from the real-world space in which users actually exist. In this paper, we present a method that enables such detached spaces to connect in an integrated space. In particular, we show that by using a specially manufactured calibration board and three consumer-grade position tracking devices, a reference coordinate system can easily be set up in the physical space, whose geometric relationship with the virtual reality space is estimated with high numerical accuracy and stability. Then, we demonstrate that, combined with traditional computer vision techniques for marker tracking, the presented technique allows colocated users from virtual, augmented, and mixed realities to cooperate with each other while making effective use of technologies from other realities.
SP  - 1
EP  - 17
JF  - Virtual Reality
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Antoine, Axel; Nancel, Mathieu; Ge, Ella; Zheng, Jingjie; Zolghadr, Navid; Casiez, Géry
TI  - UIST - Modeling and Reducing Spatial Jitter caused by Asynchronous Input and Output Rates
PY  - 2020
AB  - Jitter in interactive systems occurs when visual feedback is perceived as unstable or trembling even though the input signal is smooth or stationary. It can have multiple causes such as sensing noise, or feedback calculations introducing or exacerbating sensing imprecisions. Jitter can however occur even when each individual component of the pipeline works perfectly, as a result of the differences between the input frequency and the display refresh rate. This asynchronicity can introduce rapidly-shifting latencies between the rendered feedbacks and their display on screen, which can result in trembling cursors or viewports. % This paper contributes a better understanding of this particular type of jitter. We first detail the problem from a mathematical standpoint, from which we develop a predictive model of jitter amplitude as a function of input and output frequencies, and a new metric to measure this spatial jitter. Using touch input data gathered in a study, we developed a simulator to validate this model and to assess the effects of different techniques and settings with any output frequency. The most promising approach, when the time of the next display refresh is known, is to estimate (interpolate or extrapolate) the user's position at a fixed time interval before that refresh. % When input events occur at 125~Hz, as is common in touch screens, we show that an interval of 4 to 6~ms works well for a wide range of display refresh rates. This method effectively cancels most of the jitter introduced by input/output asynchronicity, while introducing minimal imprecision or latency.
SP  - 869
EP  - 881
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415833
ER  - 

TY  - CHAP
AU  - Karaca, Ozan; Demir, Kadir; Çalişkan, S. Ayhan
TI  - Virtual, Augmented and Mobile Learning in Health Education
PY  - 2018
AB  - <jats:p>Currently, technology is used for various purposes in every aspect of life. One of these areas is the field of education, which has an important role in advancing the technologies. Educators often desire to concentrate on their own courses or fields. However, the changing perception of the students and the technologies they are interested in are necessitating educators to follow technological innovations and developments in instructional technologies. The objective of this chapter is to attract attention to some of the global virtual, augmented, and mobile applications that are currently utilized within the medical learning arena. The authors also address technologies that aim to train human resources in health and other fields. The chapter illuminates the use of virtual, augmented, and mobile learning environments via their use in health education.</jats:p>
SP  - 168
EP  - 195
JF  - Computer-Mediated Learning for Workforce Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.4018/978-1-5225-4111-0.ch009
ER  - 

TY  - JOUR
AU  - Jindal, Swati; Kaur, Harsimran; Manduchi, Roberto
TI  - Tracker/Camera Calibration for Accurate Automatic Gaze Annotation of Images and Videos.
PY  - 2022
AB  - Modern appearance-based gaze tracking algorithms require vast amounts of training data, with images of a viewer annotated with "ground truth" gaze direction. The standard approach to obtain gaze annotations is to ask subjects to fixate at specific known locations, then use a head model to determine the location of "origin of gaze". We propose using an IR gaze tracker to generate gaze annotations in natural settings that do not require the fixation of target points. This requires prior geometric calibration of the IR gaze tracker with the camera, such that the data produced by the IR tracker can be expressed in the camera's reference frame. This contribution introduces a simple tracker/camera calibration procedure based on the PnP algorithm and demonstrates its use to obtain a full characterization of gaze direction that can be used for ground truth annotation.
SP  - NA
EP  - NA
JF  - Proceedings. Eye Tracking Research & Applications Symposium
VL  - 2022
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529643
ER  - 

TY  - NA
AU  - Walker, Julie; Zemiti, Nabil; Poignet, Philippe; Okamura, Allison M.
TI  - WHC - Holdable Haptic Device for 4-DOF Motion Guidance
PY  - 2019
AB  - Hand-held haptic devices can allow for greater freedom of motion and larger workspaces than traditional grounded haptic devices. They can also provide more compelling haptic sensations to the users’ fingertips than many wearable haptic devices because reaction forces can be distributed over a larger area of skin far away from the stimulation site. This paper presents a hand-held kinesthetic gripper that provides guidance cues in four degrees of freedom (DOF). 2-DOF tangential forces on the thumb and index finger combine to create cues to translate or rotate the hand. We demonstrate the device’s capabilities in a three-part user study. First, users moved their hands in response to haptic cues before receiving instruction or training. Then, they trained on cues in eight directions in a forced-choice task. Finally, they repeated the first part, now knowing what each cue intended to convey. Users were able to discriminate each cue over 90% of the time. Users moved correctly in response to the guidance cues both before and after the training and indicated that the cues were easy to follow. The results show promise for holdable kinesthetic devices for haptic feedback and guidance in applications such as medical training, teleoperation, and virtual reality.
SP  - 109
EP  - 114
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816171
ER  - 

TY  - JOUR
AU  - Phillip, Won; Jeong, Seongmin; Majidi, Carmel; Ko, Seung Hwan
TI  - Recent advances in liquid-metal-based wearable electronics and materials.
PY  - 2021
AB  - Soft wearable electronics are rapidly developing through exploration of new materials, fabrication approaches, and design concepts. Although there have been many efforts for decades, a resurgence of interest in liquid metals (LMs) for sensing and wiring functional properties of materials in soft wearable electronics has brought great advances in wearable electronics and materials. Various forms of LMs enable many routes to fabricate flexible and stretchable sensors, circuits, and functional wearables with many desirable properties. This review article presents a systematic overview of recent progresses in LM-enabled wearable electronics that have been achieved through material innovations and the discovery of new fabrication approaches and design architectures. We also present applications of wearable LM technologies for physiological sensing, activity tracking, and energy harvesting. Finally, we discuss a perspective on future opportunities and challenges for wearable LM electronics as this field continues to grow.
SP  - 102698
EP  - 102698
JF  - iScience
VL  - 24
IS  - 7
PB  - 
DO  - 10.1016/j.isci.2021.102698
ER  - 

TY  - NA
AU  - Bergström, Joanna; Mottelson, Aske; Knibbe, Jarrod
TI  - UIST - Resized Grasping in VR: Estimating Thresholds for Object Discrimination
PY  - 2019
AB  - Previous work in VR has demonstrated how individual physical objects can represent multiple virtual objects in different locations by redirecting the user's hand. We show how individual objects can represent multiple virtual objects of different sizes by resizing the user's grasp. We redirect the positions of the user's fingers by visual translation gains, inducing an illusion that can make physical objects seem larger or smaller. We present a discrimination experiment to estimate the thresholds of resizing virtual objects from physical objects, without the user reliably noticing a difference. The results show that the size difference is easily detected when a physical object is used to represent an object less than 90% of its size. When physical objects represent larger virtual objects, however, then scaling is tightly coupled to the physical object's size: smaller physical objects allow more virtual resizing (up to a 50% larger virtual size). Resized Grasping considerably broadens the scope of using illusions to provide rich haptic experiences in virtual reality.
SP  - 1175
EP  - 1183
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347939
ER  - 

TY  - JOUR
AU  - Raees, Muhammad; Ullah, Sehat
TI  - THE-3DI: Tracing head and eyes for 3D interactions
PY  - 2019
AB  - Gesture-based interfaces offer a suitable platform for interactions in Virtual Environments (VE). However, the difficulties involved in learning and making of distinct gestures affect the performance of an interactive system. By incorporating computer vision in Virtual Reality (VR), this paper presents an intuitive interaction technique where the states and positions of eyes are traced for interaction. With comparatively low cognitive load, the technique offers an easy to use interaction solution for VR applications. Unlike other gestural interfaces, interactions are performed in distinct phases where transition from one phase to another is enacted with simple blink of eyes. In an attained phase, interaction along an arbitrary axis is performed by a perceptive gesture of head; rolling, pitching or yawing. To follow the trajectory of eyes in real time, coordinates mapping is performed dynamically. The proposed technique is implemented in a case-study project; EBI (Eyes Blinking based Interaction). In the EBI project, real time detection and tracking of eyes are performed at the back-end. At the front-end, virtual scene is rendered accordingly by using the OpenGL library. To assess accuracy, usability and cognitive load of the proposed technique, the EBI project is evaluated 280 times in three different evaluation sessions. With an ordinary camera, an average accuracy of 81.4% is achieved. However, assessment made by using a high-quality camera revealed that accuracy of the system could be raised to a higher level. As a whole, findings of the evaluations support applicability of the technique in the emerging domains of VR.
SP  - 1311
EP  - 1337
JF  - Multimedia Tools and Applications
VL  - 79
IS  - 1
PB  - 
DO  - 10.1007/s11042-019-08305-6
ER  - 

TY  - JOUR
AU  - Kudry, Peter; Cohen, Michael
TI  - Development of a wearable force-feedback mechanism for free-range haptic immersive experience
PY  - 2022
AB  - <jats:p>The recent rise in popularity of head-mounted displays (HMDs) for immersion into virtual reality has resulted in demand for new ways to interact with virtual objects. Most solutions utilize generic controllers for interaction within virtual environments and provide limited haptic feedback. We describe the construction and implementation of an ambulatory (allowing walking) haptic feedback stylus with primary use in computer-aided design. Our stylus is a modified 3D Systems Touch force-feedback arm mounted on a wearable platform carried in front of a user. The wearable harness also holds a full-sized laptop, which drives the Meta Quest 2 HMD that is also worn by the user. This design provides six degrees-of-freedom without tethered limitations, while ensuring a high precision of force-feedback from virtual interaction. Our solution also provides an experience wherein a mobile user can explore different haptic feedback simulations and create, arrange, and deform general shapes.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 3
IS  - NA
PB  - 
DO  - 10.3389/frvir.2022.824886
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Li, Pengyu; Nith, Romain; Fonseca, Joshua; Lopes, Pedro
TI  - CHI - Touch&Fold: A Foldable Haptic Actuator for Rendering Touch in Mixed Reality
PY  - 2021
AB  - We propose a nail-mounted foldable haptic device that provides tactile feedback to mixed reality (MR) environments by pressing against the user's fingerpad when a user touches a virtual object. What is novel in our device is that it quickly tucks away when the user interacts with real-world objects. Its design allows it to fold back on top of the user's nail when not in use, keeping the user's fingerpad free to, for instance, manipulate handheld tools and other objects while in MR. To achieve this, we engineered a wireless and self-contained haptic device, which measures 24×24×41 mm and weighs 9.5 g. Furthermore, our foldable end-effector also features a linear resonant actuator, allowing it to render not only touch contacts (i.e., pressure) but also textures (i.e., vibrations). We demonstrate how our device renders contacts with MR surfaces, buttons, low- and high-frequency textures. In our first user study, we found that participants perceived our device to be more realistic than a previous haptic device that also leaves the fingerpad free (i.e., fingernail vibration). In our second user study, we investigated the participants’ experience while using our device in a real-world task that involved physical objects. We found that our device allowed participants to use the same finger to manipulate handheld tools, small objects, and even feel textures and liquids, without much hindrance to their dexterity, while feeling haptic feedback when touching MR interfaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445099
ER  - 

TY  - NA
AU  - Aigner, Roland; Pointner, Andreas; Preindl, Thomas; Parzer, Patrick; Haller, Michael J.
TI  - CHI - Embroidered Resistive Pressure Sensors: A Novel Approach for Textile Interfaces
PY  - 2020
AB  - We present a novel method for augmenting arbitrary fabrics with textile-based pressure sensors using an off-the-shelf embroidery machine. We apply resistive textiles and conductive yarns on top of a base fabric, to yield a flexible and versatile continuous sensing device, which is based on the widespread principle of force sensitive resistors. The patches can easily be attached to measurement and/or computing devices, e.g. for controlling accessories. In this paper, we investigate the impacts of related design and fabrication parameters, introduce five different pattern designs, and discuss their pros and cons. We present crucial insights and recommendations for design and manufacturing of embroidered pressure sensors. Our sensors show a very low activation threshold, as well as good dynamic range, signal-to-noise ratio, and part-to-part repeatability.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376305
ER  - 

TY  - NA
AU  - Gong, Jun; Wu, Yu; Yan, Lei; Seyed, Teddy; Yang, Xing-Dong
TI  - UIST - Tessutivo: Contextual Interactions on Interactive Fabrics with Inductive Sensing
PY  - 2019
AB  - We present Tessutivo, a contact-based inductive sensing technique for contextual interactions on interactive fabrics. Our technique recognizes conductive objects (mainly metallic) that are commonly found in households and workplaces, such as keys, coins, and electronic devices. We built a prototype containing six by six spiral-shaped coils made of conductive thread, sewn onto a four-layer fabric structure. We carefully designed the coil shape parameters to maximize the sensitivity based on a new inductance approximation formula. Through a ten-participant study, we evaluated the performance of our proposed sensing technique across 27 common objects. We yielded 93.9% real-time accuracy for object recognition. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique.
SP  - 29
EP  - 41
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347897
ER  - 

TY  - NA
AU  - Riva, Oriana; Kace, Jason
TI  - UIST - Etna: Harvesting Action Graphs from Websites
PY  - 2021
AB  - Knowledge bases, such as Google knowledge graph, contain millions of entities (people, places, etc.) and billions of facts about them. While much is known about entities, little is known about the actions these entities relate to. On the other hand, the Web has lots of information about human tasks. A website for restaurant reservations, for example, implicitly knows about various restaurant-related actions (making reservations, delivering food, etc.), the inputs these actions require and their expected output; it can also be automated to execute those actions. To harvest action knowledge from websites, we propose Etna. Users demonstrate how to accomplish various tasks in a website, and Etna constructs an action-state model of the website visualized as an action graph. An action graph includes definitions of tasks and actions, knowledge about their start/end states, and execution scripts for their automation. We report on our experience in building action-state models of many commercial websites and use cases that leveraged them.
SP  - 312
EP  - 331
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474752
ER  - 

TY  - NA
AU  - He, Han; Chen, Xiaochen; Raivio, Leevi; Huttunen, Heikki; Virkki, Johanna
TI  - Passive RFID-based Textile Touchpad
PY  - 2020
AB  - This paper presents the first prototype of a passive RFID-based textile touchpad. Our unique solution takes advantage of ICs from passive UHF RFID technology. These components are combined into a textile-integrated IC array, which can be used for handwritten character recognition. As the solution is fully passive and gets all the needed energy from the RFID reader, it enables a maintenance-free and cost-effective user interface that can be integrated into clothing and into textiles around us.
SP  - 1
EP  - 4
JF  - 2020 14th European Conference on Antennas and Propagation (EuCAP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/eucap48036.2020.9135201
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Riva, Oriana
TI  - MobiSys - Kite: Building Conversational Bots from Mobile Apps
PY  - 2018
AB  - Task-oriented chatbots allow users to carry out tasks (e.g., ordering a pizza) using natural language conversation. The widely-used slot-filling approach for building bots of this type requires significant hand-coding, which hinders scalability. Recently, neural network models have been shown to be capable of generating natural "chitchat" conversations, but it is unclear whether they will ever work for task modeling. Kite is a practical system for bootstrapping task-oriented bots, leveraging both approaches above. Kite's key insight is that while bots encapsulate the logic of user tasks into conversational forms, existing apps encapsulate the logic of user tasks into graphical user interfaces. A developer demonstrates a task using a relevant app, and from the collected interaction traces Kite automatically derives a task model, a graph of actions and associated inputs representing possible task execution paths. A task model represents the logical backbone of a bot, on which Kite layers a question-answer interface generated using a hybrid rule-based and neural network approach. Using Kite, developers can automatically generate bot templates for many different tasks. In our evaluation, it extracted accurate task models from 25 popular Android apps spanning 15 tasks. Appropriate questions and high-quality answers were also generated. Our developer study suggests that developers, even without any bot developing experience, can successfully generate bot templates using Kite.
SP  - 96
EP  - 109
JF  - Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210240.3210339
ER  - 

TY  - BOOK
AU  - Berry, Justin; Chantiles-Wertz, Lance; Shelanski, Isaac
TI  - GEM - The Clamshell: Rethinking the Virtual Reality Interface
PY  - 2019
AB  - This project seeks to challenge the dominance of button-based interfacing with Virtual Reality by creating a platform with which anyone can customize their own virtual reality controller based on any sensors of their choosing. After two prototypes and as many demonstrations, the project has successfully yielded a proof of concept that showed results both in the technical creation of a custom VR controller and in the effects of creative rethinking of the physical interface on the VR experience.
SP  - 1
EP  - 5
JF  - 2019 IEEE Games, Entertainment, Media Conference (GEM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/gem.2019.8811553
ER  - 

TY  - JOUR
AU  - Tilmon, Brevin; Jain, Eakta; Ferrari, Silvia; Koppal, Sanjeev J.
TI  - Fast Foveating Cameras for Dense Adaptive Resolution
PY  - 2022
AB  - Traditional cameras field of view (FOV) and resolution predetermine computer vision algorithm performance. These trade-offs decide the range and performance in computer vision algorithms. We present a novel foveating camera whose viewpoint is dynamically modulated by a programmable micro-electromechanical (MEMS) mirror, resulting in a natively high-angular resolution wide-FOV camera capable of densely and simultaneously imaging multiple regions of interest in a scene. We present calibrations, novel MEMS control algorithms, a real-time prototype, and comparisons in remote eye-tracking performance against a traditional smartphone, where high-angular resolution and wide-FOV are necessary, but traditionally unavailable.
SP  - 1
EP  - 1
JF  - IEEE transactions on pattern analysis and machine intelligence
VL  - 44
IS  - 9
PB  - 
DO  - 10.1109/tpami.2021.3071588
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Lin, Cheng-Lung; Chiang, Chi-huan; Kuo, Tzu-Sheng; Chan, Liwei; Huang, Da-Yuan; Chen, Bing-Yu
TI  - UIST - TilePoP: Tile-type Pop-up Prop for Virtual Reality
PY  - 2019
AB  - We present TilePoP, a new type of pneumatically-actuated interface deployed as floor tiles which dynamically pop up by inflating into large shapes constructing proxy objects for whole-body interactions in Virtual Reality. TilePoP consists of a 2D array of stacked cube-shaped airbags designed with specific folding structures, enabling each airbag to be inflated into a physical proxy and then deflated down back to its original tile shape when not in use. TilePoP is capable of providing haptic feedback for the whole body and can even support human body weight. Thus, it allows new interaction possibilities in VR. Herein, the design and implementation of TilePoP are described in detail along with demonstrations of its applications and the results of a preliminary user evaluation conducted to understand the users' experience with TilePoP.
SP  - 639
EP  - 649
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347958
ER  - 

TY  - NA
AU  - Strasnick, Evan; Holz, Christian; Ofek, Eyal; Sinclair, Mike; Benko, Hrvoje
TI  - CHI - Haptic Links: Bimanual Haptics for Virtual Reality Using Variable Stiffness Actuation
PY  - 2018
AB  - We present Haptic Links, electro-mechanically actuated physical connections capable of rendering variable stiffness between two commodity handheld virtual reality (VR) controllers. When attached, Haptic Links can dynamically alter the forces perceived between the user's hands to support the haptic rendering of a variety of two-handed objects and interactions. They can rigidly lock controllers in an arbitrary configuration, constrain specific degrees of freedom or directions of motion, and dynamically set stiffness along a continuous range. We demonstrate and compare three prototype Haptic Links: Chain, Layer-Hinge, and Ratchet-Hinge. We then describe interaction techniques and scenarios leveraging the capabilities of each. Our user evaluation results confirm that users can perceive many two-handed objects or interactions as more realistic with Haptic Links than with typical unlinked VR controllers.
SP  - 644
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174218
ER  - 

TY  - JOUR
AU  - Xiong, Quan; Liang, Xuanquan; Wei, Daiyue; Wang, Huacen; Zhu, Renjie; Wang, Ting; Mao, Jianjun; Wang, Hongqiang
TI  - So-EAGlove: VR Haptic Glove Rendering Softness Sensation With Force-Tunable Electrostatic Adhesive Brakes
PY  - 2022
AB  - Haptic gloves allow the players to feel the virtual world more realistically by providing force feedback. At present, haptic gloves are mainly driven by conventional motors, and for them, being lightweight, low cost, low in power consumption, and intrinsically safe to operators are challenging. Here, we designed a haptic glove (So-EAGlove) integrating a flexible electrostatic adhesive brake to resist human fingers with a tunable braking force and render a softness sensation. This glove weighs only 51 g, but is capable of simulating objects in a large range of Young's modulus from 540 Pa to 5.4 MPa. The electrostatic adhesive brake costs only approximately 2.43 mW during operation, i.e., more than three days powered by a small button battery. We built a feedforward control model and evaluated its performance. Experimental results show that this glove can generate an accurate force to follow the force-displacement profile of the corresponding real objects. The error is less than 7%, barely noticeable by the subjects. The subjective tests also demonstrate that little statistical difference exists between the real objects and the virtual objects for the subjects.
SP  - 3450
EP  - 3462
JF  - IEEE Transactions on Robotics
VL  - 38
IS  - 6
PB  - 
DO  - 10.1109/tro.2022.3172498
ER  - 

TY  - JOUR
AU  - Lee, Sang Won; Krosnick, Rebecca; Park, Sun Young; Keelean, Brandon; Vaidya, Sach; O'Keefe, Stephanie D.; Lasecki, Walter S.
TI  - Exploring Real-Time Collaboration in Crowd-Powered Systems Through a UI Design Tool
PY  - 2018
AB  - Real-time collaboration between a requester and crowd workers expands the scope of tasks that crowdsourcing can be used for by letting requesters and crowd workers interactively create various artifacts (e.g., a sketch prototype, writing, or program code). In such systems, it is increasingly common to allow requesters to verbally describe their requests, receive responses from workers, and provide immediate and continuous feedback to enhance the overall outcome of the two groups' real-time collaboration. This work is motivated by the lack of a deep understanding of the challenges that end users of such systems face in their communication with workers and the need of design implications that can address such challenges for other similar systems. In this paper, we investigate how requesters verbally communicate and collaborate with crowd workers to solve a complex task. Using a crowd-powered UI design tool, we conducted a qualitative user study to explore how requesters with varying expertise communicate and collaborate with crowd workers. Our work also identifies the unique challenges that collaborative crowdsourcing systems pose: potential expertise differences between requesters and crowd workers, the asymmetry of two-way communication (e.g., speech versus text), and the shared artifact's concurrent modification by two disparate groups. Finally, we make design recommendations that can inform the design of future real-time collaboration processes in crowdsourcing systems.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 2
IS  - CSCW
PB  - 
DO  - 10.1145/3274373
ER  - 

TY  - JOUR
AU  - Vallett, Richard; McDonald, Denisa Qori; Dion, Genevieve; Kim, Youngmoo E.; Shokoufandeh, Ali
TI  - Toward Accurate Sensing with Knitted Fabric: Applications and Technical Considerations
PY  - 2020
AB  - Fabric sensors have been introduced to enable flexible touch-based interaction. We advance the technical capabilities of a scalable and low-profile knitted capacitive touch sensing system by introducing methods to improve its touch localization accuracy. The sensor hardware design tends toward minimalism by using a single conductive yarn and two external connections located at each endpoint. Fewer connectors simplify the textile system integration, but this comes at the expense of reduced signal information output from the system. The electrical continuity of the sensing element, essential to the process of knitting, also increases the uncertainty of localizing touch. We propose using Bode analysis to measure changes in signal due to capacitive touch, as well as design a new algorithm, MSD, which retains the most significant aspects of the signal in terms of touch location identification. We do not classify location of touch, but focus on an invariant signal representation. To evaluate our methods, we introduce ELD, a distance metric to compute the similarity of pairs of key-presses, generalizable to computing distances of tensors of varying lengths. Our experiments show that the proposed sensing method results in high-fidelity signals. Furthermore, the sparse representation of key-presses produced by MSD significantly increases separability between different touch locations. Possible applications based on these sensors are also illustrated through prototypes and use case descriptions.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - EICS
PB  - 
DO  - 10.1145/3394981
ER  - 

TY  - NA
AU  - Cauz, Maxime; Cleve, Anthony
TI  - RCIS - Interacting with Overlaid Information in Augmented Reality Systems for Maintenance: A Preliminary Review
PY  - 2019
AB  - In this paper, we present a preliminary study over how augmented reality systems for industrial maintenance present and let users interact with virtual information. Ten papers are analyzed through three comparison tables presenting the type of information displayed, how the information is displayed with respect to the real world and the interaction techniques available to users.
SP  - 1
EP  - 2
JF  - 2019 13th International Conference on Research Challenges in Information Science (RCIS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/rcis.2019.8876966
ER  - 

TY  - JOUR
AU  - Zhiyong, Chen; Wu, Ronghui; Guo, Shihui; Liu, Xiang-Yang; Fu, Hongbo; Jin, Xiaogang; Liao, Minghong
TI  - 3D Upper Body Reconstruction with Sparse Soft Sensors
PY  - 2020
AB  - Three-dimensional (3D) reconstruction of human body has wide applications, for example, for customized design of clothes and digital avatar production. Existing vision-based systems for 3D body rec...
SP  - 226
EP  - 239
JF  - Soft robotics
VL  - 8
IS  - 2
PB  - 
DO  - 10.1089/soro.2019.0187
ER  - 

TY  - NA
AU  - McArthur, Angela; van Tonder, Cobi; Gaston-Bird, Leslie; Knight-Hill, Andrew
TI  - A survey of 3D audio through the browser: practitioner perspectives
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 2021 Immersive and 3D Audio: from Architecture to Automotive (I3DA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/i3da48870.2021.9610839
ER  - 

TY  - NA
AU  - Tilmon, Brevin; Jain, Eakta; Ferrari, Silvia; Koppal, Sanjeev J.
TI  - ICCP - FoveaCam: A MEMS Mirror-Enabled Foveating Camera
PY  - 2020
AB  - Most cameras today photograph their entire visual field. In contrast, decades of active vision research have proposed foveating camera designs, which allow for selective scene viewing. However, active vision's impact is limited by slow options for mechanical camera movement. We propose a new design, called FoveaCam, and which works by capturing reflections off a tiny, fast moving mirror. FoveaCams can obtain high resolution imagery on multiple regions of interest, even if these are at different depths and viewing directions. We first discuss our prototype and optical calibration strategies. We then outline a control algorithm for the mirror to track target pairs. Finally, we demonstrate a practical application of the full system to enable eye tracking at a distance for frontal faces.
SP  - 1
EP  - 11
JF  - 2020 IEEE International Conference on Computational Photography (ICCP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccp48838.2020.9105183
ER  - 

TY  - JOUR
AU  - Aranovskiy, Stanislav; Ushirobira, Rosane; Efimov, Denis; Casiez, Géry
TI  - An adaptive FIR filter for trajectory prediction and latency reduction in direct Human–Computer interactions
PY  - 2019
AB  - The problem of latency reduction in direct Human-Computer interactions is considered. The proposed method is based on a frequency-domain approximation of a non-causal ideal predictor with a finite impulse response filter. Given a sufficiently rich dataset, the parameters of the filter can be either optimized off-line or tuned on-line with the proposed adaptive algorithm. The performance of the proposed solution is evaluated in an experimental study consisting of drawings on a touchscreen.
SP  - 104093
EP  - NA
JF  - Control Engineering Practice
VL  - 91
IS  - NA
PB  - 
DO  - 10.1016/j.conengprac.2019.07.011
ER  - 

TY  - CHAP
AU  - Le Goc, Mathieu; Zhao, Allen; Wang, Ye; Dietz, Griffin; Semmens, Robert; Follmer, Sean
TI  - Investigating Active Tangibles and Augmented Reality for Creativity Support in Remote Collaboration
PY  - 2019
AB  - Physical manipulation is a key part of externalizing representations of knowledge and the creative process. However, contemporary tools for remote collaboration ignore physical manipulation and the haptic modality. We are interested in exploring remote physical manipulation in the context of ideation and brainstorming. Augmented Reality provides much of the benefits of spatial representation of remote participants, yet AR does not allow for rich physical manipulation and haptic feedback. Thus, we propose to use pairs of multi-robot system to provide synchronized haptic proxies in conjunction with the AR system. These small, tangible robots can be used directly as handles for digital models. We share insights gathered during experimentation to help design platforms combining AR and actuated tangibles, and present several application scenarios to illustrate their potential for remote collaboration.
SP  - 185
EP  - 200
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-28960-7_12
ER  - 

TY  - NA
AU  - Kato, Jun; Goto, Masataka
TI  - VL/HCC - DeployGround: A Framework for Streamlined Programming from API playgrounds to Application Deployment
PY  - 2018
AB  - Interactive web pages for learning programming languages and application programming interfaces (APIs), called “playgrounds,” allow programmers to run and edit example codes in place. Despite the benefits of this live programming experience, programmers need to leave the playground at some point and restart the development from scratch in their own programming environments. This paper proposes “DeployGround,” a framework for creating web-based tutorials that streamlines learning APIs on playgrounds and developing and deploying applications. As a case study, we created a web-based tutorial for browser-based and Node.js-based JavaScript APIs. A preliminary user study found appreciation of the streamlined and social workflow of the DeplovGround framework.
SP  - 259
EP  - 263
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506562
ER  - 

TY  - JOUR
AU  - Irlitti, Andrew; Piumsomboon, Thammathip; Jackson, Daniel; Thomas, Bruce H.
TI  - Conveying spatial awareness cues in xR collaborations
PY  - 2019
AB  - Spatial Augmented Reality (SAR) systems can be suitably combined with other existing Extended Reality (xR) technologies to support collaboration. In existing strategies, users unencumbered by a viewing technology, such as a tablet interface or a head-mounted display, must rely on the transmission of their collaborators' positioning through interpreting a first-person camera view. This design creates a seam between a user's experience of the augmented physical environment in SAR, and their collaborators' experience inside the virtual environment. To assist in development and evaluation of spatial cues to support spatial awareness in SAR environments, an egocentric spatial-communication taxonomy is presented given two determining dimensions, a cue's attachment (physical/virtual) and animation (local/world). We developed four egocentric cues which characterize the four independent dimensions of the matrix: arrow, path, glow , and radial , and a single exocentric world in miniature visualization. Our study shows that virtual attachment cues are preferred, providing the highest accuracy, highest performance when collaborators are occluded, and produce the least mental effort when used with a single virtual collaborator. For multiple collaborators however, the virtual attached, world animated radial cue produces significant increases in mental load and reductions in preference, demonstrating the impact of visual augmentation clutter. The single exocentric visualization produced higher levels of head movement, and poorer accuracy, however the novelty of the visualization produced positive qualitative results.
SP  - 3178
EP  - 3189
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2019.2932173
ER  - 

TY  - NA
AU  - Wang, Chi; Huang, Da-Yuan; Hsu, Shuo-wen; Lin, Cheng-Lung; Chiu, Yeu-Luen; Hou, Chu-En; Chen, Bing-Yu
TI  - CHI - Gaiters: Exploring Skin Stretch Feedback on Legs for Enhancing Virtual Reality Experiences
PY  - 2020
AB  - We propose generating two-dimensional skin stretch feedback on the user's legs. Skin stretch is useful cutaneous feedback to induce the perception of virtual textures and illusory forces and to deliver directional cues. This feedback has been applied to the head, body, and upper limbs to simulate rich physical properties in virtual reality (VR). However, how to expand the benefit of skin stretch feedback and apply it to the lower limbs, remains to be explored. Our first two psychophysical studies examined the minimum changes in skin stretch distance and stretch angle that are perceivable by participants. We then designed and implemented Gaiters, a pair of ungrounded, leg-worn devices, each of which is able to generate multiple two-dimensional skin stretches on the skin of the user's leg. With Gaiters, we conducted an exploratory study to understand participants' experiences when coupling skin stretch patterns with various lower limb actions. The results indicate that rich haptic experiences can be created by our prototype. Finally, a user evaluation indicates that participants enjoyed the experiences when using Gaiters and considered skin stretch as compelling haptic feedback on the legs.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376865
ER  - 

TY  - NA
AU  - Mehmood, Adnan; He, Han; Chen, Xiaochen; Vianto, Aleksi; Buruk, Oğuz Turan; Virkki, Johanna
TI  - SeGAH - ClothFace: Battery-Free User Interface Solution Embedded into Clothing and Everyday Surroundings
PY  - 2020
AB  - This paper introduces ClothFace, a passive ultrahigh frequency (UHF) radio frequency identification (RFID) -based user interface solution, which can be embedded into clothing and into our everyday surroundings. The user interface platform consists of RFID tags, each of which has a unique ID. All the tags are initially readable to an external RFID reader. A specific tag can be switched off by covering it with a hand, which change can then be used as a digital input to any connected device. Because of the used passive UHF RFID technology, there is no need for embedded energy sources, but the interface platform gets all the needed energy from the external RFID reader. In this study, two test setups were created to an office environment: For the Body Test, the interface was integrated into a cotton shirt and into an item. For the Table Test, the interface was integrated into a wooden table. A gamelike testing software was created for both setups and two male test subjects tested the platform. The achieved results were very promising: success rates of 99–100 % and 94–98 % were reached in the Body Test and in the Table Test, respectively. Based on these promising preliminary results, we can envision the employment of ClothFace for developing multi-modal interfaces that can provide on-body gestural controls in body-based serious game applications.
SP  - 1
EP  - 5
JF  - 2020 IEEE 8th International Conference on Serious Games and Applications for Health (SeGAH)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/segah49190.2020.9201771
ER  - 

TY  - NA
AU  - Wang, Xinyi; Xue, Cheng; Sun, Tongxin; Liu, Jingyi; Fu, Xinyi
TI  - MobileHCI - Mobile Screen-based User Interface Design Guideline for Panoramic VR in Shopping Scene
PY  - 2021
AB  - The pandemic has shifted people’s shopping behavior towards online resources. Mobile screen-based panoramic virtual reality (VR) appears to be a promising form for future online shopping. However, the existing user interface (UI) design of panoramic VR shopping needs more natural interactions based on a rational standard design guideline. In this article, we first conducted a heuristic study to compare six representative virtual store apps. This revealed a gap between users’ requirements and the current design attempts. Thus, a UI design guideline for panoramic VR in the shopping scene was presented. Then, we conducted a verification study with a demo optimized according to our design guideline. Both user experience and interactive efficiency improved as a result of the design guideline. Our design guidelines and discoveries derived from user studies provided references for the design, development, and potential use of dynamic panoramic VR in shopping scene.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472055
ER  - 

TY  - BOOK
AU  - Schweigert, Robin; Schwind, Valentin; Mayer, Sven
TI  - Mensch &amp; Computer - EyePointing: A Gaze-Based Selection Technique
PY  - 2019
AB  - Interacting with objects from a distance is not only challenging in the real world but also a common problem in virtual reality (VR). One issue concerns the distinction between attention for exploration and attention for selection -- also known as the Midas-touch problem. Researchers proposed numerous approaches to overcome that challenge using additional devices, gaze input cascaded pointing, and using eye blinks to select the remote object. While techniques such as MAGIC pointing still require additional input for confirming a selection using eye gaze and, thus, forces the user to perform unnatural behavior, there is still no solution enabling a truly natural and unobtrusive device free interaction for selection. In this paper, we propose EyePointing: a technique which combines the MAGIC pointing technique and the referential mid-air pointing gesture to selecting objects in a distance. While the eye gaze is used for referencing the object, the pointing gesture is used as a trigger.
SP  - 719
EP  - 723
JF  - Proceedings of Mensch und Computer 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340764.3344897
ER  - 

TY  - JOUR
AU  - Esteves, Augusto; Shin, Yonghwan; Oakley, Ian
TI  - Comparing selection mechanisms for gaze input techniques in head-mounted displays
PY  - 2020
AB  - NA
SP  - 102414
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 139
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2020.102414
ER  - 

TY  - NA
AU  - Friston, Sebastian; Griffith, Elias J.; Swapp, David; Marshall, Alan G.; Steed, Anthony
TI  - VR - Profiling Distributed Virtual Environments by Tracing Causality
PY  - 2018
AB  - Real-time interactive systems such as virtual environments have high performance requirements, and profiling is a key part of the optimisation process to meet them. Traditional techniques based on metadata and static analysis have difficulty following causality in asynchronous systems. In this paper we explore a new technique for such systems. Timestamped samples of the system state are recorded at instrumentation points at runtime. These are assembled into a graph, and edges between dependent samples recovered. This approach minimises the invasiveness of the instrumentation, while retaining high accuracy. We describe how our instrumentation can be implemented natively in common environments, how its output can be processed into a graph describing causality, and how heterogeneous data sources can be incorporated into this to maximise the scope of the profiling. Across three case studies, we demonstrate the efficacy of this approach, and how it supports a variety of metrics for comprehensively bench-marking distributed virtual environments.
SP  - 238
EP  - 245
JF  - 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2018.8446135
ER  - 

TY  - JOUR
AU  - Ionescu, Tudor B.; Schlund, Sebastian
TI  - A Participatory Programming Model for Democratizing Cobot Technology in Public and Industrial Fablabs
PY  - 2019
AB  - Abstract Collaborative robots (cobots) can improve productivity by flexibly assisting production workers. Nevertheless, due to missing use cases, cobots have yet to be widely adopted in manufacturing. Democratizing robot programming by introducing cobots in public fablabs (or makerspaces) could lead to new applications transferrable to industry thanks to the creativity of interested laypersons. Fablabs could also mitigate the risk of unemployment due to automation by training cobot programmers for manufacturing jobs. In support of the proposed approach this paper introduces a participatory three-layer programming model designed to enable people with little or no programming experience to become effective cobot operators. The model builds on task-oriented programming environments extending them by more versatile yet intuitive programming environments. The model has been successfully evaluated on the basis of a real human-robot assembly application.
SP  - 93
EP  - 98
JF  - Procedia CIRP
VL  - 81
IS  - NA
PB  - 
DO  - 10.1016/j.procir.2019.03.017
ER  - 

TY  - NA
AU  - Pearson, Jennifer; Bailey, Gavin; Robinson, Simon; Jones, Matt; Owen, Tom; Zhang, Chi; Reitmaier, Thomas; Steer, Cameron; Carter, Anna; Sahoo, Deepak Ranjan; Raju, Dani Kalarikalayil
TI  - Can't Touch This: Rethinking Public Technology in a COVID-19 Era
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501980
ER  - 

TY  - NA
AU  - He, Zhenyi; Du, Ruofei; Perlin, Ken
TI  - ISMAR - CollaboVR: A Reconfigurable Framework for Creative Collaboration in Virtual Reality
PY  - 2020
AB  - Writing or sketching on whiteboards is an essential part of collaborative discussions in business meetings, reading groups, design sessions, and interviews. However, prior work in collaborative virtual reality (VR) systems has rarely explored the design space of multi-user layouts and interaction modes with virtual whiteboards. In this paper, we present CollaboVR, a reconfigurable framework for both co-located and geographically dispersed multi-user communication in VR. Our system unleashes users’ creativity by sharing freehand drawings, converting 2D sketches into 3D models, and generating procedural animations in real-time. To minimize the computational expense for VR clients, we leverage a cloud architecture in which the computational expensive application (Chalktalk) is hosted directly on the servers, with results being simultaneously streamed to clients. We have explored three custom layouts – integrated, mirrored, and projective – to reduce visual clutter, increase eye contact, or adapt different use cases. To evaluate CollaboVR, we conducted a within-subject user study with 12 participants. Our findings reveal that users appreciate the custom configurations and real-time interactions provided by CollaboVR. We have open sourced CollaboVR at https://github.com/snowymo/CollaboVR to facilitate future research and development of natural user interfaces and real-time collaborative systems in virtual and augmented reality.
SP  - 542
EP  - 554
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00082
ER  - 

TY  - NA
AU  - Speicher, Maximilian; Hall, Brian D.; Nebeling, Michael
TI  - CHI - What is Mixed Reality
PY  - 2019
AB  - What is Mixed Reality (MR)? To revisit this question given the many recent developments, we conducted interviews with ten AR/VR experts from academia and industry, as well as a literature survey of 68 papers. We find that, while there are prominent examples, there is no universally agreed on, one-size-fits-all definition of MR. Rather, we identified six partially competing notions from the literature and experts' responses. We then started to isolate the different aspects of reality relevant for MR experiences, going beyond the primarily visual notions and extending to audio, motion, haptics, taste, and smell. We distill our findings into a conceptual framework with seven dimensions to characterize MR applications in terms of the number of environments, number of users, level of immersion, level of virtuality, degree of interaction, input, and output. Our goal with this paper is to support classification and discussion of MR applications' design and provide a better means to researchers to contextualize their work within the increasingly fragmented MR landscape.
SP  - 537
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300767
ER  - 

TY  - NA
AU  - Feick, Martin; Regitz, Kora Persephone; Tang, Anthony; Krüger, Antonio
TI  - Designing Visuo-Haptic Illusions with Proxies in Virtual Reality: Exploration of Grasp, Movement Trajectory and Object Mass
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517671
ER  - 

TY  - NA
AU  - Kumar, Ranjitha
TI  - CHIIR - Data-Driven Design: Beyond A/B Testing
PY  - 2019
AB  - A/B testing has become the de facto standard for optimizing design, helping designers craft more effective user experiences by leveraging data. A typical A/B test involves dividing user traffic between two experimental conditions (A and B), and looking for statistically significant differences in performance indicators (e.g., conversion rates) between them. While this technique is popular, there are other, powerful data-driven methods --- complementary to A/B testing --- that can tie design choices to desired outcomes. Mining data from existing designs can expose designers to a greater space of divergent solutions than A/B testing [1,4] ,RICO:2017. Since companies cannot predict a priori if the engineering effort for creating alternatives will be commensurate with a performance increase, they often test small changes, along gradients to local optima. With the millions of websites and mobile apps available today, it is likely that almost any UX problem a designer encounters has already been considered and solved by someone. The challenges are finding relevant existing solutions, measuring their performance, and correlating these metrics with design features. Recent systems that capture and aggregate interaction data from third-party Android apps --- with zero code integration --- open-source analytics that were previously locked away in each app, allowing designers to test and compare UI/UX patterns found in the wild: [2,3] 2017.Lightweight prototypes with tight user feedback loops, or experimentation engines, can bootstrap product design involving technologies that are actively being developed (e.g., artificial intelligence, virtual/augmented reality), where both use cases and capabilities are not well-understood [5]. These systems afford staged automation: initially, "Wizard of Oz'' techniques can scaffold needfinding, and eventually be replaced with automated solutions informed by the collected data. For example, a chatbot deployed on social media can serve as an experimentation engine for automating fashion advice [7]. At first, a pool of personal stylists can power the chatbot to collect organic conversations revealing common fashion problems, effective interaction patterns for addressing them, and design considerations for automation. Once technologies are developed to scale useful interventions [8,9], the chatbot platform provides a testbed for iteratively refining them. Generative models trained on a set of effective design examples can support predictive workflows that allow designers to rapidly prototype new, performant solutions [6]. Models such as generative adversarial networks and variational autoencoders can produce designs based on high-level constraints, or complete them given partial specifications. For example, a mobile wireframing tool backed by such a model could suggest adding "username" and "password" input fields to a screen with a centrally placed "login" button.
SP  - 1
EP  - 2
JF  - Proceedings of the 2019 Conference on Human Information Interaction and Retrieval
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3295750.3300046
ER  - 

TY  - JOUR
AU  - Gentile, Vito; Khamis, Mohamed; Milazzo, Fabrizio; Sorce, Salvatore; Malizia, Alessio; Alt, Florian
TI  - Predicting mid-air gestural interaction with public displays based on audience behaviour
PY  - 2020
AB  - Abstract Knowledge about the expected interaction duration and expected distance from which users will interact with public displays can be useful in many ways. For example, knowing upfront that a certain setup will lead to shorter interactions can nudge space owners to alter the setup. If a system can predict that incoming users will interact at a long distance for a short amount of time, it can accordingly show shorter versions of content (e.g., videos/advertisements) and employ at-a-distance interaction modalities (e.g., mid-air gestures). In this work, we propose a method to build models for predicting users’ interaction duration and distance in public display environments, focusing on mid-air gestural interactive displays. First, we report our findings from a field study showing that multiple variables, such as audience size and behaviour, significantly influence interaction duration and distance. We then train predictor models using contextual data, based on the same variables. By applying our method to a mid-air gestural interactive public display deployment, we build a model that predicts interaction duration with an average error of about 8 s, and interaction distance with an average error of about 35 cm. We discuss how researchers and practitioners can use our work to build their own predictor models, and how they can use them to optimise their deployment.
SP  - 102497
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 144
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2020.102497
ER  - 

TY  - NA
AU  - Chen, Xiaochen; He, Han; Mehmood, Adnan; Raumonen, Pasi; Leino, Mirka; Merilampi, Sari; Virkki, Johanna
TI  - ClothFace: Battery-Free On-body Interface Platform for Future Human-Machine Interaction
PY  - 2022
AB  - Smooth communication between people and machines plays an important role in the intelligent environments of the future, where the best sides of both people and machines are to be exploited in the name of efficiency and flexibility. This requires a functional human-machine interface that allows the necessary control actions but does not require any use of robot-specific devices. In this paper, we further introduce a passive ultra-high frequency (UHF) radio frequency identification (RFID)-based platform, which is an attractive solution for on-body interfaces, as it is passive and maintenance-free, does not require a line-of-sight to function, and has reading distances of several meters. The unique aspect of this study is the established fully textile-based battery-free touchpad for writing digits, which can be integrated into everyday clothing and worn on body. Further, the paper presents various use cases for the developed technology.
SP  - NA
EP  - NA
JF  - 2022 16th European Conference on Antennas and Propagation (EuCAP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/eucap53622.2022.9769251
ER  - 

TY  - NA
AU  - Mäkelä, Ville; Khamis, Mohamed; Mecke, Lukas; James, Jobin; Turunen, Markku; Alt, Florian
TI  - CHI - Pocket Transfers: Interaction Techniques for Transferring Content from Situated Displays to Mobile Devices
PY  - 2018
AB  - We present Pocket Transfers: interaction techniques that allow users to transfer content from situated displays to a personal mobile device while keeping the device in a pocket or bag. Existing content transfer solutions require direct manipulation of the mobile device, making inter-action slower and less flexible. Our introduced tech-niques employ touch, mid-air gestures, gaze, and a mul-timodal combination of gaze and mid-air gestures. We evaluated the techniques in a novel user study (N=20), where we considered dynamic scenarios where the user approaches the display, completes the task, and leaves. We show that all pocket transfer techniques are fast and seen as highly convenient. Mid-air gestures are the most efficient touchless method for transferring a single item, while the multimodal method is the fastest touchless method when multiple items are transferred. We provide guidelines to help researchers and practitioners choose the most suitable content transfer techniques for their systems.
SP  - 135
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173709
ER  - 

TY  - NA
AU  - Han, Ping-Hsuan; Chen, Yang-Sheng; Hsieh, Chiao-En; Wang, Hao-Cheng; Hung, Yi-Ping
TI  - WHC - Hapmosphere: Simulating the Weathers for Walking Around in Immersive Environment with Haptics Feedback
PY  - 2019
AB  - With the advance of locomotion techniques and virtual reality head-mount display (VR-HMD), the users can explore the virtual world by moving around in the virtual environment. Although VR-HMD can provide immersive visual and auditory feedback, without haptic technologies, the users cannot perceive the multiple tactile sensations from the virtual environment. In this paper, we present Hapmosphere, a multiple tactile display for simulating weather in the immersive environment, which can provide thermal, wind, and humidity feedback simultaneously in a room-scale space. This system consists of a steerable structure and haptic modules rigged on the ceiling, so the users can walk around in the physical area. Furthermore, to evaluate the ability of the tactile display, we propose seven kinds of weathers via design consideration and conduct a user study to investigate the feasibility. In our study, the result has shown the potential of utilizing this haptic technique to enhance the immersive experience. Finally, we highlight the limitations and challenges of designing an immersive environment with haptic feedback.
SP  - 247
EP  - 252
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816140
ER  - 

TY  - NA
AU  - Wang, Chiu-Hsuan; Tsai, Chia-En; Yong, Seraphina; Chan, Liwei
TI  - UIST - Slice of Light: Transparent and Integrative Transition Among Realities in a Multi-HMD-User Environment
PY  - 2020
AB  - This work presents Slice of Light, a visualization design created to enhance transparency and integrative transition between realities of Head-Mounted Display (HMD) users sharing the same physical environment. Targeted at reality-guests, Slice of Light's design enables guests to view other HMD users' interactions contextualized in their own virtual environments while allowing the guests to navigate among these virtual environments. In this paper, we detail our visualization design and the implementation. We demonstrate Slice of Light with a block-world construction scenario that involves a multi-HMD-user environment. VR developer and HCI expert participants were recruited to evaluate the scenario, and responded positively to Slice of Light. We discuss their feedback, our design insights, and the limitations of this work.
SP  - 805
EP  - 817
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415868
ER  - 

TY  - NA
AU  - Wang, Zeyu; Nguyen, Cuong; Asente, Paul; Dorsey, Julie
TI  - CHI - DistanciAR: Authoring Site-Specific Augmented Reality Experiences for Remote Environments
PY  - 2021
AB  - Most augmented reality (AR) authoring tools only support the author’s current environment, but designers often need to create site-specific experiences for a different environment. We propose DistanciAR, a novel tablet-based workflow for remote AR authoring. Our baseline solution involves three steps. A remote environment is captured by a camera with LiDAR; then, the author creates an AR experience from a different location using AR interactions; finally, a remote viewer consumes the AR content on site. A formative study revealed understanding and navigating the remote space as key challenges with this solution. We improved the authoring interface by adding two novel modes: Dollhouse, which renders a bird’s-eye view, and Peek, which creates photorealistic composite images using captured images. A second study compared this improved system with the baseline, and participants reported that the new modes made it easier to understand and navigate the remote scene.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445552
ER  - 

TY  - NA
AU  - Henderson, Jay; Mizobuchi, Sachi; Li, Wei; Lank, Edward
TI  - MobileHCI - Exploring Cross-Modal Training via Touch to Learn a Mid-Air Marking Menu Gesture Set
PY  - 2019
AB  - While mid-air gestures are an attractive modality with an extensive research history, one challenge with their usage is that the gestures are not self-revealing. Scaffolding techniques to teach these gestures are difficult to implement since the input device, e.g. a hand, wand or arm, cannot present the gestures to the user. In contrast, for touch gestures, feedforward mechanisms (such as Marking Menus or OctoPocus) have been shown to effectively support user awareness and learning. In this paper, we explore whether touch gesture input can be leveraged to teach users to perform mid-air gestures. We show that marking menu touch gestures transfer directly to knowledge of mid-air gestures, allowing performance of these gestures without intervention. We argue that cross-modal learning can be an effective mechanism for introducing users to mid-air gestural input.
SP  - 8
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3340119
ER  - 

TY  - JOUR
AU  - Son, Eunjin; Song, Hayoung; Nam, Seonghyeon; Kim, Youngwon
TI  - Development of a Virtual Object Weight Recognition Algorithm Based on Pseudo-Haptics and the Development of Immersion Evaluation Technology
PY  - 2022
AB  - <jats:p>In this work, we propose a qualitative immersion evaluation technique based on a pseudo-haptic-based user-specific virtual object weight recognition algorithm and an immersive experience questionnaire (IEQ). The proposed weight recognition algorithm is developed by considering the moving speed of a natural hand tracking-based, user-customized virtual object using a camera in a VR headset and the realistic offset of the object’s weight when lifting it in real space. Customized speeds are defined to recognize customized weights. In addition, an experiment is conducted to measure the speed of lifting objects by weight in real space to obtain the natural object lifting speed weight according to the weight. In order to evaluate the weight and immersion of the developed simulation content, the participants’ qualitative immersion evaluation is conducted through three IEQ-based immersion evaluation surveys. Based on the analysis results of the experimental participants and the interview, this immersion evaluation technique shows whether it is possible to evaluate a realistic tactile experience in VR content. It is predicted that the proposed weight recognition algorithm and evaluation technology can be applied to various fields, such as content production and service support, in line with market demand in the rapidly growing VR, AR, and MR fields.</jats:p>
SP  - 2274
EP  - 2274
JF  - Electronics
VL  - 11
IS  - 14
PB  - 
DO  - 10.3390/electronics11142274
ER  - 

TY  - JOUR
AU  - Yoon, Sang Ho; Paredes, Luis; Huo, Ke; Ramani, Karthik
TI  - MultiSoft: Soft Sensor Enabling Real-Time Multimodal Sensing with Contact Localization and Deformation Classification
PY  - 2018
AB  - We introduce MultiSoft, a multilayer soft sensor capable of sensing real-time contact localization, classification of deformation types, and estimation of deformation magnitudes. We propose a multimodal sensing pipeline that carries out both inverse problem solving and machine learning tasks. Specifically, we employ an electrical impedance tomography (EIT) for contact localization and a support vector machine (SVM) for classifying deformations and regressing their magnitudes. We propose a deformation-aware system which enables maintaining a persistent single-point contact localization throughout the deformation. By updating a time-varying distribution of conductivity change caused by deformations, a single-point contact localization can be maintained and restored to support interaction using both contact localization and deformations.We devise a multilayer structure to fabricate a highly stretchable and flexible soft sensor with a short sensor settlement after excitations. Through a series of experiments and evaluations, we validate both raw sensor and multimodal sensing performance with the proposed method. We further demonstrate applicability and feasibility of MultiSoft with example applications.
SP  - 145
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 3
PB  - 
DO  - 10.1145/3264955
ER  - 

TY  - JOUR
AU  - Sereno, Mickael; Wang, Xiyao; Besançon, Lonni; McGuffin, Michael J.; Isenberg, Tobias
TI  - Collaborative Work in Augmented Reality: A Survey.
PY  - 2022
AB  - In Augmented Reality (AR), users perceive virtual content anchored in the real world. It is used in medicine, education, games, navigation, maintenance, product design, and visualization, in both single-user and multi-user scenarios. Multi-user AR has received limited attention from researchers, even though AR has been in development for more than two decades. We present the state of existing work at the intersection of AR and Computer-Supported Collaborative Work (AR-CSCW), by combining a systematic survey approach with an exploratory, opportunistic literature search. We categorize 65 papers along the dimensions of space, time, role symmetry (whether the roles of users are symmetric), technology symmetry (whether the hardware platforms of users are symmetric), and output and input modalities. We derive design considerations for collaborative AR environments, and identify under-explored research topics. These include the use of heterogeneous hardware considerations and 3D data exploration research areas. This survey is useful for newcomers to the field, readers interested in an overview of CSCW in AR applications, and domain experts seeking up-to-date information.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 01
PB  - 
DO  - 10.1109/tvcg.2020.3032761
ER  - 

TY  - NA
AU  - Zhang, Yongzhao; Chen, Yi-Chao; Wang, Haonan; Jin, Xingyu
TI  - UbiComp/ISWC Adjunct - CELIP: Ultrasonic-based Lip Reading with Channel Estimation Approach for Virtual Reality Systems
PY  - 2021
AB  - We developed an ultrasonic-based silent speech interface for Virtual Reality (VR). As more and more customized devices are proposed to enhance the immersion and experience of VR, our system can be used to improve the capability of interactions between users and the systems, while retaining the possibilities of using various customized devices and avoiding some limitations of traditional speech recognition. By employing the channel estimation techniques with ultrasonic waves, we can derive movement characteristics of users’ lips, which can be used to fine-tune existing speech recognition models and augmented by vast open-sourced speech datasets. Moreover, we use the speech interface to guide the initialization of customized models for new users, so that they can easily have the access to our system. A two-stage experiment has been conducted and the results show that our system can achieve 90.8% command-level accuracy and 1.3% word-error-rate in sentence-level accuracy.
SP  - 580
EP  - 585
JF  - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460418.3480163
ER  - 

TY  - NA
AU  - Luo, Yiyue; Wu, Kui; Palacios, Tomas; Matusik, Wojciech
TI  - CHI - KnitUI: Fabricating Interactive and Sensing Textiles with Machine Knitting
PY  - 2021
AB  - With the recent interest in wearable electronics and smart garments, digital fabrication of sensing and interactive textiles is in increasing demand. Recently, advances in digital machine knitting offer opportunities for the programmable, rapid fabrication of soft, breathable textiles. In this paper, we present KnitUI, a novel, accessible machine-knitted user interface based on resistive pressure sensing. Employing conductive yarns and various machine knitting techniques, we computationally design and automatically fabricate the double-layered resistive sensing structures as well as the coupled conductive connection traces with minimal manual post-processing. We present an interactive design interface for users to customize KnitUI’s colors, sizes, positions, and shapes. After investigating design parameters for the optimized sensing and interactive performance, we demonstrate KnitUI as a portable, deformable, washable, and customizable interactive and sensing platform. It obtains diverse applications, including wearable user interfaces, tactile sensing wearables, and artificial robot skin.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445780
ER  - 

TY  - BOOK
AU  - Riegler, Andreas; Anthes, Christoph; Jetter, Hans-Christian; Heinzl, Christoph; Holzmann, Clemens; Jodlbauer, Herbert; Brunner, Manuel; Auer, Stefan; Friedl, Judith; Fröhler, Bernhard; Leitner, Christina; Pointecker, Fabian; Schwajda, Daniel Josef; Tripathi, Shailesh
TI  - XR@ISS - Cross-Virtuality Visualization, Interaction and Collaboration.
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Lykke, Jesper Rask; Olsen, August Birk; Berman, Philip; Bærentzen, J. Andreas; Frisvad, Jeppe Revall
TI  - Accounting for object weight in interaction design for virtual reality
PY  - 2019
AB  - Interaction design for virtual reality (VR) rarely takes the weight of an object – let alone its moment of inertia –into account. This clearly reduces user immersion and could lead to a break-in-presence. In this work, we propose methods for providing a higher fidelity in interactions with virtual objects. Specifically, we present different methods for picking up, handling, swinging, and throwing objects based on their weight, size, and affordances. We conduct user studies in order to gauge the differences in performance as well as sense of presence of the proposed techniques compared to conventional interaction techniques. While these methods all rely on the use of unmodified VR controllers, we also investigate the difference between using controllers to simulate a baseball bat and swinging a real baseball bat. Interestingly, we find that realism of the motions during interaction is not necessarily an important concern for all users. Our modified interaction techniques, however, have the ability to push user performance towards the slower motions that we observe when a real bat is used instead of a VR controller on its own.
SP  - 131
EP  - 140
JF  - Journal of  WSCG
VL  - 27
IS  - 2
PB  - 
DO  - 10.24132/jwscg.2019.27.2.6
ER  - 

TY  - JOUR
AU  - Zhou, Yaqian; Liu, Yu; Zhou, Heyu; Cheng, Zhiyong; Li, Xuanya; Liu, An-An
TI  - Learning Transferable and Discriminative Representations for 2D Image-Based 3D Model Retrieval
PY  - 2022
AB  - Existing research on the 2D image-based 3D model retrieval task focuses on learning transferable representations directly to narrow the domain discrepancy. However, it is not easy to achieve in practice due to the significant variations across two domains. In addition, some methods design a domain discriminator to distinguish the feature arising from source or target domains for transferable feature representations learning, which will lead to an unexpected deterioration of the feature discriminability. To settle these problems, we propose jointly learning transferable and discriminative representations for 2D image-based 3D model retrieval. Specifically, we extract features from the 2D images and 3D models (described as multiple views) by CNN. Considering the difficulty of directly narrowing the discrepancy of two domains, we are prone to connect 2D image and 3D model domains to an intermediate domain, where the domain gap aims to be eliminated. However, the feature transferability does not denote well discriminability. Based on the batch spectral penalization (BSP) theory, the feature transferability is dominated by feature vectors with higher singular values, while the feature discriminability depends on more eigenvectors with lower singular values to convey rich discriminative structures. Therefore, we penalize the largest singular values so that the feature vectors with lower singular values are appropriately enhanced, thereby strengthening feature discriminability. A series of experiments on two challenging datasets, MI3DOR and MI3DOR-2, indicate that our method can significantly improve performance.
SP  - 7147
EP  - 7159
JF  - IEEE Transactions on Circuits and Systems for Video Technology
VL  - 32
IS  - 10
PB  - 
DO  - 10.1109/tcsvt.2022.3168967
ER  - 

TY  - BOOK
AU  - Batmaz, Anil Ufuk; Mutasim, Aunnoy K; Stuerzlinger, Wolfgang
TI  - VR Workshops - Precision vs. Power Grip: A Comparison of Pen Grip Styles for Selection in Virtual Reality
PY  - 2020
AB  - While commercial Virtual Reality (VR) controllers are mostly designed to be held in a power grip, previous research showed that using pen-like devices with a precision grip can improve user performance for selection in VR, potentially even matching that achievable with a mouse. However, it is not known if the improvement is due to the grip style. In this work, 12 subjects performed a Fitts’ task at 3 different depth conditions with a pen-like input device used in both a precision and power grip. Our results identify that the precision grip significantly improves user performance in VR through a significant reduction in error rate, but we did not observe a significant effect of the distance of targets from the user. We believe that our results are useful for designers and researchers to improve the usability of and user performance in VR systems.
SP  - 23
EP  - 28
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00012
ER  - 

TY  - JOUR
AU  - Richard, Grégoire; Pietrzak, Thomas; Casiez, Géry; Sanz, Ferran Argelaguet; Lécuyer, Anatole
TI  - Studying the Role of Haptic Feedback on Virtual Embodiment in a Drawing Task
PY  - 2021
AB  - The role of haptic feedback on virtual embodiment is investigated in this paper in a context of active and fine manipulation. In particular, we explore which haptic cue, with varying ecological validity, has more influence on virtual embodiment. We conducted a within-subject experiment with 24 participants and compared self-reported embodiment over a humanoid avatar during a coloring task under three conditions: force-feedback, vibrotactile feedback, no haptic feedback. In the experiment, force-feedback was more ecological as it matched reality more closely, while vibrotactile feedback was more symbolic. Taken together, our results show significant superiority of force-feedback over no haptic feedback regarding embodiment, and significant superiority of force-feedback over the other two conditions regarding subjective performance. Those results suggest that a more ecological feedback is better suited to elicit embodiment during fine manipulation tasks.
SP  - 573167
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 1
IS  - NA
PB  - 
DO  - 10.3389/frvir.2020.573167
ER  - 

TY  - NA
AU  - Steed, Anthony; Friston, Sebastian; Pawar, Vijay; Swapp, David
TI  - Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations of Grounded and Worn Devices
PY  - 2020
AB  - Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user's location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418943
ER  - 

TY  - NA
AU  - Nebeling, Michael; Lewis, Katy; Chang, Yu-Cheng; Zhu, Lihan; Chung, Michelle; Wang, Piaoyang; Nebeling, Janet
TI  - CHI - XRDirector: A Role-Based Collaborative Immersive Authoring System
PY  - 2020
AB  - Immersive authoring is an increasingly popular technique to design AR/VR scenes because design and testing can be done concurrently. Most existing systems, however, are single-user and limited to either AR or VR, thus constrained in the interaction techniques. We present XRDirector, a role-based collaborative immersive authoring system that enables designers to freely express interactions using AR and VR devices as puppets to manipulate virtual objects in 3D physical space. In XRDirector, we adapt roles known from filmmaking to structure the authoring process and help coordinate multiple designers in immersive authoring tasks. We study how novice AR/VR creators can take advantage of the roles and modes in XRDirector to prototype complex scenes with animated 3D characters, light effects, and camera movements, and also simulate interactive system behavior in a Wizard of Oz style. XRDirector's design was informed by case studies around complex 3D movie scenes and AR/VR games, as well as workshops with novice AR/VR creators. We show that XRDirector makes it easier and faster to create AR/VR scenes without the need for coding, characterize the issues in coordinating designers between AR and VR, and identify the strengths and weaknesses of each role and mode to mitigate the issues.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376637
ER  - 

TY  - NA
AU  - Drewes, Heiko; Khamis, Mohamed; Alt, Florian
TI  - MUM - DialPlates: enabling pursuits-based user interfaces with large target numbers
PY  - 2019
AB  - In this paper we introduce a novel approach for smooth pursuits eye movement detection and demonstrate that it allows up to 160 targets to be distinguished. With this work we advance the well-established smooth pursuits technique, which allows gaze interaction without calibration. The approach is valuable for researchers and practitioners, since it enables novel user interfaces and applications to be created that employ a large number of targets, for example, a pursuits-based keyboard or a smart home where many different objects can be controlled using gaze. We present findings from two studies. In particular, we compare our novel detection algorithm based on linear regression with the correlation method. We quantify its accuracy for around 20 targets on a single circle and up to 160 targets on multiple circles. Finally, we implemented a pursuits-based keyboard app with 108 targets as proof-of-concept.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365610.3365626
ER  - 

TY  - NA
AU  - Krosnick, Rebecca; Lee, Sang Won; Laseck, Walter S.; Onev, Steve
TI  - VL/HCC - Expresso: Building Responsive Interfaces with Keyframes
PY  - 2018
AB  - Web developers use responsive web design to create user interfaces that can adapt to many form factors. To define responsive pages, developers must use Cascading Style Sheets (CSs) or libraries and tools built on top of it. CSS provides high customizability, but requires significant experience. As a result, non-programmers and novice programmers generally lack a means of easily building custom responsive web pages. In this paper, we present a new approach that allows users to create custom responsive user interfaces without writing program code. We demonstrate the feasibility and effectiveness of the approach through a new system we built, named Expresso. With Expresso, users define “keyframes” - examples of how their VI should look for particular viewport sizes - by simply directly manipulating elements in a WYSIWYG editor. Expresso uses these keyframes to infer rules about the responsive behavior of elements, and automatically renders the appropriate css for a given viewport size. To allow users to create the desired appearance of their page at all viewport sizes, Expresso lets users define either a “smooth” or “jump” transition between adjacent keyframes. We conduct a user study and show that participants are able to effectively use Expresso to build realistic responsive interfaces.
SP  - 39
EP  - 47
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506516
ER  - 

TY  - NA
AU  - Chang, Ya-Kuei; Huang, Jui-Wei; Chen, Chien-Hua; Chen, Chien-Wen; Peng, Jian-Wei; Hu, Min-Chun; Yao, Chih-Yuan; Chu, Hung-Kuo
TI  - VRST - A lightweight and efficient system for tracking handheld objects in virtual reality
PY  - 2018
AB  - While the content of virtual reality (VR) has grown explosively in recent years, the advance of designing user-friendly control interfaces in VR still remains a slow pace. The most commonly used device, such as gamepad or controller, has fixed shape and weight, and thus can not provide realistic haptic feedback when interacting with virtual objects in VR. In this work, we present a novel and lightweight tracking system in the context of manipulating handheld objects in VR. Specifically, our system can effortlessly synchronize the 3D pose of arbitrary handheld objects between the real world and VR in realtime performance. The tracking algorithm is simple, which delicately leverages the power of Leap Motion and IMU sensor to respectively track object's location and orientation. We demonstrate the effectiveness of our system with three VR applications use pencil, ping-pong paddle, and smartphone as control interfaces to provide users more immersive VR experience.
SP  - 3281567
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281567
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Khan, Arshad; Kruttwig, Klaus; Kraus, Tobias; Steimle, Jürgen
TI  - CHI - PhysioSkin: Rapid Fabrication of Skin-Conformal Physiological Interfaces
PY  - 2020
AB  - Advances in rapid prototyping platforms have made physiological sensing accessible to a wide audience. However, off-the-shelf electrodes commonly used for capturing biosignals are typically thick, non-conformal and do not support customization. We present PhysioSkin, a rapid, do-it-yourself prototyping method for fabricating custom multi-modal physiological sensors, using commercial materials and a commodity desktop inkjet printer. It realizes ultrathin skin-conformal patches (~1μm) and interactive textiles that capture sEMG, EDA and ECG signals. It further supports fabricating devices with custom levels of thickness and stretchability. We present detailed fabrication explorations on multiple substrate materials, functional inks and skin adhesive materials. Informed from the literature, we also provide design recommendations for each of the modalities. Evaluation results show that the sensor patches achieve a high signal-to-noise ratio. Example applications demonstrate the functionality and versatility of our approach for prototyping a next generation of physiological devices that intimately couple with the human body.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376366
ER  - 

TY  - JOUR
AU  - Strese, Matti; Hassen, Rania; Noll, Andreas; Steinbach, Eckehard
TI  - A Tactile Computer Mouse for the Display of Surface Material Properties
PY  - 2018
AB  - We present a novel input/output device to display the tactile properties of surface materials. The proposed Tactile Computer Mouse (TCM) is equipped with a series of actuators that can create perceptually relevant tactile cues to a user. The display capabilities of our TCM match the major tactile dimensions in human surface material perception, namely, hardness, friction, warmth, microscopic roughness, and macroscopic roughness. The TCM also preserves necessary interaction capabilities of a typical computer mouse. In addition to the TCM design, we introduce data acquisition procedures and concepts that are necessary to derive a parametric representation of a surface material and further demonstrate the corresponding rendering approach on the TCM. We conducted subjective experiments to determine tactile property ratings of real materials, perceived property ratings using the TCM, and how precisely subjects match the real materials to corresponding virtual material representations using the TCM in the absence of visual and audible clues. Our experimental results show that our TCM successfully displays the five fundamental tactile dimensions and that the twenty participants were able to perceive the TCM-produced virtual surface material tactile sensations with a recognition rate of 89.6 percent for ten different materials.
SP  - 18
EP  - 33
JF  - IEEE transactions on haptics
VL  - 12
IS  - 1
PB  - 
DO  - 10.1109/toh.2018.2864751
ER  - 

TY  - NA
AU  - Kellerhals, Ursina; Burgess, Naomi; Wetzel, Richard
TI  - FDG - Let it Bee: A Case Study of Applying Triadic Game Design for Designing Virtual Reality Training Games for Beekeepers
PY  - 2021
AB  - The Triadic Game Design (TGD) framework is a way to structure the design process of serious games along the worlds of reality, meaning, and play. We used TGD to undertake the first steps in designing serious virtual reality games for training of beekeepers. Beekeeping is a demanding activity as it requires both theoretical knowledge and physically demanding practice. Current educational offers can only rely on “traditional” media such as texts, images, and videos. Training with actual bee hives would be desirable but is difficult to provide, for a variety reasons (e.g., bees being very sensitive organisms). After an initial assessment of the current state of things, we developed a simple VR prototype, where players had to find a queen inside a hive. We then used this prototype to conduct a user study with beekeepers of different competency levels and elicited information about beekeeping in general, the specific game experience, and how such a game could help in the future of beekeeper training. We identified seven learning modes and developed three of them further into game concepts that focus on different aspects. We reflect on how Triadic Game Design supported our design process and provide a hands-on overview of seven steps that future designers can follow in similar circumstances.
SP  - NA
EP  - NA
JF  - The 16th International Conference on the Foundations of Digital Games (FDG) 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472538.3472558
ER  - 

TY  - NA
AU  - Huang, Jing Yuan; Hung, Wei Hsuan; Hsu, Tzu Yin; Liao, Yi Chun; Han, Ping-Hsuan
TI  - SIGGRAPH Asia XR - Pumping Life: Embodied Virtual Companion for Enhancing Immersive Experience with Multisensory Feedback
PY  - 2019
AB  - With the advance of virtual reality (VR) head-mounted display, the appearance of the virtual companion can be more realistic and full of vitality, such as breathing and facial expression. However, the users cannot interact physically with the companions due to they do not have a physical body. In this work, our goal is to enable the virtual companion with multisensory feedback in the VR, which allows the users to play with the virtual companion physically in the immersive environment. We present Pumping Life, a dynamic flow system for enhancing the virtual companion with multisensory feedback, which utilizes water pumps and heater to provide shape deformation and thermal feedback. In this work, to show the interactive gameplay with our system, we deploy the system into a teddy bear and design a VR role-playing game. In this game, the player needs to collaborate with the teddy bear to complete the mission, which would perceive the vitality and expression of the teddy bear with multiple tactile sensations.
SP  - 34
EP  - 35
JF  - SIGGRAPH Asia 2019 XR
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3355355.3361887
ER  - 

TY  - CHAP
AU  - Leriche, Sébastien; Conversy, Stéphane; Picard, Célia; Prun, Daniel; Magnaudet, Mathieu
TI  - STAF Workshops - Towards Handling Latency in Interactive Software
PY  - 2018
AB  - Usability of an interactive software can be highly impacted by the delays of propagation of data and events and by its variations, i.e. latency and jitter. The problem is striking for applications involving tactile interactions or augmented reality, where the shifts between interaction and representation can make the system unusable. For as much, latency is often taken into account only during the validation phase of the software by means of a value which constitutes an acceptable limit. In this shor paper, we present and discuss an alternative approach: we want to handle the latency at all phases of the life cycle of the interactive software, from specification to runtime adaptation and formal validation for certification purposes. We plan to integrate and validate these ideas into Smala, our language dedicated to the development of highly interactive and visual user interfaces.
SP  - 233
EP  - 239
JF  - Software Technologies: Applications and Foundations
VL  - 11176
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-04771-9_18
ER  - 

TY  - CONF
AU  - Oppenlaender, Jonas; Kuosmanen, Elina; Lucero, Andrés; Hosio, Simo
TI  - CHI - Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer Design Feedback in the Classroom
PY  - 2021
AB  - Feedback is an important aspect of design education, and crowdsourcing has emerged as a convenient way to obtain feedback at scale. In this paper, we investigate how crowdsourced design feedback compares to peer design feedback within a design-oriented HCI class and across two metrics: perceived quality and perceived fairness. We also examine the perceived monetary value of crowdsourced feedback, which provides an interesting contrast to the typical requester-centric view of the value of labor on crowdsourcing platforms. Our results reveal that the students (N = 106) perceived the crowdsourced design feedback as inferior to peer design feedback in multiple ways. However, they also identified various positive aspects of the online crowds that peers cannot provide. We discuss the meaning of the findings and provide suggestions for teachers in HCI and other researchers interested in crowd feedback systems on using crowds as a potential complement to peers.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ebel, Patrick; Lingenfelder, Christoph; Vogelsang, Andreas
TI  - Visualizing Event Sequence Data for User Behavior Evaluation of In-Vehicle Information Systems
PY  - 2021
AB  - With modern IVIS becoming more capable and complex than ever, their evaluation becomes increasingly difficult. The analysis of large amounts of user behavior data can help to cope with this complexity and can support UX experts in designing IVIS that serve customer needs and are safe to operate while driving. We, therefore, propose a Multi-level User Behavior Visualization Framework providing effective visualizations of user behavior data that is collected via telematics from production vehicles. Our approach visualizes user behavior data on three different levels: (1) The Task Level View aggregates event sequence data generated through touchscreen interactions to visualize user flows. (2) The Flow Level View allows comparing the individual flows based on a chosen metric. (3) The Sequence Level View provides detailed insights into touch interactions, glance, and driving behavior. Our case study proves that UX experts consider our approach a useful addition to their design process.
SP  - 219
EP  - 229
JF  - 13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3409118.3475140
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Tsai, Chieh; Liao, Yu-So; Chiang, Yi-Ting; Zhang, Zhong-Yi
TI  - FingerX: Rendering Haptic Shapes of Virtual Objects Augmented by Real Objects using Extendable and Withdrawable Supports on Fingers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517489
ER  - 

TY  - JOUR
AU  - Guo, Philip J.
TI  - Building tools to help students learn to program
PY  - 2017
AB  - <jats:p> The <jats:italic>Communications</jats:italic> Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of <jats:italic>Communications</jats:italic> , we'll publish selected posts or excerpts. </jats:p> <jats:p> <jats:bold>twitter</jats:bold> </jats:p> <jats:p>Follow us on Twitter at http://twitter.com/blogCACM</jats:p> <jats:p>http://cacm.acm.org/blogs/blog-cacm</jats:p> <jats:p>Philip Guo summarizes his first three years of research into building tools to support those learning computer programming.</jats:p>
SP  - 8
EP  - 9
JF  - Communications of the ACM
VL  - 60
IS  - 12
PB  - 
DO  - 10.1145/3148245
ER  - 

TY  - NA
AU  - Sun, Yuqian; Yoshida, Shigeo; Narumi, Takuji; Hirose, Michitaka
TI  - CHI - PaCaPa: A Handheld VR Device for Rendering Size, Shape, and Stiffness of Virtual Objects in Tool-based Interactions
PY  - 2019
AB  - We present PaCaPa, a handheld device that renders haptics on a user's palm when the user interacts with virtual objects using virtual tools such as a stick. PaCaPa is a cuboid device with two wings that open and close. As the user's stick makes contact with a virtual object, the wings open by a specific degree to dynamically change the pressure on the palm and fingers. The open angle of the wings is calculated from the angle between the virtual stick and hand direction. As the stick bites into the target object, a large force is generated. Our device enables three kinds of renderings: size, shape, and stiffness. We conducted user studies to evaluate the performance of our device. We also evaluated our device in two application scenarios. User feedback and qualitative ratings indicated that our device can make indirect interaction with handheld tools more realistic.
SP  - 452
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300682
ER  - 

TY  - JOUR
AU  - Júnior, Misael C.; Amalfitano, Domenico; Garcés, Lina; Fasolino, Anna Rita; Andrade, Stevão A.; Delamaro, Márcio
TI  - Dynamic Testing Techniques of Non-functional Requirements in Mobile Apps: A Systematic Mapping Study
PY  - 2022
AB  - <jats:p> <jats:bold>Context:</jats:bold> The mobile app market is continually growing offering solutions to almost all aspects of people’s lives, e.g., healthcare, business, entertainment, as well as the stakeholders’ demand for apps that are more secure, portable, easy to use, among other non-functional requirements (NFRs). Therefore, manufacturers should guarantee that their mobile apps achieve high-quality levels. A good strategy is to include software testing and quality assurance activities during the whole life cycle of such solutions. </jats:p> <jats:p> <jats:bold>Problem:</jats:bold> Systematically warranting NFRs is not an easy task for any software product. Software engineers must take important decisions before adopting testing techniques and automation tools to support such endeavors. </jats:p> <jats:p> <jats:bold>Proposal:</jats:bold> To provide to the software engineers with a broad overview of existing dynamic techniques and automation tools for testing mobile apps regarding NFRs. </jats:p> <jats:p> <jats:bold>Methods:</jats:bold> We planned and conducted a Systematic Mapping Study (SMS) following well-established guidelines for executing secondary studies in software engineering. </jats:p> <jats:p> <jats:bold>Results:</jats:bold> We found 56 primary studies and characterized their contributions based on testing strategies, testing approaches, explored mobile platforms, and the proposed tools. </jats:p> <jats:p> <jats:bold>Conclusions:</jats:bold> The characterization allowed us to identify and discuss important trends and opportunities that can benefit both academics and practitioners. </jats:p>
SP  - 1
EP  - 38
JF  - ACM Computing Surveys
VL  - 54
IS  - 10s
PB  - 
DO  - 10.1145/3507903
ER  - 

TY  - NA
AU  - Kelton, Conor; Ryoo, Jihoon; Balasubramanian, Aruna; Bi, Xiaojun; Das, Samir R.
TI  - MobileHCI - Modeling User-Centered Page Load Time for Smartphones
PY  - 2020
AB  - Page Load Time (PLT) is critical in measuring web page load performance. However, the existing PLT metrics are designed to measure the Web page load performance on desktops/laptops and do not consider user interactions on mobile browsers. As a result, they are ill-suited to measure mobile page load performance from the perspective of the user. In this work, we present the Mobile User-Centered Page Load Time Estimator (muPLTest), a model that estimates the PLT of users on Web pages for mobile browsers. We show that traditional methods to measure user PLT for desktops are unsuited to mobiles because they only consider the initial viewport, which is the part of the screen that is in the user’s view when they first begin to load the page. However, mobile users view multiple viewports during the page load process since they start to scroll even before the page is loaded. We thus construct the muPLTest to account for page load activities across viewports. We train our model with crowdsourced scrolling behavior from live users. We show that muPLTest predicts ground truth user-centered PLT, or the muPLT, obtained from live users with an error of 10-15% across 50 Web pages. Comparatively, traditional PLT metrics perform within 44-90% of the muPLT. Finally, we show how developers can use the muPLTest to scalably estimate changes in user experience when applying different Web optimizations.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403565
ER  - 

TY  - JOUR
AU  - Xie, Zhipeng; Xiao, Tingting; Qin, Huanyu
TI  - Typeface effect in marketing
PY  - 2021
AB  - NA
SP  - 365
EP  - 380
JF  - Advances in Psychological Science
VL  - 29
IS  - 2
PB  - 
DO  - 10.3724/sp.j.1042.2021.00365
ER  - 

TY  - JOUR
AU  - Klaib, Ahmad F.; Alsrehin, Nawaf O.; Melhem, Wasen Y.; Bashtawi, Haneen O.; Magableh, Aws A.
TI  - Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies
PY  - 2021
AB  - NA
SP  - 114037
EP  - NA
JF  - Expert Systems with Applications
VL  - 166
IS  - NA
PB  - 
DO  - 10.1016/j.eswa.2020.114037
ER  - 

TY  - NA
AU  - Hu, Yupeng; He, Weiping; Zhang, Li; Li, Silian
TI  - OZCHI - Enhancing Realism and Presence with Active Physical Reactions in Augmented Reality
PY  - 2020
AB  - This paper proposes a novel method to directly interact with physical content through virtual objects instead of directly using a physical proxy. The physical objects can be virtually triggered and then perform active reactions that conform to general physical laws and user expectations. We designed and implemented a prototype system to enable users to operate a virtual hammer to hit cups with different amounts of water inside. The cups could produce different audio and vibration feedback and water waves in response to the virtual hammer’s collisions. We conducted a pilot study and found that the system could enhance the realism and presence in AR interaction, compared with using a physical proxy.
SP  - 700
EP  - 704
JF  - 32nd Australian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441000.3441080
ER  - 

TY  - JOUR
AU  - Comunità, Marco; Gerino, Andrea; Lim, Veranika; Picinali, Lorenzo
TI  - Design and Evaluation of a Web- and Mobile-Based Binaural Audio Platform for Cultural Heritage
PY  - 2021
AB  - PlugSonic is a suite of web- and mobile-based applications for the curation and experience of 3D interactive soundscapes and sonic narratives in the cultural heritage context. It was developed as part of the PLUGGY EU project (Pluggable Social Platform for Heritage Awareness and Participation) and consists of two main applications: PlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, to create and experience 3D soundscapes for headphones playback. The audio processing within PlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while the mobile exploration of soundscapes in a physical space is obtained using Apple’s ARKit. The main goal of PlugSonic is technology democratisation; PlugSonic users—whether cultural institutions or citizens—are all given the instruments needed to create, process and experience 3D soundscapes and sonic narratives; without the need for specific devices, external tools (software and/or hardware), specialised knowledge or custom development. The aims of this paper are to present the design and development choices, the user involvement processes as well as a final evaluation conducted with inexperienced users on three tasks (creation, curation and experience), demonstrating how PlugSonic is indeed a simple, effective, yet powerful tool.
SP  - 1540
EP  - NA
JF  - Applied Sciences
VL  - 11
IS  - 4
PB  - 
DO  - 10.3390/app11041540
ER  - 

TY  - NA
AU  - Johnstone, Ross; McDonnell, Neil; Williamson, Julie R.
TI  - When Virtuality Surpasses Reality: Possible Futures of Ubiquitous XR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3516396
ER  - 

TY  - JOUR
AU  - George, Ceenu; Tamunjoh, Patrick; Hussmann, Heinrich
TI  - Invisible Boundaries for VR: Auditory and Haptic Signals as Indicators for Real World Boundaries
PY  - 2020
AB  - Maintaining awareness of real world boundaries whilst being immersed in virtual reality (VR) with head mounted displays (HMDs), is a necessity for the physical integrity of the user. This paper explores whether individual human senses can be allocated to the real and the virtual world and what effect this has on workload, presence, performance and perceived safety. We present the results of a lab study ( $N=33$ ) where the auditory and haptic sense of participants was trained to be an indicator for real world boundaries, while their visual sense was bound to a VR experience with an HMD. Our results suggests that allocating senses increases workload. However, while performance is comparable to purely visual indications of boundaries, sense allocation seems to improve presence. Participants prefer the signals to be separate or combined subsequently, depending on the priority and proximity to the boundary. This exploratory study is valuable for developers and researchers who want to start including audio and haptic signals as indicators for real world boundaries.
SP  - 3414
EP  - 3422
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2020.3023607
ER  - 

TY  - NA
AU  - Kim, Myung Jin; Ryu, Neung; Chang, Wooje; Pahud, Michel; Sinclair, Mike; Bianchi, Andrea
TI  - SpinOcchio: Understanding Haptic-Visual Congruency of Skin-Slip in VR with a Dynamic Grip Controller
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517724
ER  - 

TY  - NA
AU  - Bhowmick, Shimmila; Singh, Aditi; Protim Borah, Pranjal; Goswami, Vrushin; Sorathia, Keyur
TI  - Novel Input Interactions for a Möbius Shaped Flexible Handheld Device
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - India HCI 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3506469.3506489
ER  - 

TY  - NA
AU  - Ku, Pin-Sung; Gong, Jun; Wu, Te-Yen; Wei, Yixin; Tang, Yiwen; Ens, Barrett; Yang, Xing-Dong
TI  - CHI - Zippro: The Design and Implementation of An Interactive Zipper
PY  - 2020
AB  - Zippers are common in a wide variety of objects that we use daily. This work investigates how we can take advantage of such common daily activities to support seamless interaction with technology. We look beyond simple zipper-sliding interactions explored previously to determine how to weave foreground and background interactions into a vocabulary of natural usage patterns. We begin by conducting two user studies to understand how people typically interact with zippers. The findings identify several opportunities for zipper input and sensing, which inform the design of Zippro, a self-contained prototype zipper slider, which we evaluate with a standard jacket zipper. We conclude by demonstrating several applications that make use of the identified foreground and background input methods.
SP  - 627
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376756
ER  - 

TY  - NA
AU  - Lee, Injung; Kim, Sunjun; Lee, Byungjoo
TI  - CHI - Geometrically Compensating Effect of End-to-End Latency in Moving-Target Selection Games
PY  - 2019
AB  - Effects of unintended latency on gamer performance have been reported. End-to-end latency can be corrected by post-input manipulation of activation times, but this gives the player unnatural gameplay experience. For moving-target selection games such as Flappy Bird, the paper presents a predictive model of latency on error rate and a novel compensation method for the latency effects by adjusting the game's geometry design -- e.g., by modifying the size of the selection region. Without manipulation of the game clock, this can keep the user's error rate constant even if the end-to-end latency of the system changes. The approach extends the current model of moving-target selection with two additional assumptions about the effects of latency: (1) latency reduces players' cue-viewing time and (2) pushes the mean of the input distribution backward. The model and method proposed have been validated through precise experiments.
SP  - 560
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300790
ER  - 

TY  - NA
AU  - Li, Nianlong; Kim, Han-Jong; Shen, Luyao; Tian, Feng; Han, Teng; Yang, Xing-Dong; Nam, Tek-Jin
TI  - UIST - HapLinkage: Prototyping Haptic Proxies for Virtual Hand Tools Using Linkage Mechanism
PY  - 2020
AB  - Haptic simulation of hand tools like wrenches, pliers, scissors and syringes are beneficial for finely detailed skill training in VR, but designing for numerous hand tools usually requires an expert-level knowledge of specific mechanism and protocol. This paper presents HapLinkage, a prototyping framework based on linkage mechanism, that provides typical motion templates and haptic renderers to facilitate proxy design of virtual hand tools. The mechanical structures can be easily modified, for example, to scale the size, or to change the range of motion by selectively changing linkage lengths. Resistant, stop, release, and restoration force feedback are generated by an actuating module as part of the structure. Additional vibration feedback can be generated with a linear actuator. HapLinkage enables easy and quick prototypting of hand tools for diverse VR scenarios, that embody both of their kinetic and haptic properties. Based on interviews with expert designers, it was confirmed that HapLinkage is expressive in designing haptic proxy of hand tools to enhance VR experiences. It also identified potentials and future development of the framework.
SP  - 1261
EP  - 12274
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415812
ER  - 

TY  - NA
AU  - Sakata, Kazuki; Shizuki, Buntarou; Kawaguchi, Ikkaku; Takahashi, Shin
TI  - A Hat-shaped Pressure-Sensitive Multi-Touch Interface for Virtual Reality
PY  - 2021
AB  - We developed a hat-shaped touch interface for virtual reality viewpoint control. The hat is made of conductive fabric and thus is lightweight. The user can touch, drag, and push the surface, enabling three-dimensional viewpoint control.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489909
ER  - 

TY  - JOUR
AU  - Zenner, André; Makhsadov, Akhmajon; Klingner, Sören; Liebemann, David; Krüger, Antonio
TI  - Immersive Process Model Exploration in Virtual Reality
PY  - 2020
AB  - In many professional domains, relevant processes are documented as abstract process models, such as event-driven process chains (EPCs). EPCs are traditionally visualized as 2D graphs and their size varies with the complexity of the process. While process modeling experts are used to interpreting complex 2D EPCs, in certain scenarios such as, for example, professional training or education, also novice users inexperienced in interpreting 2D EPC data are facing the challenge of learning and understanding complex process models. To communicate process knowledge in an effective yet motivating and interesting way, we propose a novel virtual reality (VR) interface for non-expert users. Our proposed system turns the exploration of arbitrarily complex EPCs into an interactive and multi-sensory VR experience. It automatically generates a virtual 3D environment from a process model and lets users explore processes through a combination of natural walking and teleportation. Our immersive interface leverages basic gamification in the form of a logical walkthrough mode to motivate users to interact with the virtual process. The generated user experience is entirely novel in the field of immersive data exploration and supported by a combination of visual, auditory, vibrotactile and passive haptic feedback. In a user study with $\mathrm{N}=27$ novice users, we evaluate the effect of our proposed system on process model understandability and user experience, while comparing it to a traditional 2D interface on a tablet device. The results indicate a tradeoff between efficiency and user interest as assessed by the UEQ novelty subscale, while no significant decrease in model understanding performance was found using the proposed VR interface. Our investigation highlights the potential of multi-sensory VR for less time-critical professional application domains, such as employee training, communication, education, and related scenarios focusing on user interest.
SP  - 2104
EP  - 2114
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2020.2973476
ER  - 

TY  - NA
AU  - Frutos-Pascual, Maite; Harrison, Jake Michael; Creed, Chris; Williams, Ian
TI  - ICMI - Evaluation of Ultrasound Haptics as a Supplementary Feedback Cue for Grasping in Virtual Environments
PY  - 2019
AB  - This paper presents an evaluation of ultrasound mid-air haptics as a supplementary feedback cue for grasping and lifting virtual objects in Virtual Reality (VR). We present a user study with 27 participants and evaluate 6 different object sizes ranging from 40 mm to 100 mm. We compare three supplementary feedback cues in VR; mid-air haptics, visual feedback (glow effect) and no supplementary feedback. We report on precision metrics (time to completion, grasp aperture and grasp accuracy) and interaction metrics (post-test questionnaire, observations and feedback) to understand general trends and preferences. The results showed an overall preference for visual cues for bigger objects () while ultrasound mid-air haptics were preferred for small virtual targets ().
SP  - 310
EP  - 318
JF  - 2019 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340555.3353720
ER  - 

TY  - NA
AU  - Bragg, Danielle; Kushalnagar, Raja S.; Ladner, Richard E.
TI  - ASSETS - Designing an Animated Character System for American Sign Language
PY  - 2018
AB  - Sign languages lack a standard written form, preventing millions of Deaf people from accessing text in their primary language. A major barrier to adoption is difficulty learning a system which represents complex 3D movements with stationary symbols. In this work, we leverage the animation capabilities of modern screens to create the first animated character system prototype for sign language, producing text that combines iconic symbols and movement. Using animation to represent sign movements can increase resemblance to the live language, making the character system easier to learn. We explore this idea through the lens of American Sign Language (ASL), presenting 1) a pilot study underscoring the potential value of an animated ASL character system, 2) a structured approach for designing animations for an existing ASL character system, and 3) a design probe workshop with ASL users eliciting guidelines for the animated character system design.
SP  - 282
EP  - 294
JF  - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3234695.3236338
ER  - 

TY  - NA
AU  - Gonzalez-Franco, Mar; Sinclair, Mike; Ofek, Eyal
TI  - SAP - Asymmetry of Grasp in Haptic Perception
PY  - 2020
AB  - In this paper we present evidence that human perception of grasp might be most dependent on the information retrieved during the inward latch rather than the release of objects. This research is motivated by a number of haptic simulations and devices and grounded in perception science. We ran a user study (n=12) with two devices one capable of delivering compliant simulations for both grip and release (CLAW), i.e. symmetric device; the other only capable of delivering adaptive grip simulations (CapstanCrunch), i.e. asymmetric device. We fund that both performed similarly well for realism scores in a grasping task with objects of different stiffness. That similar performance was despite CapstanCrunch release was delivered by a constant spring independently of the compliance of the object. Our results show preliminary evidence that when simulating haptic grasp the release might be less important. And we propose a new theory of asymmetry of grasp in haptic perception.
SP  - NA
EP  - NA
JF  - ACM Symposium on Applied Perception 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385955.3407934
ER  - 

TY  - NA
AU  - Grandi, Jeronimo G; Debarba, Henrique Galvan; Maciel, Anderson
TI  - VR - Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities
PY  - 2019
AB  - We present an assessment of asymmetric interactions in Collaborative Virtual Environments (CVEs). In our asymmetric setup, two co-located users interact with virtual 3D objects, one in immersive Virtual Reality (VR) and the other in mobile Augmented Reality (AR). We conducted a study with 36 participants to evaluate performance and collaboration aspects of pair work, and compare it with two symmetric scenarios, either with both users in immersive VR or mobile AR. To perform this experiment, we adopt a collaborative AR manipulation technique from literature and develop and evaluate a VR manipulation technique of our own. Our results indicate that pairs in asymmetric VR-AR achieved significantly better performance than the AR symmetric condition, and similar performance to VR symmetric. Regardless of the condition, pairs had similar work participation indicating a high cooperation level even when there is a visualization and interaction asymmetry between the participants.
SP  - 127
EP  - 135
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8798080
ER  - 

TY  - NA
AU  - Wang, Yuntao; Chen, Zichao; Li, Hanchuan; Cao, Zhengyi; Luo, Huiyi; Zhang, Tengxiang; Ou, Ke; Raiti, John; Yu, Chun; Patel, Shwetak N.; Shi, Yuanchun
TI  - CHI - MoveVR: Enabling Multiform Force Feedback in Virtual Reality using Household Cleaning Robot
PY  - 2020
AB  - Haptic feedback can significantly enhance the realism and immersiveness of virtual reality (VR) systems. In this paper, we propose MoveVR, a technique that enables realistic, multiform force feedback in VR leveraging commonplace cleaning robots. MoveVR can generate tension, resistance, impact and material rigidity force feedback with multiple levels of force intensity and directions. This is achieved by changing the robot's moving speed, rotation, position as well as the carried proxies. We demonstrated the feasibility and effectiveness of MoveVR through interactive VR gaming. In our quantitative and qualitative evaluation studies, participants found that MoveVR provides more realistic and enjoyable user experience when compared to commercially available haptic solutions such as vibrotactile haptic systems.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376286
ER  - 

TY  - JOUR
AU  - Paredes, Luis; Ipsita, Ananya; Mesa, Juan C.; Martinez Garrido, Ramses V.; Ramani, Karthik
TI  - StretchAR
PY  - 2022
AB  - <jats:p>Over the past decade, augmented reality (AR) developers have explored a variety of approaches to allow users to interact with the information displayed on smart glasses and head-mounted displays (HMDs). Current interaction modalities such as mid-air gestures, voice commands, or hand-held controllers provide a limited range of interactions with the virtual content. Additionally, these modalities can also be exhausting, uncomfortable, obtrusive, and socially awkward. There is a need to introduce comfortable interaction techniques for smart glasses and HMDS without the need for visual attention. This paper presents StretchAR, wearable straps that exploit touch and stretch as input modalities to interact with the virtual content displayed on smart glasses. StretchAR straps are thin, lightweight, and can be attached to existing garments to enhance users' interactions in AR. StretchAR straps can withstand strains up to 190% while remaining sensitive to touch inputs. The strap allows the effective combination of these inputs as a mode of interaction with the content displayed through AR widgets, maps, menus, social media, and Internet of Things (IoT) devices. Furthermore, we conducted a user study with 15 participants to determine the potential implications of the use of StretchAR as input modalities when placed on four different body locations (head, chest, forearm, and wrist). This study reveals that StretchAR can be used as an efficient and convenient input modality for smart glasses with a 96% accuracy. Additionally, we provide a collection of 28 interactions enabled by the simultaneous touch-stretch capabilities of StretchAR. Finally, we facilitate recommendation guidelines for the design, fabrication, placement, and possible applications of StretchAR as an interaction modality for AR content displayed on smart glasses.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550305
ER  - 

TY  - NA
AU  - Lakhnati, Younes; Springer, Raphael; White, Edward; Gerken, Jens
TI  - MUM - Is it real?: understanding interaction mechanics within the reality-virtuality continuum
PY  - 2019
AB  - The concept of Mixed Reality has existed in research for decades but has experienced rapid growth in recent years, mainly due to technological advances and peripherals such as the Microsoft HoloLens reaching the market. Despite this, certain design aspects of Mixed Reality experiences, such as the different nuances of real and virtual elements, remain largely unexplored. This paper presents an explorative study with 15 participants which aims to investigate and gain a better understanding of the different qualities of real and virtual objects. To that end, we developed a Mixed Reality board game that offered different combinations of real and virtual game components, such as the board, the pieces and the dice. Our analysis shows that the participants generally preferred the completely virtual variant but appreciated different qualities of real and virtual elements. The results also indicate that virtual interaction elements work better on a real background than vice versa. However, this conflicts with some participants' preference of using physical pieces for the haptic experience, creating a design trade-off. This study represents a first step in exploring how the experience changes when swapping elements of differing realities for one another and identifying these trade-offs.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365610.3365634
ER  - 

TY  - JOUR
AU  - Esteves, Augusto; Bouquet, Elizabeth; Pfeuffer, Ken; Alt, Florian
TI  - One-handed Input for Mobile Devices via Motion Matching and Orbits Controls
PY  - 2022
AB  - <jats:p>We introduce a novel one-handed input technique for mobile devices that is not based on pointing, but on motion matching -where users select a target by mimicking its unique animation. Our work is motivated by the findings of a survey (N=201) on current mobile use, from which we identify lingering opportunities for one-handed input techniques. We follow by expanding on current motion matching implementations - previously developed in the context of gaze or mid-air input - so these take advantage of the affordances of touch-input devices. We validate the technique by characterizing user performance via a standard selection task (N=24) where we report success rates (&gt;95%), selection times (~1.6 s), input footprint, grip stability, usability, and subjective workload - in both phone and tablet conditions. Finally, we present a design space that illustrates six ways in which motion matching can be embedded into mobile interfaces via a camera prototype application.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534624
ER  - 

TY  - NA
AU  - Zhang, Xiong; Guo, Philip J.
TI  - UIST - Fusion: Opportunistic Web Prototyping with UI Mashups
PY  - 2018
AB  - Modern web development is rife with complexity at all layers, ranging from needing to configure backend services to grappling with frontend frameworks and dependencies. To lower these development barriers, we introduce a technique that enables people to prototype opportunistically by borrowing pieces of desired functionality from across the web without needing any access to their underlying codebases, build environments, or server backends. We implemented this technique in a browser extension called Fusion, which lets users create web UI mashups by extracting components from existing unmodified webpages and hooking them together using transclusion and JavaScript glue code. We demonstrate the generality and versatility of Fusion via a case study where we used it to create seven UI mashups in domains such as programming tools, data science, web design, and collaborative work. Our mashups include replicating portions of prior HCI systems (Blueprint for in-situ code search and DS.js for in-browser data science), extending the p5.js IDE for Processing with real-time collaborative editing, and integrating Python Tutor code visualizations into static tutorials. These UI mashups each took less than 15 lines of JavaScript glue code to create with Fusion.
SP  - 951
EP  - 962
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242632
ER  - 

TY  - NA
AU  - Oogjes, Doenja; Wakkary, Ron
TI  - Weaving Stories: Toward Repertoires for Designing Things
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501901
ER  - 

TY  - NA
AU  - Maekawa, Azumi; Matsubara, Seito; Wakisaka, Sohei; Uriu, Daisuke; Hiyama, Atsushi; Inami, Masahiko
TI  - CHI - Dynamic Motor Skill Synthesis with Human-Machine Mutual Actuation
PY  - 2020
AB  - This paper presents an approach for coupling robotic capability with human ability in dynamic motor skills, called "Human-Machine Mutual Actuation (HMMA)." We focus specifically on throwing motions and propose a method to control the release timing computationally. A system we developed achieves our concept, HMMA, by a robotic handheld device that acts as a release controller. We conducted user studies to validate the feasibility of the concept and clarify related technical issues to be tackled. We recognized that the system successfully performs on throwing according to the target while it exploits human ability. These empirical experiments suggest that robotic capability can be embedded into the users' motions without losing their senses of control. Throughout the user study, we also revealed several issues to be tackled in further research contributing to HMMA.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376705
ER  - 

TY  - NA
AU  - Verweij, David; Esteves, Augusto; Bakker, Saskia; Khan, Vassilis-Javed
TI  - Tangible and Embedded Interaction - Designing Motion Matching for Real-World Applications: Lessons from Realistic Deployments
PY  - 2019
AB  - Amongst the variety of (multi-modal) interaction techniques that are being developed and explored, the Motion Matching paradigm provides a novel approach to selection and control. In motion matching, users interact by rhythmically moving their bodies to track the continuous movements of different interface targets. This paper builds upon the current algorithmic and usability focused body of work by exploring the product possibilities and implications of motion matching. Through the development and qualitative study of four novel and different real-world motion matching applications --- with 20 participants --- we elaborate on the suitability of motion matching in different multi-user scenarios, the less pertinent use in home environments and the necessity for multi-modal interaction. Based on these learnings, we developed three novel motion matching based interactive lamps, which report on clear paths for further dissemination of the embodied interaction technique's experience. This paper hereby informs the design of future motion matching interfaces and products.
SP  - 645
EP  - 656
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295628
ER  - 

TY  - JOUR
AU  - Wu, Hui-Yin; Calabrèse, Aurélie; Kornprobst, Pierre
TI  - Towards Accessible News Reading Design in Virtual Reality for Low Vision
PY  - 2021
AB  - Low-vision conditions resulting in partial loss of the visual field strongly affect patients' daily tasks and routines, and none more prominently than the ability to access text. Though vision aids such as magnifiers, digital screens, and text-to-speech devices can improve overall accessibility to text, news media, which is non-linear and has complex and volatile formatting, is still inaccessible, barring low-vision patients from easy access to essential news content. This paper positions virtual reality as the next step towards accessible and enjoyable news reading for the low vision. We first conduct an extensive review into existing research on low-vision reading technologies and accessibility for modern news media. From previous research and studies, we then conduct an analysis into the advantages of virtual reality for low-vision reading and propose comprehensive guidelines for visual accessibility design in virtual reality, with a focus on reading. This is coupled with a hands-on study of eight reading applications in virtual reality to evaluate how accessibility design is currently implemented in existing products. Finally, we present a framework that integrates the design principles resulting from our analysis and study, and implement a proof-of-concept for this framework using browser-based graphics to demonstrate the feasibility of our proposal with modern virtual reality technology.
SP  - 20
EP  - 27278
JF  - Multimedia Tools and Applications
VL  - 80
IS  - 18
PB  - 
DO  - 10.1007/s11042-021-10899-9
ER  - 

TY  - NA
AU  - Tanaka, Kojiro; Mikawa, Masahiko; Fujisawa, Makoto
TI  - CW - Intermediation Family: Workspace for Sharing Spatial Design among Multiple Users
PY  - 2020
AB  - This paper presents a novel workspace for sharing a virtual space among multiple users with tangible interfaces and multiple perspectives. The multiple users can share a virtual space such as a spatial design through first-person perspectives (FPPs) and third-person perspectives (TPPs), and can discuss it while switching between the FPPs and TPPs. The workspace, named the Intermediation Family, is composed of box-shaped movable tangible interfaces, a fixed large display, a simple removable HMD and a computer that makes a virtual space. Since the tangible interface plays as both a user’s avatar and a camera object for the FPP in the virtual space, the user can observe the virtual space from the FPP on a display mounted on the tangible interface while moving it. When the user wants a more immersive experience, she/he can also use the HMD. Moreover, the users can observe the entire virtual space through the TPP on the fixed large display. The multiple users can share the spatial design by switching their perspectives and exchanging the tangible interface and the HMD one after another. In this paper, two applications were implemented to hear opinions of some users. One is a virtual walk-through application, the other is a hands-on room layout application.
SP  - 55
EP  - 62
JF  - 2020 International Conference on Cyberworlds (CW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cw49994.2020.00016
ER  - 

TY  - NA
AU  - Lopes, Pedro; You, Sijing; Ion, Alexandra; Baudisch, Patrick
TI  - CHI - Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation
PY  - 2018
AB  - We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline.
SP  - 446
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174020
ER  - 

TY  - NA
AU  - Heller, Florian; Todi, Kashyap; Luyten, Kris
TI  - MobileHCI - An Interactive Design Space for Wearable Displays
PY  - 2021
AB  - The promise of on-body interactions has led to widespread development of wearable displays. They manifest themselves in highly variable shapes and form, and are realized using technologies with fundamentally different properties. Through an extensive survey of the field of wearable displays, we characterize existing systems based on key qualities of displays and wearables, such as location on the body, intended viewers or audience, and the information density of rendered content. We present the results of this analysis in an open, web-based interactive design space that supports exploration and refinement along various parameters. The design space, which currently encapsulates 129 cases of wearable displays, aims to inform researchers and practitioners on existing solutions and designs, and enable the identification of gaps and opportunities for novel research and applications. Further, it seeks to provide them with a thinking tool to deliberate on how the displayed content should be adapted based on key design parameters. Through this work, we aim to facilitate progress in wearable displays, informed by existing solutions, by providing researchers with an interactive platform for discovery and reflection.
SP  - 4
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472034
ER  - 

TY  - NA
AU  - Liu, Hangxin; Zhang, Zhenliang; Xie, Xu; Zhu, Yixin; Liu, Yue; Wang, Yongtian; Zhu, Song-Chun
TI  - ICRA - High-Fidelity Grasping in Virtual Reality using a Glove-based System
PY  - 2019
AB  - This paper presents a design that jointly provides hand pose sensing, hand localization, and haptic feedback to facilitate real-time stable grasps in Virtual Reality (VR). The design is based on an easy-to-replicate glove-based system that can reliably perform (i) a high-fidelity hand pose sensing in real time through a network of 15 IMUs, and (ii) the hand localization using a Vive Tracker. The supported physics-based simulation in VR is capable of detecting collisions and contact points for virtual object manipulation, which drives the collision event to trigger the physical vibration motors on the glove to signal the user, providing a better realism inside virtual environments. A caging-based approach using collision geometry is integrated to determine whether a grasp is stable. In the experiment, we showcase successful grasps of virtual objects with large geometry variations. Comparing to the popular LeapMotion sensor, we demonstrate the proposed glove-based design yields a higher success rate in various tasks in VR. We hope such a glove-based system can simplify the data collection of human manipulations with VR.
SP  - 5180
EP  - 5186
JF  - 2019 International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra.2019.8794230
ER  - 

TY  - NA
AU  - Zhiyuan, Liu; Feitong, Qiao; Haotian, Long; Guang, Li
TI  - SenSys - GazeLabel: A Cost-free Data Labeling System with Public Displays using Eye-tracking
PY  - 2018
AB  - In this paper, we introduce GazeLabel: a cost-free data labeling system with public displays (at bus stops, metro stations, shopping malls, etc.) using eye-tracking technology. With built-in cameras, the proposed system captures pedestrians' gaze points on the screen, from which we can extract information and get a substantial amount of free labeled data. Based on this idea, we implemented a prototype eye-tracking data-labeling system for binary image classifications. A pilot study demonstrated the potential and practicability in a data labeling process using GazeLabel.
SP  - 343
EP  - 344
JF  - Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3274783.3275174
ER  - 

TY  - JOUR
AU  - Herbst, Yair; Zelnik-Manor, Lihi; Wolf, Alon
TI  - Analysis of subject specific grasping patterns.
PY  - 2020
AB  - Existing haptic feedback devices are limited in their capabilities and are often cumbersome and heavy. In addition, these devices are generic and do not adapt to the users' grasping behavior. Potentially, a human-oriented design process could generate an improved design. While current research done on human grasping was aimed at finding common properties within the research population, we investigated the dynamic patterns that make human grasping behavior distinct rather than generalized, i.e. subject specific. Experiments were conducted on 31 subjects who performed grasping tasks on five different objects. The kinematics and kinetics parameters were measured using a motion capture system and force sensors. The collected data was processed through a pipeline of dimensionality reduction and clustering algorithms. Using finger joint angles and reaction forces as our features, we were able to classify these tasks with over 95% success. In addition, we examined the effects of the objects' mechanical properties on those patterns and the significance of the different features for the differentiation. Our results suggest that grasping patterns are, indeed, subject-specific; this, in turn, could suggest that a device capable of providing personalized feedback can improve the user experience and, in turn, increase the usability in different applications. This paper explores an undiscussed aspect of human dynamic patterns. Furthermore, the collected data offer a valuable dataset of human grasping behavior, containing 1083 grasp instances with both kinetics and kinematics data.
SP  - e0234969
EP  - NA
JF  - PloS one
VL  - 15
IS  - 7
PB  - 
DO  - 10.1371/journal.pone.0234969
ER  - 

TY  - BOOK
AU  - Valkov, Dimitar; Thiele, Sebastian; Huesmann, Karim; Gebauer, Eike; Risse, Benjamin
TI  - VR Workshops - Touch Recognition on Complex 3D Printed Surfaces using Filter Response Analysis
PY  - 2021
AB  - Touch sensing on various surfaces has played a prominent role in human-computer interaction in the last decades. However, current technologies are mostly suited for flat or sufficiently smooth surfaces and touch sensing on complex geometries remains a challenging task, especially when the sensing hardware needs to be embedded into the interactive object. In this paper, we introduce a novel sensing approach based on the observation that conductive materials and the user’s hand or finger can be considered a complex filter system with well-conditioned input-output relationships. Different hand postures can be disambiguated by mapping the response of these filters using an intentionally small convolutional neural network. Our experiments show that even straight-forward electrode geometries provided by common 3D printers and filaments can be used to achieve high accuracy, rendering expressive interactions with complex 3D shapes possible while allowing to integrate the touch surface directly into the interactive object. Ultimately, our low-cost and versatile sensing approach enables rich interaction on a variety of objects and surfaces which is demonstrated through a series of illustrative experiments.
SP  - 195
EP  - 200
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00043
ER  - 

TY  - NA
AU  - Zenner, André; Krüger, Antonio
TI  - CHI - Drag:on: A Virtual Reality Controller Providing Haptic Feedback Based on Drag and Weight Shift
PY  - 2019
AB  - Standard controllers for virtual reality (VR) lack sophisticated means to convey a realistic, kinesthetic impression of size, resistance or inertia. We present the concept and implementation of Drag:on, an ungrounded shape-changing VR controller that provides dynamic passive haptic feedback based on drag, i.e. air resistance, and weight shift. Drag:on leverages the airflow occurring at the controller during interaction. By dynamically adjusting its surface area, the controller changes the drag and rotational inertia felt by the user. In a user study, we found that Drag:on can provide distinguishable levels of haptic feedback. Our prototype increases the haptic realism in VR compared to standard controllers and when rotated or swung improves the perception of virtual resistance. By this, Drag:on provides haptic feedback suitable for rendering different virtual mechanical resistances, virtual gas streams, and virtual objects differing in scale, material and fill state.
SP  - 211
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300441
ER  - 

TY  - JOUR
AU  - Tritchkov, Ivan
TI  - Systematic Mapping Study on Verification and Validation of Industrial Third-party Iot Applications
PY  - 2017
AB  - <jats:p>The next industrial revolution commonly known as Industry 4.0 represents the idea of interconnected manufacturing, where intelligent devices, systems and processes exchange information, resources and artifacts to optimize the complete value-added chain and to reduce costs and time-to- market. Industrial software ecosystems are a good example how the latest digitalization trends are applied in the industry domain and how with the help of industrial IoT applications the production process can be optimized. However, the use of third- party applications exposes to a risk the systems and devices part of the manufacturing process. To address these risks a set of quality measures must be carried out in the ecosystem. This paper presents the results of a systematic mapping study carried out in the area of verification and validation of industrial IoT third-party applications. The goal of the study is to structure the scientific landscape and to provide an up-to-date snapshot of the current state of the research field.</jats:p>
SP  - 30
EP  - 44
JF  - Advances in Cyber-Physical Systems
VL  - 5
IS  - 1
PB  - 
DO  - 10.23939/acps2020.01.030
ER  - 

TY  - BOOK
AU  - Wallace, Leland; Delaurante, Tony; Simon, Mara; Austin, Rebecca; Rolich, Timothy; Khadka, Rajiv; Banic, Amy
TI  - VR Workshops - Squishy Volumes: Evaluation of Silicone as Camera-less Pressure-Based Input for 3-Dimensional Interaction
PY  - 2020
AB  - Low cost sensors and materials are increasingly of interest to designers for developing new ways to gather 3-Dimensional input. Silicone is a low cost material with capabilities of a variety of forms and sizes, thereby facilitating flexible construction. Given these properties, users can construct unique input solutions for a variety of applications. However, aside from other existing methods of measuring volume deformation, molded silicone (without added components inside the silicone and without added external cameras) for volumetric input has not been largely explored. In this paper we present an evaluation that investigated the parameters of silicone as volumetric input. The silicone volume has no added components inside making it easy to construct and use, however some external but small, flexible, and portable low-cost components are used for deformation measurement. We present the 3-dimensional input results as a function of the physical pressure on the silicone by the volume of silicone. Researchers can use these input metrics to design a silicone-based device with desired size and thickness to achieve the desired sensitivity and resolution of input for their application.
SP  - 29
EP  - 34
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00013
ER  - 

TY  - JOUR
AU  - Kim, Jinwook; Kim, Seonghyeon; Lee, Jeongmi
TI  - The Effect of Multisensory Pseudo-Haptic Feedback on Perception of Virtual Weight
PY  - 2022
AB  - Providing realistic haptic feedback of virtual objects is critical for immersive VR experience, and there have been many approaches to simulate haptic properties. Most of them, however, are limited to a narrow modulation range of simulated perception. To overcome this limitation, the current paper examines the effect of multisensory pseudo-haptic feedback that combines control-to-display (C/D) ratio manipulation and electrical muscle stimulation (EMS) on simulated weight perception. In two experiments, we independently manipulated the C/D ratio and EMS status and observed the effects on the absolute and difference thresholds of simulated weight perception. From the absolute thresholds results, we specify the effective range of C/D ratio that can successfully induce weight perception and show that the range can be more than twice widened by multisensory pseudo-haptic feedback. Furthermore, we demonstrate that the sensitivity to weight difference increases as the standard C/D ratio decreases from the difference thresholds results, which provides practical design guidelines for assigning multiple levels of weight to virtual objects. This study contributes to understanding the psychological effects of multisensory pseudo-haptic feedback on simulated weight perception in virtual reality.
SP  - 5129
EP  - 5140
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3140438
ER  - 

TY  - CHAP
AU  - Amemiya, Tomohiro
TI  - HCI (4) - Virtual Reality Applications Using Pseudo-attraction Force by Asymmetric Oscillation.
PY  - 2020
AB  - Because virtual reality (VR) systems are accessible to anyone through inexpensive high-end consumer headsets and input devices, researchers are seeking a technique to enrich the VR experience using modalities other than audiovisual ones, such as touch. The author is developing a haptic display that utilizes the properties of human illusions, which makes humans experience an illusory force similar to the sensation of being pulled continuously in a particular direction through asymmetric vibrations. Using illusory force in the VR applications is not a popular concept. This paper discusses the possibility of whether such a pseudo-attraction force can be applied to VR applications and introduces several applications for the implementation of this pseudo-attraction force in real world scenarios.
SP  - 331
EP  - 340
JF  - Human Interface and the Management of Information. Designing Information
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50020-7_23
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles; Haliyo, Sinan; Frey, Pascal
TI  - "Can I Touch This?": Survey of Virtual Reality Interactions via Haptic Solutions
PY  - 2021
AB  - Haptic feedback has become crucial to enhance the user experiences in Virtual Reality (VR). This justifies the sudden burst of novel haptic solutions proposed these past years in the HCI community. This article is a survey of Virtual Reality interactions, relying on haptic devices. We propose two dimensions to describe and compare the current haptic solutions: their degree of physicality, as well as their degree of actuation. We depict a compromise between the user and the designer, highlighting how the range of required or proposed stimulation in VR is opposed to the haptic interfaces flexibility and their deployment in real-life use-cases. This paper (1) outlines the variety of haptic solutions and provides a novel perspective for analysing their associated interactions, (2) highlights the limits of the current evaluation criteria regarding these interactions, and finally (3) reflects the interaction, operation and conception potentials of "encountered-type of haptic devices".
SP  - NA
EP  - NA
JF  - 32e Conférence Francophone sur l'Interaction Homme-Machine
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450522.3451323
ER  - 

TY  - NA
AU  - Kytö, Mikko; Ens, Barrett; Piumsomboon, Thammathip; Lee, Gun A.; Billinghurst, Mark
TI  - CHI - Pinpointing: Precise Head- and Eye-Based Target Selection for Augmented Reality
PY  - 2018
AB  - Head and eye movement can be leveraged to improve the user's interaction repertoire for wearable displays. Head movements are deliberate and accurate, and provide the current state-of-the-art pointing technique. Eye gaze can potentially be faster and more ergonomic, but suffers from low accuracy due to calibration errors and drift of wearable eye-tracking sensors. This work investigates precise, multimodal selection techniques using head motion and eye gaze. A comparison of speed and pointing accuracy reveals the relative merits of each method, including the achievable target size for robust selection. We demonstrate and discuss example applications for augmented reality, including compact menus with deep structure, and a proof-of-concept method for on-line correction of calibration drift.
SP  - 81
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173655
ER  - 

TY  - NA
AU  - Lee, Sang Won; Willette, Aaron; Koutra, Danai; Lasecki, Walter S.
TI  - Creativity &amp; Cognition - The Effect of Social Interaction on Facilitating Audience Participation in a Live Music Performance
PY  - 2019
AB  - Facilitating audience participation in a music performance brings with it challenges in involving non-expert users in large-scale collaboration. A musical piece needs to be created live, over a short period of time, with limited communication channels. To address this challenge, we propose to incorporate social interaction through mobile music instruments that the audience is given to play with, and examine how this feature sustains and affects the audience involvement. We test this idea with an audience participation music system, Crowd in C. We realized a participation-based musical performance with the system and validated our approach by analyzing the interaction traces of the audience at a performance. The result indicates that the audience members were actively engaged throughout the performance, with multiple layers of social interaction available in the system. We also present how the social interactivity among the audience shaped their interaction in the music making process.
SP  - 108
EP  - 120
JF  - Proceedings of the 2019 on Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3325480.3325509
ER  - 

TY  - CHAP
AU  - Li, Yue; Ch'ng, Eugene
TI  - A Framework for Sharing Cultural Heritage Objects in Hybrid Virtual and Augmented Reality Environments
PY  - 2022
AB  - NA
SP  - 471
EP  - 492
JF  - Visual Heritage: Digital Approaches in Heritage Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77028-0_23
ER  - 

TY  - NA
AU  - Miyakami, Masahiro; Murata, Karen. A.; Kajimoto, Hiroyuki
TI  - SIGGRAPH ASIA Emerging Technologies - Hapballoon: Wearable Haptic Balloon-Based Feedback Device
PY  - 2019
AB  - We developed The “Hapballoon”, a novel device that is worn on the fingertips. The device can present three types of sensations: force, warmth, and vibration. Force sensations are presented when the inflated balloons on individual devices contact one another. The device is easy to attach on the finger, and does not obstruct common optical finger tracking methods that track the back side of the hand in virtual reality (VR) applications. Each module weights approximately 6 grams, and the balloons are inflated via an air tube connected to a device on the user’s arm. This wearable haptic presentation device may improve the realism of various VR applications.
SP  - 17
EP  - 18
JF  - SIGGRAPH Asia 2019 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3355049.3360535
ER  - 

TY  - NA
AU  - Cheng, Jen-Hao; Chen, Yi; Chang, Ting-Yi; Lin, Hsu-En; Wang, Po-Yao Cosmos; Cheng, Lung-Pan
TI  - VR - Impossible Staircase: Vertically Real Walking in an Infinite Virtual Tower
PY  - 2021
AB  - We present Impossible Staircase, a real-walking virtual reality system that allows users to climb an infinite virtual tower. Our set-up consists of an one-level scaffold and a lifter. A user climbs up the scaffold by real walking on a stairway while wearing a head-mounted display, and gets reset to the ground level by a lifter imperceptibly. By repeating this process, the user perceives an illusion of climbing an infinite number of levels. Our system achieves the illusion by (1) controlling the movement of the lifter to generate reverse and imperceptible motion, (2) guiding the user through the scaffold with delay mechanisms to reset the lifter in time, and (3) procedural generating overlapping structures to enlarge perceived height of each level. We built a working system and demonstrated it with a 15-min experience. With the working system, we conducted user studies to gain deeper insights into vertical motion simulation and vertical real walking in virtual reality.
SP  - 50
EP  - 56
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00025
ER  - 

TY  - JOUR
AU  - Lipomi, Darren J.; Dhong, Charles; Carpenter, Cody W.; Root, Nicholas B.; Ramachandran, Vilayanur S.
TI  - Organic Haptics: Intersection of Materials Chemistry and Tactile Perception
PY  - 2019
AB  - The goal of the field of haptics is to create technologies that manipulate the sense of touch. In virtual and augmented reality, haptic devices are for touch what loudspeakers and RGB displays are for hearing and vision. Haptic systems that utilize micromotors or other miniaturized mechanical devices (e.g., for vibration and pneumatic actuation) produce interesting effects, but are quite far from reproducing the feeling of real materials. They are especially deficient in recapitulating surface properties: fine texture, friction, viscoelasticity, tack, and softness. The central argument of this Progress Report is that to reproduce the feel of everyday objects requires chemistry: molecular control over the properties of materials and ultimately design of materials which can change these properties in real time. Stimuli-responsive organic materials, such as polymers and composites, are a class of materials which can change their oxidation state, conductivity, shape, and rheological properties, and thus might be useful in future haptic technologies. Moreover, the use of such materials in research on tactile perception could help elucidate the limits of human tactile sensitivity. The work described represents the beginnings of this new area of inquiry, in which the defining approach is the marriage of materials science and psychology.
SP  - 1906850
EP  - NA
JF  - Advanced functional materials
VL  - 30
IS  - 29
PB  - 
DO  - 10.1002/adfm.201906850
ER  - 

TY  - JOUR
AU  - Ebel, Patrick; Orlovska, Julia; Hünemeyer, Sebastian; Wickman, Casper; Vogelsang, Andreas; Söderberg, Rikard
TI  - Automotive UX design and data-driven development: Narrowing the gap to support practitioners
PY  - 2021
AB  - Abstract The development and evaluation of In-Vehicle Information Systems (IVISs) is strongly based on insights from qualitative studies conducted in artificial contexts (e.g., driving simulators or lab experiments). However, the growing complexity of the systems and the uncertainty about the context in which they are used, create a need to augment qualitative data with quantitative data, collected during real-world driving. In contrast to many digital companies that are already successfully using data-driven methods, Original Equipment Manufacturers (OEMs) are not yet succeeding in releasing the potentials such methods offer. We aim to understand what prevents automotive OEMs from applying data-driven methods, what needs practitioners formulate, and how collecting and analyzing usage data from vehicles can enhance UX activities. We adopted a Multiphase Mixed Methods approach comprising two interview studies with more than 15 UX practitioners and two action research studies conducted with two different OEMs. From the four studies, we synthesize the needs of UX designers, extract limitations within the domain that hinder the application of data-driven methods, elaborate on unleveraged potentials, and formulate recommendations to improve the usage of vehicle data. We conclude that, in addition to modernizing the legal, technical, and organizational infrastructure, UX and Data Science must be brought closer together by reducing silo mentality and increasing interdisciplinary collaboration. New tools and methods need to be developed and UX experts must be empowered to make data-based evidence an integral part of the UX design process.
SP  - 100455
EP  - NA
JF  - Transportation Research Interdisciplinary Perspectives
VL  - 11
IS  - NA
PB  - 
DO  - 10.1016/j.trip.2021.100455
ER  - 

TY  - NA
AU  - Young, Eric M.; Kuchenbecker, Katherine J.
TI  - CHI - Ungrounded Vari-Dimensional Tactile Fingertip Feedback for Virtual Object Interaction
PY  - 2021
AB  - Compared to grounded force feedback, providing tactile feedback via a wearable device can free the user and broaden the potential applications of simulated physical interactions. However, neither the limitations nor the full potential of tactile-only feedback have been precisely examined. Here we investigate how the dimensionality of cutaneous fingertip feedback affects user movements and virtual object recognition. We combine a recently invented 6-DOF fingertip device with motion tracking, a head-mounted display, and novel contact-rendering algorithms to enable a user to tactilely explore immersive virtual environments. We evaluate rudimentary 1-DOF, moderate 3-DOF, and complex 6-DOF tactile feedback during shape discrimination and mass discrimination, also comparing to interactions with real objects. Results from 20 naive study participants show that higher-dimensional tactile feedback may indeed allow completion of a wider range of virtual tasks, but that feedback dimensionality surprisingly does not greatly affect the exploratory techniques employed by the user.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445369
ER  - 

TY  - NA
AU  - Zhang, Zhenliang; Wang, Cong; Weng, Dongdong; Liu, Yue; Wang, Yongtian
TI  - VR - Symmetrical Reality: Toward a Unified Framework for Physical and Virtual Reality
PY  - 2019
AB  - In this paper, we review the background of physical reality, virtual reality, and some traditional mixed forms of them. Based on the current knowledge, we propose a new unified concept called symmetrical reality to describe the physical and virtual world in a unified perspective. Under the framework of symmetrical reality, the traditional virtual reality, augmented reality, inverse virtual reality, and inverse augmented reality can be interpreted using a unified presentation. We analyze the characteristics of symmetrical reality from two different observation locations (i.e., from the physical world and from the virtual world), where all other forms of physical and virtual reality can be treated as special cases of symmetrical reality.
SP  - 1275
EP  - 1276
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797970
ER  - 

TY  - NA
AU  - Strohmeier, Paul; Knibbe, Jarrod; Boring, Sebastian; Hornbæk, Kasper
TI  - Tangible and Embedded Interaction - zPatch: Hybrid Resistive/Capacitive eTextile Input
PY  - 2018
AB  - We present zPatch: an eTextile patch for hover, touch, and pressure input, using both resistive and capacitive sensing. zPatches are made by layering a piezo-resistive material between silver-plated ripstop, and embedding it in non-conductive fabric to form a patch. zPatches can be easily ironed onto most fabrics, in any location, enabling easy prototyping or ad hoc modifications of existing garments. We provide open-source resources for building and programming zPatches and present measures of the achieva-ble sensing resolution of a zPatch. A pressure based targeting task demonstrated users could reliably hit pressure tar-gets at up to 13 levels, given appropriate feedback. We demonstrate that the hybrid sensing approach reduces false activations and helps distinguish between gestures. Finally, we present example applications in which we use zPatches for controlling a music player, text entry and gaming input.
SP  - 188
EP  - 198
JF  - Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173225.3173242
ER  - 

TY  - NA
AU  - Yang, Jing; Chan, Cheuk Yu
TI  - MUM - Audio-augmented museum experiences with gaze tracking
PY  - 2019
AB  - In this work, we enrich landscape and genre paintings by spatializing sounds for the drawn objects and scenes, which expands visitors' perception of the paintings and immerses them in the depicted scenarios. Plus, we personalize such spatial audio perception based on visitors' viewing behavior by applying gaze tracking. Through a preliminary user study with 14 participants, we observed that the gaze tracking-based audio augmentation helped people better focus on the areas of interest in the paintings, and enhanced their overall viewing experience.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3365610.3368415
ER  - 

TY  - JOUR
AU  - Roberto, Rafael; Lima, João Paulo; Uchiyama, Hideaki; Teichrieb, Veronica; Taniguchi, Rin-ichiro
TI  - Geometrical and statistical incremental semantic modeling on mobile devices
PY  - 2019
AB  - NA
SP  - 199
EP  - 211
JF  - Computers & Graphics
VL  - 84
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2019.09.003
ER  - 

TY  - JOUR
AU  - Hirankerd, Kongkiat; Kittisunthonphisarn, Nattapakal
TI  - E-Learning Management System Based on Reality Technology with AI
PY  - 2020
AB  - NA
SP  - 259
EP  - 264
JF  - International Journal of Information and Education Technology
VL  - 10
IS  - 4
PB  - 
DO  - 10.18178/ijiet.2020.10.4.1373
ER  - 

TY  - JOUR
AU  - Kent, Lee; Snider, Chris; Hicks, Ben
TI  - MIXED REALITY PROTOTYPING: SYNCHRONICITY AND ITS IMPACT ON A DESIGN WORKFLOW
PY  - 2021
AB  - <jats:title>Abstract</jats:title><jats:p>Design is multi-modal, and depending on the current stage in the process, progress can be facilitated through working in either the physical or virtual domain with frequent iterations commonly required between. Traditionally, prototyping workflows are sequential, although current trends such as Digital Twinning and Mixed Reality (MR) enable decreased domain transition times, reducing the cycle time. This leads towards fully integrated digital-physical prototypes, enabling work in both domains simultaneously by increasing synchronicity of select variables. This paper considers those variables involved, the sensors that measure them and their rate of synchronisation, thereby investigating the feasibility of MR workflow interventions, and exploring the benefits that may be realised. The paper identifies four components of MR implementations in prototyping and myriad methods by which domain transition may occur and uses these in context of a case study to propose four levels of workflow synchronisation. It was found achieving some high rates of synchronicity is possible, but achieving the highest levels as prescribed by digital twinning is neither feasible nor pragmatic against current MR capabilities and design prototyping workflows.</jats:p>
SP  - 2117
EP  - 2126
JF  - Proceedings of the Design Society
VL  - 1
IS  - NA
PB  - 
DO  - 10.1017/pds.2021.473
ER  - 

TY  - JOUR
AU  - Apaolaza, Aitor; Vigo, Markel
TI  - Assisted Pattern Mining for Discovering Interactive Behaviours on the Web
PY  - 2019
AB  - Abstract When the hypotheses about users’ behaviour on interactive systems are unknown or weak, mining user interaction logs in a data-driven fashion can provide valuable insights. Yet, this process is full of challenges that prevent broader adoption of data-driven methods. We address these pitfalls by assisting user researchers in customising event sets, filtering the noisy outputs of the algorithms and providing tools for analysing such outputs in an exploratory fashion. This tooling facilitates the agile testing and refinement of the formulated hypotheses of use. A user study with twenty participants indicates that compared to the baseline approach, assisted pattern mining is perceived to be more useful and produces more actionable insights, despite being more difficult to learn.
SP  - 196
EP  - 208
JF  - International Journal of Human-Computer Studies
VL  - 130
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.06.012
ER  - 

TY  - NA
AU  - Tsunekawa, Hiroki; Matsuura, Akihiro
TI  - Cartoon Sliding: An MR System for Experiencing Sliding Down a Cliff
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2021 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3476124.3488644
ER  - 

TY  - NA
AU  - Han, Teng; Anderson, Fraser; Irani, Pourang; Grossman, Tovi
TI  - UIST - HydroRing: Supporting Mixed Reality Haptics Using Liquid Flow
PY  - 2018
AB  - Current haptic devices are often bulky and rigid, making them unsuitable for ubiquitous interaction and scenarios where the user must also interact with the real world. To address this gap, we propose HydroRing, an unobtrusive, finger-worn device that can provide the tactile sensations of pressure, vibration, and temperature on the fingertip, enabling mixed-reality haptic interactions. Different from previous explorations, HydroRing in active mode delivers sensations using liquid travelling through a thin, flexible latex tube worn across the fingerpad, and has minimal impact on a user's dexterity and their perception of stimuli in passive mode. Two studies evaluated participants' ability to perceive and recognize sensations generated by the device, as well as their ability to perceive physical stimuli while wearing the device. We conclude by exploring several applications leveraging this mixed-reality haptics approach.
SP  - 913
EP  - 925
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242667
ER  - 

TY  - NA
AU  - Richard, Gregoire; Pietrzak, Thomas; Argelaguet, Ferran; Lecuyer, Anatole; Casiez, Gery
TI  - Within or Between? Comparing Experimental Designs for Virtual Embodiment Studies
PY  - 2022
AB  - When designing virtual embodiment studies, one of the key choices is the nature of the experimental factors, either between-subjects or within-subjects. However, it is well known that each design has ad-vantages and disadvantages in terms of statistical power, sample size requirements and confounding factors. This paper reports a within-subjects experiment with 92 participants comparing self-reported embodiment scores under a visuomotor task with two conditions: synchronous motions and asynchronous motions with a latency of 300 ms. With the gathered data, using a Monte-Carlo method, we created numerous simulations of within- and between-subjects experiments by selecting subsets of the data. In particular, we explored the impact of the number of participants on the replicability of the results from the 92 within-subjects experiment. For the between-subjects simulations, only the first condition for each user was considered to create the simulations. The results showed that while the replicability of the results increased as the number of participants increased for the within-subjects simulations, no matter the number of participants, between-subjects simulations were not able to replicate the initial results. We discuss the potential reasons that could have led to this surprising result and potential methodological practices to mitigate them.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00037
ER  - 

TY  - JOUR
AU  - Shell, Aliyah K.; Pena, Andres E.; Abbas, James J.; Jung, Ranu
TI  - Novel Neurostimulation-Based Haptic Feedback Platform for Grasp Interactions With Virtual Objects
PY  - 2022
AB  - <jats:p>Haptic perception is a vital part of the human experience that enriches our engagement with the world, but the ability to provide haptic information in virtual reality (VR) environments is limited. Neurostimulation-based sensory feedback has the potential to enhance the immersive experience within VR environments by supplying relevant and intuitive haptic feedback related to interactions with virtual objects. Such feedback may contribute to an increase in the sense of presence and realism in VR and may contribute to the improvement of virtual reality simulations for future VR applications. This work developed and evaluated xTouch, a neuro-haptic platform that extends the sense of touch to virtual environments. xTouch is capable of tracking a user’s grasp and manipulation interactions with virtual objects and delivering haptic feedback based on the resulting grasp forces. Seven study participants received haptic feedback delivered via multi-channel transcutaneous electrical stimulation of the median nerve at the wrist to receive the haptic feedback. xTouch delivered different percept intensity profiles designed to emulate grasp forces during manipulation of objects of different sizes and compliance. The results of a virtual object classification task showed that the participants were able to use the active haptic feedback to discriminate the size and compliance of six virtual objects with success rates significantly better than the chance of guessing it correctly (63.9 ± 11.5%, chance = 16.7%, <jats:italic>p</jats:italic> &amp;lt; 0.001). We demonstrate that the platform can reliably convey interpretable information about the physical characteristics of virtual objects without the use of hand-mounted devices that would restrict finger mobility. Thus, by offering an immersive virtual experience, xTouch may facilitate a greater sense of belonging in virtual worlds.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 3
IS  - NA
PB  - 
DO  - 10.3389/frvir.2022.910379
ER  - 

TY  - JOUR
AU  - Nilsson, Niels Christian; Zenner, André; Simeone, Adalberto L.; Johnsen, Kyle; Sandor, Christian; Billinghurst, Mark
TI  - Propping Up Virtual Reality With Haptic Proxies
PY  - 2021
AB  - Physical props serving as proxies for virtual objects (haptic proxies) offer a cheap, convenient, and compelling way of delivering a sense of touch in virtual reality (VR). To successfully use haptic proxies for VR, they have to be both similar to and colocated with their virtual counterparts. In this article, we introduce a taxonomy organizing techniques using haptic proxies for VR into eight categories based on when the techniques are deployed (offline or real-time), what reality is being manipulated (physical or virtual reality), and the purpose of the techniques (to affect object perception or the mapping between real and virtual objects). Finally, we discuss key advantages and limitations of the different categories of techniques.
SP  - 104
EP  - 112
JF  - IEEE computer graphics and applications
VL  - 41
IS  - 5
PB  - 
DO  - 10.1109/mcg.2021.3097671
ER  - 

TY  - NA
AU  - Lau, Samuel; Ragavan, Sruti Srinivasa; Milne, Ken; Barik, Titus; Sarkar, Advait
TI  - CHI - TweakIt: Supporting End-User Programmers Who Transmogrify Code
PY  - 2021
AB  - End-user programmers opportunistically copy-and-paste code snippets from colleagues or the web to accomplish their tasks. Unfortunately, these snippets often don’t work verbatim, so these people—who are non-specialists in the programming language—make guesses and tweak the code to understand and apply it successfully. To support their desired workflow and facilitate tweaking and understanding, we built a prototype tool, TweakIt, that provides users with a familiar live interaction to help them understand, introspect, and reify how different code snippets would transform their data. Through a usability study with 14 data analysts, participants found the tool to be useful to understand the function of otherwise unfamiliar code, to increase their confidence about what the code does, to identify relevant parts of code specific to their task, and to proactively explore and evaluate code. Overall, our participants were enthusiastic about incorporating TweakIt in their own day-to-day work.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445265
ER  - 

TY  - JOUR
AU  - Saga, Satoshi; Ikeda, Naoto
TI  - Dynamic Brake Control for a Wearable Impulsive Force Display by a String and a Brake System
PY  - 2021
AB  - <jats:p>In recent years, it has become possible to experience sports in the virtual reality (VR) space. Although many haptic displays in the VR environment currently use vibrators as the mainstream, the vibrators’ presentation is not suitable to express ball-receiving in the VR sports experience. Therefore, we have developed a novel haptic display that reproduces an impulsive force by instantaneously applying traction to the palm using a string and wearable brake system. This paper proposes a method to present various reaction forces by dynamic control of the braking system and report the quantitative evaluation of the device’s physical and psychological usability.</jats:p>
SP  - 1075
EP  - 1081
JF  - Journal of Robotics and Mechatronics
VL  - 33
IS  - 5
PB  - 
DO  - 10.20965/jrm.2021.p1075
ER  - 

TY  - NA
AU  - Fang, Cathy; Zhang, Yang; Dworman, Matthew; Harrison, Chris
TI  - CHI - Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics
PY  - 2020
AB  - Today's virtual reality (VR) systems allow users to explore immersive new worlds and experiences through sight. Unfortunately, most VR systems lack haptic feedback, and even high-end consumer systems use only basic vibration motors. This clearly precludes realistic physical interactions with virtual objects. Larger obstacles, such as walls, railings, and furniture are not simulated at all. In response, we developed Wireality, a self-contained worn system that allows for individual joints on the hands to be accurately arrested in 3D space through the use of retractable wires that can be programmatically locked. This allows for convincing tangible interactions with complex geometries, such as wrapping fingers around a railing. Our approach is lightweight, low-cost, and low-power, criteria important for future, worn consumer uses. In our studies, we further show that our system is fast-acting, spatially-accurate, high-strength, comfortable, and immersive.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376470
ER  - 

TY  - NA
AU  - Skinner, Haven; Possignolo, Rafael Trapani; Wang, Sheng-Hong; Renau, Jose
TI  - ISPASS - LiveSim: A Fast Hot Reload Simulator for HDLs
PY  - 2020
AB  - With the increased complexity of digital architectures and aggregation of specialized hardware, functional simulation has become a major bottleneck in digital design. During functional and performance verification of a design, engineers make several iterations to determine the impact of code changes into the simulation result. These iterations are time-consuming both because the compilation time of hardware description to binary is slow and because simulation can take several hours until the point of interest is reached. In contrast, live programming environments allow developers to manipulate the system under development as it is being run. They have become increasingly popular as they provide rapid feedback, yet there is no available live environment for hardware development. In this paper, we propose a live programming and simulation environment that targets hardware design. Our approach is language-independent and leverages incremental compilation, hot binary reloading, and checkpointing to provide fast feedback to the user. We take special care to not replicate code for multiple instances of the same module and thus prevent code bloat, for instance, for multi-and many-core architectures. Our framework also is careful in verifying the consistency across checkpoints, to leverage parallel execution and reduce the amount of code that requires compilation. Our results show that this approach can provide simulation feedback in under 2 seconds, even when simulating a 256 RISC-V multicore architecture. As a reference, Verilator did not finish compiling this architecture after 24 hours of runtime.
SP  - 126
EP  - 135
JF  - 2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ispass48437.2020.00028
ER  - 

TY  - JOUR
AU  - Nguyen, Hung Q.; Le, Thang D.; Nguyen, Diep N.; Le, Tuan D.; Lang, Thang V.; Ngo, Thang V.
TI  - Development of 3-DOF Force Feedback System Using Spherical Arm Mechanism and MR Brakes
PY  - 2020
AB  - NA
SP  - 170
EP  - 176
JF  - International Journal of Mechanical Engineering and Robotics Research
VL  - NA
IS  - NA
PB  - 
DO  - 10.18178/ijmerr.9.2.170-176
ER  - 

TY  - NA
AU  - Huard, Andrew; Chen, Mengyu; Sra, Misha
TI  - CardsVR: A Two-Person VR Experience with Passive Haptic Feedback from a Deck of Playing Cards
PY  - 2022
AB  - Presence in virtual reality (VR) is meaningful for remotely connecting with others and facilitating social interactions despite great distance while providing a sense of "being there." This work presents CardsVR, a two-person VR experience that allows remote participants to play a game of cards together. An entire deck of tracked cards are used to recreate the sense of playing cards in-person. Prior work in VR commonly provides passive haptic feedback either through a single object or through static objects in the environment. CardsVR is novel in providing passive haptic feedback through multiple cards that are individually tracked and represented in the virtual environment. Participants interact with the physical cards by picking them up, holding them, playing them, or moving them on the physical table. Our participant study (N=23) shows that passive haptic feedback provides significant improvement in three standard measures of presence: Possibility to Act, Realism, and Haptics.
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00070
ER  - 

TY  - JOUR
AU  - Gavrilenkov, Sergey I.; Petrenko, Elizaveta O.; Arbuzov, Evgeny V.
TI  - A Digital Device for Automatic Checking of Homework Assignments in the Digital Circuits Course
PY  - 2020
AB  - This paper considers a digital device for automatic checking of homework assignments in the digital circuits course. The assignment is to make a digital circuit corresponding to a given logical expression; the circuit is comprised of elementary logic gates. The process of manual testing the built circuit is very labor-intensive because checking a circuit with N inputs variables requires checking the correctness of the output variable for 2N cases. We propose automating this pro-cess with a special digital device. The device is comprised of a microcontroller connected to the circuit tested. The microcontroller is connected to a personal computer with an application written in C# for executing the main operations of the testing process. During testing, the student chooses from a database or enters the logical expression corresponding to the circuit tested. For the expression, the software generates truth tables where actual and required responses of the circuit are given. Actual circuit responses are acquired by probing the circuit via the microcontroller, and the expected values are calculated from the logical expression. The truth tables are then presented to the student with a message of whether the circuit works correctly or not. The device was integrated into the process of checking homework assignments in the digital electronics course, and it significantly sped up the process of checking homework assignment circuits, resulting in better education quality.
SP  - 04009
EP  - NA
JF  - ITM Web of Conferences
VL  - 35
IS  - NA
PB  - 
DO  - 10.1051/itmconf/20203504009
ER  - 

TY  - NA
AU  - Cabrera, Miguel Altamirano; Heredia, Juan; Tsetserukou, Dzmitry
TI  - Tactile Perception of Objects by the User's Palm for the Development of Multi-contact Wearable Tactile Displays
PY  - 2020
AB  - The user's palm plays an important role in object detection and manipulation. The design of a robust multi-contact tactile display must consider the sensation and perception of of the stimulated area aiming to deliver the right stimuli at the correct location. To the best of our knowledge, there is no study to obtain the human palm data for this purpose. The objective of this work is to introduce the method to investigate the user's palm sensations during the interaction with objects. An array of fifteen Force Sensitive Resistors (FSRs) was located at the user's palm to get the area of interaction, and the normal force delivered to four different convex surfaces. Experimental results showed the active areas at the palm during the interaction with each of the surfaces at different forces. The obtained results can be applied in the development of multi-contact wearable tactile and haptic displays for the palm, and in training a machine-learning algorithm to predict stimuli aiming to achieve a highly immersive experience in Virtual Reality.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Thomas, Lisa May; Deeks, Helen M.; Jones, Alex J.; Metatla, Oussama; Glowacki, David R.
TI  - Somatic Practices for Understanding Real, Imagined, and Virtual Realities.
PY  - 2019
AB  - In most VR experiences, the visual sense dominates other modes of sensory input, encouraging non-visual senses to respond as if the visual were real. The simulated visual world thus becomes a sort of felt actuality, where the 'actual' physical body and environment can 'drop away', opening up possibilities for designing entirely new kinds of experience. Most VR experiences place visual sensory input (of the simulated environment) in the perceptual foreground, and the physical body in the background. In what follows, we discuss methods for resolving the apparent tension which arises from VR's prioritization of visual perception. We specifically aim to understand how somatic techniques encouraging participants to 'attend to their attention' enable them to access more subtle aspects of sensory phenomena in a VR experience, bound neither by rigid definitions of vision-based virtuality nor body-based corporeality. During a series of workshops, we implemented experimental somatic-dance practices to better understand perceptual and imaginative subtleties that arise for participants whilst they are embedded in a multi-person VR framework. Our preliminary observations suggest that somatic methods can be used to design VR experiences which enable (i) a tactile quality or felt sense of phenomena in the virtual environment (VE), (ii) lingering impacts on participant imagination even after the VR headset is taken off, and (iii) an expansion of imaginative potential.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Tengxiang; Yi, Xin; Wang, Ruolin; Gao, Jiayuan; Wang, Yuntao; Yu, Chun; Li, Simin; Shi, Yuanchun
TI  - Facilitating Temporal Synchronous Target Selection through User Behavior Modeling
PY  - 2019
AB  - Temporal synchronous target selection is an association-free selection technique: users select a target by generating signals (e.g., finger taps and hand claps) in sync with its unique temporal pattern. However, classical pattern set design and input recognition algorithm of such techniques did not leverage users' behavioral information, which limits their robustness to imprecise inputs. In this paper, we improve these two key components by modeling users' interaction behavior. In the first user study, we asked users to tap a finger in sync with blinking patterns with various period and delay, and modeled their finger tapping ability using Gaussian distribution. Based on the results, we generated pattern sets for up to 22 targets that minimized the possibility of confusion due to imprecise inputs. In the second user study, we validated that the optimized pattern sets could reduce error rate from 23% to 7% for the classical Correlation recognizer. We also tested a novel Bayesian, which achieved higher selection accuracy than the Correlation recognizer when the input sequence is short. The informal evaluation results show that the selection technique can be effectively scaled to different modalities and sensing techniques.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 4
PB  - 
DO  - 10.1145/3369839
ER  - 

TY  - NA
AU  - Li, Jie; Chen, Guo; de Ridder, Huib; Cesar, Pablo
TI  - CHI Extended Abstracts - Designing a Social VR Clinic for Medical Consultations
PY  - 2020
AB  - Social Virtual Reality (VR) invites multiple users to "interact" in a shared immersive environment, which creates new opportunities for remote communication, and can potentially be a new tool for remote medical consultations. Using knee osteoarthritis consultation as a use case, this paper presents a social VR clinic that allows patients to consult a nurse represented as a virtual avatar with head, upper body and hands visible. We started with an ethnographic study at a hospital with three medical professionals and observed three patient consultation sessions to map the patient treatment journey (PTJ) and distill design requirements for social VR consultation. Based on the results of the study, we designed and implemented a social VR clinic to meet the identified requirements. Our work expands on the potential of social VR to help reshape patient treatment by reducing the workload of medical staff and the travel time of patients. In the future, we plan to conduct user studies to compare face-to-face (F2F) with social VR consultations.
SP  - 3382836
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382836
ER  - 

TY  - NA
AU  - Shahmiri, Fereshteh; Chen, Chaoyu; Waghmare, Anandghan; Zhang, Dingtian; Mittal, Shivan; Zhang, Steven L.; Wang, Yi-Cheng; Wang, Zhong Lin; Starner, Thad; Abowd, Gregory D.
TI  - CHI - Serpentine: A Self-Powered Reversibly Deformable Cord Sensor for Human Input
PY  - 2019
AB  - We introduce Serpentine, a self-powered sensor that is a reversibly deformable cord capable of sensing a variety of human input. The material properties and structural design of Serpentine allow it to be flexible, twistable, stretchable and squeezable, enabling a broad variety of expressive input modalities. The sensor operates using the principle of Triboelectric Nanogenerators (TENG), which allows it to sense mechanical deformation without an external power source. The affordances of the cord include six interactions---Pluck, Twirl, Stretch, Pinch, Wiggle and Twist. Serpentine demonstrates the ability to simultaneously recognize these inputs through a single physical interface. A 12-participant user study illustrates 95.7% accuracy for a user-dependent recognition model using a realtime system and 92.17% for user-independent offline detection. We conclude by demonstrating how Serpentine can be employed in everyday ubiquitous computing applications.
SP  - 545
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300775
ER  - 

TY  - NA
AU  - Bockes, Florian; Wimmer, Raphael; Schmid, Andreas
TI  - CHI Extended Abstracts - LagBox -- Measuring the Latency of USB-Connected Input Devices
PY  - 2018
AB  - High latency in an interactive system liMassachusetts Institute of Technologys its usability. In order to reduce end-to-end latency of such systems, it is necessary to analyze and optimize the latency of individual contributors, such as input devices, applica-tions, or displays. We present a simple tool for measur-ing the latency of USB-connected input devices with sub-millisecond accuracy. The tool, based on a Rasp-berry Pi 2 microcomputer, repeatedly toggles a button of a game controller, mouse, or keyboard via an opto-coupler soldered to the button and measures the time until the input event arrives. This helps researchers, developers and users to identify and characterize sources of input lag. An initial comparison of multiple input devices shows differences not only in average latency but also in its variance.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188632
ER  - 

TY  - NA
AU  - Lee, Injung; Kim, Hyun-Chul; Lee, Byungjoo
TI  - ACM Multimedia - Automated Playtesting with a Cognitive Model of Sensorimotor Coordination
PY  - 2021
AB  - Playtesting is widely performed in the game industry to gauge the difficulty of a game. A large number of test participants with different skills must be recruited for reliable test results, resulting in high costs. Automated playtesting based on player simulation is expected to reduce playtesting costs. Still, it has not yet been widely applied due to the lack of a method that realistically simulates players' gameplays with different skills. Based on a cognitive model of sensorimotor coordination that explains the human button input process, we propose a novel automated playtesting technique that predicts the game difficulty experienced by players with different skills in moving-target acquisition (MTA) games. The model has free parameters representing the inherent skills of players. Once the parameters are obtained for a specific population (e.g., seniors), it is possible to estimate the game difficulty at the population level in multiple games. We applied the technique to two simple MTA games and showed that it could predict the relative difference in game difficulties experienced by players with different skills.
SP  - 4920
EP  - 4929
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3475429
ER  - 

TY  - JOUR
AU  - Wang, April Yi; Mittal, Anant; Brooks, Christopher; Oney, Steve
TI  - How Data Scientists Use Computational Notebooks for Real-Time Collaboration
PY  - 2019
AB  - Effective collaboration in data science can leverage domain expertise from each team member and thus improve the quality and efficiency of the work. Computational notebooks give data scientists a convenient interactive solution for sharing and keeping track of the data exploration process through a combination of code, narrative text, visualizations, and other rich media. In this paper, we report how synchronous editing in computational notebooks changes the way data scientists work together compared to working on individual notebooks. We first conducted a formative survey with 195 data scientists to understand their past experience with collaboration in the context of data science. Next, we carried out an observational study of 24 data scientists working in pairs remotely to solve a typical data science predictive modeling problem, working on either notebooks supported by synchronous groupware or individual notebooks in a collaborative setting. The study showed that working on the synchronous notebooks improves collaboration by creating a shared context, encouraging more exploration, and reducing communication costs. However, the current synchronous editing features may lead to unbalanced participation and activity interference without strategic coordination. The synchronous notebooks may also amplify the tension between quick exploration and clear explanations. Building on these findings, we propose several design implications aimed at better supporting collaborative editing in computational notebooks, and thus improving efficiency in teamwork among data scientists.
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359141
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Ning, Chih-Wei; Wang, Po-Yao; Cheng, Jen-Hao; Cheng, Lung-Pan
TI  - CHI Extended Abstracts - Haptic-go-round: A Surrounding Platform for Encounter-type Haptics in Virtual Reality Experiences
PY  - 2020
AB  - We present Haptic-go-round, a surrounding platform that allows deploying props and devices to provide haptic feedbacks in any direction in virtual reality experiences. The key component of Haptic-go-round is a motorized turntable that rotates the correct haptic device to the right direction at the right time to match what users are about to touch. We implemented a working platform including plug-and-play prop cartridges and a software interface that allow experience designers to agilely add their haptic components and use the platform for their applications. We conducted technical experiments and two user studies on Haptic-go-round to evaluate its performance. We report the results and discuss our insights and limitations.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376476
ER  - 

TY  - JOUR
AU  - Xiong, Ying-Zi; Lorsung, Ethan A.; Mansfield, John Stephen; Bigelow, Charles; Legge, Gordon E.
TI  - Fonts Designed for Macular Degeneration: Impact on Reading.
PY  - 2018
AB  - Purpose People with macular degeneration (MD) experience difficulties in reading due to central-field loss. Two new fonts, Eido and Maxular Rx, have been designed specifically for individuals with MD. We have compared reading performance of these new fonts with three mainstream fonts (Times-Roman, Courier, and Helvetica). Methods Subjects with MD (n = 19) and normally sighted subjects (n = 40) were tested with digital versions of the MNREAD test using the five fonts. Maximum reading speed (MRS), critical print size (CPS), and reading acuity (RA) were estimated to characterize reading performance. Physical properties of the fonts were quantified by interletter spacing and perimetric complexity. Results Reading with MD showed font differences in MRS, CPS, and RA. Compared with Helvetica and Times, Maxular Rx permitted both smaller CPS and RA, and Eido permitted smaller RA. However, the two new fonts presented no advantage over Courier. Spacing, but not Complexity, was a significant predictor of reading performance for subjects with MD. Conclusions The two fonts, designed specifically for MD, permit smaller print to be read, but provide no advantage over Courier.
SP  - 4182
EP  - 4189
JF  - Investigative ophthalmology & visual science
VL  - 59
IS  - 10
PB  - 
DO  - 10.1167/iovs.18-24334
ER  - 

TY  - NA
AU  - Liu, Zhe; Chen, Chunyang; Wang, Junjie; Huang, Yuekai; Hu, Jun; Wang, Qing
TI  - Guided Bug Crush: Assist Manual GUI Testing of Android Apps via Hint Moves
PY  - 2022
AB  - Mobile apps are indispensable for people's daily life. Complementing with automated GUI testing, manual testing is the last line of defence for app quality. However, the repeated actions and easily missing of functionalities make manual testing time-consuming and inefficient. Inspired by the game candy crush with flashy candies as hint moves for players, we propose an approach named NaviDroid for navigating testers via highlighted next operations for more effective and efficient testing. Within NaviDroid, we construct an enriched state transition graph with the triggering actions as the edges for two involved states. Based on it, we utilize the dynamic programming algorithm to plan the exploration path, and augment the GUI with visualized hints for testers to quickly explore untested activities and avoid duplicate explorations. The automated experiments demonstrate the high coverage and efficient path planning of NaviDroid and a user study further confirms its usefulness. The NaviDroid can help us develop more robust software that works in more mission-critical settings, not only by performing more thorough testing with the same effort that has been put in before, but also by integrating these techniques into different parts of development pipeline.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501903
ER  - 

TY  - NA
AU  - Hartmann, Jeremy; Holz, Christian; Ofek, Eyal; Wilson, Andrew D.
TI  - CHI - RealityCheck: Blending Virtual Environments with Situated Physical Reality
PY  - 2019
AB  - Today's virtual reality (VR) systems offer chaperone rendering techniques that prevent the user from colliding with physical objects. Without a detailed geometric model of the physical world, these techniques offer limited possibility for more advanced compositing between the real world and the virtual. We explore this using a realtime 3D reconstruction of the real world that can be combined with a virtual environment. RealityCheck allows users to freely move, manipulate, observe, and communicate with people and objects situated in their physical space without losing the sense of immersion or presence inside their virtual world. We demonstrate RealityCheck with seven existing VR titles, and describe compositing approaches that address the potential conflicts when rendering the real world and a virtual environment together. A study with frequent VR users demonstrate the affordances provided by our system and how it can be used to enhance current VR experiences.
SP  - 347
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300577
ER  - 

TY  - NA
AU  - Clarence, Aldrich; Knibbe, Jarrod; Cordeil, Maxime; Wybrow, Michael
TI  - VR - Unscripted Retargeting: Reach Prediction for Haptic Retargeting in Virtual Reality
PY  - 2021
AB  - Research is exploring novel ways of adding haptics to VR. One popular technique is haptic retargeting, where real and virtual hands are decoupled to enable the reuse of physical props. However, this technique requires the system to know the users' intended interaction target, or requires additional hardware for prediction. We explore software-based reach prediction as a means of facilitating responsive, unscripted retargeting. We trained a Long Short-Term Memory network on users' reach trajectories to predict intended targets. We achieved an accuracy of 81.1 % at approximately 65% of movement. This could enable haptic retargeting during the last 35% of movement. We discuss the implications for possible physical proxy locations.
SP  - 150
EP  - 159
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00036
ER  - 

TY  - CHAP
AU  - Fournier, Helene; Molyneaux, Heather; Kondratova, Irina; Ali, Noor
TI  - Privacy by Design and Cybersecurity for Safe, Effective and Reliable Home Health Care for Aging in Place
PY  - 2019
AB  - This short paper presents findings from a research and development project on remote home health monitoring, specifically tools and technologies for Aging in Place to help seniors live independently at home for longer. A comprehensive literature review was completed to feed into an early concept design and prototype for a mobile video-conferencing application for Android, for use in remote health care services. Findings from the literature review will be presented as well as next phases of the research and development process.
SP  - 442
EP  - 450
JF  - Learning and Analytics in Intelligent Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-36778-7_49
ER  - 

TY  - JOUR
AU  - Vieira, Daniel; Carvalho, Helder; Providência, Bernado
TI  - E-Textiles for Sports: A Systematic Review
PY  - 2022
AB  - <jats:p>This work presents a systematic review to provide an overview of the possibilities for coupling, fabrication or embedding of electronics into textiles whilst assuring the capability of these products to meet the requirements of a sports modality. The development of smart wearables systems for sports based on textiles attracts more and more users – motivated by design, by technology, as well as by the expectation of increased performance. A bibliographic search was carried out using the following databases: Scopus, Web of Science, IEEE Xplore and Science Direct. This study includes 32 articles and discusses these in a new taxonomy with three dimensions: measured variable, types of feedback and applications. Of the 23 technologies surveyed, this review showed that these wearable systems are mainly used for vital signs monitoring and to provide feedback on the electrical activity of the heart, with sensors mostly placed in the chest. Usually, the technologies are externally attachable rather than embedded in the textile. We observed that the implementation of design as the process of development of e-textile products is still only scarcely present in these studies.</jats:p>
SP  - 37
EP  - 46
JF  - Journal of Biomimetics, Biomaterials and Biomedical Engineering
VL  - 57
IS  - NA
PB  - 
DO  - 10.4028/p-e03md3
ER  - 

TY  - NA
AU  - Strohmeier, Paul; Pourjafarian, Narjes; Koelle, Marion; Honnet, Cedric; Fruchard, Bruno; Steimle, Jürgen
TI  - AHs - Sketching On-Body Interactions using Piezo-Resistive Kinesiology Tape
PY  - 2020
AB  - Skin is personal and sensitive. As a result, design and placement of on-body physical interfaces need to be well thought out. One way of "getting the design right" is to quickly sketch a multitude of designs to be modified, adjusted and elaborated on. To date, on-body rapid prototyping methods do not afford these "quick-and-dirty" design processes. We propose using piezo-resistive kinesiology tape as a low-cost and versatile resource for sketching functional on-skin interfaces. Our method uses pretreated kinesiology tape, which is made piezo-resistive through polymerization, and serves as touch, pressure and stretch sensor. We illustrate ketching techniques with both pretreated and untreated tape for iterative design of on-skin interfaces. In addition, we contribute a set of sensor primitives that facilitate various input modalities for creating interactive sketches.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384774
ER  - 

TY  - NA
AU  - Margiene, Asta; Ramanauskaite, Simona
TI  - Trends and Challenges of Multimodal User Interfaces
PY  - 2019
AB  - The development of information technologies brings new possibilities for its users. The development is noticeable in user interface design as well – new data input and output methods and facilities are proposed, increasing resource capabilities allow more realistic user interfaces. However together with the growth of user interface possibilities and technologies new challenges rise too. Therefore it is important to understand current challenges in order to understand future trends. In this paper we review the peculiarities of multimodal user interfaces and propose multimodal user interface taxonomy. This taxonomy defines possible types of data input and output and is used for input/output combination matrix generation. We summarize existing solutions and challenges based on the matrix and evaluate each combination. The evaluated combinations in the matrix highlight the main tendencies in natural user interface design area.
SP  - NA
EP  - NA
JF  - 2019 Open Conference of Electrical, Electronic and Information Sciences (eStream)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/estream.2019.8732156
ER  - 

TY  - NA
AU  - Mathis, Florian; Vaniea, Kami; Khamis, Mohamed
TI  - CHI - RepliCueAuth: Validating the Use of a Lab-Based Virtual Reality Setup for Evaluating Authentication Systems
PY  - 2021
AB  - Evaluating novel authentication systems is often costly and time-consuming. In this work, we assess the suitability of using Virtual Reality (VR) to evaluate the usability and security of real-world authentication systems. To this end, we conducted a replication study and built a virtual replica of CueAuth [52], a recently introduced authentication scheme, and report on results from: (1) a lab-based in-VR usability study (N=20) evaluating user performance; (2) an online security study (N=22) evaluating system’s observation resistance through virtual avatars; and (3) a comparison between our results and those previously reported in the real-world evaluation. Our analysis indicates that VR can serve as a suitable test-bed for human-centred evaluations of real-world authentication schemes, but the used VR technology can have an impact on the evaluation. Our work is a first step towards augmenting the design and evaluation spectrum of authentication systems and offers ground work for more research to follow.
SP  - 1
EP  - 18
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445478
ER  - 

TY  - NA
AU  - Çamci, Anil
TI  - Audio Mostly Conference - Teaching immersive media at the "dawn of the new everything"
PY  - 2020
AB  - In this paper, we discuss the design and implementation of a college-level course on immersive media at a performing arts institution. Focusing on the artistic applications of modern virtual reality technologies, the course aims to offer students a practice-based understanding of the concepts, tools and techniques involved in the design of audiovisual immersive systems and experiences. We describe the course structure and outline the intermixing of practical exercises with critical theory. We provide details of the design projects and discussion tasks assigned throughout the semester. We then discuss the outcome of a course evaluation session conducted with students. Finally, we identify the main challenges and opportunities for educators dealing with modern immersive media technologies with the hope that the findings offered in this paper can support the design and delivery of similar courses in a range of music and arts curricula.
SP  - 229
EP  - 232
JF  - Proceedings of the 15th International Audio Mostly Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411109.3411121
ER  - 

TY  - JOUR
AU  - Sundin, Lovisa; Sakr, Nourhan; Leinonen, Juho; Cutts, Quintin
TI  - Facilitating API lookup for novices learning data wrangling using thumbnail graphics
PY  - 2021
AB  - <jats:p xml:lang="fr">&lt;p style='text-indent:20px;'&gt;With the rising demand for data science skills, the ability to wrangle data programmatically becomes a crucial barrier. In this paper, we discuss the centrality of API (application programming interface) lookup to data wrangling, and how an ontology-structured command menu could facilitate it. We design thumbnail graphics as visual alternatives to explaining data wrangling operations and use a survey to validate their quality. We furthermore predict that thumbnail graphics make the menu more navigable, improving lookup efficiency and performance. Our predictions are tested using Slice N Dice, an online data wrangling tutorial platform that collects learner activity. It includes both non-programmatic and programmatic data wrangling exercises. Participants from a multi-institutional sample (&lt;i&gt;n&lt;/i&gt; = 200) were randomly assigned the tutorial either with or without thumbnail graphics. Our results show that thumbnail graphics reduce the need for clarifications, thereby assisting API lookup for novices learning data wrangling. We further present some negative results regarding performance gain and follow up with a discussion on why the differences are subtle and how they can be improved. Last but not least, we complement our statistical results with a qualitative study where we receive positive feedback from our participants on the design and helpfulness of the thumbnail graphics.&lt;/p&gt;</jats:p>
SP  - 0
EP  - NA
JF  - Foundations of Data Science
VL  - 0
IS  - 0
PB  - 
DO  - 10.3934/fods.2021032
ER  - 

TY  - NA
AU  - Khamis, Mohamed; Alt, Florian; Bulling, Andreas
TI  - MobileHCI - The past, present, and future of gaze-enabled handheld mobile devices: survey and lessons learned
PY  - 2018
AB  - While first-generation mobile gaze interfaces required special-purpose hardware, recent advances in computational gaze estimation and the availability of sensor-rich and powerful devices is finally fulfilling the promise of pervasive eye tracking and eye-based interaction on off-the-shelf mobile devices. This work provides the first holistic view on the past, present, and future of eye tracking on handheld mobile devices. To this end, we discuss how research developed from building hardware prototypes, to accurate gaze estimation on unmodified smartphones and tablets. We then discuss implications by laying out 1) novel opportunities, including pervasive advertising and conducting in-the-wild eye tracking studies on handhelds, and 2) new challenges that require further research, such as visibility of the user's eyes, lighting conditions, and privacy implications. We discuss how these developments shape MobileHCI research in the future, possibly the next 20 years.
SP  - 38
EP  - NA
JF  - Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229434.3229452
ER  - 

TY  - NA
AU  - Shimizu, Shuntaro; Hashimoto, Takeru; Yoshida, Shigeo; Matsumura, Reo; Narumi, Takuji; Kuzuoka, Hideaki
TI  - VR - Unident: Providing Impact Sensations on Handheld Objects via High-Speed Change of the Rotational Inertia
PY  - 2021
AB  - Several virtual reality (VR) proxies have been developed that can emulate impact sensations by generating actual forces on the hand. Although these proxies contribute to increasing the reality of VR, they still have some limitations, such as high latency, high power consumption, and low frequency to provide impact sensations. To overcome these limitations, we first propose a method to provide an impact sensation without actual force generation by quickly changing the rotational inertia of a handheld proxy while users are swinging it. Then, we developed Unident, a handheld proxy capable of changing its rotational inertia by moving a weight along one axis at a high speed. Two experiments were conducted to evaluate the ability of Unident to provide users with impact sensations. In the first experiment, we demonstrate that Unident can physically provide an impact sensation applied to a handheld object by analyzing the pressure on the user's palm. The second experiment shows that Unident can provide an impact sensation with various magnitudes depending on the amount of rotational inertia to be changed. Finally, we present an application that can be enabled by Unident.
SP  - 11
EP  - 20
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00021
ER  - 

TY  - JOUR
AU  - Feger, Sebastian S.; Semmler, Lars; Schmidt, Albrecht; Kosch, Thomas
TI  - ElectronicsAR: Design and Evaluation of a Mobile and Tangible High-Fidelity Augmented Electronics Toolkit
PY  - 2022
AB  - <jats:p>Exploring and interacting with electronics is challenging as the internal processes of components are not visible. Further barriers to engagement with electronics include fear of injury and hardware damage. In response, Augmented Reality (AR) applications address those challenges to make internal processes and the functionality of circuits visible. However, current apps are either limited to abstract low-fidelity applications or entirely virtual environments. We present ElectronicsAR, a tangible high-fidelity AR electronics kit with scaled hardware components representing the shape of real electronics. Our evaluation with 24 participants showed that users were more efficient and more effective at naming components, as well as building and debugging circuits. We discuss our findings in the context of ElectronicsAR's unique characteristics that we contrast with related work. Based on this, we discuss opportunities for future research to design functional mobile AR applications that meet the needs of beginners and experts.</jats:p>
SP  - 700
EP  - 721
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567740
ER  - 

TY  - JOUR
AU  - Çamci, Anil; Hamilton, Robert
TI  - Audio-first VR: New perspectives on musical experiences in virtual environments
PY  - 2020
AB  - This special issue of the Journal of New Music Research explores VR (Virtual Reality) through the lenses of music, art and technology, each focusing on foregrounded sonic expression – an audio-firs...
SP  - 1
EP  - 7
JF  - Journal of New Music Research
VL  - 49
IS  - 1
PB  - 
DO  - 10.1080/09298215.2019.1707234
ER  - 

TY  - CHAP
AU  - Cabrera, Miguel Altamirano; Heredia, Juan; Tsetserukou, Dzmitry
TI  - EuroHaptics - Tactile Perception of Objects by the User’s Palm for the Development of Multi-contact Wearable Tactile Displays
PY  - 2020
AB  - The user’s palm plays an important role in object detection and manipulation. The design of a robust multi-contact tactile display must consider the sensation and perception of the stimulated area, aiming to deliver the right stimuli at the correct location. To the best of our knowledge, there is no study to obtain the human palm data for this purpose. The objective of this work is to introduce a method to investigate the user’s palm sensations during the interaction with objects. An array of fifteen Force Sensitive Resistors (FSRs) was located at the user’s palm to get the area of interaction, and the normal force delivered to four different convex surfaces. Experimental results showed the active areas at the palm during the interaction with each of the surfaces at different forces. The obtained results were verified in an experiment for pattern recognition to discriminate the applied force. The patterns were delivered in correlation with the acquired data from the previous experiment. The overall recognition rate equals 84%, which means that user can distinguish four patterns with high confidence. The obtained results can be applied in the development of multi-contact wearable tactile and haptic displays for the palm, and in training a machine-learning algorithm to predict stimuli aiming to achieve a highly immersive experience in Virtual Reality.
SP  - 51
EP  - 59
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58147-3_6
ER  - 

TY  - NA
AU  - Lee, Sang Won
TI  - Liveness in Interactive Systems.
PY  - NA
AB  - Creating an artifact in front of public offers an opportunity to involve spectators in the creation process. For example, in a live music concert, audience members can clap, stomp and sing with the musicians to be part of the music piece. Live creation can facilitate collaboration with the spectators. The questions I set out to answer are what does it mean to have liveness in interactive systems to support large-scale hybrid events that involve audience participation. The notion of liveness is subtle in human-computer interaction. In this paper, I revisit the notion of liveness and provide definitions of both live and liveness from the perspective of designing interactive systems. In addition, I discuss why liveness matters in facilitating hybrid events and suggest future research works
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.5281/zenodo.1471026
ER  - 

TY  - BOOK
AU  - Casanova, Nicolas; Cabezas, Holman; Cespedes, Angie; Araque, Dario; Ospina, Daniel; Garzon-Morales, Elizabeth; Cortés-Rico, Laura; Sarmiento, Wilson J.
TI  - VR Workshops - Touch & Live. An immersive experience for acting in others’ bodies
PY  - 2020
AB  - This work shows an immersive experience for the user to feel a disability while doing a simple task. The narrative looks for that the user feels empathy with other’s conditions through living a situation multiple times, first in the body of someone without disabilities and then, each time in the body of a different person with a different disability: hearing impairment, visual disability, and reduced mobility. So, we support the experience in human-mediated-interactions, focused on triggering the animations and providing physical-haptic feedback, including a touch that triggers the virtual change of the body for re-living the situation, like a deja vu.
SP  - 507
EP  - 508
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00105
ER  - 

TY  - NA
AU  - Trinitatova, Daria; Tsetserukou, Dzmitry
TI  - WHC - DeltaTouch: a 3D Haptic Display for Delivering Multimodal Tactile Stimuli at the Palm
PY  - 2019
AB  - DeltaTouch is a novel wearable haptic display with inverted Delta structure for providing multimodal tactile stimuli at any point of the palm of Virtual Reality (VR) user or operator of remote robot. It is capable of generating 3D force vector at the contact point and presenting multimodal tactile sensation of weight, slippage, encounter, and texture. The BallFeel, BalanceIt, and AnimalFeel applications have been developed to demonstrate the capabilities of the proposed technology. In the first user study, we evaluated the perception of tactile cues delivered by the haptic display on a participants’ palm. The second experiment estimated the distinction in the perception of different forces generating by DeltaTouch on the human palm in a virtual weight sorting task. DeltaTouch can potentially bring a new level of immersion of the user in VR and improve the quality of teleoperation of the robot equipped with tactile and force sensors.
SP  - 73
EP  - 78
JF  - 2019 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc.2019.8816136
ER  - 

TY  - JOUR
AU  - Jungnickel, Tobias; von Jan, Ute; Albrecht, Urs-Vito
TI  - Implementation of Mobile Psychological Testing on Smart Devices: Evaluation of a ResearchKit-Based Design Approach for the Implicit Association Test.
PY  - 2022
AB  - <AbstractText Label="Objective" NlmCategory="UNASSIGNED">To determine whether a framework-based approach for mobile apps is appropriate for the implementation of psychological testing, and equivalent to established methods.</AbstractText> <AbstractText Label="Methods" NlmCategory="UNASSIGNED">Apple's ResearchKit was used for implementing native implicit association test methods (IAT), and an exemplary app was developed to examine users' implicit attitudes toward overweight or thin individuals. For comparison, a web-based IAT app, based on code provided by Project Implicit, was used. Adult volunteers were asked to test both versions on an iPad with touch as well as keyboard input (altogether four tests per participant, random order). Latency values were recorded and used to calculate parameters relevant to the implicit setting. Measurements were analyzed with respect to app type and input method, as well as test order (ANOVA and χ<sup>2</sup> tests).</AbstractText> <AbstractText Label="Results" NlmCategory="UNASSIGNED">Fifty-one datasets were acquired (female, <i>n</i> = 21; male, <i>n</i> = 30, average age 35 ± 4.66 years). Test order and combination of app type and input method influenced the latency values significantly (both <i>P</i>&lt;0.001). This was not mirrored for the D scores or average number of errors vs. app type combined with input method (D scores: <i>P</i> = 0.66; number of errors: <i>P</i> = 0.733) or test order (D scores: <i>P</i> = 0.096; number of errors: <i>P</i> = 0.85). <i>Post-hoc</i> power analysis of the linear ANOVA showed 0.8 by <i>f</i> <sup>2</sup>=0.25, with α = 0.05 and 4 predictors.</AbstractText> <AbstractText Label="Conclusions" NlmCategory="UNASSIGNED">The results suggest that a native mobile implementation of the IAT may be comparable to established implementations. The validity of the acquired measurements seems to depend on the properties of the chosen test rather than the specifics of the chosen platform or input method.</AbstractText> <CopyrightInformation>Copyright © 2022 Jungnickel, von Jan and Albrecht.</CopyrightInformation>
SP  - 785591
EP  - NA
JF  - Frontiers in digital health
VL  - 4
IS  - NA
PB  - 
DO  - 10.3389/fdgth.2022.785591
ER  - 

TY  - NA
AU  - Lee, Jaeyeon; Sinclair, Mike; Gonzalez-Franco, Mar; Ofek, Eyal; Holz, Christian
TI  - CHI - TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction
PY  - 2019
AB  - Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. Our evaluation showed that using TORC, participants could manipulate virtual objects more precisely (e.g., position and rotate objects in 3D) than when using a conventional VR controller.
SP  - 71
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300301
ER  - 

TY  - NA
AU  - Khorsandi, Pouya M.; Nousir, Alaa; Nabil, Sara
TI  - Functioning E-Textile Sensors for Car Infotainment Applications
PY  - 2022
AB  - Car interiors are envisioned to be living spaces that support a variety of non-driving-related activities. Previous work focuses on enhancing driving-related functions, performance and safety. By developing textile-based sensors, we focus on enabling non-driving activities integrated in the car interior and supporting a richer user experience. In this paper, we introduce an array of new applications using e-textile sensors to the design space of car interiors. Our functional prototypes implement hand interactions (such as press and double tap gestures) on the leather or fabric of the steering wheel and back of the head rest. We then propose applications for these sensors to control media, car windows, and air-conditioning. Overall, the paper contributes a novel tactile input modality to support drivers and empower backseat passengers.
SP  - NA
EP  - NA
JF  - The 3rd International Conference on the Challenges, Opportunities, Innovations and Applications in Electronic Textiles
VL  - NA
IS  - NA
PB  - 
DO  - 10.3390/engproc2022015022
ER  - 

TY  - NA
AU  - Saquib, Nazmus; Kazi, Rubaiat Habib; Wei, Li-Yi; Li, Wilmot
TI  - CHI - Interactive Body-Driven Graphics for Augmented Video Performance
PY  - 2019
AB  - We present a system that augments live presentation videos with interactive graphics to create a powerful and expressive storytelling environment. Using our system, the presenter interacts with the graphical elements in real-time with gestures and postures, thus leveraging our innate, everyday skills to enhance our communication capabilities with the audience. However, crafting such an interactive and expressive performance typically requires programming, or highly-specialized tools tailored for experts. Our core contribution is a flexible, direct manipulation UI which enables amateurs and experts to craft such presentations beforehand by mapping a variety of body movements to a wide range of graphical manipulations. By simplifying the mapping between gestures, postures, and their corresponding output effects, our UI enables users to craft customized, rich interactions with the graphical elements. Our user study demonstrates the potential usage and unique affordance of this mixed-reality medium for storytelling and presentation across a range of application domains.
SP  - 622
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300852
ER  - 

TY  - NA
AU  - Pu, Kevin; Fu, Rainey; Dong, Rui; Wang, Xinyu; Chen, Yan; Grossman, Tovi
TI  - SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545691
ER  - 

TY  - JOUR
AU  - Park, So Yeon; Lee, Sang Won
TI  - Lost in Co-curation: Uncomfortable Interactions and the Role of Communication in Collaborative Music Playlists
PY  - 2021
AB  - Online tools enable users to co-create artifacts remotely. However, creative collaborations can also occur for the social process of collaboration itself, for which measures of success and engagement expectations can be more ambiguous, and individuals' dedication and social dynamics more important. Co-curating music in collaborative playlists (CPs) is one example of creative collaboration that encompasses both roles, and can therefore have more subtleties within its interactions. We conducted two studies using online surveys to understand perceived comfort with and hesitation toward the social dynamics embedded in CPs. Differences in collaborators' ownership perceptions toward CPs and their comfort in interacting with these CPs emerged. We also found a varying desire for situated communication, dependent upon the action taken and perceived ownership (of both a CP and the songs contained), with more users expecting greater comfort when a communication channel exists. From these results, we present four design considerations for more positive and engaging experiences in creative online co-curation.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
PB  - 
DO  - 10.1145/3449137
ER  - 

TY  - NA
AU  - Pratte, Sydney
TI  - Designing Fashion Technology Garments for Awareness
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3503580
ER  - 

TY  - NA
AU  - Ryu, Neung; Lee, Woojin; Kim, Myung Jin; Bianchi, Andrea
TI  - UIST - ElaStick: A Handheld Variable Stiffness Display for Rendering Dynamic Haptic Response of Flexible Object
PY  - 2020
AB  - Haptic controllers have an important role in providing rich and immersive Virtual Reality (VR) experiences. While previous works have succeeded in creating handheld devices that simulate dynamic properties of rigid objects, such as weight, shape, and movement, recreating the behavior of flexible objects with different stiffness using ungrounded controllers remains an open challenge. In this paper we present ElaStick, a variable-stiffness controller that simulates the dynamic response resulting from shaking or swinging flexible virtual objects. This is achieved by dynamically changing the stiffness of four custom elastic tendons along a joint that effectively increase and reduce the overall stiffness of a perceived object in 2-DoF. We show that with the proposed mechanism, we can render stiffness with high precision and granularity in a continuous range between 10.8 and 71.5Nmm/degree. We estimate the threshold of the human perception of stiffness with a just-noticeable difference (JND) study and investigate the levels of immersion, realism and enjoyment using a VR application.
SP  - 1035
EP  - 1045
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415862
ER  - 

TY  - JOUR
AU  - Raees, Muhammad; Ullah, Sehat
TI  - RUN: rational ubiquitous navigation, a model for automated navigation and searching in virtual environments
PY  - 2020
AB  - By now, the realm of virtual reality is abuzz with high-quality visuals, enough to simulate a real-world scene. The use of intelligence in virtual reality systems, however, is a milestone yet to be achieved to make possible seamless realism in a virtual environment. This paper presents a model, rational ubiquitous navigation to improve believability of a virtual environment. The model intends to augment maturity of a virtual agent by inculcating in it the human-like learning capability. A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality. An intelligent virtual agent learns objects of interest along with the paths followed for navigation. A mental map is molded dynamically as a user navigates in the environment. The map is followed by the agent during self-directed navigation to access any known object. After reaching at a location where an object of interest resides, the required object is selected on the basis of front-facet feature. The model is implemented in a case-study project learn objects on path (LOOP). Twelve users evaluated the model in the immersive maze-like environment of LOOP. Results of the evaluation assure applicability of the model in various cross-modality applications.
SP  - 511
EP  - 521
JF  - Virtual Reality
VL  - 25
IS  - 2
PB  - 
DO  - 10.1007/s10055-020-00468-0
ER  - 

TY  - JOUR
AU  - Bartalucci, Lorenzo; Secciani, Nicola; Brogi, Chiara; Topini, Alberto; Della Valle, Andrea; Ridolfi, Alessandro; Allotta, Benedetto
TI  - An original mechatronic design of a kinaesthetic hand exoskeleton for virtual reality-based applications
PY  - NA
AB  - NA
SP  - 102947
EP  - NA
JF  - Mechatronics
VL  - 90
IS  - NA
PB  - 
DO  - 10.1016/j.mechatronics.2023.102947
ER  - 

TY  - NA
AU  - Velloso, Eduardo; Morimoto, Carlos H.
TI  - A Probabilistic Interpretation of Motion Correlation Selection Techniques
PY  - 2021
AB  - Motion correlation interfaces are those that present targets moving in different patterns, which the user can select by matching their motion. In this paper, we re-formulate the task of target selection as a probabilistic inference problem. We demonstrate that previous interaction techniques can be modelled using a Bayesian approach and that how modelling the selection task as transmission of information can help us make explicit the assumptions behind similarity measures. We propose ways of incorporating uncertainty into the decision-making process and demonstrate how the concept of entropy can illuminate the measurement of the quality of a design. We apply these techniques in a case study and suggest guidelines for future work.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445184
ER  - 

TY  - NA
AU  - Zhang, Yiran; Nguyen, Huyen; Ladeveze, Nicolas; Fleury, Cedric; Bourdot, Patrick
TI  - Virtual Workspace Positioning Techniques during Teleportation for Co-located Collaboration in Virtual Reality using HMDs
PY  - 2022
AB  - In many collaborative virtual reality applications, co-located users often have their relative position in the virtual environment matching the one in the real world. The resulting spatial consistency facilitates the co-manipulation of shared tangible props and enables the users to have direct physical contact with each other. However, these applications usually exclude their individual virtual navigation capability, such as teleportation, as it may break the spatial configuration between the real and virtual world. As a result, the users can only explore the virtual environment of approximately similar size and shape compared to their physical workspace. Moreover, their individual tasks with unlimited virtual navigation capability, which often take part in a continuous workflow of a complex collaborative scenario, have to be removed due to this constraint. This work aims to help overcome these limits by allowing users to recover spatial consistency after individual teleportation in order to re-establish their position in the current context of the collaborative task. We use a virtual representation of the user&#x2019;s shared physical workspace and develop two different techniques to position it in the virtual environment. The first technique allows one user to fully position the virtual workspace, and the second approach enables concurrent positioning by equally integrating the input from all the users. We compared these two techniques in a controlled experiment in a virtual assembly task. The results show that allowing two users to manipulate the workspace significantly reduced the time they spent negotiating the position of the future workspace. However, the inevitable conflicts in simultaneous co-manipulation were also a little confusing to them.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00088
ER  - 

TY  - JOUR
AU  - Poretski, Lev; Lanir, Joel; Margalit, Ram; Arazy, Ofer
TI  - Physicality As an Anchor for Coordination: Examining Collocated Collaboration in Physical and Mobile Augmented Reality Settings
PY  - 2021
AB  - Design and co-creation activities around 3D artifacts often require close collocated coordination between multiple users. Augmented reality (AR) technology can support collocated work enabling users to flexibly work with digital objects while still being able to use the physical space for coordination. With most of current research focusing on remote AR collaboration, less is known about collocated collaboration in AR, particularly in relation to interpersonal dynamics between the collocated collaborators. Our study aims at understanding how shared augmented reality facilitated by mobile devices (mobile augmented reality or MAR) affects collocated users' coordination. We compare the coordination behaviors that emerged in a MAR setting with those in a comparable fully physical setting by simulating the same task -of the shared physical dimension for participants' ability to coordinate in the context of collaborative co-creation. Namely, participants working in a fully physical setting were better able to leverage the work artifact itself for their coordination needs, working in a mode that we term artifact-oriented coordination. Conversely, participants collaborating around an AR artifact leveraged the shared physical workspace for their coordination needs, working in what we refer to as space-oriented coordination. We discuss implications for a AR-based collaboration and propose directions for designers of AR tools.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW2
PB  - 
DO  - 10.1145/3479857
ER  - 

TY  - JOUR
AU  - Wee, Chyanna; Yap, Kian Meng; Lim, Woan Ning
TI  - Haptic Interfaces for Virtual Reality: Challenges and Research Directions
PY  - 2021
AB  - The sense of touch (haptics) has been applied in several areas such as tele-haptics, tele-medicine, training, education, and entertainment. As of today, haptics is used and explored by researchers in many more multi-disciplinary and inter-disciplinary areas. The utilization of haptics is also enhanced with other forms of media such as audio, video, and even sense of smell. For example, the use of haptics is prevalent in virtual reality environments to increase the immersive experience for users. However, while there has been significant progress within haptic interfaces throughout the years, there are still many challenges that limit their development. This review highlights haptic interfaces for virtual reality ranging from wearables, handhelds, encountered-type devices, and props, to mid-air approaches. We discuss and summarize these approaches, along with interaction domains such as skin receptors, object properties, and force. This is in order to arrive at design challenges for each interface, along with existing research gaps.
SP  - 112145
EP  - 112162
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3103598
ER  - 

TY  - NA
AU  - Zhu, Fengyuan; Grossman, Tovi
TI  - CHI - BISHARE: Exploring Bidirectional Interactions Between Smartphones and Head-Mounted Augmented Reality
PY  - 2020
AB  - In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376233
ER  - 

TY  - NA
AU  - Olwal, Alex; Moeller, Jon; Priest-Dorman, Greg; Starner, Thad; Carroll, Ben
TI  - I/O Braid
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3271651
ER  - 

TY  - NA
AU  - Wang, Chiu-Hsuan; Chen, Bing-Yu; Chan, Liwei
TI  - RealityLens: A User Interface for Blending Customized Physical World View into Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545686
ER  - 

TY  - NA
AU  - McDonald, Denisa Qori; Mahajan, Shruti; Vallett, Richard; Dion, Genevieve; Shokoufandeh, Ali; Solovey, Erin
TI  - Interaction with Touch-Sensitive Knitted Fabrics: User Perceptions and Everyday Use Experiments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502077
ER  - 

TY  - BOOK
AU  - Velloso, Eduardo; Coutinho, Flávio Luiz; Kurauchi, Andrew; Morimoto, Carlos H.
TI  - ETRA - Circular orbits detection for gaze interaction using 2D correlation and profile matching algorithms
PY  - 2018
AB  - Recently, interaction techniques in which the user selects screen targets by matching their movement with the input device have been gaining popularity, particularly in the context of gaze interaction (e.g. Pursuits, Orbits, AmbiGaze, etc.). However, though many algorithms for enabling such interaction techniques have been proposed, we still lack an understanding of how they compare to each other. In this paper, we introduce two new algorithms for matching eye movements: Profile Matching and 2D Correlation, and present a systematic comparison of these algorithms with two other state-of-the-art algorithms: the Basic Correlation algorithm used in Pursuits and the Rotated Correlation algorithm used in PathSync. We also examine the effects of two thresholding techniques and post-hoc filtering. We evaluated the algorithms on a user dataset and found the 2D Correlation with one-level thresholding and post-hoc filtering to be the best performing algorithm.
SP  - 25
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204524
ER  - 

TY  - BOOK
AU  - Khamis, Mohamed; Kienle, Anna; Alt, Florian; Bulling, Andreas
TI  - DroNet@MobiSys - GazeDrone: Mobile Eye-Based Interaction in Public Space Without Augmenting the User
PY  - 2018
AB  - Gaze interaction holds a lot of promise for seamless human-computer interaction. At the same time, current wearable mobile eye trackers require user augmentation that negatively impacts natural user behavior while remote trackers require users to position themselves within a confined tracking range. We present GazeDrone, the first system that combines a camera-equipped aerial drone with a computational method to detect sidelong glances for spontaneous (calibration-free) gaze-based interaction with surrounding pervasive systems (e.g., public displays). GazeDrone does not require augmenting each user with on-body sensors and allows interaction from arbitrary positions, even while moving. We demonstrate that drone-supported gaze interaction is feasible and accurate for certain movement types. It is well-perceived by users, in particular while interacting from a fixed position as well as while moving orthogonally or diagonally to a display. We present design implications and discuss opportunities and challenges for drone-supported gaze interaction in public.
SP  - 66
EP  - 71
JF  - Proceedings of the 4th ACM Workshop on Micro Aerial Vehicle Networks, Systems, and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3213526.3213539
ER  - 

TY  - JOUR
AU  - Yan, Chenggang; Shao, Biyao; Zhao, Hao; Ruixin, Ning; Zhang, Yongdong; Xu, Feng
TI  - 3D Room Layout Estimation From a Single RGB Image
PY  - 2020
AB  - 3D layout is crucial for scene understanding and reconstruction, and very useful in applications like real estate and furniture design. In this paper, we propose a fully automatic solution to estimate 3D layout of an indoor scene from a single 2D image. Our technique contains two key components. Firstly, we train a neural network that directly estimates room structure lines from the input image. Secondly, we propose a novel technique to automatically identify the layout topology of an input image, followed by a nonlinear optimization with equality constraints to estimate the final 3D layout of a scene. Based on our knowledge, this is the first fully automatic technique to achieve single image-based 3D layout estimation of an indoor scene. We evaluate our method on the public datasets $LSUN$ , $Hedau$ and $3DGP$ and the results show that the proposed method achieves accurate 3D layout reconstruction on various images with different layout topologies.
SP  - 3014
EP  - 3024
JF  - IEEE Transactions on Multimedia
VL  - 22
IS  - 11
PB  - 
DO  - 10.1109/tmm.2020.2967645
ER  - 

TY  - JOUR
AU  - Choi, Yoonseo; Monserrat, Toni-Jan Keith Palma; Park, Jeongeon; Shin, Hyungyu; Lee, Nyoungwoo; Kim, Juho
TI  - ProtoChat: Supporting the Conversation Design Process with Crowd Feedback
PY  - 2021
AB  - Similar to a design process for designing graphical user interfaces, conversation designers often apply an iterative design process by defining a conversation flow, testing with users, reviewing user data, and improving the design. While it is possible to iterate on conversation design with existing chatbot prototyping tools, there still remain challenges in recruiting participants on-demand and collecting structured feedback on specific conversational components. These limitations hinder designers from running rapid iterations and making informed design decisions. We posit that involving a crowd in the conversation design process can address these challenges, and introduce ProtoChat, a crowd-powered chatbot design tool built to support the iterative process of conversation design. ProtoChat makes it easy to recruit crowd workers to test the current conversation within the design tool. ProtoChat's crowd-testing tool allows crowd workers to provide concrete and practical feedback and suggest improvements on specific parts of the conversation. With the data collected from crowd-testing, ProtoChat provides multiple types of visualizations to help designers analyze and revise their design. Through a three-day study with eight designers, we found that ProtoChat enabled an iterative design process for designing a chatbot. Designers improved their design by not only modifying the conversation design itself, but also adjusting the persona and getting UI design implications beyond the conversation design itself. The crowd responses were helpful for designers to explore user needs, contexts, and diverse response formats. With ProtoChat, designers can successfully collect concrete evidence from the crowd and make decisions to iteratively improve their conversation design.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW3
PB  - 
DO  - 10.1145/3432924
ER  - 

TY  - NA
AU  - Hertel, Julia; Karaosmanoglu, Sukran; Schmidt, Susanne; Bräker, Julia; Semmann, Martin; Steinicke, Frank
TI  - ISMAR - A Taxonomy of Interaction Techniques for Immersive Augmented Reality based on an Iterative Literature Review
PY  - 2021
AB  - Developers of interactive systems have a variety of interaction techniques to choose from, each with individual strengths and limitations in terms of the considered task, context, and users. While there are taxonomies for desktop, mobile, and virtual reality applications, augmented reality (AR) taxonomies have not been established yet. However, recent advances in immersive AR technology (i.e., head-worn or projection-based AR), such as the emergence of untethered headsets with integrated gesture and speech sensors, have enabled the inclusion of additional input modalities and, therefore, novel multimodal interaction methods have been introduced. To provide an overview of interaction techniques for current immersive AR systems, we conducted a literature review of publications between 2016 and 2021. Based on 44 relevant papers, we developed a comprehensive taxonomy focusing on two identified dimensions – task and modality. We further present an adaptation of an iterative taxonomy development method to the field of human-computer interaction. Finally, we discuss observed trends and implications for future work.
SP  - 431
EP  - 440
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00060
ER  - 

TY  - NA
AU  - Clarke, Christopher; Cavdir, Doga; Chiu, Patrick; Denoue, Laurent; Kimber, Don
TI  - UIST - Reactive Video: Adaptive Video Playback Based on User Motion for Supporting Physical Activity
PY  - 2020
AB  - Videos are a convenient platform to begin, maintain, or improve a fitness program or physical activity. Traditional video systems allow users to manipulate videos through specific user interface actions such as button clicks or mouse drags, but have no model of what the user is doing and are unable to adapt in useful ways. We present adaptive video playback, which seamlessly synchronises video playback with the user's movements, building upon the principle of direct manipulation video navigation. We implement adaptive video playback in Reactive Video, a vision-based system which supports users learning or practising a physical skill. The use of pre-existing videos removes the need to create bespoke content or specially authored videos, and the system can provide real-time guidance and feedback to better support users when learning new movements. Adaptive video playback using a discrete Bayes and particle filter are evaluated on a data set collected of participants performing tai chi and radio exercises. Results show that both approaches can accurately adapt to the user's movements, however reversing playback can be problematic.
SP  - 196
EP  - 208
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415591
ER  - 

TY  - NA
AU  - Nith, Romain; Teng, Shan-Yuan; Li, Pengyu; Tao, Yujie; Lopes, Pedro
TI  - UIST - DextrEMS: Increasing Dexterity in Electrical Muscle Stimulation by Combining it with Brakes
PY  - 2021
AB  - Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user's fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.
SP  - 414
EP  - 430
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474759
ER  - 

TY  - NA
AU  - Dementyev, Artem; Olwal, Alex; Lyon, Richard F.
TI  - UIST - Haptics with Input: Back-EMF in Linear Resonant Actuators to Enable Touch, Pressure and Environmental Awareness
PY  - 2020
AB  - Today's wearable and mobile devices typically use separate hardware components for sensing and actuation. In this work, we introduce new opportunities for the Linear Resonant Actuator (LRA), which is ubiquitous in such devices due to its capability for providing rich haptic feedback. By leveraging strategies to enable active and passive sensing capabilities with LRAs, we demonstrate their benefits and potential as self-contained I/O devices. Specifically, we use the back-EMF voltage to classify if the LRA is tapped, touched, as well as how much pressure is being applied. The back-EMF sensing is already integrated into many motor and LRA drivers. We developed a passive low-power tap sensing method that uses just 37.7 uA. Furthermore, we developed active touch and pressure sensing, which is low-power, quiet (2 dB), and minimizes vibration. The sensing method works with many types of LRAs. We show applications, such as pressure-sensing side-buttons on a mobile phone. We have also implemented our technique directly on an existing mobile phone's LRA to detect if the phone is handheld or placed on a soft or hard surface. Finally, we show that this method can be used for haptic devices to determine if the LRA makes good contact with the skin. Our approach can add rich sensing capabilities to the ubiquitous LRA actuators without requiring additional sensors or hardware.
SP  - 420
EP  - 429
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415823
ER  - 

TY  - NA
AU  - Zhou, Ran; Wu, Yanzhe; Sareen, Harpreet
TI  - SIGGRAPH ASIA Emerging Technologies - HexTouch: A Wearable Haptic Robot for Complementary Interactions to Companion Agents in Virtual Reality
PY  - 2020
AB  - We propose a forearm-mounted robot that performs complementary touches in relation to the behaviors of a companion agent in virtual reality (VR). The robot consists of a series of tactors driven by servo motors that render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy) and other notification cues. We showcase this through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase user immersion in specific scenarios. The player collaborates with the agent to complete a mission while receiving affective haptic cues with the potential to enhance sociality in the virtual world.
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2020 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3415255.3422881
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Dachselt, Raimund
TI  - CHI Extended Abstracts - ARCord: Visually Augmented Interactive Cords for Mobile Interaction
PY  - 2018
AB  - Research on wearable controllers has shown that body-worn cords have many interesting physical affordances that make them powerful as a novel input device to control mobile applications in an unobtrusive manner. With this paper, we want to extend the interaction and application repertoire of body-worn cords by contributing the concept of visually augmented interactive cords using state-of-the-art augmented reality (AR) glasses. This novel combination of simultaneous input and output on a cord has the potential to create rich AR user interfaces that seamlessly support direct interaction and reduce cognitive burden by providing visual and tactile feedback. As a main contribution, we present a set of cord-based interaction techniques for browsing menus, selecting items, adjusting continuous values & ranges and solving advanced tasks in AR. In addition, we present our current implementation including different touch-enabled cords, its data transmission and AR visualization. Finally, we conclude with future challenges.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188456
ER  - 

TY  - NA
AU  - Daiber, Florian; Degraen, Donald; Zenner, André; Döring, Tanja; Steinicke, Frank; Ariza Nunez, Oscar Javier; Simeone, Adalberto L.
TI  - Everyday Proxy Objects for Virtual Reality
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3441343
ER  - 

TY  - NA
AU  - Mackamul, Eva Babette; Esteves, Augusto
TI  - SUI - A Look at the Effects of Handheld and Projected Augmented-reality on a Collaborative Task
PY  - 2018
AB  - This paper presents a comparative study between two popular AR systems during a collocated collaborative task. The goal of the study is to start a body of knowledge that describes the effects of different AR approaches in users' experience and performance; i.e., to look at AR not as a single entity with uniform characteristics. Pairs of participants interacted with a game of Match Pairs in both hand-held and project AR conditions, and their engagement, preference, task completion time, and number of game moves was recorded. Participants were also video-recorded during play for additional insights. No significant differences were found between users' self-reported engagement, and 56.25% of participants described a preference for the hand-held experience. On the other hand, participants completed the task significantly faster in the projected condition, despite having performed more game moves (card flips). We conclude the paper by discussing the effect of these two AR prototypes in participants' communication strategies, and how to design hand-held interfaces that could elicit the benefits of projected AR.
SP  - 74
EP  - 78
JF  - Proceedings of the Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267782.3267793
ER  - 

TY  - NA
AU  - Zhou, Ran; Wu, Yanzhe; Sareen, Harpreet
TI  - VRST - HexTouch: Affective Robot Touch for Complementary Interactions to Companion Agents in Virtual Reality
PY  - 2020
AB  - There is a growing need for social interaction in Virtual Reality (VR). Current social VR applications enable human-agent or interpersonal communication, usually by means of visual and audio cues. Touch, which is also an essential method for affective communication, has not received as much attention. To address this, we introduce HexTouch, a forearm-mounted robot that performs touch behaviors in sync with the behaviors of a companion agent, to complement visual and auditory feedback in virtual reality. The robot consists of four robotic tactors driven by servo motors, which render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy). We demonstrate HexTouch through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase the immersion of the VR experience. The player will receive affective haptic cues while collaborating with the agent to complete the mission in the game. The multisensory system for affective communication also has the potential to enhance sociality in the virtual world.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3422100
ER  - 

TY  - NA
AU  - Wang, Xian; Monteiro, Diego; Lee, Lik-Hang; Hui, Pan; Liang, Hai-Ning
TI  - VibroWeight: Simulating Weight and Center of Gravity Changes of Objects in Virtual Reality for Enhanced Realism
PY  - 2022
AB  - Haptic feedback in virtual reality (VR) allows users to perceive the physical properties of virtual objects (e.g., their weight and motion patterns). However, the lack of haptic sensations deteriorates users' immersion and overall experience. In this work, we designed and implemented a low-cost hardware prototype with liquid metal, VibroWeight, which can work in complementarity with commercial VR handheld controllers. VibroWeight is characterized by bimodal feedback cues in VR, driven by adaptive absolute mass (weights) and gravity shift. To our knowledge, liquid metal is used in a VR haptic device for the first time. Our 29 participants show that VibroWeight delivers significantly better VR experiences in realism and comfort.
SP  - NA
EP  - NA
JF  - 2022 IEEE Haptics Symposium (HAPTICS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/haptics52432.2022.9765609
ER  - 

TY  - NA
AU  - Ku, Pin-Sung; Shao, Qijia; Wu, Te-Yen; Gong, Jun; Zhu, Ziyan; Zhou, Xia; Yang, Xing-Dong
TI  - CHI - ThreadSense: Locating Touch on an Extremely Thin Interactive Thread
PY  - 2020
AB  - We propose a new sensing technique for one-dimensional touch input workable on an interactive thread of less than 0.4 mm thick. Our technique locates up to two touches using impedance sensing with a spacing resolution unachievable by the existing methods. Our approach is also unique in that it locates a touch based on a mathematical model describing the change in thread impedance in relation to the touch locations. This allows the system to be easily calibrated by the user touching a known location(s) on the thread. The system can thus quickly adapt to various environmental settings and users. A system evaluation showed that our system could track the slide motion of a finger with an average error distance of 6.13 mm and 4.16 mm using one and five touches for calibration, respectively. The system could also distinguish between single touch and two concurrent touches with an accuracy of 99% and could track two concurrent touches with an average error distance of 8.55 mm. We demonstrate new interactions enabled by our sensing approach in several unique applications.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376779
ER  - 

TY  - JOUR
AU  - 권, 상모; 임, 인성
TI  - 가상 환경에서의 손동작을 사용한 물체 조작에 대한 시각적 피드백 시스템
PY  - 2020
AB  - NA
SP  - 9
EP  - 19
JF  - Journal of the Korea Computer Graphics Society
VL  - 26
IS  - 3
PB  - 
DO  - 10.15701/kcgs.2020.26.3.9
ER  - 

TY  - NA
AU  - Masson, Damien; Goguey, Alix; Malacria, Sylvain; Casiez, Géry
TI  - UIST - WhichFingers: Identifying Fingers on Touch Surfaces and Keyboards using Vibration Sensors
PY  - 2017
AB  - HCI researchers lack low latency and robust systems to support the design and development of interaction techniques using finger identification. We developed a low cost prototype using piezo based vibration sensors attached to each finger. By combining the events from an input device with the information from the vibration sensors we demonstrate how to achieve low latency and robust finger identification. Our prototype was evaluated in a controlled experiment, using two keyboards and a touchpad, showing recognition rates of 98.2% for the keyboard and, for the touchpad, 99.7% for single touch and 94.7% for two simultaneous touches. These results were confirmed in an additional laboratory style experiment with ecologically valid tasks. Last we present new interactions techniques made possible using this technology.
SP  - 41
EP  - 48
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126619
ER  - 

TY  - JOUR
AU  - Ukonu, Michael O.; Ohaja, Edith U.; Okeke, Somtochukwu V.; Ruth, O. Okwumbu
TI  - Interactive effects of institutional requirements and screen vs. Print platforms on preference of Times New Roman and Calibri among university students
PY  - 2021
AB  - The replacement of Times New Roman (TNR—serif) with Calibri (sans serif) as default typeface on Microsoft Office Word reflects the growing prevalence of screen-based reading. The change also points...
SP  - NA
EP  - NA
JF  - Cogent Education
VL  - 8
IS  - 1
PB  - 
DO  - 10.1080/2331186x.2021.1968779
ER  - 

TY  - NA
AU  - Johansen, Stine S.; van Berkel, Niels; Fritsch, Jonas
TI  - Characterising Soundscape Research in Human-Computer Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533458
ER  - 

TY  - NA
AU  - Elkin, Lisa A.; Kay, Matthew; Higgins, James J.; Wobbrock, Jacob O.
TI  - An Aligned Rank Transform Procedure for Multifactor Contrast Tests
PY  - 2021
AB  - Data from multifactor HCI experiments often violates the normality assumption of parametric tests (i.e., nonconforming data). The Aligned Rank Transform (ART) is a popular nonparametric analysis technique that can find main and interaction effects in nonconforming data, but leads to incorrect results when used to conduct contrast tests. We created a new algorithm called ART-C for conducting contrasts within the ART paradigm and validated it on 72,000 data sets. Our results indicate that ART-C does not inflate Type I error rates, unlike contrasts based on ART, and that ART-C has more statistical power than a t-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also extended a tool called ARTool with our ART-C algorithm for both Windows and R. Our validation had some limitations (e.g., only six distribution types, no mixed factorial designs, no random slopes), and data drawn from Cauchy distributions should not be analyzed with ART-C.
SP  - NA
EP  - NA
JF  - arXiv: Methodology
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - James, Raphaël; Bezerianos, Anastasia; Chapuis, Olivier; Cordeil, Maxime; Dwyer, Tim; Prouzeau, Arnaud
TI  - Personal+Context navigation: combining AR and shared displays in network path-following
PY  - 2020
AB  - Shared displays are well suited to public viewing and collaboration, however they lack personal space to view private information and act without disturbing others. Combining them with Augmented Reality (AR) headsets allows interaction without altering the context on the shared display. We study a set of such interaction techniques in the context of network navigation, in particular path following, an important network analysis task. Applications abound, for example planning private trips on a network map shown on a public display.The proposed techniques allow for hands-free interaction, rendering visual aids inside the headset, in order to help the viewer maintain a connection between the AR cursor and the network that is only shown on the shared display. In two experiments on path following, we found that adding persistent connections between the AR cursor and the network on the shared display works well for high precision tasks, but more transient connections work best for lower precision tasks. More broadly, we show that combining personal AR interaction with shared displays is feasible for network navigation.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - George, Ceenu; Tien, An Ngo; Hussmann, Heinrich
TI  - ISMAR - Seamless, Bi-directional Transitions along the Reality-Virtuality Continuum: A Conceptualization and Prototype Exploration
PY  - 2020
AB  - With head mounted displays, consumers are able to transition from the real world to virtual realities. However, this requires frequent transitions between the two realities to maintain their physical integrity and awareness of the real world while in the virtual space. We completed two consecutive studies to investigate the dimensions of a system that supports seamless transition between realities without requiring the user to remove the headset. Our results are twofold: First, based on the the analysis of structured interviews (n=20), we present a conceptualization of existing solutions (n=37) and novel ideas (n=9) in the form of a design space. Second, we present the results of a user study (n=36) in which we tested two exemplary prototypes that evolved from the design space, called “Sky Portal” and “Virtual Phone.” Our exploration shows that our “Virtual Phone” metaphor has the potential to support HMD users in completing bidirectional transitions along Milgram’s reality-virtuality continuum. Users are also enabled to complete micro-interactions across the realities, even without performance loss.
SP  - 412
EP  - 424
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00067
ER  - 

TY  - JOUR
AU  - Bošnjak, Leon; Brumen, Boštjan
TI  - Shoulder surfing experiments: A systematic literature review
PY  - 2020
AB  - Abstract In search of the silver bullet to solve the password problem, the field of knowledge-based authentication has become bloated with novel proposals aiming to replace textual passwords. The emphasis on the quantity of studies as opposed to the quality of evaluation has made it difficult to compare the methods, as well as to validate and generalize the results. To improve the quality of security and usability evaluations, experimental design decisions should be reviewed and standardized. In this systematic review, we focus on the evaluation of the shoulder surfing attack (SSA) vulnerability. We formulate two research questions to help us determine how the design of the method should affect the SSA experimental design process, and how different design decisions affect the validity and interpretability of the results under various assumptions and threat models. To provide the researchers with comprehensive literature on SSA evaluation, we identify empirical shoulder surfing studies conforming to a predefined set of quality criteria. Based on the design features extracted from the experiments, we develop an evaluation framework for the assessment of the shoulder surfing experimental setup. In the follow-up analysis, we assess the proposed methods’ design features, and the quality of their SSA experiments, using Schaub et al.’s design aspect and our SSA evaluation frameworks, respectively. Through exhaustive analysis, we strive to streamline and standardize experimental decisions by showcasing their impact on the outcome of the study, and generate guidelines for a more objective design of shoulder surfing experiments.
SP  - 102023
EP  - NA
JF  - Computers & Security
VL  - 99
IS  - NA
PB  - 
DO  - 10.1016/j.cose.2020.102023
ER  - 

TY  - NA
AU  - Monteiro, Diego; Liang, Hai-Ning; Wang, Xian; Xu, Wenge; Tu, Huawei
TI  - ICMI - Design and Development of a Low-cost Device for Weight and Center of Gravity Simulation in Virtual Reality
PY  - 2021
AB  - With rapid advances in virtual reality (VR) technology, the use of haptics has become important to allow users to feel the physical properties of virtual objects. Current research has focused mainly on either weight variation or changing the center of gravity, which limits the simulation potential and may affect the feeling of immersion. This research explores the design and development of a device that can simulate both weight and center of gravity using low-cost components. Through an iterative design process and continuous testing with users, we arrived at a final prototype, FluidWeight, a device that can be attached to a typical VR handheld controller. FluidWeight uses fluid, which is transported from a central storage to a receptacle attached to the controller. A final experiment shows that users enjoyed using it because it could help increase the sense of realism in VR applications.
SP  - 453
EP  - 460
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3479907
ER  - 

TY  - JOUR
AU  - Li, Heng; Lau, Siu-Kit
TI  - A review of audio-visual interaction on soundscape assessment in urban built environments
PY  - 2020
AB  - NA
SP  - 107372
EP  - NA
JF  - Applied Acoustics
VL  - 166
IS  - NA
PB  - 
DO  - 10.1016/j.apacoust.2020.107372
ER  - 

TY  - NA
AU  - Patras, Cristian; Cibulskis, Mantas; Nilsson, Niels Christian
TI  - Body Warping Versus Change Blindness Remapping: A Comparison of Two Approaches to Repurposing Haptic Proxies for Virtual Reality
PY  - 2022
AB  - When using tangible props as proxies for virtual objects, it is important that these haptic proxies are similar to and co-located with their virtual counterparts. This makes it challenging to scale virtual scenarios because more proxies are needed as scenarios grow more complex. Haptic retargeting, or virtual remapping, makes it possible to repurpose the same physical prop as a proxy for multiple virtual objects. This paper details a user study comparing two techniques for repurposing haptic proxies; namely haptic retargeting based on body warping and change blindness remapping. Participants performed a simple button-pressing task, and 24 virtual buttons were mapped onto four haptic proxies with varying degrees of misalignment. Body warping and change blindness remapping were used to realign the real and virtual buttons, and the results indicate that users failed to reliably detect realignment of up to 7.9 cm for body warping and up to 9.7 cm for change blindness remapping. Moreover, change blindness remapping yielded significantly higher self-reported agency, and marginally higher ownership. Taken together these results suggest that this less explored technique has potential when it comes to repurposing haptic proxies for virtual reality.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00039
ER  - 

TY  - NA
AU  - Khan, Arshad; Roo, Joan Sol; Kraus, Tobias; Steimle, Jürgen
TI  - UIST - Soft Inkjet Circuits: Rapid Multi-Material Fabrication of Soft Circuits using a Commodity Inkjet Printer
PY  - 2019
AB  - Despite the increasing popularity of soft interactive devices, their fabrication remains complex and time consuming. We contribute a process for rapid do-it-yourself fabrication of soft circuits using a conventional desktop inkjet printer. It supports inkjet printing of circuits that are stretchable, ultrathin, high resolution, and integrated with a wide variety of materials used for prototyping. We introduce multi-ink functional printing on a desktop printer for realizing multi-material devices, including conductive and isolating inks. We further present DIY techniques to enhance compatibility between inks and substrates and the circuits' elasticity. This enables circuits on a wide set of materials including temporary tattoo paper, textiles, and thermoplastic. Four application cases demonstrate versatile uses for realizing stretchable devices, e-textiles, body-based and re-shapeable interfaces.
SP  - 341
EP  - 354
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347892
ER  - 

TY  - CHAP
AU  - Amemiya, Tomohiro
TI  - HCI (4) - Asymmetric Gravitational Oscillation on Fingertips Increased the Perceived Heaviness of a Pinched Object
PY  - 2021
AB  - Studies have shown that changes in shear or friction force at the fingertips contribute to weight perception. Pinching an actively moving object induces a similar change in shear or friction force at the fingertips, and humans may perceive the object as feeling heavier than a motionless object. However, little is known about weight perception when pinching an oscillating object. This study aimed to investigate the role of an object’s oscillation in weight perception, with participants assessing heaviness when pinching a pair of boxes. The results showed that a vibrating box felt heavier than a static box, and an asymmetrically vibrating box felt heavier than a symmetrical box. These findings are similar to those of our previous study, when participants grasped a larger and heavier box that vibrated asymmetrically in the vertical direction.
SP  - 247
EP  - 256
JF  - Human Interface and the Management of Information. Information Presentation and Visualization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78321-1_19
ER  - 

TY  - JOUR
AU  - Mills, Gregory; Gregoromichelaki, Eleni; Howes, Chris; Maraev, Vladislav
TI  - Influencing laughter with AI-mediated communication
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>Previous experimental findings support the hypothesis that laughter and positive emotions are contagious in face-to-face and mediated communication. To test this hypothesis, we describe four experiments in which participants communicate via a chat tool that artificially adds or removes laughter (e.g. <jats:italic>haha</jats:italic> or <jats:italic>lol</jats:italic>), without participants being aware of the manipulation. We found no evidence to support the contagion hypothesis. However, artificially exposing participants to more <jats:italic>lol</jats:italic>s decreased participants’ use of <jats:italic>haha</jats:italic>s but led to more involvement and improved task-performance. Similarly, artificially exposing participants to more <jats:italic>haha</jats:italic>s decreased use of <jats:italic>haha</jats:italic> but increased lexical alignment. We conclude that, even though the interventions have effects on coordination, they are incompatible with contagion as a primary explanatory mechanism. Instead, these results point to an interpretation that involves a more sophisticated view of dialogue mechanisms along the lines of Conversational Analysis and similar frameworks and we suggest directions for future research.</jats:p>
SP  - 416
EP  - 463
JF  - Interaction Studies
VL  - 22
IS  - 3
PB  - 
DO  - 10.1075/is.00011.mil
ER  - 

TY  - NA
AU  - Choi, Inrak; Ofek, Eyal; Benko, Hrvoje; Sinclair, Mike; Holz, Christian
TI  - CHI Extended Abstracts - Demonstration of CLAW: A Multifunctional Handheld VR Haptic Controller
PY  - 2018
AB  - CLAW is a handheld virtual reality controller that augments the typical controller functionality with force feedback and actuated movement to the index finger. Our controller enables three distinct interactions (grasping virtual object, touching virtual surfaces, and triggering) and changes its corresponding haptic rendering by sensing the differences in the user's grasp. A servo motor coupled with a force sensor renders controllable forces to the index finger during grasping and touching. Using position tracking, a voice coil actuator at the index fingertip generates vibrations for various textures synchronized with finger movement. CLAW also supports a haptic force feedback in the trigger mode when the user holds a gun.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186505
ER  - 

TY  - NA
AU  - Mehmood, Adnan; He, Han; Chen, Xiaochen; Vianto, Aleksi; Vianto, Ville; Virkki, Johanna
TI  - SeGAH - ClothFace: A Batteryless Glove-Integrated User Interface Solution based on Passive UHF RFID Technology
PY  - 2020
AB  - Due to their unique advantages, wearable user interface solutions have gained a lot of research and commercial interest. This paper introduces ClothFace technology by presenting a batteryless glove-integrated user interface. The solution is based on passive ultrahigh frequency (UHF) radio frequency identification (RFID) technology. The first prototype of this solution was fabricated from copper tape to a normal cotton-based glove. The user interface on the glove consists of three antenna parts on three different fingers of the glove, each of which has an RFID microchip with a unique ID. Further, an additional antenna part is attached to the thumb of the glove. The antennas are initially separated from each other, and none of the microchips is readable for the RFID reader. When thumb antenna touches any of the three finger antennas, the touch creates an electrical connection, and the corresponding microchip can be detected by the RFID reader. In this study, the developed glove-integrated user interface was evaluated in an office environment by three test subjects, who all received 100 orders from a specific testing software. The average success rate in this first test was 98 %. These initial results are very encouraging, especially when considering that the glove-integrated user interface, being light, flexible, and cost-effective, promises versatile interesting applications in several fields.
SP  - 1
EP  - 5
JF  - 2020 IEEE 8th International Conference on Serious Games and Applications for Health (SeGAH)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/segah49190.2020.9201793
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Ning, Chih-Wei; Wang, Po-Yao (Cosmos); Cheng, Jen-Hao; Cheng, Lung-Pan
TI  - Haptic-go-round: A Surrounding Platform for Encounter-type Haptic in Virtual Reality Experiences
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383136
ER  - 

TY  - JOUR
AU  - Ochkov, Valery; Vasileva, Inna
TI  - Application of Difference Schemes to Decision the Pursuit Problem
PY  - 2019
AB  - The problem of the pursuit curve construction in the case when the tangent to pursuer’s motion trajectory passes at any time through the point representing the pursued is considered. A new approach to construct the pursuit curves using difference schemes is proposed. The proposed technique eliminates the need to derive the differential equations for the description of the pursuit curves, which is quite difficult task in the general case. In addition, the application of difference methods is justified in a situation where it is complicated to find the analytical solution of an existing differential equation and it is possible to obtain the pursuit curve only numerically. Various modifications of difference schemes respectively equivalent to the Euler, to the Adams – Bashforth and to the Milne methods are constructed. Their software implementation is realized by using the mathematical package Mathcad. We consider the case of a uniform rectilinear motion of the pursued whose differential equation describing the path of the pursuer and its analytical solution are known. We compare the numerical solutions obtained by the different methods with the well-known analytical solution. The error of the obtained numerical solutions is examined. Moreover, an application is considered illustrating the construction of the difference schemes for the case of an arbitrary trajectory of the pursued. Also, we extend the proposed method to the case of cyclic pursuit with several participants in the three-dimensional space. In particular, we construct a difference scheme equivalent to the Euler method for a three-dimensional analogue of the "bugs problem". The results obtained are demonstrated by means of animated examples for either two-dimensional or three-dimensional cases.
SP  - 1407
EP  - 1433
JF  - SPIIRAS Proceedings
VL  - 18
IS  - 6
PB  - 
DO  - 10.15622/sp.2019.18.6.1407-1433
ER  - 

TY  - NA
AU  - Chen, Daniel K. Y.; Chossat, Jean-Baptiste; Shull, Peter B.
TI  - CHI - HaptiVec: Presenting Haptic Feedback Vectors in Handheld Controllers using Embedded Tactile Pin Arrays
PY  - 2019
AB  - HaptiVec is a new haptic feedback paradigm for handheld controllers which allows users to feel directional haptic pressure vectors on their fingers and hands while interacting with virtual environments. We embed a 3 by 5 tactile pin array (with an average pin spacing of 25 mm) into the handles of two custom VR type controllers. By presenting directional pressure vectors in eight cardinal directions (N, NE, E, SE, S, SW, W, NW) to users without prior training, they were able to distinguish the correct direction with an accuracy of at least 79%. We illustrate two applications where our device enhances virtual experiences over traditional vibrotactile feedback. In the first application, through the classic first-person shooter Doom, we demonstrate that users can receive directional pressure feedback corresponding to the direction of incident enemy projectiles. In the second application, we demonstrate how our controller can create a more immersive experience by allowing the user to feel their virtual climate by randomizing the directional vectors and presenting the user with "haptic rain" which adapts with the intensity of the rainfall.
SP  - 171
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300401
ER  - 

TY  - NA
AU  - Ueda, Kentaro; Terada, Tsutomu; Tsukamoto, Masahiko
TI  - MoMM - Evaluation of Input using Wrinkles on Clothes
PY  - 2018
AB  - Wearable computing has created textile-based interfaces utilizing the interaction between the user and cloth for operation, as well as the touch and the pinch input operation. The user wears and uses the device in various postures, environments, and operating positions that affect the operation speed and accuracy. However, no study has assessed touching and pinching using the same input interface. One of the textile interfaces has an input interface using wrinkles on clothes, and we have evaluated the input for the same. We designed three touch input methods and one pinch input method. We implemented the input and the output device which use wrinkles and carried out four evaluations. The results indicated that the pinch input reached the highest accuracy of 98% of four input methods after learning and achieved the accuracy of 95% or more irrespective of postures and positions. The narrowing-down selection reached the fastest input time of 1.85 seconds of four methods after learning and achieved the quickest input when participants were stationary, while the double touch was the most rapid when walking. According to the result of the wrinkle recognition, users have a high accuracy of the identification of wrinkles of 89.4% and recognize their shape in approximately 12 seconds.
SP  - 66
EP  - 75
JF  - Proceedings of the 16th International Conference on Advances in Mobile Computing and Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3282353.3282365
ER  - 

TY  - NA
AU  - Zhang, Zhenliang; Weng, Dongdong; Jiang, Haiyan; Liu, Yue; Wang, Yongtian
TI  - Inverse Augmented Reality: A Virtual Agent's Perspective
PY  - 2018
AB  - We propose a framework called inverse augmented reality (IAR) which describes the scenario that a virtual agent living in the virtual world can observe both virtual objects and real objects. This is different from the traditional augmented reality. The traditional virtual reality, mixed reality and augmented reality are all generated for humans, i.e., they are human-centered frameworks. On the contrary, the proposed inverse augmented reality is a virtual agent-centered framework, which represents and analyzes the reality from a virtual agent's perspective. In this paper, we elaborate the framework of inverse augmented reality to argue the equivalence of the virtual world and the physical world regarding the whole physical structure.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Drewes, Heiko; Khamis, Mohamed; Alt, Florian
TI  - DialPlate: Enhancing the Detection of Smooth Pursuits Eye Movements Using Linear Regression
PY  - 2018
AB  - We introduce and evaluate a novel approach for detecting smooth pursuit eye movements that increases the number of distinguishable targets and is more robust against false positives. Being natural and calibration-free, Pursuits has been gaining popularity in the past years. At the same time, current implementations show poor performance when more than eight on-screen targets are being used, thus limiting its applicability. Our approach (1) leverages the slope of a regression line, and (2) introduces a minimum signal duration that improves both the new and the traditional detection method. After introducing the approach as well as the implementation, we compare it to the traditional correlation-based Pursuits detection method. We tested the approach up to 24 targets and show that, if accepting a similar error rate, nearly twice as many targets can be distinguished compared to state of the art. For fewer targets, accuracy increases significantly. We believe our approach will enable more robust pursuit-based user interfaces, thus making it valuable for both researchers and practitioners.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Yang, Xing-Dong
TI  - iWood: Makeable Vibration Sensor for Interactive Plywood
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545640
ER  - 

TY  - NA
AU  - Jin, Haojian; Wang, Jingxian; Yang, Zhijian; Kumar, Swarun; Hong, Jason
TI  - MobiSys - WiSh: Towards a Wireless Shape-aware World using Passive RFIDs
PY  - 2018
AB  - This paper presents WiSh, a solution that makes ordinary surfaces shape-aware, relaying their real-time geometry directly to a user's handheld device. WiSh achieves this using inexpensive, light-weight and battery-free RFID tags attached to these surfaces tracked from a compact single-antenna RFID reader. In doing so, WiSh enables several novel applications: shape-aware clothing that can detect a user's posture, interactive shape-aware toys or even shape-aware bridges that report their structural health. WiSh's core algorithm infers the shape of ordinary surfaces using the wireless channels of signals reflected off RFID tags received at a single-antenna RFID reader. Indeed, locating every RFID tag using a single channel measurement per-tag is challenging, given that neither their 3-D coordinates nor orientation are known a priori. WiSh presents a novel algorithm that models the geometric constraints between the coordinates of the RFID tags based on flexibility of the surface upon which they are mounted. By inferring surface curvature parameters rather than the locations of individual RFID tags, we greatly reduce the number of variables our system needs to compute. Further, WiSh overcomes a variety of system-level challenges stemming from signal multipath, stretching of fabric and modeling large surfaces. We implement WiSh on commodity RFID readers and tags attached to a variety of surfaces and demonstrate mm-accurate shape-tracking across various applications.
SP  - 428
EP  - 441
JF  - Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210240.3210328
ER  - 

TY  - JOUR
AU  - Koshikawa, Koki; Nagamatsu, Takashi; Takemura, Kentaro
TI  - Model-based Gaze Estimation with Transparent Markers on Large Screens
PY  - 2022
AB  - <jats:p>Several technical issues that affect eye-tracking have arisen concomitantly with the steadily increasing sizes of personal displays recently. One such issue is the loss of the illumination reflection of the near-infrared light-emitting diodes used as reference points around the edge of the display. Another issue is that reference identification is required for practical usage. Therefore, this paper proposes gaze estimation with transparent markers for large display environments to solve these problems. The transparent markers can be distributed on the screen, and a unique ID is assigned to each marker using linear polarization angles. The reference is detected using a polarization camera through the reflection on the cornea. The results of experiments conducted using a 50-inch display indicate that the proposed method can estimate the point-of-gaze to within 2.1 degrees of error. We confirmed that on-screen markers in sizable displays could be effectively used as references instead of illumination sources.</jats:p>
SP  - 1
EP  - 16
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ETRA
PB  - 
DO  - 10.1145/3530888
ER  - 

TY  - JOUR
AU  - Zhang, Yunxiang; Liang, Benjamin; Chen, Boyuan; Torrens, Paul M.; Atashzar, S. Farokh; Lin, Dahua; Sun, Qi
TI  - Force-Aware Interface via Electromyography for Natural VR/AR Interaction
PY  - 2022
AB  - <jats:p>While tremendous advances in visual and auditory realism have been made for virtual and augmented reality (VR/AR), introducing a plausible sense of physicality into the virtual world remains challenging. Closing the gap between real-world physicality and immersive virtual experience requires a closed interaction loop: applying user-exerted physical forces to the virtual environment and generating haptic sensations back to the users. However, existing VR/AR solutions either completely ignore the force inputs from the users or rely on obtrusive sensing devices that compromise user experience.</jats:p> <jats:p>By identifying users' muscle activation patterns while engaging in VR/AR, we design a learning-based neural interface for natural and intuitive force inputs. Specifically, we show that lightweight electromyography sensors, resting non-invasively on users' forearm skin, inform and establish a robust understanding of their complex hand activities. Fuelled by a neural-network-based model, our interface can decode finger-wise forces in real-time with 3.3% mean error, and generalize to new users with little calibration. Through an interactive psychophysical study, we show that human perception of virtual objects' physical properties, such as stiffness, can be significantly enhanced by our interface. We further demonstrate that our interface enables ubiquitous control via finger tapping. Ultimately, we envision our findings to push forward research towards more realistic physicality in future VR/AR.</jats:p>
SP  - 1
EP  - 18
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 6
PB  - 
DO  - 10.1145/3550454.3555461
ER  - 

TY  - NA
AU  - Bonfert, Michael; Porzel, Robert; Malaka, Rainer
TI  - VR - Get a Grip! Introducing Variable Grip for Controller-Based VR Systems
PY  - 2019
AB  - We propose an approach to facilitate adjustable grip for object interaction in virtual reality. It enables the user to handle objects with loose and firm grip using conventional controllers. Pivotal design properties were identified and evaluated in a qualitative pilot study. Two revised interaction designs with variable grip were compared to the status quo of invariable grip in a quantitative study. The users performed placing actions with all interaction modes. Performance, clutching, task load, and usability were measured. While the handling time increased slightly using variable grip, the usability score was significantly higher. No substantial differences were measured in positioning accuracy. The results lead to the conclusion that variable grip can be useful and improve realism depending on tasks, goals, and user preference.
SP  - 604
EP  - 612
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797824
ER  - 

TY  - JOUR
AU  - Comunità, Marco; Gerino, Andrea; Picinali, Lorenzo
TI  - PlugSonic: a web- and mobile-based platform for dynamic and navigable binaural audio
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>PlugSonic is a series of web- and mobile-based applications designed to edit samples and apply audio effects (PlugSonic Sample) and create and experience dynamic and navigable soundscapes and sonic narratives (PlugSonic Soundscape). The audio processing within PlugSonic is based on the Web Audio API while the binaural rendering uses the 3D Tune-In Toolkit. Exploration of soundscapes in a physical space is made possible by adopting Apple’s ARKit. The present paper describes the implementation details, the signal processing chain and the necessary steps to curate and experience a soundscape. We also include some metrics and performance details. The main goal of PlugSonic is to give users a complete set of tools, without the need for specific devices, external software and/or hardware specialised knowledge, or custom development, with the idea that spatial audio has the potential to become a readily accessible and easy to understand technology, for anyone to adopt, whether for creative or research purposes.</jats:p>
SP  - NA
EP  - NA
JF  - EURASIP Journal on Audio, Speech, and Music Processing
VL  - 2022
IS  - 1
PB  - 
DO  - 10.1186/s13636-022-00250-x
ER  - 

TY  - NA
AU  - Antoine, Axel; Malacria, Sylvain; Casiez, Géry
TI  - CHI Extended Abstracts - TurboMouse: End-to-end Latency Compensation in Indirect Interaction
PY  - 2018
AB  - End-to-end latency corresponds to the temporal difference between a user input and the corresponding output from a system. It has been shown to degrade user performance in both direct and indirect interaction. If it can be reduced to some extend, latency can also be compensated through software compensation by trying to predict the future position of the cursor based on previous positions, velocities and accelerations. In this paper, we propose a hybrid hardware and software prediction technique specifically designed for partially compensating end-to-end latency in indirect pointing. We combine a computer mouse with a high frequency accelerometer to predict the future location of the pointer using Euler based equations.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186542
ER  - 

TY  - NA
AU  - Drewes, Heiko; Sakel, Sophia; Hussmann, Heinrich
TI  - User Perception of Smooth Pursuit Target Speed
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529234
ER  - 

TY  - NA
AU  - Leiva, Germán; Beaudouin-Lafon, Michel
TI  - UIST - Montage: A Video Prototyping System to Reduce Re-Shooting and Increase Re-Usability
PY  - 2018
AB  - Video prototypes help capture and communicate interaction with paper prototypes in the early stages of design. However, designers sometimes find it tedious to create stop-motion videos for continuous interactions and to re-shoot clips as the design evolves. We introduce Montage, a proof-of-concept implementation of a computer-assisted process for video prototyping. Montage lets designers progressively augment video prototypes with digital sketches, facilitating the creation, reuse and exploration of dynamic interactions. Montage uses chroma keying to decouple the prototyped interface from its context of use, letting designers reuse or change them independently. We describe how Montage enhances video prototyping by combining video with digital animated sketches, encourages the exploration of different contexts of use, and supports prototyping of different interaction styles.
SP  - 675
EP  - 682
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242613
ER  - 

TY  - JOUR
AU  - Ens, Barrett; Lanir, Joel; Tang, Anthony; Bateman, Scott; Lee, Gun A.; Piumsomboon, Thammathip; Billinghurst, Mark
TI  - Revisiting collaboration through mixed reality: The evolution of groupware
PY  - 2019
AB  - NA
SP  - 81
EP  - 98
JF  - International Journal of Human-Computer Studies
VL  - 131
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.05.011
ER  - 

TY  - NA
AU  - Strandholt, Patrick L.; Dogaru, Oana A.; Nilsson, Niels Christian; Nordahl, Rolf; Serafin, Stefania
TI  - CHI - Knock on Wood: Combining Redirected Touching and Physical Props for Tool-Based Interaction in Virtual Reality
PY  - 2020
AB  - When physical props serve as proxies for virtual tools used to manipulate the virtual environment, it is challenging to provide appropriate haptic feedback. Redirected tool-mediated manipulation addresses this challenge by distorting the mapping between physical and virtual tools to provide a sensation of manipulating the virtual environment, when the physical tool comes into contact with another physical prop. For example, a virtual hammer's position can be offset to ensure that physical impacts accompany each strike of a virtual nail. We demonstrate the idea by showing that it can be used to create sensations of impact and resistance when driving a virtual nail into a surface, when tightening a virtual screw, and when sawing through a virtual plank. The results of a user study indicate that the proposed approach is perceived as more realistic than interaction with a single physical prop or controller and no notable detriments to precision were observed.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376303
ER  - 

TY  - NA
AU  - He, Zhenyi; Perlin, Ken
TI  - Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching with Chalktalk
PY  - 2019
AB  - Teaching that uses projected presentation media such as slide-shows lacks support for dynamic content whose form and behaviors require live changes during a lecture. Recent software alternatives such as the Chalktalk software platform allow the creation of interactive simulations in arbitrary sequences and combinations within presentations. These more dynamic solutions, however, do not optimize for face-to-face interactions: eye-contact, gaze direction, and concurrent awareness of another person's movements together with the presented content. To explore the extent to which these face-to-face interactions may improve learning and engagement during a lecture, we propose a Mixed Reality (MR) platform that places Chalktalk's behaviors and simulations within a mirrored virtual world environment designed for face-to-face, one-on-one interactions. We compare our system with projected Chalktalk to evaluate its relative effectiveness for learning, retention, and level of engagement.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Comunità, Marco; Gerino, Andrea; Lim, Veranika; Picinali, Lorenzo
TI  - PlugSonic: a web- and mobile-based platform for binaural audio and sonic narratives.
PY  - 2020
AB  - PlugSonic is a suite of web- and mobile-based applications for the curation and experience of binaural interactive soundscapes and sonic narratives. It was developed as part of the PLUGGY EU project (Pluggable Social Platform for Heritage Awareness and Participation) and consists of two main applications: PlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, to create and experience binaural soundscapes. The audio processing within PlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while the exploration of soundscapes in a physical space is obtained using Apple's ARKit. In this paper we present the design choices, the user involvement processes and the implementation details. The main goal of PlugSonic is technology democratisation; PlugSonic users - whether institutions or citizens - are all given the instruments needed to create, process and experience 3D soundscapes and sonic narrative; without the need for specific devices, external tools (software and/or hardware), specialised knowledge or custom development. The evaluation, which was conducted with inexperienced users on three tasks - creation, curation and experience - demonstrates how PlugSonic is indeed a simple, effective, yet powerful tool.
SP  - NA
EP  - NA
JF  - arXiv: Sound
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Drewes, Heiko; Khamis, Mohamed; Alt, Florian
TI  - MUM - Smooth Pursuit Target Speeds and Trajectories
PY  - 2018
AB  - In this paper we present an investigation of how the speed and trajectory of smooth pursuits targets impact on detection rates in gaze interfaces. Previous work optimized these values for the specific application for which smooth pursuit eye movements were employed. However, this may not always be possible. For example UI designers may want to minimize distraction caused by the stimulus, integrate it with a certain UI element (e.g., a button), or limit it to a certain area of the screen. In these cases an in-depth understanding of the interplay between speed, trajectory, and accuracy is required. To achieve this, we conducted a user study with 15 participants who had to follow targets with different speeds and on different trajectories using their gaze. We evaluated the data with respect to detectability. As a result, we obtained reasonable ranges for target speeds and demonstrate the effects of trajectory shapes. We show that slow moving targets are hard to detect by correlation and that introducing a delay improves the detection rate for fast moving targets. Our research is complemented by design rules which enable designers to implement better pursuit detectors and pursuit-based user interfaces.
SP  - 139
EP  - 146
JF  - Proceedings of the 17th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3282894.3282913
ER  - 

TY  - CHAP
AU  - Savov, Anton; Kessler, Martina; Reichardt, Lea; Züst, Viturin; Hall, Daniel; Dillenburger, Benjamin
TI  - Constructing Building Layouts and Mass Models with Hand Gestures in Multiple Mixed Reality Modes
PY  - 2022
AB  - NA
SP  - 360
EP  - 373
JF  - Towards Radical Regeneration
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-13249-0_30
ER  - 

TY  - NA
AU  - Schmitz, Martin; Günther, Sebastian; Schön, Dominik; Müller, Florian
TI  - Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501981
ER  - 

TY  - JOUR
AU  - Liu, Ruibo; Shao, Qijia; Wang, Siqi; Ru, Christina; Balkcom, Devin; Zhou, Xia
TI  - Reconstructing Human Joint Motion with Computational Fabrics
PY  - 2019
AB  - Accurate and continuous monitoring of joint rotational motion is crucial for a wide range of applications such as physical rehabilitation [6, 85] and motion training [22, 54, 68]. Existing motion capture systems, however, either need instrumentation of the environment, or fail to track arbitrary joint motion, or impose wearing discomfort by requiring rigid electrical sensors right around the joint area. This work studies the use of everyday fabrics as a flexible and soft sensing medium to monitor joint angular motion accurately and reliably. Specifically we focus on the primary use of conductive stretchable fabrics to sense the skin deformation during joint motion and infer the joint rotational angle. We tackle challenges of fabric sensing originated by the inherent properties of elastic materials by leveraging two types of sensing fabric and characterizing their properties based on models in material science. We apply models from bio-mechanics to infer joint angles and propose the use of dual strain sensing to enhance sensing robustness against user diversity and fabric position offsets. We fabricate prototypes using off-the-shelf fabrics and micro-controller. Experiments with ten participants show 9.69° median angular error in tracking joint angle and its sensing robustness across various users and activities.
SP  - 19
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 1
PB  - 
DO  - 10.1145/3314406
ER  - 

TY  - NA
AU  - Tamura, Yuto; Takemura, Kentaro
TI  - UIST (Adjunct Volume) - Estimating Focused Object using Smooth Pursuit Eye Movements and Interest Points in the Real World
PY  - 2019
AB  - User calibration is a significant problem in eye-based interaction. To overcome this, several solutions, such as the calibration-free method and implicit user calibration, have been proposed. Pursuits-based interaction is another such solution that has been studied for public screens and virtual reality. It has been applied to select graphical user interfaces (GUIs) because the movements in a GUI can be designed in advance. Smooth pursuit eye movements (smooth pursuits) occur when a user looks at objects in the physical space as well and thus, we propose a method to identify the focused object by using smooth pursuits in the real world. We attempted to determine the focused objects without prior information under several conditions by using the pursuits-based approach and confirmed the feasibility and limitations of the proposed method through experimental evaluations.
SP  - 21
EP  - 23
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357102
ER  - 

TY  - NA
AU  - Trinitatova, Daria; Tsetserukou, Dzmitry
TI  - SIGGRAPH Asia XR - TouchVR: a Wearable Haptic Interface for VR Aimed at Delivering Multi-modal Stimuli at the User’s Palm
PY  - 2019
AB  - TouchVR is a novel wearable haptic interface which can deliver multimodal tactile stimuli on the palm by DeltaTouch haptic display and vibrotactile feedback on the fingertips by vibration motors for the Virtual Reality (VR) user. DeltaTouch display is capable of generating 3D force vector at the contact point and presenting multimodal tactile sensation of weight, slippage, encounter, softness, and texture. The VR system consists of HTC Vive Pro base stations, head-mounted display (HMD), and Leap Motion controller for tracking the user’s hand motion. The MatrixTouch, BallFeel, RoboX, and AnimalFeel applications have been developed to demonstrate the capabilities of the proposed technology. A novel haptic interface can potentially bring a new level of immersion of the user in VR and make it more interactive and tangible.
SP  - 42
EP  - 43
JF  - SIGGRAPH Asia 2019 XR
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3355355.3361896
ER  - 

TY  - JOUR
AU  - Putze, Felix; Putze, Susanne; Sagehorn, Merle; Micek, Christopher; Solovey, Erin T.
TI  - Understanding HCI Practices and Challenges of Experiment Reporting with Brain Signals: Towards Reproducibility and Reuse
PY  - 2022
AB  - <jats:p>In human-computer interaction (HCI), there has been a push towards open science, but to date, this has not happened consistently for HCI research utilizing brain signals due to unclear guidelines to support reuse and reproduction. To understand existing practices in the field, this paper examines 110 publications, exploring domains, applications, modalities, mental states and processes, and more. This analysis reveals variance in how authors report experiments, which creates challenges to understand, reproduce, and build on that research. It then describes an overarching experiment model that provides a formal structure for reporting HCI research with brain signals, including definitions, terminology, categories, and examples for each aspect. Multiple distinct reporting styles were identified through factor analysis and tied to different types of research. The paper concludes with recommendations and discusses future challenges. This creates actionable items from the abstract model and empirical observations to make HCI research with brain signals more reproducible and reusable.</jats:p>
SP  - 1
EP  - 43
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 4
PB  - 
DO  - 10.1145/3490554
ER  - 

TY  - CHAP
AU  - Mills, Gregory; Boschker, Remko
TI  - Using Virtual Reality to Investigate the Emergence of Gaze Conventions in Interpersonal Coordination
PY  - 2022
AB  - NA
SP  - 564
EP  - 571
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-19679-9_71
ER  - 

TY  - NA
AU  - Cheng, Chih-Hao; Chang, Chia-Chi; Chen, Ying-Hsuan; Lin, Ying-Li; Huang, Jing-Yuan; Han, Ping-Hsuan; Ko, Ju-Chun; Lee, Lai-Chung
TI  - VRST - GravityCup: a liquid-based haptics for simulating dynamic weight in virtual reality
PY  - 2018
AB  - During interaction in a virtual environment, haptic displays provide users with sensations such as vibration, texture simulation, and electrical muscle stimulation. However, as humans perceive object weights naturally in daily life, objects picked up in virtual reality feel unrealistically light. To create an immersive experience in virtual reality that includes weight sensation, we propose GravityCup, a liquid-based haptic feedback device that simulates realistic object weights and inertia when moving virtual handheld objects. In different scenarios, GravityCup uses liquid to provide users with a dynamic weight sensation experience that enhances interaction with handheld objects in virtual reality.
SP  - NA
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281569
ER  - 

TY  - JOUR
AU  - Simões, Bruno; De Amicis, Raffaele; Segura, Álvaro; Martín, Miguel; Ipiña, Ibon
TI  - A cross reality wire assembly training system for workers with disabilities
PY  - 2021
AB  - The once exclusive technology empowering immersive and interactive training systems is now more affordable and accessible to mainstream use cases. Its adoption in the manufacturing industry can help reshape training processes as an intrinsic part of production routines and reduce the mental resources required to complete a task. Current academic literature does not integrate components to describe the skills and attributes of workers with impairments. In contrast, the research in this paper addressed the design and evaluation of a new immersive and interactive training system that can effectively provide new human augmentation opportunities for workers with impairments by reducing the mental resources required to complete a task. Automated machine interpretation of tasks and actions of workers with impairments is still a long way off and one of the reasons is that individual skills are still difficult to describe formally. Therefore, in this paper, skill transfer is assessed through external evaluation. The results of the preliminary evaluation of our Cross Reality (XR) prototype for training and error minimization in the manufacturing of electrical cabinets confirmed significant productivity gains and high adoption by participants, validating the suitability of the solution for workers in industrial manufacturing processes.
SP  - 429
EP  - 440
JF  - International Journal on Interactive Design and Manufacturing (IJIDeM)
VL  - 15
IS  - 4
PB  - 
DO  - 10.1007/s12008-021-00772-2
ER  - 

TY  - NA
AU  - Lau, Samuel; Drosos, Ian; Markel, Julia M.; Guo, Philip J.
TI  - VL/HCC - The Design Space of Computational Notebooks: An Analysis of 60 Systems in Academia and Industry
PY  - 2020
AB  - Computational notebooks such as Jupyter are now used by millions of data scientists, machine learning engineers, and computational researchers to do exploratory and end-user programming. In recent years, dozens of different notebook systems have been developed across academia and industry. However, we still lack an understanding of how their individual designs relate to one another and what their tradeoffs are. To provide a holistic view of this rapidly-emerging landscape, we performed, to our knowledge, the first comprehensive design analysis of dozens of notebook systems. We analyzed 60 notebooks (16 academic papers, 29 industry products, and 15 experimental/R&D projects) and formulated a design space that succinctly captures variations in system features. Our design space covers 10 dimensions that include diverse ways of importing data, editing code and prose, running code, and publishing notebook outputs. We conclude by suggesting ways for researchers to push future projects beyond the current bounds of this space.
SP  - 1
EP  - 11
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127201
ER  - 

TY  - NA
AU  - Samad, Majed; Gatti, Elia; Hermes, Anne; Benko, Hrvoje; Parise, Cesare
TI  - CHI - Pseudo-Haptic Weight: Changing the Perceived Weight of Virtual Objects By Manipulating Control-Display Ratio
PY  - 2019
AB  - In virtual reality, the lack of kinesthetic feedback often prevents users from experiencing the weight of virtual objects. Control-to-display (C/D) ratio manipulation has been proposed as a method to induce weight perception without kinesthetic feedback. Based on the fact that lighter (heavier) objects are easier (harder) to move, this method induces an illusory perception of weight by manipulating the rendered position of users' hands---increasing or decreasing their displayed movements. In a series of experiments we demonstrate that C/D-ratio induces a genuine perception of weight, while preserving ownership over the virtual hand. This means that such a manipulation can be easily introduced in current VR experiences without disrupting the sense of presence. We discuss these findings in terms of estimation of physical work needed to lift an object. Our findings provide the first quantification of the range of C/D-ratio that can be used to simulate weight in virtual reality.
SP  - 320
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300550
ER  - 

TY  - NA
AU  - Genay, Adelaide; Lecuyer, Anatole; Hachet, Martin
TI  - What Can I Do There? Controlling AR Self-Avatars to Better Perceive Affordances of the Real World
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00061
ER  - 

TY  - NA
AU  - Mirhosseini, Seyedkoosha; Ghahremani, Parmida; Ojar, Sushant; Marino, Joseph; Kaufrnan, Arie
TI  - VR - Exploration of Large Omnidirectional Images in Immersive Environments
PY  - 2019
AB  - Navigation is a major challenge in exploring data within immersive environments, especially of large omnidirectional spherical images. We propose a method of auto-scaling to allow users to navigate using teleportation within the safe boundary of their physical environment with different levels of focus. Our method combines physical navigation with virtual teleportation. We also propose a “peek then warp” behavior when using a zoom lens and evaluate our system in conjunction with different teleportation transitions, including a proposed transition for exploration of omnidirectional and 360-degree panoramic imagery, termed Envelop, wherein the destination view expands out from the zoom lens to completely envelop the user. In this work, we focus on visualizing and navigating large omnidirectional or panoramic images with application to GIS visualization as an inside-out omnidirectional image of the earth. We conducted two user studies to evaluate our techniques over a search and comparison task. Our results illustrate the advantages of our techniques for navigation and exploration of omnidirectional images in an immersive environment.
SP  - 413
EP  - 422
JF  - 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr.2019.8797777
ER  - 

TY  - JOUR
AU  - Song, Jean Y.; Chung, John Joon Young; Fouhey, David F.; Lasecki, Walter S.
TI  - C-Reference: Improving 2D to 3D Object Pose Estimation Accuracy via Crowdsourced Joint Object Estimation
PY  - 2020
AB  - Converting widely-available 2D images and videos, captured using an RGB camera, to 3D can help accelerate the training of machine learning systems in spatial reasoning domains ranging from in-home assistive robots to augmented reality to autonomous vehicles. However, automating this task is challenging because it requires not only accurately estimating object location and orientation, but also requires knowing currently unknown camera properties (e.g., focal length). A scalable way to combat this problem is to leverage people's spatial understanding of scenes by crowdsourcing visual annotations of 3D object properties. Unfortunately, getting people to directly estimate 3D properties reliably is difficult due to the limitations of image resolution, human motor accuracy, and people's 3D perception (i.e., humans do not "see" depth like a laser range finder). In this paper, we propose a crowd-machine hybrid approach that jointly uses crowds' approximate measurements of multiple in-scene objects to estimate the 3D state of a single target object. Our approach can generate accurate estimates of the target object by combining heterogeneous knowledge from multiple contributors regarding various different objects that share a spatial relationship with the target object. We evaluate our joint object estimation approach with 363 crowd workers and show that our method can reduce errors in the target object's 3D location estimation by over 40%, while requiring only $35$% as much human time. Our work introduces a novel way to enable groups of people with different perspectives and knowledge to achieve more accurate collective performance on challenging visual annotation tasks.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW1
PB  - 
DO  - 10.1145/3392858
ER  - 

TY  - BOOK
AU  - Dhengre, Snehal; Mathur, Jayant; Oghazian, Farzaneh; Tan, Xiaomei; McComb, Christopher
TI  - ICCC - Towards Enhanced Creativity in Interface Design through Automated Usability Evaluation.
PY  - 2020
AB  - NA
SP  - 366
EP  - 369
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Boyce, Michael W.; Thomson, Robert H.; Cartwright, Joel K.; Feltner, David T.; Stainrod, Cortnee R.; Flynn, Jeremy; Ackermann, Christian; Emezie, John; Amburn, Charles R; Rovira, Ericka
TI  - Enhancing Military Training Using Extended Reality: A Study of Military Tactics Comprehension
PY  - 2022
AB  - <jats:p>This study identifies that increasing the fidelity of terrain representation does not necessarily increase overall understanding of the terrain in a simulated mission planning environment using the Battlefield Visualization and Interaction software (BVI; formerly known as ARES (M. W. Boyce et al., International Conference on Augmented Cognition, 2017, 411–422). Prior research by M. Boyce et al. (Military Psychology, 2019, 31(1), 45–59) compared human performance on a flat surface (tablet) versus topographically-shaped surface (BVI on a sand table integrated with top-down projection). Their results demonstrated that the topographically-shaped surface increased the perceived usability of the interface and reduced cognitive load relative to the flat interface, but did not affect overall task performance (i.e., accuracy and response time). The present study extends this work by adding BVI onto a Microsoft HoloLens™. A sample of 72 United States Military Academy cadets used BVI on three different technologies: a tablet, a sand table (a projection-based display onto a military sand table), and on the HoloLens™ in a within-subjects design. Participants answered questions regarding military tactics in the context of conducting an attack in complex terrain. While prior research (Dixon et al., Display Technologies and Applications for Defense, Security, and Avionics III, 2009, 7327) suggested that the full 3D visualization used by the Hololens™ should improve performance relative to the sand table and tablet, our results demonstrated that the HoloLens™ performed relatively worse than the other modalities in accuracy, response time, cognitive load, and usability. Implications and limitations of this work will be discussed.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 3
IS  - NA
PB  - 
DO  - 10.3389/frvir.2022.754627
ER  - 

TY  - NA
AU  - Cheng, Lung-Pan; Chang, Li; Marwecki, Sebastian; Baudisch, Patrick
TI  - CHI Extended Abstracts - iTurk: Turning Passive Haptics into Active Haptics by Making Users Reconfigure Props in Virtual Reality
PY  - 2018
AB  - We present a system that complements virtual reality experiences with passive props, yet still allows modifying the virtual world at runtime. The main contribution of our system is that it does not require any actuators; instead, our system employs the user to reconfigure and actuate otherwise passive props. We demonstrate a foldable prop that users reconfigure to represent a suitcase, a fuse cabinet, a railing, and a seat. A second prop, suspended from a long pendulum, not only stands in for inanimate objects, but also for objects that move and demonstrate proactive behavior, such as a group of flying droids that physically attack the user. Our approach conveys a sense of a living, animate world, when in reality the user is the only animate entity present in the system, complemented with only one or two physical props. In our study, participants rated their experience as more enjoyable and realistic than a corresponding no-haptics condition.
SP  - 89
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3186482
ER  - 

TY  - BOOK
AU  - Mattusch, Thomas; Mirzamohammad, Mahsa; Khamis, Mohamed; Bulling, Andreas; Alt, Florian
TI  - ETRA - Hidden pursuits: evaluating gaze-selection via pursuits when the stimuli's trajectory is partially hidden
PY  - 2018
AB  - The idea behind gaze interaction using Pursuits is to leverage the human's smooth pursuit eye movements performed when following moving targets. However, humans can also anticipate where a moving target would reappear if it temporarily hides from their view. In this work, we investigate how well users can select targets using Pursuits in cases where the target's trajectory is partially invisible (HiddenPursuits): e.g., can users select a moving target that temporarily hides behind another object? Although HiddenPursuits was not studied in the context of interaction before, understanding how well users can perform HiddenPursuits presents numerous opportunities, particularly for small interfaces where a target's trajectory can cover area outside of the screen. We found that users can still select targets quickly via Pursuits even if their trajectory is up to 50% hidden, and at the expense of longer selection times when the hidden portion is larger. We discuss how gaze-based interfaces can leverage HiddenPursuits for an improved user experience.
SP  - 27
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204569
ER  - 

TY  - JOUR
AU  - Zhang, Tengxiang; Yi, Xin; Wang, Ruolin; Wang, Yuntao; Yu, Chun; Lu, Yiqin; Shi, Yuanchun
TI  - Tap-to-Pair: Associating Wireless Devices with Synchronous Tapping
PY  - 2018
AB  - Ad-hoc wireless device pairing enables impromptu interactions in smart spaces, such as resource sharing and remote control. The pairing experience is mainly determined by the device association process, during which users express their pairing intentions between the advertising device and the scanning device. Currently, most wireless devices are associated by selecting the advertiser's name from a list displayed on the scanner's screen, which becomes less efficient and often misplaced as the number of wireless devices increases. In this paper, we propose Tap-to-Pair, a spontaneous device association mechanism that initiates pairing from advertising devices without hardware or firmware modifications. Tapping an area near the advertising device's antenna can change its signal strength. Users can then associate two devices by synchronizing taps on the advertising device with the blinking pattern displayed by the scanning device. By leveraging the wireless transceiver for sensing, Tap-to-Pair does not require additional resources from advertising devices and needs only a binary display (e.g. LED) on scanning devices. We conducted a user study to test users' synchronous tapping ability and demonstrated that Tap-to-Pair can reliably detect users' taps. We ran simulations to optimize parameters for the synchronization recognition algorithm and provide pattern design guidelines. We used a second user study to evaluate the on-chip performance of Tap-to-Pair. The results show that Tap-to-Pair can achieve an overall successful pairing rate of 93.7% with three scanning devices at different distances.
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287079
ER  - 

TY  - NA
AU  - Wimmer, Raphael; Schmid, Andreas; Bockes, Florian
TI  - CHI - On the Latency of USB-Connected Input Devices
PY  - 2019
AB  - We propose a method for accurately and precisely measuring the intrinsic latency of input devices and document measurements for 36 keyboards, mice and gamepads connected via USB. Our research shows that devices differ not only in average latency, but also in the distribution of their latencies, and that forced polling at 1000 Hz decreases latency for some but not all devices. Existing practices - measuring end-to-end latency as a proxy of input latency and reporting only mean values and standard deviations - hide these characteristic latency distributions caused by device intrinsics and polling rates. A probabilistic model of input device latency demonstrates these issues and matches our measurements. Thus, our work offers guidance for researchers, engineers, and hobbyists who want to measure the latency of input devices or select devices with low latency.
SP  - 420
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300650
ER  - 

TY  - CHAP
AU  - Nakayama, Shota; Manabe, Mitsuki; Ushiyama, Keigo; Miyakami, Masahiro; Takahashi, Akifumi; Kajimoto, Hiroyuki
TI  - Pilot Study on Presenting Pulling Sensation by Electro-Tactile Stimulation
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>When an object that is grasped with a finger is pulled by an external force, the traction force is perceived by cutaneous receptors and proprioception in the finger. Several attempts have been made to simulate the pulling sensation by using wearable devices, including mechanical asymmetric vibration and tightening by belt. In this study, we developed a new method that uses electrical simulation to generate an illusory force sensation by simulating the activity pattern of the cutaneous receptors. We validated our method through two experiments, one based on force direction judgment and the other on force magnitude adjustment.</jats:p>
SP  - 66
EP  - 74
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06249-0_8
ER  - 

TY  - NA
AU  - Quintal, Filipe; Esteves, Augusto; Caires, Fabio; Baptista, Vitor; Mendes, Pedro
TI  - Tangible and Embedded Interaction - Wattom: A Consumption and Grid Aware Smart Plug with Mid-air Controls
PY  - 2019
AB  - This paper presents Wattom, a highly interactive ambient eco-feedback smart plug that aims to support a more sustainable use of electricity by being tightly coupled to users' energy-related activities. We describe three use cases of the system: using Wattom to power connected appliances and understand the environmental impact of their use in real time; scheduling these power events; and presenting users with personal consumption data desegregated by device. We conclude with a user study in which the effectiveness of the plug's novel interactive capabilities is assessed (mid-air, hand-based motion matching). The study explores the effectiveness of Wattom and motion matching input in a realistic setup, where the user is not always directly ahead of the interface, and not always willing to point straight at the device (e.g., when the plug is at an uncomfortable angle). Despite not using a graphical display, our results demonstrate that our motion matching implementation was effective in line with previous work, and that participants' pointing angle did not significantly affect their performance. On the other hand, participants were more effective while pointing straight at Wattom, but reported not to finding this significantly more strenuating then when pointing to a comfortable position of their choice.
SP  - 307
EP  - 313
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3295642
ER  - 

TY  - NA
AU  - Romeo, Pablo Alvarez; Altinsoy, Mehmet Ercan
TI  - WHC - Passively Actuated Clutch for Enhanced Backdrivability in Bidirectional Kinesthetic Feedback Applications
PY  - 2021
AB  - Portable and wearable force feedback devices for the hand present limitations in multiple aspects, related partially to their actuation systems. Commercial solutions commonly perform a trade-off by offering limited force feedback in the grasping motion in exchange for a lower volume, weight and complexity. However bidirectional feedback is usually discarded. In this paper, an inexpensive and compact clutch module design is presented. Passively controlled by the feedback actuator, it enables active torque transmission in both rotation directions, otherwise offering high backdrivability when unclutched. Throughout the various sections the theoretical design, the system’s operation and its characterization will be described, followed by a set of tests and finishing with an analysis of its advantages and disadvantages.
SP  - 97
EP  - 102
JF  - 2021 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc49131.2021.9517238
ER  - 

TY  - NA
AU  - Sakura, Rei; Han, Changyo; Watanabe, Keisuke; Yamamura, Ryosuke; Kakehi, Yasuaki
TI  - Design of 3D-Printed Soft Sensors for Wire Management and Customized Softness
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519906
ER  - 

TY  - NA
AU  - Alcala, Rony R.; Arceo, Zuedmar G.; Baterisna, Jonathan N.; Morada, Jimmy O.; Ramirez, Julius Oliver D.; Tolentino, Roselito E.
TI  - Design and Implementation of Body Wearable Device and Oculus Rift Controlled Panning and Aiming Sentry Gun Turret
PY  - 2020
AB  - The article presents a study of design and implementation of pan movement to gun turret resulted in diminishing the range limitations of the gun turret. Gun turret is a mounted weapon which has a mechanism for firing projectile from inside and can aim and shoot its desired target within its range of operation. The proponents designed a gun turret that can move more than 360 degrees. The gun turret aiming is controlled by Oculus Rift DK2 while the added gun turret's pan movement is controlled by the body-wearable device. The proponents used MS Visual Studio for extracting and graphing of the data then used statistical methods to evaluate the response of the system. The acquired sensor's data from the Oculus Rift and body-wearable device was compared to the data of the sensors placed on the gun turret. The result shown that the gun turret design created can perform both aiming and panning movements. The proponents also concluded that the pan movement added to the gun turret system does not affect the aiming of the gun.
SP  - 871
EP  - 876
JF  - 2020 4th International Conference on Trends in Electronics and Informatics (ICOEI)(48184)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icoei48184.2020.9143032
ER  - 

TY  - NA
AU  - Maeda, Tomosuke; Yoshida, Shigeo; Murakami, Takaki; Matsuda, Kenroh; Tanikawa, Tomohiro; Sakai, Hiroyuki
TI  - Fingeret: A Wearable Fingerpad-Free Haptic Device for Mixed Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567703
ER  - 

TY  - NA
AU  - Drosos, Ian; Barik, Titus; Guo, Philip J.; DeLine, Robert; Gulwani, Sumit
TI  - CHI - Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists
PY  - 2020
AB  - Data wrangling is a difficult and time-consuming activity in computational notebooks, and existing wrangling tools do not fit the exploratory workflow for data scientists in these environments. We propose a unified interaction model based on programming-by-example that generates readable code for a variety of useful data transformations, implemented as a Jupyter notebook extension called Wrex. User study results demonstrate that data scientists are significantly more effective and efficient at data wrangling with Wrex over manual programming. Qualitative participant feedback indicates that Wrex was useful and reduced barriers in having to recall or look up the usage of various data transform functions. The synthesized code allowed data scientists to verify the intended data transformation, increased their trust and confidence in Wrex, and fit seamlessly within their cell-based notebook workflows. This work suggests that presenting readable code to professional data scientists is an indispensable component of offering data wrangling tools in notebooks.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376442
ER  - 

TY  - NA
AU  - Tanaka, Yudai; Horie, Arata; Chen, Xiang 'Anthony'
TI  - VRST - DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback
PY  - 2020
AB  - We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418964
ER  - 

TY  - NA
AU  - Mirza, Tabasum; Tuli, Neha; Mantri, Archana
TI  - Virtual Reality, Augmented Reality, and Mixed Reality Applications: Present Scenario
PY  - 2022
AB  - With tremendous research, innovation in human computer interaction, infographics and greater accessibility of visualization technologies i.e., augmented reality; virtual reality and mixed reality is to find new possibilities for its applications. The ability to transform the user&#x0027;s perception and interaction with his environment has increased his popularity of use in all spheres of life. This study analyses the literature on augmented reality, virtual reality and mixed reality applications in order to find the research gaps in these fields. This review study has taken 103 papers of 2010-2021 into account from SCOPUS, SCI and IEEE databases. The results of the analysis can provide some useful insights regarding potential and use of augmented reality, virtual reality and mixed reality in the different areas of application.
SP  - NA
EP  - NA
JF  - 2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icacite53722.2022.9823482
ER  - 

TY  - JOUR
AU  - Fernandez, Misahael; Mathis, Florian; Khamis, Mohamed
TI  - GazeWheels: Recommendations for using wheel widgets for feedback during dwell-time gaze input
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>We present GazeWheels: a series of visual feedback methods for dwell-based gaze input in the form of a wheel that is filled gradually until target selection. We evaluate three variations: Resetting, Pause &amp; Resume and Infinite GazeWheel, and study how dwell duration and visual feedback position (co-located vs remote) impact performance. Findings from a user study (N = 19) show that Infinite and Pause &amp; Resume GazeWheels are error prone but significantly faster than Resetting GazeWheel even when including error correction time. We conclude with five design recommendations.</jats:p>
SP  - 145
EP  - 156
JF  - it - Information Technology
VL  - 63
IS  - 3
PB  - 
DO  - 10.1515/itit-2020-0042
ER  - 

TY  - NA
AU  - Faltaous, Sarah; Prochazka, Marvin; Auda, Jonas; Keppel, Jonas; Wittig, Nick; Gruenefeld, Uwe; Schneegass, Stefan
TI  - Give Weight to VR: Manipulating Users' Perception of Weight in Virtual Reality with Electric Muscle Stimulation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3547571
ER  - 

TY  - NA
AU  - Clarence, Aldrich; Knibbe, Jarrod; Cordeil, Maxime; Wybrow, Michael
TI  - Investigating The Effect of Direction on The Limits of Haptic Retargeting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00078
ER  - 

TY  - JOUR
AU  - Cristina, Stefania; Camilleri, Kenneth P.
TI  - Unobtrusive and pervasive video-based eye-gaze tracking
PY  - 2018
AB  - Abstract Eye-gaze tracking has long been considered a desktop technology that finds its use inside the traditional office setting, where the operating conditions may be controlled. Nonetheless, recent advancements in mobile technology and a growing interest in capturing natural human behaviour have motivated an emerging interest in tracking eye movements within unconstrained real-life conditions, referred to as pervasive eye-gaze tracking. This critical review focuses on emerging passive and unobtrusive video-based eye-gaze tracking methods in recent literature, with the aim to identify different research avenues that are being followed in response to the challenges of pervasive eye-gaze tracking. Different eye-gaze tracking approaches are discussed in order to bring out their strengths and weaknesses, and to identify any limitations, within the context of pervasive eye-gaze tracking, that have yet to be considered by the computer vision community.
SP  - 21
EP  - 40
JF  - Image and Vision Computing
VL  - 74
IS  - NA
PB  - 
DO  - 10.1016/j.imavis.2018.04.002
ER  - 

TY  - NA
AU  - Aigner, Roland; Haberfellner, Mira Alida; Haller, Michael
TI  - spaceR: Knitting Ready-Made, Tactile, and Highly Responsive Spacer-Fabric Force Sensors for Continuous Input
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545694
ER  - 

TY  - JOUR
AU  - An, Jaepung; Choi, Gyujin; Chun, Wooyoung; Joo, Yesle; Park, Sanghun; Ihm, Insung
TI  - Accurate and stable alignment of virtual and real spaces using consumer-grade trackers
PY  - 2021
AB  - NA
SP  - 125
EP  - 141
JF  - Virtual Reality
VL  - 26
IS  - 1
PB  - 
DO  - 10.1007/s10055-021-00542-1
ER  - 

TY  - NA
AU  - Zhang, Andrew; Jacobs, Jennifer; Sra, Misha; Höllerer, Tobias
TI  - Multi-View AR Streams for Interactive 3D Remote Teaching
PY  - 2021
AB  - In this work, we present a system that adds augmented reality interaction and 3D-space utilization to educational videoconferencing for a more engaging distance learning experience. We developed infrastructure and user interfaces that enable the use of an instructor’s physical 3D space as a teaching stage, promote student interaction, and take advantage of the flexibility of adding virtual content to the physical world. The system is implemented using hand-held mobile augmented reality to maximize device availability, scalability, and ready deployment, elevating traditional video lectures to immersive mixed reality experiences. We use multiple devices on the teacher’s end to provide different simultaneous views of a teaching space towards a better understanding of the 3D space.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489950
ER  - 

TY  - NA
AU  - Nabil, Sara; Jones, Lee; Girouard, Audrey
TI  - TEI - Soft Speakers: Digital Embroidering of DIY Customizable Fabric Actuators
PY  - 2021
AB  - We introduce Soft Speakers, a systematic approach for designing custom fabric actuators that can be used as audio speakers and vibro-haptic actuators. Digitally-embroidered with e-textiles, we implement Soft Speakers as tactile, malleable and aesthetic designs to be part of wearables, soft furnishing and fabric objects. We present a rapid technique for the DIY fabrication of audio feedback into soft interfaces. We also discuss and evaluate 7 factors for their parametric design in additive and constructive methods. To demonstrate the feasibility of our approach and the breadth of new designs that it enables, we developed 5 prototypes: 3 wearables, a piece of furniture and a soft toy. Studying Soft Speakers with maker-users expanded the design space, empowering users and supporting inclusive design. Our study includes insights on user experience of real-world interactive applications for remote communication, e-learning, entertainment, navigation and gaming, enabled by Soft Speakers’ customizable and scalable form factor.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440630
ER  - 

TY  - NA
AU  - Pamparău, Cristian; Vatavu, Radu-Daniel
TI  - The User Experience of Journeys in the Realm of Augmented Reality Television
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - ACM International Conference on Interactive Media Experiences
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3505284.3529969
ER  - 

TY  - NA
AU  - Hashimoto, Takeru; Yoshida, Shigeo; Narumi, Takuji
TI  - MetamorphX: An Ungrounded 3-DoF Moment Display that Changes its Physical Properties through Rotational Impedance Control
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545650
ER  - 

TY  - NA
AU  - Rajaram, Shwetha; Nebeling, Michael
TI  - Paper Trail: An Immersive Authoring System for Augmented Reality Instructional Experiences
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517486
ER  - 

TY  - JOUR
AU  - Amesaka, Takashi; Watanabe, Hiroki; Sugimoto, Masanori; Shizuki, Buntarou
TI  - Gesture Recognition Method Using Acoustic Sensing on Usual Garment
PY  - 2022
AB  - <jats:p>In this study, we show a new gesture recognition method for clothing-based gesture input methods using active and passive acoustic sensing. Our system consists of a piezoelectric speaker and a microphone. The speaker transmits ultrasonic swept sine signals, and the microphone simultaneously records the ultrasonic signals that propagate through the garment and the rubbing sounds generated by the gestures on the garment. Our method recognizes a variety of gestures, such as pinch, twist, touch, and swipe, by incorporating active and passive acoustic sensing. An important feature of our method is that it does not require a dedicated garment or embroidery embedded since our system only requires a pair of piezoelectric elements to be attached to the usual garment with a magnet. We performed recognition experiments of 11 gestures on the forearm with four types of garments made from different materials and recognition experiments of five one-handed gestures on the button of a shirt and the pocket of pants. The results of a per-user classifier confirmed that the f-scores were 83.9% and 95.9% for 11 gestures with four different types of garments and 5 gestures that were selected assuming actual use, respectively. In addition, we confirmed that the system recognizes five gestures, which can be performed with one hand, with 89.2% and 92.6% accuracy in the button and pocket sites, respectively.</jats:p>
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534579
ER  - 

TY  - NA
AU  - Orlosky, Jason; Sra, Misha; Bektaş, Kenan; Peng, Huaishu; Kim, Jeeeun; Kos'myna, Nataliya; Höllerer, Tobias; Steed, Anthony; Kiyokawa, Kiyoshi; Akşit, Kaan
TI  - Telelife: The Future of Remote Living
PY  - 2021
AB  - In recent years, everyday activities such as work and socialization have steadily shifted to more remote and virtual settings. With the COVID-19 pandemic, the switch from physical to virtual has been accelerated, which has substantially affected various aspects of our lives, including business, education, commerce, healthcare, and personal life. This rapid and large-scale switch from in-person to remote interactions has revealed that our current technologies lack functionality and are limited in their ability to recreate interpersonal interactions. To help address these limitations in the future, we introduce "Telelife," a vision for the near future that depicts the potential means to improve remote living better aligned with how we interact, live and work in the physical world. Telelife encompasses novel synergies of technologies and concepts such as digital twins, virtual prototyping, and attention and context-aware user interfaces with innovative hardware that can support ultrarealistic graphics, user state detection, and more. These ideas will guide the transformation of our daily lives and routines soon, targeting the year 2035. In addition, we identify opportunities across high-impact applications in domains related to this vision of Telelife. Along with a recent survey of relevant fields such as human-computer interaction, pervasive computing, and virtual reality, the directions outlined in this paper will guide future research on remote living.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cools, Robbe; Gottsacker, Matt; Simeone, Adalberto; Bruder, Gerd; Welch, Greg; Feiner, Steven
TI  - Towards a Desktop-AR Prototyping Framework: Prototyping Cross-Reality Between Desktops and Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00040
ER  - 

TY  - JOUR
AU  - Mehmood, Adnan; He, Han; Chen, Xiaochen; Merilampi, Sari; Sydanheimo, Lauri; Ukkonen, Leena; Virkki, Johanna
TI  - Body Movement-Based Controlling Through Passive RFID Integrated Into Clothing
PY  - 2020
AB  - We present a passive ultra-high frequency (UHF) radiofrequency identification (RFID)-based strain sensor, which is designed for simple and efficient body movement monitoring. This RFID platform is fabricated from electro-textile materials and can thus be seamlessly integrated into our everyday clothing. The sensor platform has an integrated reference tag, in order to avoid the effects of reflections or external disturbances on the sensor tag performance. This sensor platform prototype has an on-body read range of 1 meter in a normal office environment with an off-the-shelf RFID reader. The wireless performance of the sensor tag has a significant change caused by arm elongation. Thus, the sensor functionality can be based on variation of the sensor tag’s backscattered power percentage ( $\Delta {P}{\%}$ ). Based on the preliminary results achieved in this study, this passive and cost-effective sensor platform could be an efficient future way to turn human gestures into inputs for digital devices.
SP  - 414
EP  - 419
JF  - IEEE Journal of Radio Frequency Identification
VL  - 4
IS  - 4
PB  - 
DO  - 10.1109/jrfid.2020.3010717
ER  - 

TY  - JOUR
AU  - Alessandrini, Andrea
TI  - A Study of Students Engaged in Electronic Circuit Wiring in an Undergraduate Course
PY  - 2022
AB  - NA
SP  - 78
EP  - 95
JF  - Journal of Science Education and Technology
VL  - 32
IS  - 1
PB  - 
DO  - 10.1007/s10956-022-09994-9
ER  - 

TY  - NA
AU  - McClelland, John
TI  - Haptic Feedback in Virtual Reality with Deformation and Shape-Change
PY  - 2018
AB  - Past Virtual Reality (VR) research shows that haptic feedback increases presence and improves users' task performance. However, providing haptic feedback for multiple virtual objects usually requires complex, immobile systems, or multiple haptic props. We present a new approach that applies deformable, shape-changing devices to VR haptics, leveraging the dominance of human vision in VR to provide realistic haptic feedback with physical shape approximations. Our first study evaluates our HaptoBend prototype through an elicitation study. Results support the use of physical shape approximations and reveal important user preferences. We translate these results and past work into a Design Criteria to inform our second prototype, Adaptic. In our second study, we compare docking performance and adherence to our Design Criteria with Adaptic, a Razor Hydra Controller, and haptic props. We found Adaptic did well in satisfying our Design Criteria and had little difference in performance compared to the other haptic approaches.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - van Oosterhout, Anke; Hoggan, Eve
TI  - CHI Extended Abstracts - Deformation Techniques for Shape Changing Interfaces
PY  - 2021
AB  - Since the introduction of shape changing interfaces, research in the field has contributed a number of taxonomies to classify shape changing interfaces according to different characteristics, including shape, interaction mapping, material, actuation, and information affordances, in an attempt to grasp the diversity of these interfaces in terms of design and information. However, to our knowledge there exists no classifications of input techniques that are used to deform shape changing interfaces through physical interaction. The interaction affordances provided by shape changing interfaces are important for interaction design and interaction mapping. The work presented here aims to analyse how deformable properties in shape changing interfaces are related to deformation techniques, in order to provide a first step towards the development of design guidelines for physical interaction with shape changing interfaces. The results of the study contribute an overview of interaction techniques based on the analysis of a set of 7 models with different deformable properties, each presented in 2D and 3D form.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451622
ER  - 

TY  - NA
AU  - Narayanan, Archana; Hu, Erzhen; Heo, Seongkook
TI  - Enabling Remote Hand Guidance in Video Calls Using Directional Force Illusion
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Companion Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3500868.3559470
ER  - 

TY  - NA
AU  - Cheng, Lung-Pan; Chang, Li; Marwecki, Sebastian; Baudisch, Patrick
TI  - iTurk
PY  - 2018
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173663
ER  - 

TY  - JOUR
AU  - Steinbach, Eckehard; Strese, Matti; Eid, Mohamad; Liu, Xun; Bhardwaj, Amit; Liu, Qian; Al-Ja'afreh, Mohammad; Mahmoodi, Toktam; Hassen, Rania; Saddik, Abdulmotaleb El; Holland, Oliver
TI  - Haptic Codecs for the Tactile Internet
PY  - 2019
AB  - The Tactile Internet will enable users to physically explore remote environments and to make their skills available across distances. An important technological aspect in this context is the acquisition, compression, transmission, and display of haptic information. In this paper, we present the fundamentals and state of the art in haptic codec design for the Tactile Internet. The discussion covers both kinesthetic data reduction and tactile signal compression approaches. We put a special focus on how limitations of the human haptic perception system can be exploited for efficient perceptual coding of kinesthetic and tactile information. Further aspects addressed in this paper are the multiplexing of audio and video with haptic information and the quality evaluation of haptic communication solutions. Finally, we describe the current status of the ongoing IEEE standardization activity P1918.1.1 which has the ambition to standardize the first set of codecs for kinesthetic and tactile information exchange across communication networks.
SP  - 447
EP  - 470
JF  - Proceedings of the IEEE
VL  - 107
IS  - 2
PB  - 
DO  - 10.1109/jproc.2018.2867835
ER  - 

TY  - NA
AU  - Zhong, Mingyuan; Li, Gang; Li, Yang
TI  - Spacewalker: Rapid UI Design Exploration Using Lightweight Markup Enhancement and Crowd Genetic Programming
PY  - 2021
AB  - User interface design is a complex task that involves designers examining a wide range of options. We present Spacewalker, a tool that allows designers to rapidly search a large design space for an optimal web UI with integrated support. Designers first annotate each attribute they want to explore in a typical HTML page, using a simple markup extension we designed. Spacewalker then parses the annotated HTML specification, and intelligently generates and distributes various configurations of the web UI to crowd workers for evaluation. We enhanced a genetic algorithm to accommodate crowd worker responses from pairwise comparison of UI designs, which is crucial for obtaining reliable feedback. Based on our experiments, Spacewalker allows designers to effectively search a large design space of a UI, using the language they are familiar with, and improve their design rapidly at a minimal cost.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Wang, Po-Yao; Cheng, Jen-Hao; Ning, Chih-Wei; Wang, Ping-Yi; Cheng, Lung-Pan
TI  - SIGGRAPH Immersive Pavilion - Haptic-go-round: A Surrounding Platform for Encounter-type Haptic in Virtual Reality Experiences
PY  - 2020
AB  - We present Haptic-go-round, a surrounding platform that allows deploying props and devices to provide haptic feedbacks in any direction in virtual reality experiences. The key component of Haptic-go-round is a motorized turntable that rotates the correct haptic device to the right direction at the right time to match what users are about to touch. We implemented a working platform including plug-and-play prop cartridges and a software interface that allow experience designers to agilely add their haptic components and use the platform for their applications.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2020 Immersive Pavilion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3388536.3407886
ER  - 

TY  - JOUR
AU  - Wolfartsberger, Josef; Riedl, René; Jodlbauer, Herbert; Haslinger, Niklas; Hlibchuk, Andrii; Kirisits, Alexander; Schuh, Stefan
TI  - Virtual Reality als Trainingsmethode: Eine Laborstudie aus dem Industriebereich
PY  - 2021
AB  - <jats:title>Zusammenfassung</jats:title><jats:p>Virtual Reality (VR) gilt als vielversprechende Technologie, nicht zuletzt deshalb, weil damit komplexe Inhalte vermittelt werden können. Da VR ein hohes Potenzial für Interaktivität und Immersion aufweist, findet die Technologie auch in der Industrie Anwendung, beispielsweise für die realitätsnahe Simulation von Montage- und Instandhaltungsarbeiten. Wissenschaftliche Befunde zur Wirksamkeit des Lernens in industriellen VR-Umgebungen existieren bislang kaum. Es ist daher auch unklar, wie Lerneffekte in virtuellen Umgebungen verbessert werden können. Der vorliegende Artikel untersucht, ob VR-gestütztes Training im Vergleich zu klassischem training-on-the-job (begleitet von einem Tutor) zu einer Steigerung des Lernerfolgs führt. Basierend auf einem VR-Trainingstool zum Erlernen von Montageabläufen wurde eine Laborstudie mit 24 ProbandInnen durchgeführt. Die Ergebnisse zeigen, dass VR-gestütztes Training zwar als weniger anstrengend empfunden wurde, der Lernerfolg jedoch im traditionellen training-on-the-job signifikant besser ausfiel. Auf Basis dieser Erkenntnisse werden die Chancen und Risiken aktueller VR-Trainingssimulationen diskutiert und konkrete Handlungsempfehlungen zur Steigerung des Lerneffekts formuliert.</jats:p>
SP  - 295
EP  - 308
JF  - HMD Praxis der Wirtschaftsinformatik
VL  - 59
IS  - 1
PB  - 
DO  - 10.1365/s40702-021-00819-8
ER  - 

TY  - NA
AU  - Chen, Yan; Pandey, Maulishree; Song, Jean Y.; Lasecki, Walter S.; Oney, Steve
TI  - CHI - Improving Crowd-Supported GUI Testing with Structural Guidance
PY  - 2020
AB  - Crowd testing is an emerging practice in Graphical User Interface (GUI) testing, where developers recruit a large number of crowd testers to test GUI features. It is often easier and faster than a dedicated quality assurance team, and its output is more realistic than that of automated testing. However, crowds of testers working in parallel tend to focus on a small set of commonly-used User Interface (UI) navigation paths, which can lead to low test coverage and redundant effort. In this paper, we introduce two techniques to increase crowd testers' coverage: interactive event-flow graphs and GUI-level guidance. The interactive event-flow graphs track and aggregate every tester's interactions into a single directed graph that visualizes the cases that have already been explored. Crowd testers can interact with the graphs to find new navigation paths and increase the coverage of the created tests. We also use the graphs to augment the GUI (GUI-level guidance) to help testers avoid only exploring common paths. Our evaluation with 30 crowd testers on 11 different test pages shows that the techniques can help testers avoid redundant effort while also increasing untrained testers' coverage by 55%. These techniques can help us develop more robust software that works in more mission-critical settings not only by performing more thorough testing with the same effort that has been put in before but also by integrating them into different parts of the development pipeline to make more reliable software in the early development stage.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376835
ER  - 

TY  - NA
AU  - Waugh, Kieran; McGill, Mark; Freeman, Euan
TI  - Push or Pinch? Exploring Slider Control Gestures for Touchless User Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546155.3546702
ER  - 

TY  - NA
AU  - Dementyev, Artem; Gálvez, Tomás Vega; Olwal, Alex
TI  - UIST - SensorSnaps: Integrating Wireless Sensor Nodes into Fabric Snap Fasteners for Textile Interfaces
PY  - 2019
AB  - Adding electronics to textiles can be time-consuming and requires technical expertise. We introduce SensorSnaps, low-power wireless sensor nodes that seamlessly integrate into caps of fabric snap fasteners. SensorSnaps provide a new technique to quickly and intuitively augment any location on the clothing with sensing capabilities. SensorSnaps securely attach and detach from ubiquitous commercial snap fasteners. Using inertial measurement units, the SensorSnaps detect tap and rotation gestures, as well as track body motion. We optimized the power consumption for SensorSnaps to work continuously for 45 minutes and up to 4 hours in capacitive touch standby mode. We present applications in which the SensorSnaps are used as gestural interfaces for a music player controller, cursor control, and motion tracking suit. The user study showed that SensorSnap could be attached in around 71 seconds, similar to attaching off-the-shelf snaps, and participants found the gestures easy to learn and perform. SensorSnaps could allow anyone to effortlessly add sophisticated sensing capacities to ubiquitous snap fasteners.
SP  - 17
EP  - 28
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347913
ER  - 

TY  - NA
AU  - Biener, Verena; Ofek, Eyal; Pahud, Michel; Kristensson, Per Ola; Grubert, Jens
TI  - Extended Reality for Knowledge Work in Everyday Environments
PY  - 2021
AB  - Virtual and Augmented Reality have the potential to change information work. The ability to modify the workers senses can transform everyday environments into a productive office, using portable head-mounted displays combined with conventional interaction devices, such as keyboards and tablets. While a stream of better, cheaper and lighter HMDs have been introduced for consumers in recent years, there are still many challenges to be addressed to allow this vision to become reality. This chapter summarizes the state of the art in the field of extended reality for knowledge work in everyday environments and proposes steps to address the open challenges.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Velloso, Eduardo; Morimoto, Carlos H.
TI  - CHI - A Probabilistic Interpretation of Motion Correlation Selection Techniques
PY  - 2021
AB  - Motion correlation interfaces are those that present targets moving in different patterns, which the user can select by matching their motion. In this paper, we re-formulate the task of target selection as a probabilistic inference problem. We demonstrate that previous interaction techniques can be modelled using a Bayesian approach and that how modelling the selection task as transmission of information can help us make explicit the assumptions behind similarity measures. We propose ways of incorporating uncertainty into the decision-making process and demonstrate how the concept of entropy can illuminate the measurement of the quality of a design. We apply these techniques in a case study and suggest guidelines for future work.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gu, Yizheng; Yu, Chun; Li, Zhipeng; Li, Weiqi; Xu, Shuchang; Xiaoying, Wei; Shi, Yuanchun
TI  - UIST - Accurate and Low-Latency Sensing of Touch Contact on Any Surface with Finger-Worn IMU Sensor
PY  - 2019
AB  - Head-mounted Mixed Reality (MR) systems enable touch in­teraction on any physical surface. However, optical methods (i.e., with cameras on the headset) have difficulty in determin­ing the touch contact accurately. We show that a finger ring with Inertial Measurement Unit (IMU) can substantially im­prove the accuracy of contact sensing from 84.74% to 98.61% (f1 score), with a low latency of 10 ms. We tested different ring wearing positions and tapping postures (e.g., with different fingers and parts). Results show that an IMU-based ring worn on the proximal phalanx of the index finger can accurately sense touch contact of most usable tapping postures. Partici­pants preferred wearing a ring for better user experience. Our approach can be used in combination with the optical touch sensing to provide robust and low-latency contact detection.
SP  - 1059
EP  - 1070
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347947
ER  - 

TY  - NA
AU  - Kato, Kunihiro; Ikematsu, Kaori; Igarashi, Yuki; Kawahara, Yoshihiro
TI  - Paper-Woven Circuits: Fabrication Approach for Papercraft-based Electronic Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3502253
ER  - 

TY  - NA
AU  - Chen, Xiang 'Anthony'; Coros, Stelian; Hudson, Scott E.
TI  - CHI - Medley: A Library of Embeddables to Explore Rich Material Properties for 3D Printed Objects
PY  - 2018
AB  - In our everyday life, we interact with and benefit from objects with a wide range of material properties. In contrast, personal fabrication machines (e.g., desktop 3D printers) currently only support a much smaller set of materials. Our goal is to close the gap between current limitations and the future of multi-material printing by enabling people to explore the reuse of material from everyday objects into their custom designs. To achieve this, we develop a library of embeddables--everyday objects that can be cut, worked and embedded into 3D printable designs. We describe a design space that characterizes the geometric and material properties of embeddables. We then develop Medley---a design tool whereby users can import a 3D model, search for embeddables with desired material properties, and interactively edit and integrate their geometry to fit into the original design. Medley also supports the final fabrication and embedding process, including instructions for carving or cutting the objects, and generating optimal paths for inserting embeddables. To validate the expressiveness of our library, we showcase numerous examples augmented by embeddables that go beyond the objects' original printed materials.
SP  - 162
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173736
ER  - 

TY  - NA
AU  - Oppenlaender, Jonas; Hosio, Simo
TI  - CHI Extended Abstracts - Experizone: Integrating Situated Scientific Experimentation with Teaching of the Scientific Method
PY  - 2019
AB  - Citizen Science projects ask their participants to contribute work to pre-defined topics, thereby typically rendering the participants as mere consumers of often narrowly defined tasks. In this work-in-progress paper, we present our work on an interactive experimentation platform that allows anybody - researchers as well as members of the crowd - to run experiments and test scientific hypotheses with a local crowd of volunteers. The platform also enforces a lightweight review process for teaching its users how to formulate valid scientific hypotheses and experimental designs.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3313043
ER  - 

TY  - JOUR
AU  - Alenezi, Hadeel S.; Faisal, Maha
TI  - Utilizing crowdsourcing and machine learning in education: Literature review
PY  - 2020
AB  - For many years, learning continues to be a vital developing field since it is the key measure of the world’s civilization and evolution with its enormous effect on both individuals and societies. Enhancing existing learning activities in general will have a significant impact on literacy rates around the world. One of the crucial activities in education is the assessment method because it is the primary way used to evaluate the student during their studies. The main purpose of this review is to examine the existing learning and e-learning approaches that use either crowdsourcing, machine learning, or both crowdsourcing and machine learning in their proposed solutions. This review will also investigate the addressed applications to identify the existing researches related to the assessment. Identifying all existing applications will assist in finding the unexplored gaps and limitations. This study presents a systematic literature review investigating 30 papers from the following databases: IEEE and ACM Digital Library. After performing the analysis, we found that crowdsourcing is utilized in 47.8% of the investigated learning activities, while each of the machine learning and the hybrid solutions are utilized in 26% of the investigated learning activities. Furthermore, all the existing approaches regarding the exam assessment problem that are using machine learning or crowdsourcing were identified. Some of the existing assessment systems are using the crowdsourcing approach and other systems are using the machine learning, however, none of the approaches provide a hybrid assessment system that uses both crowdsourcing and machine learning. Finally, it is found that using either crowdsourcing or machine learning in the online courses will enhance the interactions between the students. It is concluded that the current learning activities need to be enhanced since it is directly affecting the student’s performance. Moreover, merging both the machine learning to the crowd wisdom will increase the accuracy and the efficiency of education.
SP  - 2971
EP  - 2986
JF  - Education and Information Technologies
VL  - 25
IS  - 4
PB  - 
DO  - 10.1007/s10639-020-10102-w
ER  - 

TY  - NA
AU  - Drey, Tobias; Albus, Patrick; der Kinderen, Simon; Milo, Maximilian; Segschneider, Thilo; Chanzab, Linda; Rietzler, Michael; Seufert, Tina; Rukzio, Enrico
TI  - Towards Collaborative Learning in Virtual Reality: A Comparison of Co-Located Symmetric and Asymmetric Pair-Learning
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517641
ER  - 

TY  - JOUR
AU  - Fayazi, Morteza; Colter, Zachary; Youbi, Zineb Benameur-El; Bagherzadeh, Javad; Ajayi, Tutu; Dreslinski, Ronald
TI  - FASCINET: A Fully Automated Single-Board Computer Generator Using Neural Networks
PY  - 2022
AB  - Designing Single-Board Computers (SBCs) is becoming more challenging given the growing number of discrete components that are made available and the rate at which this number grows. Keeping track of all available components options, revisions, and functionalities is challenging for SBC designers who are striving for faster design cycles. Moreover, the procedure of deciding peripheral components, their values, and connections of an SBC is not only difficult because of various parameters that need to be considered, but also is time-consuming as there exist numerous components on a typical SBC nowadays. In this paper, an SBC generator tool, FASCINET, is presented that uses a Neural Network (NN) model to design customized peripheral circuits for SBCs. The tool creates a large Commercial Off-the-Shelf Database (COTS DB) of existing components, efficiently searches through them, and selects optimal components for both main and peripheral components based on the user’s requirements. Creating such a broad COTS DB requires processing abundant datasheets. A manual approach is time-consuming, even if only a fraction of all available datasheets is considered. In order to automate this process, this paper describes a novel NNbased approach for automatically categorizing datasheets and proposes an extraction technique for parsing relevant functional information from tables within. Our evaluation using a test set that contains over 770,000 components shows that the category of datasheets is identified correctly over 95% of the time. Additionally, the table extractor has a precision above 96%. Our proposed fully autonomous SBC design approach reduces the time for generating the schematic of an SBC to as little as two minutes. For validating the accuracy of our model, the netlists of 400 SBCs designed by FASCINET are compared to the humandesigned versions. This evaluation shows that FASCINET is able to design SBCs that are identical to the manually-designed ones except for minor differences.
SP  - 5435
EP  - 5448
JF  - IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
VL  - 41
IS  - 12
PB  - 
DO  - 10.1109/tcad.2022.3158073
ER  - 

TY  - NA
AU  - Weiler, Jennifer; Fernando, Piyum; Ingalls, Todd; Kuznetsov, Stacey
TI  - Tangible and Embedded Interaction - Lithobox: Creative Practice at the Intersection of Craft and Technology
PY  - 2019
AB  - The integration of new digital and physical fabrication tools with fine arts has the potential to provide new outlets for artistic expression, while at the same time raising questions about the role of material and process in artistic practice. In this work, we present Lithobox, a system that translates the traditional ceramic and lighting technique of lithophanes into a means of creating illuminated 3D models through a creative approach that utilizes both digital and tangible construction. Through work sessions with nine artists, we explored how the Lithobox fabrication process impacted the way artists manifest design ideas and engage in creative exploration in crafting. At the TEI arts track, we plan to show our system and the physical lithophanes from our work with artists. The attendees will likely discuss the design, material, and artistic aspects of our exhibit. From these discussions, our goal is to gain insight into beneficial directions for integrating digital technology into traditional fine arts practices.
SP  - 471
EP  - 477
JF  - Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3294109.3301258
ER  - 

TY  - NA
AU  - Oppenlaender, Jonas; Mackeprang, Maximilian; Khiat, Abderrahmane; Vukovic, Maja; Goncalves, Jorge; Hosio, Simo
TI  - CHI Extended Abstracts - DC2S2: Designing Crowd-powered Creativity Support Systems
PY  - 2019
AB  - Supporting creativity has been considered as one of the grand challenges of Human Computer Interaction. All creativity lies within humanity and crowdsourcing is a powerful approach for tapping into the collective insights of diverse crowds. Thus, crowdsourcing has great potential in supporting creativity. In this workshop, we brainstorm new crowdsourcing systems and concepts for supporting creativity, by bringing together researchers and industry professionals in a full-day workshop. The workshop consists of discussions of ideas contributed by the participants and hands-on brainstorming sessions in groups for ideating new crowd-powered systems that support creativity. We center the workshop around two themes: supporting the individual and facilitating creativity in groups.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3299027
ER  - 

TY  - NA
AU  - Müller, Leon; Pfeuffer, Ken; Gugenheimer, Jan; Pfleging, Bastian; Prange, Sarah; Alt, Florian
TI  - CHI - SpatialProto: Exploring Real-World Motion Captures for Rapid Prototyping of Interactive Mixed Reality
PY  - 2021
AB  - Spatial computing devices that blend virtual and real worlds have the potential to soon become ubiquitous. Yet, creating experiences for spatial computing is non-trivial and needs skills in programming and 3D content creation, rendering them inaccessible to a wider group of users. We present SpatialProto, an in-situ spatial prototyping system for lowering the barrier to engage in spatial prototyping. With a depth-sensing capable mixed reality headset, SpatialProto lets users record animated objects of the real-world environment (e.g. paper, clay, people, or any other prop), extract only the relevant parts, and directly place and transform these recordings in their physical environment. We describe the design and implementation of SpatialProto, a user study evaluating the system’s prototype with non-expert users (n = 9), and demonstrate applications where multiple captures are fused for compelling augmented reality experiences.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445560
ER  - 

TY  - NA
AU  - Jun, Eunice; Arian, Morelle; Reinecke, Katharina
TI  - L@S - The potential for scientific outreach and learning in mechanical turk experiments
PY  - 2018
AB  - The global reach of online experiments and their wide adoption in fields ranging from political science to computer science poses an underexplored opportunity for learning at scale: the possibility of participants learning about the research to which they contribute data. We conducted three experiments on Amazon's Mechanical Turk to evaluate whether participants of paid online experiments are interested in learning about research, what information they find most interesting, and whether providing them with such information actually leads to learning gains. Our findings show that 40% of our participants on Mechanical Turk actively sought out post-experiment learning opportunities despite having already received their financial compensation. Participants expressed high interest in a range of research topics, including previous research and experimental design. Finally, we find that participants comprehend and accurately recall facts from post-experiment learning opportunities. Our findings suggest that Mechanical Turk can be a valuable platform for learning at scale and scientific outreach.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifth Annual ACM Conference on Learning at Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3231644.3231666
ER  - 

TY  - JOUR
AU  - Chow, Kevin; Coyiuto, Caitlin; Nguyen, Cuong; Yoon, Dongwook
TI  - Challenges and Design Considerations for Multimodal Asynchronous Collaboration in VR
PY  - 2019
AB  - Studies on collaborative virtual environments (CVEs) have suggested capture and later replay of multimodal interactions (e.g., speech, body language, and scene manipulations), which we refer to as multimodal recordings, as an effective medium for time-distributed collaborators to discuss and review 3D content in an immersive, expressive, and asynchronous way. However, there exist gaps of empirical knowledge in understanding how this multimodal asynchronous VR collaboration (MAVRC) context impacts social behaviors in mediated-communication, workspace awareness in cooperative work, and user requirements for authoring and consuming multimedia recording. This study aims to address these gaps by conceptualizing MAVRC as a type of CSCW and by understanding the challenges and design considerations of MAVRC systems. To this end, we conducted an exploratory need-finding study where participants (N = 15) used an experimental MAVRC system to complete a representative spatial task in an asynchronously collaborative setting, involving both consumption and production of multimodal recordings. Qualitative analysis of interview and observation data from the study revealed unique, core design challenges of MAVRC in: (1) coordinating proxemic behaviors between asynchronous collaborators, (2) providing traceability and change awareness across different versions of 3D scenes, (3) accommodating viewpoint control to maintain workspace awareness, and (4) supporting navigation and editing of multimodal recordings. We discuss design implications, ideate on potential design solutions, and conclude the paper with a set of design recommendations for MAVRC systems.
SP  - 40
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359142
ER  - 

TY  - JOUR
AU  - Zhou, Jianshu; Chen, Yonghua; Chen, Xiaojiao; Wang, Zheng; Li, Yunquan; Liu, Yun-Hui
TI  - A Proprioceptive Bellows (PB) Actuator With Position Feedback and Force Estimation
PY  - 2020
AB  - Soft robot is known for great safety in human-centered environments due to its inherent compliance. However, the compliance resulting from the soft continuum structure and viscoelastic material also induces challenges for sensing and control of soft robots. In this letter, we propose a proprioceptive soft actuator design approach based on 3D printed conductive material and 3D printed deformable structure, such as bellows. The conductive bellow exhibits effective resistance change and structural deformation, thus provides a promising solution for the challenge of deformable soft robots that need integrated actuation and sensing. The proposed proprioceptive bellow actuator (PB actuator) achieves effective position feedback and real-time output force estimation. Using a dedicated control logic of the pressure controller, the PB actuator can not only provide anticipated motion but also estimate the interactive force based on real-time position sensing and input pressure. The design, fabrication, modeling, control, and experimental validation of our proposed PB actuator are discussed in detail in this letter. The parameters of PB actuator are highly customizable depending on the intended applications. Based on the proposed PB actuator, two specialized grippers, T and Y gripper, are designed and prototyped to demonstrate the grasping force estimation capability. The proposed proprioceptive soft robotic approach provides a promising solution to design behavior steerable soft robots.
SP  - 1867
EP  - 1874
JF  - IEEE Robotics and Automation Letters
VL  - 5
IS  - 2
PB  - 
DO  - 10.1109/lra.2020.2969920
ER  - 

TY  - NA
AU  - Conlen, Matthew; Heer, Jeffrey
TI  - UIST - Idyll: A Markup Language for Authoring and Publishing Interactive Articles on the Web
PY  - 2018
AB  - The web has matured as a publishing platform: news outlets regularly publish rich, interactive stories while technical writers use animation and interaction to communicate complex ideas. This style of interactive media has the potential to engage a large audience and more clearly explain concepts, but is expensive and time consuming to produce. Drawing on industry experience and interviews with domain experts, we contribute design tools to make it easier to author and publish interactive articles. We introduce Idyll, a novel "compile-to-the-web" language for web-based interactive narratives. Idyll implements a flexible article model, allowing authors control over document style and layout, reader-driven events (such as button clicks and scroll triggers), and a structured interface to JavaScript components. Through both examples and first-use results from undergraduate computer science students, we show how Idyll reduces the amount of effort and custom code required to create interactive articles.
SP  - 977
EP  - 989
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242600
ER  - 

TY  - JOUR
AU  - Peng, Hao; Lu, Lin; Liu, Lin; Sharf, Andrei; Chen, Baoquan
TI  - Fabricating QR codes on 3D objects using self-shadows
PY  - 2019
AB  - NA
SP  - 91
EP  - 100
JF  - Computer-Aided Design
VL  - 114
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2019.05.029
ER  - 

TY  - NA
AU  - Wang, Tianyi; Huo, Ke; Chawla, Pratik; Chen, Guiming; Banerjee, Siddharth; Ramani, Karthik
TI  - Conference on Designing Interactive Systems - Plain2Fun: Augmenting Ordinary Objects with Interactive Functions by Auto-Fabricating Surface Painted Circuits
PY  - 2018
AB  - The growing makers' community demands better supports for designing and fabricating interactive functional objects. Most of the current approaches focus on embedding desired functions within new objects. Instead, we advocate repurposing the existing objects and rapidly authoring interactive functions onto them. We present Plain2Fun, a design and fabrication pipeline enabling users to quickly transform ordinary objects into interactive and functional ones. Plain2Fun allows users to directly design the circuit layouts onto the surfaces of the scanned 3D model of existing objects. Our design tool automatically generates as short as possible circuit paths between any two points while avoiding intersections. Further, we build a digital machine to construct the conductive paths accurately. With a specially designed housing base, users can simply snap the electronic components onto the surfaces and obtain working physical prototypes. Moreover, we evaluate the usability of our system with multiple use cases and a preliminary user study.
SP  - 1095
EP  - 1106
JF  - Proceedings of the 2018 Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3196709.3196791
ER  - 

TY  - JOUR
AU  - Papp, György; Hoffmann, Miklós; Papp, Ildikó
TI  - Improved Embedding of QR Codes onto Surfaces to be 3D Printed
PY  - 2021
AB  - Abstract Providing additional information for parts or items usually means to enclose it next to the object or affix it on the component when that is possible. However, another solution is available by gaining the benefits of an additive manufacturing technology, 3D printing. This technology makes it possible to embed the additional information onto the surface of the items, for example, in the forms of QR codes. In the work of Kikuchi et al. (2018), the QR code is embedded into CAD models that consist of B-spline surfaces by grooving its dark regions to shadow them. The method proposed by Peng et al. (2019) optimized the modules of the QR code and the depth to carve its dark modules into any general mesh. However, embedding the QR code with these methods, in some cases, especially in case of highly curved surfaces, the QR code is deformed during the process of projection onto the surface. This deformation can highly restrict the readability of the QR code. In this paper, we propose an improved method to embed QR codes onto free-form surfaces by using a low-end consumer-level 3D printer. Our aim is to provide a robust method to project the QR code onto surfaces even with high curvature. We discuss the problematic cases for the works mentioned above, and we present a process to find an optimal position and direction of projection for the QR code to avoid deformations on highly curved surfaces. To validate our method, we compare our results with the outcomes of Kikuchi et al. (2018) and Peng et al. (2019).
SP  - 102961
EP  - NA
JF  - Computer-Aided Design
VL  - 131
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2020.102961
ER  - 

TY  - JOUR
AU  - Mathisen, Andreas; Horak, Tom; Klokmose, Clemens Nylandsted; Grønbæk, Kaj; Elmqvist, Niklas
TI  - InsideInsights: Integrating Data-Driven Reporting in Collaborative Visual Analytics
PY  - 2019
AB  - NA
SP  - 649
EP  - 661
JF  - Computer Graphics Forum
VL  - 38
IS  - 3
PB  - 
DO  - 10.1111/cgf.13717
ER  - 

TY  - NA
AU  - Maranes, Carlos; Gutierrez, Diego; Serrano, Ana
TI  - VR - Exploring the impact of 360° movie cuts in users’ attention
PY  - 2020
AB  - Virtual Reality (VR) has grown since the first devices for personal use became available on the market. However, the production of cinematographic content in this new medium is still in an early exploratory phase. The main reason is that cinematographic language in VR is still under development, and we still need to learn how to tell stories effectively. A key element in traditional film editing is the use of different cutting techniques, in order to transition seamlessly from one sequence to another. A fundamental aspect of these techniques is the placement and control over the camera. However, VR content creators do not have full control of the camera. Instead, users in VR can freely explore the 360° of the scene around them, which potentially leads to very different experiences. While this is desirable in certain applications such as VR games, it may hinder the experience in narrative VR. In this work, we perform a systematic analysis of users’ viewing behavior across cut boundaries while watching professionally edited, narrative 360° videos. We extend previous metrics for quantifying user behavior in order to support more complex and realistic footage, and we introduce two new metrics that allow us to measure users’ exploration in a variety of different complex scenarios. From this analysis, (i) we confirm that previous insights derived for simple content hold for professionally edited content, and (ii) we derive new insights that could potentially influence VR content creation, informing creators about the impact of different cuts in the audience’s behavior.
SP  - 73
EP  - 82
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1580727911717
ER  - 

TY  - NA
AU  - Kubo, Yuki; Eguchi, Kana; Aoki, Ryosuke; Kondo, Shigekuni; Azuma, Shozo; Indo, Takuya
TI  - CHI Extended Abstracts - FabAuth: Printed Objects Identification Using Resonant Properties of Their Inner Structures
PY  - 2019
AB  - We present a method we propose called FabAuth for identifying 3D-printed objects, which utilizes the differences in the resonant properties of such objects. We focus on changing the internal structures of each object made through a 3D printing process to assign a unique resonant property to it even if multiple objects have the same appearance. To identify the objects, the method identifies resonant property differences by using vibration that can pass through 3D-printed objects. The method can be applied even to low-filled 3D-printed objects as long as an acoustic wave can travel through the objects from one sensor to another. To validate the method's feasibility, we conducted a preliminary experiment to confirm whether it can be applied to low-filled 3D-printed objects and found that its average classification accuracy reached 92.2%.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290607.3313005
ER  - 

TY  - JOUR
AU  - Hsiao, Luke; Wu, Sen; Chiang, Nicholas; Ré, Christopher; Levis, Philip
TI  - Creating Hardware Component Knowledge Bases with Training Data Generation and Multi-task Learning
PY  - 2020
AB  - Hardware component databases are vital resources in designing embedded systems. Since creating these databases requires hundreds of thousands of hours of manual data entry, they are proprietary, limited in the data they provide, and have random data entry errors. We present a machine learning based approach for creating hardware component databases directly from datasheets. Extracting data directly from datasheets is challenging because: (1) the data is relational in nature and relies on non-local context, (2) the documents are filled with technical jargon, and (3) the datasheets are PDFs, a format that decouples visual locality from locality in the document. Addressing this complexity has traditionally relied on human input, making it costly to scale. Our approach uses a rich data model, weak supervision, data augmentation, and multi-task learning to create these knowledge bases in a matter of days. We evaluate the approach on datasheets of three types of components and achieve an average quality of 77 F1 points—quality comparable to existing human-curated knowledge bases. We perform application studies that demonstrate the extraction of multiple data modalities including numerical properties and images. We show how different sources of supervision such as heuristics and human labels have distinct advantages that can be utilized together to improve knowledge base quality. Finally, we present a case study to show how this approach changes the way practitioners create hardware component knowledge bases.
SP  - 1
EP  - 26
JF  - ACM Transactions on Embedded Computing Systems
VL  - 19
IS  - 6
PB  - 
DO  - 10.1145/3391906
ER  - 

TY  - NA
AU  - Zhong, Mingyuan; Li, Gang; Chi, Peggy; Li, Yang
TI  - UIST - HelpViz: Automatic Generation of Contextual Visual Mobile Tutorials from Text-Based Instructions
PY  - 2021
AB  - We present HelpViz, a tool for generating contextual visual mobile tutorials from text-based instructions that are abundant on the web. HelpViz transforms text instructions to graphical tutorials in batch, by extracting a sequence of actions from each text instruction through an instruction parsing model, and executing the extracted actions on a simulation infrastructure that manages an array of Android emulators. The automatic execution of each instruction produces a set of graphical and structural assets, including images, videos, and metadata such as clicked elements for each step. HelpViz then synthesizes a tutorial by combining parsed text instructions with the generated assets, and contextualizes the tutorial to user interaction by tracking the user’s progress and highlighting the next step. Our experiments with HelpViz indicate that our pipeline improved tutorial execution robustness and that participants preferred tutorials generated by HelpViz over text-based instructions. HelpViz promises a cost-effective approach for generating contextual visual tutorials for mobile interaction at scale.
SP  - 1144
EP  - 1153
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474812
ER  - 

TY  - NA
AU  - Zhou, Sharon; Valentine, Melissa; Bernstein, Michael S.
TI  - CHI - In Search of the Dream Team: Temporally Constrained Multi-Armed Bandits for Identifying Effective Team Structures
PY  - 2018
AB  - Team structures---roles, norms, and interaction patterns---define how teams work. HCI researchers have theorized ideal team structures and built systems nudging teams towards them, such as those increasing turn-taking, deliberation, and knowledge distribution. However, organizational behavior research argues against the existence of universally ideal structures. Teams are diverse and excel under different structures: while one team might flourish under hierarchical leadership and a critical culture, another will flounder. In this paper, we present DreamTeam: a system that explores a large space of possible team structures to identify effective structures for each team based on observable feedback. To avoid overwhelming teams with too many changes, DreamTeam introduces multi-armed bandits with temporal constraints: an algorithm that manages the timing of exploration--exploitation trade-offs across multiple bandits simultaneously. A field experiment demonstrated that DreamTeam teams outperformed self-managing teams by 38%, manager-led teams by 46%, and teams with unconstrained bandits by 41%. This research advances computation as a powerful partner in establishing effective teamwork.
SP  - 108
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173682
ER  - 

TY  - JOUR
AU  - Li, Changjian; Pan, Hao; Bousseau, Adrien; Mitra, Niloy J.
TI  - Sketch2CAD: sequential CAD modeling by sketching in context
PY  - 2020
AB  - We present a sketch-based CAD modeling system, where users create objects incrementally by sketching the desired shape edits, which our system automatically translates to CAD operations. Our approach is motivated by the close similarities between the steps industrial designers follow to draw 3D shapes, and the operations CAD modeling systems offer to create similar shapes. To overcome the strong ambiguity with parsing 2D sketches, we observe that in a sketching sequence, each step makes sense and can be interpreted in the context of what has been drawn before. In our system, this context corresponds to a partial CAD model, inferred in the previous steps, which we feed along with the input sketch to a deep neural network in charge of interpreting how the model should be modified by that sketch. Our deep network architecture then recognizes the intended CAD operation and segments the sketch accordingly, such that a subsequent optimization estimates the parameters of the operation that best fit the segmented sketch strokes. Since there exists no datasets of paired sketching and CAD modeling sequences, we train our system by generating synthetic sequences of CAD operations that we render as line drawings. We present a proof of concept realization of our algorithm supporting four frequently used CAD operations. Using our system, participants are able to quickly model a large and diverse set of objects, demonstrating Sketch2CAD to be an alternate way of interacting with current CAD modeling systems.
SP  - 1
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 6
PB  - 
DO  - 10.1145/3414685.3417807
ER  - 

TY  - CONF
AU  - Rule, Adam; Drosos, Ian; Tabard, Aurélien; Hollan, James D.
TI  - Aiding Collaborative Reuse of Computational Notebooks with Annotated Cell Folding
PY  - 2018
AB  - Computational notebooks aim to support collaborative data analysis by combining code, visualizations, and text in a single easily shared document. Yet, as notebooks evolve and grow they often become difficult to navigate or understand, discouraging sharing and reuse. We present the design and evaluation of a Jupyter Notebook extension providing facilities for annotated cell folding. Through a lab study and multi-week deployment we find cell folding aids notebook navigation and comprehension, not only by the original author, but also by collaborators viewing the notebook in a meeting or revising it on their own. However, in some cases cell folding encouraged collaborators to overlook folded sections or spend longer reviewing a notebook before editing it. These findings extend our understanding of code folding's trade-offs to a new medium and demonstrate its benefits for everyday collaboration. We conclude by discussing how dynamic reorganization can support sharing and reuse of computational notebooks.
SP  - 1
EP  - 12
JF  - NA
VL  - 2
IS  - 150
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Shibata, Ryuichi; Hashimoto, Wataru; Mizutani, Yasuharu; Nishiguchi, Satoshi
TI  - HCI (45) - Presentation of a Three-Dimensional Image by Rotating Pepper’s Ghost
PY  - 2021
AB  - In this research, we present a three-dimensional image of a Pepper’s Ghost, and simultaneously observed from multiple angles by rotating it. Pepper’s Ghost is a technique used to produce aerial images by combining the image reflected on a transparent plate with the background. A three-dimensional image is presented at the center of the rotation by rotating the transparent plate and displaying the image according to its orientation. The proposed system consists of a display placed on its side, transparent plate with a privacy filter attached to it, and motor. The top and bottom of the pyramid-shaped transparent plate, which is often used for the Pepper’s Ghost exhibit, are switched to show a three-dimensional image. A PI-controlled motor is used to rotate the transparent plate and facilitate observation from multiple angles by following the images. Because the horizontal viewing angle of Pepper’s Ghost exceeds 45 degrees, the image of the neighboring surface will appear to leak when the surfaces are adjacent. Therefore, we used a privacy filter to reduce the leakage of images from neighboring surfaces. We use a 240-fps display as the video playback device and present several images on a rotating the pyramid-shaped transparent plate.
SP  - 489
EP  - 496
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90179-0_62
ER  - 

TY  - NA
AU  - Suzuki, Yutaro; Sekimori, Kodai; Shizuki, Buntarou; Takahashi, Shin
TI  - PerCom Workshops - Touch Sensing on the Forearm Using the Electrical Impedance Method
PY  - 2019
AB  - We present a novel on-skin touch sensing approach based on the electrical impedance method (EIM). Our approach enables the user to detect touch across the surface of the forearm by wearing two bands, one each on the lower and upper forearm. EIM uses a conductive substance to identify the area being touched. We focused on the electrical conductivity of the skin and applied EIM to the forearm. The two bands have electrodes on the inside. Signals are applied to these electrodes, and the resulting voltage on the surface of the skin is then measured. The advantages of our approach are that it works over a large area of the forearm, and can recognize both hand gestures and touch.
SP  - 255
EP  - 260
JF  - 2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/percomw.2019.8730739
ER  - 

TY  - JOUR
AU  - Tai, Nan-Ching
TI  - On-Site Architectural Drawing with Hand-held Mobile Instructions
PY  - 2022
AB  - <jats:p>The advances in computer-aided design tools have enabled design visualization and realization to become more efficient and effective. However, these fast-growing digital technologies are also gradually reducing the presence of hand drawings in architectural education. This leads to a reduction in the ability to be inspired from the direct observation of the architectural environment through on-site freehand sketching. This study aims to implement digital technology as a teaching aid to retrieve these lost abilities. Analytical drawing is a method that encourages thinking before drawing, laying out the invisible underlying structure, and finalizing it with a visible appearance. This method remains an effective way of three-dimensional visual thinking. Accordingly, this study presents an interactive smartphone application that brings computer-assisted instructions into mobile learning. Promising responses from students revealed that using digital technology as a teaching aid can help to retrieve the lost abilities of visual thinking through on-site sketching.</jats:p>
SP  - 1
EP  - 6
JF  - International Journal of Information and Education Technology
VL  - 12
IS  - 1
PB  - 
DO  - 10.18178/ijiet.2022.12.1.1579
ER  - 

TY  - NA
AU  - Knorr, Sebastian; Ozcinar, Cagri; Fearghail, Colm O; Smolic, Aljosa
TI  - CVMP - Director's cut: a combined dataset for visual attention analysis in cinematic VR content
PY  - 2018
AB  - Methods of storytelling in cinema have well established conventions that have been built over the course of its history and the development of the format. In 360° film many of the techniques that have formed part of this cinematic language or visual narrative are not easily applied or are not applicable due to the nature of the format i.e. not contained the border of the screen. In this paper, we analyze how end-users view 360° video in the presence of directional cues and evaluate if they are able to follow the actual story of narrative 360° films. We first let filmmakers create an intended scan-path, the so called director's cut, by setting position markers in the equirectangular representation of the omnidirectional content for eight short 360° films. Alongside this the filmmakers provided additional information regarding directional cues and plot points. Then, we performed a subjective test with 20 participants watching the films with a head-mounted display and recorded the center position of the viewports. The resulting scan-paths of the participants are then compared against the director's cut using different scan-path similarity measures. In order to better visualize the similarity between the scan-paths, we introduce a new metric which measures and visualizes the viewport overlap between the participants' scan-paths and the director's cut. Finally, the entire dataset, i.e. the director's cuts including the directional cues and plot points as well as the scan-paths of the test subjects, is publicly available with this paper.
SP  - 3
EP  - NA
JF  - Proceedings of the 15th ACM SIGGRAPH European Conference on Visual Media Production
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3278471.3278472
ER  - 

TY  - NA
AU  - Wang, Hongbo; Bernardeschi, Irene; Beccai, Lucia
TI  - Developing Reliable Foam Sensors with Novel Electrodes
PY  - 2019
AB  - This paper presents an ultra-light, highly compressible, resistive sensor based on open-cell polyurethane foam coated with PEDOT:PSS. A novel electrodes configuration is developed to eliminate the unstable contact resistance, providing reliable electrical and mechanical connections for foam-based sensors. Thereby, the proposed sensors have a low resistance of 15 ohm, and can detect small strain variations (<0.1%) with negligible hysteresis (4%). Multiple samples were characterized and analyzed. The resistance only increases 4% after 100 cycles of 70% compression. The proposed foam sensor provides a low-cost, easy-to-implement, robust sensing solution for real-world applications in robotics and wearable systems.
SP  - NA
EP  - NA
JF  - 2019 IEEE SENSORS
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sensors43011.2019.8956750
ER  - 

TY  - JOUR
AU  - Kim, Jongyong; Song, J.H; Park, Jong-Geun; Nam, Ji-in; Yoon, Seung-Hyun; Park, Sanghun
TI  - Mixed Reality Extension System Using Beam Projectors : Beyond the Sight
PY  - 2019
AB  - NA
SP  - 65
EP  - 73
JF  - Journal of the Korea Computer Graphics Society
VL  - 25
IS  - 3
PB  - 
DO  - 10.15701/kcgs.2019.25.3.65
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Kazi, Rubaiat Habib; Wei, Li-Yi; DiVerdi, Stephen; Li, Wilmot; Leithinger, Daniel
TI  - UIST (Adjunct Volume) - RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching
PY  - 2020
AB  - We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.
SP  - 135
EP  - 138
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415892
ER  - 

TY  - NA
AU  - Li, Zhengqing; Chan, Liwei; Teo, Theophilus; Koike, Hideki
TI  - CHI Extended Abstracts - OmniGlobeVR: A Collaborative 360° Communication System for VR
PY  - 2020
AB  - In this paper, we propose OmniGlobeVR, a novel collaboration tool based on an asymmetric cooperation system that supports communication and cooperation between a VR user (occupant) and multiple non-VR users (designers) across the virtual and physical platform. The OmniGlobeVR allows designer(s) to access the content of a VR space from any point of view using two view modes: 360° first-person mode and third-person mode. Furthermore, a proper interface of a shared gaze awareness cue is designed to enhance communication between the occupant and the designer(s). The system also has a face window feature that allows designer(s) to share their facial expressions and upper body gesture with the occupant in order to exchange and express information in a nonverbal context. Combined together, the OmniGlobeVR allows collaborators between the VR and non-VR platforms to cooperate while allowing designer(s) to easily access physical assets while working synchronously with the occupant in the VR space.
SP  - 3382869
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382869
ER  - 

TY  - JOUR
AU  - Xu, Pengfei; Fu, Hongbo; Zheng, Youyi; Singh, Karan; Huang, Hui; Tai, Chiew-Lan
TI  - Model-Guided 3D Sketching
PY  - 2018
AB  - We present a novel 3D model-guided interface for in-situ sketching on 3D planes. Our work is motivated by evolutionary design, where existing 3D objects form the basis for conceptual re-design or further design exploration. We contribute a novel workflow that exploits the geometry of an underlying 3D model to infer 3D planes on which 2D strokes drawn that are on and around the 3D model should be meaningfully projected. This provides users with the nearly modeless fluidity of a sketching interface, and is particularly useful for 3D sketching over planes that are not easily accessible or do not preexist. We also provide an additional set of tools, including sketching with explicit plane selection and model-aware canvas manipulation. Our system is evaluated with a user study, showing that our technique is easy to learn and effective for rapid sketching of product design variations around existing 3D models.
SP  - 2927
EP  - 2939
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 10
PB  - 
DO  - 10.1109/tvcg.2018.2860016
ER  - 

TY  - NA
AU  - Chiton, Lydia B.; Petridis, Savvas; Agrawala, Maneesh
TI  - UIST (Adjunct Volume) - An Interactive Pipeline for Creating Visual Blends
PY  - 2018
AB  - Visual blends are an advanced graphic design technique to draw users' attention to a message. They blend together two objects in a way that is novel and useful in conveying a message symbolically. This demo presents an interactive pipeline for creating visual blends that follows the iterative design process. Our pipeline decomposes the process into both computational techniques and human microtasks. It allows users to collaboratively generate visual blends with steps involving brainstorming, synthesis, and iteration. Our demo allows individual users to see how existing visual blends were made, edit or improve existing visual blends, and create new visual blends.
SP  - 188
EP  - 190
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3271646
ER  - 

TY  - JOUR
AU  - Plopski, Alexander; Hirzle, Teresa; Norouzi, Nahal; Qian, Long; Bruder, Gerd; Langlotz, Tobias
TI  - The Eye in Extended Reality: A Survey on Gaze Interaction and Eye Tracking in Head-worn Extended Reality
PY  - 2022
AB  - <jats:p>With innovations in the field of gaze and eye tracking, a new concentration of research in the area of gaze-tracked systems and user interfaces has formed in the field of Extended Reality (XR). Eye trackers are being used to explore novel forms of spatial human–computer interaction, to understand human attention and behavior, and to test expectations and human responses. In this article, we review gaze interaction and eye tracking research related to XR that has been published since 1985, which includes a total of 215 publications. We outline efforts to apply eye gaze for direct interaction with virtual content and design of attentive interfaces that adapt the presented content based on eye gaze behavior and discuss how eye gaze has been utilized to improve collaboration in XR. We outline trends and novel directions and discuss representative high-impact papers in detail.</jats:p>
SP  - 1
EP  - 39
JF  - ACM Computing Surveys
VL  - 55
IS  - 3
PB  - 
DO  - 10.1145/3491207
ER  - 

TY  - JOUR
AU  - Wintersberger, Philipp; Schartmüller, Clemens; Sadeghian, Shadan; Frison, Anna-Katharina; Riener, Andreas
TI  - Evaluation of Imminent Take-Over Requests With Real Automation on a Test Track.
PY  - 2021
AB  - <AbstractText Label="OBJECTIVE" NlmCategory="OBJECTIVE">Investigating take-over, driving, non-driving related task (NDRT) performance, and trust of conditionally automated vehicles (AVs) in critical transitions on a test track.</AbstractText> <AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Most experimental results addressing driver take-over were obtained in simulators. The presented experiment aimed at validating relevant findings while uncovering potential effects of motion cues and real risk.</AbstractText> <AbstractText Label="METHOD" NlmCategory="METHODS">Twenty-two participants responded to four critical transitions on a test track. Non-driving related task modality (reading on a handheld device vs. auditory) and take-over timing (cognitive load) were varied on two levels. We evaluated take-over and NDRT performance as well as gaze behavior. Further, trust and workload were assessed with scales and interviews.</AbstractText> <AbstractText Label="RESULTS" NlmCategory="RESULTS">Reaction times were significantly faster than in simulator studies. Further, reaction times were only barely affected by varying visual, physical, or cognitive load. Post-take-over control was significantly degraded with the handheld device. Experiencing the system reduced participants' distrust, and distrusting participants monitored the system longer and more frequently. NDRTs on a handheld device resulted in more safety-critical situations.</AbstractText> <AbstractText Label="CONCLUSION" NlmCategory="CONCLUSIONS">The results confirm that take-over performance is mainly influenced by visual-cognitive load, while physical load did not significantly affect responses. Future take-over request (TOR) studies may investigate situation awareness and post-take-over control rather than reaction times only. Trust and distrust can be considered as different dimensions in AV research.</AbstractText> <AbstractText Label="APPLICATION" NlmCategory="CONCLUSIONS">Conditionally AVs should offer dedicated interfaces for NDRTs to provide an alternative to using nomadic devices. These interfaces should be designed in a way to maintain drivers' situation awareness.</AbstractText> <AbstractText Label="PR&#xC9;CIS" NlmCategory="UNASSIGNED">This paper presents a test track experiment addressing conditionally automated driving systems. Twenty-two participants responded to critical TORs, where we varied NDRT modality and take-over timing. In addition, we assessed trust and workload with standardized scales and interviews.</AbstractText>
SP  - 187208211051435
EP  - 001872082110514
JF  - Human factors
VL  - NA
IS  - NA
PB  - 
DO  - 10.1177/00187208211051435
ER  - 

TY  - NA
AU  - Fukusato, Tsukasa; Maejima, Akinobu
TI  - View-Dependent Formulation of 2.5D Cartoon Models.
PY  - 2021
AB  - 2.5D cartoon models are methods to simulate three-dimensional (3D)-like movements, such as out-of-plane rotation, from two-dimensional (2D) shapes in different views. However, cartoon objects and characters have several distorted parts which do not correspond to any real 3D positions (e.g., Mickey Mouse's ears), that implies that existing systems are not suitable for designing such representations. Hence, we formulate it as a view-dependent deformation (VDD) problem, which has been proposed in the field of 3D character animation. The distortions in an arbitrary viewpoint are automatically obtained by blending the user-specified 2D shapes of key views. This model is simple enough to easily implement in an existing animation system. Several examples demonstrate the robustness of our method over previous methods. In addition, we conduct a user study and confirm that the proposed system is effective for animating classic cartoon characters.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - del Molino, Javier; Bibiloni, Toni; Oliver, Antoni
TI  - Keys for successful 360° hypervideo design: A user study based on an xAPI analytics dashboard
PY  - 2020
AB  - One of the most recent trends in the evaluation of immersive virtual environments is the incorporation of user metrics. In this article, we conduct a user study on a 360° hypervideo, using a dashboard based on detailed metrics obtained from users’ interactions with 360° hypervideos. It is essential to evaluate the quality of experience to monitor service quality from the perspectives of consumers. We demonstrate a framework to examine the user experiences of 360° environments and evaluate them using the xAPI specification, facilitating the development of analytics solutions centered in the user experience; and how the graphs and related data composing the dashboard provide valuable information about ways of navigating and interacting with 360° video experiences, as well as the time invested in them. In the user study, we include the visual perception, attention, tracking and interaction of users watching Proemaid (a 360° multimedia production), collected from an interactive 360° video player in the form of xAPI statements. The Proemaid production has been played in a vast variety of contexts by stakeholders from government, technology and education, among others. Therefore, the quantitative results and the qualitative analysis of the user study are intended to outline a sketch of the users’ ways of navigating, interacting and investing time in 360° hypervideo productions. We consider that these metrics will be very interesting in the specification of new omnidirectional storyboards for film producers of content in 360°. Finally, we propose potential directions for empirical investigation that highlight its great potential in many fields.
SP  - 22771
EP  - 22796
JF  - Multimedia Tools and Applications
VL  - 79
IS  - 31
PB  - 
DO  - 10.1007/s11042-020-09059-2
ER  - 

TY  - JOUR
AU  - Olivier, Pauline; Chabrier, Renaud; Rohmer, Damien; de Thoisy, Eric; Cani, Marie-Paule
TI  - Nested Explorative Maps: A new 3D canvas for conceptual design in architecture
PY  - 2019
AB  - Abstract In this digital age, architects still need to alternate between paper sketches and 3D modeling software for their designs. Indeed, while 3D models enable to explore different views, creating them at very early stages might reduce creativity since they do not allow to superpose several tentative designs nor to refine them progressively, as sketches do. To enable exploratory design in 3D, we introduce Nested Explorative Maps, a new system dedicated to interactive design in architecture. Our model enables coarse to fine sketching of nested architectural structures, enabling to progressively sketch a 3D building from floor plan to interior design, thanks to a series of nested maps able to spread in 3D. Each map allows the visual representation of uncertainty as well as the interactive exploration of the alternative, tentative options. We validate the model through a user study conducted with professional architects, enabling us to highlight the potential of Nested Explorative Maps for conceptual design in architecture.
SP  - 203
EP  - 213
JF  - Computers & Graphics
VL  - 82
IS  - 82
PB  - 
DO  - 10.1016/j.cag.2019.05.027
ER  - 

TY  - JOUR
AU  - Prilla, Michael; Janssen, Marc; Kunzendorff, Timo
TI  - How to Interact with Augmented Reality Head Mounted Devices in Care Work? A Study Comparing Handheld Touch (Hands-on) and Gesture (Hands-free) Interaction
PY  - 2019
AB  - NA
SP  - 157
EP  - 178
JF  - AIS Transactions on Human-Computer Interaction
VL  - 11
IS  - 3
PB  - 
DO  - 10.17705/1thci.00118
ER  - 

TY  - NA
AU  - Kim, Bogyeong; Lee, Chaeeun; Huh, Jung; Lee, Woohun
TI  - CHI Extended Abstracts - Puppet Book: Digital Storybook with Back-of-Device Puppeteering Interface for Parent and Child
PY  - 2020
AB  - Puppet Book is a new concept of a digital storybook that is incorporated with puppetry. It enables parents to manipulate characters in real-time through a back-of-device interface while reading a storybook to their children. Puppet Book aims to provide enhanced expressiveness for parents and immersion for children. The Puppet Book interface was implemented carefully to minimize parents' task workload and maximize the expressiveness of puppeteering. A user study with 11 parent-child groups was conducted. Parents who easily adapted to the interface showed a higher motivation to tell the story, by identifying themselves with the characters. Children showed increased concentration and motivation for reading.
SP  - 1
EP  - 4
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383175
ER  - 

TY  - NA
AU  - Truong, Anh; Chen, Sara; Yumer, Ersin; Salesin, David; Li, Wilmot
TI  - CHI - Extracting Regular FOV Shots from 360 Event Footage
PY  - 2018
AB  - Video summaries are a popular way to share important events, but creating good summaries is hard. It requires expertise in both capturing and editing footage. While hiring a professional videographer is possible, this is too costly for most casual events. An alternative is to place 360 video cameras around an event space to capture footage passively and then extract regular field-of-view (RFOV) shots for the summary. This paper focuses on the problem of extracting such RFOV shots. Since we cannot actively control the cameras or the scene, it is hard to create "ideal' shots that adhere strictly to traditional cinematography rules. To better understand the tradeoffs, we study human preferences for static and moving camera RFOV shots generated from 360 footage. From the findings, we derive design guidelines. As a secondary contribution, we use these guidelines to develop automatic algorithms that we demonstrate in a prototype user interface for extracting RFOV shots from 360 videos.
SP  - 316
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173890
ER  - 

TY  - JOUR
AU  - Swaminathan, Saiganesh; Rivera, Michael L.; Kang, Runchang; Luo, Zheng; Ozutemiz, Kadri Bugra; Hudson, Scott E.
TI  - Input, Output and Construction Methods for Custom Fabrication of Room-Scale Deployable Pneumatic Structures
PY  - 2019
AB  - In this paper, we examine the future of designing room-scale deployable pneumatic structures that can be fabricated with interactive capabilities and thus be responsive to human input and environments. While there have been recent advances in fabrication methods for creating large-scale structures, they have mainly focused around creating passive structures. Hence in this work, we collectively tackle three main challenges that need to be solved for designing room-scale interactive deployable structures namely -- the input, output (actuation) and construction methods. First, we explore three types of sensing methods --- acoustic, capacitive and pressure --- in order to embed input into these structures. These sensing methods enable users to perform gestures such as knock, squeeze and swipe with specific parts of our fabricated structure such as doors, windows, etc. and make them interactive. Second, we explore three types of actuation mechanisms -- inflatable tendon drive, twisted tendon drive and roll bending actuator -- that are implemented at structural scale and can be embedded into our structures to enable a variety of responsive actuation. Finally, we provide a construction method to custom fabricate and assemble inter-connected pneumatic trusses with embedded sensing and actuation capability to prototype interactions with room-scale deployable structures. To further illustrate the collective (input, output and construction) usage of the system, we fabricated three exemplar interactive deployable structures -- a responsive canopy, an interactive geodesic dome and a portable table (Figures 1 and 2). These can be deployed from a compact deflated state to a much larger inflated state which takes on a desired form while offering interactivity.
SP  - 62
EP  - 17
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 2
PB  - 
DO  - 10.1145/3328933
ER  - 

TY  - NA
AU  - Wang, Tianyi; Qian, Xun; He, Fengming; Hu, Xiyun; Huo, Ke; Cao, Yuanzhi; Ramani, Karthik
TI  - UIST - CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications
PY  - 2020
AB  - Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.
SP  - 328
EP  - 341
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415815
ER  - 

TY  - NA
AU  - Su, Qingkun; Bai, Xue; Fu, Hongbo; Tai, Chiew-Lan; Wang, Jue
TI  - CHI - Live Sketch: Video-driven Dynamic Deformation of Static Drawings
PY  - 2018
AB  - Creating sketch animations using traditional tools requires special artistic skills, and is tedious even for trained professionals. To lower the barrier for creating sketch animations, we propose a new system, emphLive Sketch, which allows novice users to interactively bring static drawings to life by applying deformation-based animation effects that are extracted from video examples. Dynamic deformation is first extracted as a sparse set of moving control points from videos and then transferred to a static drawing. Our system addresses a few major technical challenges, such as motion extraction from video, video-to-sketch alignment, and many-to-one motion-driven sketch animation. While each of the sub-problems could be difficult to solve fully automatically, we present reliable solutions by combining new computational algorithms with intuitive user interactions. Our pilot study shows that our system allows both users with or without animation skills to easily add dynamic deformation to static drawings.
SP  - 662
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174236
ER  - 

TY  - NA
AU  - Becker, Vincent; Oldrati, Pietro; Barrios, Liliana; Sörös, Gábor
TI  - Touchsense: classifying finger touches and measuring their force with an electromyography armband
PY  - 2018
AB  - Identifying the finger used for touching and measuring the force of the touch provides valuable information on manual interactions. This information can be inferred from electromyography (EMG) of the forearm, measuring the activation of the muscles controlling the hand and fingers. We present Touch-Sense, which classifies the finger touches using a novel neural network architecture and estimates their force on a smartphone in real time based on data recorded from the sensors of an inexpensive and wireless EMG armband. Using data collected from 18 participants with force ground truth, we evaluate our system's performance and limitations. Our system could allow for new interaction paradigms with appliances and objects, which we exemplarily showcase in four applications.
SP  - 1
EP  - 8
JF  - Proceedings of the 2018 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267242.3267250
ER  - 

TY  - CHAP
AU  - Dube, Tafadzwa Joseph; Arif, Ahmed Sabbir
TI  - HCI (2) - Text Entry in Virtual Reality: A Comprehensive Review of the Literature
PY  - 2019
AB  - The availability of consumer-ready Virtual Reality (VR) Head-Mounted Displays (HMDs) has resulted in a surge in VR applications. It has also prompted the design and development of numerous text entry techniques for the paradigm. However, it is difficult to understand the mechanism of these techniques and extract meaningful average performance data from this body of work since they were evaluated in diverse experiment conditions and report different performance metrics. To remedy this, this paper classifies the existing text entry techniques for VR based on their input mechanism and discusses their strengths, limitations, and performance.
SP  - 419
EP  - 437
JF  - Human-Computer Interaction. Recognition and Interaction Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-22643-5_33
ER  - 

TY  - NA
AU  - Zhu, Junyi; Snowden, Jackson C.; Verdejo, Joshua; Chen, Emily; Zhang, Paul; Ghaednia, Hamid; Schwab, Joseph H.; Mueller, Stefanie
TI  - UIST (Adjunct Volume) - EIT-kit Demo: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing
PY  - 2021
AB  - In this paper, we propose EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1) an extension to a 3D editor for personalizing the form factor of electrode arrays and electrode distribution, (2) a customized EIT sensing motherboard for performing the measurements, (3) a microcontroller library that automates signal calibration and facilitates data collection, and (4) an image reconstruction library for mobile devices for interpolating and visualizing the measured data. Together, these EIT-kit components allow for applications that require 2- or 4-terminal setups, up to 64 electrodes, and single or multiple (up to four) electrode arrays simultaneously. We motivate the design of each component of EIT-kit with a formative study, and conduct a technical evaluation of the data fidelity of our EIT measurements. We demonstrate the design space that EIT-kit enables by showing various applications in health as well as motion sensing and control.
SP  - 100
EP  - 102
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474758
ER  - 

TY  - NA
AU  - Vazquez, Christian; Tan, Nicole; Sadalgi, Shrenik
TI  - SIGGRAPH Immersive Pavilion - Home Studio: DIY Interior Design in Mixed Reality
PY  - 2021
AB  - Virtual staging of real estate listings increases the appeal of a property by letting prospective buyers envision a living space remotely. However, existing tools employed to stage homes limit the scale of the visualization to a set of fixed images provided by customers or require 3D artist expertise to reconstruct the space. The adoption of 3D Matterport scans has accelerated due to the Covid-19 pandemic as a means to enable virtual tours and adhere to social distancing guidelines. We present Home Studio, a virtual staging tool that empowers non-experts, letting them furnish any Matterport scene and create photo-realistic renders in a matter of minutes. Our tool lets customers dive into their designs using a virtual reality headset to assess the final product in an immersive experience.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2021 Immersive Pavilion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450615.3464528
ER  - 

TY  - NA
AU  - Xia, Haijun; Herscher, Sebastian; Perlin, Ken; Wigdor, Daniel
TI  - UIST - Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality
PY  - 2018
AB  - Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users' agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.
SP  - 853
EP  - 866
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242597
ER  - 

TY  - CHAP
AU  - Fanini, Bruno; Pagano, Alfonsina; Pietroni, Eva; Ferdani, Daniele; Demetrescu, Emanuel; Palombini, Augusto
TI  - Augmented Reality for Cultural Heritage
PY  - 2021
AB  - NA
SP  - 391
EP  - 411
JF  - Springer Handbooks
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-67822-7_16
ER  - 

TY  - NA
AU  - Lee, David T.; Hamedian, Emily S.; Wolff, Greg; Liu, Amy
TI  - CHI - Causeway: Scaling Situated Learning with Micro-Role Hierarchies
PY  - 2019
AB  - While educational technologies such as MOOCs have helped scale content-based learning, scaling situated learning is still challenging. The time it takes to define a real-world project and to mentor learners is often prohibitive, especially given the limited contributions that novices are able to make. This paper introduces micro-role hierarchies, a form of coordination that integrates workflows and hierarchies to help short-term novices predictably contribute to complex projects. Individuals contribute through micro-roles, small experiential assignments taking roughly 2 hours. These micro-roles support execution of the desired work process, but also sequence into learning pathways, resulting in a learning dynamic similar to moving up an organizational hierarchy. We demonstrate micro-role hierarchies through Causeway, a platform for learning web development while building websites for nonprofits. We carry out a proof-of-concept study in which learners built static websites for refugee resettlement agencies in 2 hour long roles.
SP  - 74
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300304
ER  - 

TY  - CHAP
AU  - Rothe, Sylvia; Montagud, Mario; Mai, Christian; Buschek, Daniel; Hußmann, Heinrich
TI  - ICIDS - Social Viewing in Cinematic Virtual Reality: Challenges and Opportunities
PY  - 2018
AB  - Cinematic Virtual Reality (CVR) has been increasing in popularity in the last years. However, viewers can feel isolated when watching 360° movies with a Head-Mounted Display. Since watching movies is a social experience for most people, we investigate if the use of Head Mounted Displays is appropriate for enabling shared CVR experiences. In this context, even if viewers are watching the movie simultaneously, they do not automatically see the same field of view, since they can freely choose the viewing direction. Based on the literature and experiences from past user studies, we identify seven challenges. To address these challenges, we present and discuss design ideas for a CVR social movie player and highlight directions for future work.
SP  - 338
EP  - 342
JF  - Interactive Storytelling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-04028-4_37
ER  - 

TY  - NA
AU  - Capallera, Marine; Meteier, Quentin; de Salis, Emmanuel; Angelini, Leonardo; Carrino, Stefano; Khaled, Omar Abou; Mugellini, Elena
TI  - Secondary task and situation awareness, a mobile application for semi-autonomous vehicle
PY  - 2019
AB  - Autonomous vehicles are developing rapidly and will lead to a significant change in the driver’s role: he/she will have to move from the role of actor to the role of supervisor. Indeed, the driver will soon be able to perform a secondary task but he/she must be able to take over control in the event of a critical situation that is not managed by the autonomous system. This implies that the role of new interfaces and interactions within the vehicle is important to take into account. This article describes the design of an application that provides the driver with information about the environment perceived by his/her vehicle in the form of modules. This application is displayed as split screen on a tablet by which a secondary task can be performed. Initial tests were carried out with this application in a driving simulator. They made it possible to test the acceptance of the application and the clarity of the information transmitted. The results generally showed that the participants correctly identified some of the factors limiting the proper functioning of the autonomous pilot while performing a secondary task on a tablet.Les véhicules autonomes se développent rapidement et entraîneront un changement de rôle important chez le conducteur: ce dernier sera amené à passer du rôle d'acteur à celui de superviseur. En effet, le conducteur sera bientôt en mesure d'effectuer une tâche secondaire mais devra toutefois être capable de reprendre le contrôle dans le cas d'une situation critique non gérée par le système autonome. Ceci implique que le rôle des nouvelles interfaces et interactions au sein du véhicule est important à prendre en compte. Cet article décrit la conception d'une application transmettant au conducteur des informations relatives à l'environnement perçu par son véhicule sous forme de modules. Cette application s'affiche en partage d'écran sur une tablette grâce à laquelle une tâche secondaire peut être effectuée.de premiers tests ont été effectués avec cette application dans un simulateur de conduite. Ils ont permis de tester l'acceptation de l'application et la clarté des informations transmises. Les résultats ont globalement montré que les participants ont correctement identifié certains facteurs limitant le bon fonctionnement du pilote autonome tout en réalisant une tâche secondaire sur tablette
SP  - NA
EP  - NA
JF  - Proceedings of the 31st Conference on l'Interaction Homme-Machine
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3366550.3372258
ER  - 

TY  - JOUR
AU  - Paczkowski, Patrick; Dorsey, Julie; Rushmeier, Holly; Kim, Min H.
TI  - PaperCraft3D: Paper-Based 3D Modeling and Scene Fabrication
PY  - 2018
AB  - A 3D modeling system with all-inclusive functionality is too demanding for a casual 3D modeler to learn. There has been a shift towards more approachable systems, with easy-to-learn, intuitive interfaces. However, most modeling systems still employ mouse and keyboard interfaces, despite the ubiquity of tablet devices and the benefits of multi-touch interfaces. We introduce an alternative 3D modeling and fabrication paradigm using developable surfaces, inspired by traditional papercrafting, and we implement it as a complete system designed for a multi-touch tablet, allowing a user to fabricate 3D scenes. We demonstrate the modeling and fabrication process of assembling complex 3D scenes from a collection of simpler models, in turn shaped through operations applied to virtual paper. Our fabrication method facilitates the assembly of the scene with real paper by automatically converting scenes into a series of cutouts with appropriately added fiducial markers and supporting structures. Our system assists users in creating occluded supporting structures to help maintain the spatial and rigid properties of a scene without compromising its aesthetic qualities. We demonstrate several 3D scenes modeled and fabricated in our system, and evaluate the faithfulness of our fabrications relative to their virtual counterparts and 3D-printed fabrications.
SP  - 1717
EP  - 1731
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 4
PB  - 
DO  - 10.1109/tvcg.2018.2820068
ER  - 

TY  - BOOK
AU  - Corno, Fulvio; De Russis, Luigi; Saenz, Juan Pablo
TI  - EICS - On Computational Notebooks to Empower Physical Computing Novices
PY  - 2021
AB  - The ever-increasing availability and variety of resources to create physical computing systems keep attracting electronics hobbyists and do-it-yourself enthusiasts. Nevertheless, the prototyping and development of these systems are still challenging to the novices. In this paper, we propose a tool (built on top of the Jupyter computational notebook) as a way for supporting step-by-step assisted learning and knowledge sharing. We extended the Jupyter notebook functionalities and implemented a custom-tailored kernel to seamlessly enable the interaction between the end-user web interface and the Arduino boards. We consider that this approach can effectively support physical computing novices in understanding, writing, and executing the code while empowering them to document and share the development steps they followed.
SP  - 22
EP  - 25
JF  - Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3459926.3464752
ER  - 

TY  - CHAP
AU  - Liu, Chuanyi; Zhang, Jiali; Ma, Kang
TI  - ICIC (3) - Natural and Fluid 3D Operations with Multiple Input Channels of a Digital Pen
PY  - 2018
AB  - We propose six 3D operation patterns with multiple input channels of a digital pen: these patterns allow users to transfer pre-existing knowledge of physical pens to digital pens on performing 3D operations simply, naturally, intuitively, and fluidly. A prototype system was designed under the patterns and implemented. An informal user study showed that eight novices grasped to perform 3D operations with the prototype system within several minutes and gained more fun than with the typical interfaces.
SP  - 585
EP  - 598
JF  - Intelligent Computing Methodologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-95957-3_61
ER  - 

TY  - NA
AU  - Chae, Han Joo; Hwang, Jeong-in; Seo, Jinwook
TI  - UIST - Wall-based Space Manipulation Technique for Efficient Placement of Distant Objects in Augmented Reality
PY  - 2018
AB  - We present a wall-based space manipulation (WSM) technique that enables users to efficiently select and move distant objects by dynamically squeezing their surrounding space in augmented reality. Users can bring a target object closer by dragging a solid plane behind the object and squeezing the space between them and the plane so that they can select and move the object more delicately and efficiently. We furthermore discuss the unique design challenges of WSM, including the dimension of space reduction and the recognition of the reduced space in relation to the real space. We conducted a user evaluation to verify how WSM improves the performance of the hand-centered object manipulation technique on the HoloLens for moving near objects far away and vice versa. The results indicate that WSM overall performed consistently well and significantly improved efficiency while alleviating arm fatigue.
SP  - 45
EP  - 52
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242631
ER  - 

TY  - JOUR
AU  - Rothe, Sylvia; Schmidt, Alexander; Montagud, Mario; Buschek, Daniel; Hußmann, Heinrich
TI  - Social viewing in cinematic virtual reality: a design space for social movie applications
PY  - 2020
AB  - Since watching movies is a social experience for most people, it is important to know how an application should be designed for enabling shared cinematic virtual reality (CVR) experiences via head-mounted displays (HMDs). Viewers can feel isolated when watching omnidirectional movies with HMDs. Even if they are watching the movie simultaneously, they do not automatically see the same field of view, since they can freely choose their viewing direction. Our goal is to explore interaction techniques to efficiently support social viewing and to improve social movie experiences in CVR. Based on the literature review and insights from earlier work, we identify seven challenges that need to be addressed: communication, field-of-view (FoV) awareness, togetherness, accessibility, interaction techniques, synchronization, and multiuser environments. We investigate four aspects (voice chat, sending emotion states, FoV indication, and video chat) to address some of the challenges and report the results of four user studies. Finally, we present and discuss a design space for CVR social movie applications and highlight directions for future work.
SP  - 613
EP  - 630
JF  - Virtual Reality
VL  - 25
IS  - 3
PB  - 
DO  - 10.1007/s10055-020-00472-4
ER  - 

TY  - JOUR
AU  - Khadpe, Pranav; Kulkarni, Chinmay; Kaufman, Geoff
TI  - Empathosphere: Promoting Constructive Communication in Ad-hoc Virtual Teams through Perspective-taking Spaces
PY  - 2022
AB  - <jats:p>When members of ad-hoc virtual teams need to collectively ideate or deliberate, they often fail to engage with each others' perspectives in a constructive manner. At best, this leads to sub-optimal outcomes, and, at worst, it can cause conflicts that lead to teams not wanting to continue working together. Prior work has attempted to facilitate constructive communication by highlighting problematic communication patterns and nudging teams to alter their interaction norms. However, these approaches achieve limited success because they fail to acknowledge two social barriers: (1) it is hard to reset team norms mid-interaction, and (2) corrective nudges have limited utility unless team members believe it is safe to voice their opinion and that their opinion will be heard. This paper introduces Empathosphere, a chat-embedded intervention to mitigate these barriers and foster constructive communication in teams. To mitigate the first barrier, Empathosphere leverages the known benefits of "experimental spaces" in dampening existing norms and creating a climate conducive to change. Empathosphere instantiates this "space'' as a separate communication channel in a team's workspace. To mitigate the second barrier, Empathosphere harnesses the benefits of perspective-taking to cultivate a group climate that promotes a norm of members speaking up and engaging with each other. Empathosphere achieves this by orchestrating authentic socio-emotional exchanges designed to induce perspective-taking. A controlled study ($N=110$) compared Empathosphere to an alternate intervention strategy of prompting teams to reflect on their team experience. We found that Empathosphere led to higher work satisfaction, encouraged more open communication and feedback within teams, and boosted teams' desire to continue working together. This work demonstrates that "experimental spaces," particularly those that integrate methods of encouraging perspective-taking, can be a powerful means of improving communication in virtual teams.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512902
ER  - 

TY  - JOUR
AU  - Badam, Sriram Karthik; Mathisen, Andreas; Rädle, Roman; Klokmose, Clemens Nylandsted; Elmqvist, Niklas
TI  - Vistrates: A Component Model for Ubiquitous Analytics
PY  - 2018
AB  - Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components—the building blocks of this model—can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce V istrates , a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic “anytime” and “anywhere” motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.
SP  - 586
EP  - 596
JF  - IEEE transactions on visualization and computer graphics
VL  - 25
IS  - 1
PB  - 
DO  - 10.1109/tvcg.2018.2865144
ER  - 

TY  - NA
AU  - Yung-Ta, Lin; Liao, Yi-Chi; Teng, Shan-Yuan; Chung, Yi-Ju; Chan, Liwei; Chen, Bing-Yu
TI  - UIST - Outside-In: Visualizing Out-of-Sight Regions-of-Interest in a 360° Video Using Spatial Picture-in-Picture Previews
PY  - 2017
AB  - 360-degree video contains a full field of environmental content. However, browsing these videos, either on screens or through head-mounted displays (HMDs), users consume only a subset of the full field of view per a natural viewing experience. This causes a search problem when a region-of-interest (ROI) in a video is outside of the current field of view (FOV) on the screen, or users may search for non-existing ROIs. We propose Outside-In, a visualization technique which re-introduces off-screen regions-of-interest (ROIs) into the main screen as spatial picture-in-picture (PIP) previews. The geometry of the preview windows further encodes a ROI's relative location vis-a-vis the main screen view, allowing for effective navigation. In an 18-participant study, we compare Outside-In with traditional arrow-based guidance within three types of 360-degree video. Results show that Outside-In outperforms in regard to understanding spatial relationship, the storyline of the content and overall preference. Two applications are demonstrated for use with Outside-In in 360-degree video navigation with touchscreens, and live telepresence.
SP  - 255
EP  - 265
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126656
ER  - 

TY  - NA
AU  - Zubaidi-Polli, Anna Maria Al; Anderst-Kotsis, Gabriele
TI  - iiWAS - Conceptual Design of a hybrid Participatory IT supporting in-situ and ex-situ collaborative text authoring
PY  - 2018
AB  - This paper reports on two design experiments conducted in a semi-public space: the first enabled collaborative writing in situ via mobile phones and the second provided additional ex-situ access through a web-based platform. Our analysis revealed that (i) in-situ engagement through mobile phones is a trigger that supports participation and (ii) ex-situ action via web-applications increases engagement in a collaborative writing process linked to a semi-public space. Hence, we propose a conceptual design for a hybrid Participatory Information Technology (PIT) that combines these two technologies. With the hybrid PIT we address one of the most common critiques in the context of today's semi-public leisure spaces -- namely that visitors lack support in gaining both a stronger connection to the place and a sense of ownership of related activities and their outcomes. Our conceptual design gives participants continuous access to the process of collaborative writing from locations of their preference (i.e., either collocated in a semi-public space or remote) and allows them to author a text both synchronously and asynchronously.
SP  - 243
EP  - 252
JF  - Proceedings of the 20th International Conference on Information Integration and Web-based Applications & Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3282373.3282383
ER  - 

TY  - JOUR
AU  - Wang, April Yi; Chen, Yan; Chung, John Joon Young; Brooks, Christopher; Oney, Steve
TI  - PuzzleMe: Leveraging Peer Assessment for In-Class Programming Exercises
PY  - 2021
AB  - Peer assessment, as a form of collaborative learning, can engage students in active learning and improve their learning gains. However, current teaching platforms and programming environments provide little support to integrate peer assessment for in-class programming exercises. We identified challenges in conducting such exercises and adopting peer assessment through formative interviews with instructors of introductory programming courses. To address these challenges, we introduce PuzzleMe, a tool to help Computer Science instructors to conduct engaging in-class programming exercises. PuzzleMe leverages peer assessment to support a collaboration model where students provide timely feedback on their peers' work. We propose two assessment techniques tailored to in-class programming exercises: live peer testing and live peer code review. Live peer testing can improve students' code robustness by allowing them to create and share lightweight tests with peers. Live peer code review can improve code understanding by intelligently grouping students to maximize meaningful code reviews. A two-week deployment study revealed that PuzzleMe encourages students to write useful test cases, identify code problems, correct misunderstandings, and learn a diverse set of problem-solving approaches from peers.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW2
PB  - 
DO  - 10.1145/3479559
ER  - 

TY  - NA
AU  - Park, Keunwoo; Kim, Sunbum; Yoon, Youngwoo; Kim, Tae-Kyun; Lee, Geehyuk
TI  - UIST - DeepFisheye: Near-Surface Multi-Finger Tracking Technology Using Fisheye Camera
PY  - 2020
AB  - Near-surface multi-finger tracking (NMFT) technology expands the input space of touchscreens by enabling novel interactions such as mid-air and finger-aware interactions. We present DeepFisheye, a practical NMFT solution for mobile devices, that utilizes a fisheye camera attached at the bottom of a touchscreen. DeepFisheye acquires the image of an interacting hand positioned above the touchscreen using the camera and employs deep learning to estimate the 3D position of each fingertip. We created two new hand pose datasets comprising fisheye images, on which our network was trained. We evaluated DeepFisheye's performance for three device sizes. DeepFisheye showed average errors with approximate value of 20 mm for fingertip tracking across the different device sizes. Additionally, we created simple rule-based classifiers that estimate the contact finger and hand posture from DeepFisheye's output. The contact finger and hand posture classifiers showed accuracy of approximately 83 and 90%, respectively, across the device sizes.
SP  - 1132
EP  - 1146
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415818
ER  - 

TY  - JOUR
AU  - Vanhulst, Pierre; Tuor, Raphaël; Evéquoz, Florian; Lalanne, Denis
TI  - Colvis—A Structured Annotation Acquisition System for Data Visualization
PY  - 2021
AB  - Annotations produced by analysts during the exploration of a data visualization are a precious source of knowledge. Harnessing this knowledge requires a thorough structure of annotations, but also a means to acquire them without harming user engagement. The main contribution of this article is a method, taking the form of an interface, that offers a comprehensive “subject-verb-complement” set of steps for analysts to take annotations, and seamlessly translate these annotations within a prior classification framework. Technical considerations are also an integral part of this study: through a concrete web implementation, we prove the feasibility of our method, but also highlight some of the unresolved challenges that remain to be addressed. After explaining all concepts related to our work, from a literature review to JSON Specifications, we follow by showing two use cases that illustrate how the interface can work in concrete situations. We conclude with a substantial discussion of the limitations, the current state of the method and the upcoming steps for this annotation interface.
SP  - 158
EP  - NA
JF  - Information
VL  - 12
IS  - 4
PB  - 
DO  - 10.3390/info12040158
ER  - 

TY  - NA
AU  - Fremerey, Stephan; Singla, Ashutosh; Meseberg, Kay; Raake, Alexander
TI  - MMSys - AVtrack360: an open dataset and software recording people's head rotations watching 360° videos on an HMD
PY  - 2018
AB  - In this paper, we present a viewing test with 48 subjects watching 20 different entertaining omnidirectional videos on an HTC Vive Head Mounted Display (HMD) in a task-free scenario. While the subjects were watching the contents, we recorded their head movements. The obtained dataset is publicly available in addition to the links and timestamps of the source contents used. Within this study, subjects were also asked to fill in the Simulator Sickness Questionnaire (SSQ) after every viewing session. Within this paper, at first SSQ results are presented. Several methods for evaluating head rotation data are presented and discussed. In the course of the study, the collected dataset is published along with the scripts for evaluating the head rotation data. The paper presents the general angular ranges of the subjects' exploration behavior as well as an analysis of the areas where most of the time was spent. The collected information can be presented as head-saliency maps, too. In case of videos, head-saliency data can be used for training saliency models, as information for evaluating decisions during content creation, or as part of streaming solutions for region-of-interest-specific coding as with the latest tile-based streaming solutions, as discussed also in standardization bodies such as MPEG.
SP  - 403
EP  - 408
JF  - Proceedings of the 9th ACM Multimedia Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204949.3208134
ER  - 

TY  - JOUR
AU  - Li, Jianping Kelvin; Ma, Kwan-Liu
TI  - P6: A Declarative Language for Integrating Machine Learning in Visual Analytics
PY  - 2021
AB  - We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.
SP  - 380
EP  - 389
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3030453
ER  - 

TY  - CHAP
AU  - Borowski, Marcel; Larsen-Ledet, Ida
TI  - IS-EUD - Lessons Learned from Using Reprogrammable Prototypes with End-User Developers.
PY  - 2021
AB  - Involving end-users in the development of a product before it is deployed has great potential to increase the fit between a product and individual users’ needs. While end-users can be directly involved in modifying low-fidelity prototypes, they are left out when it comes to high-fidelity interactive prototypes—in part because these cannot be modified directly or require time-consuming edit-compile-run cycles. High-fidelity prototypes, however, are more engaging for users. We created a reprogrammable high-fidelity prototype and explored its use in short-term prototyping workshops with end-user developers, i.e. end-users with programming experience, in the domain of collaborative writing. We report observations and pitfalls, and distill four lessons learned into guidelines on how to use reprogrammable high-fidelity prototypes with end-users in contexts with limited resources. Our experiences demonstrate, among other things, that reprogrammable high-fidelity prototypes are difficult to work with—even for experienced programmers—and emphasize the need for careful attention to guiding participants, time for familiarization, and catering to multiple levels of programming experience.
SP  - 136
EP  - 152
JF  - End-User Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-79840-6_9
ER  - 

TY  - NA
AU  - Oh, Seungjae; Park, Chaeyong; Jeon, Yo-Seb; Choi, Seungmoon
TI  - UIST - Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication
PY  - 2021
AB  - As computing paradigms shift toward mobile and ubiquitous interaction, there is an increasing demand for wearable interfaces supporting multifaceted input in smart living environments. In this regard, we introduce a system that identifies contact fingers using vibration as a modality of communication. We investigate the vibration characteristics of the communication channels involved and simulate the transmission of vibration sequences. In the simulation, we test and refine modulation and demodulation methods to design vibratory communication protocols that are robust to environmental noises and can detect multiple simultaneous contact fingers. As a result, we encode an on-off keying sequence with a unique carrier frequency to each finger and demodulate the sequences by applying cross-correlation. We verify the communication protocols in two environments, laboratory and cafe, where the resulting highest accuracy was 93 % and 90.5 %, respectively. Our system achieves over 91 % accuracy in identifying seven contact states from three fingers while wearing only two actuator rings with the aid of a touch screen. Our findings shed light on diversifying touch interactions on rigid surfaces by means of vibratory communication.
SP  - 208
EP  - 222
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474745
ER  - 

TY  - NA
AU  - Qian, Xun; He, Fengming; Hu, Xiyun; Wang, Tianyi; Ipsita, Ananya; Ramani, Karthik
TI  - ScalAR: Authoring Semantically Adaptive Augmented Reality Experiences in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517665
ER  - 

TY  - CHAP
AU  - Lim, Dokshin; Lee, Jihoon; Kim, Sung Mahn
TI  - HCI (32) - User Perception and the Effect of Forms and Movements in Human-Machine Interaction Applying Steer-By-Wire for Autonomous Vehicles
PY  - 2020
AB  - As a result of the increasing trend towards highly autonomous driving, steer-by-wire (SbW) is currently experiencing a further development surge in terms of design as well as engineering. Our work explores the perception of human-machine interaction (HMI) by examining how people respond to the design and behavior of new SbW systems when asked to evaluate how much they are innovative, futuristic, acceptable, human-like and convenient depending on forms, sizes and movements. We develop eight video files to measure people’s perception and analyze the relationship between people’s perception and their profiles such as their age group, gender, car ownership and their background (either they are designers or engineers). There is no significant difference of people’s emotion by size and speed of movement based upon design alternatives we proposed. People perceived differently by shapes. Our results show that people find two shapes (circle type vs. bar type) significantly different in terms of traditional vs. futuristic, unacceptable vs. acceptable and inconvenient vs. convenient. We also find that younger group (20’s) is less sensitive to bar type’s acceptability than people in 30–40’s. Men compared to women and engineers compared to designers find bar type less convenient than circle type. It is possible to infer that new SbW systems are likely to appeal to younger people who are in 20’s and do not own a car yet, women or designers than the other groups respectively.
SP  - 58
EP  - 77
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50523-3_5
ER  - 

TY  - NA
AU  - Queguiner, Glen; Fradet, Matthieu; Rouhani, Mohammad Hossein
TI  - ISMAR Adjunct - Towards Mobile Diminished Reality
PY  - 2018
AB  - We present a diminished reality application running live on consumer mobile devices. In our pre-observation-based approach, the clean 3D scene, free of undesired objects, is scanned beforehand and reconstructed as a high resolution textured 3D model. At runtime, objects added in a region of interest are efficiently removed by projecting the previously captured background. Differences of illumination conditions between scan time and run-time are compensated to obtain seamless results. The proposed approach requires no segmentation or manual input other than the definition of the 3D region of interest to be diminished, and is not based on any particular assumption on the background geometry. We show the potential of our approach by processing a variety of challenging unknown 3D scenes including textured backgrounds, dynamic illumination conditions and foreground objects partially occluding the diminished region. We provide details on our compute shader implementation to make as easy as possible the reimplementation by the community.
SP  - 226
EP  - 231
JF  - 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct.2018.00073
ER  - 

TY  - NA
AU  - Abdelwahed, Mehdi; Pitti, Alexandre; Romain, Olivier; Ouezdou, Fethi Ben
TI  - ICARM - Use of Multi-frequency Electrical Impedance Tomography as Tactile Sensor for Material Discrimination
PY  - 2020
AB  - Electrical Impedance Tomography (EIT) is an imaging technique used recently as a tactile sensor. The advantages are the absence of electrodes within the sensor area, low-cost design and material. Moreover, it also includes low electric consumption and it can be shaped freely. Although EIT reconstruction gives a low spatial resolution, the retrieved impedance provides other information. Usually, frequencies in EIT are chosen to match specific impedance material. This paper proposes a new approach to retrieve the contact type as a new modality for tactile sensors. We propose to use Multi-Frequency Analysis (MFA) to retrieve the contact type. Our device uses the frequency range from 1k to 200k Hz. The results show the ability to classify 4 different types of material using clustering algorithms from raw data. This method is also able to differentiate hands from different people. MFA shows the use of multi-frequencies allows discriminating material. Furthermore, within one type of material, sight differences are also detectable. This new approach could lead to a cheap multi-modal sensor and allows better robotics arms manipulation.
SP  - 588
EP  - 594
JF  - 2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icarm49381.2020.9195347
ER  - 

TY  - CHAP
AU  - Song, Xue Ting; Kuo, Jo-Yu; Chen, Chun-Hsien
TI  - Design methodologies for conventional and additive manufacturing
PY  - 2022
AB  - NA
SP  - 97
EP  - 143
JF  - Digital Manufacturing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1016/b978-0-323-95062-6.00007-3
ER  - 

TY  - JOUR
AU  - Guo, Anhong; Canberk, Ilter; Murphy, Hannah; Monroy-Hernández, Andrés; Vaish, Rajan
TI  - Blocks: Collaborative and Persistent Augmented Reality Experiences.
PY  - 2019
AB  - We introduce Blocks, a mobile application that enables people to co-create AR structures that persist in the physical environment. Using Blocks, end users can collaborate synchronously or asynchronously, whether they are colocated or remote. Additionally, the AR structures can be tied to a physical location or can be accessed from anywhere. We evaluated how people used Blocks through a series of lab and field deployment studies with over 160 participants, and explored the interplay between two collaborative dimensions: space and time. We found that participants preferred creating structures synchronously with colocated collaborators. Additionally, they were most active when they created structures that were not restricted by time or place. Unlike most of today's AR experiences, which focus on content consumption, this work outlines new design opportunities for persistent and collaborative AR experiences that empower anyone to collaborate and create AR content.
SP  - 83
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 3
IS  - 3
PB  - 
DO  - 10.1145/3351241
ER  - 

TY  - NA
AU  - Lin, Richard; Ramesh, Rohit; Chi, Connie; Jain, Nikhil; Dutta, Prabal; Hartmann, Björn
TI  - CHI Extended Abstracts - Supporting Circuit Design with a Block-Based, Generator Language
PY  - 2020
AB  - Modern electronic design automation (EDA) tooling tends to focus on either the system-level design or the low-level electrical connectivity between physical components on a printed circuit board (PCB). We believe that a usable and functional system for circuit design needs to be able to interleave both levels of abstraction seamlessly and allow designers to transition between them freely. Existing work has experimented with approaches like circuit synthesis, functional characterization, or fine grained physical modeling. Each of these approaches augment the design process as it exists today, with its fundamental split between various levels of abstraction. We notice that hierarchical block diagrams can capture both high-level system structure as well as fine grained physical connectivity, and use that symmetry to construct a model for electronic circuits that can span the entire design process. Additionally, we construct user interfaces for our model that can support users of different skill levels throughout a design task. We discuss the design of our system, detailing both fundamental abstractions and usability trade-offs, and demonstrate its current capabilities through the design of example electronics projects.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382887
ER  - 

TY  - NA
AU  - Hofmann, Megan; Mankoff, Jennifer; Hudson, Scott E.
TI  - UIST - KnitGIST: A Programming Synthesis Toolkit for Generating Functional Machine-Knitting Textures
PY  - 2020
AB  - Automatic knitting machines are robust, digital fabrication devices that enable rapid and reliable production of attractive, functional objects by combining stitches to produce unique physical properties. However, no existing design tools support optimization for desirable physical and aesthetic knitted properties. We present KnitGIST (Generative Instantiation Synthesis Toolkit for knitting), a program synthesis pipeline and library for generating hand- and machine-knitting patterns by intuitively mapping objectives to tactics for texture design. KnitGIST generates a machine-knittable program in a domain-specific programming language.
SP  - 1234
EP  - 1247
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415590
ER  - 

TY  - JOUR
AU  - Merino, Mauricio Verano; Vinju, Jurgen; van der Storm, Tijs
TI  - Bacatá : Notebooks for DSLs, Almost for Free
PY  - 2020
AB  - Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools used to interact with general purpose languages in the context of DSLs. Computational notebooks are an example of such tools. Then, our main question is: What is an efficient and effective method of designing and implementing notebook interfaces for DSLs? By addressing this question we might be able to speed up the development of DSL tools, and ease the interaction between end-users and DSLs. Approach: In this paper, we present Bacat\'a, a mechanism for generating notebook interfaces for DSLs in a language parametric fashion. We designed this mechanism in a way in which language engineers can reuse as many language components (e.g., language processors, type checkers, code generators) as possible. Knowledge: Our results show that notebook interfaces generated by Bacat\'a can be automatically generated with little manual configuration. There are few considerations and caveats that should be addressed by language engineers that rely on language design aspects. The creation of a notebook for a DSL with Bacat\'a becomes a matter of writing the code that wires existing language components in the Rascal language workbench with the Jupyter platform. Grounding: We evaluate Bacat\'a by generating functional computational notebook interfaces for three different non-trivial DSLs, namely: a small subset of Halide (a DSL for digital image processing), SweeterJS (an extended version of JavaScript), and QL (a DSL for questionnaires). Additionally, it is relevant to generate notebook implementations rather than implementing them manually. We measured and compared the number of Source Lines of Code (SLOCs) that we reused from existing implementations of those languages. Importance: The adoption of notebooks by novice-programmers and end-users has made them very popular in several domains such as exploratory programming, data science, data journalism, and machine learning. Why are they popular? In (data) science, it is essential to make results reproducible as well as understandable. However, notebooks are only available for GPLs. This paper opens up the notebook metaphor for DSLs to improve the end-user experience when interacting with code and to increase DSLs adoption.
SP  - 11
EP  - NA
JF  - The Art, Science, and Engineering of Programming
VL  - 4
IS  - 3
PB  - 
DO  - 10.22152/programming-journal.org/2020/4/11
ER  - 

TY  - NA
AU  - Borowski, Marcel; Rädle, Roman; Klokmose, Clemens Nylandsted
TI  - CHI Extended Abstracts - Codestrate Packages: An Alternative to "One-Size-Fits-All" Software
PY  - 2018
AB  - We present Codestrate Packages, a package-based system to create extensible software within Codestrates. Codestrate Packages turns content creation from an application-centric model into a document-centric model. Codestrate Packages no longer restrict users to the feature set of the application. Instead packages allow users to add new features to their documents while already working on them. They can match the features to their current task at hand. Supporting the reprogrammable nature of Codestrates, new features can also be implemented by users themselves and shared with other people without having to leave the document. We illustrate the application of Codestrate Packages in an example scenario and present its technical concepts. We plan to conduct multiple user studies to investigate the benefits and barriers of Codestrate Packages' document-centric approach.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188563
ER  - 

TY  - NA
AU  - Hong, Yang; MacQuarrie, Andrew; Steed, Anthony
TI  - VRST - The effect of chair type on users' viewing experience for 360-degree video
PY  - 2018
AB  - The consumption of 360-degree videos with head-mounted displays (HMDs) is increasing rapidly. A large number of HMD users watch 360-degree videos at home, often on non-swivel seats; however videos are frequently designed to require the user to turn around. This work explores how the difference in users' chair type might influence their viewing experience. A between-subject experiment was conducted with 41 participants. Three chair conditions were used: fixed, half-swivel and full-swivel. A variety of measures were explored using eye-tracking, questionnaires, tasks and semi-structured interviews. Results suggest that the fixed and half-swivel chairs discouraged exploration for certain videos compared with the full-swivel chair. Additionally, participants in the fixed chair had worse spatial awareness and greater concern about missing something for certain video than those in the full-swivel chair. No significant differences were found in terms of incidental memory, general engagement and simulator sickness among the three chair conditions. Furthermore, thematic analysis of post-experiment interviews revealed four themes regarding the restrictive chairs: physical discomfort, difficulty following moving objects, reduced orientation and guided attention. Based on the findings, practical implications, limitations and future work are discussed.
SP  - 30
EP  - NA
JF  - Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3281505.3281519
ER  - 

TY  - NA
AU  - Hueber, Sebastian; Cherek, Christian; Wacker, Philipp; Borchers, Jan; Voelker, Simon
TI  - MobileHCI - Headbang: Using Head Gestures to Trigger Discrete Actions on Mobile Devices
PY  - 2020
AB  - We present Headbang, an interaction technique that enriches touch input on handheld devices through slight head movement gestures. This way, users can easily execute shortcuts, like Copy, Paste, or Share, to on-screen targets while touching them. Headbang utilizes the capabilities of commodity smartphones to track the user’s head with their front facing cameras. We evaluated Headbang in two studies and show that the system can be reliably used while sitting and walking and offers a similar accuracy and speed as touch interaction.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403538
ER  - 

TY  - CHAP
AU  - Zheng, Changxi
TI  - Physics-Based Computational Design for Digital Fabrication
PY  - 2018
AB  - Physics-based simulation models have been long studied in computer graphics. The current trend is to capture complex physical phenomena that have multi-physics, multi-scale, and multi-modality. Meanwhile, the advent of digital manufacturing techniques is also striking. The barrier of making objects with complex geometries and materials constantly lowers down. The confluence of numerical simulation models and powerful digital fabrication inspires us to rethink the design of objects that can be digitally manufactured. In this paper, I will present our work on physics-based simulation models and their use in various design tasks. I will show that seamless integration of simulation models into the design process opens the door to optimal, unrealized, and even unconventional product designs.
SP  - 133
EP  - 149
JF  - Mathematical Insights into Advanced Computer Graphics Techniques
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-13-2850-3_10
ER  - 

TY  - NA
AU  - Seaborn, Katie; Mähönen, Johanna; Rogers, Yvonne
TI  - Conference on Designing Interactive Systems - Scaling Up to Tackle Low Levels of Urban Food Waste Recycling
PY  - 2020
AB  - Addressing societal problems is complex; little is known about which paths or approaches are successful. We discuss what is involved in knowing when and how and for whom change needs to occur, as well as the impact of doing so at scale-especially when novelty and academic contributions may be compromised. To this end, we present a 'scaling up' framework based on a societal project where we worked with multiple stakeholders to improve food waste recycling rates in a housing estate. We propose three main factors involved in scaling up: (i) 'the people,' through reimagining roles and relationships, (ii) 'the method,' requiring flexibility in design and research, and (iii) 'the impact,' informing new measures by handing over the evaluation. We reflect on the challenges, dilemmas, and successes encountered, as well as discuss the benefit of 'handing over' the evaluation process to gather scalable metrics based on economic modelling.
SP  - 1327
EP  - 1340
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395524
ER  - 

TY  - NA
AU  - Rivera, Michael L.; Hudson, Scott E.
TI  - CHI - Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles
PY  - 2019
AB  - We present a new type of 3D printer that combines rigid plastic printing with melt electrospinning? a technique that uses electrostatic forces to create thin fibers from a molten polymer. Our printer enables custom-shaped textile sheets (similar in feel to wool felt) to be produced alongside rigid plastic using a single material (i.e., PLA) in a single process. We contribute open-source firmware, hardware specifications, and printing parameters to achieve melt electrospinning. Our approach offers new opportunities for fabricating interactive objects and sensors that blend the flexibility, absorbency and softness of produced electrospun textiles with the structure and rigidity of hard plastic for actuation, sensing, and tactile experiences.
SP  - 204
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300434
ER  - 

TY  - NA
AU  - Koelle, Marion; Nicolae, Madalina; Nittala, Aditya Shekhar; Teyssier, Marc; Steimle, Jürgen
TI  - Prototyping Soft Devices with Interactive Bioplastics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545623
ER  - 

TY  - JOUR
AU  - Sadri, Behnam; Abete, Alberto Miralles; Martinez, Ramses V.
TI  - Simultaneous electrophysiological recording and self-powered biosignal monitoring using epidermal, nanotexturized, triboelectronic devices
PY  - 2019
AB  - The fabrication of multifunctional epidermal electronic devices capable of efficiently reading electrophysiological signals and converting low-amplitude mechanical signals into electric outputs promises to pave the way towards the development of self-powered wearable sensors, smart consumer electronics, and human-machine interfaces. This article describes the scalable and cost-effective fabrication of epidermal, nanotexturized, triboelectronic devices (EnTDs). EnTDs can be conformably worn on the skin and efficiently monitor electrophysiological signals, temperature, and hydration levels. EnTDs, while measuring electrophysiological signals, can also convert imperceptible time-variant body motions into electrical signals using a nanotexturized triboelectric layer, enabling the self-powered monitoring of respiration, swallowing, and arterial pulse. These results suggest the potential of EnTDs as a new class of multifunctional skin-like sensors for biomedical monitoring and self-powered sensing applications.
SP  - 274003
EP  - 274003
JF  - Nanotechnology
VL  - 30
IS  - 27
PB  - 
DO  - 10.1088/1361-6528/ab10e9
ER  - 

TY  - NA
AU  - Kaimoto, Hiroki; Monteiro, Kyzyl; Faridan, Mehrad; Li, Jiatong; Farajian, Samin; Kakehi, Yasuaki; Nakagaki, Ken; Suzuki, Ryo
TI  - Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI
PY  - 2022
AB  - This paper introduces Sketched Reality, an approach that combines AR sketching and actuated tangible user interfaces (TUI) for bidirectional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to "affect" each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional -- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545626
ER  - 

TY  - BOOK
AU  - Tamura, Yuto; Takemura, Kentaro
TI  - ETRA Short Papers - Estimating Point-of-Gaze using Smooth Pursuit Eye Movements without Implicit and Explicit User-Calibration
PY  - 2020
AB  - Detecting the point-of-gaze in the real world is a challenging problem in eye-tracking applications. The point-of-gaze is estimated using geometry constraints, and user-calibration is required. In addition, the distances of the focused targets are variable and large in the real world. Therefore, a calibration-free approach without geometry constraints is needed to estimate the point-of-gaze. Recent studies have investigated smooth pursuit eye movements (smooth pursuits) for human-computer interaction applications, and we consider that these smooth pursuits can also be employed in eye tracking. Therefore, we developed a method for estimating the point-of-gaze using smooth pursuits without any requirement for implicit and explicit user-calibration. In this method, interest points are extracted from the scene image, and the point-of-gaze is detected using these points, which are strongly correlated with eye movements. We performed a comparative experiment in a real environment and demonstrated the feasibility of the proposed method.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379156.3391343
ER  - 

TY  - NA
AU  - Xiao, Chang; Rossi, Ryan; Koh, Eunyee
TI  - iMarker: Instant and True-to-scale AR with Invisible Markers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558721
ER  - 

TY  - BOOK
AU  - Merrill, Devon J.; Garza, Jorge; Swanson, Steven
TI  - SCF - Echidna: mixed-domain computational implementation via decision trees
PY  - 2019
AB  - Custom mechatronic devices offer personalized functionality, but also come with many non-functional requirements that are unfamiliar to those inexperienced with electronics such as current draw and servo power. The Echidna prototype system enables non-electrical engineers to move from conception to implementation with their mechatronic ideas by generating and searching through a design space that automatically fills in supporting requirements, such as PCB placement and wiring, around their functional specification. The space is modeled as a decision tree whose root is the user's list of lights, motors, sensors, and other functional components that need to be connected, powered, and controlled. Once found, a complete and valid design can be used to synthesize geometry for 3D printing, circuits, and firmware resulting in a set of "plug and fabricate" files for creating their device. We demonstrate how Echidna realizes several designs and discuss how it can be further customized to task-specific applications.
SP  - NA
EP  - NA
JF  - Proceedings of the ACM Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3328939.3329004
ER  - 

TY  - NA
AU  - Kumar, Kartikaeya; Poretski, Lev; Li, Jiannan; Tang, Anthony
TI  - Tourgether360: Exploring 360° Tour Videos with Others
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519738
ER  - 

TY  - NA
AU  - Subramonyam, Hariharan; Li, Wilmot; Adar, Eytan; Dontcheva, Mira
TI  - UIST - TakeToons: Script-driven Performance Animation
PY  - 2018
AB  - Performance animation is an expressive method for animating characters through human performance. However, character motion is only one part of creating animated stories. The typical workflow also involves writing a script, coordinating actors, and editing recorded performances. In most cases, these steps are done in isolation with separate tools, which introduces friction and hinders iteration. We propose TakeToons, a script-driven approach that allows authors to annotate standard scripts with relevant animation events like character actions, camera positions, and scene backgrounds. We compile this script into a story model that persists throughout the production process and provides a consistent structure for organizing and assembling recorded performances and propagating script or timing edits to existing recordings. TakeToons enables writing, performing and editing to happen in an integrated and interleaved manner that streamlines production and facilitates iteration. Informal feedback from professional animators suggests that our approach can benefit many existing workflows supporting individual authors and production teams with many different contributors.
SP  - 663
EP  - 674
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242618
ER  - 

TY  - JOUR
AU  - Chen, Ying; Liu, Haibin
TI  - Location-Dependent Performance of Large-Area Piezoresistive Tactile Sensors Based on Electrical Impedance Tomography
PY  - 2021
AB  - The technique of electrical impedance tomography (EIT) has been recognized as a promising method to design tactile sensors with continuous sensing capability over a large area. The mechanism of electrical impedance tomography allows reconstructing tactile information within the sensing area based on measurements made only at the boundary. However, spatial performance of EIT-based tactile sensors has demonstrated location dependency, which severely affects correct interpretation of tactile stimuli. Here, we analyzed the effect of hyperparameter on the spatial performance, in terms of amplitude, size, position error, and shape deformation in the reconstructed images. To obtain uniform sensitivity throughout the entire sensing area, we developed an intensity scaling method to correct reconstructed amplitudes based on simulation studies. A diagonal scaling matrix was developed for a symmetric circular sensing area, and the scaling value were constructed according to the radial positions of the finite elements. The correction method was further evaluated on a compliant EIT-based touch sensor made of polymer filled composites with underlying paddings. We found that the developed method effectively produced a more uniform sensitivity distribution, and improved spatial profiles of shape deformation. The findings shown here help better interpret the strength information of tactile stimuli located at different positions of large area EIT-based sensors.
SP  - 21622
EP  - 21630
JF  - IEEE Sensors Journal
VL  - 21
IS  - 19
PB  - 
DO  - 10.1109/jsen.2021.3103988
ER  - 

TY  - JOUR
AU  - Ma, Gang; Hao, Zhiliang; Wu, Xuan; Wang, Xiaojie
TI  - An Optimal Electrical Impedance Tomography Drive Pattern for Human-Computer Interaction Applications
PY  - 2020
AB  - In this article, we presented an optimal Electrical Impedance Tomography (EIT) drive pattern based on feature selection and model explanation, and proposed a portable EIT system for applications in human-computer interaction for gesture recognition and contact detection, which can reduce the measurement time and realize a performance trade-off between the accuracy and the time response. In our experiment, eleven hand gestures were designed to verify the proposed approach and EIT system. Compared to the traditional eight-electrode method, the optimal electrode drive pattern achieved a recognition accuracy of 97.5% with seven electrodes and the measurement time was reduced by 60%. To illustrate the universality of this method, we performed a contact detection experiment. By setting seven labels on the conductive panel and using optimal electrode drive pattern, the detection accuracy reached 100% with seven electrodes and the measurement time was reduced by 85%.
SP  - 402
EP  - 411
JF  - IEEE transactions on biomedical circuits and systems
VL  - 14
IS  - 3
PB  - 
DO  - 10.1109/tbcas.2020.2967785
ER  - 

TY  - JOUR
AU  - Tone, Daiki; Iwai, Daisuke; Hiura, Shinsaku; Sato, Kosuke
TI  - FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping
PY  - 2020
AB  - This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range.
SP  - 2030
EP  - 2040
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2020.2973444
ER  - 

TY  - JOUR
AU  - Dvorožnák, Marek; Li, Wilmot; Kim, Vladimir G.; Sýkora, Daniel
TI  - Toonsynth: example-based synthesis of hand-colored cartoon animations
PY  - 2018
AB  - We present a new example-based approach for synthesizing hand-colored cartoon animations. Our method produces results that preserve the specific visual appearance and stylized motion of manually authored animations without requiring artists to draw every frame from scratch. In our framework, the artist first stylizes a limited set of known source skeletal animations from which we extract a style-aware puppet that encodes the appearance and motion characteristics of the artwork. Given a new target skeletal motion, our method automatically transfers the style from the source examples to create a hand-colored target animation. Compared to previous work, our technique is the first to preserve both the detailed visual appearance and stylized motion of the original hand-drawn content. Our approach has numerous practical applications including traditional animation production and content creation for games.
SP  - 167
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201326
ER  - 

TY  - JOUR
AU  - Zhao, T; Wu, Chenning; Soleimani, Manuchehr
TI  - Ionic liquid based distributed touch sensor using electrical impedance tomography
PY  - 2020
AB  - <jats:title>Abstract</jats:title> <jats:p>Inspired by the human skin sensory mechanism, there are growing interests in creating a sense of touch in robotics. This work describes a new impedance based design to create an artificial tactile sensing skin. It has demonstrated that the electrical impedance tomography imaging technique allows for detecting the pressure distribution in a large area by a distributed touch sensor. The sensor is fabricated by filling a circular shaped phantom with liquid conductor and covering with an elastic shell on the top. The proposed sensor can detect the pressure applied to the elastic top using electrical impedance tomography imaging method. The sensor can therefore operate as a touch sensor mimicking a piezo-impedance operation in a simple fashion. The new sensor can differentiate between various force levels and their locations and thus produces a distribution of pressure. Such a simple sensor can function as a large area skin, enabling smarter human-machine interactions in emerging augmented reality and robotic applications.</jats:p>
SP  - 025005
EP  - NA
JF  - IOP SciNotes
VL  - 1
IS  - 2
PB  - 
DO  - 10.1088/2633-1357/abb345
ER  - 

TY  - NA
AU  - Kwan, Kin Chung; Fu, Hongbo
TI  - CHI - Mobi3DSketch: 3D Sketching in Mobile AR
PY  - 2019
AB  - Mid-air 3D sketching has been mainly explored in Virtual Reality (VR) and typically requires special hardware for motion capture and immersive, stereoscopic displays. The recently developed motion tracking algorithms allow real-time tracking of mobile devices, and have enabled a few mobile applications for 3D sketching in Augmented Reality (AR). However, they are more suitable for making simple drawings only, since they do not consider special challenges with mobile AR 3D sketching, including the lack of stereo display, narrow field of view, and the coupling of 2D input, 3D input and display. To address these issues, we present Mobi3DSketch, which integrates multiple sources of inputs with tools, mainly different versions of 3D snapping and planar/curves surface proxies. Our multimodal interface supports both absolute and relative drawing, allowing easy creation of 3D concept designs in situ. The effectiveness and expressiveness of Mobi3DSketch are demonstrated via a pilot study.
SP  - 176
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300406
ER  - 

TY  - JOUR
AU  - Salanti, Georgia; Cipriani, Andrea; Furukawa, Toshi A.; Peter, Natalie; Tonia, Thomy; Papakonstantinou, Theodoros; Holloway, Alexander; Leucht, Stefan
TI  - An efficient way to assess the effect of COVID-19 on mental health in the general population.
PY  - 2021
AB  - The current article discusses an efficient way to assess the effect of COVID-19 on mental health in the general population. It is maintained tat a meta-ecological study is needed to explore te effect of geograpically and temporally different pandemic characteristics o npopulations;mental health. Despite their potential shortcomings due to confounding and aggregation bias, meta-ecological study designs have successfully answered similar global questions, such as the role of air pollution on morbidity. In a meta-ecological study, a systematic review of prevalence before and during the pandemic in various locations with different responses to the pandemic should shed light on changes in mental health problems. It is concluded that because a meta-ecological study uses published and regularly updated information, it does not require expensive or time-consuming data collection. The crowdsourcing approach will speed up the process and could be the way forward to do large-scale research in times of social isolation. Investment in methods for harnessing information in a reliable and rapid way will enable decision makers worldwide to integrate a mental health science perspective into their response to the pandemic. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
SP  - e14
EP  - e15
JF  - The lancet. Psychiatry
VL  - 8
IS  - 5
PB  - 
DO  - 10.1016/s2215-0366(21)00067-5
ER  - 

TY  - NA
AU  - Dogan, Mustafa Doga; Faruqi, Faraz; Churchill, Andrew Day; Friedman, Kenneth; Cheng, Leon; Subramanian, Sriram; Mueller, Stefanie
TI  - CHI - G-ID: Identifying 3D Prints Using Slicing Parameters
PY  - 2020
AB  - We present G-ID, a method that utilizes the subtle patterns left by the 3D printing process to distinguish and identify objects that otherwise look similar to the human eye. The key idea is to mark different instances of a 3D model by varying slicing parameters that do not change the model geometry but can be detected as machine-readable differences in the print. As a result, G-ID does not add anything to the object but exploits the patterns appearing as a by-product of slicing, an essential step of the 3D printing pipeline. We introduce the G-ID slicing and labeling interface that varies the settings for each instance, and the G-ID mobile app, which uses image processing techniques to retrieve the parameters and their associated labels from a photo of the 3D printed object. Finally, we evaluate our method's accuracy under different lighting conditions, when objects were printed with different filaments and printers, and with pictures taken from various positions and angles.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376202
ER  - 

TY  - NA
AU  - Furukawa, Kenji; Nakata, Susumu
TI  - SIGGRAPH ASIA Posters - Automatic generation of hair motion of 3D characters following japanese anime style
PY  - 2018
AB  - Recent Japanese animation is progressively using three-dimensional computer graphics (3DCG). However, Japanese animation created according to the Japanese traditional method called "limited animation" is different from photorealistic motion of 3DCG. In particular, hair motion obtained via this method is different from that obtained by physical calculation. In this study, we formulate a method of hair motion of traditional Japanese hand-drawn animation.
SP  - 76
EP  - NA
JF  - SIGGRAPH Asia 2018 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3283289.3283306
ER  - 

TY  - NA
AU  - Zhao, Zhenjie; Ma, Xiaojuan
TI  - AIVR - A Compensation Method of Two-Stage Image Generation for Human-AI Collaborated In-Situ Fashion Design in Augmented Reality Environment
PY  - 2018
AB  - In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real humancomputer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design.
SP  - 76
EP  - 83
JF  - 2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr.2018.00018
ER  - 

TY  - CHAP
AU  - Tyng, Khoo Shiang; Zaman, Halimah Badioze; Mohamad, Ummul Hanan; Ahmad, Azlina
TI  - IVIC - Visual Learning Application in Mathematics Using Holographic Display Based on Multi-touch Technology
PY  - 2021
AB  - The Malaysian Education System have recently implemented the home-based teaching and learning or Pengajaran dan Pembelajaran di Rumah (PdPR) classes due to the outbreak of the pandemic COVID19. Since students were not able to go to schools during the lockdown period, teachers and parents have resorted to creating various methods to engage students in their learning process. Multi-touch technology on tablet appears as a promising tool in visual learning especially during this pandemic out-break. This paper presents a preliminary study conducted on a research project in developing a Visual Learning Application for Mathematics using Holographic Display for the topic on Shape and Space based on Multi-Touch Technology called MEL-VIS. A preliminary study was conducted on fifteen (15) primary school teachers. The results of the preliminary study showed that the topic on Shape and Space is a topic that most students had major problems when learning Mathematics at three (3) primary schools. Students were found to have difficulty in understanding the concepts being taught due to factors such as: abstractive phenomena and concepts; as well as possessing low imagination.
SP  - 98
EP  - 110
JF  - Advances in Visual Informatics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90235-3_9
ER  - 

TY  - NA
AU  - Yang, Humphrey; Yan, Zeyu; Luo, Danli; Yao, Lining
TI  - FoamFactor: Hydrogel-Foam Composite with Tunable Stiffness and Compressibility.
PY  - 2021
AB  - This paper presents FoamFactor, a novel material with tunable stiffness and compressibility between hydration states, and a tailored pipeline to design and fabricate artifacts consisting of it. This technique compounds hydrogel with open-cell foams via additive manufacturing to produce a water-responsive composite material. Enabled by the large volumetric changes of hydrogel dispersions, the material is soft and compressible when dehydrated and becomes stiffer and rather incompressible when hydrated. Leveraging this material property transition, we explore its design space in various aspects pertaining to the transition of hydration states, including multi-functional shoes, amphibious cars, mechanical transmission systems, and self-deploying robotic grippers.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Márquez, Jesús Omar Álvarez; Ziegler, Jürgen
TI  - Mensch &amp; Computer - Augmented-Reality-Enhanced Product Comparison in Physical Retailing
PY  - 2019
AB  - Augmented reality technology has experienced great improvement in recent years and it has been successfully applied to industry and entertainment settings. However, its application in everyday contexts such as shopping is still very limited. One of the requirements to seamlessly incorporate augmented reality into everyday tasks is to find intuitive, natural methods to make use of it. Due to the inherent capabilities of augmented reality to work as a visual aid to explore and extend the knowledge a user has of the surroundings, this paper proposes the combination of AR technology and product advisors in a novel approach for product comparison. The user's awareness of the differences between multiple physically present objects is enhanced through virtual augmentations, supporting an intuitive way of comparing two or more products while shopping. To assess the validity of the concept, a prototype for an AR-based shopping assistant for comparing vacuum cleaners has been implemented and evaluated in a user study, testing different methods of visual comparison and interaction.
SP  - 55
EP  - 65
JF  - Proceedings of Mensch und Computer 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3340764.3340800
ER  - 

TY  - JOUR
AU  - Rahimizhian, Sima; Ozturen, Ali; İlkan, Mustafa
TI  - Emerging realm of 360-degree technology to promote tourism destination
PY  - 2020
AB  - NA
SP  - 101411
EP  - NA
JF  - Technology in Society
VL  - 63
IS  - NA
PB  - 
DO  - 10.1016/j.techsoc.2020.101411
ER  - 

TY  - NA
AU  - Li, Qisheng; Gajos, Krzysztof Z.; Reinecke, Katharina
TI  - ASSETS - Volunteer-Based Online Studies With Older Adults and People with Disabilities
PY  - 2018
AB  - There are few large-scale empirical studies with people with disabilities or older adults, mainly because recruiting partici­pants with specific characteristics is even harder than recruit­ing young and/or non-disabled populations. Analyzing four online experiments on LabintheWild with a total of 355,656 participants, we show that volunteer-based online experiments that provide personalized feedback attract large numbers of participants with diverse disabilities and ages and allow ro­bust studies with these populations that replicate and extend the findings of prior laboratory studies. To find out what mo­tivates people with disabilities to take part, we additionally analyzed participants' feedback and forum entries that discuss LabintheWild experiments. The results show that participants use the studies to diagnose themselves, compare their abilities to others, quantify potential impairments, self-experiment, and share their own stories -- findings that we use to inform design guidelines for online experiment platforms that adequately support and engage people with disabilities.
SP  - 229
EP  - 241
JF  - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3234695.3236360
ER  - 

TY  - NA
AU  - Capallera, Marine; de Salis, Emmanuel; Meteier, Quentin; Angelini, Leonardo; Carrino, Stefano; Khaled, Omar Abou; Mugellini, Elena
TI  - AutomotiveUI (adjunct) - Secondary task and situation awareness, a mobile application for conditionally automated vehicles
PY  - 2019
AB  - Autonomous vehicles are developing rapidly and will lead to a significant change in the driver's role: s/he will have to move from the role of actor to the role of supervisor. Indeed, s/he will soon be able to perform a secondary task but s/he must be able to take over control when a critical situation is not managed by the driving system. The role of new interfaces and interactions within the vehicle is important to take into account. This article describes the design of an application that provides the driver with information about the environment perceived by the vehicle. This application is displayed as split screen on a tablet by which a secondary task can be performed. The results of initial experiment showed that the participants correctly identified all the factors limiting the proper functioning of the driving system while performing a secondary task on the tablet.
SP  - 86
EP  - 92
JF  - Proceedings of the 11th International Conference on Automotive User Interfaces and Interactive Vehicular Applications: Adjunct Proceedings
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3349263.3351500
ER  - 

TY  - NA
AU  - Oshim, Farhan Tasnim; Killingback, Julian; Follette, Dave; Peng, Huaishu; Rahman, Tauhidur
TI  - UIST - MechanoBeat: Monitoring Interactions with Everyday Objects using 3D Printed Harmonic Oscillators and Ultra-Wideband Radar
PY  - 2020
AB  - In this paper we present MechanoBeat, a 3D printed mechanical tag that oscillates at a unique frequency upon user interaction. With the help of an ultra-wideband (UWB) radar array, MechanoBeat can unobtrusively monitor interactions with both stationary and mobile objects. MechanoBeat consists of small, scalable, and easy-to-install tags that do not require any batteries, silicon chips, or electronic components. Tags can be produced using commodity desktop 3D printers with cheap materials. We develop an efficient signal processing and deep learning method to locate and identify tags using only the signals reflected from the tag vibrations. MechanoBeat is capable of detecting simultaneous interactions with high accuracy, even in noisy environments. We leverage UWB radar signals' high penetration property to sense interactions behind walls in a non-line-of-sight (NLOS) scenario. A number of applications using MechanoBeat have been explored and the results have been presented in the paper.
SP  - 430
EP  - 444
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415902
ER  - 

TY  - JOUR
AU  - Papp, György; Hoffmann, Miklós; Papp, Ildikó
TI  - Embedding QR Code onto Triangulated Meshes using Horizon Based Ambient Occlusion
PY  - 2021
AB  - NA
SP  - 29
EP  - 45
JF  - Computer Graphics Forum
VL  - 41
IS  - 1
PB  - 
DO  - 10.1111/cgf.14394
ER  - 

TY  - NA
AU  - Jiang, Weiwei; Wang, Chaofan; Sarsenbayeva, Zhanna; Irlitti, Andrew; Knibbe, Jarrod; Dingler, Tilman; Goncalves, Jorge; Kostakos, Vassilis
TI  - InfoPrint: Embedding Information into 3D Printed Objects.
PY  - 2021
AB  - We present a technique to embed information invisible to the eye inside 3D printed objects. The information is integrated in the object model, and then fabricated using off-the-shelf dual-head FDM (Fused Deposition Modeling) 3D printers. Our process does not require human intervention during or after printing with the integrated model. The information can be arbitrary symbols, such as icons, text,binary, or handwriting. To retrieve the information, we evaluate two different infrared-based imaging devices that are readily available-thermal cameras and near-infrared scanners. Based on our results, we propose design guidelines for a range of use cases to embed and extract hidden information. We demonstrate how our method can be used for different applications, such as interactive thermal displays, hidden board game tokens, tagging functional printed objects, and autographing non-fungible fabrication work.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Park, Seonwook; Gebhardt, Christoph; Rädle, Roman; Feit, Anna Maria; Vrzakova, Hana; Dayama, Niraj Ramesh; Yeo, Hui-Shyong; Klokmose, Clemens Nylandsted; Quigley, Aaron; Oulasvirta, Antti; Hilliges, Otmar
TI  - CHI - AdaM: Adapting Multi-User Interfaces for Collaborative Environments in Real-Time
PY  - 2018
AB  - Developing cross-device multi-user interfaces (UIs) is a challenging problem. There are numerous ways in which content and interactivity can be distributed. However, good solutions must consider multiple users, their roles, their preferences and access rights, as well as device capabilities. Manual and rule-based solutions are tedious to create and do not scale to larger problems nor do they adapt to dynamic changes, such as users leaving or joining an activity. In this paper, we cast the problem of UI distribution as an assignment problem and propose to solve it using combinatorial optimization. We present a mixed integer programming formulation which allows real-time applications in dynamically changing collaborative settings. It optimizes the allocation of UI elements based on device capabilities, user roles, preferences, and access rights. We present a proof-of-concept designer-in-the-loop tool, allowing for quick solution exploration. Finally, we compare our approach to traditional paper prototyping in a lab study.
SP  - 184
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173758
ER  - 

TY  - BOOK
AU  - Fox, Amy Rae; Guo, Philip J.; Klokmose, Clemens Nylandsted; Dalsgaard, Peter; Satyanarayan, Arvind; Xia, Haijun; Hollan, James D.
TI  - Programming - Towards a dynamic multiscale personal information space: beyond application and document centered views of information
PY  - 2020
AB  - The historical moment when a person worked in front of a single computer has passed. Computers are now ubiquitous and embedded in virtually every new device and system, connecting our personal and professional activities to ever-expanding information resources with previously unimaginable computational power. Yet with all the increases in capacity, speed, and connectivity, our experiences too often remain difficult, awkward, and frustrating. Even after six decades of design evolution there is little of the naturalness and contextual sensitivity required for convivial interaction with computer-mediated information. We envision a future in which the existing world of documents and applications is linked to a multiscale personalized information space in which dynamic visual entities behave in accordance with cognitively motivated rules sensitive to tasks, personal and group interaction histories, and context. The heart of the project is to rethink the nature of computer-mediated information as a basis to begin to fully realize the potential of computers to assist information-based activities. This requires challenging fundamental presuppositions that have led to today’s walled gardens and information silos. Our goal is to catalyze an international research community to rethink the nature of information as a basis for radically advancing the human-centered design of information-based work and helping to ensure the future is one of convivial, effective, and humane systems. In this paper, we propose a new view of information systems, discuss cognitive requirements for a human-centered information space, and sketch a research agenda and approach.
SP  - 136
EP  - 143
JF  - Conference Companion of the 4th International Conference on Art, Science, and Engineering of Programming
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397537.3397542
ER  - 

TY  - JOUR
AU  - Masia, Belen; Camon, Javier; Gutierrez, Diego; Serrano, Ana
TI  - Influence of Directional Sound Cues on Users’ Exploration Across 360° Movie Cuts
PY  - 2021
AB  - Virtual reality (VR) is a powerful medium for $360^{\circ }$360∘ storytelling, yet content creators are still in the process of developing cinematographic rules for effectively communicating stories in VR. Traditional cinematography has relied for over a century on well-established techniques for editing, and one of the most recurrent resources for this are cinematic cuts that allow content creators to seamlessly transition between scenes. One fundamental assumption of these techniques is that the content creator can control the camera; however, this assumption breaks in VR: Users are free to explore $360^{\circ }$360∘ around them. Recent works have studied the effectiveness of different cuts in $360^{\circ }$360∘ content, but the effect of directional sound cues while experiencing these cuts has been less explored. In this work, we provide the first systematic analysis of the influence of directional sound cues in users’ behavior across $360^{\circ }$360∘ movie cuts, providing insights that can have an impact on deriving conventions for VR storytelling.
SP  - 64
EP  - 75
JF  - IEEE computer graphics and applications
VL  - 41
IS  - 4
PB  - 
DO  - 10.1109/mcg.2021.3064688
ER  - 

TY  - NA
AU  - Cheng, Hao Fei; Yu, Bowen; Fu, Siwei; Zhao, Jian; Hecht, Brent; Konstan, Joseph A.; Terveen, Loren; Yarosh, Svetlana; Zhu, Haiyi
TI  - L@S - Teaching UI Design at Global Scales: A Case Study of the Design of Collaborative Capstone Projects for MOOCs
PY  - 2019
AB  - Group projects are an essential component of teaching user interface (UI) design. We identified six challenges in transferring traditional group projects into the context of Massive Open Online Courses: managing dropout, avoiding free-riding, appropriate scaffolding, cultural and time zone differences, and establishing common ground. We present a case study of the design of a group project for a UI Design MOOC, in which we implemented technical tools and social structures to cope with the above challenges. Based on survey analysis, interviews, and team chat data from the students over a six-month period, we found that our socio-technical design addressed many of the obstacles that MOOC learners encountered during remote collaboration. We translate our findings into design implications for better group learning experiences at scale.
SP  - NA
EP  - NA
JF  - Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3330430.3333635
ER  - 

TY  - JOUR
AU  - Pfeuffer, Ken; Abdrabou, Yasmeen; Esteves, Augusto; Rivu, Radiah; Abdelrahman, Yomna; Meitner, Stefanie; Saadi, Amr; Alt, Florian
TI  - ARtention: A design space for gaze-adaptive user interfaces in augmented reality
PY  - 2021
AB  - NA
SP  - 1
EP  - 12
JF  - Computers & Graphics
VL  - 95
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.01.001
ER  - 

TY  - NA
AU  - Garza, Jorge; Merrill, Devon J.; Swanson, Steven
TI  - UIST (Adjunct Volume) - Appliancizer: Transforming Web Pages into Electronic Gadgets
PY  - 2020
AB  - Making electronic gadgets that meet today's consumer standards has become a difficult task. Electronic gadgets are expected to have displays with visually appealing interfaces and, at the same time, be physically and screen-interactive, making the development process of these devices time-consuming and challenging. To address this problem, we have created Appliancizer, an online synthesis tool that can automatically generate sophisticated electronic devices from web pages. Appliancizer takes advantage of the similarities between software and physical interfaces to reduce development steps and allow the rapid development of electronic devices. By matching the interface of hardware components with the interface of graphical HTML elements found on web pages, our tool allows a designer to transform HTML elements from a digital to a tangible interface without changing the application source code. Finally, a modular design enables our tool to automatically combine the circuit design and low-level hardware code of selected hardware components into a complete design. Attendees can interact with our online tool and produce manufacturable PCBs from web pages.
SP  - 142
EP  - 144
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416158
ER  - 

TY  - NA
AU  - Wang, Tianyi; Huo, Ke; Chawla, Pratik; Chen, Guiming; Banerjee, Siddharth; Ramani, Karthik
TI  - CHI Extended Abstracts - Plain2Fun: Augmenting Ordinary Objects with Surface Painted Circuits
PY  - 2018
AB  - The growing makers' community demands better supports for designing and fabricating interactive functional objects. Most of the current approaches focus on embedding desired functions within new objects. Instead, we advocate re-purposing existing objects and authoring interactive functions onto them. We present Plain2Fun, a design and fabrication pipeline enabling users to quickly transform ordinary objects into interactive and functional ones. Plain2Fun allows users to directly design the circuit layouts onto the surfaces of the scanned 3D model of existing objects. Our design tool automatically generates as short as possible circuit paths between any two points while avoiding intersections. Further, we build a digital machine to construct the conductive paths accurately. With a specially designed housing base, users can simply snap the electronic components onto the surfaces and obtain working physical prototypes. Moreover, we evaluate the usability of our system with multiple use cases.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188655
ER  - 

TY  - NA
AU  - Getschmann, Christopher; Echtler, Florian
TI  - TEI - Seedmarkers: Embeddable Markers for Physical Objects
PY  - 2021
AB  - We present Seedmarkers, shape-independent topological markers that can be embedded in physical objects manufactured with common rapid-prototyping techniques. Many markers are optimized for technical performance while visual appearance or the feasibility of permanently merging marker and physical object is not considered. We give an overview of the aesthetic properties of a wide range of existing markers and conducted a short online survey to assess the perception of popular marker designs. Based on our findings we introduce our generation algorithm making use of weighted Voronoi diagrams for topological optimization. With our generator, Seedmarkers can be created from technical drawings during the design process to fill arbitrary shapes on any surface. Given dimensions and manufacturing constraints, different configurations for 3 or 6 degrees of freedom tracking are possible. We propose a set of application examples for shape-independent markers, including 3D printed tangibles, laser cut plates and functional markers on printed circuit boards.
SP  - 1
EP  - 11
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440645
ER  - 

TY  - NA
AU  - Shi, Rongkai; Zhu, Nan; Liang, Hai-Ning; Zhao, Shengdong
TI  - ISMAR - Exploring Head-based Mode-Switching in Virtual Reality
PY  - 2021
AB  - Mode-switching supports multilevel operations using a limited number of input methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches for mode-switching use buttons, controllers, and users’ hands. However, they are inefficient and challenging to do with tasks that require both hands (e.g., when users need to use two hands during drawing operations). Using head gestures for mode-switching can be an efficient and cost-effective way, allowing for a more continuous and smooth transition between modes. In this paper, we explore the use of head gestures for mode-switching especially in scenarios when both users’ hands are performing tasks. We present a first user study that evaluated eight head gestures that could be suitable for VR HMD with a dual-hand line-drawing task. Results show that move forward, move backward, roll left, and roll right led to better performance and are preferred by participants. A second study integrating these four gestures in Tilt Brush, an open-source painting VR application, is conducted to further explore the applicability of these gestures and derive insights. Results show that Tilt Brush with head gestures allowed users to change modes with ease and led to improved interaction and user experience. The paper ends with a discussion on some design recommendations for using head-based mode-switching in VR HMD.
SP  - 118
EP  - 127
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00026
ER  - 

TY  - BOOK
AU  - ElSayed, Karim A.; Dachowicz, Adam; Panchal, Jitesh H.
TI  - AMSec@CCS - Information Embedding in Additive Manufacturing through Printing Speed Control
PY  - 2021
AB  - Additive manufacturing (AM) is rapidly developing, and new applications are continuously emerging. While AM is increasingly becoming integral to many industries, including aerospace, automotive, and biomedical, it has opened a host of unique security concerns, from theft of technical data to process sabotage and counterfeiting. In this work, we present a method to address the counterfeiting problem by embedding information in additively manufactured parts through controlling printing process parameters. Variations in printing speed, the encoding parameter in this work, introduce subtle localized height differences on parts' surfaces, which are readable using an optical profilometer. The profilometry data is captured after printing, and this data is processed to predict the intended bit response for each embedding region on the surface of the part. We experimentally demonstrate the feasibility of the proposed scheme for embedding and reading the information in 3D printed parts and show that it achieves 80% accuracy for a 53 mm/s difference in printing speed between the encoded bits. Finally, we characterize the performance of the proposed scheme, measured as the accuracy in decoded messages, as a function of the difference in printing speed used to perform the embedding.
SP  - 31
EP  - 37
JF  - Proceedings of the 2021 Workshop on Additive Manufacturing (3D Printing) Security
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462223.3485623
ER  - 

TY  - JOUR
AU  - Nawaz, Mehmood; Chan, Russell W.; Malik, Anju; Khan, Tariq; Cao, Peng
TI  - Hand Gestures Classification Using Electrical Impedance Tomography Images
PY  - 2022
AB  - Human–computer communication using hand gestures has always been difficult. More than half a century ago, people used differentways of interactionwith computers from the early mediums such as perforated game cards. Nowadays, if a richer lexicon of gestures is given, people can communicate more effectively with computers. Machine learning is now used to recognize and classify hand gestures in amore preciseway. In order to increase the communication between computers and humans, we proposed a technique, which uses a wearable low-cost device to generate the electrical impedance tomography (EIT) images to recover the inner impedance structure of a user’s wrist. This is done by measuring the transverse impedance between all the 16 pairs of electrodesofwrist band that lie on the skin of the user hand. The proposedtechnique is enough to integrate the technology into the prototypewrist band tomonitor and classify gestures in real time. We have conducted a study of 16 gestures with a focus on gross hand and pinch finger gestures. The results evaluation shows that the gross hand gestures achieved 90% accuracy inwrist position, while pinch gestures achieved 93% accuracy.
SP  - 18922
EP  - 18932
JF  - IEEE Sensors Journal
VL  - 22
IS  - 19
PB  - 
DO  - 10.1109/jsen.2022.3193718
ER  - 

TY  - NA
AU  - Mahyar, Narges; James, Michael R.; Ng, Michelle M.; Wu, Reginald A.; Dow, Steven
TI  - CHI - CommunityCrit: Inviting the Public to Improve and Evaluate Urban Design Ideas through Micro-Activities
PY  - 2018
AB  - While urban design affects the public, most people do not have the time or expertise to participate in the process. Many online tools solicit public input, yet typically limit interaction to collecting complaints or early-stage ideas. This paper explores how to engage the public in more complex stages of urban design without requiring a significant time commitment. After observing workshops, we designed a system called CommunityCrit that offers micro-activities to engage communities in elaborating and evaluating urban design ideas. Through a four-week deployment, in partnership with a local planning group seeking to redesign a street intersection, CommunityCrit yielded 352 contributions (around 10 minutes per participant). The planning group reported that CommunityCrit provided insights on public perspectives and raised awareness for their project, but noted the importance of setting expectations for the process. People appreciated that the system provided a window into the planning process, empowered them to contribute, and supported diverse levels of skills and availability.
SP  - 195
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173769
ER  - 

TY  - NA
AU  - Delmotte, Arnaud; Tanaka, Kenichiro; Kubo, Hiroyuki; Funatomi, Takuya; Mukaigawa, Yasuhiro
TI  - Blind Watermarking for 3-D Printed Objects using Surface Norm Distribution
PY  - 2018
AB  - We present a new blind watermarking algorithm for 3d printed objects that has applications for copyright protection, proof of authenticity, object identification, and traitor tracing. It allows to embed a few bits of data in a 3d printed object and retrieve it by 3d scanning without requiring the original mesh. While prior methods embed the watermark in the vertex of the object, our method embeds in the histogram of shape to obtain the robustness for resampling and can thus work with any 3d printer and scanner. In addition, our method avoids the shape degradation by subdividing the bins of the histogram, and increases the robustness of bin localization by introducing bin margins. In the experiment, our method has been successfully tested with print simulation and with real print-scan.
SP  - 282
EP  - 288
JF  - 2018 Joint 7th International Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iciev.2018.8640986
ER  - 

TY  - NA
AU  - Le, Huy Viet; Mayer, Sven; Henze, Niels
TI  - MUM - Imprint-Based Input Techniques for Touch-Based Mobile Devices
PY  - 2020
AB  - Touchscreens translate touches of all kinds into 2D coordinates. This limits the input vocabulary and constrains effective interaction to touches by the fingertip. Previous tabletop research extended the input vocabulary with a myriad of promising input techniques using the shape of fingers and hands. However, these techniques are not applicable to mobile devices due to differences in size, ergonomics, and technology. We conducted ideation sessions (N=17) to explore novel input techniques and use cases for imprint-based touch sensing on mobile devices. As a case study, we present FlexionTouch, a novel input technique that recognizes the finger flexion on a touchscreen. Using the finger flexion as an additional input dimension, FlexionTouch provides an always-available shortcut and can be used for value inputs, document previews, and gestures. We propose five example use cases for FlexionTouch input which we evaluated in a second user study (N=20). While the low resolution of the capacitive images leads to a less accurate input compared to tabletops, participants still find the presented use cases helpful. As our input technique is purely software-based, it can be readily deployed to every mobile device with a capacitive touchscreen.
SP  - 32
EP  - 41
JF  - 19th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3428361.3428393
ER  - 

TY  - NA
AU  - Gupta, Aakar; Yang, Jiushan; Balakrishnan, Ravin
TI  - UIST - Asterisk and Obelisk: Motion Codes for Passive Tagging
PY  - 2018
AB  - Machine readable passive tags for tagging physical objects are ubiquitous today. We propose Motion Codes, a passive tagging mechanism that is based on the kinesthetic motion of the user's hand. Here, the tag comprises of a visual pattern that is displayed on a physical surface. To scan the tag and receive the encoded information, the user simply traces their finger over the pattern. The user wears an inertial motion sensing (IMU) ring on the finger that records the traced pattern. We design two motion code schemes, Asterisk and Obelisk that rely on directional vector data processed from the IMU. We evaluate both schemes for the effects of orientation, size, and data density on their accuracies. We further conduct an in-depth analysis of the sources of motion deviations in the ring data as compared to the ground truth finger movement data. Overall, Asterisk achieves a 95% accuracy for an information capacity of 16.8 million possible sequences.
SP  - 725
EP  - 736
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242637
ER  - 

TY  - NA
AU  - Meier, Manuel; Streli, Paul; Fender, Andreas; Holz, Christian
TI  - VR - TaplD: Rapid Touch Interaction in Virtual Reality using Wearable Sensing
PY  - 2021
AB  - Current Virtual Reality systems typically use cameras to capture user input from controllers or free-hand mid-air interaction. In this paper, we argue that this is a key impediment to productivity scenarios in VR, which require continued interaction over prolonged periods of time-a requirement that controller or free-hand input in mid-air does not satisfy. To address this challenge, we bring rapid touch interaction on surfaces to Virtual Reality-the input modality that users have grown used to on phones and tablets for continued use. We present TapID, a wrist-based inertial sensing system that complements headset-tracked hand poses to trigger input in VR. TapID embeds a pair of inertial sensors in a flexible strap, one at either side of the wrist; from the combination of registered signals, TapID reliably detects surface touch events and, more importantly, identifies the finger used for touch. We evaluated TapID in a series of user studies on event-detection accuracy (F1 = 0.997) and hand-agnostic finger-identification accuracy (within-user: F1 = 0.93; across users: F1 = 0.91 after 10 refinement taps and F1 = 0.87 without refinement) in a seated table scenario. We conclude with a series of applications that complement hand tracking with touch input and that are uniquely enabled by TapID, including UI control, rapid keyboard typing and piano playing, as well as surface gestures.
SP  - 519
EP  - 528
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00076
ER  - 

TY  - NA
AU  - Chen, Charles H.; Guo, Philip J.
TI  - L@S - Improv: Teaching Programming at Scale via Live Coding
PY  - 2019
AB  - Computer programming instructors frequently perform live coding in settings ranging from MOOC lecture videos to online livestreams. However, there is little tool support for this mode of teaching, so presenters must now either screen-share or use generic slideshow software. To overcome the limitations of these formats, we propose that programming environments should directly facilitate live coding for education. We prototyped this idea by creating Improv, an IDE extension for preparing and delivering code-based presentations informed by Mayer's principles of multimedia learning. Improv lets instructors synchronize blocks of code and output with slides and create preset waypoints to guide their presentations. A case study on 30 educational videos containing 28 hours of live coding showed that Improv was versatile enough to replicate approximately 96% of the content within those videos. In addition, a preliminary user study on four teaching assistants showed that Improv was expressive enough to allow them to make their own custom presentations in a variety of styles and improvise by live coding in response to simulated audience questions. Users mentioned that Improv lowered cognitive load by minimizing context switching and made it easier to fix errors on-the-fly than using slide-based presentations.
SP  - NA
EP  - NA
JF  - Proceedings of the Sixth (2019) ACM Conference on Learning @ Scale
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3330430.3333627
ER  - 

TY  - CHAP
AU  - Fearghail, Colm O; Ozcinar, Cagri; Knorr, Sebastian; Smolic, Aljosa
TI  - ICIDS - Director’s Cut - Analysis of Aspects of Interactive Storytelling for VR Films
PY  - 2018
AB  - To explore methods that are currently used by professional virtual reality (VR) filmmakers to tell their stories and guide users, we analyze how end-users view \(360^\circ \) video in the presence of directional cues and evaluate if they are able to follow the actual story of narrative \(360^\circ \) films. In this context, we first collected data from five professional VR filmmakers. The data contains eight \(360^\circ \) videos, the directors cut, which is the intended viewing direction of the director, plot points and directional cues used for user guidance. Then, we performed a subjective experiment with 20 test subjects viewing the videos while their head orientation was recorded. Finally, we present and discuss the experimental results and show, among others, that visual discomfort and disorientation on part of the viewer not only lessen the immersive quality of the films but also cause difficulties in the viewer gaining a full understanding of the narrative that the director wished them to view.
SP  - 308
EP  - 322
JF  - Interactive Storytelling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-04028-4_34
ER  - 

TY  - CHAP
AU  - de Salis, Emmanuel; Capallera, Marine; Meteier, Quentin; Angelini, Leonardo; Khaled, Omar Abou; Mugellini, Elena; Widmer, Marino; Carrino, Stefano
TI  - HCI (2) - Designing an AI-companion to support the driver in highly autonomous cars
PY  - 2020
AB  - In this paper, we propose a model for an AI-Companion for conditionally automated cars, able to maintain awareness of the driver regarding the environment but also to able design take-over requests (TOR) on the fly, with the goal of better support the driver in case of a disengagement.
SP  - 335
EP  - 349
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_23
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Braud, Tristan; Zhou, Pengyuan; Wang, Lin; Xu, Dianlei; Lin, Zijun; Kumar, Abhishek; Bermejo, Carlos; Hui, Pan
TI  - All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda
PY  - 2021
AB  - Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.
SP  - NA
EP  - NA
JF  - arXiv: Computers and Society
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Han, Changyo; Takahashi, Ryo; Yahagi, Yuchi; Naemura, Takeshi
TI  - CHI Extended Abstracts - PneuModule: Using Inflatable Pin Arrays for Reconfigurable Physical Controls on Pressure-Sensitive Touch Surfaces
PY  - 2020
AB  - We present PneuModule, a tangible interface platform that enables users to reconfigure physical controls on pressure-sensitive touch surfaces using pneumatically-actuated inflatable pin arrays. PneuModule consists of a main module and extension modules. The main module is tracked on the touch surface and forwards continuous inputs from attached multiple extension modules to the touch surface. Extension modules have distinct mechanisms for user input, which pneumatically actuates the inflatable pins at the bottom of the main module through internal air pipes. The main module accepts multi-dimensional inputs since each pin is individually inflated by the corresponding air chamber. Also, since the extension modules are swappable and identifiable owing to the marker design, users can quickly customize the interface layout. We contribute to design details of inflatable pins and diverse pneumatic input control design examples for PneuModule. We also showcase the feasibility of PneuModule through a series of evaluations and interactive prototypes.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376838
ER  - 

TY  - JOUR
AU  - Kikuchi, Ryosuke; Yoshikawa, Sora; Jayaraman, Pradeep Kumar; Zheng, Jianmin; Maekawa, Takashi
TI  - Embedding QR codes onto B-spline surfaces for 3D printing
PY  - 2018
AB  - NA
SP  - 215
EP  - 223
JF  - Computer-Aided Design
VL  - 102
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2018.04.025
ER  - 

TY  - JOUR
AU  - Sassatelli, Lucile; Winckler, Marco; Fisichella, Thomas; Dezarnaud, Antoine; Lemaire, Julien; Aparicio-Pardo, Ramon; Trevisan, Daniela
TI  - New interactive strategies for virtual reality streaming in degraded context of use
PY  - 2020
AB  - Abstract Virtual reality videos are an important element in the range of immersive contents as they open new perspectives for story-telling, journalism or education. Accessing these immersive contents through Internet streaming is however much more difficult owing to the required data rates much higher than for regular videos. While current streaming strategies rely on video compression, in this paper we investigate a radically new stance: we posit that degrading the visual quality is not the only choice to reduce the required data rate, and not necessarily the best. Instead, we propose two new impairments, Virtual Walls (VWs) and Slow Downs (SDs), that change the way the user can interact with the 360∘ video in an environment with insufficient available bandwidth. User experiments with a double-stimulus approach show that, when triggered in proper time periods, these impairments are better perceived than visual quality degradation from video compression. We confirm with network simulations the usefulness of these new types of impairments: incorporated into a FoV-based adaptation, they can enable reduction in stalls and startup delay, and increase quality in FoV, even in the presence of substantial playback buffers.
SP  - 27
EP  - 41
JF  - Computers & Graphics
VL  - 86
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2019.10.005
ER  - 

TY  - JOUR
AU  - Jensen, Mads Møller; Thiel, Sarah-Kristin; Hoggan, Eve; Bødker, Susanne
TI  - Physical Versus Digital Sticky Notes in Collaborative Ideation
PY  - 2018
AB  - In this paper, we compare the use of physical and digital sticky notes in collaborative ideation. Inspired by a case study in a design company, we focus on a collaborative ideation task, which is often part of pair-wise brainstorming in design. For comparison and to focus on the different materiality, we developed a digital sticky notes setup designed to be as close to the physical setup as possible, not adding any advanced digital features, even though technology has reached a stage where more sophisticated use of digital sticky notes on digital boards is possible. In this paper, we present a study of ideation among pairs of experienced sticky note users. The ideation sessions were video recorded and analyzed to focus on how collaboration is supported across the two setups. Based on quantitative analyses of the participants’ interactions with the artefacts, talking patterns, position and attention during the sessions, we qualify how the differences and similarities between the 2 setups have an impact on note handling, ideation techniques, group dynamics and socio-spatial configuration, e.g. the use of the room, the boards and tables. We conclude that, while the physical setup seems more appropriate for creating notes and posting notes, the digital setup invites more note interaction. Nevertheless, we did not find significant differences in the ideation outcome (e.g., number of notes created) or how participants collaborated between the 2 setups. Hence, we argue that collaborative ideation can successfully be supported in a digital setup as well. Consequently, we believe that the next step in a technological setup is not an either or, but should bring the best of the two worlds together.
SP  - 609
EP  - 645
JF  - Computer Supported Cooperative Work (CSCW)
VL  - 27
IS  - 3
PB  - 
DO  - 10.1007/s10606-018-9325-1
ER  - 

TY  - NA
AU  - Jensen, Mads Møller; Rädle, Roman; Klokmose, Clemens Nylandsted; Bødker, Susanne
TI  - CHI - Remediating a Design Tool: Implications of Digitizing Sticky Notes
PY  - 2018
AB  - Sticky notes are ubiquitous in design processes because of their tangibility and ease of use. Yet, they have well-known limitations in professional design processes, as documentation and distribution are cumbersome at best. This paper compares the use of sticky notes in ideation with a remediated digital sticky notes setup. The paper contributes with a nuanced understanding of what happens when remediating a physical design tool into digital space, by emphasizing focus shifts and breakdowns caused by the technology, but also benefits and promises inherent in the digital media. Despite users' preference for creating physical notes, handling digital notes on boards was easier and the potential of proper documentation make the digital setup a possible alternative. While the analogy in our remediation supported a transfer of learned handling, the users' experiences across technological setups impact their use and understanding, yielding new concerns regarding cross-device transfer and collaboration.
SP  - 224
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173798
ER  - 

TY  - NA
AU  - Wu, Kai; Cheng, Zhanglin
TI  - RefAR: 3D Sketch-Based Modeling with In-situ References
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00108
ER  - 

TY  - NA
AU  - Nakayama, Ryosuke; Suzuki, Ryo; Nakamaru, Satoshi; Niiyama, Ryuma; Kawahara, Yoshihiro; Kakehi, Yasuaki
TI  - Conference on Designing Interactive Systems - MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction
PY  - 2019
AB  - We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO's hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO's unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.
SP  - 975
EP  - 986
JF  - Proceedings of the 2019 on Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3322276.3322337
ER  - 

TY  - JOUR
AU  - Kosch, Thomas; Matviienko, Andrii; Müller, Florian; Bersch, Jessica; Katins, Christopher; Schön, Dominik; Mühlhäuser, Max
TI  - NotiBike: Assessing Target Selection Techniques for Cyclist Notifications in Augmented Reality
PY  - 2022
AB  - <jats:p>Cyclists' attention is often compromised when interacting with notifications in traffic, hence increasing the likelihood of road accidents. To address this issue, we evaluate three notification interaction modalities and investigate their impact on the interaction performance while cycling: gaze-based Dwell Time, Gestures, and Manual And Gaze Input Cascaded (MAGIC) Pointing. In a user study (N=18), participants confirmed notifications in Augmented Reality (AR) using the three interaction modalities in a simulated biking scenario. We assessed the efficiency regarding reaction times, error rates, and perceived task load. Our results show significantly faster response times for MAGIC Pointing compared to Dwell Time and Gestures, while Dwell Time led to a significantly lower error rate compared to Gestures. Participants favored the MAGIC Pointing approach, supporting cyclists in AR selection tasks. Our research sets the boundaries for more comfortable and easier interaction with notifications and discusses implications for target selections in AR while cycling.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546732
ER  - 

TY  - NA
AU  - Liu, Luyang; Zhong, Ruiguang; Zhang, Wuyang; Liu, Yunxin; Zhang, Jiansong; Zhang, Lintao; Gruteser, Marco
TI  - MobiSys - Cutting the Cord: Designing a High-quality Untethered VR System with Low Latency Remote Rendering
PY  - 2018
AB  - This paper introduces an end-to-end untethered VR system design and open platform that can meet virtual reality latency and quality requirements at 4K resolution over a wireless link. High-quality VR systems generate graphics data at a data rate much higher than those supported by existing wireless-communication products such as Wi-Fi and 60GHz wireless communication. The necessary image encoding, makes it challenging to maintain the stringent VR latency requirements. To achieve the required latency, our system employs a Parallel Rendering and Streaming mechanism to reduce the add-on streaming latency, by pipelining the rendering, encoding, transmission and decoding procedures. Furthermore, we introduce a Remote VSync Driven Rendering technique to minimize display latency. To evaluate the system, we implement an end-to-end remote rendering platform on commodity hardware over a 60Ghz wireless network. Results show that the system can support current 2160x1200 VR resolution at 90Hz with less than 16ms end-to-end latency, and 4K resolution with 20ms latency, while keeping a visually lossless image quality to the user.
SP  - 68
EP  - 80
JF  - Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3210240.3210313
ER  - 

TY  - NA
AU  - Park, Keunwoo; Kim, Daehwa; Heo, Seongkook; Lee, Geehyuk
TI  - CHI - MagTouch: Robust Finger Identification for a Smartwatch Using a Magnet Ring and a Built-in Magnetometer
PY  - 2020
AB  - Completing tasks on smartwatches often requires multiple gestures due to the small size of the touchscreens and the lack of sufficient number of touch controls that are easily accessible with a finger. We propose to increase the number of functions that can be triggered with the touch gesture by enabling a smartwatch to identify which finger is being used. We developed MagTouch, a method that uses a magnetometer embedded in an off-the-shelf smartwatch. It measures the magnetic field of a magnet fixed to a ring worn on the middle finger. By combining the measured magnetic field and the touch location on the screen, MagTouch recognizes which finger is being used. The tests demonstrated that MagTouch can differentiate among the three fingers used to make contacts at a success rate of 95.03%.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376234
ER  - 

TY  - JOUR
AU  - Wang, Zeyu; Qiu, Shiyu; Chen, Qingyang; Trayan, Natallia; Ringlein, Alexander; Dorsey, Julie; Rushmeier, Holly
TI  - AniCode: Authoring Coded Artifacts for Network-Free Personalized Animations
PY  - 2019
AB  - Time-based media are used in applications ranging from demonstrating the operation of home appliances to explaining new scientific discoveries. However, creating effective time-based media is challenging. We introduce a new framework for authoring and consuming time-based media. An author encodes an animation in a printed code and affixes the code to an object. A consumer captures an image of the object through a mobile application, and the image together with the code is used to generate a video on their local device. Our system is designed to be low cost and easy to use. By not requiring an Internet connection to deliver the animation, the framework enhances privacy of the communication. By requiring the user to have a direct line-of-sight view of the object, the framework provides personalized animations that only decode in the intended context. Animation schemes in the system include 2D and 3D geometric transformations, color transformation, and annotation. We demonstrate the new framework with sample applications from a wide range of domains. We evaluate the ease of use and effectiveness of our system with a user study.
SP  - 885
EP  - 897
JF  - The Visual Computer
VL  - 35
IS  - 6
PB  - 
DO  - 10.1007/s00371-019-01681-y
ER  - 

TY  - JOUR
AU  - Sola, Antonella; Sai, Yilin; Trinchi, Adrian; Chu, Clement; Shen, Shirley; Chen, Shiping
TI  - How Can We Provide Additively Manufactured Parts with a Fingerprint? A Review of Tagging Strategies in Additive Manufacturing.
PY  - 2021
AB  - Additive manufacturing (AM) is rapidly evolving from "rapid prototyping" to "industrial production". AM enables the fabrication of bespoke components with complicated geometries in the high-performance areas of aerospace, defence and biomedicine. Providing AM parts with a tagging feature that allows them to be identified like a fingerprint can be crucial for logistics, certification and anti-counterfeiting purposes. Whereas the implementation of an overarching strategy for the complete traceability of AM components downstream from designer to end user is, by nature, a cross-disciplinary task that involves legal, digital and technological issues, materials engineers are on the front line of research to understand what kind of tag is preferred for each kind of object and how existing materials and 3D printing hardware should be synergistically modified to create such tag. This review provides a critical analysis of the main requirements and properties of tagging features for authentication and identification of AM parts, of the strategies that have been put in place so far, and of the future challenges that are emerging to make these systems efficient and suitable for digitalisation. It is envisaged that this literature survey will help scientists and developers answer the challenging question: "How can we embed a tagging feature in an AM part?".
SP  - 85
EP  - 85
JF  - Materials (Basel, Switzerland)
VL  - 15
IS  - 1
PB  - 
DO  - 10.3390/ma15010085
ER  - 

TY  - JOUR
AU  - Andersen, Leif; Ballantyne, Michael; Felleisen, Matthias
TI  - Adding interactive visual syntax to textual code
PY  - 2020
AB  - Many programming problems call for turning geometrical thoughts into code: tables, hierarchical structures, nests of objects, trees, forests, graphs, and so on. Linear text does not do justice to such thoughts. But, it has been the dominant programming medium for the past and will remain so for the foreseeable future. This paper proposes a novel mechanism for conveniently extending textual programming languages with problem-specific visual syntax. It argues the necessity of this language feature, demonstrates the feasibility with a robust prototype, and sketches a design plan for adapting the idea to other languages.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - OOPSLA
PB  - 
DO  - 10.1145/3428290
ER  - 

TY  - NA
AU  - Liang, Rong-Hao; Hsieh, Meng-Ju; Ke, Jheng-You; Guo, Jr-Ling; Chen, Bing-Yu
TI  - UIST - RFIMatch: Distributed Batteryless Near-Field Identification Using RFID-Tagged Magnet-Biased Reed Switches
PY  - 2018
AB  - This paper presents a technique enabling distributed batteryless near-field identification (ID) between two passive radio frequency ID (RFID) tags. Each conventional ultra-high-frequency (UHF) RFID tag is modified by connecting its antenna and chip to a reed switch and then attaching a magnet to one of the reed switch's terminals, thus transforming it into an always-on switch. When the two modules approach each other, the magnets counteract each other and turn off both switches at the same time. The coabsence of IDs thus indicates a unique interaction event. In addition to sensing, the module also provides native haptic feedback through magnetic repulsion force, enabling users to perceive the system's state eyes-free, without physical constraints. Additional visual feedback can be provided through an energy-harvesting module and a light emitting diode. This specific hardware design supports contactless, orientation-invariant sensing, with a form factor compact enough for embedded and wearable use in ubiquitous computing applications.
SP  - 473
EP  - 483
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242620
ER  - 

TY  - NA
AU  - Zhu, Zhengzhe; Liu, Ziyi; Wang, Tianyi; Zhang, Youyou; Qian, Xun; Raja, Pashin Farsak; Villanueva, Ana; Ramani, Karthik
TI  - MechARspace: An Authoring System Enabling Bidirectional Binding of Augmented Reality with Toys in Real-time
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545668
ER  - 

TY  - NA
AU  - Krause, Markus; Schiöberg, Doris; Smeddinck, Jan David
TI  - CHI Extended Abstracts - Mooqita: Empowering Hidden Talents with a Novel Work-Learn Model
PY  - 2018
AB  - We present a case study of Mooqita, a platform to support learners in online courses by enabling them to earn money, gather real job task experiences, and build a meaningful portfolio. This includes placing optional additional assignments in online courses. Learners solve these individual assignments, provide peer reviews for other learners, and give feedback on each review they receive. Based on these data points teams are selected to work on a final paid assignment. Companies offer these assignments and in return receive interview recommendations from the pool of learners together with solutions for their challenges. We report the results of a pilot deployment in an online programming course offered by UC BerkeleyX. Six learners out of 158 participants were selected for the paid group assignment paying $600 per person. Four of these six were invited for interviews at the participating companies Crowdbotics (2) and Telefonica Innovation Alpha (2).
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3174351
ER  - 

TY  - NA
AU  - Albaugh, Lea; McCann, James; Hudson, Scott E.; Yao, Lining
TI  - CHI - Engineering Multifunctional Spacer Fabrics Through Machine Knitting
PY  - 2021
AB  - Machine knitting is an increasingly accessible fabrication technology for producing custom soft goods. However, recent machine knitting research has focused on knit shaping, or on adapting hand-knitting patterns. We explore a capability unique to machine knitting: producing multilayer spacer fabrics. These fabrics consist of two face layers connected by a monofilament filler yarn which gives the structure stiffness and volume. We show how to vary knit patterning and yarn parameters in spacer fabrics to produce tactile materials with embedded functionality for forming soft actuated mechanisms and sensors with tunable density, stiffness, material bias, and bristle properties. These soft mechanisms can be rapidly produced on a computationally-controlled v-bed knitting machine and integrated directly into soft objects.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445564
ER  - 

TY  - JOUR
AU  - Li, Jianping Kelvin; Xu, Shenyu; Ye, Yecong; Ma, Kwan-Liu
TI  - Resolving Conflicting Insights in Asynchronous Collaborative Visual Analysis
PY  - 2020
AB  - NA
SP  - 497
EP  - 509
JF  - Computer Graphics Forum
VL  - 39
IS  - 3
PB  - 
DO  - 10.1111/cgf.13997
ER  - 

TY  - NA
AU  - Garza, Jorge; Merrill, Devon J.; Swanson, Steven
TI  - CHI - Appliancizer: Transforming Web Pages into Electronic Devices
PY  - 2021
AB  - Prototyping electronic devices that meet today’s consumer standards is a time-consuming task that requires multi-domain expertise. Consumers expect electronic devices to have visually appealing interfaces with both tactile and screen-based interfaces. Appliancizer, our interactive computational design tool, exploits the similarities between graphical and tangible interfaces, allowing web pages to be rapidly transformed into physical electronic devices. Using a novel technique we call essential interface mapping, our tool converts graphical user interface elements (e.g., an HTML button) into tangible interface components (e.g., a physical button) without changing the application source code. Appliancizer automatically generates the PCB and low-level code from web-based prototypes and HTML mock-ups. This makes the prototyping of mixed graphical-tangible interactions as easy as modifying a web page and allows designers to leverage the well-developed ecosystem of web technologies. We demonstrate how our technique simplifies and accelerates prototyping by developing two devices with Appliancizer.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445732
ER  - 

TY  - NA
AU  - Brunvand, Erik
TI  - ACM Great Lakes Symposium on VLSI - Extending Student Labs with SMT Circuit Implementation
PY  - 2019
AB  - Computer Science and Computer Engineering classes related to digital circuits, embedded systems, Human Computer Interaction (HCI), and a wide variety of "maker" subjects, would often like to include physical computing projects. Extending these physical computing ideas to physical realization of circuits is the next logical step, and has traditionally happened with low-cost non-soldered prototyping such as breadboards. However, many modern circuit components are now available only in tiny surface-mount technology (SMT) packages. Using these components essentially requires that students are able to design printed circuit boards (PCBs) specifically for their module. In this paper we describe an experience report of how we enhanced a student lab in a cost-effective way to enable students to develop, assemble, and solder custom PCBs that contain SMT components. This allows modern circuit components to be used, offers an enriched hands-on experience, and enables an expansion of the scope and complexity of student projects.
SP  - 231
EP  - 236
JF  - Proceedings of the 2019 on Great Lakes Symposium on VLSI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3299874.3317968
ER  - 

TY  - NA
AU  - Schmitz, Martin; Stitz, Martin; Müller, Florian; Funk, Markus; Mühlhäuser, Max
TI  - CHI - ./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects
PY  - 2019
AB  - Hover, touch, and force are promising input modalities that get increasingly integrated into screens and everyday objects. However, these interactions are often limited to flat surfaces and the integration of suitable sensors is time-consuming and costly. To alleviate these limitations, we contribute Trilaterate: A fabrication pipeline to 3D print custom objects that detect the 3D position of a finger hovering, touching, or forcing them by combining multiple capacitance measurements via capacitive trilateration. Trilaterate places and routes actively-shielded sensors inside the object and operates on consumer-level 3D printers. We present technical evaluations and example applications that validate and demonstrate the wide applicability of Trilaterate.
SP  - 454
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300684
ER  - 

TY  - CHAP
AU  - Whitlock, Matt; Mitchell, Jake; Pfeufer, Nick; Arnot, Brad; Craig, Ryan; Wilson, Bryce; Chung, Brian; Szafir, Danielle Albers
TI  - HCI (10) - MRCAT: In Situ Prototyping of Interactive AR Environments
PY  - 2020
AB  - Augmented reality (AR) blends physical and virtual components to create a mixed reality experience. This unique display medium presents new opportunities for application design, as applications can move beyond the desktop and integrate with the physical environment. In order to build effective applications for AR displays, we need to be able to iteratively design for different contexts or scenarios. We present MRCAT (Mixed Reality Content Authoring Toolkit), a tool for in situ prototyping of mixed reality environments. We discuss the initial design of MRCAT and iteration after a study (\(N = 14\)) to evaluate users’ abilities to craft AR applications with MRCAT and with a 2D prototyping tool. We contextualize our system in a case study of museum exhibit development, identifying how existing ideation and prototyping workflows could be bolstered with the approach offered by MRCAT. With our exploration of in situ prototyping, we enumerate key aspects both of AR application design and targeted domains that help guide design of more effective AR prototyping tools.
SP  - 235
EP  - 255
JF  - Virtual, Augmented and Mixed Reality. Design and Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49695-1_16
ER  - 

TY  - NA
AU  - Badami, Maisie; Baez, Marcos; Zamanirad, Shayan; Kang, Wei
TI  - On how Cognitive Computing will plan your next Systematic Review
PY  - 2020
AB  - Systematic literature reviews (SLRs) are at the heart of evidence-based research, setting the foundation for future research and practice. However, producing good quality timely contributions is a challenging and highly cognitive endeavor, which has lately motivated the exploration of automation and support in the SLR process. In this paper we address an often overlooked phase in this process, that of planning literature reviews, and explore under the lenses of cognitive process augmentation how to overcome its most salient challenges. In doing so, we report on the insights from 24 SLR authors on planning practices, its challenges as well as feedback on support strategies inspired by recent advances in cognitive computing. We frame our findings under the cognitive augmentation framework, and report on a prototype implementation and evaluation focusing on further informing the technical feasibility.
SP  - NA
EP  - NA
JF  - arXiv: Digital Libraries
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Zhenning; Pan, Zhigeng; Li, Weiqing; Su, Zhiyong
TI  - Imitative Collaboration: A mirror-neuron inspired mixed reality collaboration method with remote hands and local replicas
PY  - 2022
AB  - NA
SP  - 103600
EP  - 103600
JF  - Journal of Visual Communication and Image Representation
VL  - 88
IS  - NA
PB  - 
DO  - 10.1016/j.jvcir.2022.103600
ER  - 

TY  - NA
AU  - Watanabe, Keisuke; Yamamura, Ryosuke; Kakehi, Yasuaki
TI  - CHI Extended Abstracts - foamin: A Deformable Sensor for Multimodal Inputs Based on Conductive Foam with a Single Wire
PY  - 2021
AB  - Soft sensors made of deformable materials, that are capable of sensing touches or gestures, have attracted considerable attention for use in tangible interfaces or soft robotics. However, to achieve multimodal gesture detection with soft sensors, prior studies have combined multiple sensors or utilized complex configurations with multiple wires. To achieve multimodal gesture sensing with a simpler configuration, a novel soft sensor consisting of a conductive foam with a single wire, was proposed in this study. This sensor was named as foamin and utilizes an impedance measurement technique at multiple frequencies called foamin. Additionally, a surface-shielding method was designed for improving the detection performance of the sensor. Several patterns of foamin were implemented to investigate the detection accuracy and three application scenarios based on the sensor were proposed.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451547
ER  - 

TY  - BOOK
AU  - Burks, Andrew; Renambot, Luc; Johnson, Andrew
TI  - PEARC - VisSnippets: A Web-Based System for Impromptu Collaborative Data Exploration on Large Displays
PY  - 2020
AB  - The VisSnippets system is designed to facilitate effective collaborative data exploration. VisSnippets leverages SAGE2 middleware that enables users to manage the display of digital media content on large displays, thereby providing collaborators with a high-resolution common workspace. Based in JavaScript, VisSnippets provides users with the flexibility to implement and/or select visualization packages and to quickly access data in the cloud. By simplifying the development process, VisSnippets removes the need to scaffold and integrate interactive visualization applications by hand. Users write reusable blocks of code called “snippets” for data retrieval, transformation, and visualization. By composing dataflows from the group’s collective snippet pool, users can quickly execute and explore complementary or contrasting analyses. By giving users the ability to explore alternative scenarios, VisSnippets facilitates parallel work for collaborative data exploration leveraging large-scale displays. We describe the system, its design and implementation, and showcase its flexibility through two example applications.
SP  - 144
EP  - 151
JF  - Practice and Experience in Advanced Research Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3311790.3396666
ER  - 

TY  - CHAP
AU  - Nishino, Ko; Subpa-asa, Art; Asano, Yuta; Shimano, Mihoko; Sato, Imari
TI  - Variable Ring Light Imaging: Capturing Transient Subsurface Scattering with an Ordinary Camera
PY  - 2018
AB  - NA
SP  - 624
EP  - 639
JF  - Computer Vision – ECCV 2018
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01252-6_37
ER  - 

TY  - NA
AU  - Pourmemar, Majid; Poullis, Charalambos
TI  - VRCAI - Visualizing and Interacting with Hierarchical Menus in Immersive Augmented Reality
PY  - 2019
AB  - Graphical User Interfaces (GUIs) have long been used as a way to inform the user of the large number of available actions and options. GUIs in desktop applications traditionally appear in the form of two-dimensional hierarchical menus due to the limited screen real estate, the spatial restrictions imposed by the hardware e.g. 2D, and the available input modalities e.g. mouse/keyboard point-and-click, touch, dwell-time etc. In immersive Augmented Reality (AR), there are no such restrictions and the available input modalities are different (i.e. hand gestures, head pointing or voice recognition), yet the majority of the applications in AR still use the same type of GUIs as with desktop applications. In this paper we focus on identifying the most efficient combination of (hierarchical menu type, input modality) to use in immersive applications using AR headsets. We report on the results of a within-subjects study with 25 participants who performed a number of tasks using four combinations of the most popular hierarchical menu types with the most popular input modalities in AR, namely: (drop-down menu, hand gestures), (drop-down menu, voice), (radial menu, hand gestures), and (radial menu, head pointing). Results show that the majority of the participants (60%, 15) achieved a faster performance using the hierarchical radial menu with head pointing control. Furthermore, the participants clearly indicated the radial menu with head pointing control as the most preferred interaction technique due to the limited physical demand as opposed to the current de facto interaction technique in AR i.e. hand gestures, which after prolonged use becomes physically demanding leading to arm fatigue known as ’Gorilla arms’.
SP  - NA
EP  - NA
JF  - Proceedings of the 17th International Conference on Virtual-Reality Continuum and its Applications in Industry
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359997.3365693
ER  - 

TY  - JOUR
AU  - Cha, Seunghoon; Lee, Jungjin; Jeong, Seunghwa; Kim, Younghui; Noh, Junyong
TI  - Enhanced Interactive 360° Viewing via Automatic Guidance
PY  - 2020
AB  - We present a new interactive playback method to enhance 360° viewing experiences. Our method automatically rotates the virtual camera of a 360° panoramic video (360° video) player during interactive viewing to guide the viewer through the most important regions of the video. With this method, the viewer can watch a 360° video with minimum efforts to find important events in a scene both in interactive (e.g., HMD) and less-interactive (e.g., PC and TV) viewing environments. To estimate the importance of each viewing direction, we combine spatial and temporal saliency with cluster-based weighting. A maximum backward cumulative importance volume (MBCIV) is then constructed by accumulating this importance in the video space. During playback, which uses a forward tracing scheme through the MBCIV, the initial optimal path is found based on the viewer’s viewing direction. A smooth path is then derived using penalized curve fitting. Finally, the virtual camera is rotated to follow the path. The experiments and user studies demonstrate that our method allows the viewer to effectively enjoy 360° videos with minimum interaction efforts, or even through a non-interactive display.
SP  - 1
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 5
PB  - 
DO  - 10.1145/3183794
ER  - 

TY  - JOUR
AU  - Rafael, Ballagas; Ghosh, Sarthak; Landay, James A.
TI  - The Design Space of 3D Printable Interactivity
PY  - 2018
AB  - The capabilities of 3D printers are rapidly progressing towards fabrication of fully interactive products. For designers to reason about the best way to achieve their interaction design goals, it is helpful to not only know what exists in the literature, but to also understand the design space of options. Such an understanding can help in comparison, analysis, selection of suitable technology, and also in the generation of new ideas. In this article, we survey the state of the art in 3D printing fully functional sensors and actuators to support explicit interaction techniques. We classify and organize the surveyed works around the following parameters: mechanism, designed affordances, interaction primitives, and output modality. The design space is presented in the form of a multidimensional matrix known as a Zwicky box. Using the tables, we can make observations about the existing literature and also identify gaps in this design space. Many such gaps can potentially lead to exciting new research or engineering opportunities.
SP  - 61
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 2
PB  - 
DO  - 10.1145/3214264
ER  - 

TY  - JOUR
AU  - Yang, Yuan; Zhou, Wenbo; Chen, Xuyang; Ye, Jinhua; Wu, Haibin
TI  - A flexible touching sensor with the variation of electrical impedance distribution
PY  - 2021
AB  - NA
SP  - 109778
EP  - NA
JF  - Measurement
VL  - 183
IS  - NA
PB  - 
DO  - 10.1016/j.measurement.2021.109778
ER  - 

TY  - JOUR
AU  - Maia, Henrique Teles; Li, Dingzeyu; Yang, Yuan; Zheng, Changxi
TI  - LayerCode: optical barcodes for 3D printed shapes
PY  - 2019
AB  - With the advance of personal and customized fabrication techniques, the capability to embed information in physical objects becomes evermore crucial. We present LayerCode, a tagging scheme that embeds a carefully designed barcode pattern in 3D printed objects as a deliberate byproduct of the 3D printing process. The LayerCode concept is inspired by the structural resemblance between the parallel black and white bars of the standard barcode and the universal layer-by-layer approach of 3D printing. We introduce an encoding algorithm that enables the 3D printing layers to carry information without altering the object geometry. We also introduce a decoding algorithm that reads the LayerCode tag of a physical object by just taking a photo. The physical deployment of LayerCode tags is realized on various types of 3D printers, including Fused Deposition Modeling printers as well as Stereolithography based printers. Each offers its own advantages and tradeoffs. We show that LayerCode tags can work on complex, nontrivial shapes, on which all previous tagging mechanisms may fail. To evaluate LayerCode thoroughly, we further stress test it with a large dataset of complex shapes using virtual rendering. Among 4,835 tested shapes, we successfully encode and decode on more than 99% of the shapes.
SP  - 112
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 38
IS  - 4
PB  - 
DO  - 10.1145/3306346.3322960
ER  - 

TY  - NA
AU  - d'Eon, Gregory
TI  - Applying Fair Reward Divisions to Collaborative Work
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chi, Peggy; Frey, Nathan; Panovich, Katrina; Essa, Irfan
TI  - UIST - Automatic Instructional Video Creation from a Markdown-Formatted Tutorial
PY  - 2021
AB  - We introduce HowToCut, an automatic approach that converts a Markdown-formatted tutorial into an interactive video that presents the visual instructions with a synthesized voiceover for narration. HowToCut extracts instructional content from a multimedia document that describes a step-by-step procedure. Our method selects and converts text instructions to a voiceover. It makes automatic editing decisions to align the narration with edited visual assets, including step images, videos, and text overlays. We derive our video editing strategies from an analysis of 125 web tutorials and apply Computer Vision techniques to the assets. To enable viewers to interactively navigate the tutorial, HowToCut’s conversational UI presents instructions in multiple formats upon user commands. We evaluated our automatically-generated video tutorials through user studies (N=20) and validated the video quality via an online survey (N=93). The evaluation shows that our method was able to effectively create informative and useful instructional videos from a web tutorial document for both reviewing and following.
SP  - 677
EP  - 690
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474778
ER  - 

TY  - JOUR
AU  - Chen, Taizhou; Xu, Lantian; Zhu, Kening
TI  - FritzBot: A data-driven conversational agent for physical-computing system design
PY  - 2021
AB  - NA
SP  - 102699
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 155
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2021.102699
ER  - 

TY  - NA
AU  - Oppenlaender, Jonas; Shireen, Naghmi; Mackeprang, Maximilian; Erhan, Halil; Goncalves, Jorge; Hosio, Simo
TI  - Creativity &amp; Cognition - Crowd-powered Interfaces for Creative Design Thinking
PY  - 2019
AB  - Crowdsourcing is a powerful approach for tapping into the collective insights of diverse crowds. Thus, crowdsourcing has potential to support designers in making sense of a design space. In this hands-on workshop, we will brainstorm and conceptualise new user interfaces and crowdsourcing systems for supporting designers in the design process. The workshop consists of developmental discussions of ideas contributed by the participants. In brainstorming and design sessions in groups, the participants will ideate new crowd-powered systems and user interfaces that support the designer's divergent and convergent thinking.
SP  - 722
EP  - 729
JF  - Proceedings of the 2019 on Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3325480.3326553
ER  - 

TY  - NA
AU  - Conlen, Matthew; Vo, Megan; Tan, Alan; Heer, Jeffrey
TI  - UIST - Idyll Studio: A Structured Editor for Authoring Interactive & Data-Driven Articles
PY  - 2021
AB  - Interactive articles are an effective medium of communication in education, journalism, and scientific publishing, yet are created using complex general-purpose programming tools. We present Idyll Studio, a structured editor for authoring and publishing interactive and data-driven articles. We extend the Idyll framework to support reflective documents, which can inspect and modify their underlying program at runtime, and show how this functionality can be used to reify the constituent parts of a reactive document model—components, text, state, and styles—in an expressive, interoperable, and easy-to-learn graphical interface. In a study with 18 diverse participants, all could perform basic editing and composition, use datasets and variables, and specify relationships between components. Most could choreograph interactive visualizations and dynamic text, although some struggled with advanced uses requiring unstructured code editing. Our findings suggest Idyll Studio lowers the threshold for non-experts to create interactive articles and allows experts to rapidly specify a wide range of article designs.
SP  - 1
EP  - 12
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474731
ER  - 

TY  - NA
AU  - Iyer, Vikram; Chan, Justin; Culhane, Ian; Mankoff, Jennifer; Gollakota, Shyamnath
TI  - UIST - Wireless Analytics for 3D Printed Objects
PY  - 2018
AB  - We present the first wireless physical analytics system for 3D printed objects using commonly available conductive plastic filaments. Our design can enable various data capture and wireless physical analytics capabilities for 3D printed objects, without the need for electronics. To achieve this goal, we make three key contributions: (1) demonstrate room scale backscatter communication and sensing using conductive plastic filaments, (2) introduce the first backscatter designs that detect a variety of bi-directional motions and support linear and rotational movements, and (3) enable data capture and storage for later retrieval when outside the range of the wireless coverage, using a ratchet and gear system. We validate our approach by wirelessly detecting the opening and closing of a pill bottle, capturing the joint angles of a 3D printed e-NABLE prosthetic hand, and an insulin pen that can store information to track its use outside the range of a wireless receiver.
SP  - 141
EP  - 152
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242639
ER  - 

TY  - NA
AU  - Jeong, Yunwoo; Kim, Han-Jong; Yun, Gyeongwon; Nam, Tek-Jin
TI  - UIST - WIKA: A Projected Augmented Reality Workbench for Interactive Kinetic Art
PY  - 2020
AB  - Iterative artistic exploration, mechanism building, and interaction programming are essential processes of prototyping interactive kinetic art (IKA). However, scattered tools and interwoven workflows across digital and physical worlds make the task difficult. We present WIKA, an integrated environment supporting the whole creation process of IKA in the form of a layered picture frame in a single workspace. A projected AR system with a mobile device efficiently makes an interactive tabletop. The projected information connected with physical components (e.g. sensors and motors) enables the programming and simulation on the workspace. Physical components are applied from the initial phase of prototyping using an AR plate, and this supports the iterative trial-and-error process by bridging the workflow. A user study shows that WIKA enabled non-experts to create diverse IKA with their ideas. A tangible interaction and projected information enable the iterative and rapid creation. The method that integrates the hardware and software in the physical environment can be applied to other prototyping tools that support the creation of interactive and kinetic elements.
SP  - 999
EP  - 1009
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415880
ER  - 

TY  - NA
AU  - Hsieh, Meng-Ju; Guo, Jr-Ling; Lu, Chin-Yuan; Hsieh, Han-Wei; Liang, Rong-Hao; Chen, Bing-Yu
TI  - UIST - RFTouchPads: Batteryless and Wireless Modular Touch Sensor Pads Based on RFID
PY  - 2019
AB  - This paper presents RFTouchPads, a system of batteryless and wireless modular hardware designs of two-dimensional (2D) touch sensor pads based on the ultra-high frequency (UHF) radio-frequency identification (RFID) technology. In this system, multiple RFID IC chips are connected to an antenna in parallel. Each chip connects only one of its endpoints to the antenna; hence, the module normally turns off when it gets insufficient energy to operate. When a finger touches the circuit trace attached to another endpoint of the chip, the finger functions as part of the antenna that turns the connected chip on, while the finger touch location is determined according to the chip's ID. Based on this principle, we propose two hardware designs, namely, StickerPad and TilePad. StickerPad is a flexible 3×3 touch-sensing pad suitable for applications on curved surfaces such as the human body. TilePad is a modular 3×3 touch-sensing pad that supports the modular area expansion by tiling and provides a more flexible deployment because its antenna is folded. Our implementation allows 2D touch inputs to be reliability detected 2 m away from a remote antenna of an RFID reader. The proposed batteryless, wireless, and modular hardware design enables fine-grained and less-constrained 2D touch inputs in various ubiquitous computing applications.
SP  - 999
EP  - 1011
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347910
ER  - 

TY  - CHAP
AU  - Kalantari, Farzan; Gueorguiev, David; Lank, Edward; Bremard, Nicolas; Grisoni, Laurent
TI  - EuroHaptics (1) - Exploring Fingers’ Limitation of Texture Density Perception on Ultrasonic Haptic Displays
PY  - 2018
AB  - Recent research in haptic feedback is motivated by the crucial role that tactile perception plays in everyday touch interactions. In this paper, we describe psychophysical experiments to investigate the perceptual threshold of individual fingers on both the right and left hand of right-handed participants using active dynamic touch for spatial period discrimination of both sinusoidal and square-wave gratings on ultrasonic haptic touchscreens. Both one-finger and multi-finger touch were studied and compared. Our results indicate that users’ finger identity (index finger, middle finger, etc.) significantly affect the perception of both gratings in the case of one-finger exploration. We show that index finger and thumb are the most sensitive in all conditions whereas little finger followed by ring are the least sensitive for haptic perception. For multi-finger exploration, the right hand was found to be more sensitive than the left hand for both gratings. Our findings also demonstrate similar perception sensitivity between multi-finger exploration and the index finger of users’ right hands (i.e. dominant hand in our study), while significant difference was found between single and multi-finger perception sensitivity for the left hand.
SP  - 354
EP  - 365
JF  - Haptics: Science, Technology, and Applications
VL  - 10893
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-93445-7_31
ER  - 

TY  - NA
AU  - Horak, Tom; Mathisen, Andreas; Klokmose, Clemens Nylandsted; Dachselt, Raimund; Elmqvist, Niklas
TI  - CHI - Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups
PY  - 2019
AB  - We present Vistribute, a framework for the automatic distribution of visualizations and UI components across multiple heterogeneous devices. Our framework consists of three parts: (i) a design space considering properties and relationships of interactive visualizations, devices, and user preferences in multi-display environments; (ii) specific heuristics incorporating these dimensions for guiding the distribution for a given interface and device ensemble; and (iii) a web-based implementation instantiating these heuristics to automatically generate a distribution as well as providing interaction mechanisms for user-defined adaptations. In contrast to existing UI distribution systems, we are able to infer all required information by analyzing the visualizations and devices without relying on additional input provided by users or programmers. In a qualitative study, we let experts create their own distributions and rate both other manual distributions and our automatic ones. We found that all distributions provided comparable quality, hence validating our framework.
SP  - 616
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300846
ER  - 

TY  - NA
AU  - Barrera-Machuca, Mayra D.; Cassinelli, Alvaro; Sandor, Christian
TI  - UIST (Adjunct Volume) - Context-Based 3D Grids for Augmented Reality User Interfaces
PY  - 2020
AB  - Accurate 3D registration of real and virtual objects is a crucial step in AR, especially when manipulating those objects in space. Previous work simplifies mid-air 3D manipulations by removing one or more degrees of freedom by constraining motion using automatic algorithms. However, when designing objects, limiting the user's actions can affect their creativity. To solve this problem, we present a new system called Context-based 3D Grids that allows users to do precise mid-air 3D manipulations without constraining their actions. Our system creates 3D grids for each object in the scene that change depending on the object pose. Users can display additional reference frames inside the virtual environment using natural hand gestures that are commonly used when designing an object. Our goal is to help users visualize more clearly the spatial relation and the differences in pose and size of the objects.
SP  - 73
EP  - 76
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416190
ER  - 

TY  - JOUR
AU  - Griffin, Ruairi; Langlotz, Tobias; Zollmann, Stefanie
TI  - 6DIVE: 6 Degrees-of-Freedom Immersive Video Editor
PY  - 2021
AB  - Editing 6DoF videos using standard video editing tools is challenging, especially for non-expert users. There is a large gap between the 2D interface used for traditional video editing and the immersive VR environment used for replay. In this paper, we present 6DIVE, a 6 degrees-of-freedom (DoF) immersive video editor. 6DIVE allows users to edit these 6DoF videos directly in an immersive VR environment. In this work, we explored options for a timeline representation as well as UI placement suitable for immersive video editing. We used our prototypical implementation of an immersive video editor to conduct a user study to analyze the feasibility and usability of immersive 6DoF editing. We compared 6DIVE to a desktop-based implementation of a VR video editor. Our initial results suggest that 6DIVE allows even non-expert users to perform basic editing operations on videos in VR. While we did not find any statistically significant differences for the workload between the VR and the desktop interface, we found a statistically significant difference in user preference, with a preference for the VR interface. We also found higher ratings for the user experience metrics in VR captured by the user experience questionnaire.
SP  - 676895
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 2
IS  - NA
PB  - 
DO  - 10.3389/frvir.2021.676895
ER  - 

TY  - JOUR
AU  - Yan, Yukang; Yu, Chun; Yi, Xin; Shi, Yuanchun
TI  - HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices
PY  - 2018
AB  - We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.
SP  - 198
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287076
ER  - 

TY  - BOOK
AU  - George, Ceenu; Buschek, Daniel; Ngao, Andrea; Khamis, Mohamed
TI  - AVR (1) - GazeRoomLock: Using Gaze and Head-Pose to Improve the Usability and Observation Resistance of 3D Passwords in Virtual Reality
PY  - 2020
AB  - Authentication has become an important component of Immersive Virtual Reality (IVR) applications, such as virtual shopping stores, social networks, and games. Recent work showed that compared to traditional graphical and alphanumeric passwords, a more promising form of passwords for IVR is 3D passwords. This work evaluates four multimodal techniques for entering 3D passwords in IVR that consist of multiple virtual objects selected in succession. Namely, we compare eye gaze and head pose for pointing, and dwell time and tactile input for selection. A comparison of a) usability in terms of entry time, error rate, and memorability, and b) resistance to real world and offline observations, reveals that: multimodal authentication in IVR by pointing at targets using gaze, and selecting them using a handheld controller significantly improves usability and security compared to the other methods and to prior work. We discuss how the choice of pointing and selection methods impacts the usability and security of 3D passwords in IVR.
SP  - 61
EP  - 81
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58465-8_5
ER  - 

TY  - CHAP
AU  - Nakamura, Takuto; Shizuki, Buntarou
TI  - HCI (2) - Identification Method of Digits for Expanding Touchpad Input
PY  - 2020
AB  - A method is presented for identifying the digits, i.e., the thumb and/or finger(s), that touch a touchpad as a means to expand the input vocabulary available on a touchpad. It will enable application designers to assign different commands to touch gestures performed with the same number of digits and the same movement but with different digits. No additional sensors are required for identification; instead the digits are identified on the basis of machine learning using only data acquired from a mutual-capacitance touchpad as learning data. Experimental results revealed an average identification accuracy of 86.3%.
SP  - 463
EP  - 474
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_31
ER  - 

TY  - NA
AU  - Berman, Alexander; Thakare, Ketan; Howell, Joshua; Quek, Francis; Kim, Jeeeun
TI  - IUI - HowDIY: Towards Meta-Design Tools to Support Anyone to 3D Print Anywhere
PY  - 2021
AB  - The promise of anyone being able to 3D print anywhere relies on both technological advances and incremental shifts in social organizations to trigger changes in human behavior. While much research has focused on how people learn aspects of predefined printing processes, such as expressively utilizing particular design-software (e.g. CAD) and fabrication-machinery (e.g. 3D Printers), this work explores how anyone may gain an understanding of what can be 3D printed through dynamic-processes in computationally-guided exploration of online resources and 3D printing facilities. Investigations surrounding online printing services reveal accessible 3D printing processes that do not require end-users to have experience with design-software or fabrication-machinery, only requiring end-users to specify printable ideas. We present these accessible printing processes alongside associated technologies in a meta-design framework for supporting end-users’ specification of 3D printing ideas. Informed by this framework and a series of formative studies, we designed the website HowDIY to introduce anyone to 3D printing by encouraging and facilitating the intelligent exploration of various online resources. HowDIY was deployed over several weeks with diverse newcomers to 3D printing, validating that intelligent user interfaces can support anyone to participate in the utilization and design of 3D printing tools and processes.
SP  - 491
EP  - 503
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450638
ER  - 

TY  - NA
AU  - Linxen, Sebastian; Sturm, Christian; Brühlmann, Florian; Cassau, Vincent; Opwis, Klaus; Reinecke, Katharina
TI  - CHI - How WEIRD is CHI
PY  - 2021
AB  - Computer technology is often designed in technology hubs in Western countries, invariably making it “WEIRD”, because it is based on the intuition, knowledge, and values of people who are Western, Educated, Industrialized, Rich, and Democratic. Developing technology that is universally useful and engaging requires knowledge about members of WEIRD and non-WEIRD societies alike. In other words, it requires us, the CHI community, to generate this knowledge by studying representative participant samples. To find out to what extent CHI participant samples are from Western societies, we analyzed papers published in the CHI proceedings between 2016-2020. Our findings show that 73% of CHI study findings are based on Western participant samples, representing less than 12% of the world’s population. Furthermore, we show that most participant samples at CHI tend to come from industrialized, rich, and democratic countries with generally highly educated populations. Encouragingly, recent years have seen a slight increase in non-Western samples and those that include several countries. We discuss suggestions for further broadening the international representation of CHI participant samples.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445488
ER  - 

TY  - NA
AU  - Li, Changjian; Pan, Hao; Bousseau, Adrien; Mitra, Niloy J.
TI  - Sketch2CAD: Sequential CAD Modeling by Sketching in Context
PY  - 2020
AB  - We present a sketch-based CAD modeling system, where users create objects incrementally by sketching the desired shape edits, which our system automatically translates to CAD operations. Our approach is motivated by the close similarities between the steps industrial designers follow to draw 3D shapes, and the operations CAD modeling systems offer to create similar shapes. To overcome the strong ambiguity with parsing 2D sketches, we observe that in a sketching sequence, each step makes sense and can be interpreted in the \emph{context} of what has been drawn before. In our system, this context corresponds to a partial CAD model, inferred in the previous steps, which we feed along with the input sketch to a deep neural network in charge of interpreting how the model should be modified by that sketch. Our deep network architecture then recognizes the intended CAD operation and segments the sketch accordingly, such that a subsequent optimization estimates the parameters of the operation that best fit the segmented sketch strokes. Since there exists no datasets of paired sketching and CAD modeling sequences, we train our system by generating synthetic sequences of CAD operations that we render as line drawings. We present a proof of concept realization of our algorithm supporting four frequently used CAD operations. Using our system, participants are able to quickly model a large and diverse set of objects, demonstrating Sketch2CAD to be an alternate way of interacting with current CAD modeling systems.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Takahashi, Ryo; Fukumoto, Masaaki; Han, Changyo; Sasatani, Takuya; Narusue, Yoshiaki; Kawahara, Yoshihiro
TI  - UIST - TelemetRing: A Batteryless and Wireless Ring-shaped Keyboard using Passive Inductive Telemetry
PY  - 2020
AB  - TelemetRing is a batteryless and wireless ring-shaped keyboard that supports command and text entry in daily lives by detecting finger typing on various surfaces. The proposed inductive telemetry approach eliminates bulky batteries or capacitors from the ring part. Each ring consists of a sensor coil (the ring part itself), 1-DoF piezoelectric accelerometer, and varactor diode; moreover, it has different resonant frequencies. Typing shocks slightly shift the resonant frequency, and these are detected by a wrist-mounted readout coil. 5-bit chord keyboard is realized by attaching five sensor rings on five fingers. Our evaluation shows that the prototype achieved the tiny (6 g, 3.5 cm^3) ring sensor and 89.7% of typing detection ratio.
SP  - 1161
EP  - 1168
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415873
ER  - 

TY  - BOOK
AU  - Pazhayedath, Priyanka; Belchior, Pedro; Prates, Rafael; Silveira, Filipe; Lopes, Daniel Simões; Cools, Robbe; Esteves, Augusto; Simeone, Adalberto L.
TI  - VR Workshops - Exploring Bi-Directional Pinpointing Techniques for Cross-Reality Collaboration
PY  - 2021
AB  - Virtual Reality (VR) technology enables users to immerse themselves in artificial worlds. However, it isolates users from the outside world and impedes them from collaborating with other users who might be outside of the VR experience and vice-versa. We implemented two systems where we explore how such an external user in the real world can interact across realities with a user immersed in virtual reality, either locally or remotely, in order to to share pinpoint locations. In the first we investigate three cross-reality techniques for the external user to draw the attention of their VR counterpart on specific objects present in the virtual environment (Voice, Highlight, and Arrow). Participants performed better overall and preferred the Arrow technique, followed by the Highlight technique. In the second system we expand on these two techniques to explore an even starker cross-reality interaction between users in VR and users interacting via a tablet computer to direct each other to pinpoint objects in the scene. We adapted the previous two techniques and implemented two others (Vision cone, Pointing) that support bi-directional communication between users. When it comes to bi-directional pinpointing, VR users still showed preference for the Arrow technique (now described as Pointing in Giant mode), while mobile users were split between the Vision cone and the Highlight techniques.
SP  - 264
EP  - 270
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00055
ER  - 

TY  - NA
AU  - Larsen-Ledet, Ida; Borowski, Marcel
TI  - OZCHI - “It Looks Like You Don’t Agree”: Idiosyncratic Practices and Preferences in Collaborative Writing
PY  - 2020
AB  - This paper addresses collaborative writing in academia. Recent research has indicated that while many tools for collaborative writing exist and continue to be developed, co-writers frequently employ workarounds and cumbersome substitutions to accommodate their writing approaches and collaborative needs. As part of a process to address these issues, we conducted a co-design study on collaborative academic writing with 18 participants. The paper details a three-stage co-design approach developed for this purpose. During this three-stage workshop series, the participants discussed needs, frustrations, and desires in their experiences with collaborative writing. These discussions revealed how participants’ different ways of practicing and experiencing collaborative writing entail contrasting needs that are difficult to balance. Based on an analysis of discussions and artifacts from the workshops, we argue that researchers and designers should aim to support diverse practices and propose a protocol for examining and drawing on the contradictions that arise from co-writers’ idiosyncratic preferences.
SP  - 339
EP  - 354
JF  - 32nd Australian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441000.3441032
ER  - 

TY  - NA
AU  - Lykourentzou, Ioanna; Vinella, Federica Lucia; Ahmed, Faez; Papastathis, Costas; Papangelis, Konstantinos; Khan, Vassilis-Javed; Masthoff, Judith
TI  - Self-Organizing Teams in Online Work Settings.
PY  - 2021
AB  - As the volume and complexity of distributed online work increases, the collaboration among people who have never worked together in the past is becoming increasingly necessary. Recent research has proposed algorithms to maximize the performance of such teams by grouping workers according to a set of predefined decision criteria. This approach micro-manages workers, who have no say in the team formation process. Depriving users of control over who they will work with stifles creativity, causes psychological discomfort and results in less-than-optimal collaboration results. In this work, we propose an alternative model, called Self-Organizing Teams (SOTs), which relies on the crowd of online workers itself to organize into effective teams. Supported but not guided by an algorithm, SOTs are a new human-centered computational structure, which enables participants to control, correct and guide the output of their collaboration as a collective. Experimental results, comparing SOTs to two benchmarks that do not offer user agency over the collaboration, reveal that participants in the SOTs condition produce results of higher quality and report higher teamwork satisfaction. We also find that, similarly to machine learning-based self-organization, human SOTs exhibit emergent collective properties, including the presence of an objective function and the tendency to form more distinct clusters of compatible teammates.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Rule, Adam
TI  - Design and Use of Computational Notebooks
PY  - 2018
AB  - Author(s): Rule, Adam | Advisor(s): Hollan, James D | Abstract: Individuals and organizations increasingly rely on data analysis to generate insights and make decisions. Yet, small changes in how data are collected, cleaned, or modeled can lead to vastly different results. If data-driven insights are to be reviewed, reused, or trusted the process used to generate them must be tracked and communicated in detail. But data analysis is typically an iterative and exploratory process that is hard to articulate, especially when it involves programming. Computational notebooks aim to ease tracking and sharing of complex analyses by enabling analysts to write rich \emph{computational narratives} combining executable code, interactive visualizations, and explanatory text in a single document. While millions of people use computational notebooks, we know little about how they use them, or how well they help people track and share complex analyses.In this dissertation I present three studies of how people currently use computational notebooks, demonstrating that few notebooks, even those published alongside academic papers, have much in the way of narrative. Instead, most notebooks are loose collections of notes and scripts that even the original analyst struggles to understand. I then present two systems demonstrating how computational notebooks might be designed to support clearer communication of complex analyses. The first system, Janus, shows how current notebooks might be modified to aid both ongoing analysis and later communication by adding interactive hierarchy for selectively showing and hiding portions of the notebook. The second system, ActiveNotes, a prototype clinical note editor, demonstrates how computational notebooks might support data-driven work even when programming is not the primary means of interacting with data. Together, these studies demonstrate that tracking and sharing of complex analyses is hindered by a tension between exploration and explanation, but that computational notebooks and other media can reduce this tension by supporting not only the combination of, but also flexible organization and navigation of analytical steps, explanatory text, and computed results.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Deng, Congyue; Huang, Jiahui; Yang, Yong-Liang
TI  - Interactive modeling of lofted shapes from a single image
PY  - 2019
AB  - Modeling the complete geometry of general shapes from a single image is an ill-posed problem. User hints are often incorporated to resolve ambiguities and provide guidance during the modeling process. In this work, we present a novel interactive approach for extracting high-quality freeform shapes from a single image. This is inspired by the popular lofting technique in many CAD systems, and only requires minimal user input. Given an input image, the user only needs to sketch several projected cross sections, provide a “main axis”, and specify some geometric relations. Our algorithm then automatically optimizes the common normal to the sections with respect to these constraints, and interpolates between the sections, resulting in a high-quality 3D model that conforms to both the original image and the user input. The entire modeling session is efficient and intuitive. We demonstrate the effectiveness of our approach based on qualitative tests on a variety of images, and quantitative comparisons with the ground truth using synthetic images.
SP  - 279
EP  - 289
JF  - Computational Visual Media
VL  - 6
IS  - 3
PB  - 
DO  - 10.1007/s41095-019-0153-0
ER  - 

TY  - NA
AU  - Almoctar, Hassoumi; Irani, Pourang; Peysakhovich, Vsevolod; Hurter, Christophe
TI  - ICMI - Path Word: A Multimodal Password Entry Method for Ad-hoc Authentication Based on Digits' Shape and Smooth Pursuit Eye Movements
PY  - 2018
AB  - We present PathWord (PATH passWORD), a multimodal digit entry method for ad-hoc authentication based on known digits shape and user relative eye movements. PathWord is a touch-free, gaze-based input modality, which attempts to decrease shoulder surfing attacks when unlocking a system using PINs. The system uses a modified web camera to detect the user's eye. This enables suppressing direct touch, making it difficult for passer-bys to be aware of the input digits, thus reducing shoulder surfing and smudge attacks. In addition to showing high accuracy rates (Study 1: 87.1% successful entries) and strong confidentiality through detailed evaluations with 42 participants (Study 2), we demonstrate how PathWord considerably diminishes the potential of stolen passwords (on average 2.38% stolen passwords with PathWord vs. over 90% with traditional PIN screen). We show use-cases of PathWord and discuss its advantages over traditional input modalities. We envision PathWord as a method to foster confidence while unlocking a system through gaze gestures.
SP  - 268
EP  - 277
JF  - Proceedings of the 20th ACM International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242969.3243008
ER  - 

TY  - NA
AU  - Xing, Jun; Nagano, Koki; Chen, Weikai; Xu, Haotian; Wei, Li-Yi; Zhao, Yajie; Lu, Jingwan; Kim, Byungmoon; Li, Hao
TI  - UIST - HairBrush for Immersive Data-Driven Hair Modeling
PY  - 2019
AB  - While hair is an essential component of virtual humans, it is also one of the most challenging digital assets to create. Existing automatic techniques lack the generality and flexibility to create rich hair variations, while manual authoring interfaces often require considerable artistic skills and efforts, especially for intricate 3D hair structures that can be difficult to navigate. We propose an interactive hair modeling system that can help create complex hairstyles in minutes or hours that would otherwise take much longer with existing tools. Modelers, including novice users, can focus on the overall hairstyles and local hair deformations, as our system intelligently suggests the desired hair parts. Our method combines the flexibility of manual authoring and the convenience of data-driven automation. Since hair contains intricate 3D structures such as buns, knots, and strands, they are inherently challenging to create using traditional 2D interfaces. Our system provides a new 3D hair authoring interface for immersive interaction in virtual reality (VR). Users can draw high-level guide strips, from which our system predicts the most plausible hairstyles via a deep neural network trained from a professionally curated dataset. Each hairstyle in our dataset is composed of multiple variations, serving as blend-shapes to fit the user drawings via global blending and local deformation. The fitted hair models are visualized as interactive suggestions that the user can select, modify, or ignore. We conducted a user study to confirm that our system can significantly reduce manual labor while improve the output quality for modeling a variety of head and facial hairstyles that are challenging to create via existing techniques.
SP  - 263
EP  - 279
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347876
ER  - 

TY  - JOUR
AU  - Beck, Susanne; Bergenholtz, Carsten; Bogers, Marcel; Brasseur, Tiare-Maria; Conradsen, Marie Louise; Di Marco, Diletta; Distel, Andreas P.; Dobusch, Leonhard; Dörler, Daniel; Effert, Agnes; Fecher, Benedikt; Filiou, Despoina; Frederiksen, Lars; Gillier, Thomas; Grimpe, Christoph; Gruber, Marc; Haeussler, Carolin; Heigl, Florian; Hoisl, Karin; Hyslop, Katie; Kokshagina, Olga; LaFlamme, Marcel; Lawson, Cornelia; Lifshitz-Assaf, Hila; Lukas, Wolfgang; Nordberg, Markus; Norn, Maria Theresa; Poetz, Marion; Ponti, Marisa; Pruschak, Gernot; Pujol Priego, Laia; Radziwon, Agnieszka; Rafner, Janet; Romanova, Gergana; Ruser, Alexander; Sauermann, Henry; Shah, Sonali K.; Sherson, Jacob F.; Suess-Reyes, Julia; Tucci, Christopher L.; Tuertscher, Philipp; Vedel, Jane Bjørn; Velden, Theresa; Verganti, Roberto; Wareham, Jonathan; Wiggins, Andrea; Xu, Sunny Mosangzi
TI  - The Open Innovation in Science research field: a collaborative conceptualisation approach
PY  - 2020
AB  - Openness and collaboration in scientific research are attracting increasing attention from scholars and practitioners alike. However, a common understanding of these phenomena is hindered by disciplinary boundaries and disconnected research streams. We link dispersed knowledge on Open Innovation, Open Science, and related concepts such as Responsible Research and Innovation by proposing a unifying Open Innovation in Science (OIS) Research Framework. This framework captures the antecedents, contingencies, and consequences of open and collaborative practices along the entire process of generating and disseminating scientific insights and translating them into innovation. Moreover, it elucidates individual-, team-, organisation-, field-, and society-level factors shaping OIS practices. To conceptualise the framework, we employed a collaborative approach involving 47 scholars from multiple disciplines, highlighting both tensions and commonalities between existing approaches. The OIS Research Framework thus serves as a basis for future research, informs policy discussions, and provides guidance to scientists and practitioners
SP  - 136
EP  - 185
JF  - Industry and Innovation
VL  - 29
IS  - 2
PB  - 
DO  - 10.1080/13662716.2020.1792274
ER  - 

TY  - NA
AU  - Nicolet, Baptiste; Philip, Julien; Drettakis, George
TI  - I3D - Repurposing a Relighting Network for Realistic Compositions of Captured Scenes
PY  - 2020
AB  - Multi-view stereo can be used to rapidly create realistic virtual content, such as textured meshes or a geometric proxy for free-viewpoint Image-Based Rendering (IBR). These solutions greatly simplify the content creation process compared to traditional methods, but it is difficult to modify the content of the scene. We propose a novel approach to create scenes by composing (parts of) multiple captured scenes. The main difficulty of such compositions is that lighting conditions in each captured scene are different; to obtain a realistic composition we need to make lighting coherent. We propose a two-pass solution, by adapting a multi-view relighting network. We first match the lighting conditions of each scene separately and then synthesize shadows between scenes in a subsequent pass. We also improve the realism of the composition by estimating the change in ambient occlusion in contact areas between parts and compensate for the color balance of the different cameras used for capture. We illustrate our method with results on multiple compositions of outdoor scenes and show its application to multi-view image composition, IBR and textured mesh creation.
SP  - NA
EP  - NA
JF  - Symposium on Interactive 3D Graphics and Games
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384382.3384523
ER  - 

TY  - NA
AU  - Nair, Pranav; Wang, Wei; Lin, Hongnan
TI  - Lookie Here! Designing Directional User Indicators across Displays in Conditional Driving Automation
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - SAE Technical Paper Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.4271/2020-01-1201
ER  - 

TY  - NA
AU  - Li, Dingzeyu
TI  - Interacting with Acoustic Simulation and Fabrication
PY  - 2017
AB  - Incorporating accurate physics-based simulation into interactive design tools is challenging. However, adding the physics accurately becomes crucial to several emerging technologies. For example, in virtual/augmented reality (VR/AR) videos, the faithful reproduction of surrounding audios is required to bring the immersion to the next level. Similarly, as personal fabrication is made possible with accessible 3D printers, more intuitive tools that respect the physical constraints can help artists to prototype designs. One main hurdle is the sheer amount of computation complexity to accurately reproduce the real-world phenomena through physics-based simulation. In my thesis research, I develop interactive tools that implement efficient physics-based simulation algorithms for automatic optimization and intuitive user interaction.
SP  - 99
EP  - 102
JF  - Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3131785.3131842
ER  - 

TY  - NA
AU  - Khadpe, Pranav; Kulkarni, Chinmay; Kaufman, Geoff
TI  - Empathosphere: Promoting Constructive Communication in Ad-hoc Virtual Teams through Perspective-taking Spaces
PY  - 2021
AB  - When members of ad-hoc virtual teams need to collectively ideate or deliberate, they often fail to engage with each others' perspectives in a constructive manner. At best, this leads to sub-optimal outcomes and, at worst, it can cause conflicts that lead to teams not wanting to continue working together. Prior work has attempted to facilitate constructive communication by highlighting problematic communication patterns and nudging teams to alter interaction norms. However, these approaches achieve limited success because they fail to acknowledge two social barriers: (1) it is hard to reset team norms mid-interaction, and (2) corrective nudges have limited utility unless team members believe it is safe to voice their opinion and that their opinion will be heard. This paper introduces Empathosphere, a chat-embedded intervention to mitigate these barriers and foster constructive communication in teams. To mitigate the first barrier, Empathosphere leverages the benefits of "experimental spaces" in dampening existing norms and creating a climate conducive to change. To mitigate the second barrier, Empathosphere harnesses the benefits of perspective-taking to cultivate a group climate that promotes a norm of members speaking up and engaging with each other. Empathosphere achieves this by orchestrating authentic socio-emotional exchanges designed to induce perspective-taking. A controlled study (N=110) compared Empathosphere to an alternate intervention strategy of prompting teams to reflect on their team experience. We found that Empathosphere led to higher work satisfaction, encouraged more open communication and feedback within teams, and boosted teams' desire to continue working together. This work demonstrates that ``experimental spaces,'' particularly those that integrate methods of encouraging perspective-taking, can be a powerful means of improving communication in virtual teams.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yi, Xin; Lu, Yiqin; Cai, Ziyin; Wu, Zihan; Wang, Yuntao; Shi, Yuanchun
TI  - GazeDock: Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu
PY  - 2022
AB  - Gaze-only input techniques in VR face the challenge of avoiding false triggering due to continuous eye tracking while maintaining interaction performance. In this paper, we proposed GazeDock, a technique for enabling fast and robust gaze-based menu selection in VR. GazeDock features a view-fixed peripheral menu layout that automatically triggers appearing and selection when the user&#x2019;s gaze approaches and leaves the menu zone, thus facilitating interaction speed and minimizing the false triggering rate. We built a dataset of 12 participants&#x2019; natural gaze movements in typical VR applications. By analyzing their gaze movement patterns, we designed the menu UI personalization and optimized selection detection algorithm of GazeDock. We also examined users&#x2019; gaze selection precision for targets on the peripheral menu and found that 4&#x2013;8 menu items yield the highest throughput when considering both speed and accuracy. Finally, we validated the usability of GazeDock in a VR navigation game that contains both scene exploration and menu selection. Results showed that GazeDock achieved an average selection time of 471ms and a false triggering rate of 3.6%. And it received higher user preference ratings compared with dwell-based and pursuit-based techniques.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00105
ER  - 

TY  - NA
AU  - Correia, António; Jameel, Shoaib; Schneider, Daniel; Fonseca, Benjamim; Paredes, Hugo
TI  - HICSS - Theoretical Underpinnings and Practical Challenges of Crowdsourcing as a Mechanism for Academic Study
PY  - 2020
AB  - Researchers in a variety of fields are increasingly adopting crowdsourcing as a reliable instrument for performing tasks that are either complex for humans and computer algorithms. As a result, new forms of collective intelligence have emerged from the study of massive crowd-machine interactions in scientific work settings as a field for which there is no known theory or model able to explain how it really works. Such type of crowd work uses an open participation model that keeps the scientific activity (including datasets, methods, guidelines, and analysis results) widely available and mostly independent from institutions, which distinguishes crowd science from other crowd-assisted types of participation. In this paper, we build on the practical challenges of crowd-AI supported research and propose a conceptual framework for addressing the socio-technical aspects of crowd science from a CSCW viewpoint. Our study reinforces a manifested lack of systematic and empirical research of the symbiotic relation of AI with human computation and crowd computing in scientific endeavors
SP  - 1
EP  - 10
JF  - Proceedings of the Annual Hawaii International Conference on System Sciences
VL  - NA
IS  - NA
PB  - 
DO  - 10.24251/hicss.2020.568
ER  - 

TY  - NA
AU  - Mayer, Sven; Lischke, Lars; Lanksweirt, Adrian; Le, Huy Viet; Henze, Niels
TI  - NordiCHI - How to communicate new input techniques
PY  - 2018
AB  - Touchscreens are among the most ubiquitous input technologies. Commercial devices typically limit the input to 2D touch points. While a body of work enhances the interaction through finger recognition and diverse gestures, advanced input techniques have had a limited commercial impact. A major challenge is explaining new input techniques to users. In this paper, we investigate how to communicate novel input techniques for smartphones. Through interviews with 12 Ux experts, we identified three potential approaches: Depiction uses an icon to visualize the input technique, Pop-up shows a modal dialog when the input technique is available, and Tutorial explains all available input techniques in a centralized way. To understand which approach is most preferred by users we conducted a study with 36 participants that introduced novel techniques using one of the communication methods. While Depiction was preferred, we found that the approach should be selected based on the complexity of the interaction, novelty to the user, and the device size.
SP  - 460
EP  - 472
JF  - Proceedings of the 10th Nordic Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3240167.3240176
ER  - 

TY  - NA
AU  - Li, Jingyi; Li, Wilmot; Follmer, Sean; Agrawala, Maneesh
TI  - UIST - Automated Accessory Rigs for Layered 2D Character Illustrations
PY  - 2021
AB  - Mix-and-match character creation tools enable users to quickly produce 2D character illustrations by combining various predefined accessories, like clothes and hairstyles, which are represented as separate, interchangeable artwork layers. However, these accessory layers are often designed to fit only the default body artwork, so users cannot modify the body without manually updating all the accessory layers as well. To address this issue, we present a method that captures and preserves important relationships between artwork layers so that the predefined accessories adapt with the character’s body. We encode these relationships with four types of constraints that handle common interactions between layers: (1) occlusion, (2) attachment at a point, (3) coincident boundaries, and (4) overlapping regions. A rig is a set of constraints that allow a motion or deformation specified on the body to transfer to the accessory layers. We present an automated algorithm for generating such a rig for each accessory layer, but also allow users to select which constraints to apply to specific accessories. We demonstrate how our system supports a variety of modifications to body shape and pose using artwork from mix-and-match data sets.
SP  - 1100
EP  - 1108
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474809
ER  - 

TY  - CHAP
AU  - Priyadharshini, S.; Mahapatra, Ansuman
TI  - 360$$^\circ $$ User-Generated Videos: Current Research and Future Trends
PY  - 2020
AB  - The 360\(^\circ \) video, also known as immersive or spherical video, allows the observer to have a 360\(^\circ \) view and an immersive experience of the surroundings. Each direction in this video is recorded at the same time either by an omni-direction camera or by an assembly of cameras synchronized together. The viewing perspectives are controlled by the viewer during playbacks. This article gives an overview of the existing research areas and methods in the user-generated 360\(^\circ \) videos for streaming, transcoding, viewport-based projections, video standardization, and summarization. This survey also provides an analysis of the experience estimation in 360\(^\circ \) videos. The study of multiple quality evaluation criteria is also reviewed. Moreover, 360\(^\circ \) video user experience studies are also focused on this survey. The merits and demerits of each technique are investigated in depth.
SP  - 117
EP  - 135
JF  - Studies in Computational Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-15-6844-2_9
ER  - 

TY  - NA
AU  - Fan, Xinyi; Bermano, Amit; Kim, Vladimir G.; Popović, Jovan; Rusinkiewicz, Szymon
TI  - Expressive - Tooncap: a layered deformable model for capturing poses from cartoon characters
PY  - 2018
AB  - Characters in traditional artwork such as children's books or cartoon animations are typically drawn once, in fixed poses, with little opportunity to change the characters' appearance or re-use them in a different animation. To enable these applications one can fit a consistent parametric deformable model--- a puppet ---to different images of a character, thus establishing consistent segmentation, dense semantic correspondence, and deformation parameters across poses. In this work, we argue that a layered deformable puppet is a natural representation for hand-drawn characters, providing an effective way to deal with the articulation, expressive deformation, and occlusion that are common to this style of artwork. Our main contribution is an automatic pipeline for fitting these models to unlabeled images depicting the same character in various poses. We demonstrate that the output of our pipeline can be used directly for editing and re-targeting animations.
SP  - 16
EP  - NA
JF  - Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3229147.3229149
ER  - 

TY  - CHAP
AU  - Funakoshi, Minto; Fujita, Shun; Minawa, Kaori; Shizuki, Buntarou
TI  - HCI (2) - SilverCodes: Thin, Flexible, and Single-Line Connected Identifiers Inputted by Swiping with a Finger
PY  - 2020
AB  - This study investigates SilverCodes, thin and flexible identifiers that serve to constitute an input interface. SilverCodes are thin and flexible barcode-shaped identifiers. User input is achieved by swiping an identifier with a finger. SilverCodes are made from two sheets of paper pasted with conductive ink. Identifiers can thus be conveniently printed using an ordinary ink-jet printer. Multiple SilverCodes can be connected by a single-line wire to an external module that recognizes the identifier swiped by the user. Furthermore, SilverCodes can be stacked and identifiers can be recognized to an average accuracy of 95.3%.
SP  - 350
EP  - 362
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_24
ER  - 

TY  - NA
AU  - Mott, Martez E.; Tang, John C.; Kane, Shaun K.; Cutrell, Edward; Morris, Meredith Ringel
TI  - ASSETS - “I just went into it assuming that I wouldn't be able to have the full experience”: Understanding the Accessibility of Virtual Reality for People with Limited Mobility
PY  - 2020
AB  - Virtual reality (VR) has the potential to transform many aspects of our daily lives, including work, entertainment, communication, and education. However, there has been little research into understanding the usability of VR for people with mobility limitations. In this paper, we present the results of an exploration to understand the accessibility of VR for people with limited mobility. We conducted semi-structured interviews with 16 people with limited mobility about their thoughts on, and experiences with, VR systems. We identified 7 barriers related to the physical accessibility of VR devices that people with limited mobility might encounter, ranging from the initial setup of a VR system to keeping VR controllers in view of cameras embedded in VR headsets. We also elicited potential improvements to VR systems that would address some accessibility concerns. Based on our findings, we discuss the importance of considering the abilities of people with limited mobility when designing VR systems, as the abilities of many participants did not match the assumptions embedded in the design of current VR systems.
SP  - NA
EP  - NA
JF  - The 22nd International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3373625.3416998
ER  - 

TY  - NA
AU  - Trösterer, Sandra; Streitwieser, Benedikt; Meschtscherjakov, Alexander; Tscheligi, Manfred
TI  - AutomotiveUI (adjunct) - LED Visualizations for Drivers' Attention: An Exploratory Study on Experience and Associated Information Contents
PY  - 2018
AB  - When it comes to highly automated driving, several studies indicate that drivers should be "kept in the loop" when driving in automated mode in order to be better prepared when they need to take over. The challenge lies in finding a way that raises the drivers' situation awareness without annoying the driver, who may be occupied with another task. Ambient light systems using LED visualizations provide a feasible way to draw attention, however, the kind of information that can be communicated is limited. In this paper, we present an exploratory study, where we investigated the semantic quality of different LED patterns (shown on an LED-strip) by capturing experience and associated information contents. Our initial findings show that LED visualizations, which are experienced quite similar at first, can nonetheless be distinctive with regard to the associated information contents.
SP  - 192
EP  - 197
JF  - Adjunct Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3239092.3265966
ER  - 

TY  - JOUR
AU  - Delmotte, Arnaud; Tanaka, Kenichiro; Kubo, Hiroyuki; Funatomi, Takuya; Mukaigawa, Yasuhiro
TI  - Blind 3D-Printing Watermarking Using Moment Alignment and Surface Norm Distribution
PY  - 2021
AB  - The recent development of 3D printing technology has brought concerns about its potential misuse, such as in copyright infringement, and crimes. Although there have been many studies on blind 3D mesh watermarking for the copyright protection of digital objects, methods applicable to 3D printed objects are rare. In this paper, we propose a novel blind watermarking algorithm for 3D printed objects with applications for copyright protection, traitor tracing, object identification, and crime investigation. Our method allows us to embed a few bits of data into a 3D-printed object, and retrieve it by 3D scanning without requiring any information about the original mesh. The payload is embedded on the object's surface by slightly modifying the distribution of surface norms, that is, the distance between the surface, and the center of gravity. It is robust to resampling and can work with any 3D printer, and scanner technology. In addition, our method increases the capacity, and resistance by subdividing the mesh into a set of bins, and spreading the data over the entire surface to negate the effect of local printing artifacts. The method's novelties include extending the vertex norm histogram to a continuous surface, and the use of 3D moments to synchronize a watermark signal in a 3D-printing context. In the experiments, our method was evaluated using a public dataset against center, orientation, minimum, and maximum norm misalignments; a printing simulation; and actual print/scan experiments using a standard 3D printer, and scanner.
SP  - 3467
EP  - 3482
JF  - IEEE Transactions on Multimedia
VL  - 23
IS  - NA
PB  - 
DO  - 10.1109/tmm.2020.3025660
ER  - 

TY  - JOUR
AU  - Delmotte, Arnaud; Tanaka, Kenichiro; Kubo, Hiroyuki; Funatomi, Takuya; Mukaigawa, Yasuhiro
TI  - Blind Watermarking for 3-D Printed Objects by Locally Modifying Layer Thickness
PY  - 2020
AB  - We propose a new blind watermarking algorithm for 3D printed objects that has applications in metadata embedding, robotic grasping, counterfeit prevention, and crime investigation. Our method can be used on fused deposition modeling (FDM) 3D printers and works by modifying the printed layer thickness on small patches of the surface of an object. These patches can be applied to multiple regions of the object, thereby making it resistant to various attacks such as cropping, local deformation, local surface degradation, or printing errors. The novelties of our method are the use of the thickness of printed layers as a one-dimensional carrier signal to embed data, the minimization of distortion by only modifying the layers locally, and one-shot detection using a common paper scanner. To correct encoding or decoding errors, our method combines multiple patches and uses a 2D parity check to estimate the error probability of each bit to obtain a higher correction rate than a naive majority vote. The parity bits included in the patches have a double purpose because, in addition to error detection, they are also used to identify the orientation of the patches. In our experiments, we successfully embedded a watermark into flat surfaces of 3D objects with various filament colors using a standard FDM 3D printer, extracted it using a common 2D paper scanner and evaluated the sensitivity to surface degradation and signal amplitude.
SP  - 2780
EP  - 2791
JF  - IEEE Transactions on Multimedia
VL  - 22
IS  - 11
PB  - 
DO  - 10.1109/tmm.2019.2962306
ER  - 

TY  - JOUR
AU  - Hong, Sanghwa; Heo, Seongkook; Lee, Byungjoo
TI  - MaterialSense: Estimating and utilizing material properties of contact objects in multi-touch interaction
PY  - NA
AB  - NA
SP  - 102985
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 172
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2022.102985
ER  - 

TY  - CHAP
AU  - Yang, Ji Hyun; Lee, Seul Chan; Nadri, Chihab; Kim, Jaewon; Shin, Jaekon; Jeon, Myounghoon
TI  - Multimodal Displays for Takeover Requests
PY  - 2022
AB  - NA
SP  - 397
EP  - 424
JF  - Studies in Computational Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77726-5_15
ER  - 

TY  - CHAP
AU  - Trösterer, Sandra; Streitwieser, Benedikt; Meschtscherjakov, Alexander; Tscheligi, Manfred
TI  - INTERACT (2) - Shared Gaze While Driving: How Drivers Can Be Supported by an LED-Visualization of the Front-Seat Passenger's Gaze
PY  - 2019
AB  - The front-seat passenger in a vehicle may assist a driver in providing hints towards points of interest in a driving situation. In order to communicate spatial information efficiently, the so-called shared gaze approach has been introduced in previous research. Thereby, the gaze of the front-seat passenger is visualized for the driver. So far, this approach has been solely investigated in driving simulator environments. In this paper, we present a study on how well shared gaze works in a real driving situation (n = 8). We examine identification rates of different object types in the driving environment based on the visualization of the front-seat passenger’s gaze via glowing LEDs on an LED-strip. Our results show that this rate is dependent on object relevance for the driving task and movement of the object. We found that perceived visual distraction was low and that the usefulness of shared gaze for navigational tasks was considered high.
SP  - 329
EP  - 350
JF  - Human-Computer Interaction – INTERACT 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-29384-0_21
ER  - 

TY  - NA
AU  - Avery, Jeff; Malacria, Sylvain; Nancel, Mathieu; Casiez, Géry; Lank, Edward
TI  - CHI - Introducing Transient Gestures to Improve Pan and Zoom on Touch Surfaces
PY  - 2018
AB  - Despite the ubiquity of touch-based input and the availability of increasingly computationally powerful touchscreen devices, there has been comparatively little work on enhancing basic canonical gestures such as swipe-to-pan and pinch-to-zoom. In this paper, we introduce transient pan and zoom, i.e. pan and zoom manipulation gestures that temporarily alter the view and can be rapidly undone. Leveraging typical touchscreen support for additional contact points, we design our transient gestures such that they co-exist with traditional pan and zoom interaction. We show that our transient pan-and-zoom reduces repetition in multi-level navigation and facilitates rapid movement between document states. We conclude with a discussion of user feedback, and directions for future research.
SP  - 25
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173599
ER  - 

TY  - NA
AU  - Lilija, Klemen; Pohl, Henning; Hornbæk, Kasper
TI  - CHI - Who Put That There? Temporal Navigation of Spatial Recordings by Direct Manipulation
PY  - 2020
AB  - Spatial recordings allow viewers to move within them and freely choose their viewpoint. However, such recordings make it easy to miss events and difficult to follow moving objects when skipping through the recording. To alleviate these problems we present the Who Put That There system that allows users to navigate through time by directly manipulating objects in the scene. By selecting an object, the user can navigate to moments where the object changed. Users can also view trajectories of objects that changed location and directly manipulate them to navigate. We evaluated the system with a set of sensemaking questions in a think-aloud study. Participants understood the system and found it useful for finding events of interest, while being present and engaged in the recording.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376604
ER  - 

TY  - JOUR
AU  - Borowski, Marcel; Fog, Bjarke V.; Griggio, Carla F.; Eagan, James R.; Klokmose, Clemens N.
TI  - Between Principle and Pragmatism: Reflections on Prototyping Computational Media with Webstrates
PY  - 2022
AB  - <jats:p>Computational media describes a vision of software, which, in contrast to application-centric software, is (1) malleable, so users can modify existing functionality, (2) computable, so users can run custom code, (3) distributable, so users can open documents across different devices, and (4) shareable, so users can easily share and collaborate on documents. Over the last ten years, the Webstrates and Codestrates projects aimed to realize this vision of computational media. Webstrates is a server application that synchronizes the DOM of websites. Codestrates builds on top of Webstrates and adds an authoring environment, which blurs the use and development of applications. Grounded in a chronology of the development of Webstrates and Codestrates, we present eight tensions that we needed to balance during their development. We use these tensions as an analytical lens in three case studies and a game challenge in which participants created games using Codestrates. We discuss the results of the game challenge based on these tensions and present key takeaways for six of them. Finally, we present six lessons learned from our endeavor to realize the vision of computational media, demonstrating the balancing act of weighing the vision against the pragmatics of implementing a working system.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569895
ER  - 

TY  - CHAP
AU  - Yamamoto, Kenta; Kawamura, Ryota; Takazawa, Kazuki; Osone, Hiroyuki; Ochiai, Yoichi
TI  - HCI (36) - A Preliminary Study for Identification of Additive Manufactured Objects with Transmitted Images
PY  - 2021
AB  - Additive manufacturing has the potential to become a standard method for manufacturing products, and product information is indispensable for the item distribution system. While most products are given barcodes to the exterior surfaces, research on embedding barcodes inside products is underway. This is because additive manufacturing makes it possible to carry out manufacturing and information adding at the same time, and embedding information inside does not impair the exterior appearance of the product. However, products that have not been embedded information can not be identified, and embedded information can not be rewritten later. In this study, we have developed a product identification system that does not require embedding barcodes inside. This system uses a transmission image of the product which contains information of each product such as different inner support structures and manufacturing errors. We have shown through experiments that if datasets of transmission images are available, objects can be identified with an accuracy of over 90%. This result suggests that our approach can be useful for identifying objects without embedded information.
SP  - 439
EP  - 458
JF  - Artificial Intelligence in HCI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77772-2_29
ER  - 

TY  - JOUR
AU  - Rigon, Riccardo; Formetta, Giuseppe; Bancheri, Marialaura; Tubini, Niccolò; D'Amato, Concetta; David, Olaf; Massari, Christian
TI  - HESS Opinions: Participatory Digital eARth Twin Hydrology systems (DARTHs) for everyone – a blueprint for hydrologists
PY  - 2022
AB  - <jats:p>Abstract. The “Digital Earth” (DE) metaphor is very useful for both end users and hydrological modelers (i.e., the coders). In this opinion paper, we analyze different categories of models with the view of making them part of Digital eARth Twin Hydrology systems (DARTHs). We stress the idea that DARTHs are not models, rather they are an appropriate infrastructure that hosts (certain types of) models and provides some basic services for connecting to input data. We also argue that a modeling-by-component strategy is the right one for accomplishing the requirements of the DE. Five technological steps are envisioned to move from the current state of the art of modeling. In step 1, models are decomposed into interacting modules with, for instance, the agnostic parts dealing with inputs and outputs separated from the model-specific parts that contain the algorithms. In steps 2 to 4, the appropriate software layers are added to gain transparent model execution in the cloud, independently of the hardware and the operating system of computer, without human intervention. Finally, step 5 allows models to be selected as if they were interchangeable with others without giving deceptive answers. This step includes the use of hypothesis testing, the inclusion of error of estimates, the adoption of literate programming and guidelines to obtain informative clean code. The urgency for DARTHs to be open source is supported here in light of the open-science movement and its ideas. Therefore, it is argued that DARTHs must promote a new participatory way of performing hydrological science, in which researchers can contribute cooperatively to characterize and control model outcomes in various territories. Finally, three enabling technologies are also discussed in the context of DARTHs – Earth observations (EOs), high-performance computing (HPC) and machine learning (ML) – as well as how these technologies can be integrated in the overall system to both boost the research activity of scientists and generate knowledge. </jats:p>
SP  - 4773
EP  - 4800
JF  - Hydrology and Earth System Sciences
VL  - 26
IS  - 18
PB  - 
DO  - 10.5194/hess-26-4773-2022
ER  - 

TY  - NA
AU  - Chi, Peggy; Dong, Tao; Frueh, Christian; Colonna, Brian; Kwatra, Vivek; Essa, Irfan
TI  - Synthesis-Assisted Video Prototyping From a Document
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545676
ER  - 

TY  - NA
AU  - Leiva, Germán; Nguyen, Cuong; Kazi, Rubaiat Habib; Asente, Paul
TI  - CHI - Pronto: Rapid Augmented Reality Video Prototyping Using Sketches and Enaction
PY  - 2020
AB  - Designers have limited tools to prototype AR experiences rapidly. Can lightweight, immediate tools let designers prototype dynamic AR interactions while capturing the nuances of a 3D experience? We interviewed three AR experts and identified several recurring issues in AR design: creating and positioning 3D assets, handling the changing user position, and orchestrating multiple animations. We introduce PROJECT PRONTO, a tablet-based video prototyping system that combines 2D video with 3D manipulation. PRONTO supports four intertwined activities: capturing 3D spatial information alongside a video scenario, positioning and sketching 2D drawings in a 3D world, and enacting animations with physical interactions. An observational study with professional designers shows that participants can use PRONTO to prototype diverse AR experiences. All participants performed two tasks: replicating a sample non-trivial AR experience and prototyping their open-ended designs. All participants completed the replication task and found PRONTO easy to use. Most participants found that PRONTO encourages more exploration of designs than their current practices.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376160
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Wong, Andrew; Qi, Yini; Blumberg, Lotta-Gili; Kim, Yoonji; Mueller, Stefanie
TI  - Conference on Designing Interactive Systems - Adapt2Learn: A Toolkit for Configuring the Learning Algorithm for Adaptive Physical Tools for Motor-Skill Learning
PY  - 2021
AB  - A recent study on motor-skill training showed that adaptive training tools that use shape-change to adapt the training difficulty based on learners’ performance can lead to higher learning gains. However, to date, no support tools exist to help designers create adaptive learning tools. Our formative study shows that developing the adaptive learning algorithm poses a particular challenge. To address this, we built Adapt2Learn, a toolkit that auto-generates the learning algorithm for adaptive tools. Designers choose their tool’s sensors and actuators, Adapt2Learn then configures the learning algorithm and generates a microcontroller script that designers can deploy on the tool. Once uploaded, the script assesses the learner’s performance via the sensors, computes the training difficulty, and actuates the tool to adapt the difficulty. Adapt2Learn’s visualization tool then lets designers visualize their tool’s adaptation and evaluate the learning algorithm. To validate that Adapt2Learn can generate adaptation algorithms for different tools, we built several application examples that demonstrate successful deployment.
SP  - 1301
EP  - 1312
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462128
ER  - 

TY  - NA
AU  - Arora, Rahul; Kazi, Rubaiat Habib; Grossman, Tovi; Fitzmaurice, George; Singh, Karan
TI  - CHI - SymbiosisSketch: Combining 2D & 3D Sketching for Designing Detailed 3D Objects in Situ
PY  - 2018
AB  - We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air.
SP  - 185
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173759
ER  - 

TY  - JOUR
AU  - Krafft, Peter M.
TI  - A Simple Computational Theory of General Collective Intelligence.
PY  - 2018
AB  - Researchers have recently demonstrated that group performance across tasks tends to be correlated, motivating the use of a single metric for the general collective intelligence of groups akin to general intelligence metrics for individuals. High general collective intelligence is achieved when a group performs well across a wide variety of tasks. A number of factors have been shown to be predictive of general collective intelligence, but there is sparse formal theory explaining the presence of correlations across tasks, betraying a fundamental gap in our understanding of what general collective intelligence is measuring. Here, we formally argue that general collective intelligence arises from groups achieving commitment to group goals, accurate shared beliefs, and coordinated actions. We then argue for the existence of generic mechanisms that help groups achieve these cognitive alignment conditions. The presence or absence of such mechanisms can potentially explain observed correlations in group performance across tasks. Under our view, general collective intelligence can be conceived as measuring group performance on classes of tasks that have particular combinations of cognitive alignment requirements.
SP  - 374
EP  - 392
JF  - Topics in cognitive science
VL  - 11
IS  - 2
PB  - 
DO  - 10.1111/tops.12341
ER  - 

TY  - JOUR
AU  - Jolly, Andrew; Caulfield, Laura; Sojka, Bozena; Iafrati, S.; Rees, James; Massie, Rachel
TI  - Café Delphi: Hybridising ‘World Café’ and ‘Delphi Techniques’ for successful remote academic collaboration
PY  - 2021
AB  - Abstract Developing collaborative and cooperative research across academic disciplines and university administrative boundaries can be a challenge. In an attempt to understand and propose solutions to this challenge, the authors of this paper set out to: test an innovative combination of methods to generate and evaluate ideas and strategies; and to write about the findings using collaborative online methods. During this process universities in the UK moved to online working and so the authors completed this paper through entirely online means. The authors - a team of academic researchers from the University of AAA - came together in sessions designed as a hybrid of World Cafe and Delphi technique approaches to discuss challenges and solutions. The findings were written up drawing on insights from the use of massively authored papers (also known as ‘massively open online papers’, MOOPs), and online tools to enable remote collaboration. Expert consensus was sought in this project within a group of participants (N ​= ​7) in one university setting to create a MOOP. This paper presents details of the process, the findings, and reflections on this collaborative and cooperative exercise. That this paper was written using the methods discussed within it, highlights the value and success of the approach. In light of the current Coronavirus pandemic and the increased need to work remotely, this paper offers academics useful strategies for meaningful and productive online collaboration.
SP  - 100095
EP  - NA
JF  - Social Sciences & Humanities Open
VL  - 3
IS  - 1
PB  - 
DO  - 10.1016/j.ssaho.2020.100095
ER  - 

TY  - NA
AU  - Zheng, Jingjie; Lewis, Blaine; Avery, Jeff; Vogel, Daniel
TI  - UIST - FingerArc and FingerChord: Supporting Novice to Expert Transitions with Guided Finger-Aware Shortcuts
PY  - 2018
AB  - Keyboard shortcuts can be more efficient than graphical input, but they are underused by most users. To alleviate this, we present "Guided Finger-Aware Shortcuts" to reduce the gulf between graphical input and shortcut activation. The interaction technique works by recognising when a special hand posture is used to press a key, then allowing secondary finger movements to select among related shortcuts if desired. Novice users can learn the mappings through dynamic visual guidance revealed by holding a key down, but experts can trigger shortcuts directly without pausing. Two variations are described: FingerArc uses the angle of the thumb, and FingerChord uses a second key press. The techniques are motivated by an interview study identifying factors hindering the learning, use, and exploration of keyboard shortcuts. A controlled comparison with conventional keyboard shortcuts shows the techniques encourage overall shortcut usage, make interaction faster, less error-prone, and provide advantages over simply adding visual guidance to standard shortcuts.
SP  - 347
EP  - 363
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242589
ER  - 

TY  - CHAP
AU  - Gonsher, Ian; Rapoport, Daniel; Marbach, Alice; Kurniawan, Dana; Eiseman, Seth; Zhang, Emily; Qu, Amy; Abela, Mikey; Li, Xinru; Sheth, Aanchal M.; Chen, Annie; Upadhyayula, Rohan; Li, Sunny; Bansal, Sahil; Zhao, Ivan; Chen, Grace; Tan, Charles; Lei, Zhenhong
TI  - Designing the Metaverse: A Study of Design Research and Creative Practice from Speculative Fictions to Functioning Prototypes
PY  - 2022
AB  - AbstractAs ubiquitous computing evolves towards greater adoption of Virtual, Augmented, and Mixed (VAM) Reality design paradigms, designers, engineers, and artists must develop new creative strategies to advance both the technical innovations necessary for new technology to emerge, as well as a critical framework to examine the consequences that these new technologies may have on society and culture. In this paper we present design research that explores these themes through a creative process of iterative prototyping and critique, from speculative fictions to functioning prototypes. These strategies draw heavily on literature, performance, as well as other areas in the humanities not usually considered within the domain of engineering. The prototypes presented in this paper demonstrate innovations for the design of VAM Reality within the emerging metaverse, as they give insight into the creative process from which they developed.KeywordsCreative processMetaverse designMixed RealityVirtual RealityDesign processCCS ConceptsHuman-centered computingInteraction designInteraction design process and methodsInterface design prototyping
SP  - 561
EP  - 573
JF  - Proceedings of the Future Technologies Conference (FTC) 2022, Volume 2
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-18458-1_38
ER  - 

TY  - JOUR
AU  - Liu, Fang; Deng, Xiaoming; Song, Jiancheng; Lai, Yu-Kun; Liu, Yong-Jin; Wang, Hao; Ma, Cuixia; Qin, Shengfeng; Wang, Hongan
TI  - SketchMaker: Sketch Extraction and Reuse for Interactive Scene Sketch Composition
PY  - 2022
AB  - <jats:p> Sketching is an intuitive and simple way to depict sciences with various object form and appearance characteristics. In the past few years, widely available touchscreen devices have increasingly made sketch-based human-AI co-creation applications popular. One key issue of sketch-oriented interaction is to prepare input sketches efficiently by non-professionals because it is usually difficult and time-consuming to draw an ideal sketch with appropriate outlines and rich details, especially for novice users with no sketching skills. Thus, sketching brings great obstacles for sketch applications in daily life. On the other hand, hand-drawn sketches are scarce and hard to collect. Given the fact that there are several large-scale sketch datasets providing sketch data resources, but they usually have a limited number of objects and categories in sketch, and do not support users to collect new sketch materials according to their personal preferences. In addition, few sketch-related applications support the reuse of existing sketch elements. Thus, knowing how to extract sketches from existing drawings and effectively re-use them in interactive scene sketch composition will provide an elegant way for <jats:bold>sketch-based image retrieval (SBIR)</jats:bold> applications, which are widely used in various touch screen devices. In this study, we first conduct a study on current SBIR to better understand the main requirements and challenges in sketch-oriented applications. Then we develop the SketchMaker as an interactive sketch extraction and composition system to help users generate scene sketches via reusing object sketches in existing scene sketches with minimal manual intervention. Moreover, we demonstrate how SBIR improves from composited scene sketches to verify the performance of our interactive sketch processing system. We also include a sketch-based video localization task as an alternative application of our sketch composition scheme. Our pilot study shows that our system is effective and efficient, and provides a way to promote practical applications of sketches. </jats:p>
SP  - 1
EP  - 26
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 12
IS  - 3
PB  - 
DO  - 10.1145/3543956
ER  - 

TY  - NA
AU  - Hong, Sanghwa; Jeong, Eunseok; Heo, Seongkook; Lee, Byungjoo
TI  - UIST - FDSense: Estimating Young's Modulus and Stiffness of End Effectors to Facilitate Kinetic Interaction on Touch Surfaces
PY  - 2018
AB  - We make touch input by physically colliding an end effector (e.g., a body part or a stylus) with a touch surface. Prior studies have examined the use of kinematic variables of collision between objects, such as position, velocity, force, and impact. However, the nature of the collision can be understood more thoroughly by considering the known physical relationships that exist between directly measurable variables (i.e., kinetics). Based on this collision kinetics, this study proposes a novel touch technique called FDSense. By simultaneously observing the force and contact area measured from the touchpad, FDSense allows estimation of the Young's modulus and stiffness of the object being contacted. Our technical evaluation showed that FDSense could effectively estimate the Young's modulus of end effectors made of various materials, and the stiffness of each part of the human hand. Two applications using FDSense were demonstrated, for digital painting and digital instruments, where the result of the expression varies significantly depending on the elasticity of the end effector. In a following informal study, participants assessed the technique positively.
SP  - 809
EP  - 823
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242644
ER  - 

TY  - NA
AU  - Willett, Nora S.; Kazi, Rubaiat Habib; Chen, Michael; Fitzmaurice, George; Finkelstein, Adam; Grossman, Tovi
TI  - UIST - A Mixed-Initiative Interface for Animating Static Pictures
PY  - 2018
AB  - We present an interactive tool to animate the visual elements of a static picture, based on simple sketch-based markup. While animated images enhance websites, infographics, logos, e-books, and social media, creating such animations from still pictures is difficult for novices and tedious for experts. Creating automatic tools is challenging due to ambiguities in object segmentation, relative depth ordering, and non-existent temporal information. With a few user drawn scribbles as input, our mixed initiative creative interface extracts repetitive texture elements in an image, and supports animating them. Our system also facilitates the creation of multiple layers to enhance depth cues in the animation. Finally, after analyzing the artwork during segmentation, several animation processes automatically generate kinetic textures that are spatio-temporally coherent with the source image. Our results, as well as feedback from our user evaluation, suggest that our system effectively allows illustrators and animators to add life to still images in a broad range of visual styles.
SP  - 649
EP  - 661
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242612
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Kato, Kunihiro; Ikematsu, Kaori; Kawahara, Yoshihiro; Siio, Itiro
TI  - CircWood: Laser Printed Circuit Boards and Sensors for Affordable DIY Woodworking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501317
ER  - 

TY  - NA
AU  - Willett, Nora S.; Shin, Hijung Valentina; Jin, Zeyu; Li, Wilmot; Finkelstein, Adam
TI  - IUI - Pose2Pose: pose selection and transfer for 2D character animation
PY  - 2020
AB  - An artist faces two challenges when creating a 2D animated character to mimic a specific human performance. First, the artist must design and draw a collection of artwork depicting portions of the character in a suitable set of poses, for example arm and hand poses that can be selected and combined to express the range of gestures typical for that person. Next, to depict a specific performance, the artist must select and position the appropriate set of artwork at each moment of the animation. This paper presents a system that addresses these challenges by leveraging video of the target human performer. Our system tracks arm and hand poses in an example video of the target. The UI displays clusters of these poses to help artists select representative poses that capture the actor's style and personality. From this mapping of pose data to character artwork, our system can generate an animation from a new performance video. It relies on a dynamic programming algorithm to optimize for smooth animations that match the poses found in the video. Artists used our system to create four 2D characters and were pleased with the final automatically animated results. We also describe additional applications addressing audio-driven or text-based animations.
SP  - 88
EP  - 99
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377505
ER  - 

TY  - NA
AU  - Kubo, Yuki; Eguchi, Kana; Aoki, Ryosuke
TI  - CHI Extended Abstracts - 3D-Printed Object Identification Method using Inner Structure Patterns Configured by Slicer Software
PY  - 2020
AB  - We present an identification method for 3D-printed objects that uses differences in inner structure patterns formed by printing conditions which we can configure in slicer software. Resonant properties change depending on the shape, material, and boundary conditions of the objects, so that our method identifies objects on the basis of differences in resonant properties caused by different inner structure patterns using a machine-learning algorithm. We measured resonant properties as frequency properties using active acoustic sensing. Our method is applicable to 3D-printed objects with a low filling rate while reducing the workload of modeling the inner structure to be used as a tag. To investigate the feasibility of our method, we conducted two experimental evaluations. The results of one showed that our method can identify eight objects with an average classification accuracy of 99.3%.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382847
ER  - 

TY  - CHAP
AU  - Lykourentzou, Ioanna; Liapis, Antonios; Papastathis, Costas; Papangelis, Konstantinos; Vassilakis, Costas
TI  - I3E Workshops - Exploring Self-organisation in Crowd Teams
PY  - 2020
AB  - Online crowds have the potential to do more complex work in teams, rather than as individuals. Team formation algorithms typically maximize some notion of global utility of team output by allocating people to teams or tasks. However, decisions made by these algorithms do not consider the decisions or preferences of the people themselves. This paper explores a complementary strategy, which relies on the crowd itself to self-organize into effective teams. Our preliminary results show that users perceive the ability to choose their teammate extremely useful in a crowdsourcing setting. We also find that self-organisation makes users feel more productive, creative and responsible for their work product.
SP  - 164
EP  - 175
JF  - IFIP Advances in Information and Communication Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-39634-3_15
ER  - 

TY  - NA
AU  - Kim, Insu; Park, Keunwoo; Yoon, Youngwoo; Lee, Geehyuk
TI  - UIST (Adjunct Volume) - Touch180: Finger Identification on Mobile Touchscreen using Fisheye Camera and Convolutional Neural Network
PY  - 2018
AB  - We present Touch180, a computer vision based solution for identifying fingers on a mobile touchscreen with a fisheye camera and deep learning algorithm. As a proof-of-concept research, this paper focused on robustness and high accuracy of finger identification. We generated a new dataset for Touch180 configuration, which is named as Fisheye180. We trained a CNN (Convolutional Neural Network)-based network utilizing touch locations as auxiliary inputs. With our novel dataset and deep learning algorithm, finger identification result shows 98.56% accuracy with VGG16 model. Our study will serve as a step stone for finger identification on a mobile touchscreen.
SP  - 29
EP  - 32
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266091
ER  - 

TY  - CHAP
AU  - Krishna, Ranjay; Gordon, Mitchell; Fei-Fei, Li; Bernstein, Michael S.
TI  - Visual Intelligence through Human Interaction
PY  - 2021
AB  - Over the last decade, Computer Vision, the branch of Artificial Intelligence aimed at understanding the visual world, has evolved from simply recognizing objects in images to describing pictures, answering questions about images, aiding robots maneuver around physical spaces, and even generating novel visual content. As these tasks and applications have modernized, so too has the reliance on more data, either for model training or for evaluation. In this chapter, we demonstrate that novel interaction strategies can enable new forms of data collection and evaluation for Computer Vision. First, we present a crowdsourcing interface for speeding up paid data collection by an order of magnitude, feeding the data-hungry nature of modern vision models. Second, we explore a method to increase volunteer contributions using automated social interventions. Third, we develop a system to ensure human evaluation of generative vision models are reliable, affordable, and grounded in psychophysics theory. We conclude with future opportunities for Human–Computer Interaction to aid Computer Vision.
SP  - 257
EP  - 314
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_9
ER  - 

TY  - NA
AU  - Bærentzen, Andreas; Frisvad, Jeppe Revall; Singh, Karan
TI  - VRST - Signifier-Based Immersive and Interactive 3D Modeling
PY  - 2019
AB  - Interactive 3D modeling in VR is both aided by immersive 3D input and hampered by model disjunct, tool-based or selection-action user interfaces. We propose a direct, signifier-based approach to the popular interactive technique of creating 3D models through a sequence of extrusion operations. Motivated by handles and signifiers that communicate the affordances of everyday objects, we define a set of design principles for an immersive, signifier-based modeling interface. We then present an interactive 3D modeling system where all modeling affordances are modelessly reachable and signified on the model itself.
SP  - NA
EP  - NA
JF  - 25th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3359996.3364257
ER  - 

TY  - NA
AU  - Chilton, Lydia B.; Petridis, Savvas; Agrawala, Maneesh
TI  - CHI - VisiBlends: A Flexible Workflow for Visual Blends
PY  - 2019
AB  - Visual blends are an advanced graphic design technique to draw attention to a message. They combine two objects in a way that is novel and useful in conveying a message symbolically. This paper presents VisiBlends, a flexible workflow for creating visual blends that follows the iterative design process. We introduce a design pattern for blending symbols based on principles of human visual object recognition. Our workflow decomposes the process into both computational techniques and human microtasks. It allows users to collaboratively generate visual blends with steps involving brainstorming, synthesis, and iteration. An evaluation of the workflow shows that decentralized groups can generate blends in independent microtasks, co-located groups can collaboratively make visual blends for their own messages, and VisiBlends improves novices' ability to make visual blends.
SP  - 172
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300402
ER  - 

TY  - NA
AU  - Sharma, Rhea; Nair, Atira; Guo, Ana; Palea, Dustin; Lee, David T.
TI  - "It's usually not worth the effort unless you get really lucky": Barriers to Undergraduate Research Experiences from the Perspective of Computing Faculty
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Conference on International Computing Education Research V.1
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3501385.3543976
ER  - 

TY  - JOUR
AU  - Gu, Yizheng; Yu, Chun; Li, Zhipeng; Li, Zhaoheng; Xiaoying, Wei; Shi, Yuanchun
TI  - QwertyRing: Text Entry on Physical Surfaces Using a Ring
PY  - 2020
AB  - The software keyboard is widely used on digital devices such as smartphones, computers, and tablets. The software keyboard operates via touch, which is efficient, convenient, and familiar to users. However, some emerging technology devices such as AR/VR headsets and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing, a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement Unit) ring. Users wear the ring on the middle phalanx of the index finger and type on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing, users do not focus on monitoring the hand motions. They receive text feedback on a separate screen, e.g., an AR/VR headset or a digital device display, such as a computer monitor. The basic idea of QwertyRing is to detect touch events and predict users' desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing through a five-day user study. Participants achieved a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based techniques [24, 30, 45, 68] and is 86.48% of the speed of typing on a smartphone with an index finger. The results show that QwertyRing enables efficient touch-based text entry on physical surfaces.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432204
ER  - 

TY  - NA
AU  - Butler, Crystal; Oster, Harriet; Togelius, Julian
TI  - IVA - Human-in-the-Loop AI for Analysis of Free Response Facial Expression Label Sets
PY  - 2020
AB  - Facial expressions (FEs) communicate a rich variety of social, grammatical, and affective signals. However, the most generally accepted set of recognizable FEs remains limited to seven basic displays of emotion: happiness, sadness, fear, anger, disgust, surprise and contempt. To develop intelligent virtual agents capable of interpreting and synthesizing nuanced facial behavior, we need a more complete lexicon. One roadblock has been the limiting nature of forced-choice study designs, the most common paradigm for investigating observer judgements of FEs. However, there has been no consensus on an objective way to evaluate alternative free response designs. We present a human-in-the-loop artificial intelligence pipeline for analyzing sets of freely chosen natural language labels. The pipeline, FreeRes-NLP, makes it possible to automatically identify whether there is consensus on the signal value of an FE and which label best classifies it. FreeRes-NLP scales to process very large datasets. We validate our approach in two stages: 1) comparison between label synonymy scores from ten computer algorithms and human raters across three synonym datasets, and 2) examples of pipeline results compared with manual data processing results from emotion and FE recognition studies. The pipeline can potentially improve automated facial expression recognition and procedural modeling of virtual humans.
SP  - NA
EP  - NA
JF  - Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3383652.3423892
ER  - 

TY  - CONF
AU  - Nishino, Ko; Subpa-asa, Art; Asano, Yuta; Shimano, Mihoko; Sato, Imari
TI  - ECCV (11) - Variable Ring Light Imaging: Capturing Transient Subsurface Scattering with An Ordinary Camera
PY  - 2018
AB  - Subsurface scattering plays a significant role in determining the appearance of real-world surfaces. A light ray penetrating into the subsurface is repeatedly scattered and absorbed by particles along its path before reemerging from the outer interface, which determines its spectral radiance. We introduce a novel imaging method that enables the decomposition of the appearance of a fronto-parallel real-world surface into images of light with bounded path lengths, i.e., transient subsurface light transport. Our key idea is to observe each surface point under a variable ring light: a circular illumination pattern of increasingly larger radius centered on it. We show that the path length of light captured in each of these observations is naturally lower-bounded by the ring light radius. By taking the difference of ring light images of incrementally larger radii, we compute transient images that encode light with bounded path lengths. Experimental results on synthetic and complex real-world surfaces demonstrate that the recovered transient images reveal the subsurface structure of general translucent inhomogeneous surfaces. We further show that their differences reveal the surface colors at different surface depths. The proposed method is the first to enable the unveiling of dense and continuous subsurface structures from steady-state external appearance using ordinary camera and illumination.
SP  - 598
EP  - 613
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Welsford-Ackroyd, Finn; Chalmers, Andrew; Anjos, Rafael Kuffner dos; Medeiros, Daniel; Kim, Hyejin; Rhee, Taehyun
TI  - Spectator View: Enabling Asymmetric Interaction between HMD Wearers and Spectators with a Large Display
PY  - 2021
AB  - In this paper, we present a system that allows a user with a head-mounted display (HMD) to communicate and collaborate with spectators outside of the headset. We evaluate its impact on task performance, immersion, and collaborative interaction. Our solution targets scenarios like live presentations or multi-user collaborative systems, where it is not convenient to develop a VR multiplayer experience and supply each user (and spectator) with an HMD. The spectator views the virtual world on a large-scale tiled video wall and is given the ability to control the orientation of their own virtual camera. This allows spectators to stay focused on the immersed user's point of view or freely look around the environment. To improve collaboration between users, we implemented a pointing system where a spectator can point at objects on the screen, which maps an indicator directly onto the objects in the virtual world. We conducted a user study to investigate the influence of rotational camera decoupling and pointing gestures in the context of HMD-immersed and non-immersed users utilizing a large-scale display. Our results indicate that camera decoupling and pointing positively impacts collaboration. A decoupled view is preferable in situations where both users need to indicate objects of interest in the scene, such as presentations and joint-task scenarios, as it requires a shared reference space. A coupled view, on the other hand, is preferable in synchronous interactions such as remote-assistant scenarios.
SP  - 1
EP  - 17
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3486951
ER  - 

TY  - JOUR
AU  - Lykourentzou, Ioanna; Vinella, Federica Lucia; Ahmed, Faez; Papastathis, Costas; Papangelis, Konstantinos; Khan, Vassilis-Javed; Masthoff, Judith
TI  - Self-organization in online collaborative work settings
PY  - 2022
AB  - <jats:p> As the volume and complexity of distributed online work increases, collaboration among people who have never worked together in the past is becoming increasingly necessary. Recent research has proposed algorithms to maximize the performance of online collaborations by grouping workers in a top-down fashion and according to a set of predefined decision criteria. This approach often means that workers have little say in the collaboration formation process. Depriving users of control over whom they will work with can stifle creativity and initiative-taking, increase psychological discomfort, and, overall, result in less-than-optimal collaboration results—especially when the task concerned is open-ended, creative, and complex. In this work, we propose an alternative model, called Self-Organizing Pairs (SOPs), which relies on the crowd of online workers themselves to organize into effective work dyads. Supported but not guided by an algorithm, SOPs are a new human-centered computational structure, which enables participants to control, correct, and guide the output of their collaboration as a collective. Experimental results, comparing SOPs to two benchmarks that do not allow user agency, and on an iterative task of fictional story writing, reveal that participants in the SOPs condition produce creative outcomes of higher quality, and report higher satisfaction with their collaboration. Finally, we find that similarly to machine learning-based self-organization, human SOPs exhibit emergent collective properties, including the presence of an objective function and the tendency to form more distinct clusters of compatible collaborators. </jats:p>
SP  - 263391372210780
EP  - 263391372210780
JF  - Collective Intelligence
VL  - 1
IS  - 1
PB  - 
DO  - 10.1177/26339137221078005
ER  - 

TY  - NA
AU  - Mirhosseini, Samim; Parnin, Chris
TI  - ESEC/SIGSOFT FSE - Docable: evaluating the executability of software tutorials
PY  - 2020
AB  - The typical software tutorial includes step-by-step instructions for installing developer tools, editing files and code, and running commands. When these software tutorials are not executable, either due to missing instructions, ambiguous steps, or simply broken commands, their value is diminished. Non-executable tutorials impact developers in several ways, including frustrating learning experiences, and limiting usability of developer tools. To understand to what extent software tutorials are executable---and why they may fail---we conduct an empirical study on over 600 tutorials, including nearly 15,000 code blocks. We find a naive execution strategy achieves an overall executability rate of only 26%. Even a human-annotation-based execution strategy---while doubling executability---still yields no tutorial that can successfully execute all steps. We identify several common executability barriers, ranging from potentially innocuous causes, such as interactive prompts requiring human responses, to insidious errors, such as missing steps and inaccessible resources. We validate our findings with major stakeholders in technical documentation and discuss possible strategies for improving software tutorials, such as providing accessible alternatives for tutorial takers, and investing in automated tutorial testing to ensure continuous quality of software tutorials.
SP  - 375
EP  - 385
JF  - Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3368089.3409706
ER  - 

TY  - NA
AU  - Chi, Pei-Yu Peggy; Long, Matthew; Gaur, Akshay; Deora, Abhimanyu; Batra, Anurag; Luong, Daphne
TI  - MobileHCI - Crowdsourcing Images for Global Diversity
PY  - 2019
AB  - Crowdsourcing enables human workers to perform designated tasks unbounded by time and location. As mobile devices and embedded cameras have become widely available, we deployed an image capture task globally for more geographically diverse images. Via our micro-crowdsourcing mobile application, users capture images of surrounding subjects, tag with keywords, and can choose to open source their work. We open-sourced 478,000 images collected from worldwide users as a dataset "Open Images Extended" that aims to add global diversity to imagery training data. We describe our approach and workers' feedback through survey responses from 171 global contributors to this task.
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3338286.3347546
ER  - 

TY  - NA
AU  - Ly, Duy-Nam; La, Thanh-Thai; Le, Khanh-Duy; Nguyen, Cuong; Fjeld, Morten; Tran, Thanh Ngoc-Dat; Tran, Minh-Triet
TI  - 360TourGuiding: Towards Virtual Reality Training for Tour Guiding
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Publication of the 24th International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3528575.3551436
ER  - 

TY  - CHAP
AU  - Mayer, Anjela; Combe, Théo; Chardonnet, Jean-Rémy; Ovtcharova, Jivka
TI  - Asynchronous Manual Work in Mixed Reality Remote Collaboration
PY  - 2022
AB  - AbstractResearch in Collaborative Virtual Environments (CVEs) is becoming more and more significant with increasing accessibility of Virtual Reality (VR) and Augmented Reality (AR) technology, additionally reinforced by the increasing demand for remote collaboration groupware. While the research is focusing on methods for synchronous remote collaboration, asynchronous remote collaboration remains a niche. Nevertheless, future CVEs should support both paradigms of collaborative work, since asynchronous collaboration has as well its benefits, for instance a more flexible time-coordination. In this paper we present a concept of recording and later playback of highly interactive collaborative tasks in Mixed Reality (MR). Furthermore, we apply the concept in an assembly training scenario from the manufacturing industry and test it during pilot user experiments. The pilot study compared two modalities, the first one with a manufacturing manual, and another using our concept and featuring a ghost avatar. First results revealed no significant differences between both modalities in terms of time completion, hand movements, cognitive workload and usability. Some differences were not expected, however, these results and the feedback brought by the participants provide insights to further develop our concept.KeywordsAsynchronous remote collaborationCollaborative Virtual EnvironmentsMixed RealityAsymmetric collaboration
SP  - 17
EP  - 33
JF  - Extended Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-15553-6_2
ER  - 

TY  - NA
AU  - Borowski, Marcel; Klokmose, Clemens Nylandsted
TI  - Webstrates, Codestrates v2, and Varv: A Software Stack for Computational Media
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3547522.3547714
ER  - 

TY  - JOUR
AU  - Paredes, Luis; Reddy, Sai Swarup; Chidambaram, Subramanian; Vagholkar, Devashri; Zhang, Yunbo; Benes, Bedrich; Ramani, Karthik
TI  - FabHandWear: An End-to-End Pipeline from Design to Fabrication of Customized Functional Hand Wearables
PY  - 2021
AB  - Current hand wearables have limited customizability, they are loose-fit to an individual's hand and lack comfort. The main barrier in customizing hand wearables is the geometric complexity and size variation in hands. Moreover, there are different functions that the users can be looking for; some may only want to detect hand's motion or orientation; others may be interested in tracking their vital signs. Current wearables usually fit multiple functions and are designed for a universal user with none or limited customization. There are no specialized tools that facilitate the creation of customized hand wearables for varying hand sizes and provide different functionalities. We envision an emerging generation of customizable hand wearables that supports hand differences and promotes hand exploration with additional functionality. We introduce FabHandWear, a novel system that allows end-to-end design and fabrication of customized functional self-contained hand wearables. FabHandWear is designed to work with off-the-shelf electronics, with the ability to connect them automatically and generate a printable pattern for fabrication. We validate our system by using illustrative applications, a durability test, and an empirical user evaluation. Overall, FabHandWear offers the freedom to create customized, functional, and manufacturable hand wearables.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463518
ER  - 

TY  - BOOK
AU  - Eriksson, Eva; Iversen, Ole Sejer; Baykal, Gökçe Elif; Van Mechelen, Maarten; Smith, Rachel Charlotte; Wagner, Marie-Louise; Fog, Bjarke Vognstrup; Klokmose, Clemens Nylandsted; Cumbo, Bronwyn J.; Hjorth, Arthur; Musaeus, Line Have; Petersen, Marianne Graves; Bouvin, Niels Olof
TI  - FabLearn - Widening the scope of FabLearn Research: Integrating Computational Thinking, Design and Making
PY  - 2019
AB  - FabLearn has primarily been concerned with studies of digital fabrication technologies in education, however, we witness an increased interest in integrating other related topics such as computational thinking, digital design and empowerment as an integrated whole. In this paper, we present a five years design research program for digital fabrication, computational thinking and design, to highlight why the FabLearn community should embrace this wider agenda to accomplish its ultimate goal to encourage a new generation to critically and constructively engage in the design of digital technology. The contribution of this paper is a number of open questions and considerations regarding the scope of European FabLearn research that we hope the community will consider and that might give rise to further discussions.
SP  - 3335070
EP  - NA
JF  - Proceedings of the FabLearn Europe 2019 Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3335055.3335070
ER  - 

TY  - JOUR
AU  - Colley, Mark; Wankmüller, Bastian; Rukzio, Enrico
TI  - A Systematic Evaluation of Solutions for the Final 100m Challenge of Highly Automated Vehicles
PY  - 2022
AB  - <jats:p>Automated vehicles will change the interaction with the user drastically. While freeing the user of the driving task for most of the journey, the "final 100 meters problem'', directing the vehicle to the final parking spot, could require human intervention. Therefore, we present a classification of interaction concepts for automated vehicles based on modality and interaction mode. In a subsequent Virtual Reality study (N=16), we evaluated sixteen interaction concepts. We found that the medially abstracted interaction mode was consistently rated most usable over all modalities (joystick, speech, gaze, gesture, and tablet). While the steering wheel was still preferred, our findings indicate that other interaction concepts are usable if the steering wheel were unavailable.</jats:p>
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546713
ER  - 

TY  - NA
AU  - Thoravi Kumaravel, Balasaravanan; Wilson, Andrew D
TI  - DreamStream: Immersive and Interactive Spectating in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517508
ER  - 

TY  - JOUR
AU  - Fanchao, Zhong; Wenqiang, Liu; Zhou, Yu; Yan, Xin; Wan, Yi; Lu, Lin
TI  - Ceramic 3D printed sweeping surfaces
PY  - 2020
AB  - NA
SP  - 108
EP  - 115
JF  - Computers & Graphics
VL  - 90
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2020.05.007
ER  - 

TY  - NA
AU  - Grønbæk, Jens Emil; Saatçi, Banu; Griggio, Carla F.; Klokmose, Clemens Nylandsted
TI  - CHI - MirrorBlender: Supporting Hybrid Meetings with a Malleable Video-Conferencing System
PY  - 2021
AB  - In hybrid meetings, multiple co-located participants communicate with remote participants through video. But video communication inhibits non-verbal cues, and this often causes remote participants to feel excluded. To address this issue, we built MirrorBlender: a What-You-See-Is-What-I-See video-conferencing system for blending, repositioning, and resizing mirrors. Mirrors here denote shared video feeds of people and screens. In a qualitative study of MirrorBlender with three hybrid meeting sessions, we found that the shared control of mirrors supported users in negotiating a blended interpersonal space. Moreover, it enabled diverse acts of inclusion of remote participants. In particular, remote participants brought attention to themselves by manipulating the position, scale, and translucency of their camera and screen feeds. Participants also embodied and leveraged their mirror images for deictic gestures and playful interactions. Based on these findings, we discuss new opportunities for supporting video-mediated collaboration.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445698
ER  - 

TY  - NA
AU  - Wang, Miao; Li, Yi-Jun; Zhang, Wen-Xuan; Richardt, Christian; Hu, Shi-Min
TI  - ISMAR - Transitioning360: Content-aware NFoV Virtual Camera Paths for 360° Video Playback
PY  - 2020
AB  - Despite the increasing number of head-mounted displays, many 360° VR videos are still being viewed by users on existing 2D displays. To this end, a subset of the 360° video content is often shown inside a manually or semi-automatically selected normal-field-of-view (NFoV) window. However, during the playback, simply watching an NFoV video can easily miss concurrent off-screen content. We present Transitioning360, a tool for 360° video navigation and playback on 2D displays by transitioning between multiple NFoV views that track potentially interesting targets or events. Our method computes virtual NFoV camera paths considering content awareness and diversity in an offline preprocess. During playback, the user can watch any NFoV view corresponding to a precomputed camera path. Moreover, our interface shows other candidate views, providing a sense of concurrent events. At any time, the user can transition to other candidate views for fast navigation and exploration. Experimental results including a user study demonstrate that the viewing experience using our method is more enjoyable and convenient than previous methods.
SP  - 185
EP  - 194
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00040
ER  - 

TY  - JOUR
AU  - Zhang, Jiayi Eris; Bang, Seungbae; Levin, David I. W.; Jacobson, Alec
TI  - Complementary dynamics
PY  - 2020
AB  - We present a novel approach to enrich arbitrary rig animations with elastodynamic secondary effects. Unlike previous methods which pit rig displacements and physical forces as adversaries against each other, we advocate that physics should complement artists' intentions. We propose optimizing for elastodynamic displacements in the subspace orthogonal to displacements that can be created by the rig. This ensures that the additional dynamic motions do not undo the rig animation. The complementary space is high-dimensional, algebraically constructed without manual oversight, and capable of rich high-frequency dynamics. Unlike prior tracking methods, we do not require extra painted weights, segmentation into fixed and free regions or tracking clusters. Our method is agnostic to the physical model and plugs into non-linear FEM simulations, geometric as-rigid-as-possible energies, or mass-spring models. Our method does not require a particular type of rig and adds secondary effects to skeletal animations, cage-based deformations, wire deformers, motion capture data, and rigid-body simulations.
SP  - 1
EP  - 11
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 6
PB  - 
DO  - 10.1145/3414685.3417819
ER  - 

TY  - NA
AU  - Schön, Dominik; Kosch, Thomas; Schmitz, Martin; Müller, Florian; Günther, Sebastian; Kreutz, Johannes; Mühlhäuser, Max
TI  - TrackItPipe: A Fabrication Pipeline To Incorporate Location and Rotation Tracking Into 3D Printed Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558719
ER  - 

TY  - BOOK
AU  - Correia, António; Jameel, Shoaib; Paredes, Hugo; Fonseca, Benjamim; Schneider, Daniel
TI  - Macrotask Crowdsourcing - Hybrid Machine-Crowd Interaction for Handling Complexity: Steps Toward a Scaffolding Design Framework
PY  - 2019
AB  - Much research attention on crowd work is paid to the development of solutions for enhancing microtask crowdsourcing settings. Although decomposing difficult problems into microtasks is appropriate for many situations, several problems are non-decomposable and require high levels of coordination among crowd workers. In this chapter, we aim to gain a better understanding of the macrotask crowdsourcing problem and the integration of crowd-AI mechanisms for solving complex tasks distributed across expert crowds and machines. We also explore some design implications of macrotask crowdsourcing systems taking into account their scaling abilities to support complex work in science.
SP  - 149
EP  - 161
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-12334-5_5
ER  - 

TY  - NA
AU  - Jonsson, Martin; Tholander, Jakob
TI  - Cracking the code: Co-coding with AI in creative programming education
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3532801
ER  - 

TY  - NA
AU  - Nguyen, Cuong; DiVerdi, Stephen; Hertzmann, Aaron; Liu, Feng
TI  - CHI - Depth Conflict Reduction for Stereo VR Video Interfaces
PY  - 2018
AB  - Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search.
SP  - 64
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173638
ER  - 

TY  - JOUR
AU  - Lu, Min; Fish, Noa; Wang, Shuaiqi; Lanir, Joel; Cohen-Or, Daniel; Huang, Hui
TI  - Enhancing Static Charts with Data-driven Animations.
PY  - 2022
AB  - Static visual attributes such as color and shape are used with great success in visual charts designed to be displayed in static, hard-copy form. However, nowadays digital displays become ubiquitous in the visualization of any form of data, lifting the confines of static presentations. In this work, we propose incorporating data-driven animations to bring static charts to life, with the purpose of encoding and emphasizing certain attributes of the data. We lay out a design space for data-driven animated effects and experiment with three versatile effects, marching ants, geometry deformation and gradual appearance. For each, we provide practical details regarding their mode of operation and extent of interaction with existing visual encodings. We examine the impact and effectiveness of our enhancements through an empirical user study to assess preference as well as gauge the influence of animated effects on human perception in terms of speed and accuracy of visual understanding.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2020.3037300
ER  - 

TY  - NA
AU  - Schmitz, Martin; Herbers, Martin; Dezfuli, Niloofar; Günther, Sebastian; Mühlhäuser, Max
TI  - CHI - Off-Line Sensing: Memorizing Interactions in Passive 3D-Printed Objects
PY  - 2018
AB  - Embedding sensors into objects allow them to recognize various interactions. However, sensing usually requires active electronics that are often costly, need time to be assembled, and constantly draw power. Thus, we propose off-line sensing: passive 3D-printed sensors that detect one-time interactions, such as accelerating or flipping, but neither require active electronics nor power at the time of the interaction. They memorize a pre-defined interaction via an embedded structure filled with a conductive medium (e.g., a liquid). Whether a sensor was exposed to the interaction can be read-out via a capacitive touchscreen. Sensors are printed in a single pass on a consumer-level 3D printer. Through a series of experiments, we show the feasibility of off-line sensing.
SP  - 182
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3173756
ER  - 

TY  - NA
AU  - Widjojo, EA
TI  - Using virtual reality in the structural measurement of plantation Pinus radiata
PY  - 2021
AB  - Point cloud is a set of data points that is generally used for big data visualisation. Point cloud can render massive and complex data points in 3D space to represent objects or structures. Advanced user interfaces are widely integrated into modern computing devices enabling interaction between human and large data. Virtual Reality (VR) technologies have demonstrated their potential to provide virtual environment as a medium in exploration of large point cloud data, which is crucial in data analysis. VR technologies have showed positive results when integrated as training/simulation to some domains such as economic, military defence, and education. Integrating point cloud data into immersive VR could potentially support structural estimation of point cloud. This research focuses on the structural estimation of the point cloud data in VR using radiata pine plantation data. This research compares task performance between VR-point cloud assessment and field assessment, focusing on radiata pine plantation data. In addition to the task performance comparison, feedback about experience and impression of assessing radiata pine in VR-point cloud was collected from practitioners and analysed both quantitatively and qualitatively. Results from this research are useful to reveal the strengths and weaknesses of the VR-point cloud for structural estimation tasks in radiata pine trees.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kumaravel, Balasaravanan Thoravi; Nguyen, Cuong; DiVerdi, Stephen; Hartmann, Björn
TI  - CHI - TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality
PY  - 2019
AB  - Virtual Reality painting is a form of 3D-painting done in a Virtual Reality (VR) space. Being a relatively new kind of art form, there is a growing interest within the creative practices community to learn it. Currently, most users learn using community posted 2D-videos on the internet, which are a screencast recording of the painting process by an instructor. While such an approach may suffice for teaching 2D-software tools, these videos by themselves fail in delivering crucial details that required by the user to understand actions in a VR space. We conduct a formative study to identify challenges faced by users in learning to VR-paint using such video-based tutorials. Informed by results of this study, we develop a VR-embedded tutorial system that supplements video tutorials with 3D and contextual aids directly in the user's VR environment. An exploratory evaluation showed users were positive about the system and were able to use the proposed system to recreate painting tasks in VR.
SP  - 284
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300514
ER  - 

TY  - JOUR
AU  - Kumar, Kartikaeya; Poretski, Lev; Li, Jiannan; Tang, Anthony
TI  - Tourgether360: Collaborative Exploration of 360° Videos using Pseudo-Spatial Navigation
PY  - 2022
AB  - <jats:p>Collaborative exploration of 360 videos with contemporary interfaces is challenging because collaborators do not have awareness of one another's viewing activities. Tourgether360 enhances social exploration of 360° tour videos using a pseudo-spatial navigation technique that provides both an overhead "context" view of the environment as a minimap, as well as a shared pseudo-3D environment for exploring the video. Collaborators are embodied as avatars along a track depending on their position in the video timeline and can point and synchronize their playback. We evaluated the Tourgether360 concept through two studies: first, a comparative study with a simplified version of Tourgether360 with collaborator embodiments and a minimap versus a conventional interface; second, an exploratory study where we studied how collaborators used Tourgether360 to navigate and explore 360° environments together. We found that participants adopted the Tourgether360 approach with ease and enjoyed the shared social aspects of the experience. Participants reported finding the experience similar to an interactive social video game.</jats:p>
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555604
ER  - 

TY  - JOUR
AU  - Peng, Hao; Liu, Peiqing; Lu, Lin; Sharf, Andrei; Liu, Lin; Lischinski, Dani; Chen, Baoquan
TI  - Fabricable Unobtrusive 3D-QR-Codes with Directional Light
PY  - 2020
AB  - NA
SP  - 15
EP  - 27
JF  - Computer Graphics Forum
VL  - 39
IS  - 5
PB  - 
DO  - 10.1111/cgf.14065
ER  - 

TY  - NA
AU  - Rule, Adam; Tabard, Aurélien; Hollan, James D.
TI  - CHI - Exploration and Explanation in Computational Notebooks
PY  - 2018
AB  - Computational notebooks combine code, visualizations, and text in a single document. Researchers, data analysts, and even journalists are rapidly adopting this new medium. We present three studies of how they are using notebooks to document and share exploratory data analyses. In the first, we analyzed over 1 million computational notebooks on GitHub, finding that one in four had no explanatory text but consisted entirely of visualizations or code. In a second study, we examined over 200 academic computational notebooks, finding that although the vast majority described methods, only a minority discussed reasoning or results. In a third study, we interviewed 15 academic data analysts, finding that most considered computational notebooks personal, exploratory, and messy. Importantly, they typically used other media to share analyses. These studies demonstrate a tension between exploration and explanation in constructing and sharing computational notebooks. We conclude with opportunities to encourage explanation in computational media without hindering exploration.
SP  - 32
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - 32
PB  - 
DO  - 10.1145/3173574.3173606
ER  - 

TY  - NA
AU  - Lin, Richard; Ramesh, Rohit; Dutta, Prabal; Hartmann, Bjoern; Mehta, Ankur
TI  - Hierarchical Computational Design of Board-Level Electronics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3565588
ER  - 

TY  - NA
AU  - Wang, Tianyi; Qian, Xun; He, Fengming; Hu, Xiyun; Cao, Yuanzhi; Ramani, Karthik
TI  - UIST - GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications
PY  - 2021
AB  - Freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR. Further, we demonstrate multiple application scenarios enabled by GesturAR, such as interactive virtual objects, robots, and avatars, room-level interactive AR spaces, embodied AR presentations, etc. Finally, we evaluate the performance and usability of GesturAR through a user study.
SP  - 552
EP  - 567
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474769
ER  - 

TY  - JOUR
AU  - Cai, Ruifan; Lin, Yingying; Li, Honglin; Zhu, Yuzhen; Tang, Xiangjun; Weng, Yanjun; You, Lihua; Jin, Xiaogang
TI  - Wowtao: A personalized pottery-making system
PY  - 2021
AB  - NA
SP  - 103325
EP  - NA
JF  - Computers in Industry
VL  - 124
IS  - NA
PB  - 
DO  - 10.1016/j.compind.2020.103325
ER  - 

TY  - NA
AU  - Krosnick, Rebecca; Anderson, Fraser; Matejka, Justin; Oney, Steve; Lasecki, Walter S.; Grossman, Tovi; Fitzmaurice, George
TI  - CHI - Think-Aloud Computing: Supporting Rich and Low-Effort Knowledge Capture
PY  - 2021
AB  - When users complete tasks on the computer, the knowledge they leverage and their intent is often lost because it is tedious or challenging to capture. This makes it harder to understand why a colleague designed a component a certain way or to remember requirements for software you wrote a year ago. We introduce think-aloud computing, a novel application of the think-aloud protocol where computer users are encouraged to speak while working to capture rich knowledge with relatively low effort. Through a formative study we find people shared information about design intent, work processes, problems encountered, to-do items, and other useful information. We developed a prototype that supports think-aloud computing by prompting users to speak and contextualizing speech with labels and application context. Our evaluation shows more subtle design decisions and process explanations were captured in think-aloud than via traditional documentation. Participants reported that think-aloud required similar effort as traditional documentation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445066
ER  - 

TY  - JOUR
AU  - Hu, Chendi
TI  - Evaluation of physical education classes in colleges and universities using machine learning
PY  - 2022
AB  - NA
SP  - 10765
EP  - 10773
JF  - Soft Computing
VL  - 26
IS  - 20
PB  - 
DO  - 10.1007/s00500-022-06983-3
ER  - 

TY  - JOUR
AU  - Peng, Zhenhui; Ma, Xiaojuan
TI  - A survey on construction and enhancement methods in service chatbots design
PY  - 2019
AB  - Chatbots are being widely applied in many service industries to help schedule meetings, online shopping, restaurant reservations, customer care and so on. The key to the success of the service chatbots design is to provide satisfying responses to the given user’s requests. This survey aims to provide a comprehensive review of chatbots construction and enhancement methods. We first introduce major techniques for the three core design philosophies, which are rule-based, retrieval-based and generation-based methods, followed by a brief summary of the evaluation metrics. Then we present methods to enhance service chatbot’s capabilities with either an ensemble of multiple chatbots, collaborating with human workers or learning from users. Finally, in future directions we discuss the promising response generation models for chatbots using the recent progress in the transformer and contextual embeddings, as well as potential ways to construct a chatbot with personality to achieve a better user experience.
SP  - 204
EP  - 223
JF  - CCF Transactions on Pervasive Computing and Interaction
VL  - 1
IS  - 3
PB  - 
DO  - 10.1007/s42486-019-00012-3
ER  - 

TY  - NA
AU  - Rasel, Islam
TI  - EdgeGlass: Exploring Tapping Performance on Smart Glasses while Sitting and Walking
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Yang, Jingru; Peng, Hao; Liu, Lin; Lu, Lin
TI  - 3D printed perforated QR codes
PY  - 2019
AB  - NA
SP  - 117
EP  - 124
JF  - Computers & Graphics
VL  - 81
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2019.04.005
ER  - 

TY  - NA
AU  - Le, Huy Viet; Mayer, Sven; Henze, Niels
TI  - IUI - Investigating the feasibility of finger identification on capacitive touchscreens using deep learning
PY  - 2019
AB  - Touchscreens enable intuitive mobile interaction. However, touch input is limited to 2D touch locations which makes it challenging to provide shortcuts and secondary actions similar to hardware keyboards and mice. Previous work presented a wide range of approaches to provide secondary actions by identifying which finger touched the display. While these approaches are based on external sensors which are inconvenient, we use capacitive images from mobile touchscreens to investigate the feasibility of finger identification. We collected a dataset of low-resolution fingerprints and trained convolutional neural networks that classify touches from eight combinations of fingers. We focused on combinations that involve the thumb and index finger as these are mainly used for interaction. As a result, we achieved an accuracy of over 92% for a position-invariant differentiation between left and right thumbs. We evaluated the model and two use cases that users find useful and intuitive. We publicly share our data set (CapFingerld) comprising 455,709 capacitive images of touches from each finger on a representative mutual capacitive touchscreen and our models to enable future work using and improving them.
SP  - 637
EP  - 649
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302295
ER  - 

TY  - NA
AU  - Huang, Haikun; Zhang, Yuxuan; Weiss, Tomer; Perry, Rebecca W.; Yu, Lap-Fai
TI  - AIVR - Interactive Design of Gallery Walls via Mixed Reality
PY  - 2020
AB  - We present a novel interactive design tool that allows users to create and visualize gallery walls via a mixed reality device. To use our tool, a user selects a wall to decorate and chooses a focal art item. Our tool then helps the user complete their design by optionally recommending additional art items or automatically completing both the selection and placement of additional art items. Our tool holistically considers common design criteria such as alignment, color, and style compatibility in the synthesis of a gallery wall. Through a mixed reality device, such as a Magic Leap One headset, the user can instantly visualize the gallery wall design in situ and can interactively modify the design in collaboration with our tool’s suggestion engine. We describe the suggestion engine and its adaptability to users with different design goals. We also evaluate our mixedreality-based tool for creating gallery wall designs and compare it with a 2D interface, providing insights for devising mixed reality interior design applications.
SP  - 17
EP  - 26
JF  - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr50618.2020.00013
ER  - 

TY  - BOOK
AU  - Hassoumi, Almoctar; Peysakhovich, Vsevolod; Hurter, Christophe
TI  - ETRA - EyeFlow: pursuit interactions using an unmodified camera
PY  - 2019
AB  - We investigate the smooth pursuit eye movement based interaction using an unmodified off-the-shelf RGB camera. In each pair of sequential video frames, we compute the indicative direction of the eye movement by analyzing flow vectors obtained using the Lucas-Kanade optical flow algorithm. We discuss how carefully selected low vectors could replace the traditional pupil centers detection in smooth pursuit interaction. We examine implications of unused features in the eye camera imaging frame as potential elements for detecting gaze gestures. This simple approach is easy to implement and abstains from many of the complexities of pupil based approaches. In particular, EyeFlow does not call for either a 3D pupil model or 2D pupil detection to track the pupil center location. We compare this method to state-of-the-art approaches and ind that this can enable pursuit interactions with standard cameras. Results from the evaluation with 12 users data yield an accuracy that compares to previous studies. In addition, the benefit of this work is that the approach does not necessitate highly matured computer vision algorithms and expensive IR-pass cameras.
SP  - 1
EP  - 10
JF  - Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314111.3319820
ER  - 

TY  - CONF
AU  - Simoiu, Camelia; Sumanth, Chiraag; Mysore, Alok; Goel, Sharad
TI  - Studying the “Wisdom of Crowds” at Scale
PY  - 2019
AB  - In a variety of problem domains, it has been observed that the aggregate opinions of groups are often more accurate than those of the constituent individuals, a phenomenon that has been dubbed the “wisdom of the crowd”. However, due to the varying contexts, sample sizes, methodologies, and scope of previous studies, it has been difficult to gauge the extent to which conclusions generalize. To investigate this question, we carried out a large online experiment to systematically evaluate crowd performance on 1,000 questions across 50 topical domains. We further tested the effect of different types of social influence on crowd performance. For example, in one condition, participants could see the cumulative crowd answer before providing their own. In total, we collected more than 500,000 responses from nearly 2,000 participants. We have three main results. First, averaged across all questions, we find that the crowd indeed performs better than the average individual in the crowd—but we also find substantial heterogeneity in performance across questions. Second, we find that crowd performance is generally more consistent than that of individuals; as a result, the crowd does considerably better than individuals when performance is computed on a full set of questions within a domain. Finally, we find that social influence can, in some instances, lead to herding, decreasing crowd performance. Our findings illustrate some of the subtleties of the wisdom-of-crowds phenomenon, and provide insights for the design of social recommendation platforms.
SP  - 171
EP  - 179
JF  - NA
VL  - 7
IS  - 1
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Hsiao, Luke; Wu, Sen; Chiang, Nicholas; Ré, Christopher; Levis, Philip
TI  - LCTES - Automating the generation of hardware component knowledge bases
PY  - 2019
AB  - Hardware component databases are critical resources in designing embedded systems. Since generating these databases requires hundreds of thousands of hours of manual data entry, they are proprietary, limited in the data they provide, and have many random data entry errors. We present a machine-learning based approach for automating the generation of component databases directly from datasheets. Extracting data directly from datasheets is challenging because: (1) the data is relational in nature and relies on non-local context, (2) the documents are filled with technical jargon, and (3) the datasheets are PDFs, a format that decouples visual locality from locality in the document. The proposed approach uses a rich data model and weak supervision to address these challenges. We evaluate the approach on datasheets of three classes of hardware components and achieve an average quality of 75 F1 points which is comparable to existing human-curated knowledge bases. We perform two applications studies that demonstrate the extraction of multiple data modalities such as numerical properties and images. We show how different sources of supervision such as heuristics and human labels have distinct advantages which can be utilized together within a single methodology to automatically generate hardware component knowledge bases.
SP  - 163
EP  - 176
JF  - Proceedings of the 20th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3316482.3326344
ER  - 

TY  - NA
AU  - Chi, Peggy; Sun, Zheng; Panovich, Katrina; Essa, Irfan
TI  - UIST - Automatic Video Creation From a Web Page
PY  - 2020
AB  - Creating marketing videos from scratch can be challenging, especially when designing for multiple platforms with different viewing criteria. We present URL2Video, an automatic approach that converts a web page into a short video given temporal and visual constraints. URL2Video captures quality materials and design styles extracted from a web page, including fonts, colors, and layouts. Using constraint programming, URL2Video's design engine organizes the visual assets into a sequence of shots and renders to a video with user-specified aspect ratio and duration. Creators can review the video composition, modify constraints, and generate video variation through a user interface. We learned the design process from designers and compared our automatically generated results with their creation through interviews and an online survey. The evaluation shows that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.
SP  - 279
EP  - 292
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415814
ER  - 

TY  - NA
AU  - Vazquez, Christian; Tan, Nicole; Sadalgi, Shrenik
TI  - CHI Extended Abstracts - Home Studio: A Mixed Reality Staging Tool for Interior Design
PY  - 2021
AB  - Home design services employ virtual real estate staging to visually convey proposals to prospective customers, instilling confidence in property owners on how spaces will look once they are furnished. However, creating room-scale visualizations requires days of expert labor to composite products on images provided by the customer. Advances in 3D capture now let users scan their spaces with a smartphone in a matter of minutes, enabling scale-accurate mixed reality experiences that can be leveraged to lower the skill bar and time required to produce visualizations of furniture in the user’s context. We present Home Studio, a web-based tool that lets designers stage any Matterport scan and generate photorealistic renders of products in the user’s context. Our tool enables a drag and drop experience that reconstructs the perspective view of the Matterport scene in a remote service, employing a rendering engine to produce a photorealistic composite.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451711
ER  - 

TY  - NA
AU  - Li, Jianping Kelvin; Ma, Kwan-Liu
TI  - P6: A Declarative Language for Integrating Machine Learning in Visual Analytics.
PY  - 2020
AB  - We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Ibarra, Alejandro; Darbois-Texier, Baptiste; Melo, Francisco
TI  - Designing a Contact Fingertip Sensor Made Using a Soft 3D Printing Technique.
PY  - 2022
AB  - The development of highly compliant materials and actuators has enabled the design of soft robots that can be applied in rescue operations, in secure human-robot interactions, to manipulate fragile devices or objects, and for robot locomotion within complex environments. To develop reliable solutions for soft robotics applications, devices with the ability to deform and change shape are required, which must be equipped with appropriate sensors capable of withstanding large deformations at suitable speeds and respond repeatedly. This work presents a methodology to build strain sensors made of sensitive, thin, and conductive channels printed inside a soft matrix, using three-dimensional printing. As proof of concept, rectangular beams and semispherical caps embedded with sensitive circuits are developed that are designed to deform under applied forces and detect the gradual contact with objects. The rectangular beam with conductive lines separated from the neutral plane exhibits a quasi-linear electrical response as a function of the applied shear strain. Mechanical diodes, which trigger an activated response once a given deformation onset is exceeded, are implemented using circumferential conductive channels that are centered with the spherical body sensor. Sinusoidally shaped conductive channels located at a given distance from the spherical surface produce a monotonic electrical response, which detects deformations over a broad range. Linear sensors, with enhanced sensitivity to compression, are created if the sensitive conductive channels are oriented along the compression direction. Numerical calculations, used to guide the design of the sensor, show the capability of these sensors to measure simultaneous normal and tangential forces, making them suitable for applications involving fragile object manipulation and robot locomotion. An example of application of these sensors in the control of the forces applied by soft gripper lifting an object is given.
SP  - 1210
EP  - 1219
JF  - Soft robotics
VL  - 9
IS  - 6
PB  - 
DO  - 10.1089/soro.2021.0128
ER  - 

TY  - JOUR
AU  - Shin, Choonsung; Hong, Sung-Hee; Jeong, Hieyoung; Yoon, Hyoseok; Koh, Byoungsoo
TI  - All-in-one encoder/decoder approach for non-destructive identification of 3D-printed objects
PY  - 2022
AB  - <jats:p xml:lang="fr">&lt;abstract&gt;&lt;p&gt;This paper presents an all-in-one encoder/decoder approach for the nondestructive identification of three-dimensional (3D)-printed objects. The proposed method consists of three parts: 3D code insertion, terahertz (THz)-based detection, and code extraction. During code insertion, a relevant one-dimensional (1D) identification code is generated to identify the 3D-printed object. A 3D barcode corresponding to the identification barcode is then generated and inserted into a blank bottom area inside the object's stereolithography (STL) file. For this objective, it is necessary to find an appropriate area of the STL file and to merge the 3D barcode and the model within the STL file. Next the information generated inside the object is extracted by using THz waves that are transmitted and reflected by the output 3D object. Finally, the resulting THz signal from the target object is detected and analyzed to extract the identification information. We implemented and tested the proposed method using a 3D graphic environment and a THz time-domain spectroscopy system. The experimental results indicate that one-dimensional barcodes are useful for identifying 3D-printed objects because they are simple and practical to process. Furthermore, information efficiency can be increased by using an integral fast Fourier transform to identify any code located in areas deeper within the object. As 3D printing is used in various fields, the proposed method is expected to contribute to the acceleration of the distribution of 3D printing empowered by the integration of the internal code insertion and recognition process.&lt;/p&gt;&lt;/abstract&gt;</jats:p>
SP  - 14102
EP  - 14115
JF  - Mathematical Biosciences and Engineering
VL  - 19
IS  - 12
PB  - 
DO  - 10.3934/mbe.2022657
ER  - 

TY  - NA
AU  - Dube, Tafadzwa Joseph; Arif, Ahmed Sabbir
TI  - CHI Extended Abstracts - Impact of Key Shape and Dimension on Text Entry in Virtual Reality
PY  - 2020
AB  - Virtual Qwerty is the most popular method of text entry in virtual reality. Since virtual keyboards are not constrained by the physical limitations of actual keyboards, designers are taking the liberty of designing novelty keys for these keyboards. However, it is unknown whether key design affects text entry performance or user experience. This work presents results of a user study that investigated the effects of different key shapes and dimensions on text entry performance and user experience. Results revealed that key shape affects text entry speed, dimension affects accuracy, while both affect user experience. Overall, square-shaped 3D keys yielded the best actual and perceived performance, also was the most preferred by the users.
SP  - 1
EP  - 10
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382882
ER  - 

TY  - NA
AU  - Yamamoto, Kenta; Kawamura, Ryota; Takazawa, Kazuki; Osone, Hiroyuki; Ochiai, Yoichi
TI  - A Preliminary Study for Identification of Additive Manufactured Objects with Transmitted Images
PY  - 2020
AB  - Additive manufacturing has the potential to become a standard method for manufacturing products, and product information is indispensable for the item distribution system. While most products are given barcodes to the exterior surfaces, research on embedding barcodes inside products is underway. This is because additive manufacturing makes it possible to carry out manufacturing and information adding at the same time, and embedding information inside does not impair the exterior appearance of the product. However, products that have not been embedded information can not be identified, and embedded information can not be rewritten later. In this study, we have developed a product identification system that does not require embedding barcodes inside. This system uses a transmission image of the product which contains information of each product such as different inner support structures and manufacturing errors. We have shown through experiments that if datasets of transmission images are available, objects can be identified with an accuracy of over 90%. This result suggests that our approach can be useful for identifying objects without embedded information.
SP  - NA
EP  - NA
JF  - arXiv: Image and Video Processing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Bermejo-Berros, Jesús; Martínez, Miguel Angel Gil
TI  - The relationships between the exploration of virtual space, its presence and entertainment in virtual reality, 360º and 2D
PY  - 2021
AB  - This research investigates the relationships between the way virtual space is explored, the perception of presence and the degree of entertainment experienced during the experience. All participants (N = 147) interact with an omnidirectional video clip in three different conditions (VR, 360o, 2D). Throughout the two experimental sessions, affective, cognitive, and behavioural information is collected from the participant, which allows us to relate their interactive behaviour, their perception of presence and degree of entertainment. The possible influence of experience with interactive systems on current interactive behaviour is also analysed. The results highlight the complex relationships between these nuclear dimensions of VR and indicate the existence of two types of exploratory behaviour that we have called interface dependent and interface independent. When the first is present, there is no connection with the positive perception of presence and entertainment, but there is in the second. This typology shows the need to consider the learning processes in the access to the content through the interface in digital interactive systems such as VR and 360o.
SP  - 1043
EP  - 1059
JF  - Virtual Reality
VL  - 25
IS  - 4
PB  - 
DO  - 10.1007/s10055-021-00510-9
ER  - 

TY  - NA
AU  - Wang, Zeyu; Nguyen, Cuong; Asente, Paul; Dorsey, Julie
TI  - Point Cloud Capture and Editing for AR Environmental Design
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558636
ER  - 

TY  - CONF
AU  - Brun, Damien; Gouin-Vallerand, Charles; George, Sébastien
TI  - Toward Discreet Interactions and Publicly Explicit Activities
PY  - 2019
AB  - In this position paper, we present two different approaches to help social acceptance of head-mounted display in social and shared spaces: (1) discreet interactions and (2) a public spectator view. On a daily basis we evolve in shared and social spaces which may have different levels of acceptance for the use of head-mounted display (HMD), either from the users or the surroundings people. In contexts where the HMD is usually accepted by the non-HMD users (e.g. public transport) but still struggle to be adopted, we will approach the potential benefits of discreet interactions. Finally, we suggest and raise questions about the public spectator view approach to address contexts where the use of HMD is perceived by the surroundings as inappropriate
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Regal, Georg; Schrom-Feiertag, Helmut; Nguyen, Quynh; Aust, Marco; Murtinger, Markus; Smit, Dorothé; Tscheligi, Manfred; Billinghurst, Mark
TI  - VR [we are] Training - Workshop on Collaborative Virtual Training for Challenging Contexts
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3503710
ER  - 

TY  - JOUR
AU  - Liu, Zhejun; Jin, Yunshui; Ma, Minhua; Li, Jiachen
TI  - A Comparison of Immersive and Non-Immersive VR for the Education of Filmmaking
PY  - 2022
AB  - NA
SP  - 1
EP  - 14
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2078462
ER  - 

TY  - NA
AU  - Jin, Qiao; Liu, Yu; Yarosh, Svetlana; Han, Bo; Qian, Feng
TI  - How Will VR Enter University Classrooms? Multi-stakeholders Investigation of VR in Higher Education
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517542
ER  - 

TY  - NA
AU  - Dmitrenko, Dmitrijs; Maggioni, Emanuela; Brianza, Giada; Holthausen, Brittany E.; Walker, Bruce N.; Obrist, Marianna
TI  - CHI - CARoma Therapy: Pleasant Scents Promote Safer Driving, Better Mood, and Improved Well-Being in Angry Drivers
PY  - 2020
AB  - Driving is a task that is often affected by emotions. The effect of emotions on driving has been extensively studied. Anger is an emotion that dominates in such investigations. Despite the knowledge on strong links between scents and emotions, few studies have explored the effect of olfactory stimulation in a context of driving. Such an outcome provides HCI practitioners very little knowledge on how to design for emotions using olfactory stimulation in the car. We carried out three studies to select scents of different valence and arousal levels (i.e. rose, peppermint, and civet) and anger eliciting stimuli (i.e. affective pictures and on-road events). We used this knowledge to conduct the fourth user study investigating how the selected scents change the emotional state, well-being, and driving behaviour of drivers in an induced angry state. Our findings enable better decisions on what scents to choose when designing interactions for angry drivers.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376176
ER  - 

TY  - JOUR
AU  - Choi, Daewoong; Cho, Hyeonjoong; Seo, Kyeongeun; Lee, Sangyub; Lee, Jaekyu; Ko, Jae-jin
TI  - Designing Hand Pose Aware Virtual Keyboard With Hand Drift Tolerance
PY  - 2019
AB  - An unintentional hand drift adversely affects the typing performance of conventional virtual keyboards. To overcome this, we proposed to utilize the typing patterns of skilled typists. First, as most typists enter the keys in the same column with a predetermined finger only, we restricted these keys to be typed by their corresponding fingers. Second, our investigation of skilled typists demonstrated that hand poses vary when the typists touch different keys. Thus, rather than locating the touch point as in the case of existing virtual keyboards, we attempted to use unique hand poses to infer the target key. Based on these two techniques, we implemented a novel hand poses aware virtual keyboard that is tolerant of hand drift. Our experimental studies yielded the following results: 1) most of the QWERTY-familiar typists who have varying typing habits were easily adaptable to the proposed keyboard design and 2) the proposed keyboard outperformed existing virtual keyboards in terms of typing speed and several error rates, and eventually achieved a typing speed of approximately 56 WPM.
SP  - 96035
EP  - 96047
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2929310
ER  - 

TY  - NA
AU  - Willett, Nora
TI  - Tools for Live 2D Animation
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Kim, Hak Gu; Lim, Heoun-taek; Lee, Sangmin; Ro, Yong Man
TI  - VRSA Net: VR Sickness Assessment Considering Exceptional Motion for 360° VR Video
PY  - 2018
AB  - The viewing safety is one of the main issues in viewing virtual reality (VR) content. In particular, VR sickness could occur when watching immersive VR content. To deal with the viewing safety for VR content, objective assessment of VR sickness is of great importance. In this paper, we propose a novel objective VR sickness assessment (VRSA) network based on deep generative model for automatically predicting the VR sickness score. The proposed method takes into account motion patterns of VR videos in which an exceptional motion is a critical factor inducing excessive VR sickness in human motion perception. The proposed VRSA network consists of two parts, which are VR video generator and VR sickness score predictor. By training the VR video generator with common videos with non-exceptional motion, the generator learns the tolerance of VR sickness in human motion perception. As a result, the difference between the original and the generated videos by the VR video generator could represent exceptional motion of VR video causing VR sickness. In the VR sickness score predictor, the VR sickness score is predicted by projecting the difference between the original and the generated videos onto the subjective score space. For the evaluation of VR sickness assessment, we built a new dataset which consists of 360° videos (stimuli), corresponding physiological signals, and subjective questionnaires from subjective assessment experiments. Experimental results demonstrated that the proposed VRSA network achieved a high correlation with human perceptual score for VR sickness.
SP  - 1646
EP  - 1660
JF  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
VL  - 28
IS  - 4
PB  - 
DO  - 10.1109/tip.2018.2880509
ER  - 

TY  - NA
AU  - Hartmann, Jeremy; DiVerdi, Stephen; Nguyen, Cuong; Vogel, Daniel
TI  - UIST - View-Dependent Effects for 360° Virtual Reality Video
PY  - 2020
AB  - "View-dependent effects'' have parameters that change with the user's view and are rendered dynamically at runtime. They can be used to simulate physical phenomena such as exposure adaptation, as well as for dramatic purposes such as vignettes. We present a technique for adding view-dependent effects to 360 degree video, by interpolating spatial keyframes across an equirectangular video to control effect parameters during playback. An in-headset authoring tool is used to configure effect parameters and set keyframe positions. We evaluate the utility of view-dependent effects with expert 360 degree filmmakers and the perception of the effects with a general audience. Results show that experts find view-dependent effects desirable for their creative purposes and that these effects can evoke novel experiences in an audience.
SP  - 354
EP  - 364
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415846
ER  - 

TY  - NA
AU  - Hubenschmid, Sebastian; Wieland, Jonathan; Fink, Daniel Immanuel; Batch, Andrea; Zagermann, Johannes; Elmqvist, Niklas; Reiterer, Harald
TI  - ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517550
ER  - 

TY  - NA
AU  - Le, Huy Viet; Mayer, Sven; Henze, Niels
TI  - UIST - InfiniTouch: Finger-Aware Interaction on Fully Touch Sensitive Smartphones
PY  - 2018
AB  - Smartphones are the most successful mobile devices and offer intuitive interaction through touchscreens. Current devices treat all fingers equally and only sense touch contacts on the front of the device. In this paper, we present InfiniTouch, the first system that enables touch input on the whole device surface and identifies the fingers touching the device without external sensors while keeping the form factor of a standard smartphone. We first developed a prototype with capacitive sensors on the front, the back and on three sides. We then conducted a study to train a convolutional neural network that identifies fingers with an accuracy of 95.78% while estimating their position with a mean absolute error of 0.74cm. We demonstrate the usefulness of multiple use cases made possible with InfiniTouch, including finger-aware gestures and finger flexion state as an action modifier.
SP  - 779
EP  - 792
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242605
ER  - 

TY  - NA
AU  - Wang, Cheng Yao; Sakashita, Mose; Ehsan, Upol; Li, Jingjin; Won, Andrea Stevenson
TI  - CHI - Again, Together: Socially Reliving Virtual Reality Experiences When Separated
PY  - 2020
AB  - To share a virtual reality (VR) experience remotely together, users usually record videos from an individual's point of view and then co-watch these videos. However, co-watching recorded videos limits users to reliving their memories from the perspective from which the video was captured. In this paper, we describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling. We discuss the design implications for sharing VR experiences over time and space.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376642
ER  - 

TY  - CHAP
AU  - Badami, Maisie; Baez, Marcos; Zamanirad, Shayan; Kang, Wei
TI  - ICSOC Workshops - On How Cognitive Computing Will Plan Your Next Systematic Review
PY  - 2021
AB  - Systematic literature reviews (SLRs) are at the heart of evidence-based research, setting the foundation for future research and practice. However, producing good quality timely contributions is a challenging and highly cognitive endeavor, which has lately motivated the exploration of automation and support in the SLR process. In this paper we address an often overlooked phase in this process, that of planning literature reviews, and explore under the lenses of cognitive process augmentation how to overcome its most salient challenges. In doing so, we report on the insights from 24 SLR authors on planning practices, its challenges as well as feedback on support strategies inspired by recent advances in cognitive computing. We frame our findings under the cognitive augmentation framework, and report on a prototype implementation and evaluation focusing on further informing the technical feasibility.
SP  - 324
EP  - 333
JF  - Service-Oriented Computing – ICSOC 2020 Workshops
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-76352-7_32
ER  - 

TY  - BOOK
AU  - Günther, Sebastian; Müller, Florian; Hübner, Felix; Mühlhäuser, Max; Matviienko, Andrii
TI  - EICS - ActuBoard: An Open Rapid Prototyping Platform to integrate Hardware Actuators in Remote Applications
PY  - 2021
AB  - Prototyping is an essential step in developing tangible experiences and novel devices, ranging from haptic feedback to wearables. However, prototyping of actuated devices nowadays often requires repetitive and time-consuming steps, such as wiring, soldering, and programming basic communication, before HCI researchers and designers can focus on their primary interest: designing interaction. In this paper, we present ActuBoard, a prototyping platform to support 1) quick assembly, 2) less preparation work, and 3) the inclusion of non-tech-savvy users. With ActuBoard, users are not required to create complex circuitry, write a single line of firmware, or implementing communication protocols. Acknowledging existing systems, our platform combines the flexibility of low-level microcontrollers and ease-of-use of abstracted tinker platforms to control actuators from separate applications. As further contribution, we highlight the technical specifications and published the ActuBoard platform as Open Source.
SP  - 70
EP  - 76
JF  - Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3459926.3464757
ER  - 

TY  - JOUR
AU  - Barbera Hernández, Víctor Manuel
TI  - Redefinición del espacio en los filmes inmersivos: vídeos de 360º narrativos de ficción
PY  - 2022
AB  - <jats:p>Los filmes inmersivos son un nuevo formato para contar historias que utiliza el espacio de la imagen como un elemento audiovisual esencial para entender correctamente la narrativa. Hemos realizado un análisis de contenido sobre una muestra compuesta por vídeos de 3600 narrativos de ficción, a la que aplicamos un modelo compuesto por variables relacionadas con la organización del espacio alrededor del espectador. Los resultados señalan un esquema organizativo centrípeto, similar al teatro inmersivo, donde se da prioridad al entendimiento de la narrativa principal por encima de una verdadera utilización del espacio que proporciona este nuevo formato para contar historias.</jats:p>
SP  - 1
EP  - 11
JF  - VISUAL REVIEW. International Visual Culture Review / Revista Internacional de Cultura Visual
VL  - 9
IS  - Monográfico
PB  - 
DO  - 10.37467/revvisual.v9.3685
ER  - 

TY  - NA
AU  - Samayoa, Amilcar Gomez; Talavera, Javier; Sium, Siem G.; Xie, Biao; Huang, Haikun; Yu, Lap-Fai
TI  - Building a Motion-Aware, Networked Do-It-Yourself Holographic Display
PY  - 2021
AB  - The COVID-19 pandemic has motivated a shift from physical interaction, approaches, or procedures due to social distancing. More people are at home using digital displays for real-time communication and engagement. With recent innovations in computational hardware for spatial applications, such as extended reality technologies, entry barriers for hosting intimate, interpersonal, virtual events continue to fall. The barrier falls at such a rate that the production or manufacturing of an extended reality system for different and simultaneous, practical scenarios may be built to solve communication issues resulting from COVID-19. This paper aims to describe a low-cost networked holographic system that can be used for various purposes such as communication, education, and gaming. We created three different applications to show the cross-compatibility, effectiveness, and usability of our system.
SP  - 39
EP  - 48
JF  - 2021 IEEE International Conference on Intelligent Reality (ICIR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icir51845.2021.00015
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Punpongsanon, Parinya; Kim, Jeeeun
TI  - UIST - Programmable Filament: Printed Filaments for Multi-material 3D Printing
PY  - 2020
AB  - From full-color objects to functional capacitive artifacts, 3D printing multi-materials became essential to broaden the application areas of digital fabrication. We present Programmable Filament, a novel technique that enables multi-material printing using a commodity FDM 3D printer, requiring no hardware upgrades. Our technique builds upon an existing printing technique in which multiple filament segments are printed and spliced into a single threaded filament. We propose an end-to-end pipeline for 3D printing an object in multi-materials, with an introduction of the design systems for end-users. Optimized for low-cost, single-nozzle FDM 3D printers, the system is built upon our computational analysis and experiments to enhance its validity over various printers and materials to design and produce a programmable filament. Finally, we discuss application examples and speculate the future with its potential, such as custom filament manufacturing on-demand.
SP  - 1209
EP  - 1221
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415863
ER  - 

TY  - JOUR
AU  - Rule, Adam; Drosos, Ian; Tabard, Aurélien; Hollan, James D.
TI  - Aiding Collaborative Reuse of Computational Notebooks with Annotated Cell Folding
PY  - 2018
AB  - <jats:p>Computational notebooks aim to support collaborative data analysis by combining code, visualizations, and text in a single easily shared document. Yet, as notebooks evolve and grow they often become difficult to navigate or understand, discouraging sharing and reuse. We present the design and evaluation of a Jupyter Notebook extension providing facilities for annotated cell folding. Through a lab study and multi-week deployment we find cell folding aids notebook navigation and comprehension, not only by the original author, but also by collaborators viewing the notebook in a meeting or revising it on their own. However, in some cases cell folding encouraged collaborators to overlook folded sections or spend longer reviewing a notebook before editing it. These findings extend our understanding of code folding's trade-offs to a new medium and demonstrate its benefits for everyday collaboration. We conclude by discussing how dynamic reorganization can support sharing and reuse of computational notebooks.</jats:p>
SP  - 1
EP  - 12
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 2
IS  - CSCW
PB  - 
DO  - 10.1145/3274419
ER  - 

TY  - NA
AU  - Shi, Rongkai; Zhu, Nan; Liang, Hai-Ning; Zhao, Shengdong
TI  - Exploring Head-based Mode-Switching in Virtual Reality
PY  - 2021
AB  - Mode-switching supports multilevel operations using a limited number of input methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches for mode-switching use buttons, controllers, and users' hands. However, they are inefficient and challenging to do with tasks that require both hands (e.g., when users need to use two hands during drawing operations). Using head gestures for mode-switching can be an efficient and cost-effective way, allowing for a more continuous and smooth transition between modes. In this paper, we explore the use of head gestures for mode-switching especially in scenarios when both users' hands are performing tasks. We present a first user study that evaluated eight head gestures that could be suitable for VR HMD with a dual-hand line-drawing task. Results show that move forward, move backward, roll left, and roll right led to better performance and are preferred by participants. A second study integrating these four gestures in Tilt Brush, an open-source painting VR application, is conducted to further explore the applicability of these gestures and derive insights. Results show that Tilt Brush with head gestures allowed users to change modes with ease and led to improved interaction and user experience. The paper ends with a discussion on some design recommendations for using head-based mode-switching in VR HMD.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Capallera, Marine; Meteier, Quentin; de Salis, Emmanuel; Angelini, Leonardo; Carrino, Stefano; Khaled, Omar Abou; Mugellini, Elena
TI  - IHM - Secondary task and situation awareness, a mobile application for semi-autonomous vehicle
PY  - 2019
AB  - Autonomous vehicles are developing rapidly and will lead to a significant change in the driver's role: he/she will have to move from the role of actor to the role of supervisor. Indeed, the driver will soon be able to perform a secondary task but he/she must be able to take over control in the event of a critical situation that is not managed by the autonomous system. This implies that the role of new interfaces and interactions within the vehicle is important to take into account. This article describes the design of an application that provides the driver with information about the environment perceived by his/her vehicle in the form of modules. This application is displayed as split screen on a tablet by which a secondary task can be performed. Initial tests were carried out with this application in a driving simulator. They made it possible to test the acceptance of the application and the clarity of the information transmitted. The results generally showed that the participants correctly identified some of the factors limiting the proper functioning of the autonomous pilot while performing a secondary task on a tablet.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yan, Shuo; Jiang, Wenli; Xiong, Menghan; Shen, Xukun
TI  - ISMAR Adjunct - An Exploratory Study for Designing Social Experience of Watching VR Movies Based on Audience’s Voice Comments
PY  - 2020
AB  - Social experience is important when audience are watching movies. Virtual reality (VR) movies engage audience through immersive environment and interactive narrative. However, VR headsets restrict audience to an individual experience, which disrupt the potential for shared social realities. In our study, we propose an approach to design an asynchronous social experience that allows the participant to receive other audiences’ voice comments (such as their opinions, impressions or emotional reactions) in VR movies. We measured the participants’ feedback on their engagement levels, recall abilities and social presence. The results showed that in VR-Voice Comment (VR-VC) movie, the audience’s voice comments could affect participant’s engagement and the recall of information in the scenes. The participants obtained social awareness and enjoyment at the same time. A few of them were worried mainly because of the potential auditory clutter that resulted from unpredictable voice comments. We discuss the design implications for this and directions for future research. Overall, we observe a positive tendency in watching VR-VC movie, which could be adapted for future VR movie experience.
SP  - 147
EP  - 152
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct51615.2020.00049
ER  - 

TY  - NA
AU  - Breitenfellner, Marcel; Jungwirth, Florian; Ferscha, Alois
TI  - UbiComp/ISWC Adjunct - Towards 3D smooth pursuit interaction
PY  - 2019
AB  - In this position paper, we encourage the use of novel 3D gaze tracking possibilities in the field of gaze-based interaction. Smooth pursuit offers great benefits over other gaze interaction approaches, like the ability to work with uncalibrated eye trackers, but also has disadvantages like the produced visual clutter in more complex user interfaces. We examine the basic concept of smooth pursuits, its hardware and algorithmic requirements and how this can be applied to real world problems. Then we evaluate how the recent change in availability of 3D eye tracking hardware can be used to approach the challenges of 2D smooth pursuit interaction. We take a look at different research opportunities, show concrete ideas and discuss why they are relevant for future research.
SP  - 619
EP  - 623
JF  - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341162.3348385
ER  - 

TY  - JOUR
AU  - Tallman, Tyler N.; Smyl, Danny
TI  - Structural health and condition monitoring via electrical impedance tomography in self-sensing materials: a review
PY  - 2020
AB  - NA
SP  - 123001
EP  - NA
JF  - Smart Materials and Structures
VL  - 29
IS  - 12
PB  - 
DO  - 10.1088/1361-665x/abb352
ER  - 

TY  - NA
AU  - Vinella, Federica Lucia; Delić, Amra; Barile, Francesco; Lykourentzou, Ioanna; Masthoff, Judith
TI  - GMAP 2022: Workshop on Group Modeling, Adaptation and Personalization
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3511047.3536351
ER  - 

TY  - NA
AU  - Kumaravel, Balasaravanan Thoravi; Nguyen, Cuong; DiVerdi, Stephen; Hartmann, Bjoern
TI  - UIST - TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators
PY  - 2020
AB  - Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.
SP  - 182
EP  - 195
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415827
ER  - 

TY  - NA
AU  - Schneider, Daniel; Correia, António; Chaves, Ramon; Pimentel, Ana Paula; Antelio, Marcio; Lucas, Edson; de Almeida, Marcos Antonio; Oliveira, Luiz; de Souza, Jano Moreira
TI  - SMC - Turning social news curation into microtask crowdsourcing: a vision and research agenda
PY  - 2020
AB  - Over the past decade, online crowdsourcing has established itself as an emerging paradigm that industry and government have been using to harness the cognitive abilities of a multitude of users distributed around the world. In this context, microtask crowdsourcing has become the method of choice for addressing a wide range of diverse problems. Microtasks typically require a minimum of time and cognitive effort, but combined individual efforts have made it possible to accomplish great achievements. The goal of this paper is to contribute to the ongoing effort of understanding whether the same success that microtask crowdsourcing has achieved in other domains can be obtained in the field of social news curation. In particular, we ask whether it is possible to turn online news curation, typically a social and collaborative activity on the Web, into a model in which curatorial activities are mapped into microtasks to be performed by a crowd of online users.
SP  - 780
EP  - 787
JF  - 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smc42975.2020.9283090
ER  - 

TY  - NA
AU  - Andersen, Leif; Ballantyne, Michael; Felleisen, Matthias
TI  - Adding Interactive Visual Syntax to Textual Code
PY  - 2020
AB  - Many programming problems call for turning geometrical thoughts into code: tables, hierarchical structures, nests of objects, trees, forests, graphs, and so on. Linear text does not do justice to such thoughts. But, it has been the dominant programming medium for the past and will remain so for the foreseeable future. This paper proposes a novel mechanism for conveniently extending textual programming languages with problem-specific visual syntax. It argues the necessity of this language feature, demonstrates the feasibility with a robust prototype, and sketches a design plan for adapting the idea to other languages.
SP  - NA
EP  - NA
JF  - arXiv: Programming Languages
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Muehlhaus, Marie; Steimle, Jürgen; Koelle, Marion
TI  - Feather Hair: Interacting with Sensorized Hair in Public Settings
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533527
ER  - 

TY  - NA
AU  - Bernal, Guillermo; Yang, Tao; Jain, Abhinandan; Maes, Pattie
TI  - PhysioHMD: a conformable, modular toolkit for collecting physiological data from head-mounted displays
PY  - 2018
AB  - Virtual and augmented reality headsets are unique as they have access to our facial area: an area that presents an excellent opportunity for always-available input and insight into the user's state. Their position on the face makes it possible to capture bio-signals as well as facial expressions. This paper introduces the PhysioHMD, a software and hardware modular interface built for collecting affect and physiological data from users wearing a head-mounted display. The PhysioHMD platform is a flexible architecture enables researchers and developers to aggregate and interprets signals in real-time, and use those to develop novel, personalized interactions and evaluate virtual experiences. Offering an interface that is not only easy to extend but also is complemented by a suite of tools for testing and analysis. We hope that PhysioHMD can become a universal, publicly available testbed for VR and AR researchers.
SP  - 160
EP  - 167
JF  - Proceedings of the 2018 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267242.3267268
ER  - 

TY  - NA
AU  - Kim, Jiwan; Oakley, Ian
TI  - SonarID: Using Sonar to Identify Fingers on a Smartwatch
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501935
ER  - 

TY  - JOUR
AU  - Parilusyan, Brice; Teyssier, Marc; Martinez-Missir, Valentin; Duhart, Clément; Serrano, Marcos
TI  - Sensurfaces
PY  - 2022
AB  - <jats:p>Ubiquitous touch sensing surfaces are largely influenced by touchscreens' look and feel and fail to express the physical richness of existing surrounding materials. We introduce Sensurfaces, a plug-and-play electronic module that allows to rapidly experiment with touch-sensitive surfaces while preserving the original appearance of materials. Sensurfaces is composed of plug-and-play modules that can be connected together to expand the size and number of materials composing a sensitive surface. The combination of Sensurfaces modules allows the creation of small or large multi-material sensitive surfaces that can detect multi-touch but also body proximity, pose, pass, or even human steps. In this paper, we present the design and implementation of Sensurfaces. We propose a design space describing the factors of Sensurfaces interfaces. Then, through a series of technical evaluations, we demonstrate the capabilities of our system. Finally, we report on two workshops validating the usability of our system.</jats:p>
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534616
ER  - 

TY  - BOOK
AU  - Okopnyi, Pavel; Juhlin, Oskar; Guribye, Frode
TI  - IMX - Unpacking Editorial Agreements in Collaborative Video Production
PY  - 2020
AB  - Video production is a collaborative process involving creative, artistic and technical elements that require a multitude of specialised skill sets. This open-ended work is often marked by uncertainty and interpretive flexibility in terms of what the product is and should be. At the same time, most current video production tools are designed for single users. There is a growing interest, both in industry and academia, to design features that support key collaborative processes in editing, such as commenting on videos. We add to current research by unpacking specific forms of collaboration, in particular the social mechanisms and strategies employed to reduce interpretive flexibility and uncertainty in achieving agreements between editors and other collaborators. The findings contribute to the emerging design interest by identifying general design paths for how to support collaboration in video editing through scaffolding, iconic referencing, and suggestive editing.
SP  - 117
EP  - 126
JF  - ACM International Conference on Interactive Media Experiences
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3391614.3393652
ER  - 

TY  - CHAP
AU  - Kishimoto, Takuya; Imura, Masataka
TI  - Development of Fingering Learning Support System Using Fingertip Tracking from Monocular Camera
PY  - 2022
AB  - NA
SP  - 50
EP  - 56
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06391-6_7
ER  - 

TY  - JOUR
AU  - Inoue, Kosuke; Yasugi, Masaki; Yamamoto, Hirotsugu; Ninomiya, Nao
TI  - Improvement of the distortion of aerial displays and proposal for utilizing distortion to emulate three-dimensional aerial image
PY  - 2022
AB  - NA
SP  - 261
EP  - 266
JF  - Optical Review
VL  - 29
IS  - 3
PB  - 
DO  - 10.1007/s10043-021-00714-z
ER  - 

TY  - NA
AU  - Griggio, Carla F.; Nouwens, Midas; Klokmose, Clemens Nylandsted
TI  - Caught in the Network: The Impact of WhatsApp's 2021 Privacy Policy Update on Users' Messaging App Ecosystems
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502032
ER  - 

TY  - NA
AU  - Reiter, Katharina; Pfeuffer, Ken; Esteves, Augusto; Mittermeier, Tim; Alt, Florian
TI  - Look & Turn: One-handed and Expressive Menu Interaction by Gaze and Arm Turns in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529233
ER  - 

TY  - CHAP
AU  - Komiya, Kozo; Tobita, Hiroaki
TI  - MixedView: A Focus-and-Context View Mixing Omnidirectional and Detailed 2D Images
PY  - 2022
AB  - NA
SP  - 204
EP  - 211
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06417-3_28
ER  - 

TY  - CHAP
AU  - Ho, Jeffrey
TI  - HCI (1) - Towards an integrated approach to studying virtual reality-mediated social behaviors
PY  - 2018
AB  - The study on social behaviors that are mediated by virtual reality (VR) has become a major challenge for researchers given the increasing popularity of VR in the mass market. In this study, we reviewed and discussed certain issues on studying social behavior toward multi-user VR application in different settings. This study is aimed at proposing an integrated approach to studying VR-mediated social behaviors. The diversity in contexts of multi-user VR applications guarantees extensive research foci, such as team dynamics, trust, and persuasion. An integrated approach that can address this new class of research questions on social behavior in VR is necessary. The traditional research methods may capture various aspects of VR-mediated social behaviors, whereas several methods may demonstrate limitations. This paper explores the possibilities and challenges of an integrated approach to studying VR-mediated social behaviors.
SP  - 102
EP  - 113
JF  - Human-Computer Interaction. Theories, Methods, and Human Issues
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-319-91238-7_9
ER  - 

TY  - NA
AU  - Zhong, Mingyuan; Li, Gang; Chi, Peggy; Li, Yang
TI  - HelpViz: Automatic Generation of Contextual Visual MobileTutorials from Text-Based Instructions
PY  - 2021
AB  - We present HelpViz, a tool for generating contextual visual mobile tutorials from text-based instructions that are abundant on the web. HelpViz transforms text instructions to graphical tutorials in batch, by extracting a sequence of actions from each text instruction through an instruction parsing model, and executing the extracted actions on a simulation infrastructure that manages an array of Android emulators. The automatic execution of each instruction produces a set of graphical and structural assets, including images, videos, and metadata such as clicked elements for each step. HelpViz then synthesizes a tutorial by combining parsed text instructions with the generated assets, and contextualizes the tutorial to user interaction by tracking the user's progress and highlighting the next step. Our experiments with HelpViz indicate that our pipeline improved tutorial execution robustness and that participants preferred tutorials generated by HelpViz over text-based instructions. HelpViz promises a cost-effective approach for generating contextual visual tutorials for mobile interaction at scale.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Bouvin, Niels Olof
TI  - HT - From NoteCards to Notebooks: There and Back Again
PY  - 2019
AB  - Fifty years since the beginning of the Internet, and three decades of the Dexter Hypertext Reference Model and the World Wide Web mark an opportune time to take stock and consider how hypermedia has developed, and in which direction it might be headed. The modern Web has on one hand turned into a place where very few, very large companies control all major platforms with some highly unfortunately consequences. On the other hand, it has also led to the creation of a highly flexible and nigh ubiquitous set of technologies and practices, which can be used as the basis for future hypermedia research with the rise of computational notebooks as a prime example of a new kind of collaborative and highly malleable applications.
SP  - 19
EP  - 28
JF  - Proceedings of the 30th ACM Conference on Hypertext and Social Media
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3342220.3343666
ER  - 

TY  - NA
AU  - Huang, Haikun; Solah, Michael S; Li, Dingzeyu; Yu, Lap-Fai
TI  - CHI - Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery
PY  - 2019
AB  - As 360 deg cameras and virtual reality headsets become more popular, panorama images have become increasingly ubiquitous. While sounds are essential in delivering immersive and interactive user experiences, most panorama images, however, do not come with native audio. In this paper, we propose an automatic algorithm to augment static panorama images through realistic audio assignment. We accomplish this goal through object detection, scene classification, object depth estimation, and audio source placement. We built an audio file database composed of over $500$ audio files to facilitate this process. We designed and conducted a user study to verify the efficacy of various components in our pipeline. We run our method on a large variety of panorama images of indoor and outdoor scenes. By analyzing the statistics, we learned the relative importance of these components, which can be used in prioritizing for power-sensitive time-critical tasks like mobile augmented reality (AR) applications.
SP  - 621
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300851
ER  - 

TY  - NA
AU  - Hahn, Jürgen; Wimmer, Raphael
TI  - CHI Extended Abstracts - Sketchable Interaction: Drawing User Interfaces with Interactive Regions
PY  - 2021
AB  - Sketchable Interaction (SI) describes a concept and environment where end-users create regions by drawing on a canvas. These regions apply effects to each other on collision. Attributes of regions, e.g. position, can be linked to each other so that they change together once modified, e.g. moved on the canvas. Within Sketchable Interaction, all entities - mouse pointer, desktop icons, or windows - are implemented as interactive regions. End-users customize this environment by drawing new regions that apply certain actions e.g. tagging files, deleting other regions or automating processes.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451567
ER  - 

TY  - JOUR
AU  - Beck, Susanne; Bergenholtz, Carsten; Bogers, Marcel; Brasseur, Tiare; Conradsen, Maria Louise; Di Marco, Diletta; Distel, Andreas Philipp; Dobusch, Leonhard; Dörler, Daniel; Effert, Agnes; Fecher, Benedikt; Filiou, Despoina; Frederiksen, Lars; Gillier, Thomas; Grimpe, Christoph; Gruber, Marc; Haeussler, Carolin; Heigl, Florian; Hoisl, Karin; Hyslop, Katie; Kokshagina, Olga; LaFlamme, Marcel; Lawson, Cornelia; Lifshitz-Assaf, Hila; Lukas, Wolfgang; Nordberg, Markus; Norn, Maria Theresa; Poetz, Marion; Ponti, Marisa; Pruschak, Gernot; Priego, Laia Pujol; Radziwon, Agnieszka; Rafner, Janet; Romanova, Gergana Petrova; Ruser, Alexander; Sauermann, Henry; Shah, Sonali K.; Sherson, Jacob F.; Suess-Reyes, Julia; Tucci, Christopher L.; Tuertscher, Philipp; Vedel, Jane Bjørn; Velden, Theresa; Verganti, Roberto; Wareham, Jonathan; Wiggins, Andrea; Xu, Sunny Mosangzi
TI  - The Open Innovation in Science research field: a collaborative conceptualisation approach
PY  - 2020
AB  - Openness and collaboration in scientific research are attracting increasing attention from scholars and practitioners alike. However, a common understanding of these phenomena is hindered by disciplinary boundaries and disconnected research streams. We link dispersed knowledge on Open Innovation, Open Science, and related concepts such as Responsible Research and Innovation by proposing a unifying Open Innovation in Science (OIS) Research Framework. This framework captures the antecedents, contingencies, and consequences of open and collaborative practices along the entire process of generating and disseminating scientific insights and translating them into innovation. Moreover, it elucidates individual-, team-, organisation-, field-, and society‐level factors shaping OIS practices. To conceptualise the framework, we employed a collaborative approach involving 47 scholars from multiple disciplines, highlighting both tensions and commonalities between existing approaches. The OIS Research Framework thus serves as a basis for future research, informs policy discussions, and provides guidance to scientists and practitioners.
SP  - 1
EP  - 50
JF  - Industry and Innovation
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ellis, Kirsten; Dao, Emily; Smith, Osian; Lindsay, Stephen; Olivier, Patrick
TI  - CHI - TapeBlocks: A Making Toolkit for People Living with Intellectual Disabilities
PY  - 2021
AB  - The accessibility and affordability of tangible electronic toolkits are significant barriers to their uptake by people with disabilities. We present the design and evaluation of TapeBlocks, a low-cost, low-fidelity toolkit intended to be accessible for people with intellectual disabilities while promoting creativity and engagement. We evaluated TapeBlocks by interviewing makers, special educational needs teachers and support coaches. Analysis of these interviews informed the design of a series of maker workshops using TapeBlocks with young adults living with intellectual disabilities, led by support coaches with support from the research team. Participants were able to engage with TapeBlocks and making, eventually building their own TapeBlocks to make personal creations. Our evaluation reveals how TapeBlocks supports accessible making and playful discovery of electronics for people living with disabilities, and addresses a gap in existing toolkits by being tinkerable, affordable and having a low threshold for engagement.
SP  - 280
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445647
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Blikstein, Paulo; Holbert, Nathan R; Worsley, Marcelo; Jacobs, Jennifer; Anderson, Fraser; Gong, Jun; DesPortes, Kayla; Mueller, Stefanie
TI  - Reimagining Systems for Learning Hands-on Creative and Maker Skills
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3503732
ER  - 

TY  - NA
AU  - Ellingham, Richard; Giffney, Tim
TI  - Carbon black silicone piezoresistive electrical impedance tomography stress sensor device
PY  - 2022
AB  - Carbon black/silicone rubber composite is a desirable material for highly flexible strain sensors due to its repeatable piezoresistive characteristics, low cost, and simple fabrication process. By customising the conductivity and elastic modulus of the composite, the material can easily be adapted for many human physiological sensing applications. For applications such weight distribution sensors for wheelchair users, it is desirable to have not just a single sensor reading but instead a 2D map of pressure or strain. In this work, after demonstrating that the material can be modelled to give estimated strain value, we have developed a system to give a 2D map of pressure applied to the sensor using Electrical Impedance Tomography (EIT). This method has the advantage that a 2D pressure map can be obtained from a sensor using a homogeneous cast sheet of composite with electrodes around the perimeter only, without requiring complex patterning or a sensor array. Although the design is scalable, our demonstration system was fabricated using a 100 mm diameter pressure pad of carbon black (CB) silicone composite with electrodes evenly spaced around its perimeter. A low-cost circuit was developed to apply current to the material and measure the voltage between electrodes. The voltage measurements were then reconstructed into maps of resistivity and indications of compressive stress. Testing demonstrates that the fabricated pressure sensor can sense localised pressure within the pressure pad. The positional accuracy of the sensor was found to be on average 3.6 mm for the 100 mm diameter circular domain under test.
SP  - NA
EP  - NA
JF  - Electroactive Polymer Actuators and Devices (EAPAD) XXIV
VL  - NA
IS  - NA
PB  - 
DO  - 10.1117/12.2610694
ER  - 

TY  - NA
AU  - Rao, Anyi; Xu, Linning; Lin, Dahua
TI  - Shoot360: Normal View Video Creation from City Panorama Footage
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3528233.3530702
ER  - 

TY  - NA
AU  - Hossain, Tamzid; Islam, Md. Fahimul; Delamare, William; Chowdhury, Farida; Hasan, Khalad
TI  - Exploring Social Acceptability and Users' Preferences of Head- and Eye-Based Interaction with Mobile Devices
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3490636
ER  - 

TY  - JOUR
AU  - Das, Anubrata; Liu, Houjiang; Kovatchev, Venelin; Lease, Matthew
TI  - The state of human-centered NLP technology for fact-checking
PY  - NA
AB  - NA
SP  - 103219
EP  - NA
JF  - Information Processing & Management
VL  - 60
IS  - 2
PB  - 
DO  - 10.1016/j.ipm.2022.103219
ER  - 

TY  - NA
AU  - Voelker, Simon; Hueber, Sebastian; Corsten, Christian; Remy, Christian
TI  - CHI - HeadReach: Using Head Tracking to Increase Reachability on Mobile Touch Devices
PY  - 2020
AB  - People often operate their smartphones with only one hand, using just their thumb for touch input. With today's larger smartphones, this leads to a reachability issue: Users can no longer comfortably touch everywhere on the screen without changing their grip. We investigate using the head tracking in modern smartphones to address this reachability issue. We developed three interaction techniques, pure head (PH), head + touch (HT), and head area + touch (HA), to select targets beyond the reach of one's thumb. In two user studies, we found that selecting targets using HT and HA had higher success rates than the default direct touch (DT) while standing (by about 9%) and walking (by about 12%), while being moderately slower. HT and HA were also faster than one of the best techniques, BezelCursor (BC) (by about 20% while standing and 6% while walking), while having the same success rate.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376868
ER  - 

TY  - JOUR
AU  - Lee, Min Kyung; Jain, Anuraag; Cha, Hea Jin; Ojha, Shashank; Kusbit, Daniel
TI  - Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation
PY  - 2019
AB  - As algorithms increasingly take managerial and governance roles, it is ever more important to build them to be perceived as fair and adopted by people. With this goal, we propose a procedural justice framework in algorithmic decision-making drawing from procedural justice theory, which lays out elements that promote a sense of fairness among users. As a case study, we built an interface that leveraged two key elements of the framework---transparency and outcome control---and evaluated it in the context of goods division. Our interface explained the algorithm's allocative fairness properties (standards clarity) and outcomes through an input-output matrix (outcome explanation), then allowed people to interactively adjust the algorithmic allocations as a group (outcome control). The findings from our within-subjects laboratory study suggest that standards clarity alone did not increase perceived fairness; outcome explanation had mixed effects, increasing or decreasing perceived fairness and reducing algorithmic accountability; and outcome control universally improved perceived fairness by allowing people to realize the inherent limitations of decisions and redistribute the goods to better fit their contexts, and by bringing human elements into final decision-making.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359284
ER  - 

TY  - JOUR
AU  - Jiang, Weiwei; Yu, Difeng; Wang, Chaofan; Sarsenbayeva, Zhanna; van Berkel, Niels; Goncalves, Jorge; Kostakos, Vassilis
TI  - Near-infrared Imaging for Information Embedding and Extraction with Layered Structures
PY  - 2022
AB  - <jats:p>Non-invasive inspection and imaging techniques are used to acquire non-visible information embedded in samples. Typical applications include medical imaging, defect evaluation, and electronics testing. However, existing methods have specific limitations, including safety risks (e.g., X-ray), equipment costs (e.g., optical tomography), personnel training (e.g., ultrasonography), and material constraints (e.g., terahertz spectroscopy). Such constraints make these approaches impractical for everyday scenarios. In this article, we present a method that is low-cost and practical for non-invasive inspection in everyday settings. Our prototype incorporates a miniaturized near-infrared spectroscopy scanner driven by a computer-controlled 2D-plotter. Our work presents a method to optimize content embedding, as well as a wavelength selection algorithm to extract content without human supervision. We show that our method can successfully extract occluded text through a paper stack of up to 16 pages. In addition, we present a deep-learning-based image enhancement model that can further improve the image quality and simultaneously decompose overlapping content. Finally, we demonstrate how our method can be generalized to different inks and other layered materials beyond paper. Our approach enables a wide range of content embedding applications, including chipless information embedding, physical secret sharing, 3D print evaluations, and steganography.</jats:p>
SP  - 1
EP  - 26
JF  - ACM Transactions on Graphics
VL  - 42
IS  - 1
PB  - 
DO  - 10.1145/3533426
ER  - 

TY  - JOUR
AU  - Abdelwahed, Mehdi; Zerioul, Lounis; Pitti, Alexandre; Romain, Olivier
TI  - Using Novel Multi-Frequency Analysis Methods to Retrieve Material and Temperature Information in Tactile Sensing Areas.
PY  - 2022
AB  - This article presents a novel artificial skin technology based on the Electric Impedance Tomography (EIT) that employs multi-frequency currents for detecting the material and the temperature of objects in contact with piezoresistive sheets. To date, few artificial skins in the literature are capable of detecting an object's material, e.g., wood, skin, leather, or plastic. EIT-based artificial skins have been employed mostly to detect the position of the contact but not its characteristics. Thanks to multi-frequency currents, our EIT-based artificial skin is capable of characterising the spectral profile of objects in contact and identifying an object's material at ambient temperature. Moreover, our model is capable of detecting several levels of temperature (from -10 up to 60 °C) and can also maintain a certain accuracy for material identification. In addition to the known capabilities of EIT-based artificial skins concerning detecting pressure and location of objects, as well as being low cost, these two novel modalities demonstrate the potential of EIT-based artificial skins to achieve global tactile sensing.
SP  - 8876
EP  - 8876
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 22
PB  - 
DO  - 10.3390/s22228876
ER  - 

TY  - NA
AU  - Lacher, Lisa L.; Gibson, Cody M.
TI  - Crowdsourcing Exams to Increase Student Engagement in an Online Information Technology Class: An Experience Report
PY  - 2020
AB  - To achieve better success rates in online courses, we need to cultivate student engagement. Students learn more when they are actively engaged. However, most instructors still use traditional lecture methods. Because instructors have not embraced active learning in their face-to-face classes, many are finding it challenging to incorporate active learning into their courses as they move online. Prior research has found that although students learn more from active learning activities, they do not feel like they are learning more, which can have a detrimental impact to their satisfaction with the activity and the course. Studies have found that with more training and promotion, students’ attitudes can change; however, this takes additional time and effort which instructors and students can find discouraging. Our research reports on the experience of using a crowdsourcing activity to create exam questions. Without any promotion from the instructor, students reported that this activity helped them feel engaged with the course materials and helped them learn. We discuss the contributing factors that added to student positivity, as well as challenges in adopting this active learning method.
SP  - NA
EP  - NA
JF  - 2020 International Conference on Computational Science and Computational Intelligence (CSCI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/csci51800.2020.00175
ER  - 

TY  - CONF
AU  - Liu, Jiali; Boukhelifa, Nadia; Eagan, James
TI  - Understanding alternatives in data analysis activities
PY  - 2019
AB  - Data workers are non-professional data scientists who engage in data analysis activities as part of their daily work. In this position paper, we share past and on-going work to understand data workers’ sense-making practices. We use multidisciplinary approaches to explore their human-tool partnerships. We introduce our current research on the role of alternatives in data analysis activities. Finally, we conclude with open questions and research directions.
SP  - 5
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Choi, Myungguen; Sakamoto, Daisuke; Ono, Tetsuo
TI  - Kuiper Belt: Utilizing the "Out-of-natural Angle" Region in the Eye-gaze Interaction for Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517725
ER  - 

TY  - JOUR
AU  - Lin, Wei-Han; Zhu, Zhijie; Ravikumar, Vasanth; Sharma, Vinod; Tolkacheva, Elena G; McAlpine, Michael C; Ogle, Brenda M
TI  - A Bionic Testbed for Cardiac Ablation Tools.
PY  - 2022
AB  - Bionic-engineered tissues have been proposed for testing the performance of cardiovascular medical devices and predicting clinical outcomes ex vivo. Progress has been made in the development of compliant electronics that are capable of monitoring treatment parameters and being coupled to engineered tissues; however, the scale of most engineered tissues is too small to accommodate the size of clinical-grade medical devices. Here, we show substantial progress toward bionic tissues for evaluating cardiac ablation tools by generating a centimeter-scale human cardiac disk and coupling it to a hydrogel-based soft-pressure sensor. The cardiac tissue with contiguous electromechanical function was made possible by our recently established method to 3D bioprint human pluripotent stem cells in an extracellular matrix-based bioink that allows for in situ cell expansion prior to cardiac differentiation. The pressure sensor described here utilized electrical impedance tomography to enable the real-time spatiotemporal mapping of pressure distribution. A cryoablation tip catheter was applied to the composite bionic tissues with varied pressure. We found a close correlation between the cell response to ablation and the applied pressure. Under some conditions, cardiomyocytes could survive in the ablated region with more rounded morphology compared to the unablated controls, and connectivity was disrupted. This is the first known functional characterization of living human cardiomyocytes following an ablation procedure that suggests several mechanisms by which arrhythmia might redevelop following an ablation. Thus, bionic-engineered testbeds of this type can be indicators of tissue health and function and provide unique insight into human cell responses to ablative interventions.
SP  - 14444
EP  - 14444
JF  - International journal of molecular sciences
VL  - 23
IS  - 22
PB  - 
DO  - 10.3390/ijms232214444
ER  - 

TY  - NA
AU  - Higgins, Padraig; Barron, Ryan; Matuszek, Cynthia
TI  - Head Pose for Object Deixis in VR-Based Human-Robot Interaction
PY  - 2022
AB  - Modern robotics heavily relies on machine learning and has a growing need for training data. Advances and commercialization of virtual reality (VR) present an opportunity to use VR as a tool to gather such data for human-robot interactions. We present the Robot Interaction in VR simulator, which allows human participants to interact with simulated robots and environments in real-time. We are particularly interested in spoken interactions between the human and robot, which can be combined with the robot’s sensory data for language grounding. To demonstrate the utility of the simulator, we describe a study which investigates whether a user’s head pose can serve as a proxy for gaze in a VR object selection task. Participants were asked to describe a series of known objects, providing approximate labels for the focus of attention. We demonstrate that using a concept of gaze derived from head pose can be used to effectively narrow the set of objects that are the target of participants’ attention and linguistic descriptions.
SP  - NA
EP  - NA
JF  - 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ro-man53752.2022.9900631
ER  - 

TY  - JOUR
AU  - Yoshida, Shogo; Xie, Haoran; Miyata, Kazunori
TI  - Omnidirectional Haptic Stimulation System via Pneumatic Actuators for Presence Presentation
PY  - 2023
AB  - <jats:p>Recently, remote meetings and work-from-home have become more common, reducing the opportunities for face-to-face communication. To facilitate communication among remote workers, researchers have focused on virtual space technology and spatial augmented reality technology. Although these technologies can enhance immersiveness in collaborative work, they face the challenge of fostering a sense of physical contact. In this work, we aimed to foster a sense of presence through haptic stimulation using pneumatic actuators. Specifically, we developed a choker-type wearable device that presents various pressure patterns around the neck; the pattern presented depends on the message the device must convey. Various combinations of haptic presentation are achieved by pumping air to the multiple pneumatic actuators attached to the choker. In addition, we conducted experiments involving actuators of different shapes to optimize the haptic presentation. When linked with a smartphone, the proposed device can present pressure patterns to indicate incoming calls and notifications, to give warning about an obstacle that one who is texting might miss while walking, and to provide direction to a pedestrian. Furthermore, the device can be used in a wide range of applications, from those necessary in daily living to those that enhance one’s experience in the realm of entertainment. For example, haptic feedback that synchronizes with the presence of a singer or with the rhythm of a song one listens to or with a performer’s movements during a stage performance will immerse users in an enjoyable experience.</jats:p>
SP  - 584
EP  - NA
JF  - Sensors
VL  - 23
IS  - 2
PB  - 
DO  - 10.3390/s23020584
ER  - 

TY  - NA
AU  - Rogerson, Melissa J.; Sparrow, Lucy A.; Gibbs, Martin
TI  - CHI - Unpacking “Boardgames with Apps”: The Hybrid Digital Boardgame Model
PY  - 2021
AB  - Increasingly, modern boardgames incorporate digital apps and tools to deliver content in novel ways. Despite disparate approaches to incorporating digital tools in otherwise non-digital boardgames, there has to date been no detailed classification of the different roles that these tools play in supporting gameplay. In this paper, we present a model for understanding hybrid boardgame play as performing a set of discrete functions. Through a mixed-methods approach incorporating critical play sessions, a survey of 237 boardgame players, and interviews with 18 boardgame designers and publishers, we identified the key functions performed by the digital tools in these games. Using affinity mapping, we grouped these functions into eight core categories, which were tested and refined using a survey of 44 boardgame players and designers. This model defines and classifies the diverse functions that digital tools perform in hybrid digital boardgames, demonstrating their range and potential application for researchers and game designers.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445077
ER  - 

TY  - NA
AU  - Higuchi, Keita; Matsuda, Soichiro; Kamikubo, Rie; Enomoto, Takuya; Sugano, Yusuke; Yamamoto, Jun-ichi; Sato, Yoichi
TI  - IUI - Visualizing Gaze Direction to Support Video Coding of Social Attention for Children with Autism Spectrum Disorder
PY  - 2018
AB  - This paper presents a novel interface to support video coding of social attention in the assessment of children with autism spectrum disorder. Video-based evaluations of social attention during therapeutic activities allow observers to find target behaviors while handling the ambiguity of attention. Despite the recent advances in computer vision-based gaze estimation methods, fully automatic recognition of social attention under diverse environments is still challenging. The goal of this work is to investigate an approach that uses automatic video analysis in a supportive manner for guiding human judgment. The proposed interface displays visualization of gaze estimation results on videos and provides GUI support to allow users to facilitate agreement between observers by defining social attention labels on the video timeline. Through user studies and expert reviews, we show how the interface helps observers perform video coding of social attention and how human judgment compensates for technical limitations of the automatic gaze analysis.
SP  - 571
EP  - 582
JF  - 23rd International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3172944.3172960
ER  - 

TY  - NA
AU  - Cho, Youngjun
TI  - Rethinking Eye-blink: Assessing Task Difficulty through Physiological Representation of Spontaneous Blinking
PY  - 2021
AB  - Continuous assessment of task difficulty and mental workload is essential in improving the usability and accessibility of interactive systems. Eye tracking data has often been investigated to achieve this ability, with reports on the limited role of standard blink metrics. Here, we propose a new approach to the analysis of eye-blink responses for automated estimation of task difficulty. The core module is a time-frequency representation of eye-blink, which aims to capture the richness of information reflected on blinking. In our first study, we show that this method significantly improves the sensitivity to task difficulty. We then demonstrate how to form a framework where the represented patterns are analyzed with multi-dimensional Long Short-Term Memory recurrent neural networks for their non-linear mapping onto difficulty-related parameters. This framework outperformed other methods that used hand-engineered features. This approach works with any built-in camera, without requiring specialized devices. We conclude by discussing how Rethinking Eye-blink can benefit real-world applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445577
ER  - 

TY  - JOUR
AU  - Abdelhamid, Abdelaziz A.; Alotaibi, Refa; Mousa, Abdelaziz
TI  - Deep learning-based prototyping of android GUI from hand-drawn mockups
PY  - 2020
AB  - Recently, transforming graphical user interface (GUI) mockups into code becomes a common challenging practice for current software developers. However, this transformation usually takes time especially when GUI changes keep pace with evolutionary features. There are many studies admitted this challenge and presented solutions in terms of computer-based GUI mockups. However, there is a research gap in this kind of research as very few of them adopted hand-drawn mockups as an input. In this study, the authors employed YOLOv5 is a fast and accurate deep learning framework to automate the process of converting hand-drawn GUI mockups into Android-based GUI prototype. The process starts with detecting all GUI mockups in an input image and determining their bounding boxes, classifying these mockups into their corresponding GUI objects, then finally aligning these objects together to form the output prototype based on the layout presented in the input image. Experimental results show the effectiveness of the proposed approach in generating a visually appealing Android GUI from hand-drawn mockups with a recognition accuracy of 98.54% when tested on various hand-drawn GUI structures designed by five developers.
SP  - 816
EP  - 824
JF  - IET Software
VL  - 14
IS  - 7
PB  - 
DO  - 10.1049/iet-sen.2019.0378
ER  - 

TY  - JOUR
AU  - Dogan, Kemal Mert; Suzuki, Hiromasa; Gunpinar, Erkan; Kim, Myung-Soo
TI  - A generative sampling system for profile designs with shape constraints and user evaluation
PY  - 2019
AB  - NA
SP  - 93
EP  - 112
JF  - Computer-Aided Design
VL  - 111
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2019.02.002
ER  - 

TY  - BOOK
AU  - Müller, Philipp; Huang, Michael Xuelin; Zhang, Xucong; Bulling, Andreas
TI  - ETRA - Robust eye contact detection in natural multi-person interactions using gaze and speaking behaviour
PY  - 2018
AB  - Eye contact is one of the most important non-verbal social cues and fundamental to human interactions. However, detecting eye contact without specialised eye tracking equipment poses significant challenges, particularly for multiple people in real-world settings. We present a novel method to robustly detect eye contact in natural three- and four-person interactions using off-the-shelf ambient cameras. Our method exploits that, during conversations, people tend to look at the person who is currently speaking. Harnessing the correlation between people's gaze and speaking behaviour therefore allows our method to automatically acquire training data during deployment and adaptively train eye contact detectors for each target user. We empirically evaluate the performance of our method on a recent dataset of natural group interactions and demonstrate that it achieves a relative improvement over the state-of-the-art method of more than 60%, and also improves over a head pose based baseline.
SP  - 31
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204549
ER  - 

TY  - JOUR
AU  - Kordts, Börge; Kopetz, Jan Patrick; Henkel, Adrienne; Schrader, Andreas; Jochems, Nicole
TI  - Requirements and Interaction Patterns for a Novel Interaction Device for Patients in Intensive Care
PY  - 2019
AB  - <jats:title>Abstract</jats:title> <jats:p>Intensive care patients that are weaned from mechanical ventilation are facing substantial physical and psychical stress. Due to the breathing tube, they often cannot voice their basic needs adequately. Possible consequences, amongst other complications, are a prolonged healing process and a delirium. To address this issue and support patient communication in intensive care, we provide a solution tailored to patients that are dealing with limited cognitive and physiological abilities, hindering them to use traditional devices efficiently. For this purpose, we develop a novel interaction device tailored to the special situation of in-bed interaction.</jats:p> <jats:p>In this paper, we present key requirements for the device, which are relevant to the interaction itself as well as possible interaction gestures that may be performed with the device. The basis for this is a human-centered design process consisting of a comprehensive user and context analysis, as well as a requirements analysis. As a result, we identified three categories relevant for the interaction, namely look and feel, sensors, and actuators. The results of the requirement analysis were precise enough to start the actual development process of the device.</jats:p>
SP  - 67
EP  - 78
JF  - i-com
VL  - 18
IS  - 1
PB  - 
DO  - 10.1515/icom-2019-0004
ER  - 

TY  - NA
AU  - Liang, Paul Pu; Lyu, Yiwei; Fan, Xiang; Wu, Zetian; Cheng, Yun; Wu, Jason; Chen, Leslie Yufan; Wu, Peter; Lee, Michelle A.; Zhu, Yuke; Salakhutdinov, Ruslan; Morency, Louis-Philippe
TI  - MultiBench: Multiscale Benchmarks for Multimodal Representation Learning
PY  - 2021
AB  - Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.
SP  - NA
EP  - NA
JF  - arXiv: Learning
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Englmeier, David; O'Hagan, Joseph; Zhang, Mengyi; Alt, Florian; Butz, Andreas; Höllerer, Tobias; Williamson, Julie R.
TI  - NordiCHI - TangibleSphere – Interaction Techniques for Physical and Virtual Spherical Displays
PY  - 2020
AB  - Tangible interaction is generally assumed to provide benefits compared to other interaction styles due to its physicality. We demonstrate how this physicality can be brought to VR by means of TangibleSphere – a tracked, low-cost physical object that can (a) be rotated freely and (b) is overlaid with a virtual display. We present two studies, investigating performance in terms of efficiency and usability: the first study (N=16) compares TangibleSphere to a physical spherical display regarding accuracy and task completion time. We found comparable results for both types of displays. The second study (N=32) investigates the influence of physical rotation in more depth. We compare a pure VR condition to TangibleSphere in two conditions: one that allows actual physical rotation of the object and one that does not. Our findings show that physical rotation significantly improves accuracy and task completion time. These insights are valuable for researchers designing interaction techniques and interactive visualizations for spherical displays and for VR researchers aiming to incorporate physical touch into the experiences they design.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3419249.3420101
ER  - 

TY  - NA
AU  - Degaki, Richard Hada; Colonna, Juan Gabriel; Lopez, Yadini; Carvalho, José Reginaldo; Silva, Edson
TI  - Real Time Detection of Mobile Graphical User Interface Elements Using Convolutional Neural Networks
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Brazilian Symposium on Multimedia and Web
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3539637.3558044
ER  - 

TY  - NA
AU  - Li, Yang; Li, Gang; He, Luheng; Zheng, Jingjie; Li, Hong; Guan, Zhiwei
TI  - EMNLP (1) - Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements
PY  - 2020
AB  - Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,860 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.
SP  - 5495
EP  - 5510
JF  - Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2020.emnlp-main.443
ER  - 

TY  - JOUR
AU  - Mathur, Arunesh; Acar, Gunes; Friedman, Michael J.; Lucherini, Elena; Mayer, Jonathan; Chetty, Marshini; Narayanan, Arvind
TI  - Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites
PY  - 2019
AB  - Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ~53K product pages from ~11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns.
SP  - 81
EP  - 32
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359183
ER  - 

TY  - NA
AU  - Jang, Seowoo; Yoo, Soyoung; Kang, Namwoo
TI  - Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs.
PY  - 2020
AB  - Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Huang, Forrest; Canny, John; Nichols, Jeffrey
TI  - CHI - Swire: Sketch-based User Interface Retrieval
PY  - 2019
AB  - Sketches and real-world user interface examples are frequently used in multiple stages of the user interface design process. Unfortunately, finding relevant user interface examples, especially in large-scale datasets, is a highly challenging task because user interfaces have aesthetic and functional properties that are only indirectly reflected by their corresponding pixel data and meta-data. This paper introduces Swire, a sketch-based neural-network-driven technique for retrieving user interfaces. We collect the first large-scale user interface sketch dataset from the development of Swire that researchers can use to develop new sketch-based data-driven design interfaces and applications. Swire achieves high performance for querying user interfaces: for a known validation task it retrieves the most relevant example as within the top-10 results for over 60% of queries. With this technique, for the first time designers can accurately retrieve relevant user interface examples with free-form sketches natural to their design workflows. We demonstrate several novel applications driven by Swire that could greatly augment the user interface design process.
SP  - 104
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300334
ER  - 

TY  - NA
AU  - Lee, Joon Hyub; Ham, Hyung-Gi; Kim, Myung-Sung; Bae, Seok-Hyung
TI  - SIGGRAPH Labs - 3D Sketching for Multi-Pose Products: An Interactive Showcase
PY  - 2020
AB  - 2D perspective sketching is an essential tool for designers during the early stage of design. However, for products that have moving parts and take different poses during usage, 2D perspective sketching can be painstaking and time-consuming. In this interactive showcase, we present a 3D sketching system for multi-pose products. Our system lets designers easily sketch 3D curves, and part, rig, and pose them. We showcase that, with interactions that closely resemble traditional 2D perspective sketching and the physical manipulation of an articulated object, designers can quickly try many different form and movement ideas in 3D during the early stage of design.
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2020 Labs
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3388763.3407759
ER  - 

TY  - NA
AU  - Saquib, Nazmus; Kazi, Rubaiat Habib; Wei, Li-Yi; Mark, Gloria; Roy, Deb
TI  - CHI - Constructing Embodied Algebra by Sketching
PY  - 2021
AB  - Mathematical models and expressions traditionally evolved as symbolic representations, with cognitively arbitrary rules of symbol manipulation. The embodied mathematics philosophy posits that abstract math concepts are layers of metaphors grounded in our intuitive arithmetic capabilities, such as categorizing objects and part-whole analysis. We introduce a design framework that facilitates the construction and exploration of embodied representations for algebraic expressions, using interactions inspired by innate arithmetic capabilities. We instantiated our design in a sketch interface that enables construction of visually interpretable compositions that are directly mappable to algebraic expressions and explorable through a ladder of abstraction [47]. The emphasis is on bottom-up construction, with the user sketching pictures while the system generates corresponding algebra. We present diverse examples created by our prototype system. A coverage of the US Common Core curriculum and playtesting studies with children point to the future direction and potential for a sketch-based design paradigm for mathematics.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445460
ER  - 

TY  - NA
AU  - Moraes, Pedro; Teixeira, Leopoldo
TI  - SBES - Willow: A Tool for Interactive Programming Visualization to Help in the Data Structures and Algorithms Teaching-Learning Process
PY  - 2019
AB  - Data Structures and Algorithms (DSA) are one of the main pillars of software development; however, abstractions around them are hard to teach and to be understood by students. The most common approaches adopted by instructors to demonstrate the behavior of DSAs are the use of resources like slides and whiteboard sketches to create program illustrations. This task may be slow and tedious because these illustrations need to be continuously updated to represent new algorithm inputs and modifications. In this paper, we propose Willow, a tool for Program Visualization Simulation (PVS), which supports user interactions to manipulate the generated visualizations. With these manipulations in the visualization, we expect the user to be able to create better examples, resembling Algorithm Visualization Simulation tools (AVS), which are specialized in providing visualizations for specific DSAs. We evaluated our tool through a preliminary qualitative study with teaching assistants from an introductory Computer Science course who all give review lessons to the students. Our preliminary results show that the tool was well accepted by the participants, but we still need more studies to validate the use of the tool in classrooms. With the use of our tool features in the teaching-learning process, we expect that instructors may be able to interactively and more clearly explain DSAs to their students, without the hassle of hours creating slides or drawing by hand messy examples of algorithms.
SP  - 553
EP  - 558
JF  - Proceedings of the XXXIII Brazilian Symposium on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3350768.3351303
ER  - 

TY  - NA
AU  - Li, Jingyi; Hashim, Sonia; Jacobs, Jennifer
TI  - CHI - What We Can Learn From Visual Artists About Software Development
PY  - 2021
AB  - This paper explores software’s role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists’ experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445682
ER  - 

TY  - JOUR
AU  - Herzog, Vencia D; Suwelack, Stefan
TI  - Data-Efficient Machine Learning on Three-Dimensional Engineering Data
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>Decisions in engineering design are closely tied to the 3D shape of the product. Limited availability of 3D shape data and expensive annotation present key challenges for using Artificial Intelligence in product design and development. In this work we explore transfer learning strategies to improve the data-efficiency of geometric reasoning models based on deep neural networks as used for tasks such as shape retrieval and design synthesis. We address the utilization of problem- related and un-annotated 3D data to compensate for small data volumes. Our experiments show promising results for knowledge transfer on mechanical component benchmarks.</jats:p>
SP  - 1
EP  - 14
JF  - Journal of Mechanical Design
VL  - 144
IS  - 2
PB  - 
DO  - 10.1115/1.4052753
ER  - 

TY  - NA
AU  - Chen, Ke; Li, Yufei; Chen, Yingfeng; Fan, Changjie; Hu, Zhipeng; Yang, Wei
TI  - ESEC/SIGSOFT FSE - GLIB: towards automated test oracle for graphically-rich applications
PY  - 2021
AB  - Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose GLIB based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of GLIB on 20 real-world game apps (with bug reports available) and the result shows that GLIB can achieve 100% precision and 99.5% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of GLIB on another 14 real-world games (without bug reports) further demonstrates that GLIB can effectively uncover GUI glitches, with 48 of 53 bugs reported by GLIB having been confirmed and fixed so far.
SP  - 1093
EP  - 1104
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3468586
ER  - 

TY  - NA
AU  - Wu, Ziming; Kim, Taewook; Li, Quan; Ma, Xiaojuan
TI  - CHI - Understanding and Modeling User-Perceived Brand Personality from Mobile Application UIs
PY  - 2019
AB  - Designers strive to make their mobile apps stand out in a competitive market by creating a distinctive brand personality. However, it is unclear whether users can form a consistent impression of brand personality by looking at a few user interface (UI) screenshots in the app store, and if this process can be modeled computationally. To bridge this gap, we first collect crowd assessment on brand personalities depicted by the UIs of 318 applications, and statistically confirm that users can reach substantial agreement. To further model how users process mobile UI visually, we compute UI descriptors including Color, Organization, and Texture at both element and page levels. We feed these descriptors to a computational model, achieving a high accuracy of predicting perceived brand personality (MSE = 0.035 and R^2 = 0.78). This work could benefit designers by highlighting contributing visual factors to brand personality creation and providing quick, low-cost design feedback.
SP  - 213
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300443
ER  - 

TY  - JOUR
AU  - Laine, Markku; Zhang, Yu; Santala, Simo; Jokinen, Jussi P. P.; Oulasvirta, Antti
TI  - Responsive and Personalized Web Layouts with Integer Programming
PY  - 2021
AB  - Over the past decade, responsive web design (RWD) has become the de facto standard for adapting web pages to a wide range of devices used for browsing. While RWD has improved the usability of web pages, it is not without drawbacks and limitations: designers and developers must manually design the web layouts for multiple screen sizes and implement associated adaptation rules, and its "one responsive design fits all" approach lacks support for personalization. This paper presents a novel approach for automated generation of responsive and personalized web layouts. Given an existing web page design and preferences related to design objectives, our integer programming -based optimizer generates a consistent set of web designs. Where relevant data is available, these can be further automatically personalized for the user and browsing device. The paper includes presentation of techniques for runtime adaptation of the designs generated into a fully responsive grid layout for web browsing. Results from our ratings-based online studies with end users (N = 86) and designers (N = 64) show that the proposed approach can automatically create high-quality responsive web layouts for a variety of real-world websites.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3461735
ER  - 

TY  - NA
AU  - Li, Jingyi; Hashim, Sonia; Jacobs, Jennifer
TI  - What We Can Learn From Visual Artists About Software Development
PY  - 2021
AB  - This paper explores software's role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists' experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Xucong; Sugano, Yusuke; Fritz, Mario; Bulling, Andreas
TI  - MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation
PY  - 2017
AB  - Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze dataset, which contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance, while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves on the state of the art by 22 percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation.
SP  - 162
EP  - 175
JF  - IEEE transactions on pattern analysis and machine intelligence
VL  - 41
IS  - 1
PB  - 
DO  - 10.1109/tpami.2017.2778103
ER  - 

TY  - NA
AU  - Rahman, Protiva; Nandi, Arnab
TI  - IUI - Transformer: a database-driven approach to generating forms for constrained interaction
PY  - 2019
AB  - Form-based data insertion or querying is often one of the most time-consuming steps in data-driven workflows. The small screen and lack of physical keyboard in devices such as smartphones and smartwatches introduce imprecision during user input. This can lead to data quality issues such as incomplete responses and errors, increasing user input time. We present Transformer, a system that leverages the contents of the database to automatically optimize forms for constrained input settings. Our cost function models the user input effort based on the schema and data distribution. This is used by Transformer to find the user interface (UI) widget and layout with ideal input cost for each form field. We demonstrate through user studies that Transformer provides a significantly improved user experience, with up to 50% and 57% reduction in form completion time for smartphones and smartwatches respectively.
SP  - 485
EP  - 496
JF  - Proceedings of the 24th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3301275.3302269
ER  - 

TY  - CHAP
AU  - Deka, Biplab; Doosti, Bardia; Huang, Forrest; Franzen, Chad; Hibschman, Joshua; Afergan, Daniel; Li, Yang; Kumar, Ranjitha; Dong, Tao; Nichols, Jeffrey
TI  - An Early Rico Retrospective: Three Years of Uses for a Mobile App Dataset
PY  - 2021
AB  - The Rico dataset, containing design data from more than 9.7 k Android apps spanning \(27\) categories, was released in 2017. It exposes visual, textual, structural, and interactive design properties of more than 72 k unique UI screens. Over the years since its release, the original paper has been cited nearly 100 times according to Google Scholar and the dataset has been used as the basis for numerous research projects. In this chapter, we describe the creation of Rico using a system that combined crowdsourcing and automation to scalably mine design and interaction data from Android apps at runtime. We then describe two projects that we conducted using the dataset: the training of an autoencoder to identify similarity between UI designs, and an exploration of the use of Google’s Material Design within the dataset using machine learned models. We conclude with an overview of other work that has used Rico to understand our mobile UI world and build data-driven models that assist users, designers, and developers.
SP  - 229
EP  - 256
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_8
ER  - 

TY  - NA
AU  - Borowski, Marcel; Murray, Luke; Bagge, Rolf; Kristensen, Janus Bager; Satyanarayan, Arvind; Klokmose, Clemens Nylandsted
TI  - Varv: Reprogrammable Interactive Software as a Declarative Data Structure
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502064
ER  - 

TY  - NA
AU  - Hagestedt, Inken; Backes, Michael; Bulling, Andreas
TI  - Adversarial Attacks on Classifiers for Eye-based User Modelling
PY  - 2020
AB  - An ever-growing body of work has demonstrated the rich information content available in eye movements for user modelling, e.g. for predicting users' activities, cognitive processes, or even personality traits. We show that state-of-the-art classifiers for eye-based user modelling are highly vulnerable to adversarial examples: small artificial perturbations in gaze input that can dramatically change a classifier's predictions. We generate these adversarial examples using the Fast Gradient Sign Method (FGSM) that linearises the gradient to find suitable perturbations. On the sample task of eye-based document type recognition we study the success of different adversarial attack scenarios: with and without knowledge about classifier gradients (white-box vs. black-box) as well as with and without targeting the attack to a specific class, In addition, we demonstrate the feasibility of defending against adversarial attacks by adding adversarial examples to a classifier's training data.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379157.3390511
ER  - 

TY  - NA
AU  - DeLine, Robert
TI  - CHI - Glinda: Supporting Data Science with Live Programming, GUIs and a Domain-specific Language
PY  - 2021
AB  - Researchers have explored several avenues to mitigate data scientists’ frustrations with computational notebooks, including: (1) live programming, to keep notebook results consistent and up to date; (2) supplementing scripting with graphical user interfaces (GUIs), to improve ease of use; and (3) providing domain-specific languages (DSLs), to raise a script’s level of abstraction. This paper introduces Glinda, which combines these three approaches by providing a live programming experience, with interactive results, for a domain-specific language for data science. The language’s compiler uses an open-ended set of “recipes” to execute steps in the user’s data science workflow. Each recipe is intended to combine the expressiveness of a written notation with the ease-of-use of a GUI. Live programming provides immediate feedback to a user’s input, whether in the form of program edits or GUI gestures. In a qualitative evaluation with 12 professional data scientists, participants highly rated the live programming and interactive results. They found the language productive and sufficiently expressive and suggested opportunities to extend it.
SP  - 1
EP  - 11
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445267
ER  - 

TY  - CHAP
AU  - Hattab, Ammar
TI  - Linear Arrangement of Spherical Markers for Absolute Position Tracking of a Passive Stylus
PY  - 2022
AB  - NA
SP  - 42
EP  - 51
JF  - Human-Computer Interaction. Technological Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05409-9_4
ER  - 

TY  - BOOK
AU  - Müller, Philipp; Sood, Ekta; Bulling, Andreas
TI  - ETRA - Anticipating Averted Gaze in Dyadic Interactions
PY  - 2020
AB  - We present the first method to anticipate averted gaze in natural dyadic interactions. The task of anticipating averted gaze, i.e. that a person will not make eye contact in the near future, remains unsolved despite its importance for human social encounters as well as a number of applications, including human-robot interaction or conversational agents. Our multimodal method is based on a long short-term memory (LSTM) network that analyses non-verbal facial cues and speaking behaviour. We empirically evaluate our method for different future time horizons on a novel dataset of 121 YouTube videos of dyadic video conferences (74 hours in total). We investigate person-specific and person-independent performance and demonstrate that our method clearly outperforms baselines in both settings. As such, our work sheds light on the tight interplay between eye contact and other non-verbal signals and underlines the potential of computational modelling and anticipation of averted gaze for interactive applications.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391332
ER  - 

TY  - JOUR
AU  - Moran, Kevin; Bernal-Cardenas, Carlos; Curcio, Michael; Bonett, Richard; Poshyvanyk, Denys
TI  - Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps
PY  - 2020
AB  - It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection , classification , and assembly . First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled . We implemented this approach for Android in a system called ReDraw . Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91 percent and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.
SP  - 196
EP  - 221
JF  - IEEE Transactions on Software Engineering
VL  - 46
IS  - 2
PB  - 
DO  - 10.1109/tse.2018.2844788
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Chen, Jingya; Xia, Haijun; Mitchell, Tom M.; Myers, Brad A.
TI  - UIST - Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs
PY  - 2020
AB  - A major problem in task-oriented conversational agents is the lack of support for the repair of conversational breakdowns. Prior studies have shown that current repair strategies for these kinds of errors are often ineffective due to: (1) the lack of transparency about the state of the system's understanding of the user's utterance; and (2) the system's limited capabilities to understand the user's verbal attempts to repair natural language understanding errors. This paper introduces SOVITE, a new multi-modal speech plus direct manipulation interface that helps users discover, identify the causes of, and recover from conversational breakdowns using the resources of existing mobile app GUIs for grounding. SOVITE displays the system's understanding of user intents using GUI screenshots, allows users to refer to third-party apps and their GUI screens in conversations as inputs for intent disambiguation, and enables users to repair breakdowns using direct manipulation on these screenshots. The results from a remote user study with 10 users using SOVITE in 7 scenarios suggested that SOVITE's approach is usable and effective.
SP  - 1094
EP  - 1107
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415820
ER  - 

TY  - NA
AU  - Ma'ayan, Dor; Ni, Wode; Ye, Katherine; Kulkarni, Chinmay; Sunshine, Joshua
TI  - CHI - How Domain Experts Create Conceptual Diagrams and Implications for Tool Design
PY  - 2020
AB  - Conceptual diagrams are used extensively to understand abstract relationships, explain complex ideas, and solve difficult problems. To illustrate concepts effectively, experts find appropriate visual representations and translate concepts into concrete shapes. This translation step is not supported explicitly by current diagramming tools. This paper investigates how domain experts create conceptual diagrams via semi-structured interviews with 18 participants from diverse backgrounds. Our participants create, adapt, and reuse visual representations using both sketches and digital tools. However, they had trouble using current diagramming tools to transition from sketches and reuse components from earlier diagrams. Our participants also expressed frustration with the slow feedback cycles and barriers to automation of their tools. Based on these results, we suggest four opportunities of diagramming tools  exploration support, representation salience, live engagement, and vocabulary correspondence  that together enable a natural diagramming experience. Finally, we discuss possibilities to leverage recent research advances to develop natural diagramming tools.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376253
ER  - 

TY  - JOUR
AU  - Chen, Jieshan; Chen, Chunyang; Xing, Zhenchang; Xia, Xin; Zhu, Liming; Grundy, John; Wang, Jinshui
TI  - Wireframe-Based UI Design Search Through Image Autoencoder
PY  - 2020
AB  - UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results. Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks.
SP  - 3391613
EP  - 31
JF  - ACM Transactions on Software Engineering and Methodology
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3391613
ER  - 

TY  - CHAP
AU  - Huang, Forrest; Schoop, Eldon; Ha, David; Nichols, Jeffrey; Canny, John
TI  - Sketch-based Creativity Support Tools using Deep Learning
PY  - 2021
AB  - Sketching is a natural and effective visual communication medium commonly used in creative processes. Recent developments in deep-learning models drastically improved machines’ ability in understanding and generating visual content. An exciting area of development explores deep-learning approaches used to model human sketches, opening opportunities for creative applications. This chapter describes three fundamental steps in developing deep-learning-driven creativity support tools that consume and generate sketches: (1) a data collection effort that generated a new paired dataset between sketches and mobile user interfaces; (2) a sketch-based user interface retrieval system adapted from state-of-the-art computer vision techniques; and, (3) a conversational sketching system that supports the novel interaction of a natural-language-based sketch/critique authoring process. In this chapter, we survey relevant prior work in both the deep-learning and human-computer interaction communities, document the data collection process and the systems’ architectures in detail, present qualitative and quantitative results, and paint the landscape of several future research directions in this exciting area.
SP  - 379
EP  - 415
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_12
ER  - 

TY  - NA
AU  - da Cruz Alves, Nathalia; Kreuch, Leonardo; von Wangenheim, Christiane Gresse
TI  - Analyzing structural similarity of user interface layouts of Android apps using deep learning
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st Brazilian Symposium on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3554364.3559111
ER  - 

TY  - JOUR
AU  - Salimbeni, Guido; Fol Leymarie, Frederic; Latham, William
TI  - A Machine Learning Application Based on Giorgio Morandi Still-Life Paintings to Assist Artists in the Choice of 3D Compositions
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>The authors present a system built to generate arrangements of threedimensional models for aesthetic evaluation, with the aim being to support an artist in their creative process. The authors explore how this system can automatically generate aesthetically pleasing content for use in the media and design industry, based on standards originally developed in master artworks. They then demonstrate the effectiveness of their process in the context of paintings using a collection of images inspired by the work of the artist Giorgio Morandi (Bologna, 1890–1964). Finally, they compare the results of their system with the results of a well-known Generative Adversarial Network (GAN).</jats:p>
SP  - 57
EP  - 61
JF  - Leonardo
VL  - 55
IS  - 1
PB  - 
DO  - 10.1162/leon_a_02073
ER  - 

TY  - JOUR
AU  - Chen, Chunyang; Feng, Sidong; Xing, Zhenchang; Liu, Linda; Zhao, Shengdong; Wang, Jinshui
TI  - Gallery D.C.: Design Search and Knowledge Discovery through Auto-created GUI Component Gallery
PY  - 2019
AB  - Online communities like Dribbble and GraphicBurger allow GUI designers to share their design artwork and learn from each other. These design sharing platforms are important sources for design inspiration, but our survey with GUI designers suggests additional information needs unmet by existing design sharing platforms. First, designers need to see the practical use of certain GUI designs in real applications, rather than just artworks. Second, designers want to see not only the overall designs but also the detailed design of the GUI components. Third, designers need advanced GUI design search abilities (e.g., multi-facets search) and knowledge discovery support (e.g., demographic investigation, cross-company design comparison). This paper presents Gallery D.C. http://mui-collection.herokuapp.com/, a gallery of GUI design components that harness GUI designs crawled from millions of real-world applications using reverse-engineering and computer vision techniques. Through a process of invisible crowdsourcing, Gallery D.C. supports novel ways for designers to collect, analyze, search, summarize and compare GUI designs on a massive scale. We quantitatively evaluate the quality of Gallery D.C. and demonstrate that Gallery D.C. offers additional support for design sharing and knowledge discovery beyond existing platforms.
SP  - 180
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359282
ER  - 

TY  - NA
AU  - Wu, Ziming; Jiang, Yulun; Liu, Yiding; Ma, Xiaojuan
TI  - CHI - Predicting and Diagnosing User Engagement with Mobile UI Animation via a Data-Driven Approach
PY  - 2020
AB  - Animation, a common design element in user interfaces (UI), can impact user engagement (UE) with mobile applications. To avoid impairing UE due to improper design of animation, designers rely on resource-intensive evaluation methods like user studies or expert reviews. To alleviate this burden, we propose a data-driven approach to assisting designers in examining UE issues with their animation designs. We first crowdsource UE assessments of mobile UI animations. Based on the collected data, we then build a novel deep learning model that captures both spatial and temporal features of animations to predict their UE levels. Evaluations show that our model achieves a reasonable accuracy. We further leverage the animation feature encoded by our model and a sample set of expert reviews to derive potential UE issues of a particular animation. Finally, we develop a proof-of-concept tool and evaluate its potential usage in actual design practices with experts
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376324
ER  - 

TY  - NA
AU  - Zhang, Xiaoyi; Ross, Anne Spencer; Fogarty, James
TI  - UIST - Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement
PY  - 2018
AB  - Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.
SP  - 609
EP  - 621
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242616
ER  - 

TY  - NA
AU  - Do, Quan; Campbell, Kiersten; Hine, Emmie; Pham, Dzung; Taylor, Alex; Howley, Iris; Barowy, Daniel W.
TI  - Evaluating ProDirect manipulation in hour of code
PY  - 2019
AB  - We examine whether augmenting traditional coding environments with ProDirect manipulation improves several learning measures. ProDirect manipulation is a novel user interaction model that provides a bidirectional link between code and outputs. Instead of reasoning abstractly about the output a program might produce, users instead directly manipulate outputs (e.g., using a keyboard and mouse). Program text is then updated to reflect the change. We report the effects on learning using a ProDirect manipulation environment versus a standard development environment for more than one hundred middle school students. To conduct the study, we built SWELL, a programming language with ProDirect manipulation features. We conclude that within the context of an Hour-of-Code course, ProDirect manipulation does not offer a significant advantage. We also make several observations regarding the way students interact with SWELL, which may inform future language design for this age group.
SP  - NA
EP  - NA
JF  - Proceedings of the 2019 ACM SIGPLAN Symposium on SPLASH-E
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3358711.3361623
ER  - 

TY  - NA
AU  - Fu, Jingwen; Zhang, Xiaoyi; Wang, Yuwang; Zeng, Wenjun; Yang, Sam; Hilliard, Grayson
TI  - Understanding Mobile GUI: from Pixel-Words to Screen-Sentences.
PY  - 2021
AB  - The ubiquity of mobile phones makes mobile GUI understanding an important task. Most previous works in this domain require human-created metadata of screens (e.g. View Hierarchy) during inference, which unfortunately is often not available or reliable enough for GUI understanding. Inspired by the impressive success of Transformers in NLP tasks, targeting for purely vision-based GUI understanding, we extend the concepts of Words/Sentence to Pixel-Words/Screen-Sentence, and propose a mobile GUI understanding architecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the individual Words, we define the Pixel-Words as atomic visual components (text and graphic components), which are visually consistent and semantically clear across screenshots of a large variety of design styles. The Pixel-Words extracted from a screenshot are aggregated into Screen-Sentence with a Screen Transformer proposed to model their relations. Since the Pixel-Words are defined as atomic visual components, the ambiguity between their visual appearance and semantics is dramatically reduced. We are able to make use of metadata available in training data to auto-generate high-quality annotations for Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words annotations is built based on the public RICO dataset, which will be released to help to address the lack of high-quality training data in this area. We train a detector to extract Pixel-Words from screenshots on this dataset and achieve metadata-free GUI understanding during inference. We conduct experiments and show that Pixel-Words can be well extracted on RICO-PW and well generalized to a new dataset, P2S-UI, collected by ourselves. The effectiveness of PW2SS is further verified in the GUI understanding tasks including relation prediction, clickability prediction, screen retrieval, and app type classification.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Bâce, Mihai; Staal, Sander; Bulling, Andreas
TI  - CHI - Quantification of Users' Visual Attention During Everyday Mobile Device Interactions
PY  - 2020
AB  - We present the first real-world dataset and quantitative evaluation of visual attention of mobile device users in-situ, i.e. while using their devices during everyday routine. Understanding user attention is a core research challenge in mobile HCI but previous approaches relied on usage logs or self-reports that are only proxies and consequently do neither reflect attention completely nor accurately. Our evaluations are based on Everyday Mobile Visual Attention (EMVA)  a new 32-participant dataset containing around 472 hours of video snippets recorded over more than two weeks in real life using the front-facing camera as well as associated usage logs, interaction events, and sensor data. Using an eye contact detection method, we are first to quantify the highly dynamic nature of everyday visual attention across users, mobile applications, and usage contexts. We discuss key insights from our analyses that highlight the potential and inform the design of future mobile attentive user interfaces.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376449
ER  - 

TY  - NA
AU  - Nguyen, Tam; Vu, Phong Minh; Pham, Hung; Nguyen, Tung
TI  - ICSE (NIER) - Deep learning UI design patterns of mobile apps
PY  - 2018
AB  - User interface (UI) is one of the most important components of a mobile app and strongly influences users' perception of the app. However, UI design tasks are typically manual and time-consuming. This paper proposes a novel approach to (semi)-automate those tasks. Our key idea is to develop and deploy advanced deep learning models based on recurrent neural networks (RNN) and generative adversarial networks (GAN) to learn UI design patterns from millions of currently available mobile apps. Once trained, those models can be used to search for UI design samples given user-provided descriptions written in natural language and generate professional-looking UI designs from simpler, less elegant design drafts.
SP  - 65
EP  - 68
JF  - Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3183399.3183422
ER  - 

TY  - NA
AU  - Lee, Joon Hyub; Ham, Hyung-Gi; Bae, Seok-Hyung
TI  - CHI Extended Abstracts - 3D Sketching for Multi-Pose Products
PY  - 2020
AB  - During the initial stage of design, 2D perspective sketching is an essential tool for designers. However, for products with multiple moving parts that assume multiple poses during usage, 2D perspective sketching can be painstaking and time-consuming. In this study, we show that such multi-pose products are prevalent in the context of product design and therefore propose a 3D sketching system tailored to multi-pose products. Our system enables designers to sketch 3D curves that can be freely posed and easily viewed from different directions; it makes sketching chained moving parts and propagating changes to different poses and perspectives effortless. We show that, with interactions closely resembling traditional 2D perspective sketching and the physical manipulation of an articulated object, designers can focus solely on ideating, iterating, and communicating multi-pose product concepts during the initial stage of design.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382870
ER  - 

TY  - NA
AU  - Li, Yang; He, Jiacong; Zhou, Xin; Zhang, Yuan; Baldridge, Jason
TI  - Mapping Natural Language Instructions to Mobile UI Action Sequences
PY  - 2020
AB  - We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PIXELHELP, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in HowTo instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PIXELHELP.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Joolee, Joolekha Bibi; Raza, Ahsan; Abdullah, Muhammad; Jeon, Seokhee
TI  - Tracking of Flexible Brush Tip on Real Canvas: Silhouette-Based and Deep Ensemble Network-Based Approaches
PY  - 2020
AB  - Digital painting is a process of creating a digital artwork using modern human-computer interaction technologies. One of the core enabling technologies is the real-time tracking of user's strokes, which is generally supplied by a digital tablet with a stylus. While the digital tablet technology provides highly accurate tracking, the drawing should be done with a rigid stylus on a plastic surface. This sometimes destroys the realism of drawing, such as interaction with the digital tablet cannot provide the feedback of subtle texture, friction of the paper/fabric canvas and tension of soft painting brush. This becomes particularly problematic for traditional painting artists who are trained with and prefer real painting brush and paper/fabric canvas. Thus, the aim of this work is to present an alternative solution where the user's strokes can be tracked even when the actual brush and canvas are used. To this end, we proposed two approaches for digitally tracking the tip of flexible bristles of a soft brush, so that the painting can be created digitally on a computer. The first approach captures the silhouette of deforming bristles using a pair of well-aligned infra-red (IR) cameras, which extracts the tip from the silhouette, and reconstructs the 2D position of the tip. The second approach predicts the brush tip position through a deep ensemble network-based approach where the relationship between the brush tip position and brush handle pose are trained with our novel model comprising of Long-Short Term Memory Autoencoder and 1-D Convolutional Neural Network. The trained model is used to predict the brush tip position in realtime. Both approaches extensively evaluated through multiple tests. Furthermore, our model outperforms the state-of-the-art models.
SP  - 115778
EP  - 115788
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.3004173
ER  - 

TY  - JOUR
AU  - Kurniawati, Gisela; Karnalim, Oscar
TI  - Introducing a Practical Educational Tool for Correlating Algorithm Time Complexity with Real Program Execution
PY  - 2018
AB  - Algorithm time complexity is an important topic to be learned for programmer; it could define whether an algorithm is practical to be used on real environment or not. However, learning such material is not a trivial task. Based on our informal observation regarding students’ test, most of them could not correlate Big-Oh equation to real program execution. This paper proposes JCEL, an educational tool that acts as a supportive tool for learning algorithm time complexity. Using this tool, user could learn how to correlate Big-Oh equation with real program execution by providing three components: a Java source code, source code input set, and time complexity equations. According to our evaluation, students feel that JCEL is helpful for learning the correlation between Big-Oh equation and real program execution. Further, the use of Pearson correlation in JCEL shows a promising result.
SP  - 1
EP  - 15
JF  - Journal of Information Technology and Computer Science
VL  - 3
IS  - 1
PB  - 
DO  - 10.25126/jitecs.20183140
ER  - 

TY  - NA
AU  - Englmeier, David; Dorner, Julia; Butz, Andreas; Höllerer, Tobias
TI  - VR - A Tangible Spherical Proxy for Object Manipulation in Augmented Reality
PY  - 2020
AB  - In this paper, we explore how a familiarly shaped object can serve as a physical proxy to manipulate virtual objects in Augmented Reality (AR) environments. Using the example of a tangible, handheld sphere, we demonstrate how irregularly shaped virtual objects can be selected, transformed, and released. After a brief description of the implementation of the tangible proxy, we present a buttonless interaction technique suited to the characteristics of the sphere. In a user study (N = 30), we compare our approach with three different controller-based methods that increasingly rely on physical buttons. As a use case, we focused on an alignment task that had to be completed in mid-air as well as on a flat surface. Results show that our concept has advantages over two of the controller-based methods regarding task completion time and user ratings. Our findings inform research on integrating tangible interaction into AR experiences.
SP  - 221
EP  - 229
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581027011745
ER  - 

TY  - CHAP
AU  - Ionescu, Bogdan; Müller, Henning; Péteri, Renaud; Abacha, Asma Ben; Sarrouti, Mourad; Demner-Fushman, Dina; Hasan, Sadid A.; Kozlovski, Serge; Liauchuk, Vitali; Cid, Yashin Dicente; Kovalev, Vassili; Pelka, Obioma; de Herrera, Alba García Seco; Jacutprakart, Janadhip; Friedrich, Christoph M.; Berari, Raul; Tauteanu, Andrei; Fichou, Dimitri; Brie, Paul; Dogariu, Mihai; Stefan, Liviu-Daniel; Constantin, Mihai Gabriel; Chamberlain, Jon; Campello, Antonio; Clark, Adrian F.; Oliver, Thomas; Moustahfid, Hassan; Popescu, Adrian; Deshayes-Chossart, Jérôme
TI  - CLEF - Overview of the ImageCLEF 2021: Multimedia Retrieval in Medical, Nature, Internet and Social Media Applications
PY  - 2021
AB  - This paper presents an overview of the ImageCLEF 2021 lab that was organized as part of the Conference and Labs of the Evaluation Forum – CLEF Labs 2021. ImageCLEF is an ongoing evaluation initiative (first run in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and domains. In 2021, the 19th edition of ImageCLEF runs four main tasks: (i) a medical task that groups three previous tasks, i.e., caption analysis, tuberculosis prediction, and medical visual question answering and question generation, (ii) a nature coral task about segmenting and labeling collections of coral reef images, (iii) an Internet task addressing the problems of identifying hand-drawn and digital user interface components, and (iv) a new social media aware task on estimating potential real-life effects of online image sharing. Despite the current pandemic situation, the benchmark campaign received a strong participation with over 38 groups submitting more than 250 runs.
SP  - 345
EP  - 370
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85251-1_23
ER  - 

TY  - JOUR
AU  - Ban, Seonghoon; Hyun, Kyung Hoon
TI  - 3D Computational Sketch Synthesis Framework: Assisting Design Exploration Through Generating Variations of User Input Sketch and Interactive 3D Model Reconstruction
PY  - 2020
AB  - NA
SP  - 102789
EP  - NA
JF  - Computer-Aided Design
VL  - 120
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2019.102789
ER  - 

TY  - NA
AU  - Cooper, Nathan; Bernal-Cardenas, Carlos; Chaparro, Oscar; Moran, Kevin; Poshyvanyk, Denys
TI  - ICSE - It Takes Two to Tango: Combining Visual and Textual Information for Detecting Duplicate Video-Based Bug Reports
PY  - 2021
AB  - When a bug manifests in a user-facing application, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such information is reported en masse, such as during crowd-sourced testing, managing these artifacts can be a time-consuming process. As the reporting of screen-recordings in particular becomes more popular, developers are likely to face challenges related to manually identifying videos that depict duplicate bugs. Due to their graphical nature, screen-recordings present challenges for automated analysis that preclude the use of current duplicate bug report detection techniques. To overcome these challenges and aid developers in this task, this paper presents Tango, a duplicate detection technique that operates purely on video-based bug reports by leveraging both visual and textual information. Tango combines tailored computer vision techniques, optical character recognition, and text retrieval. We evaluated multiple configurations of Tango in a comprehensive empirical evaluation on 4,860 duplicate detection tasks that involved a total of 180 screen-recordings from six Android apps. Additionally, we conducted a user study investigating the effort required for developers to manually detect duplicate video-based bug reports and compared this to the effort required to use Tango. The results reveal that Tango's optimal configuration is highly effective at detecting duplicate video-based bug reports, accurately ranking target duplicate videos in the top-2 returned results in 83% of the tasks. Additionally, our user study shows that, on average, Tango can reduce developer effort by over 60%, illustrating its practicality.
SP  - 957
EP  - 969
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse43902.2021.00091
ER  - 

TY  - NA
AU  - Mohian, Soumik; Csallner, Christoph
TI  - PSDoodle
PY  - 2022
AB  - Searching through existing repositories for a specific mobile app screen design is currently either slow or tedious. Such searches are either limited to basic keyword searches (Google Image Search) or require as input a complete query screen image (SWIRE). A promising alternative is interactive partial sketching, which is more structured than keyword search and faster than complete-screen queries. PSDoodle is the first system to allow interactive search of screens via interactive sketching. PSDoodle is built on top of a combination of the Rico repository of some 58k Android app screens, the Google QuickDraw dataset of icon-level doodles, and DoodleUINet, a curated corpus of some 10k app icon doodles collected from hundreds of individuals. In our evaluation with third-party software developers, PSDoodle provided similar top-10 screen retrieval accuracy as the state of the art from the SWIRE line of work, while cutting the average time required about in half.
SP  - NA
EP  - NA
JF  - Proceedings of the 9th IEEE/ACM International Conference on Mobile Software Engineering and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3524613.3527816
ER  - 

TY  - NA
AU  - Li, Yuanchun; Yang, Ziyue; Guo, Yao; Chen, Xiangqun
TI  - A Deep Learning based Approach to Automated Android App Testing.
PY  - 2019
AB  - Automated input generators are widely used for large-scale dynamic analysis and testing of mobile apps. Such input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. In this paper, we propose Humanoid, a deep learning-based approach to automated Android app testing. Our insight is that if we can learn from human-generated interaction traces, it is possible to generate human-like test inputs based on the visual information in the current UI state and the latest state transitions. We design and implement a deep neural network model to learn how end-users would interact with an app (specifically, which UI elements to interact with and how), and show that we can successfully generate human-like inputs for any new UI based on the learned model. We then apply the model to automated testing of Android apps and demonstrate that it is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Arsan, Deniz; Zaidi, Ali; Sagar, Aravind; Kumar, Ranjitha
TI  - UIST - App-Based Task Shortcuts for Virtual Assistants
PY  - 2021
AB  - Virtual assistants like Google Assistant and Siri often interface with external apps when they cannot directly perform a task. Currently, developers must manually expose the capabilities of their apps to virtual assistants, using App Actions on Android or Shortcuts on iOS. This paper presents savant, a system that automatically generates task shortcuts for virtual assistants by mapping user tasks to relevant UI screens in apps. For a given natural language task (e.g., “send money to Joe”), savant leverages text and semantic information contained within UIs to identify relevant screens, and intent modeling to parse and map entities (e.g., “Joe”) to required UI inputs. Therefore, savant allows virtual assistants to interface with apps and handle new tasks without requiring any developer effort. To evaluate savant, we performed a user study to identify common tasks users perform with virtual assistants. We then demonstrate that savant can find relevant app screens for those tasks and autocomplete the UI inputs.
SP  - 1089
EP  - 1099
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474808
ER  - 

TY  - NA
AU  - Li, Zhengqing; Miyafuji, Shio; Sato, Toshiki; Koike, Hideki; Yamashita, Naomi; Kuzuoka, Hideaki
TI  - Impact of Display Shapes on Symmetric 360° Video Communication For Remote Collaboration
PY  - 2019
AB  - NA
SP  - 165
EP  - 176
JF  - NA
VL  - 24
IS  - 2
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Sarmadi, Hamid; Muñoz-Salinas, Rafael; Berbís, M. A.; Medina-Carnicer, Rafael
TI  - Simultaneous Multi-View Camera Pose Estimation and Object Tracking With Squared Planar Markers
PY  - 2019
AB  - Object tracking is a key aspect in many applications, such as augmented reality in medicine (e.g., tracking a surgical instrument) or robotics. Squared planar markers have become popular tools for tracking since their pose can be estimated from their four corners. While using a single marker and a single camera limits the working area considerably, using multiple markers attached to an object requires estimating their relative position, which is not trivial, for high-accuracy tracking. Likewise, using multiple cameras requires estimating their extrinsic parameters, also a tedious process that must be repeated whenever a camera is moved. This paper proposes a novel method to simultaneously solve the above-mentioned problems. From a video sequence showing a rigid set of planar markers recorded from multiple cameras, the proposed method is able to automatically obtain the three-dimensional configuration of the markers, the extrinsic parameters of the cameras, and the relative pose between the markers and the cameras at each frame. Our experiments show that our approach can obtain highly accurate results for estimating these parameters using the low-resolution cameras. Once the parameters are obtained, tracking of the object can be done in real time with a low computational cost. The proposed method is a step forward in the development of cost-effective solutions for object tracking.
SP  - 22927
EP  - 22940
JF  - IEEE Access
VL  - 7
IS  - NA
PB  - 
DO  - 10.1109/access.2019.2896648
ER  - 

TY  - JOUR
AU  - Kar, Pragma; Chattopadhyay, Samiran; Chakraborty, Sandip
TI  - Gestatten: Estimation of User's Attention in Mobile MOOCs From Eye Gaze and Gaze Gesture Tracking
PY  - 2020
AB  - The rapid proliferation of Massive Open Online Courses (MOOC) has resulted in many-fold increase in sharing the global classrooms through customized online platforms, where a student can participate in the classes through her personal devices, such as personal computers, smartphones, tablets, etc. However, in the absence of direct interactions with the students during the delivery of the lectures, it becomes difficult to judge their involvements in the classroom. In academics, the degree of student's attention can indicate whether a course is efficacious in terms of clarity and information. An automated feedback can hence be generated to enhance the utility of the course. The precision of discernment in the context of human attention is a subject of surveillance. However, visual patterns indicating the magnitude of concentration can be deciphered by analyzing the visual emphasis and the way an individual visually gesticulates, while contemplating the object of interest. In this paper, we develop a methodology called Gestsatten which captures the learner's attentiveness from his visual gesture patterns. In this approach, the learner's visual gestures are tracked along with the region of focus. We consider two aspects in this approach -- first, we do not transfer learner's video outside her device, so we apply in-device computing to protect her privacy; second, considering the fact that a majority of the learners use handheld devices like smartphones to observe the MOOC videos, we develop a lightweight approach for in-device computation. A three level estimation of learner's attention is performed based on these information. We have implemented and tested Gestatten over 48 participants from different age groups, and we observe that the proposed technique can capture the attention level of a learner with high accuracy (average absolute error rate is 8.68%), which meets her ability to learn a topic as measured through a set of cognitive tests.
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - EICS
PB  - 
DO  - 10.1145/3394974
ER  - 

TY  - NA
AU  - Faust, Rebecca; Scheidegger, Carlos; Isaacs, Katherine; Bernstein, William Z.; Sharp, Michael; North, Chris
TI  - Interactive Visualization for Data Science Scripts
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE Visualization in Data Science (VDS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vds57266.2022.00009
ER  - 

TY  - JOUR
AU  - Lopez, Christian E.; Miller, Scarlett R.; Tucker, Conrad S.
TI  - Exploring biases between human and machine generated designs
PY  - 2018
AB  - <jats:p>The objective of this work is to explore the possible biases that individuals may have toward the perceived functionality of machine generated designs, compared to human created designs. Toward this end, 1187 participants were recruited via Amazon mechanical Turk (AMT) to analyze the perceived functional characteristics of both human created two-dimensional (2D) sketches and sketches generated by a deep learning generative model. In addition, a computer simulation was used to test the capability of the sketched ideas to perform their intended function and explore the validity of participants' responses. The results reveal that both participants and computer simulation evaluations were in agreement, indicating that sketches generated via the deep generative design model were more likely to perform their intended function, compared to human created sketches used to train the model. The results also reveal that participants were subject to biases while evaluating the sketches, and their age and domain knowledge were positively correlated with their perceived functionality of sketches. The results provide evidence that supports the capabilities of deep learning generative design tools to generate functional ideas and their potential to assist designers in creative tasks such as ideation.</jats:p>
SP  - 021104
EP  - NA
JF  - Journal of Mechanical Design
VL  - 141
IS  - 2
PB  - 
DO  - 10.1115/1.4041857
ER  - 

TY  - CHAP
AU  - Ionescu, Bogdan; Müller, Henning; Péteri, Renaud; Abacha, Asma Ben; Datla, Vivek V.; Hasan, Sadid A.; Demner-Fushman, Dina; Kozlovski, Serge; Liauchuk, Vitali; Cid, Yashin Dicente; Kovalev, Vassili; Pelka, Obioma; Friedrich, Christoph M.; de Herrera, Alba García Seco; Ninh, Van-Tu; Le, Tu-Khiem; Zhou, Liting; Piras, Luca; Riegler, Michael; Halvorsen, Pål; Tran, Minh-Triet; Lux, Mathias; Gurrin, Cathal; Dang-Nguyen, Duc-Tien; Chamberlain, Jon; Clark, Adrian F.; Campello, Antonio; Fichou, Dimitri; Berari, Raul; Brie, Paul; Dogariu, Mihai; Stefan, Liviu-Daniel; Constantin, Mihai Gabriel
TI  - CLEF - Overview of the ImageCLEF 2020: Multimedia Retrieval in Medical, Lifelogging, Nature, and Internet Applications
PY  - 2020
AB  - This paper presents an overview of the ImageCLEF 2020 lab that was organized as part of the Conference and Labs of the Evaluation Forum - CLEF Labs 2020. ImageCLEF is an ongoing evaluation initiative (first run in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and domains. In 2020, the 18th edition of ImageCLEF runs four main tasks: (i) a medical task that groups three previous tasks, i.e., caption analysis, tuberculosis prediction, and medical visual question answering and question generation, (ii) a lifelog task (videos, images and other sources) about daily activity understanding, retrieval and summarization, (iii) a coral task about segmenting and labeling collections of coral reef images, and (iv) a new Internet task addressing the problems of identifying hand-drawn user interface components. Despite the current pandemic situation, the benchmark campaign received a strong participation with over 40 groups submitting more than 295 runs.
SP  - 311
EP  - 341
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58219-7_22
ER  - 

TY  - NA
AU  - Liu, Qingying; Zhang, Tao; Gao, Jerry; Liu, Shaoying; Cheng, Jing
TI  - Construction of Semantic Model for GUI of Mobile Applications Using Deep Learning
PY  - 2022
AB  - Modeling the graphical user interface (GUI) of mobile applications is a crucial task for automated robotic testing. Pixel-based modeling methods are non-intrusive and thus have potential for truly black-box automation. However, existing modeling methods can hardly produce accurate models because they do not take the semantic and structural information of GUI into account. In this paper, we propose a layered semantic approach to modeling GUI for mobile applications to address this important problem. The proposed approach adopts a method for recognizing GUI elements and a novel strategy for semantics acquisition using deep learning. The model generated using the proposed approach can support the fully black-box automated robotic testing. We evaluate the approach by conducting a small experiment on 10 mobile applications. The results demonstrate that the proposed approach is effective in generating the GUI models.
SP  - NA
EP  - NA
JF  - 2022 IEEE International Conference On Artificial Intelligence Testing (AITest)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aitest55621.2022.00010
ER  - 

TY  - JOUR
AU  - Bace, Mihai; Staal, Sander; Bulling, Andreas
TI  - How Far Are We From Quantifying Visual Attention in Mobile HCI
PY  - 2020
AB  - With an ever-increasing number of mobile devices competing for attention, quantifying when, how often, or for how long users look at their devices has emerged as a key challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying overt visual attention during everyday mobile interactions. In this article, we discuss the main challenges and sources of error associated with sensing visual attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head poses estimation, and the need for accurate gaze estimation. Our analysis informs future research on this emerging topic and underlines the potential of eye contact detection for exciting new applications toward next-generation pervasive attentive user interfaces.
SP  - 46
EP  - 55
JF  - IEEE Pervasive Computing
VL  - 19
IS  - 2
PB  - 
DO  - 10.1109/mprv.2020.2967736
ER  - 

TY  - CHAP
AU  - Chhetri, Sujit Rokka; Wan, Jiang; Canedo, Arquimedes; Faruque, Mohammad Abdullah Al
TI  - Design Automation Using Structural Graph Convolutional Neural Networks
PY  - 2019
AB  - There is a large amount of engineering design data freely available on the internet. Such engineering data is available in various forms and format (such as mechanical designs, circuits layouts, and system architecture). Typically, new designs are based on previous designs that engineers use as a starting point to be adapted to the new requirements. This design, if freely made available, can help in design automation for other engineers who may want to design similar systems. However, one of the major challenges in maximizing design re-usability is the lack of a framework to quantify the similarities and differences in engineering designs. Engineering data is rich in information (with text, images, descriptions, etc., incorporated in them), making them high-dimensional. Interlinked knowledge graph is an efficient way to represent such data. However, processing such a knowledge graph to quantify sub-structures requires better algorithms. Recently, graph convolutional neural networks have shown to be a promising direction for analyzing such graph data. In this chapter, we present a new structural graph convolutional neural network (SGCNN) capable of abstracting features from graphs based on their structure and neighborhood features. With this algorithm we classify and cluster sub-graphs according to their engineering functionality. This will allow engineers in design automation by providing an algorithm for discovery of functionally equivalent existing engineering designs.
SP  - 237
EP  - 259
JF  - Design Automation of Cyber-Physical Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-13050-3_9
ER  - 

TY  - NA
AU  - Krosnick, Rebecca; Oney, Steve
TI  - ParamMacros: Creating UI Automation Leveraging End-User Natural Language Parameterization
PY  - 2022
AB  - Prior work in programming-by-demonstration (PBD) has explored ways to enable end-users to create custom automation without needing to write code. We propose a new end-user specification model &#x2013; asking the end-user to explicitly identify parts of their natural language query that can be generalized. We built a PBD system, ParamMacros, where users first generalize a concrete natural language question &#x2013; identifying parameters and their possible values &#x2013; and then create a demonstration of how to answer the question on the website of interest. ParamMacros then infers a generalized program by using the user-provided parameter values to identify relevant patterns in the website&#x2019;s structure. In a lab study we found that participants were able to meaningfully parameterize natural language queries and felt such a parameterization and demonstration process would be useful for creating custom automation.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833005
ER  - 

TY  - JOUR
AU  - Lee, Dong-Kun; Park, Bon-Yeong
TI  - A Case Study on the Sustainability for a Stanchion of Recreational Crafts based on the Design for Additive Manufacturing Using a FFF-type 3D Printer
PY  - 2021
AB  - NA
SP  - 294
EP  - 302
JF  - Journal of the Society of Naval Architects of Korea
VL  - 58
IS  - 5
PB  - 
DO  - 10.3744/snak.2021.58.5.294
ER  - 

TY  - NA
AU  - Ikarashi, Yuka; Ragan-Kelley, Jonathan; Fukusato, Tsukasa; Kato, Jun; Igarashi, Takeo
TI  - VL/HCC - Guided Optimization for Image Processing Pipelines
PY  - 2021
AB  - Writing high-performance image processing code is challenging and labor-intensive. To address this, we propose a programming support method called “guided optimization.” Guided optimization provides programmers a set of valid optimization options and interactive feedback about their current choices, which enables them to comprehend and efficiently optimize image processing code without the time-consuming trial-and-error process of traditional text editors. We implemented a proof-of-concept system, Roly-poly, which integrates guided optimization, program visualization, and schedule cost estimation to support the comprehension and development of efficient image processing code. We conducted a user study with novice Halide programmers and confirmed that Roly-poly and its guided optimization was informative, increased productivity, and resulted in higher-performing schedules in less time.
SP  - 1
EP  - 5
JF  - 2021 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc51201.2021.9576341
ER  - 

TY  - NA
AU  - Moran, Kevin; Bernal-Cardenas, Carlos; Curcio, Michael; Bonett, Richard; Poshyvanyk, Denys
TI  - Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps
PY  - 2018
AB  - It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91% and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Feng, Sidong; Chen, Chunyang; Xing, Zhenchang
TI  - Gallery D.C.: Auto-created GUI Component Gallery for Design Search and Knowledge Discovery
PY  - 2022
AB  - GUI design is an integral part of software development. The process of designing a mobile application typically starts with the ideation and inspiration search from existing designs. However, existing information-retrieval based, and database-query based methods cannot eﬃciently gain inspirations in three requirements: design practicality, design granularity and design knowledge discovery. In this paper we propose a web application, called Gallery D.C. that aims to facilitate the process of user interface design through real world GUI component search. Gallery D.C. indexes GUI compo-nent designs using reverse engineering and deep learning based computer vision techniques on millions of real world applications. To perform an advanced design search and knowledge discovery, our approach extracts information about size, color, component type, and text information to help designers explore multi-faceted design space and distill higher-order of design knowledge. Gallery D.C. is well received via an informal evaluation with 7 professional designers.Web Link: http://mui-collection.herokuapp.com/.Demo Video Link: https://youtu.be/zVmsz_wY5OQ.
SP  - NA
EP  - NA
JF  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse-companion55297.2022.9793764
ER  - 

TY  - JOUR
AU  - Jiang, Zexun; Yin, Hao; Luo, Yan; Gong, Jiaying
TI  - AMACS: Automated Mobile Application Content Sensing
PY  - 2020
AB  - After a decade of rapid development, mobile devices have become an essential part of people’s daily life and mobile applications generate rich contents and provide important information access. Contents inside mobile applications can provide valuable insights for social sensing and application development. However, unlike Web search, there is a lack of efficient methods to sense and index information from mobile applications. This article proposes an automated mobile application content sensing (AMACS) framework, which can be the fundamental tool to extract effectively the contents and conducting measurements in mobile applications. AMACS uses a content-aware discovery model to extract effectively the contents from a variety of mobile applications without any manual intervention. Its modular design with independent building blocks can be readily expanded into a distributed system for large-scale deployment. AMACS is compared with the existing relevant mobile automated analysis methods to evaluate its performance. The results show that the crawler can cover more contents efficiently in mobile applications with low overheads, and it can be used to extract contents for application scenarios like social sensing and network measurements.
SP  - 275
EP  - 284
JF  - IEEE Transactions on Computational Social Systems
VL  - 7
IS  - 1
PB  - 
DO  - 10.1109/tcss.2019.2962104
ER  - 

TY  - JOUR
AU  - Bajammal, Mohammad; Stocco, Andrea; Mazinanian, Davood; Mesbah, Ali
TI  - A Survey on the Use of Computer Vision to Improve Software Engineering Tasks
PY  - 2022
AB  - NA
SP  - 1722
EP  - 1742
JF  - IEEE Transactions on Software Engineering
VL  - 48
IS  - 5
PB  - 
DO  - 10.1109/tse.2020.3032986
ER  - 

TY  - NA
AU  - Zhao, Dehai; Xing, Zhenchang; Chen, Chunyang; Xu, Xiwei; Zhu, Liming; Li, Guoqiang; Wang, Jinshui
TI  - ICSE - Seenomaly: vision-based linting of GUI animation effects against design-don't guidelines
PY  - 2020
AB  - GUI animations, such as card movement, menu slide in/out, snackbar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can "see" the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem. Our autoencoder learns to group similar GUI animations by "seeing" lots of unlabeled real-application GUI animations and learning to generate them. As the first work of its kind, we build the datasets of synthetic and real-world GUI animations. Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.
SP  - 1286
EP  - 1297
JF  - Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377811.3380411
ER  - 

TY  - JOUR
AU  - Jin, Haojian; Liu, Minyi; Dodhia, Kevan; Li, Yuanchun; Srivastava, Gaurav Kumar; Fredrikson, Matt; Agarwal, Yuvraj; Hong, Jason
TI  - Why Are They Collecting My Data?: Inferring the Purposes of Network Traffic in Mobile Apps
PY  - 2018
AB  - Many smartphone apps collect potentially sensitive personal data and send it to cloud servers. However, most mobile users have a poor understanding of why their data is being collected. We present MobiPurpose, a novel technique that can take a network request made by an Android app and then classify the data collection purposes, as one step towards making it possible to explain to non-experts the data disclosure contexts. Our purpose inference works by leveraging two observations: 1) developer naming conventions (e.g., URL paths) of ten offer hints as to data collection purposes, and 2) external knowledge, such as app metadata and information about the domain name, are meaningful cues that can be used to infer the behavior of different traffic requests. MobiPurpose parses each traffic request body into key-value pairs, and infers the data type and data collection purpose of each key-value pair using a combination of supervised learning and text pattern bootstrapping. We evaluated MobiPurpose's effectiveness using a dataset cross-labeled by ten human experts. Our results show that MobiPurpose can predict the data collection purpose with an average precision of 84% (among 19 unique categories).
SP  - 173
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 2
IS  - 4
PB  - 
DO  - 10.1145/3287051
ER  - 

TY  - CHAP
AU  - Chong, Eunji; Ruiz, Nataniel; Wang, Yongxin; Zhang, Yun; Rozga, Agata; Rehg, James M.
TI  - ECCV (5) - Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency
PY  - 2018
AB  - This paper addresses the challenging problem of estimating the general visual attention of people in images. Our proposed method is designed to work across multiple naturalistic social scenarios and provides a full picture of the subject’s attention and gaze. In contrast, earlier works on gaze and attention estimation have focused on constrained problems in more specific contexts. In particular, our model explicitly represents the gaze direction and handles out-of-frame gaze targets. We leverage three different datasets using a multi-task learning approach. We evaluate our method on widely used benchmarks for single-tasks such as gaze angle estimation and attention-within-an-image, as well as on the new challenging task of generalized visual attention prediction. In addition, we have created extended annotations for the MMDB and GazeFollow datasets which are used in our experiments, which we will publicly release.
SP  - 397
EP  - 412
JF  - Computer Vision – ECCV 2018
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01228-1_24
ER  - 

TY  - JOUR
AU  - De Gregorio, Daniele; Tonioni, Alessio; Palli, Gianluca; Di Stefano, Luigi
TI  - Semiautomatic Labeling for Deep Learning in Robotics
PY  - 2020
AB  - In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35 000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets. Note to Practitioners —This article was motivated by the lack of a simple and effective solution for the generation of data sets usable to train a data-driven model, such as a modern deep neural network, so as to make them accessible in an industrial environment. Specifically, a deep learning robot guidance vision system would require such a large amount of manually labeled images that it would be too expensive and impractical for a real use case, where system reconfigurability is a fundamental requirement. With our system, on the other hand, especially in the field of industrial robotics, the cost of image labeling can be reduced, for the first time, to nearly zero, thus paving the way for self-reconfiguring systems with very high performance (as demonstrated by our experimental results). One of the limitations of this approach is the need to use a manual method for the detection of objects of interest in the preliminary stages of the pipeline (ARP or graphical interface). A feasible extension, related to the field of collaborative robotics, could be used to exploit the robot itself, manually moved by the user, even for this preliminary stage, so as to eliminate any source of inaccuracy.
SP  - 611
EP  - 620
JF  - IEEE Transactions on Automation Science and Engineering
VL  - 17
IS  - 2
PB  - 
DO  - 10.1109/tase.2019.2938316
ER  - 

TY  - NA
AU  - Louis, Thibault; Troccaz, Jocelyne; Rochet-Capellan, Amélie; Bérard, François
TI  - UIST - GyroSuite: General-Purpose Interactions for Handheld Perspective Corrected Displays
PY  - 2020
AB  - Handheld Perspective-Corrected Displays (HPCDs) are physical objects that have a notable volume and that display a virtual 3D scene on their entire surface. Being handheld, they create the illusion of holding the scene in a physical container (the display). This has strong benefits for the intuitiveness of 3D interaction: manipulating objects of the virtual scene amounts to physical manipulations of the display. HPCDs have been limited so far to technical demonstrators and experimental tools to assess their merits. However, they show great potential as interactive systems for actual 3D applications. This requires that novel interactions be created to go beyond object manipulation and to offer general-purpose services such as menu command selection and continuous parameter control. Working with a two-handed spherical HPCD, we report on the design and informal evaluations of various interaction techniques for distant object selection, scene scaling, menu interaction and continuous parameter control. In particular, our design leverages the efficient two-handed control of the rotations of the display. We demonstrate how some of these techniques can be assemble in a self-contained anatomy learning application. Novice participants used the application in a qualitative user experiment. Most participants used the application effortlessly without any training or explanations.
SP  - 1248
EP  - 1260
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415893
ER  - 

TY  - NA
AU  - Ran, Dezhi; Li, Zongyang; Liu, Chenxu; Wang, Wenyu; Meng, Weizhi; Wu, Xionglin; Jin, Hui; Cui, Jing; Tang, Xing; Xie, Tao
TI  - Automated Visual Testing for Mobile Apps in an Industrial Setting
PY  - 2022
AB  - User Interface (UI) testing has become a common practice for quality assurance of industrial mobile applications (in short as apps). While many automated tools have been developed, they often do not satisfy two major industrial requirements that make a tool desirable in industrial settings: high applicability across platforms (e.g., Android, iOS, AliOS, and Harmony OS) and high capability to handle apps with non-standard UI elements (whose internal structures cannot be acquired using platform APIs). Toward addressing these industrial requirements, automated visual testing emerges to take only device screenshots as input in order to support automated test generation. In this paper, we report our experiences of developing and deploying VTest, our industrial visual testing framework to assure high quality of Taobao, a highly popular industrial app with about one billion monthly active users. VTest includes carefully designed techniques and infrastructure support, outperforming Monkey (which has been popularly deployed in industry and shown to perform superiorly or similarly compared to state-of-the-art tools) with 87.6% more activity coverage. VTEST has been deployed both internally in Alibaba and externally in the Software Green Alliance to provide testing services for top smart-phone vendors and app vendors in China. We summarize five major lessons learned from developing and deploying VTEST.
SP  - NA
EP  - NA
JF  - 2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse-seip55303.2022.9793948
ER  - 

TY  - CHAP
AU  - Ohshima, Yuki; Nakazawa, Atsushi
TI  - ACPR (2) - Eye Contact Detection from Third Person Video
PY  - 2020
AB  - Eye contact is fundamental for human communication and social interactions; therefore much effort has been made to develop automated eye-contact detection using image recognition techniques. However, existing methods use first-person-videos (FPV) that need participants to equip wearable cameras. In this work, we develop an novel eye contact detection algorithm taken from normal viewpoint (third person video) assuming the scenes of conversations or social interactions. Our system have high affordability since it does not require special hardware or recording setups, moreover, can use pre-recorded videos such as Youtube and home videos. In designing algorithm, we first develop DNN-based one-sided gaze estimation algorithms which output the states whether the one subject looks at another. Afterwards, eye contact is found at the frame when the pair of one-sided gaze happens. To verify the proposed algorithm, we generate third-person eye contact video dataset using publicly available videos from Youtube. As the result, proposed algorithms performed 0.775 in precision and 0.671 in recall, while the existing method performed 0.484 in precision and 0.061 in recall, respectively.
SP  - 667
EP  - 677
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-41299-9_52
ER  - 

TY  - CHAP
AU  - Carletto, Romain; Cardot, Hubert; Ragot, Nicolas
TI  - ICDAR (3) - Deep Learning for Document Layout Generation: A First Reproducible Quantitative Evaluation and a Baseline Model.
PY  - 2021
AB  - Deep generative models have been recently experimented in automated document layout generation, which led to significant qualitative results, assessed through user studies and displayed visuals. However, no reproducible quantitative evaluation has been settled in these works, which prevents scientific comparison of upcoming models with previous models. In this context, we propose a fully reproducible evaluation method and an original and efficient baseline model. Our evaluation protocol is meticulously defined in this work, and backed with an open source code available on this link: https://github.com/romain-rsr/quant_eval_for_document_layout_generation/tree/master.
SP  - 20
EP  - 35
JF  - Document Analysis and Recognition – ICDAR 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-86334-0_2
ER  - 

TY  - JOUR
AU  - Bessghaier, Narjes; Soui, Makram; Kolski, Christophe; Chouchane, Mabrouka
TI  - On the Detection of Structural Aesthetic Defects of Android Mobile User Interfaces with a Metrics-based Tool
PY  - 2021
AB  - Smartphone users are striving for easy-to-learn and use mobile apps user interfaces. Accomplishing these qualities demands an iterative evaluation of the Mobile User Interface (MUI). Several studies stress the value of providing a MUI with a pleasing look and feel to engaging end-users. The MUI, therefore, needs to be free from all kinds of structural aesthetic defects. Such defects are indicators of poor design decisions interfering with the consistency of a MUI and making it more difficult to use. To this end, we are proposing a tool (Aesthetic Defects DEtection Tool (ADDET)) to determine the structural aesthetic dimension of MUIs. Automating this process is useful to designers in evaluating the quality of their designs. Our approach is composed of two modules. (1) Metrics assessment is based on the static analysis of a tree-structured layout of the MUI. We used 15 geometric metrics (also known as structural or aesthetic metrics) to check various structural properties before a defect is triggered. (2) Defects detection: The manual combination of metrics and defects are time-consuming and user-dependent when determining a detection rule. Thus, we perceive the process of identification of defects as an optimization problem. We aim to automatically combine the metrics related to a particular defect and optimize the accuracy of the rules created by assigning a weight, representing the metric importance in detecting a defect. We conducted a quantitative and qualitative analysis to evaluate the accuracy of the proposed tool in computing metrics and detecting defects. The findings affirm the tool’s reliability when assessing a MUI’s structural design problems with 71% accuracy.
SP  - 1
EP  - 27
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 11
IS  - 1
PB  - 
DO  - 10.1145/3410468
ER  - 

TY  - NA
AU  - Sidiropoulos, George K.; Papakostas, George A.; Lytridis, Chris; Bazinas, Christos; Kaburlasos, Vassilis G.; Kourampa, Efi; Karageorgiou, Elpida
TI  - Measuring Engagement Level in Child-Robot Interaction Using Machine Learning Based Data Analysis
PY  - 2020
AB  - In this paper, an approach for measuring the engagement level of a child interacting with a social robot in special education is proposed. The introduced methodology uses visual data gathered during the educational procedure and utilizes known machine learning models for analyzing the data. The examined models were able to estimate the engagement level by at most 14.70% Mean Absolute Error (MAE), with the Multi-Layer Perceptron (MLP) model showing the best performance with 12.70% MAE. Moreover, a Long Short-Term Memory (LSTM) model was able to predict the engagement level by a 10.40% MAE, when handling the problem as a time series prediction task. The overall results were very promising revealing the efficiency of the machine learning models to analyze the visual data describing the complex environment of the child-robot interaction in special education.
SP  - NA
EP  - NA
JF  - 2020 International Conference on Data Analytics for Business and Industry: Way Towards a Sustainable Economy (ICDABI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icdabi51230.2020.9325676
ER  - 

TY  - NA
AU  - Mitsuzumi, Yu; Nakazawa, Atsushi
TI  - SMC - Eye Contact Detection Algorithms Using Deep Learning and Generative Adversarial Networks
PY  - 2018
AB  - Eye contact (mutual gaze) is a foundation of human communication and social interactions; therefore, it is studied in many fields such as psychology, social science, and medicine. Our group have been studied wearable vision-based eye contact detection techniques using a first person camera for the purpose of evaluating the gaze skills in the tender dementia care. In this work, we search for deep learning-based eye contact detection techniques from small number of labeled images. We implemented and tested two eye contact detection algorithms: naive deep-learning-based algorithm and generative adversarial networks (GAN)-based semi supervised learning (SSL) algorithm. These methods are learned and verified by using Columbia Gaze Dataset, Facescrub and our original datasets. The results show the effectiveness and limitations of the deep-learning-based and GAN-based approaches. Interestingly, we found the bilateral difference of the accuracy of eye contact detection with respect to the facial pose with respect to the camera, which is expected to be caused by the learning datasets.
SP  - 3927
EP  - 3931
JF  - 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smc.2018.00666
ER  - 

TY  - JOUR
AU  - Kern, Florian; Kullmann, Peter; Ganal, Elisabeth; Korwisi, Kristof; Stingl, René; Niebling, Florian; Latoschik, Marc Erich
TI  - Off-The-Shelf Stylus: Using XR Devices for Handwriting and Sketching on Physically Aligned Virtual Surfaces
PY  - 2021
AB  - This article introduces the Off-The-Shelf Stylus (OTSS), a framework for 2D interaction (in 3D) as well as for handwriting and sketching with digital pen, ink, and paper on physically aligned virtual surfaces in Virtual, Augmented, and Mixed Reality (VR, AR, MR: XR for short). OTSS supports self-made XR styluses based on consumer-grade six-degrees-of-freedom XR controllers and commercially available styluses. The framework provides separate modules for three basic but vital features: (1) The stylus module provides stylus construction and calibration features. (2) The surface module provides surface calibration and visual feedback features for virtual-physical 2D surface alignment using our so-called 3ViSuAl procedure, and surface interaction features. (3) The evaluation suite provides a comprehensive test bed combining technical measurements for precision, accuracy, and latency with extensive usability evaluations including handwriting and sketching tasks based on established visuomotor, graphomotor, and handwriting research. The framework’s development is accompanied by an extensive open source reference implementation targeting the Unity game engine using an Oculus Rift S headset and Oculus Touch controllers. The development compares three low-cost and low-tech options to equip controllers with a tip and includes a web browser-based surface providing support for interacting, handwriting, and sketching. The evaluation of the reference implementation based on the OTSS framework identified an average stylus precision of 0.98 mm (SD = 0.54 mm) and an average surface accuracy of 0.60 mm (SD = 0.32 mm) in a seated VR environment. The time for displaying the stylus movement as digital ink on the web browser surface in VR was 79.40 ms on average (SD = 23.26 ms), including the physical controller’s motion-to-photon latency visualized by its virtual representation (M = 42.57 ms, SD = 15.70 ms). The usability evaluation (N = 10) revealed a low task load, high usability, and high user experience. Participants successfully reproduced given shapes and created legible handwriting, indicating that the OTSS and it’s reference implementation is ready for everyday use. We provide source code access to our implementation, including stylus and surface calibration and surface interaction features, making it easy to reuse, extend, adapt and/or replicate previous results (https://go.uniwue.de/hci-otss).
SP  - 684498
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 2
IS  - NA
PB  - 
DO  - 10.3389/frvir.2021.684498
ER  - 

TY  - NA
AU  - Zhang, Xucong; Sugano, Yusuke; Bulling, Andreas
TI  - Evaluation of Appearance-Based Methods and Implications for Gaze-Based Applications
PY  - 2019
AB  - Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (this http URL), the first software toolkit for appearance-based gaze estimation and interaction.
SP  - 416
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300646
ER  - 

TY  - NA
AU  - Swearngin, Amanda; Wang, Chenglong; Oleson, Alannah; Fogarty, James; Ko, Amy J.
TI  - Scout: Rapid Exploration of Interface Layout Alternatives through High-Level Design Constraints.
PY  - 2020
AB  - Although exploring alternatives is fundamental to creating better interface designs, current processes for creating alternatives are generally manual, limiting the alternatives a designer can explore. We present Scout, a system that helps designers rapidly explore alternatives through mixed-initiative interaction with high-level constraints and design feedback. Prior constraint-based layout systems use low-level spatial constraints and generally produce a single design. Tosupport designer exploration of alternatives, Scout introduces high-level constraints based on design concepts (e.g.,~semantic structure, emphasis, order) and formalizes them into low-level spatial constraints that a solver uses to generate potential layouts. In an evaluation with 18 interface designers, we found that Scout: (1) helps designers create more spatially diverse layouts with similar quality to those created with a baseline tool and (2) can help designers avoid a linear design process and quickly ideate layouts they do not believe they would have thought of on their own.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376593
ER  - 

TY  - JOUR
AU  - Cunningham, James; Shu, Dule; Simpson, Timothy W.; Tucker, Conrad S.
TI  - A sparsity preserving genetic algorithm for extracting diverse functional 3D designs from deep generative neural networks
PY  - 2020
AB  - Generative neural networks (GNNs) have successfully used human-created designs to generate novel 3D models that combine concepts from disparate known solutions, which is an important aspect of design exploration. GNNs automatically learn a parameterization (or latent space) of a design space, as opposed to alternative methods that manually define a parameterization. However, GNNs are typically not evaluated using an explicit notion of physical performance, which is a critical capability needed for design. This work bridges this gap by proposing a method to extract a set of functional designs from the latent space of a point cloud generating GNN, without sacrificing the aforementioned aspects of a GNN that are appealing for design exploration. We introduce a sparsity preserving cost function and initialization strategy for a genetic algorithm (GA) to optimize over the latent space of a point cloud generating autoencoder GNN. We examine two test cases, an example of generating ellipsoid point clouds subject to a simple performance criterion and a more complex example of extracting 3D designs with a low coefficient of drag. Our experiments show that the modified GA results in a diverse set of functionally superior designs while maintaining similarity to human-generated designs in the training data set.
SP  - NA
EP  - NA
JF  - Design Science
VL  - 6
IS  - NA
PB  - 
DO  - 10.1017/dsj.2020.9
ER  - 

TY  - BOOK
AU  - Bowers, Brook; Tuttle, Alexander; Rukangu, Andrew; Franzluebbers, Anton; Ball, Catherine; Johnsen, Kyle
TI  - VR Workshops - Comparing Virtual Constraints and a Physical Stylus for Planar Writing and Drawing in Virtual Reality
PY  - 2021
AB  - Air-drawing, or drawing without the use of a physical surface, is the dominant interaction metaphor for drawing or writing in Virtual Reality (VR). However, we typically use devices that are restricted to two-dimensional planes:mice, pen-and-paper, or the digital equivalent pen-and-tablet devices. We present results from a user study examining differences in performance, user-preference, and handwriting between two implementations of air drawing and passive haptic surface drawing.
SP  - 220
EP  - 225
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00048
ER  - 

TY  - NA
AU  - Sun, Xiaolei; Li, Tongyu; Xu, Jianfeng
TI  - QRS Companion - UI Components Recognition System Based On Image Understanding
PY  - 2020
AB  - Before the release of mobile application products, a lot of repeated testing is often required. In the process of mobile application testing, the core problem is to locate the UI components on the mobile application screenshots. There are many methods to automatically identify UI components, but in some cases, such as crowdsourcing testing, it is difficult to use automatic methods to identify UI components. In view of this, the APP UI components recognition system based on image understanding provides new solutions and methods for application scenarios that are difficult to automatically locate components. We investigate Android UI component information, use image understanding analysis to extract component images on screenshot, design and implement a convolutional neural networks, and then use trained CNN to classify these images. The classification accuracy is up to 96.97%. In the end, we get the component information contained in screenshot.
SP  - 65
EP  - 71
JF  - 2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/qrs-c51114.2020.00022
ER  - 

TY  - JOUR
AU  - Soui, Makram; Haddad, Zainab; Trabelsi, Rim; Srinivasan, Karthik
TI  - Deep features extraction to assess mobile user interfaces
PY  - 2022
AB  - NA
SP  - 12945
EP  - 12960
JF  - Multimedia Tools and Applications
VL  - 81
IS  - 9
PB  - 
DO  - 10.1007/s11042-022-11978-1
ER  - 

TY  - JOUR
AU  - Long, Yonghao; Chen, Xiangping; Xie, Yuting
TI  - A data-driven approach for recommending UI element layout
PY  - 2020
AB  - NA
SP  - 1
EP  - 3
JF  - Science China Information Sciences
VL  - 63
IS  - 9
PB  - 
DO  - 10.1007/s11432-019-2860-3
ER  - 

TY  - JOUR
AU  - Jiang, Tao; Cui, Haihua; Cheng, Xiaosheng; Li, Pengcheng; Tian, Wei
TI  - A ball-shaped target development and pose estimation strategy for a tracking-based scanning system
PY  - 2022
AB  - NA
SP  - 1
EP  - 1
JF  - IEEE Transactions on Instrumentation and Measurement
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/tim.2022.3205675
ER  - 

TY  - NA
AU  - Shrestha, Nischal; Barik, Titus; Parnin, Chris
TI  - VL/HCC - It's Like Python But: Towards Supporting Transfer of Programming Language Knowledge
PY  - 2018
AB  - Expertise in programming traditionally assumes a binary novice-expert divide. Learning resources typically target programmers who are learning programming for the first time, or expert programmers for that language. An underrepresented, yet important group of programmers are those that are experienced in one programming language, but desire to author code in a different language. For this scenario, we postulate that an effective form of feedback is presented as a transfer from concepts in the first language to the second. Current programming environments do not support this form of feedback. In this study, we apply the theory of learning transfer to teach a language that programmers are less familiar with-such as R-in terms of a programming language they already know-such as Python. We investigate learning transfer using a new tool called Transfer Tutor that presents explanations for R code in terms of the equivalent Python code. Our study found that participants leveraged learning transfer as a cognitive strategy, even when unprompted. Participants found Transfer Tutor to be useful across a number of affordances like stepping through and highlighting facts that may have been missed or misunderstood. However, participants were reluctant to accept facts without code execution or sometimes had difficulty reading explanations that are verbose or complex. These results provide guidance for future designs and research directions that can support learning transfer when learning new programming languages.
SP  - 177
EP  - 185
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506508
ER  - 

TY  - JOUR
AU  - Ross, Anne Spencer; Zhang, Xiaoyi; Fogarty, James; Wobbrock, Jacob O.
TI  - An Epidemiology-inspired Large-scale Analysis of Android App Accessibility
PY  - 2020
AB  - Accessibility barriers in mobile applications (apps) can make it challenging for people who have impairments or use assistive technology to use those apps. Ross et al.’s epidemiology-inspired framework emphasizes that a wide variety of factors may influence an app's accessibility and presents large-scale analysis as a powerful tool for understanding the prevalence of accessibility barriers (i.e., inaccessibility diseases). Drawing on this framework, we performed a large-scale analysis of free Android apps, exploring the frequency of accessibility barriers and factors that may have contributed to barrier prevalence. We tested a population of 9,999 apps for seven accessibility barriers: few TalkBack-focusable elements, missing labels, duplicate labels, uninformative labels, editable TextViews with contentDescription, fully overlapping clickable elements, and undersized elements. We began by measuring the prevalence of each accessibility barrier across all relevant element classes and apps. Missing labels and undersized elements were the most prevalent barriers. As a measure of the spread of barriers across apps, we assessed the five most reused classes of elements for missing labels and undersized elements. The Image Button class was among the most barrier-prone of the high reuse element classes; 53% of Image Button elements were missing labels and 40% were undersized. We also investigated factors that may have contributed to the high barrier prevalence in certain classes of elements, selecting examples based on prior knowledge, our analyses, and metrics of reuse and barrier-proneness. These case studies explore: (1) how the few TalkBack-focusable elements accessibility barrier relates to app category (e.g., Education, Entertainment) and the tools used to implement an app, (2) the prevalence of label-based barriers in image-based buttons, (3) design patterns that affect the labeling and size of Radio Buttons and Checkboxes, and (4) accessibility implications of the sizing of third-party plug-in elements. Our work characterizes the current state of Android accessibility, suggests improvements to the app ecosystem, and demonstrates analysis techniques that can be applied in further app accessibility assessments.
SP  - 1
EP  - 36
JF  - ACM Transactions on Accessible Computing
VL  - 13
IS  - 1
PB  - 
DO  - 10.1145/3348797
ER  - 

TY  - JOUR
AU  - Watanabe, Ko; Soneda, Yusuke; Matsuda, Yuki; Nakamura, Yugo; Arakawa, Yutaka; Dengel, Andreas; Ishimaru, Shoya
TI  - DisCaaS: Micro Behavior Analysis on Discussion by Camera as a Sensor.
PY  - 2021
AB  - The emergence of various types of commercial cameras (compact, high resolution, high angle of view, high speed, and high dynamic range, etc.) has contributed significantly to the understanding of human activities. By taking advantage of the characteristic of a high angle of view, this paper demonstrates a system that recognizes micro-behaviors and a small group discussion with a single 360 degree camera towards quantified meeting analysis. We propose a method that recognizes speaking and nodding, which have often been overlooked in existing research, from a video stream of face images and a random forest classifier. The proposed approach was evaluated on our three datasets. In order to create the first and the second datasets, we asked participants to meet physically: 16 sets of five minutes data from 21 unique participants and seven sets of 10 min meeting data from 12 unique participants. The experimental results showed that our approach could detect speaking and nodding with a macro average f1-score of 67.9% in a 10-fold random split cross-validation and a macro average f1-score of 62.5% in a leave-one-participant-out cross-validation. By considering the increased demand for an online meeting due to the COVID-19 pandemic, we also record faces on a screen that are captured by web cameras as the third dataset and discussed the potential and challenges of applying our ideas to virtual video conferences.
SP  - 5719
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 21
IS  - 17
PB  - 
DO  - 10.3390/s21175719
ER  - 

TY  - CONF
AU  - Sain, Aneeshan; Bhunia, Ayan Kumar; Yang, Yongxin; Xiang, Tao; Song, Yi-Zhe
TI  - BMVC - Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval
PY  - 2020
AB  - Sketch as an image search query is an ideal alternative to text in capturing the finegrained visual details. Prior successes on fine-grained sketch-based image retrieval (FGSBIR) have demonstrated the importance of tackling the unique traits of sketches as opposed to photos, e.g., temporal vs. static, strokes vs. pixels, and abstract vs. pixelperfect. In this paper, we study a further trait of sketches that has been overlooked to date, that is, they are hierarchical in terms of the levels of detail – a person typically sketches up to various extents of detail to depict an object. This hierarchical structure is often visually distinct. In this paper, we design a novel network that is capable of cultivating sketch-specific hierarchies and exploiting them to match sketch with photo at corresponding hierarchical levels. In particular, features from a sketch and a photo are enriched using cross-modal co-attention, coupled with hierarchical node fusion at every level to form a better embedding space to conduct retrieval. Experiments on common benchmarks show our method to outperform state-of-the-arts by a significant margin.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Lee, Bokyung; Jin, Taeil; Lee, Sung-Hee; Saakes, Daniel
TI  - CHI - SmartManikin: Virtual Humans with Agency for Design Tools
PY  - 2019
AB  - When designing comfort and usability in products, designers need to evaluate aspects ranging from anthropometrics to use scenarios. Therefore, virtual and poseable mannequins are employed as a reference in early-stage tools and for evaluation in the later stages. However, tools to intuitively interact with virtual humans are lacking. In this paper, we introduceSmartManikin, a mannequin with agency that responds to high-level commands and to real-time design changes. We first captured human poses with respect to desk configurations, identified key features of the pose and trained regression functions to estimate the optimal features at a given desk setup. The SmartManikin's pose is generated by the predicted features as well as by using forward and inverse kinematics. We present our design, implementation, and an evaluation with expert designers. The results revealed that SmartManikin enhances the design experience by providing feedback concerning comfort and health in real time.
SP  - 584
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300814
ER  - 

TY  - NA
AU  - Feiz, Shirin; Wu, Jason; Zhang, Xiaoyi; Swearngin, Amanda; Barik, Titus; Nichols, Jeffrey
TI  - Understanding Screen Relationships from Screenshots of Smartphone Applications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 27th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490099.3511109
ER  - 

TY  - NA
AU  - Zhang, Xiaoyi; de Greef, Lilian; Swearngin, Amanda; White, Samuel; Murray, Kyle I.; Yu, Lisa; Shan, Qi; Nichols, Jeffrey; Wu, Jason; Fleizach, Chris; Everitt, Aaron; Bigham, Jeffrey P.
TI  - Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels
PY  - 2021
AB  - Many accessibility features available on mobile platforms require applications (apps) to provide complete and accurate metadata describing user interface (UI) components. Unfortunately, many apps do not provide sufficient metadata for accessibility features to work as expected. In this paper, we explore inferring accessibility metadata for mobile apps from their pixels, as the visual interfaces often best reflect an app's full functionality. We trained a robust, fast, memory-efficient, on-device model to detect UI elements using a dataset of 77,637 screens (from 4,068 iPhone apps) that we collected and annotated. To further improve UI detections and add semantic information, we introduced heuristics (e.g., UI grouping and ordering) and additional models (e.g., recognize UI content, state, interactivity). We built Screen Recognition to generate accessibility metadata to augment iOS VoiceOver. In a study with 9 screen reader users, we validated that our approach improves the accessibility of existing mobile apps, enabling even previously inaccessible apps to be used.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Li, Jiahao; Cui, Meilin; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities
PY  - 2020
AB  - Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kim, Jeeeun; Zhou, Qingnan; Ghassaei, Amanda; Chen, Xiang 'Anthony'
TI  - TEI - OmniSoft: A Design Tool for Soft Objects by Example
PY  - 2021
AB  - Softness is one of the most important factors in human tactile perception. With recent advances in 3Dprinting, there has been significant progress in fabricating compliant objects. However, existing methods typically remain inaccessible to end-users, mainly due to the separation between designing shapes and setting printing parameters to achieve desired softness, resulting in the exclusion of its customization in early design processes. In this work, we contribute an end-to-end design tool that takes a design-by-example approach: given a 3D model, a user can specify the region of interest and a level of softness, by shopping everyday objects as a reference. The tool then generates both geometry and 3D printing parameters to reproduce the desired softness, which can be fabricated using low-cost FDM 3D printing and materials for it. We also provide a data-driven pipeline to enable other compliance modeling methods to be generalized within our design tool. In two user studies, we demonstrated that users could easily locate existing reference objects’ softness to a 3D printed object. In a design session, end-users successfully used OmniSoft to design augmented functions.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440634
ER  - 

TY  - NA
AU  - Asai, Kentaro; Fukusato, Tsukasa; Igarashi, Takeo
TI  - CHI - Integrated Development Environment with Interactive Scatter Plot for Examining Statistical Modeling
PY  - 2020
AB  - The development of a statistical modeling program requires example data to observe and verify the behavior of the program. Such example data are either taken from an existing dataset or synthesized using commands. Programmers may want to directly design an arbitrary dataset or modify it interactively, but it is difficult to do so in current development environments. We therefore propose combining a code editor with an interactive scatter plot editor to efficiently understand the behavior of statistical modeling algorithms. The user interactively creates and modifies the dataset on the scatter plot editor, while the system continuously executes the code in the editor, taking the data as input, and shows the result in the editor. This paper presents the design rationale behind the system and introduces several usage scenarios.
SP  - 1
EP  - 7
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376455
ER  - 

TY  - CHAP
AU  - Kolthoff, Kristian; Bartelt, Christian; Ponzetto, Simone Paolo
TI  - NLDB - Automated Retrieval of Graphical User Interface Prototypes from Natural Language Requirements
PY  - 2021
AB  - High-fidelity Graphical User Interface (GUI) prototyping represents a suitable approach for allowing to clarify and refine requirements elicitated from customers. In particular, GUI prototypes can facilitate to mitigate and reduce misunderstandings between customers and developers, which may occur due to the ambiguity and vagueness of informal Natural Language (NL). However, employing high-fidelity GUI prototypes is more time-consuming and expensive compared to other simpler GUI prototyping methods. In this work, we propose a system that automatically processes Natural Language Requirements (NLR) and retrieves fitting GUI prototypes from a semi-automatically created large-scale GUI repository for mobile applications. We extract several text segments from the GUI hierarchy data to obtain textual representations for the GUIs. To achieve ad-hoc GUI retrieval from NLR, we adopt multiple Information Retrieval (IR) approaches and Automatic Query Expansion (AQE) techniques. We provide an extensive and systematic evaluation of the applied IR and AQE approaches for their effectiveness in terms of GUI retrieval relevance on a manually annotated dataset of NLR in the form of search queries and User Stories (US). We found that our GUI retrieval performs well in the conducted experiments and discuss the results.
SP  - 376
EP  - 384
JF  - Natural Language Processing and Information Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-80599-9_33
ER  - 

TY  - NA
AU  - Wacker, Philipp; Nowak, Oliver; Voelker, Simon; Borchers, Jan
TI  - CHI - ARPen: Mid-Air Object Manipulation Techniques for a Bimanual AR System with Pen & Smartphone
PY  - 2019
AB  - Modeling in Augmented Reality (AR) lets users create and manipulate virtual objects in mid-air that are aligned to their real environment. We present ARPen, a bimanual input technique for AR modeling that combines a standard smartphone with a 3D-printed pen. Users sketch with the pen in mid-air, while holding their smartphone in the other hand to see the virtual pen traces in the live camera image. ARPen combines the pen's higher 3D input precision with the rich interactive capabilities of the smartphone touchscreen. We studied subjective preferences for this bimanual input technique, such as how people hold the smartphone while drawing, and analyzed the performance of different bimanual techniques for selecting and moving virtual objects. Users preferred a bimanual technique casting a ray through the pen tip for both selection and translation. We provide initial design guidelines for this new class of bimanual AR modeling systems.
SP  - 619
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300849
ER  - 

TY  - NA
AU  - Mohian, Soumik; Csallner, Christoph
TI  - PSDoodle
PY  - 2022
AB  - Keyword-based mobile screen search does not account for screen content and fails to operate as a universal tool for all levels of users. Visual searching (e.g., image, sketch) is structured and easy to adopt. Current visual search approaches count on a complete screen and are therefore slow and tedious. PSDoodle employs a deep neural network to recognize partial screen element drawings instantly on a digital drawing interface and shows results in real-time. PSDoodle is the first tool that utilizes partial sketches and searches for screens in an interactive iterative way. PSDoodle supports different drawing styles and retrieves search results that are relevant to the user's sketch query. A short video demonstration is available online at: https://youtu.be/3cVLHFm5pY4
SP  - NA
EP  - NA
JF  - Proceedings of the 9th IEEE/ACM International Conference on Mobile Software Engineering and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3524613.3527807
ER  - 

TY  - NA
AU  - Qodseya, Mahmoud; Panta, Franck Jeveme; Sèdes, Florence
TI  - CBMI - Visual-Based Eye Contact Detection in Multi-Person Interactions
PY  - 2019
AB  - Visual non-verbal behavior analysis (VNBA) methods mainly depend on extracting an important and essential social cue, called eye contact, for performing a wide range of analysis such as dominant person detection. Besides the major need for an automated eye-contact detection method, existing state-of-the-art methods require intrusive devices for detecting any contacts at the eye-level. Also, such methods are completely dependent on supervised learning approaches to produce eye-contact classification models, raising the need for ground truth datasets. To overcome the limitations of existing techniques, we propose a novel geometrical method to detect eye contact in natural multi-person interactions without the need of any intrusive eye-tracking device. We have experimented our method on 10 social videos, each 20 minutes long. Experiments demonstrate highly competitive efficiency with regards to classification performance, compared to the classical existing supervised eye contact detection methods.
SP  - 1
EP  - 6
JF  - 2019 International Conference on Content-Based Multimedia Indexing (CBMI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cbmi.2019.8877471
ER  - 

TY  - NA
AU  - Savic, Marko; Mantyla, Mika; Claes, Maelick
TI  - Win GUI Crawler: A tool prototype for desktop GUI image and metadata collection
PY  - 2022
AB  - Despite the widespread of test automation, automatic testing of graphical user interfaces (GUI) remains a challenge. This is partly due to the difficulty of reliably identifying GUI elements over different versions of a given software system. Machine vision techniques could be a potential way of addressing this issue by automatically identifying GUI elements with the help of machine learning. However, developing a GUI testing tool relying on automatic identification of graphical elements first requires to acquire large amount of labeled data. In this paper, we present Win GUI Crawler, a tool for automatically gathering such data from Microsoft Windows GUI applications. The tool is based on Microsoft Windows Application Driver and performs actions on the GUI using a depth-first traversal of the GUI element tree. For each action performed by the crawler, screenshots are taken and metadata is extracted for each of the different screens. Bounding boxes of GUI elements are then filtered in order to identify what GUI elements are actually visible on the screen. Win GUI Crawler is then evaluated on several popular Windows applications and the current limitations are discussed.
SP  - NA
EP  - NA
JF  - 2022 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icstw55395.2022.00046
ER  - 

TY  - NA
AU  - Bunian, Sara; Li, Kai; Jemmali, Chaima; Harteveld, Casper; Fu, Yun; El-Nasr, Magy Seif
TI  - CHI - VINS: Visual Search for Mobile User Interface Design
PY  - 2021
AB  - Searching for relative mobile user interface (UI) design examples can aid interface designers in gaining inspiration and comparing design alternatives. However, finding such design examples is challenging, especially as current search systems rely on only text-based queries and do not consider the UI structure and content into account. This paper introduces VINS, a visual search framework, that takes as input a UI image (wireframe, high-fidelity) and retrieves visually similar design examples. We first survey interface designers to better understand their example finding process. We then develop a large-scale UI dataset that provides an accurate specification of the interface’s view hierarchy (i.e., all the UI components and their specific location). By utilizing this dataset, we propose an object-detection based image retrieval framework that models the UI context and hierarchical structure. The framework achieves a mean Average Precision of 76.39% for the UI detection and high performance in querying similar UI designs.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445762
ER  - 

TY  - NA
AU  - Pandian, Vinoth Pandian Sermuga; Suleri, Sarah; Jarke, Matthias
TI  - IUI Companion - SynZ: Enhanced Synthetic Dataset for Training UI Element Detectors
PY  - 2021
AB  - User Interface (UI) prototyping is an iterative process where designers initially sketch UIs before transforming them into interactive digital designs. Recent research applies Deep Neural Networks (DNNs) to identify the constituent UI elements of these UI sketches and transform these sketches into front-end code. Training such DNN models requires a large-scale dataset of UI sketches, which is time-consuming and expensive to collect. Therefore, we earlier proposed Syn to generate UI sketches synthetically by random allocation of UI element sketches. However, these UI sketches are not statistically similar to real-life UI screens. To bridge this gap, in this paper, we introduce the SynZ dataset, which contains 175,377 synthetically generated UI sketches statistically similar to real-life UI screens. To generate SynZ, we analyzed, enhanced, and extracted annotations from the RICO dataset and used 17,979 hand-drawn UI element sketches from the UISketch dataset. Further, we fine-tuned a UI element detector with SynZ and observed that it doubles the mean Average Precision of UI element detection compared to the Syn dataset.
SP  - 67
EP  - 69
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397482.3450725
ER  - 

TY  - NA
AU  - Todi, Kashyap; Leiva, Luis A.; Buschek, Daniel; Tian, Pin; Oulasvirta, Antti
TI  - Conference on Designing Interactive Systems - Conversations with GUIs
PY  - 2021
AB  - Annotated datasets of application GUIs contain a wealth of information that can be used for various purposes, from providing inspiration to designers and implementation details to developers to assisting end-users during daily use. However, users often struggle to formulate their needs in a way that computers can understand reliably. To address this, we study how people may interact with such GUI datasets using natural language. We elicit user needs in a survey (N = 120) with three target groups (designers, developers, end-users), providing insights into which capabilities would be useful and how users formulate queries. We contribute a labelled dataset of 1317 user queries, and demonstrate an application of a conversational assistant that interprets these queries and retrieves information from a large-scale GUI dataset. It can (1) suggest GUI screenshots for design ideation, (2) highlight details about particular GUI features for development, and (3) reveal further insights about applications. Our findings can inform design and implementation of intelligent systems to interact with GUI datasets intuitively.
SP  - 1447
EP  - 1457
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462124
ER  - 

TY  - CHAP
AU  - Zhang, Hongkun; Li, Qian; Zhu, Lifeng
TI  - ImageCube: A Low-Cost 6D Controller with Smooth Tracking for Mobile Augmented Reality
PY  - 2021
AB  - With the development of mobile computing powers, augmented reality has reached to common users. The users manipulate virtual contents with the help of tracking techniques. While image markers are affordable for common mobile users, the tracking space is limited due to occlusion. To better democratize augmented reality, we propose ImageCube, a simple and effective solution to enlarge the tracking space of image markers for mobile augmented reality. We design a 3D model and combine image tracking techniques to obtain full 6D motion tracking. We also propose a weighting scheme for smoothly tracking the controller. The controller is inexpensive, stable and easy to deploy. We show some applications to demonstrate the efficiency of the technique.
SP  - 233
EP  - 243
JF  - 3D Imaging Technologies—Multi-dimensional Signal Processing and Deep Learning
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-16-3391-1_26
ER  - 

TY  - NA
AU  - Sereshkeh, Alborz Rezazadeh; Leung, Gary; Perumal, Krish; Phillips, Caleb; Zhang, Minfan; Fazly, Afsaneh; Mohomed, Iqbal
TI  - IUI - VASTA: a vision and language-assisted smartphone task automation system
PY  - 2020
AB  - We present VASTA, a novel vision and language-assisted Programming By Demonstration (PBD) system for smartphone task automation. Development of a robust PBD automation system requires overcoming three key challenges: first, how to make a particular demonstration robust to positional and visual changes in the user interface (UI) elements; secondly, how to recognize changes in the automation parameters to make the demonstration as generalizable as possible; and thirdly, how to recognize from the user utterance what automation the user wishes to carry out. To address the first challenge, VASTA leverages state-of-the-art computer vision techniques, including object detection and optical character recognition, to accurately label interactions demonstrated by a user, without relying on the underlying UI structures. To address the second and third challenges, VASTA takes advantage of advanced natural language understanding algorithms for analyzing the user utterance to trigger the VASTA automation scripts, and to determine the automation parameters for generalization. We run an initial user study that demonstrates the effectiveness of VASTA at clustering user utterances, understanding changes in the automation parameters, detecting desired UI elements, and, most importantly, automating various tasks. A demo video of the system is available here: http://y2u.be/kr2xE-FixjI.
SP  - 22
EP  - 32
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377515
ER  - 

TY  - NA
AU  - Ye, Jiaming; Chen, Ke; Xie, Xiaofei; Ma, Lei; Huang, Ruochen; Chen, Yingfeng; Xue, Yinxing; Zhao, Jianjun
TI  - ESEC/SIGSOFT FSE - An empirical study of GUI widget detection for industrial mobile games
PY  - 2021
AB  - With the widespread adoption of smartphones in our daily life, mobile games experienced increasing demand over the past years. Meanwhile, the quality of mobile games has been continuously drawing more and more attention, which can greatly affect the player experience. For better quality assurance, general-purpose testing has been extensively studied for mobile apps. However, due to the unique characteristic of mobile games, existing mobile testing techniques may not be directly suitable and applicable. To better understand the challenges in mobile game testing, in this paper, we first initiate an early step to conduct an empirical study towards understanding the challenges and pain points of mobile game testing process at our industrial partner NetEase Games. Specifically, we first conduct a survey from the mobile test development team at NetEase Games via both scrum interviews and questionnaires. We found that accurate and effective GUI widget detection for mobile games could be the pillar to boost the automation of mobile game testing and other downstream analysis tasks in practice. We then continue to perform comparative studies to investigate the effectiveness of state-of-the-art general-purpose mobile app GUI widget detection methods in the context of mobile games. To this end, we also develop a technique to automatically collect GUI widgets region information of industrial mobile games, which is equipped with a heuristic-based data cleaning method for quality refinement of the labeling results. Our evaluation shows that: (1) Existing GUI widget detection methods for general-purpose mobile apps cannot perform well on industrial mobile games. (2) Mobile game exhibits obvious difference from other general-purpose mobile apps in the perspective GUI widgets. Our further in-depth analysis reveals high diversity and density characteristics of mobile game GUI widgets could be the major reasons that post the challenges for existing methods, which calls for new research methods and better industry practices. To enable further research along this line, we construct the very first GUI widget detection benchmark, specially designed for mobile games, incorporating both our collected dataset and the state-of-the-art widget detection methods for mobile apps, which could also be the basis for further study of many downstream quality assurance tasks (e.g., testing and analysis) for mobile games.
SP  - 1427
EP  - 1437
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3473935
ER  - 

TY  - NA
AU  - Yamaji, Daiki; Hakoda, Hiroyuki; Wataru, Yamada; Manabe, Hiroyuki
TI  - CHI Extended Abstracts - A Low-Cost Tracking Technique Using Retro-Reflective Marker for Smartphone Based HMD
PY  - 2018
AB  - We present a low-cost tracking technique for mobile virtual reality (VR) head-mounted displays (HMDs) that uses retro-reflective markers.Our technique allows the built-in cameras of existing smartphones to track real objects even if they are moving rapidly. The user simply attaches retro-reflective markers to the objects or creates unique objects (easily fabricated by 3D printing) and uses them as input devices for smartphone-based HMD applications. Several VR game applications that use our technique are described and testing shows that our technique works effectively.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3170427.3188516
ER  - 

TY  - NA
AU  - Ju, Ran; Zhou, Xingchen; Bo, Xu; Liang, Weiqing; Yang, Wanyi; Yuan, Cao; Zhang, Eryan; Li, Ronggen; Yinghao, Li; Ning, Ding; Li, Li; Ru, Zhang; Liu, Dongliang
TI  - IUI Companion - DUES-Adapt: Exploring Distributed User Experience With Neural UI Adaptation
PY  - 2020
AB  - Developers spend a great deal of time to adapt UI to different devices. By learning experience from massive number of human designed UI products, the adaptation work could be finished by machines. To this end, we introduce DUES-Adapt, an AI based UI adaptation system, and showcase in this demonstration. Given an input UI, DUES-Adapt parses the basic UI elements and employs the parsing results to generate a reasonable and aesthetic layout for a target device. The two AI problems, UI parsing and layout generation, are solved using deep neural network model and trained with over 10K app instances collected from mainstream Android markets. In the demonstration, we show a number of cases covering many apps like music, maps, fitness and different target terminals such as tablet, smartwatch, TV etc.
SP  - 91
EP  - 92
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379336.3381464
ER  - 

TY  - JOUR
AU  - Nathasya, Rossevine Artha; Karnalim, Oscar
TI  - The Use of Color Gradation on Program Visualization for Learning Programming
PY  - 2019
AB  - According to several works, Program Visualization (PV) enhances student understanding further about how a particular program works. However, to our knowledge, no PVs utilize color gradation as a part of their features, even though color plays an important role in visualization. Therefore, two uses of color gradation on PV are proposed on this paper. On the one hand, color gradation can be used to display execution frequency of each instruction; instruction with higher execution frequency will be assigned with more-prominent color. Such piece of information is expected to help student for understanding program complexity. On the other hand, color gradation can also be used to display access frequency of each variable; variable with higher access frequency will be assigned with more-prominent color. Such piece of information is expected to help student for understanding program-to-variable dependency. Both uses are proved to be effective for learning programming according to our evaluation. Index Terms—program visualization, color gradation, program complexity, program-to-variable dependency, computer science education
SP  - 17
EP  - 23
JF  - IJNMT (International Journal of New Media Technology)
VL  - 6
IS  - 1
PB  - 
DO  - 10.31937/ijnmt.v6i1.1040
ER  - 

TY  - NA
AU  - Su, Yuhui; Liu, Zhe; Chen, Chunyang; Wang, Junjie; Wang, Qing
TI  - ESEC/SIGSOFT FSE - OwlEyes-online: a fully automated platform for detecting and localizing UI display issues
PY  - 2021
AB  - Graphical User Interface (GUI) provides visual bridges between software apps and end users. However, due to the compatibility of software or hardware, UI display issues such as text overlap, blurred screen, image missing always occur during GUI rendering on different devices. Because these UI display issues can be found directly by human eyes, in this paper, we implement an online UI display issue detection tool OwlEyes-Online, which provides a simple and easy-to-use platform for users to realize the automatic detection and localization of UI display issues. The OwlEyes-Online can automatically run the app and get its screenshots and XML files, and then detect the existence of issues by analyzing the screenshots. In addition, OwlEyes-Online can also find the detailed area of the issue in the given screenshots to further remind developers. Finally, OwlEyes-Online will automatically generate test reports with UI display issues detected in app screenshots and send them to users. The OwlEyes-Online was evaluated and proved to be able to accurately detect UI display issues. Tool Link: http://www.owleyes.online:7476 Github Link: https://github.com/franklinbill/owleyes Demo Video Link: https://youtu.be/002nHZBxtCY
SP  - 1500
EP  - 1504
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3473109
ER  - 

TY  - NA
AU  - Sueishi, Tomohiro; Ishikawa, Masatoshi
TI  - Ellipses Ring Marker for High-speed Finger Tracking
PY  - 2021
AB  - High-speed finger tracking is necessary for augmented reality and operation in human-machine cooperation without latency discomfort, but conventional markerless finger tracking methods are not fast enough and the marker-based methods have low wearability. In this paper, we propose an ellipses ring marker (ERM), a finger-ring marker consisting of multiple ellipses and its high-speed image recognition algorithm. The finger-ring shape has highly wearing continuity, and the surface shape is suitable for various viewing angle observation. The invariance of the ellipse in the perspective projection enables accurate and low-latency posture estimation. We have experimentally investigated the advantage in normal distribution, validated the sufficient accuracy and computational cost in the marker tracking, and showed a demonstration of dynamic projection mapping on a palm.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489856
ER  - 

TY  - NA
AU  - Okuyama, Mizuki; Matoba, Yasushi; Siio, Itiro
TI  - IUI Companion - Cylindrical M-sequence Markers and its Application to AR Fitting System for Kimono Obi
PY  - 2018
AB  - This paper proposes an m-sequence cylindrical marker, which is an optical barcode marker that can be applied on cylindrical objects, such as parts of a human body and everyday things such as furniture, bottles, and cups. We use two cycles of an m-sequence barcode to maintain continuity at the joint of the cylindrical marker. By using the m-sequence characteristics, the marker is able to acquire the rotation angle by recognizing only a certain part of the barcode. To confirm feasibility, we have made a cylindrical marker that fits on the waist of a human, and have implemented an AR fitting application for kimono obi.
SP  - 6
EP  - NA
JF  - Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3180308.3180314
ER  - 

TY  - JOUR
AU  - Serra, Gerard; Miralles, David
TI  - Human-level design proposals by an artificial agent in multiple scenarios
PY  - 2021
AB  - NA
SP  - 101029
EP  - NA
JF  - Design Studies
VL  - 76
IS  - NA
PB  - 
DO  - 10.1016/j.destud.2021.101029
ER  - 

TY  - JOUR
AU  - Zoss, Gaspard; Bradley, Derek; Bérard, Pascal; Beeler, Thabo
TI  - An empirical rig for jaw animation
PY  - 2018
AB  - In computer graphics the motion of the jaw is commonly modelled by up-down and left-right rotation around a fixed pivot plus a forward-backward translation, yielding a three dimensional rig that is highly suited for intuitive artistic control. The anatomical motion of the jaw is, however, much more complex since the joints that connect the jaw to the skull exhibit both rotational and translational components. In reality the jaw does not move in a three dimensional subspace but on a constrained manifold in six dimensions. We analyze this manifold in the context of computer animation and show how the manifold can be parameterized with three degrees of freedom, providing a novel jaw rig that preserves the intuitive control while providing more accurate jaw positioning. The chosen parameterization furthermore places anatomically correct limits on the motion, preventing the rig from entering physiologically infeasible poses. Our new jaw rig is empirically designed from accurate capture data, and we provide a simple method to retarget the rig to new characters, both human and fantasy.
SP  - 59
EP  - 12
JF  - ACM Transactions on Graphics
VL  - 37
IS  - 4
PB  - 
DO  - 10.1145/3197517.3201382
ER  - 

TY  - CONF
AU  - Laich, Larissa; Bielik, Pavol; Vechev, Martin
TI  - ICLR - Guiding Program Synthesis by Learning to Generate Examples
PY  - 2020
AB  - A key challenge of existing program synthesizers is ensuring that the synthesized program generalizes well. This can be difficult to achieve as the specification provided by the end user is often limited, containing as few as one or two input-output examples. In this paper we address this challenge via an iterative approach that finds ambiguities in the provided specification and learns to resolve these by generating additional input-output examples. The main insight is to reduce the problem of selecting which program generalizes well to the simpler task of deciding which output is correct. As a result, to train our probabilistic models, we can take advantage of the large amounts of data in the form of program outputs, which are often much easier to obtain than the corresponding ground-truth programs.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gupta, Ojaswi; Hebbalaguppe, Ramya
TI  - SIGGRAPH ASIA Posters - FingertipCubes: an inexpensive D.I.Y wearable for 6-DoF per fingertip pose estimation using a single RGB camera
PY  - 2018
AB  - It is natural to use our hands for interacting with a virtual world, but this remains to be widely available. The Leap Motion controller has brought 3D hand tracking to consumers, but its high cost prohibits its mass adoption, especially for users in developing countries. To facilitate mass adoption, we present a do-it-yourself wearable that has a material cost of only 1 US Dollar, and which coupled with a webcam, can provide 6-DoF(degrees of freedom) per fingertip tracking in real-time. We also propose a novel solution to the pose ambiguity problem of a single square planar fiducial marker in monocular view.
SP  - 20
EP  - NA
JF  - SIGGRAPH Asia 2018 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3283289.3283349
ER  - 

TY  - NA
AU  - Cooper, Nathan; Bernal-Cardenas, Carlos; Chaparro, Oscar; Moran, Kevin; Poshyvanyk, Denys
TI  - It Takes Two to Tango: Combining Visual and Textual Information for Detecting Duplicate Video-Based Bug Reports.
PY  - 2021
AB  - When a bug manifests in a user-facing application, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such information is reported en masse, such as during crowd-sourced testing, managing these artifacts can be a time-consuming process. As the reporting of screen-recordings in particular becomes more popular, developers are likely to face challenges related to manually identifying videos that depict duplicate bugs. Due to their graphical nature, screen-recordings present challenges for automated analysis that preclude the use of current duplicate bug report detection techniques. To overcome these challenges and aid developers in this task, this paper presents Tango, a duplicate detection technique that operates purely on video-based bug reports by leveraging both visual and textual information. Tango combines tailored computer vision techniques, optical character recognition, and text retrieval. We evaluated multiple configurations of Tango in a comprehensive empirical evaluation on 4,860 duplicate detection tasks that involved a total of 180 screen-recordings from six Android apps. Additionally, we conducted a user study investigating the effort required for developers to manually detect duplicate video-based bug reports and compared this to the effort required to use Tango. The results reveal that Tango's optimal configuration is highly effective at detecting duplicate video-based bug reports, accurately ranking target duplicate videos in the top-2 returned results in 83% of the tasks. Additionally, our user study shows that, on average, Tango can reduce developer effort by over 60%, illustrating its practicality.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Müller, Philipp; Dietz, Michael; Schiller, Dominik; Thomas, Dominike; Zhang, Guanhua; Gebhard, Patrick; André, Elisabeth; Bulling, Andreas
TI  - ACM Multimedia - MultiMediate: Multi-modal Group Behaviour Analysis for Artificial Mediation
PY  - 2021
AB  - Artificial mediators are promising to support human group conversations but at present their abilities are limited by insufficient progress in group behaviour analysis. The MultiMediate challenge addresses, for the first time, two fundamental group behaviour analysis tasks in well-defined conditions: eye contact detection and next speaker prediction. For training and evaluation, MultiMediate makes use of the MPIIGroup Interaction dataset consisting of 22 three- to four-person discussions as well as of an unpublished test set of six additional discussions. This paper describes the MultiMediate challenge and presents the challenge dataset including novel fine-grained speaking annotations that were collected for the purpose of MultiMediate. Furthermore, we present baseline approaches and ablation studies for both challenge tasks
SP  - 4878
EP  - 4882
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3479219
ER  - 

TY  - NA
AU  - Ikarashi, Yuka; Ragan-Kelley, Jonathan; Fukusato, Tsukasa; Kato, Jun; Igarashi, Takeo
TI  - Guided Optimization for Image Processing Pipelines
PY  - 2021
AB  - Writing high-performance image processing code is challenging and labor-intensive. The Halide programming language simplifies this task by decoupling high-level algorithms from "schedules" which optimize their implementation. However, even with this abstraction, it is still challenging for Halide programmers to understand complicated scheduling strategies and productively write valid, optimized schedules. To address this, we propose a programming support method called "guided optimization." Guided optimization provides programmers a set of valid optimization options and interactive feedback about their current choices, which enables them to comprehend and efficiently optimize image processing code without the time-consuming trial-and-error process of traditional text editors. We implemented a proof-of-concept system, Roly-poly, which integrates guided optimization, program visualization, and schedule cost estimation to support the comprehension and development of efficient Halide image processing code. We conducted a user study with novice Halide programmers and confirmed that Roly-poly and its guided optimization was informative, increased productivity, and resulted in higher-performing schedules in less time.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Pandian, Vinoth Pandian Sermuga; Suleri, Sarah; Beecks, Christian; Jarke, Matthias
TI  - OZCHI - MetaMorph: AI Assistance to Transform Lo-Fi Sketches to Higher Fidelities
PY  - 2020
AB  - Transforming lo-fi UI sketches to higher-fidelities is an expensive, time-consuming process that requires significant rework. In this paper, we systematically research utilizing AI to assist the transformation of lo-fi sketches to higher fidelities. To provide this assistance, we introduce MetaMorph, an AI tool to detect the constituent UI elements of lo-fi sketches. To train MetaMorph, we collected the UISketch dataset that contains 6,785 hand-drawn sketches of 21 UI elements, 201 hand-drawn lo-fi sketches, and 125,000 synthetically generated lo-fi sketches. MetaMorph provides 63.5% mAP for hand-drawn lo-fi sketches and 82.9% mAP for synthetic lo-fi sketches. Results from ASQ indicate that designers experience an above-average satisfaction level towards ease of task completion (4.9), time taken (5.3), and supporting information (5.3) upon utilizing AI assistance for transforming lo-fi sketches. Their qualitative feedback indicates that they perceive utilizing AI as a novel and useful approach to transform lo-fi sketches into higher fidelities.
SP  - 403
EP  - 412
JF  - 32nd Australian Conference on Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441000.3441030
ER  - 

TY  - NA
AU  - Zhao, Tianming; Chen, Chunyang; Liu, Yuanning; Zhu, Xiaodong
TI  - GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks
PY  - 2021
AB  - Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our GUIGAN is based on SeqGAN by modeling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Frechet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zang, Xiaoxue; Xu, Ying; Chen, Jindong
TI  - MobileHCI - Multimodal Icon Annotation For Mobile Applications
PY  - 2021
AB  - Annotating user interfaces (UIs) that involves localization and classification of meaningful UI elements on a screen is a critical step for many mobile applications such as screen readers and voice control of devices. Annotating object icons, such as menu, search, and arrow backward, is especially challenging due to the lack of explicit labels on screens, their similarity to pictures, and their diverse shapes. Existing studies either use view hierarchy or pixel based methods to tackle the task. Pixel based approaches are more popular as view hierarchy features on mobile platforms are often incomplete or inaccurate, however it leaves out instructional information in the view hierarchy such as resource-ids or content descriptions. We propose a novel deep learning based multi-modal approach that combines the benefits of both pixel and view hierarchy features as well as leverages the state-of-the-art object detection techniques. In order to demonstrate the utility provided, we create a high quality UI dataset by manually annotating the most commonly used 29 icons in Rico, a large scale mobile design dataset consisting of 72k UI screenshots. The experimental results indicate the effectiveness of our multi-modal approach. Our model not only outperforms a widely used object classification baseline but also pixel based object detection models. Our study sheds light on how to combine view hierarchy with pixel features for annotating UI elements.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472064
ER  - 

TY  - NA
AU  - Li, Gang; Baechler, Gilles; Tragut, Manuel; Li, Yang
TI  - Learning to Denoise Raw Mobile UI Layouts for Improving Datasets at Scale
PY  - 2022
AB  - The layout of a mobile screen is a critical data source for UI design research and semantic understanding of the screen. However, UI layouts in existing datasets are often noisy, have mismatches with their visual representation, or consists of generic or app-specific types that are difficult to analyze and model. In this paper, we propose the CLAY pipeline that uses a deep learning approach for denoising UI layouts, allowing us to automatically improve existing mobile UI layout datasets at scale. Our pipeline takes both the screenshot and the raw UI layout, and annotates the raw layout by removing incorrect nodes and assigning a semantically meaningful type to each node. To experiment with our data-cleaning pipeline, we create the CLAY dataset of 59,555 human-annotated screen layouts, based on screenshots and raw layouts from Rico, a public mobile UI corpus. Our deep models achieve high accuracy with F1 scores of 82.7% for detecting layout objects that do not have a valid visual representation and 85.9% for recognizing object types, which significantly outperforms a heuristic baseline. Our work lays a foundation for creating large-scale high quality UI layout datasets for data-driven mobile UI research and reduces the need of manual labeling efforts that are prohibitively expensive.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502042
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Popowski, Lindsay; Mitchell, Tom M.; Myers, Brad A.
TI  - Screen2Vec: Semantic Embedding of GUI Screens and GUI Components
PY  - 2021
AB  - Representing the semantics of GUI screens and components is crucial to data-driven computational methods for modeling user-GUI interactions and mining GUI designs. Existing GUI semantic representations are limited to encoding either the textual content, the visual design and layout patterns, or the app contexts. Many representation techniques also require significant manual data annotation efforts. This paper presents Screen2Vec, a new self-supervised technique for generating representations in embedding vectors of GUI screens and components that encode all of the above GUI features without requiring manual annotation using the context of user interaction traces. Screen2Vec is inspired by the word embedding method Word2Vec, but uses a new two-layer pipeline informed by the structure of GUIs and interaction traces and incorporates screen- and app-specific metadata. Through several sample downstream tasks, we demonstrate Screen2Vec's key useful properties: representing between-screen similarity through nearest neighbors, composability, and capability to represent user tasks.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445049
ER  - 

TY  - JOUR
AU  - Garrelts, Enno; Huber, Marco F.; Roth, Daniel; Binz, Hansgeorg
TI  - AI-Based Topology Optimization of Freehand Sketches
PY  - 2021
AB  - NA
SP  - 1316
EP  - 1321
JF  - Procedia CIRP
VL  - 104
IS  - NA
PB  - 
DO  - 10.1016/j.procir.2021.11.221
ER  - 

TY  - JOUR
AU  - Zhang, Dingwen; Wang, Bo; Wang, Gerong; Zhang, Qiang; Zhang, Jiajia; Han, Jungong; You, Zheng
TI  - Onfocus detection: identifying individual-camera eye contact from unconstrained images
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Onfocus detection aims at identifying whether the focus of the individual captured by a camera is on the camera or not. Based on the behavioral research, the focus of an individual during face-to-camera communication leads to a special type of eye contact, i.e., the individual-camera eye contact, which is a powerful signal in social communication and plays a crucial role in recognizing irregular individual status (e.g., lying or suffering mental disease) and special purposes (e.g., seeking help or attracting fans). Thus, developing effective onfocus detection algorithms is of significance for assisting the criminal investigation, disease discovery, and social behavior analysis. However, the review of the literature shows that very few efforts have been made toward the development of onfocus detector owing to the lack of large-scale public available datasets as well as the challenging nature of this task. To this end, this paper engages in the onfocus detection research by addressing the above two issues. Firstly, we build a large-scale onfocus detection dataset, named as the onfocus detection in the wild (OFDIW). It consists of 20623 images in unconstrained capture conditions (thus called “in the wild”) and contains individuals with diverse emotions, ages, facial characteristics, and rich interactions with surrounding objects and background scenes. On top of that, we propose a novel end-to-end deep model, i.e., the eye-context interaction inferring network (ECIIN), for onfocus detection, which explores eye-context interaction via dynamic capsule routing. Finally, comprehensive experiments are conducted on the proposed OFDIW dataset to benchmark the existing learning models and demonstrate the effectiveness of the proposed ECIIN.</jats:p>
SP  - NA
EP  - NA
JF  - Science China Information Sciences
VL  - 65
IS  - 6
PB  - 
DO  - 10.1007/s11432-020-3181-9
ER  - 

TY  - NA
AU  - Shrestha, Nischal
TI  - VL/HCC - Towards Supporting Knowledge Transfer of Programming Languages
PY  - 2018
AB  - Today, there are hundreds of programming languages that are widely used. Programmers at all levels are expected to become proficient in multiple languages. Experienced programmers who have knowledge of at least one language are able to learn a second language much quicker than novices. However, the transfer process can still be difficult when there exists numerous differences from their previous language. Documentation, online courses and tutorials tend to present information geared towards novices. This type of presentation might suffice for beginners, but it doesn't support learning for experienced programmers [1] who would benefit from leveraging their knowledge of previous programming languages.
SP  - 275
EP  - 276
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506510
ER  - 

TY  - NA
AU  - Doosti, Bardia; Dong, Tao; Deka, Biplab; Nichols, Jeffrey
TI  - A Computational Method for Evaluating UI Patterns.
PY  - 2018
AB  - UI design languages, such as Google's Material Design, make applications both easier to develop and easier to learn by providing a set of standard UI components. Nonetheless, it is hard to assess the impact of design languages in the wild. Moreover, designers often get stranded by strong-opinionated debates around the merit of certain UI components, such as the Floating Action Button and the Navigation Drawer. To address these challenges, this short paper introduces a method for measuring the impact of design languages and informing design debates through analyzing a dataset consisting of view hierarchies, screenshots, and app metadata for more than 9,000 mobile apps. Our data analysis shows that use of Material Design is positively correlated to app ratings, and to some extent, also the number of installs. Furthermore, we show that use of UI components vary by app category, suggesting a more nuanced view needed in design debates.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhao, Tianming; Chen, Chunyang; Liu, Yuanning; Zhu, Xiaodong
TI  - ICSE - guigan: Learning to Generate GUI Designs Using Generative Adversarial Networks
PY  - 2021
AB  - Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our GUIGAN is based on SeqGAN by modeling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Frechet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs.
SP  - 748
EP  - 760
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse43902.2021.00074
ER  - 

TY  - NA
AU  - Watanabe, Yuki; Nakazawa, Atsushi; Mitsuzumi, Yu; Nishida, Toyoaki
TI  - MVA - Spatio-temporal eye contact detection combining CNN and LSTM
PY  - 2019
AB  - Eye contact (mutual gaze) is fundamental for human communication and social interactions; therefore, it is studied in many fields. To support the study of eye contact, much effort has been made to develop automated eye-contact detection using image recognition techniques. In recent years, convolutional neural network (CNN) based eye-contact detection techniques are becoming popular due to their performance; however, they mainly use single frame for recognition. Eye contact is a human communication behavior, so temporal information, such as temporal eye images and facial poses, is important to increase the accuracy of eye-contact detection. We incorporate temporal information into eye-contact detection by using temporal neural network structures that combine CNNs and long short-term memory (LSTM). We tested several network combinations of CNNs and LSTM and found the best solution that uses the outputs of CNNs as well as the cell state vectors of LSTM in the fully connected layers. We prepared two types of eye contact video datasets. One dataset is based on online videos, and the other was taken by a first-person camera in assumed conversational scenarios. The results show that our method is better than the approaches that use single frames. Namely, our method performs 0.8781, while the existing method (DeepEC) performed 0.8319, in F 1 -score.
SP  - 1
EP  - 7
JF  - 2019 16th International Conference on Machine Vision Applications (MVA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/mva.2019.8757989
ER  - 

TY  - NA
AU  - Sermuga Pandian, Vinoth Pandian; Shams, Abdullah; Suleri, Sarah; Jarke, Prof. Dr. Matthias
TI  - LoFi Sketch: A Large Scale Dataset of Smartphone Low Fidelity Sketches
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519624
ER  - 

TY  - JOUR
AU  - Hatchuel, Armand; Le Masson, Pascal; Thomas, Maxime; Weil, Benoit
TI  - WHAT IS GENERATIVE IN GENERATIVE DESIGN TOOLS? UNCOVERING TOPOLOGICAL GENERATIVITY WITH A C-K MODEL OF EVOLUTIONARY ALGORITHMS
PY  - 2021
AB  - Generative design (GD) algorithms is a fast growing field. From the point of view of Design Science, this fast growth leads to wonder what exactly is 'generated' by GD algorithms and how? In the last decades, advances in design theory enabled to establish conditions and operators that characterize design generativity. Thus, it is now possible to study GD algorithms with the lenses of Design Science in order to reach a deeper and unified understanding of their generative techniques, their differences and, if possible, find new paths for improving their generativity. In this paper, first, we rely on C-K ttheory to build a canonical model of GD, based independent of the field of application of the algorithm. This model shows that GD is generative if and only if it builds, not one single artefact, but a “topology of artefacts” that allows for design constructability, covering strategies, and functional comparability of designs. Second, we use the canonical model to compare four well documented and most advanced types of GD algorithms. From these cases, it appears that generating a topology enables the analyses of interdependences and the design of resilience.
SP  - 3419
EP  - 3430
JF  - Proceedings of the Design Society
VL  - 1
IS  - NA
PB  - 
DO  - 10.1017/pds.2021.603
ER  - 

TY  - NA
AU  - De Gregorio, Daniele; Tonioni, Alessio; Palli, Gianluca; Di Stefano, Luigi
TI  - Semi-Automatic Labeling for Deep Learning in Robotics
PY  - 2019
AB  - In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a semi-automatic method which leverages on moving a 2D camera by means of a robot, proving precise camera tracking, and an augmented reality pen to define initial object bounding box, to create large labeled datasets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the Deep Learning technique applied to computer vision, that typically requires very large datasets, truly automated and reliable. With the ARS pipeline, we created effortlessly two novel datasets, one on electromechanical components (industrial scenario) and one on fruits (daily-living scenario), and trained robustly two state-of-the-art object detectors, based on convolutional neural networks, such as YOLO and SSD. With respect to the conventional manual annotation of 1000 frames that takes us slightly more than 10 hours, the proposed approach based on ARS allows annotating 9 sequences of about 35000 frames in less than one hour, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15\% with respect to manual labeling. All our software is available as a ROS package in a public repository alongside the novel annotated datasets.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Kawamura, Ryota; Takazawa, Kazuki; Yamamoto, Kenta; Ochiai, Yoichi
TI  - HCI (16) - Design Method of 3D-Printable Ergonomically Personalized Stabilizer
PY  - 2019
AB  - In photography and videography, it is a challenge to align a sight toward the target continuously and steadily, as considerable practice and experience are required. Blurry photographs are often captured by camera users who lack the necessary skills. To address the problem, stabilizers have been developed. However, conventional stabilizers involve a steep learning curve because they are designed to be mass produced, and thus not tailored according to an individual. We herein present the design method of a three-dimensional printable personalized stabilizer. It is optimized ergonomically through topology optimization, which is a typical method to optimize the shape of materials.
SP  - 71
EP  - 87
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-22216-1_6
ER  - 

TY  - JOUR
AU  - Chen, Qiuyuan; Chen, Chunyang; Hassan, Safwat; Xing, Zhengchang; Xia, Xin; Hassan, Ahmed
TI  - How Should I Improve the UI of My App?: A Study of User Reviews of Popular Apps in the Google Play
PY  - 2021
AB  - UI (User Interface) is an essential factor influencing users’ perception of an app. However, it is hard for even professional designers to determine if the UI is good or not for end-users. Users’ feedback (e.g., user reviews in the Google Play) provides a way for app owners to understand how the users perceive the UI. In this article, we conduct an in-depth empirical study to analyze the UI issues of mobile apps. In particular, we analyze more than 3M UI-related reviews from 22,199 top free-to-download apps and 9,380 top non-free apps in the Google Play Store. By comparing the rating of UI-related reviews and other reviews of an app, we observe that UI-related reviews have lower ratings than other reviews. By manually analyzing a random sample of 1,447 UI-related reviews with a 95% confidence level and a 5% interval, we identify 17 UI-related issues types that belong to four categories (i.e., “Appearance,” “Interaction,” “Experience,” and “Others”). In these issue types, we find “Generic Review” is the most occurring one. “Comparative Review” and “Advertisement” are the most negative two UI issue types. Faced with these UI issues, we explore the patterns of interaction between app owners and users. We identify eight patterns of how app owners dialogue with users about UI issues by the review-response mechanism. We find “Apology or Appreciation” and “Information Request” are the most two frequent patterns. We find updating UI timely according to feedback is essential to satisfy users. Besides, app owners could also fix UI issues without updating UI, especially for issue types belonging to “Interaction” category. Our findings show that there exists a positive impact if app owners could actively interact with users to improve UI quality and boost users’ satisfactoriness about the UIs.
SP  - 37
EP  - 38
JF  - ACM Transactions on Software Engineering and Methodology
VL  - 30
IS  - 3
PB  - 
DO  - 10.1145/3447808
ER  - 

TY  - NA
AU  - Wang, Bryan; Li, Gang; Zhou, Xin; Chen, Zhourong; Grossman, Tovi; Li, Yang
TI  - Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning
PY  - 2021
AB  - Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across $\sim$22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Wagner, Tobias; Gugenheimer, Jan; Rukzio, Enrico
TI  - ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact Retrieval using Mixed Reality
PY  - 2022
AB  - Personal fabrication is made more accessible through repositories like Thingiverse, as they replace modeling with retrieval. However, they require users to translate spatial requirements to keywords, which paints an incomplete picture of physical artifacts: proportions or morphology are non-trivially encoded through text only. We explore a vision of in-situ spatial search for (future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to search for 3D models using in-situ sketches blended with textual queries. With ShapeFindAR, users search for geometry, and not necessarily precise labels, while coupling the search process to the physical environment (e.g., by sketching in-situ, extracting search terms from objects present, or tracing them). We developed ShapeFindAR for HoloLens 2, connected to a database of 3D-printable artifacts. We specify in-situ spatial search, describe its advantages, and present walkthroughs using ShapeFindAR, which highlight novel ways for users to articulate their wishes, without requiring complex modeling tools or profound domain knowledge.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517682
ER  - 

TY  - NA
AU  - Ogusu, Reo; Yamanaka, Takao
TI  - LPM: Learnable Pooling Module for Efficient Full-Face Gaze Estimation
PY  - 2019
AB  - Gaze tracking is an important technology in many domains. Techniques such as Convolutional Neural Networks (CNN) has allowed the invention of gaze tracking method that relies only on commodity hardware such as the camera on a personal computer. It has been shown that the full-face region for gaze estimation can provide better performance than from an eye image alone. However, a problem with using the full-face image is the heavy computation due to the larger image size. This study tackles this problem through compression of the input full-face image by removing redundant information using a novel learnable pooling module. The module can be trained end-to-end by backpropagation to learn the size of the grid in the pooling filter. The learnable pooling module keeps the resolution of valuable regions high and vice versa. This proposed method preserved the gaze estimation accuracy at a certain level when the image was reduced to a smaller size.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Thayer, Kyle; Guo, Philip J.; Reinecke, Katharina
TI  - VL/HCC - The Impact of Culture on Learner Behavior in Visual Debuggers
PY  - 2018
AB  - People around the world are learning to code using online resources. However, research has found that these learners might not gain equal benefit from such resources, in particular because culture may affect how people learn from and use online resources. We therefore expect to see cultural differences in how people use and benefit from visual debuggers. We investigated the use of one popular online debugger which allows users to execute Python code and navigate bidirectionally through the execution using forward-steps and back-steps. We examined behavioral logs of 78,369 users from 69 countries and conducted an experiment with 522 participants from 82 countries. We found that people from countries that tend to prefer self-directed learning (such as those from countries with a low Power Distance, which tend to be less hierarchical than others) used about twice as many back-steps. We also found that for individuals whose values aligned with instructor-directed learning (those who scored high on a “Conservation” scale), back-steps were associated with less debugging success.
SP  - 115
EP  - 124
JF  - 2018 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vlhcc.2018.8506556
ER  - 

TY  - NA
AU  - Patil, Akshay Gadi; Ben-Eliezer, Omri; Perel, Or; Averbuch-Elor, Hadar
TI  - CVPR Workshops - READ: Recursive Autoencoders for Document Layout Generation
PY  - 2020
AB  - Layout is a fundamental component of any graphic design. Creating large varieties of plausible document layouts can be a tedious task, requiring numerous constraints to be satisfied, including local ones relating different semantic elements and global constraints on the general appearance and spacing. In this paper, we present a novel framework, coined READ, for REcursive Autoencoders for Document layout generation, to generate plausible 2D layouts of documents in large quantities and varieties. First, we devise an exploratory recursive method to extract a structural decomposition of a single document. Leveraging a dataset of documents annotated with labeled bounding boxes, our recursive neural network learns to map the structural representation, given in the form of a simple hierarchy, to a compact code, the space of which is approximated by a Gaussian distribution. Novel hierarchies can be sampled from this space, obtaining new document layouts. Moreover, we introduce a combinatorial metric to measure structural similarity among document layouts. We deploy it to show that our method is able to generate highly variable and realistic layouts. We further demonstrate the utility of our generated layouts in the context of standard detection tasks on documents, showing that detection performance improves when the training data is augmented with generated documents whose layouts are produced by READ.
SP  - 2316
EP  - 2325
JF  - 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvprw50498.2020.00280
ER  - 

TY  - CHAP
AU  - Park, Seonwook; Spurr, Adrian; Hilliges, Otmar
TI  - Deep Pictorial Gaze Estimation
PY  - 2018
AB  - Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.
SP  - 741
EP  - 757
JF  - Computer Vision – ECCV 2018
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01261-8_44
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Mitchell, Tom M.; Myers, Brad A.
TI  - ACL (demo) - Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations
PY  - 2020
AB  - We summarize our past five years of work on designing, building, and studying Sugilite, an interactive task learning agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations leveraging the graphical user interfaces (GUIs) of third-party mobile apps. Through its multi-modal and mixed-initiative approaches for Human- AI interaction, Sugilite made important contributions in improving the usability, applicability, generalizability, flexibility, robustness, and shareability of interactive task learning agents. Sugilite also represents a new human-AI interaction paradigm for interactive task learning, where it uses existing app GUIs as a medium for users to communicate their intents with an AI agent instead of the interfaces for users to interact with the underlying computing services. In this chapter, we describe the Sugilite system, explain the design and implementation of its key features, and show a prototype in the form of a conversational assistant on Android.
SP  - 215
EP  - 223
JF  - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2020.acl-demos.25
ER  - 

TY  - NA
AU  - Mehralian, Forough; Salehnamadi, Navid; Malek, Sam
TI  - ESEC/SIGSOFT FSE - Data-driven accessibility repair revisited: on the effectiveness of generating labels for icons in Android apps
PY  - 2021
AB  - Mobile apps are playing an increasingly important role in our daily lives, including the lives of approximately 304 million users worldwide that are either completely blind or suffer from some form of visual impairment. These users rely on screen readers to interact with apps. Screen readers, however, cannot describe the image icons that appear on the screen, unless those icons are accompanied with developer-provided textual labels. A prior study of over 5,000 Android apps found that in around 50% of the apps, less than 10% of the icons are labeled. To address this problem, a recent award-winning approach, called LabelDroid, employed deep-learning techniques to train a model on a dataset of existing icons with labels to automatically generate labels for visually similar, unlabeled icons. In this work, we empirically study the nature of icon labels in terms of distribution and their dependency on different sources of information. We then assess the effectiveness of LabelDroid in predicting labels for unlabeled icons. We find that icon images are insufficient in representing icon labels, while other sources of information from the icon usage context can enrich images in determining proper tokens for labels. We propose the first context-aware label generation approach, called COALA, that incorporates several sources of information from the icon in generating accurate labels. Our experiments show that although COALA significantly outperforms LabelDroid in both user study and automatic evaluation, further research is needed. We suggest that future studies should be more cautious when basing their approach on automatically extracted labeled data.
SP  - 107
EP  - 118
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3468604
ER  - 

TY  - NA
AU  - Zhang, Fanxing; Liu, Zhihao; Cheng, Zhanglin; Deussen, Oliver; Chen, Baoquan; Wang, Yunhai
TI  - VR - Mid-Air Finger Sketching for Tree Modeling
PY  - 2021
AB  - 2D sketch-based tree modeling cannot guarantee to generate plausible depth values and full 3D tree shapes. With the advent of virtual reality (VR) technologies, 3D sketching enables a new form for 3D tree modeling. However, it is labor-intensive and difficult to create realistically-looking 3D trees with complicated geometry and lots of detailed twigs with a reasonable amount of effort. In this paper, we explore the use of mid-air finger 3D sketching in VR for tree modeling. We present a hybrid approach that integrates freehand 3D sketches with an automatic population of branch geometries. The user only needs to draw a few 3D strokes in mid-air to define the envelope of the foliage (denoted as lobes) and main branches. Our algorithm then automatically generates a full 3D tree model based on these stroke inputs. Additionally, the shape of the 3D tree model can be modified by freely dragging, squeezing, or moving lobes in mid-air. We demonstrate the ease-of-use, efficiency, and flexibility in tree modeling and overall shape control. We perform user studies and show a variety of realistic tree models generated instantaneously from 3D finger sketching.
SP  - 826
EP  - 834
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00110
ER  - 

TY  - NA
AU  - Li, Yang; Kumar, Ranjitha; Lasecki, Walter S.; Hilliges, Otmar
TI  - CHI Extended Abstracts - Artificial Intelligence for HCI: A Modern Approach
PY  - 2020
AB  - Artificial intelligence (AI) and Human Computer Interaction (HCI) share common roots and early work on conversational agents has laid the foundation for both fields. However, in subsequent decades the initial tight connection between the fields has become less pronounced. The recent rise of deep learning has revolutionized AI and has led to a raft of practical methods and tools that significantly impact areas outside of core-AI. In particular, modern AI techniques now power new ways for machines and humans to interact. Thus it is timely to investigate how modern AI can propel HCI research in new ways and how HCI research can help direct AI developments. This workshop offers a forum for researchers to discuss new opportunities that lie in bringing modern AI methods into HCI research, identifying important problems to investigate, showcasing computational and scientific methods that can be applied, and sharing datasets and tools that are already available or proposing those that should be further developed. The topics we are interested in including deep learning methods for understanding and modeling human behaviors and enabling new interaction modalities, hybrid intelligence that combine human and machine intelligence to solve difficult tasks, and tools and methods for interaction data curation and large-scale data-driven design. At the core of these topics, we want to start the conversation on how data-driven and data-centric approaches of modern AI can impact HCI.
SP  - 3375147
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375147
ER  - 

TY  - CHAP
AU  - Altinbas, Mehmet Dogan; Serif, Tacha
TI  - GUI Element Detection from Mobile UI Images Using YOLOv5
PY  - 2022
AB  - AbstractIn mobile application development, building a consistent user interface (UI) might be a costly and time-consuming process. This is especially the case if an organization has a separate team for each mobile platform such as iOS and Android. In this regard, the companies that choose the native mobile app development path end up going through do-overs as the UI work done on one platform needs to be repeated for other platforms too. One of the tedious parts of UI design tasks is creating a graphical user interface (GUI). There are numerous tools and prototypes in the literature that aim to create feasible GUI automation solutions to speed up this process and reduce the labor workload. However, as the technologies evolve and improve new versions of existing algorithms are created and offered. Accordingly, this study aims to employ the latest version of YOLO, which is YOLOv5, to create a custom object detection model that recognizes GUI elements in a given UI image. In order to benchmark the newly trained YOLOv5 GUI element detection model, existing work from the literature and their data set is considered and used for comparison purposes. Therefore, this study makes use of 450 UI samples of the VINS dataset for testing, a similar amount for validation and the rest for model training. Then the findings of this work are compared with another study that has used the SSD algorithm and VINS dataset to train, validate and test its model, which showed that proposed algorithm outperformed SSD’s mean average precision (mAP) by 15.69%.KeywordsObject detectionGraphical user interfaceDeep learning
SP  - 32
EP  - 45
JF  - Mobile Web and Intelligent Information Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-14391-5_3
ER  - 

TY  - NA
AU  - Shen, Yilin; Nama, Sandeep; Jin, Hongxia
TI  - MobiSys - Teach Once and Use Everywhere -- Building AI Assistant Eco-Skills via User Instruction and Demonstration (poster)
PY  - 2019
AB  - Voice-enabled AI assistants rely on developers to build every single skill, although many skills share similar functions. We propose a concept and prototype system, \ksystem, to automatically build a set of similar skills in the ecosystem (eco-skills) with one-time teaching from end users. During teaching, a user only needs to demonstrate on the screen in one (native) mobile app and provides natural language (NL) instructions.
SP  - 606
EP  - 607
JF  - Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3307334.3328648
ER  - 

TY  - NA
AU  - Gupta, Aakar; Lin, Bo Rui; Ji, Siyi; Patel, Arjav; Vogel, Daniel
TI  - CHI - Replicate and Reuse: Tangible Interaction Design for Digitally-Augmented Physical Media Objects
PY  - 2020
AB  - Technology has transformed our physical interactions into infinitely more scalable and flexible digital ones. We can peruse an infinite number of photos, news articles, and books. However, these digital experiences lack the physical experience of paging through an album, reading a newspaper, or meandering through a bookshelf. Overlaying physical objects with digital content using augmented reality is a promising avenue towards bridging this gap. In this paper, we investigate the interaction design for such digital-overlaid physical objects and their varying levels of tangibility. We first conduct a user evaluation of a physical photo album that uses tangible interactions to support physical and digital operations. We further prototype multiple objects including bookshelves and newspapers and probe users on their usage, capabilities, and interactions. We then conduct a qualitative investigation of three interaction designs with varying tangibility that use three different input modalities. Finally, we discuss the insights from our investigations and recommend design guidelines.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376139
ER  - 

TY  - JOUR
AU  - Xu, Pengfei; Li, Yifan; Yang, Zhijin; Shi, Weiran; Fu, Hongbo; Huang, Hui
TI  - Hierarchical Layout Blending with Recursive Optimal Correspondence
PY  - 2022
AB  - <jats:p>We present a novel method for blending hierarchical layouts with semantic labels. The core of our method is a hierarchical structure correspondence algorithm, which recursively finds optimal substructure correspondences, achieving a globally optimal correspondence between a pair of hierarchical layouts. This correspondence is consistent with the structures of both layouts, allowing us to define the union of the layouts' structures. The resulting compound structure helps extract intermediate layout structures, from which blended layouts can be generated via an optimization approach. The correspondence also defines a similarity measure between layouts in a hierarchically structured view. Our method provides a new way for novel layout creation. The introduced structural similarity measure regularizes the layouts in a hyperspace. We demonstrate two applications in this paper, i.e., exploratory design of novel layouts and sketch-based layout retrieval, and test them on a magazine layout dataset. The effectiveness and feasibility of these two applications are confirmed by the user feedback and the extensive results. The code is available at https://github.com/lyf7115/LayoutBlending.</jats:p>
SP  - 1
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 6
PB  - 
DO  - 10.1145/3550454.3555446
ER  - 

TY  - NA
AU  - Li, Jiahao; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - UIST - Robiot: A Design Tool for Actuating Everyday Objects with Automatically Generated 3D Printable Mechanisms
PY  - 2019
AB  - Users can now easily communicate digital information with an Internet of Things; in contrast, there remains a lack of support to automate physical tasks that involve legacy static objects, e.g. adjusting a desk lamp's angle for optimal brightness, turning on/off a manual faucet when washing dishes, sliding a window to maintain a preferred indoor temperature. Automating these simple physical tasks has the potential to improve people's quality of life, which is particularly important for people with a disability or in situational impairment. We present Robiot -- a design tool for generating mechanisms that can be attached to, motorized, and actuating legacy static objects to perform simple physical tasks. Users only need to take a short video manipulating an object to demonstrate an intended physical behavior. Robiot then extracts requisite parameters and automatically generates 3D models of the enabling actuation mechanisms by performing a scene and motion analysis of the 2D video in alignment with the object's 3D model. In an hour-long design session, six participants used Robiot to actuate seven everyday objects, imbuing them with the robotic capability to automate various physical tasks.
SP  - 673
EP  - 685
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347894
ER  - 

TY  - NA
AU  - Yang, Bo; Xing, Zhenchang; Xia, Xin; Chen, Chunyang; Ye, Deheng; Li, Shanping
TI  - ICSE (Companion Volume) - UIS-hunter: detecting UI design smells in Android apps
PY  - 2021
AB  - Similar to code smells in source code, UI design has visual design smells that indicate violations of good UI design guidelines. UI design guidelines constitute design systems for a vast variety of products, platforms, and services. Following a design system, developers can avoid common design issues and pitfalls. However, a design system is often complex, involving various design dimensions and numerous UI components. Lack of concerns on GUI visual effect results in little support for detecting UI design smells that violate the design guidelines in a complex design system. In this paper, we propose an automated UI design smell detector named UIS-Hunter (UI design Smell Hunter). The tool is able to (i) automatically process UI screenshots or prototype files to detect UI design smells and generate reports, (ii) highlight the violated UI regions and list the material design guidelines that the found design smells violate, and (iii) present conformance and violation UI design examples to assist understanding. This tool consists of a Material Design guidelines gallery website and a tool website. The gallery website is a back-end knowledge base that attaches conformance and violation examples to abstract design guidelines and allows developers and designers to explore the multi-dimensional space of a complex design system in a more structured way. As a front-end application, the tool website takes a UI design as input, returns a detailed UI design smell report, and marks the violation regions (if any). Moreover, the tool website presents conformance and violation examples based on the gallery website. Demo URL: https://uishunter.net.cn/ https://uishuntergallery.net.cn/ Demo Video: https://youtu.be/7UZ0jtD_1gM
SP  - 89
EP  - 92
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse-companion52605.2021.00043
ER  - 

TY  - NA
AU  - Micallef, Nicholas; Adi, Erwin; Misra, Gaurav
TI  - UbiComp/ISWC Adjunct - Investigating Login Features in Smartphone Apps
PY  - 2018
AB  - Recent revelations about data breaches have heightened users' consciousness about the privacy of their online activity. An often overlooked avenue of collection of users' personal information are registration processes and/or social logins, such as login with Facebook or Google, implemented by smartphone apps. Although the extent of social login implementations on websites has been widely studied, there is negligible research on the extent of implementation of login features on smartphone apps. Hence, this work contributes to further the understanding of smartphone apps ecosystem through investigating whether smartphone apps use login features, and what relationships exist between login features and apps popularity. To address this research gap this work presents the systematic analysis of the publicly available Rico dataset, which contains 72,000 unique UI screen designs that describes smartphone apps design properties for 9,717 Android apps.
SP  - 842
EP  - 851
JF  - Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3267305.3274172
ER  - 

TY  - NA
AU  - Pareddy, Sujeath; Guo, Anhong; Bigham, Jeffrey P.
TI  - ASSETS - X-Ray: Screenshot Accessibility via Embedded Metadata
PY  - 2019
AB  - Screenshots are frequently shared on social media, via personal communications, and in academic papers. Unfortunately, existing screenshot tools strip away semantics useful for making the content accessible, leaving only pixels. For example, a screenshot of a table removes the structural information useful for conveying it. We quantify the scale of the problem via a study of academic papers, showing that a large number of images included in academic papers are screenshots, and validate this via qualitative interviews with researchers about their figure generation process. We then introduce X-Ray, a system that captures and embeds the semantics of the underlying content into images. Using the X-Ray screenshot tool, semantic information is captured and stored in the Exif data of the resulting image, allowing it to "tag along" as the image is shared and reposted. We demonstrate that our approach retains accessibility for screen reader users via a study with five blind participants. More generally, our approach suggests a method for embedding accessibility metadata into otherwise inaccessible formats, enabling them to retain the more accessible representations that are present at capture time.
SP  - 389
EP  - 395
JF  - The 21st International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3308561.3353808
ER  - 

TY  - JOUR
AU  - Liu, Zhe; Chen, Chunyang; Wang, Junjie; Huang, Yuekai; Hu, Jun; Wang, Qing
TI  - Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding
PY  - 2022
AB  - Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.
SP  - 1
EP  - 1
JF  - IEEE Transactions on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/tse.2022.3150876
ER  - 

TY  - JOUR
AU  - Yau, Hong-Tzong; Liao, Shu-Wei; Chang, Chia-Hao
TI  - Modeling of digital dental articulator and its accuracy verification using optical measurement
PY  - 2020
AB  - NA
SP  - 105646
EP  - NA
JF  - Computer methods and programs in biomedicine
VL  - 196
IS  - NA
PB  - 
DO  - 10.1016/j.cmpb.2020.105646
ER  - 

TY  - NA
AU  - Schmeier, Bastian; Kopetz, Jan Patrick; Kordts, Börge; Jochems, Nicole
TI  - Manipulating Virtual Objects in Augmented Reality Using a New Ball-Shaped Input Device
PY  - 2021
AB  - Today’s Augmented Reality (AR) technology allows users to explore the real world enriched with digital artifacts, learn from it, or shape it (i.e., creating your own virtual objects). To properly use virtual objects in AR space, users must be able to manipulate them (i.e., rotate or move them). The prerequisite for manipulation is an intuitive interaction technique controlled by an input device. To explore novel AR interaction techniques, a new ball-shaped input device called BIRDY is combined with the HoloLens for the first time. This paper presents findings regarding this combination of devices. Four new interaction techniques were designed that benefit from BIRDY’s orientation invariance. Aiming to identify promising interaction rules, a prototype was developed to evaluate these interaction techniques. Results indicate that using gravity as a placement tool and separating the degrees of freedom when manipulating virtual objects provides the best experience for users. Findings further confirm the potential of using ball-shaped devices for interaction in AR.
SP  - NA
EP  - NA
JF  - 12th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460881.3460935
ER  - 

TY  - CHAP
AU  - Burns, Andrea; Arsan, Deniz; Agrawal, Sanjna; Kumar, Ranjitha; Saenko, Kate; Plummer, Bryan A.
TI  - A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility
PY  - 2022
AB  - NA
SP  - 312
EP  - 328
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-20074-8_18
ER  - 

TY  - NA
AU  - Pandian, Vinoth Pandian Sermuga; Suleri, Sarah
TI  - BlackBox Toolkit: Intelligent Assistance to UI Design.
PY  - 2020
AB  - User Interface (UI) design is an creative process that involves considerable reiteration and rework. Designers go through multiple iterations of different prototyping fidelities to create a UI design. In this research, we propose to modify the UI design process by assisting it with artificial intelligence (AI). We propose to enable AI to perform repetitive tasks for the designer while allowing the designer to take command of the creative process. This approach makes the machine act as a black box that intelligently assists the designers in creating UI design. We believe this approach would greatly benefit designers in co-creating design solutions with AI.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Kong, Xiang; Jiang, Lu; Chang, Huiwen; Zhang, Han; Hao, Yuan; Gong, Haifeng; Essa, Irfan
TI  - BLT: Bidirectional Layout Transformer for Controllable Layout Generation
PY  - 2022
AB  - NA
SP  - 474
EP  - 490
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-19790-1_29
ER  - 

TY  - JOUR
AU  - Kompatsiari, Kyveli; Ciardo, Francesca; Tikhanoff, Vadim; Metta, Giorgio; Wykowska, Agnieszka
TI  - It’s in the Eyes: The Engaging Role of Eye Contact in HRI
PY  - 2019
AB  - This paper reports a study where we examined how a humanoid robot was evaluated by users, dependent on established eye contact. In two experiments, the robot was programmed to either establish eye ...
SP  - 525
EP  - 535
JF  - International Journal of Social Robotics
VL  - 13
IS  - 3
PB  - 
DO  - 10.1007/s12369-019-00565-4
ER  - 

TY  - NA
AU  - Chi, Pei-Yu; Hu, Sen-Po; Li, Yang
TI  - CHI - Doppio: Tracking UI Flows and Code Changes for App Development
PY  - 2018
AB  - Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes.
SP  - 455
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174029
ER  - 

TY  - NA
AU  - Wang, Bryan; Li, Gang; Zhou, Xin; Chen, Zhourong; Grossman, Tovi; Li, Yang
TI  - UIST - Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning
PY  - 2021
AB  - Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across ∼ 22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces.
SP  - 498
EP  - 510
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474765
ER  - 

TY  - JOUR
AU  - Akca, Eren; Tanrıöver, Ömer Özgür
TI  - A comprehensive appraisal of perceptual visual complexity analysis methods in GUI design
PY  - 2021
AB  - NA
SP  - 102031
EP  - NA
JF  - Displays
VL  - 69
IS  - NA
PB  - 
DO  - 10.1016/j.displa.2021.102031
ER  - 

TY  - CHAP
AU  - Celiktutan, Oya; Demiris, Yiannis
TI  - ECCV Workshops (6) - Inferring Human Knowledgeability from Eye Gaze in Mobile Learning Environments
PY  - 2019
AB  - What people look at during a visual task reflects an interplay between ocular motor functions and cognitive processes. In this paper, we study the links between eye gaze and cognitive states to investigate whether eye gaze reveal information about an individual’s knowledgeability. We focus on a mobile learning scenario where a user and a virtual agent play a quiz game using a hand-held mobile device. To the best of our knowledge, this is the first attempt to predict user’s knowledgeability from eye gaze using a noninvasive eye tracking method on mobile devices: we perform gaze estimation using front-facing camera of mobile devices in contrast to using specialised eye tracking devices. First, we define a set of eye movement features that are discriminative for inferring user’s knowledgeability. Next, we train a model to predict users’ knowledgeability in the course of responding to a question. We obtain a classification performance of 59.1% achieving human performance, using eye movement features only, which has implications for (1) adapting behaviours of the virtual agent to user’s needs (e.g., virtual agent can give hints); (2) personalising quiz questions to the user’s perceived knowledgeability.
SP  - 193
EP  - 209
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-11024-6_13
ER  - 

TY  - NA
AU  - Schlattner, Philippe; Bielik, Pavol; Vechev, Martin
TI  - Learning to Infer User Interface Attributes from Images
PY  - 2019
AB  - We explore a new domain of learning to infer user interface attributes that helps developers automate the process of user interface implementation. Concretely, given an input image created by a designer, we learn to infer its implementation which when rendered, looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we additionally use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. We instantiate our approach to the task of inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhou, Zhilan; Xu, Jian; Balasubramanian, Aruna; Porter, Donald E.
TI  - MobileHCI - A Survey of Patterns for Adapting Smartphone App UIs to Smart Watches
PY  - 2020
AB  - Wearable devices, such as smart watches and fitness trackers are growing in popularity, creating a need for application developers to adapt or extend a UI, typically from a smartphone, onto these devices. Wearables generally have a smaller form factor than a phone; thus, porting an app to the watch necessarily involves reworking the UI. An open problem is identifying best practices for adapting UIs to wearable devices. This paper contributes a study and data set of the state of practice in UI adaptation for wearables. We automatically extract UI designs from a set of 101 popular Android apps that have both a phone and watch version, and manually label how each UI element, as well as how screens in the app, are translated from the phone to the wearable. The paper identifies trends in adaptation strategies and presents design guidelines. We expect that the UI adaptation strategies identified in this paper can have wide-ranging impacts for future research and identifying best practices in this space, such as grounding future user studies that evaluate which strategies improve user satisfaction or automatically adapting UIs.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403564
ER  - 

TY  - JOUR
AU  - Khan, Shahroz; Gunpinar, Erkan; Moriguchi, Masaki; Suzuki, Hiromasa
TI  - Evolving a psycho-physical distance metric for generative design exploration of diverse shapes
PY  - 2019
AB  - In this paper, a generative design approach is proposed that involves the users’ psychological aspect in the design space exploration stage to create distinct design alternatives. Users’ perceptual judgment about designs is extracted as a psycho-physical distance metric, which is then integrated into the design exploration step to generate design alternatives for the parametric computer-aided design (CAD) shapes. To do this, a CAD model is first parametrized by defining geometric parameters and determining ranges of these parameters. Initial design alternatives for the CAD model are generated using Euclidean distance-based sampling teaching–learning-based optimization (S-TLBO), which is recently proposed and can sample N space-filling design alternatives in the design space. Similar designs are then clustered, and a user study is conducted to capture the subjects’ perceptual response for the dissimilarities between the cluster pairs. In addition, a furthest-point-sorting technique is introduced to equalize the number of designs in the clusters, which are being compared by the subjects in the user study. Afterward, nonlinear regression analyses are carried out to construct a mathematical correlation between the subjects’ perceptual response and geometric parameters in the form of a psycho-physical distance metric. Finally, a psycho-physical distance metric obtained is utilized to explore distinct design alternatives for the CAD model. Another user study is designed to compare the diversification between the designs when the Euclidean and the suggested psycho-physical distance metrics are utilized. According to the user study, designs generated with the latter metric are more distinct.
SP  - 1
EP  - 16
JF  - Journal of Mechanical Design
VL  - 141
IS  - 11
PB  - 
DO  - 10.1115/1.4043678
ER  - 

TY  - NA
AU  - Bao, Yiwei; Cheng, Yihua; Liu, Yunfei; Lu, Feng
TI  - ICPR - Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets
PY  - 2021
AB  - Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method.
SP  - 9936
EP  - 9943
JF  - 2020 25th International Conference on Pattern Recognition (ICPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icpr48806.2021.9412205
ER  - 

TY  - NA
AU  - Rozanova, Julia; Ferreira, Deborah; Dubba, Krishna; Cheng, Weiwei; Zhang, Dell; Freitas, André
TI  - Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?
PY  - 2021
AB  - Models designed for intelligent process automation are required to be capable of grounding user interface elements. This task of interface element grounding is centred on linking instructions in natural language to their target referents. Even though BERT and similar pre-trained language models have excelled in several NLP tasks, their use has not been widely explored for the UI grounding domain. This work concentrates on testing and probing the grounding abilities of three different transformer-based models: BERT, RoBERTa and LayoutLM. Our primary focus is on these models' spatial reasoning skills, given their importance in this domain. We observe that LayoutLM has a promising advantage for applications in this domain, even though it was created for a different original purpose (representing scanned documents): the learned spatial features appear to be transferable to the UI grounding setting, especially as they demonstrate the ability to discriminate between target directions in natural language instructions.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Soto, Andrés; Mora, Héctor; Riascos, Jaime A.
TI  - Web Generator: An open-source software for synthetic web-based user interface dataset generation
PY  - 2022
AB  - <h2>Abstract</h2> Recently, Machine Learning algorithms have been employed to automate several processes, including software development. However, this action demands large datasets for training these algorithms. To our knowledge, there is no tool for generating synthetic datasets that contain HTML objects (interfaces, codes, wireframe.) Thus, we present the Web Generator, a software designed to mainly provide web pages, designs, and content based on the Bootstrap frontend framework. The software delivers markup code, screenshots, and labels for web elements. We aim to generate enough material for training and exploring the Machine Learning approach for automatic web design and development with this software.
SP  - 100985
EP  - 100985
JF  - SoftwareX
VL  - 17
IS  - NA
PB  - 
DO  - 10.1016/j.softx.2022.100985
ER  - 

TY  - BOOK
AU  - Zhang, Xucong; Sugano, Yusuke; Bulling, Andreas
TI  - ETRA - Revisiting data normalization for appearance-based gaze estimation
PY  - 2018
AB  - Appearance-based gaze estimation is promising for unconstrained real-world settings, but the significant variability in head pose and user-camera distance poses significant challenges for training generic gaze estimators. Data normalization was proposed to cancel out this geometric variability by mapping input images and gaze labels to a normalized space. Although used successfully in prior works, the role and importance of data normalization remains unclear. To fill this gap, we study data normalization for the first time using principled evaluations on both simulated and real data. We propose a modification to the current data normalization formulation by removing the scaling factor and show that our new formulation performs significantly better (between 9.5% and 32.7%) in the different evaluation settings. Using images synthesized from a 3D face model, we demonstrate the benefit of data normalization for the efficiency of the model training. Experiments on real-world images confirm the advantages of data normalization in terms of gaze estimation performance.
SP  - 12
EP  - NA
JF  - Proceedings of the 2018 ACM Symposium on Eye Tracking Research & Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3204493.3204548
ER  - 

TY  - JOUR
AU  - ArditoLuca, ; BottinoAndrea, ; CoppolaRiccardo, ; LambertiFabrizio, ; ManigrassoFrancesco, ; MorraLia, ; TorchianoMarco, 
TI  - Feature Matching-based Approaches to Improve the Robustness of Android Visual GUI Testing
PY  - 2021
AB  - In automated Visual GUI Testing (VGT) for Android devices, the available tools often suffer from low robustness to mobile fragmentation, leading to incorrect results when running the same tests on ...
SP  - 1
EP  - 32
JF  - ACM Transactions on Software Engineering and Methodology
VL  - 31
IS  - 2
PB  - 
DO  - 10.1145/3477427
ER  - 

TY  - NA
AU  - Kim, Sunbum; Shim, Youngbo Aram; Lee, Geehyuk
TI  - Exploration of Form Factor and Bimanual 3D Manipulation Performance of Rollable In-hand VR Controller
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565625
ER  - 

TY  - NA
AU  - Saenz, Juan Pablo; De Russis, Luigi
TI  - On How Novices Approach Programming Exercises Before and During Coding
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519655
ER  - 

TY  - JOUR
AU  - Zhao, Shenghuan; Wang, Luo; Qian, Xueming; Chen, Jianping
TI  - Enhancing performance-based generative architectural design with sketch-based image retrieval: a pilot study on designing building facade fenestrations
PY  - 2021
AB  - NA
SP  - 2981
EP  - 2997
JF  - The Visual Computer
VL  - 38
IS  - 8
PB  - 
DO  - 10.1007/s00371-021-02170-x
ER  - 

TY  - CHAP
AU  - Shah, Kunal; Laxkar, Pradeep; Chakrabarti, Prasun
TI  - A Hypothesis on Ideal Artificial Intelligence and Associated Wrong Implications
PY  - 2019
AB  - The paper points out certain novel axioms viz., Selflessness, Humility with Ambiguity and Reinforced learning. The axioms have been correlated with the fact towards realising ideal Artificial Intelligence and the associated wrong implications.
SP  - 283
EP  - 294
JF  - Advances in Intelligent Systems and Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-13-8618-3_30
ER  - 

TY  - NA
AU  - Xiang, Nan; Yang, Xiaosong; Zhang, Jian J.
TI  - ACM Multimedia - TsFPS: An Accurate and Flexible 6DoF Tracking System with Fiducial Platonic Solids
PY  - 2021
AB  - We present a vision-based system for real-time pose tracking of the rigid object, it can not only estimate a single pose in six degrees of freedom (6DoF), but also suitable for recovering compound movements. The system is comprised of a monocular camera, and a series of 3D printed platonic solids with squared fiducial markers attached on each single face, which is easy to setup and extend, extra cameras are allowed to incorporate into the pipeline for meeting different requirements. The system realizes object tracking by estimating the pose of the fiducial platonic solid (FPS) which can be fixed onto the surface of the target object. Different sizes and shapes of the platonic solids are allowed to combine with each other to adapt to different application scenarios, this strategy provides enormous flexibility and applicability to our system. In order to track the motion of the fiducial platonic solid accurately, a robust algorithm that combines the fiducial constraint and the statistical constraint is introduced, which is able to handle illumination changes, motion blur and partial occlusion. We evaluate the performance of the proposed approach with qualitative and quantitative experiments, in addition, a couple of mixed reality (MR) applications are developed for demonstrating the effectiveness of the system.
SP  - 4454
EP  - 4462
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3475597
ER  - 

TY  - JOUR
AU  - Rolnick, David; Donti, Priya L.; Kaack, Lynn H.; Kochanski, Kelly; Lacoste, Alexandre; Sankaran, Kris; Ross, Andrew Slavin; Milojevic-Dupont, Nikola; Jaques, Natasha; Waldman-Brown, Anna; Luccioni, Alexandra Sasha; Maharaj, Tegan; Sherwin, Evan D.; Mukkavilli, S. Karthik; Kording, Konrad P.; Gomes, Carla P.; Ng, Andrew Y.; Hassabis, Demis; Platt, John C.; Creutzig, Felix; Chayes, Jennifer; Bengio, Yoshua
TI  - Tackling Climate Change with Machine Learning
PY  - 2022
AB  - <jats:p>Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.</jats:p>
SP  - 1
EP  - 96
JF  - ACM Computing Surveys
VL  - 55
IS  - 2
PB  - 
DO  - 10.1145/3485128
ER  - 

TY  - NA
AU  - Burns, Andrea; Arsan, Deniz; Agrawal, Sanjna; Kumar, Ranjitha; Saenko, Kate; Plummer, Bryan A.
TI  - Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task Feasibility in Interactive Visual Environments.
PY  - 2021
AB  - In recent years, vision-language research has shifted to study tasks which require more complex reasoning, such as interactive question answering, visual common sense reasoning, and question-answer plausibility prediction. However, the datasets used for these problems fail to capture the complexity of real inputs and multimodal environments, such as ambiguous natural language requests and diverse digital domains. We introduce Mobile app Tasks with Iterative Feedback (MoTIF), a dataset with natural language commands for the greatest number of interactive environments to date. MoTIF is the first to contain natural language requests for interactive environments that are not satisfiable, and we obtain follow-up questions on this subset to enable research on task uncertainty resolution. We perform initial feasibility classification experiments and only reach an F1 score of 37.3, verifying the need for richer vision-language representations and improved architectures to reason about task feasibility.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Alves, Luís M.; Gajic, Dusan B.; Henriques, Pedro Rangel; Ivancevic, Vladimir; Ivkovic, Vladimir; Lalić, Maksim; Luković, Ivan; Pereira, Maria João Varanda; Popov, Srdan; Tavares, Paula Correia
TI  - C Tutor usage in relation to student achievement and progress: A study of introductory programming courses in Portugal and Serbia
PY  - 2020
AB  - Previous research studies on introductory programming courses in engineering education in Portugal and Serbia have indicated that although high motivation and high expectations seem to be reported by students, many students may fail the course. This prompted a further inquiry into student attitudes, behavior, and achievement, and it also led to the introduction of C Tutor, a widely known program visualization tool, into courses in both countries. As a result, in the present study, self‐reported student achievement (grades), self‐reported student progress (knowledge improvement and confidence), and self‐reported usage and helpfulness of C Tutor were investigated. Anonymous data about students and their experience in the course, which also included the usage of C Tutor, were collected in a survey in Portugal and Serbia. Quantitative methods, including descriptive statistics, clustering, statistical testing of independence, and partial correlation analysis, were applied in analyses of survey data. The distribution of grades differed between the two countries, but overall attitudes were similar. Various uncovered patterns involving student attitudes and usage of C Tutor may serve as a starting point for new research studies.info:eu-repo/semantics/publishedVersio
SP  - 1058
EP  - 1071
JF  - Computer Applications in Engineering Education
VL  - 28
IS  - 5
PB  - 
DO  - 10.1002/cae.22278
ER  - 

TY  - NA
AU  - Henley, Austin Z.; Ball, Julian; Klein, Benjamin; Rutter, Aiden; Lee, Dylan
TI  - ICSE (SEET) - An Inquisitive Code Editor for Addressing Novice Programmers' Misconceptions of Program Behavior
PY  - 2021
AB  - Novice programmers face numerous barriers while attempting to learn how to code that may deter them from pursuing a computer science degree or career in software development. In this work, we propose a tool concept to address the particularly challenging barrier of novice programmers holding misconceptions about how their code behaves. Specifically, the concept involves an inquisitive code editor that: (1) identifies misconceptions by periodically prompting the novice programmer with questions about their program's behavior, (2) corrects the misconceptions by generating explanations based on the program's actual behavior, and (3) prevents further misconceptions by inserting test code and utilizing other educational resources. We have implemented portions of the concept as plugins for the Atom code editor and conducted informal surveys with students and instructors. Next steps include deploying the tool prototype to students enrolled in introductory programming courses.
SP  - 165
EP  - 170
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse-seet52601.2021.00026
ER  - 

TY  - CHAP
AU  - Schwandt, Alexander; Yuschenko, Arkady
TI  - Intuitive Industrial Robot Programming Interface with Augmented Reality Environment
PY  - 2020
AB  - The integration costs of industrial robots into small medium-sized enterprises (SMEs) are nowadays one of the main obstacles, to make automated solutions in this sector profitable. An essential role plays the human-robot interface (HRI), which is in robots standard implementation for averagely trained technicians overly complicated and time consuming. A new intuitive HRI, based on augmented reality, is presented to shorten programming phase during in commissioning of robot cells in the production line. The proposed system was implemented on a portable computer with a head-mounted camera and wearable display for the operator. In demonstrative scenery was with the presented hardware setup shown, how basic visualization and simulation functionalities and intuitive programming procedures of industrial robots in the AR environment could be implemented. In addition to the trajectory visualization in an AR environment, the robot’s gripper pose simulation was shown, as it would be visible in operators’ field of view. With the usage of a 6D camera tracked AR-Stylus, combined with the projection of the robots’ gripper 3D model in the AR environment, an elevated level of operator integration into AR environment can be achieved.
SP  - 231
EP  - 241
JF  - Robotics: Industry 4.0 Issues & New Intelligent Control Paradigms
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-37841-7_19
ER  - 

TY  - NA
AU  - Arroyo, Diego Martin; Postels, Janis; Tombari, Federico
TI  - CVPR - Variational Transformer Networks for Layout Generation
PY  - 2021
AB  - Generative models able to synthesize layouts of different kinds (e.g. documents, user interfaces or furniture arrangements) are a useful tool to aid design processes and as a first step in the generation of synthetic data, among other tasks. We exploit the properties of self-attention layers to capture high level relationships between elements in a layout, and use these as the building blocks of the well-known Variational Autoencoder (VAE) formulation. Our proposed Variational Transformer Network (VTN) is capable of learning margins, alignments and other global design rules without explicit supervision. Layouts sampled from our model have a high degree of resemblance to the training data, while demonstrating appealing diversity. In an extensive evaluation on publicly available benchmarks for different layout types VTNs achieve state-of-the-art diversity and perceptual quality. Additionally, we show the capabilities of this method as part of a document layout detection pipeline.
SP  - 13642
EP  - 13652
JF  - 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr46437.2021.01343
ER  - 

TY  - JOUR
AU  - Sulír, Matúš; Bacikova, Michaela; Chodarev, Sergej; Porubän, Jaroslav
TI  - Visual augmentation of source code editors: A systematic mapping study
PY  - 2018
AB  - Abstract Source code written in textual programming languages is typically edited in integrated development environments (IDEs) or specialized code editors. These tools often display various visual items, such as icons, color highlights or more advanced graphical overlays directly in the main editable source code view. We call such visualizations source code editor augmentation. In this paper, we present a first systematic mapping study of source code editor augmentation tools and approaches. We manually reviewed the metadata of 5553 articles published during the last twenty years in two phases – keyword search and references search. The result is a list of 103 relevant articles and a taxonomy of source code editor augmentation tools with seven dimensions, which we used to categorize the resulting list of the surveyed articles. We also provide the definition of the term source code editor augmentation, along with a brief overview of historical development and augmentations available in current industrial IDEs.
SP  - 46
EP  - 59
JF  - Journal of Visual Languages & Computing
VL  - 49
IS  - NA
PB  - 
DO  - 10.1016/j.jvlc.2018.10.001
ER  - 

TY  - JOUR
AU  - Chen, Chunyang; Feng, Sidong; Liu, Zhengyang; Xing, Zhenchang; Zhao, Shengdong
TI  - From Lost to Found: Discover Missing UI Design Semantics through Recovering Missing Tags
PY  - 2020
AB  - Design sharing sites provide UI designers with a platform to share their works and also an opportunity to get inspiration from others' designs. To facilitate management and search of millions of UI design images, many design sharing sites adopt collaborative tagging systems by distributing the work of categorization to the community. However, designers often do not know how to properly tag one design image with compact textual description, resulting in unclear, incomplete, and inconsistent tags for uploaded examples which impede retrieval, according to our empirical study and interview with four professional designers. Based on a deep neural network, we introduce a novel approach for encoding both the visual and textual information to recover the missing tags for existing UI examples so that they can be more easily found by text queries. We achieve 82.72% accuracy in the tag prediction. Through a simulation test of 5 queries, our system on average returns hundreds more results than the default Dribbble search, leading to better relatedness, diversity and satisfaction.
SP  - 123
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW2
PB  - 
DO  - 10.1145/3415194
ER  - 

TY  - NA
AU  - Deka, Biplab; Huang, Zifeng; Franzen, Chad; Nichols, Jeffrey; Li, Yang; Kumar, Ranjitha
TI  - UIST - ZIPT: Zero-Integration Performance Testing of Mobile App Designs
PY  - 2017
AB  - To evaluate the performance of mobile app designs, designers and researchers employ techniques such as A/B, usability, and analytics-driven testing. While these are all useful strategies for evaluating known designs, comparing many divergent solutions to identify the most performant remains a costly and difficult problem. This paper introduces a design performance testing approach that leverages existing app implementations and crowd workers to enable comparative testing at scale. This approach is manifest in ZIPT, a zero-integration performance testing platform that allows designers to collect detailed design and interaction data over any Android app -- including apps they do not own and did not build. Designers can deploy scripted tests via ZIPT to collect aggregate user performance metrics (e.g., completion rate, time on task) and qualitative feedback over third-party apps. Through case studies, we demonstrate that designers can use ZIPT's aggregate data and visualizations to understand the relative performance of interaction patterns found in the wild, and identify usability issues in existing Android apps.
SP  - 727
EP  - 736
JF  - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3126594.3126647
ER  - 

TY  - CHAP
AU  - Bakaev, Maxim; Heil, Sebastian; Chirkov, Leonid; Gaedke, Martin
TI  - Benchmarking Neural Networks-Based Approaches for Predicting Visual Perception of User Interfaces
PY  - 2022
AB  - NA
SP  - 217
EP  - 231
JF  - Artificial Intelligence in HCI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05643-7_14
ER  - 

TY  - NA
AU  - Wu, Jason; Zhang, Xiaoyi; Nichols, Jeffrey; Bigham, Jeffrey P.
TI  - UIST - Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots
PY  - 2021
AB  - Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata. A first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions. In this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot. We describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance. In an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%). Finally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots.
SP  - 470
EP  - 483
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474763
ER  - 

TY  - JOUR
AU  - Leiva, Luis A.; Hota, Asutosh; Oulasvirta, Antti
TI  - Describing UI Screenshots in Natural Language
PY  - 2022
AB  - <jats:p>Being able to describe any user interface (UI) screenshot in natural language can promote understanding of the main purpose of the UI, yet currently it cannot be accomplished with state-of-the-art captioning systems. We introduce XUI, a novel method inspired by the global precedence effect to create informative descriptions of UIs, starting with an overview and then providing fine-grained descriptions about the most salient elements. XUI builds upon computational models for topic classification, visual saliency prediction, and natural language generation (NLG). XUI provides descriptions with up to three different granularity levels that, together, describe what is in the interface and what the user can do with it. We found that XUI descriptions are highly readable, are perceived to accurately describe the UI, and score similarly to human-generated UI descriptions. XUI is available as open-source software.</jats:p> <jats:p />
SP  - 1
EP  - 28
JF  - ACM Transactions on Intelligent Systems and Technology
VL  - 14
IS  - 1
PB  - 
DO  - 10.1145/3564702
ER  - 

TY  - NA
AU  - Chu, Eric
TI  - Exploring design alternatives in game development engines using visual programming
PY  - 2019
AB  - We present BPAlt ??? a system which allows game developers to create and manage alternatives for Unreal Engine???s Blueprints Visual Scripting System. BPAlt allows the user to create, save, organize and swap Blueprint alternatives for rapid testing and experimentation. We conducted a user study with 10 moderately skilled participants where we compared BPAlt to Unreal Engine alone for prototyping alternatives of game objects and mechanics in four different games. We found evidence that supporting alternatives with BPAlt is beneficial in the game developers??? workflow. In response to the results of the user study we implemented new features for selectively merging parts of one alternative Blueprint to another. We also implemented an interface for alternative scenarios.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Shih, Chi-Sheng Daniel; Chen, Hao-Yu
TI  - RACS - An Extrinsic Depth Camera Calibration Method for Narrow Field of View Color Camera
PY  - 2020
AB  - This work designs and implements a calibration method between a narrow field of view camera and a depth camera in an endoscope-like scenario. An endoscopy-like scenario has a limited specular reflective surface, a camera with a narrow field of view. Instead of pushing the accuracy of the target marker with low-resolution data, we design a new loss function, which utilizes all of the three dimensions points of the checkerboard measured with the depth camera, and calculates the distance between projected 3D positions onto 2D image surface and the color image. The final re-projected error is improved to be less than 1 millimeters on average.
SP  - 247
EP  - 254
JF  - Proceedings of the International Conference on Research in Adaptive and Convergent Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3400286.3418251
ER  - 

TY  - NA
AU  - Zhang, Xiaoyi
TI  - Runtime Repair and Enhancement of Mobile App Accessibility
PY  - 2019
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ramanujam, Murali; Madhyastha, Harsha V.; Netravali, Ravi
TI  - MobiSys - Marauder: synergized caching and prefetching for low-risk mobile app acceleration
PY  - 2021
AB  - Low interaction response times are crucial to the experience that mobile apps provide for their users. Unfortunately, existing strategies to alleviate the network latencies that hinder app responsiveness fall short in practice. In particular, caching is plagued by challenges in setting expiration times that match when a resource's content changes, while prefetching hinges on accurate predictions of user behavior that have proven elusive. We present Marauder, a system that synergizes caching and prefetching to improve the speedups achieved by each technique while avoiding their inherent limitations. Key to Marauder is our observation that, like web pages, apps handle interactions by downloading and parsing structured text resources that entirely list (i.e., without needing to consult app binaries) the set of other resources to load. Building on this, Marauder introduces two low-risk optimizations directly from the app's cache. First, guided by cached text files, Marauder prefetches referenced resources during an already-triggered interaction. Second, to improve the efficacy of cached content, Marauder judiciously prefetches about-to-expire resources, extending cache lives for unchanged resources, and downloading updates for lightweight (but crucial) text files. Across a wide range of apps, live networks, interaction traces, and phones, Marauder reduces median and 90th percentile interaction response times by 27.4% and 43.5%, while increasing data usage by only 18%.
SP  - 350
EP  - 362
JF  - Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458864.3466866
ER  - 

TY  - NA
AU  - Chen, Fanglin; Hong, Jason
TI  - UbiComp/ISWC Adjunct - Personal bits: mining interaction traces for personalized task intelligence
PY  - 2019
AB  - As we work, play, shop, and communicate in digital interfaces, we continuously generate traces of information. To turn such noisy sources of personal data into actual insight, my research introduces Personal Bits, a service that enables personalized task support in various kinds of information tasks such as instant message handling, information retrieval, and text entry. Personal Bits mines a user's interaction traces with web apps and native mobile apps and extracts task-centric entities. I present three example apps for Personal Bits: Deja Wu, MessageOnTap, and ContextBoard, to address inefficiencies presented in these information tasks. Personal Bits acts as the central nexus for intelligence between apps and interaction traces, making it easy for apps to acquire personally relevant task entities in fine granularity.
SP  - 358
EP  - 362
JF  - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3341162.3349310
ER  - 

TY  - NA
AU  - Cai, Haipeng; Pant, Shiv Raj; Li, Wen
TI  - ESEC/SIGSOFT FSE - Towards learning visual semantics
PY  - 2020
AB  - We envision visual semantics learning (VSL), a novel methodology that derives high-level functional description of given software from its visual (graphical) outputs. By visual semantics, we mean the semantic description about the software’s behaviors that are exhibited in its visual outputs. VSL works by composing this description based on visual element labels extracted from these outputs through image/video understanding and natural language generation. The result of VSL can then support tasks that may benefit from the high-level functional description. Just like a developer relies on program understanding to conduct many of such tasks, automatically understanding software (i.e., by machine rather than by human developers) is necessary to eventually enable fully automated software engineering. Apparently, VSL only works with software that does produce visual outputs that meaningfully demonstrate the software’s behaviors. Nevertheless, learning visual semantics would be a useful first step towards automated software understanding. We outline the design of our approach to VSL and present early results demonstrating its merits.
SP  - 1537
EP  - 1540
JF  - Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3368089.3417040
ER  - 

TY  - NA
AU  - Zheng, Yufeng; Park, Seonwook; Zhang, Xucong; De Mello, Shalini; Hilliges, Otmar
TI  - Self-Learning Transformations for Improving Gaze and Head Redirection.
PY  - 2020
AB  - Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, controlling specific aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under fine-grained control over eye gaze and head orientation angles. This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orientation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: this https URL
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yang, Bo; Xing, Zhenchang; Xia, Xin; Chen, Chunyang; Ye, Deheng; Li, Shanping
TI  - ICSE - Don't Do That!: Hunting Down Visual Design Smells in Complex UIs against Design Guidelines
PY  - 2021
AB  - Just like code smells in source code, UI design has visual design smells. We study 93 don't-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don't-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component metadata, typography, iconography, color, and edge) for detecting the violation of diverse don't-guidelines in Material Design. The detection accuracy of UIS-Hunter is high (precision=0.81, recall=0.90) on the 60,756 UIs of 9,286 apps. We build a guideline gallery with real-world UI design smells that UIS-Hunter detects for developers to learn the best Material Design practices. Our user studies show that UIS-Hunter is more effective than manual detection of UI design smells, and the UI design smells that are detected by UIS-Hunter have severely negative impacts on app users.
SP  - 761
EP  - 772
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse43902.2021.00075
ER  - 

TY  - JOUR
AU  - Rahmadi, Agyl Ardi; Sudaryanto, Aris
TI  - Visual Recognition Of Graphical User Interface Components Using Deep Learning Technique
PY  - 2020
AB  - Graphical User Interface (GUI) building in software development is a process which ideally need to go through several steps. Those steps in the process start from idea or rough sketch of the GUI, then refined into visual design, implemented in coding or prototype, and finally evaluated for its function and usability to discover design problem and to get feedback from users. Those steps repeated until the GUI considered satisfactory or acceptable by the user. Computer vision technique has been researched and developed to make the process faster and easier; for example generating code for implementation, or automatic GUI testing using component images. But among those techniques, there are still few for usability testing purpose. This preliminary research attempted to make the foundation for usability testing using computer vision technique by built minimalist dataset which has images of various GUI components and used the dataset in deep learning experiment for GUI components visual recognition. The experiment results showed deep learning technique suitable for the intended task, with accuracy of 95% for recognition of two different types of components, and accuracy of 72% for six different types of component.
SP  - 35
EP  - 45
JF  - Jurnal Ilmu Komputer dan Informasi
VL  - 13
IS  - 1
PB  - 
DO  - 10.21609/jiki.v13i1.845
ER  - 

TY  - NA
AU  - Kuznetsov, Konstantin; Fu, Chen; Gao, Song; Jansen, David N.; Zhang, Lijun; Zeller, Andreas
TI  - ESEC/SIGSOFT FSE - Frontmatter: mining Android user interfaces at scale
PY  - 2021
AB  - We introduce Frontmatter: the largest open-access dataset containing user interface models of about 160,000 Android apps. Frontmatter opens the door for comprehensive mining of mobile user interfaces, jumpstarting empirical research at a large scale, addressing questions such as "How many travel apps require registration?", "Which apps do not follow accessibility guidelines?", "Does the user interface correspond to the description?", and many more. The Frontmatter UI analysis tool and the Frontmatter dataset are available under an open-source license.
SP  - 1580
EP  - 1584
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3473125
ER  - 

TY  - JOUR
AU  - Wu, Yong; Li, Gongyang; Liu, Zhi; Huang, Mengke; Wang, Yang
TI  - Gaze Estimation via Modulation-Based Adaptive Network With Auxiliary Self-Learning
PY  - 2022
AB  - Given a face image, most of previous works in gaze estimation infer the gaze via a well-trained model with supervised training. However, the distribution of test data may be very different compared to that of training data since samples might be corrupted in real-world scenarios ( <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">e.g</i> ., taking a photo in strong light). This will lead to a gap between source domain ( <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">i.e</i> ., training data) and target domain ( <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">i.e</i> ., test data). In this paper, we first introduce self-supervised learning into our method for addressing challenging situations in gaze estimation. Moreover, existing appearance-based gaze estimation methods focus on directing towards the development of powerful regressors, which mainly utilize face and eye images simultaneously or face (eye) images only. However, the problem of inter cues between face and eye features has been largely overlooked. To this end, we propose a novel Modulation-based Adaptive Network (MANet) for gaze estimation, which uses high-level knowledge to filter the distractive information and bridges the intrinsic relationship between face and eye features. Further, we combine self-supervised learning and MANet to learn to adapt to challenging cases, such as abnormal lighting conditions and poor-quality images, by minimizing a self-supervised loss and a supervised loss jointly. The experimental results on several datasets demonstrate the effectiveness of our proposed approach with a real-time speed of 900 <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">fps</i> on a PC with an NVIDIA Titan RTX GPU.
SP  - 5510
EP  - 5520
JF  - IEEE Transactions on Circuits and Systems for Video Technology
VL  - 32
IS  - 8
PB  - 
DO  - 10.1109/tcsvt.2022.3152800
ER  - 

TY  - NA
AU  - Ferdowsifard, Kasra; Ordookhanians, Allen; Peleg, Hila; Lerner, Sorin; Polikarpova, Nadia
TI  - UIST - Small-Step Live Programming by Example
PY  - 2020
AB  - Live programming is a paradigm in which the programming environment continually displays runtime values. Program synthesis is a technique that can generate programs or program snippets from examples. \deltextThis paper presents a new programming paradigm called Synthesis-Aided Live Programming that combines these two prior ideas in a synergistic way. When using Synthesis-Aided Live Programming, programmers can change the runtime values displayed by the live \addtextPrevious works that combine the two have taken a holistic approach to the way examples describe the behavior of functions and programs. This paper presents a new programming paradigm called Small-Step Live Programming by Example that lets the user apply Programming by Example locally. When using Small-Step Live Programming by Example, programmers can change the runtime values displayed by the live visualization to generate local program snippets. % Live programming and program % synthesis work perfectly together because the live programming environment % reifies values, which makes it easy for programmers to provide the examples % needed by the synthesizer. We implemented this new paradigm in a tool called \toolname, and performed a user study on $13$ programmers. Our study finds that Small-Step Live Programming by Example with \toolname helps users solve harder problems faster, and that for certain types of queries, users prefer it to searching the web. Additionally, we identify the \usersynthgap, in which users' mental models of the tool do not match its ability, and needs to be taken into account in the design of future synthesis tools.
SP  - 614
EP  - 626
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415869
ER  - 

TY  - NA
AU  - Huang, Forrest; Li, Gang; Zhou, Xin; Canny, John; Li, Yang
TI  - Creating User Interface Mock-ups from High-Level Text Descriptions with Deep-Learning Models.
PY  - 2021
AB  - The design process of user interfaces (UIs) often begins with articulating high-level design goals. Translating these high-level design goals into concrete design mock-ups, however, requires extensive effort and UI design expertise. To facilitate this process for app designers and developers, we introduce three deep-learning techniques to create low-fidelity UI mock-ups from a natural language phrase that describes the high-level design goal (e.g. "pop up displaying an image and other options"). In particular, we contribute two retrieval-based methods and one generative method, as well as pre-processing and post-processing techniques to ensure the quality of the created UI mock-ups. We quantitatively and qualitatively compare and contrast each method's ability in suggesting coherent, diverse and relevant UI design mock-ups. We further evaluate these methods with 15 professional UI designers and practitioners to understand each method's advantages and disadvantages. The designers responded positively to the potential of these methods for assisting the design process.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Swearngin, Amanda; Li, Yang
TI  - Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learning.
PY  - 2019
AB  - Tapping is an immensely important gesture in mobile touchscreen interfaces, yet people still frequently are required to learn which elements are tappable through trial and error. Predicting human behavior for this everyday gesture can help mobile app designers understand an important aspect of the usability of their apps without having to run a user study. In this paper, we present an approach for modeling tappability of mobile interfaces at scale. We conducted large-scale data collection of interface tappability over a rich set of mobile apps using crowdsourcing and computationally investigated a variety of signifiers that people use to distinguish tappable versus not-tappable elements. Based on the dataset, we developed and trained a deep neural network that predicts how likely a user will perceive an interface element as tappable versus not tappable. Using the trained tappability model, we developed TapShoe, a tool that automatically diagnoses mismatches between the tappability of each element as perceived by a human user---predicted by our model, and the intended or actual tappable state of the element specified by the developer or designer. Our model achieved reasonable accuracy: mean precision 90.2\% and recall 87.0\%, in matching human perception on identifying tappable UI elements. The tappability model and TapShoe were well received by designers via an informal evaluation with 7 professional interaction designers.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Liu, Zhe; Chen, Chunyang; Wang, Junjie; Huang, Yuekai; Hu, Jun; Wang, Qing
TI  - Owl Eyes: Spotting UI Display Issues via Visual Understanding
PY  - 2020
AB  - Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues and develop a heuristics-based data augmentation method for boosting the performance of our OwlEye. The evaluation demonstrates that our OwlEye can achieve 85% precision and 84% recall in detecting UI display issues, and 90% accuracy in localizing these issues. We also evaluate OwlEye with popular Android apps on Google Play and F-droid, and successfully uncover 57 previously-undetected UI display issues with 26 of them being confirmed or fixed so far.
SP  - 398
EP  - 409
JF  - Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3324884.3416547
ER  - 

TY  - NA
AU  - Heck, Larry; Heck, Simon
TI  - Zero-Shot Visual Slot Filling as Question Answering.
PY  - 2020
AB  - This paper presents a new approach to visual zero-shot slot filling. The approach extends previous approaches by reformulating the slot filling task as Question Answering. Slot tags are converted to rich natural language questions that capture the semantics of visual information and lexical text on the GUI screen. These questions are paired with the user's utterance and slots are extracted from the utterance using a state-of-the-art ALBERT-based Question Answering system trained on the Stanford Question Answering dataset (SQuaD2). An approach to further refine the model with multi-task training is presented. The multi-task approach facilitates the incorporation of a large number of successive refinements and transfer learning across similar tasks. A new Visual Slot dataset and a visual extension of the popular ATIS dataset is introduced to support research and experimentation on visual slot filling. Results show F1 scores between 0.52 and 0.60 on the Visual Slot and ATIS datasets with no training data (zero-shot).
SP  - NA
EP  - NA
JF  - arXiv: Artificial Intelligence
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chen, Xiang 'Anthony'; Tao, Ye; Wang, Guanyun; Kang, Runchang; Grossman, Tovi; Coros, Stelian; Hudson, Scott E.
TI  - CHI - Forte: User-Driven Generative Design
PY  - 2018
AB  - Low-cost fabrication machines (e.g., 3D printers) offer the promise of creating custom-designed objects by a range of users. To maximize performance, generative design methods such as topology optimization can automatically optimize properties of a design based on high-level specifications. Though promising, such methods require people to map their design ideas--often unintuitively--to a small number of mathematical input parameters, and the relationship between those parameters and a generated design is often unclear, making it difficult to iterate a design. We present Forte, a sketch-based, real-time interactive tool for people to directly express and iterate on their designs via 2D topology optimization. Users can ask the system to add structures, provide a variation with better performance, or optimize internal material layouts. Users can globally control how much to 'deviate' from the initial sketch, or perform local suggestive editing, which interactively prompts the system to update based on the new information. Design sessions with 10 participants demonstrate that Forte empowers designers to create and explore a range of optimized designs with custom forms and styles.
SP  - 496
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174070
ER  - 

TY  - NA
AU  - Toshikatu, Nakamura; Sato, Toshiki
TI  - UIST (Adjunct Volume) - UPLIGHT: A Novel Portable Game Device with Omnidirectional Projection Display
PY  - 2021
AB  - We hypothesized that the act of actively moving one’s body to see the hidden parts of a sphere, cube, or any other structure with height and sides would be entertaining. In this paper, we propose a novel portable game device with omnidirectional display called “UPLIGHT,” which was created by combining the element of entertainment with the play style of a portable game device. We also describe the design of a prototype and a playable game application that we developed to achieve this interaction.
SP  - 142
EP  - 144
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480207
ER  - 

TY  - NA
AU  - Chen, Jieshan; Swearngin, Amanda; Wu, Jason; Barik, Titus; Nichols, Jeffrey; Zhang, Xiaoyi
TI  - Towards Complete Icon Labeling in Mobile Applications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502073
ER  - 

TY  - JOUR
AU  - Bajammal, Mohammad; Stocco, Andrea; Mazinanian, Davood; Mesbah, Ali
TI  - A Survey on the Use of Computer Vision to Improve Software Engineering Tasks
PY  - 2020
AB  - Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research.
SP  - 1
EP  - 1
JF  - IEEE Transactions on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mott, Martez E.; Wobbrock, Jacob O.
TI  - CHI - Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments
PY  - 2019
AB  - We present Cluster Touch, a combined user-independent and user-specific touch offset model that improves the accuracy of touch input on smartphones for people with motor impairments, and for people experiencing situational impairments while walking. Cluster Touch combines touch examples from multiple users to create a shared user-independent touch model, which is then updated with touch examples provided by an individual user to make it user-specific. Owing to this combination, Cluster Touch allows people to quickly improve the accuracy of their smartphones by providing only 20 touch examples. In a user study with 12 people with motor impairments and 12 people without motor impairments, but who were walking, Cluster Touch improved touch accuracy by 14.65% for the former group and 6.81% for the latter group over the native touch sensor. Furthermore, in an offline analysis of existing mobile interfaces, Cluster Touch improved touch accuracy by 8.21% and 4.84% over the native touch sensor for the two user groups, respectively.
SP  - 27
EP  - NA
JF  - Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3290605.3300257
ER  - 

TY  - NA
AU  - Feng, Sidong; Chen, Chunyang; Xing, Zhenchang
TI  - Gallery D.C
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3510454.3516873
ER  - 

TY  - NA
AU  - Ivanov, Alexander; Jacob, Christian
TI  - GECCO (Companion) - EvoIsland: immersive interactive evolutionary 3D modelling
PY  - 2019
AB  - We present EvoIsland, an interactive evolutionary augmented reality interface for parametric 3D OpenSCAD models. Our mobile prototype enables content creators to explore the full range of design possibilities encoded in a model's source code through the combination and separation of hexagonal evolutionary tiles embedded with genetic data. As these tiles are grouped into islands, localized clusters of design populations emerge for creators to explore. Interactions that take place within our EvoIsland prototype provide content creators with a novel approach for shaping evolutionary populations in an immersive environment.
SP  - 373
EP  - 374
JF  - Proceedings of the Genetic and Evolutionary Computation Conference Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3319619.3321979
ER  - 

TY  - NA
AU  - Englmeier, David; Sajko, Wanja; Butz, Andreas
TI  - VR - Spherical World in Miniature: Exploring the Tiny Planets Metaphor for Discrete Locomotion in Virtual Reality
PY  - 2021
AB  - We explore the concept of a Spherical World in Miniature (SWIM) for discrete locomotion in Virtual Reality (VR). A SWIM wraps a planar WIM around a physically embodied sphere and thereby implements the metaphor of a tangible Tiny Planet that can be rotated and moved, enabling scrolling, scaling, and avatar teleportation. The scaling factor is set according to the sphere's distance from the head-mounted display (HMD), while rotation moves the current viewing window. Teleportation is triggered with a dwell time when looking at the sphere and keeping it still. In a lab study (N=20), we compare our SWIM implementation to a planar WIM with an established VR controller technique using physical buttons. We test both concepts in a navigation task and also investigate the effects of two different screen sizes. Our results show that the SWIM, despite its less direct geometrical transformation, performed superior in most evaluations. It outperformed the planar WIM not only in terms of task completion time (TCT) and accuracy but also in subjective ratings.
SP  - 345
EP  - 352
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00057
ER  - 

TY  - JOUR
AU  - Nakazawa, Atsushi; Mitsuzumi, Yu; Watanabe, Yuki; Kurazume, Ryo; Yoshikawa, Sakiko; Honda, Miwako
TI  - First-person Video Analysis for Evaluating Skill Level in the Humanitude Tender-Care Technique
PY  - 2019
AB  - In this paper, we describe a wearable first-person video (FPV) analysis system for evaluating the skill levels of caregivers. This is a part of our project that aims to quantize and analyze the tender-care technique known as Humanitude by using wearable sensing and AI technology devices. Using our system, caregivers can evaluate and elevate their care levels by themselves. From the FPVs of care sessions taken by wearable cameras worn by caregivers, we obtained the 3D facial distance, pose and eye-contact states between caregivers and receivers by using facial landmark detection and deep neural network (DNN)-based eye contact detection. We applied statistical analysis to these features and developed algorithms that provide scores for tender-care skill. In experiments, we first evaluated the performance of our DNN-based eye contact detection by using eye contact datasets prepared from YouTube videos and FPVs that assume conversational scenes. We then performed skill evaluations by using Humanitude training scenes involving three novice caregivers, two Humanitude experts and seven middle-level students. The results showed that our eye contact detection outperformed existing methods and that our skill evaluations can estimate the care skill levels.
SP  - 103
EP  - 118
JF  - Journal of Intelligent & Robotic Systems
VL  - 98
IS  - 1
PB  - 
DO  - 10.1007/s10846-019-01052-8
ER  - 

TY  - NA
AU  - Monch, Kim
TI  - Time-Travel Debugging with Visualization of Data-Structures Based on Instrumentation
PY  - 2022
AB  - Although a large number of different debugging tools are available, very few CS students use them in programming courses. Instead, they try to understand and correct their code by adding countless print statements. We identified several reasons for these observations: (1) novice programmers find the standard debugging tools too complicated, especially the representation of the current program state, (2) they find it difficult to trace a problem back to its origin, and (3) it is not yet possible to provide them with a visualization of data structures that corresponds to the representation used in the classroom. We address these problems with BugVis, an alternative debugging approach. We describe the concept and basic ideas below, since BugVis has not yet been fully realized.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833002
ER  - 

TY  - JOUR
AU  - Velazco-Garcia, Jose D.; Navkar, Nikhil V.; Balakrishnan, Shidin; Younes, Georges; Abinahed, Julien; Al-Rumaihi, Khalid; Darweesh, Adham; Elakkad, Mohamed Soliman Mohamed; Al-Ansari, Abdulla; Christoforou, Eftychios G.; Karkoub, Mansour; Leiss, Ernst L.; Tsiamyrtzis, Panagiotis; Tsekos, Nikolaos V.
TI  - Evaluation of how users interface with holographic augmented reality surgical scenes: Interactive planning MR-Guided prostate biopsies
PY  - 2021
AB  - Background User interfaces play a vital role in the planning and execution of an interventional procedure. The objective of this study is to investigate the effect of using different user interfaces for planning transrectal robot-assisted MR-guided prostate biopsy (MRgPBx) in an augmented reality (AR) environment. Method End-user studies were conducted by simulating an MRgPBx system with end- and side-firing modes. The information from the system to the operator was rendered on HoloLens as an output interface. Joystick, mouse/keyboard, and holographic menus were used as input interfaces to the system. Results The studies indicated that using a joystick improved the interactive capacity and enabled operator to plan MRgPBx in less time. It efficiently captures the operator's commands to manipulate the augmented environment representing the state of MRgPBx system. Conclusions The study demonstrates an alternative to conventional input interfaces to interact and manipulate an AR environment within the context of MRgPBx planning.
SP  - 1
EP  - 13
JF  - The international journal of medical robotics + computer assisted surgery : MRCAS
VL  - 17
IS  - 5
PB  - 
DO  - 10.1002/rcs.2290
ER  - 

TY  - NA
AU  - Lemley, Joseph; Kar, Anuradha; Drimbarean, Alexandru; Corcoran, Peter
TI  - Efficient CNN Implementation for Eye-Gaze Estimation on Low-Power/Low-Quality Consumer Imaging Systems.
PY  - 2018
AB  - Accurate and efficient eye gaze estimation is important for emerging consumer electronic systems such as driver monitoring systems and novel user interfaces. Such systems are required to operate reliably in difficult, unconstrained environments with low power consumption and at minimal cost. In this paper a new hardware friendly, convolutional neural network model with minimal computational requirements is introduced and assessed for efficient appearance-based gaze estimation. The model is tested and compared against existing appearance based CNN approaches, achieving better eye gaze accuracy with significantly fewer computational requirements. A brief updated literature review is also provided.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Cheng, Jing; Zhao, Jiayi; Xu, Weidong; Zhang, Tao; Xue, Feng; Liu, Shaoying
TI  - Semantic Similarity-Based Mobile Application Isomorphic Graphical User Interface Identification
PY  - 2023
AB  - <jats:p>Applying robots to mobile application testing is an emerging approach to automated black-box testing. The key to supporting automated robot testing is the efficient modeling of GUI elements. Since the application under testing often contains a large number of similar GUIs, the GUI model obtained often contains many redundant nodes. This causes the state space explosion of GUI models which has a serious effect on the efficiency of GUI testing. Hence, how to accurately identify isomorphic GUIs and construct quasi-concise GUI models are key challenges faced today. We thus propose a semantic similarity-based approach to identifying isomorphic GUIs for mobile applications. Using this approach, the information of GUI elements is first identified by deep learning network models, then, the GUI structure model feature vector and the semantic model feature vector are extracted and finally merged to generate a GUI embedding vector with semantic information. Finally, the isomorphic GUIs are identified by cosine similarity. Then, three experiments are conducted to verify the generalizability and effectiveness of the method. The experiments demonstrate that the proposed method can accurately identify isomorphic GUIs and shows high compatibility in terms of cross-platform and cross-device applications.</jats:p>
SP  - 527
EP  - NA
JF  - Mathematics
VL  - 11
IS  - 3
PB  - 
DO  - 10.3390/math11030527
ER  - 

TY  - CONF
AU  - Cho, Youngjun
TI  - CHI - Rethinking Eye-blink: Assessing Task Difficulty through Physiological Representation of Spontaneous Blinking
PY  - 2021
AB  - Continuous assessment of task difficulty and mental workload is essential in improving the usability and accessibility of interactive systems. Eye tracking data has often been investigated to achieve this ability, with reports on the limited role of standard blink metrics. Here, we propose a new approach to the analysis of eye-blink responses for automated estimation of task difficulty. The core module is a time-frequency representation of eye-blink, which aims to capture the richness of information reflected on blinking. In our first study, we show that this method significantly improves the sensitivity to task difficulty. We then demonstrate how to form a framework where the represented patterns are analyzed with multi-dimensional Long Short-Term Memory recurrent neural networks for their non-linear mapping onto difficulty-related parameters. This framework outperformed other methods that used hand-engineered features. This approach works with any built-in camera, without requiring specialized devices. We conclude by discussing how Rethinking Eye-blink can benefit real-world applications.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jeon, Youngseung; Jin, Seungwan; Shih, Patrick C.; Han, Kyungsik
TI  - CHI - FashionQ: An AI-Driven Creativity Support Tool for Facilitating Ideation in Fashion Design
PY  - 2021
AB  - Recent research on creativity support tools (CST) adopts artificial intelligence (AI) that leverages big data and computational capabilities to facilitate creative work. Our work aims to articulate the role of AI in supporting creativity with a case study of an AI-based CST tool in fashion design based on theoretical groundings. We developed AI models by externalizing three cognitive operations (extending, constraining, and blending) that are associated with divergent and convergent thinking. We present FashionQ, an AI-based CST that has three interactive visualization tools (StyleQ, TrendQ, and MergeQ). Through interviews and a user study with 20 fashion design professionals (10 participants for the interviews and 10 for the user study), we demonstrate the effectiveness of FashionQ on facilitating divergent and convergent thinking and identify opportunities and challenges of incorporating AI in the ideation process. Our findings highlight the role and use of AI in each cognitive operation based on professionals’ expertise and suggest future implications of AI-based CST development.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445093
ER  - 

TY  - JOUR
AU  - Zhao, Zixuan Victoria; Lopez, Christian E.; Tucker, Conrad S.
TI  - Evaluating the Impact of Idea Dissemination Methods on Information Loss
PY  - 2019
AB  - <jats:p>Information is transferred through a process consisting of an information source, a transmitter, a channel, a receiver, and its destination. Unfortunately, during the engineering design process, there is a risk of a design idea or solution being incorrectly transferred and interpreted due to the nonlinearity of the process, and many ways to communicate and disseminate ideas or solutions. The objective of this work is to explore the amount of relevant design information transmitted by different idea dissemination methods and how the receiver's familiarity with the idea impacts the effectiveness of the methods. First, this work explores the advantages and disadvantages of different dissemination methods in engineering design. Next, an experiment is conducted with engineering and nonengineering participants in order to quantify the information transmitted by different idea dissemination methods. This work also quantifies the effect that receivers' familiarity with a design artifact has on the amount of information transmitted by different dissemination methods. Finally, the results obtained from the experiments are compared with a previous theoretical model for validation. The results indicate that while certain methods are perceived as more informative and are able to convey more information than others (e.g., linguistic textual description versus virtual three-dimensional (3D) models), the effectiveness of the methods depends on a receiver's familiarity with the ideas being transmitted. Knowledge gained from this work can aid designers in selecting a suitable dissemination method needed to effectively communicate ideas and achieve a design solution.</jats:p>
SP  - 031006
EP  - NA
JF  - Journal of Computing and Information Science in Engineering
VL  - 19
IS  - 3
PB  - 
DO  - 10.1115/1.4042553
ER  - 

TY  - JOUR
AU  - Hossain, Md. Yousuf; Zaman, Loutfouz
TI  - NCAlt: Alternatives and Difference Visualizations for Behavior Trees in Game Development Learning
PY  - 2022
AB  - <jats:p>When learning how to develop AI behavior it is common for students to test different ideas before settling on a desired outcome. This functionality is not available in modern behavior tree systems beyond the traditional methods of duplication and conditional execution. We present NCAlt - a visual behavior authoring framework for exploring multiple game AI behaviors. The system also allows selective merging of nodes between multiple behavior tree alternatives and their visual differencing in the game scene and behavior tree views. We present two worked examples which demonstrate how NCAlt can improve the workflow of game development students. We have conducted a user study where NCAlt was compared against NodeCanvas for creating and using alternatives with moderately skilled game development students. Participants rated NCAlt 15% higher than NodeCanvas on the System Usability Scale (SUS). NCAlt was also rated higher on a self-developed questionnaire. We also conducted a semi-structured interview and a detailed longitudinal follow-up and obtained mostly positive feedback for NCAlt. The results suggest NCAlt has a potential to improve the types of workflows with behavior trees that involve exploration for game development students and facilitate learning, as a result. NCAlt was built on top of NodeCanvas - a plugin for the Unity Engine.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CHI PLAY
PB  - 
DO  - 10.1145/3549508
ER  - 

TY  - CONF
AU  - Weingarten, Ariel; Lafreniere, Ben; Fitzmaurice, George; Grossman, Tovi
TI  - Graphics Interface - DreamRooms: Prototyping Rooms in Collaboration with a Generative Process
PY  - NA
AB  - Generative design techniques use algorithmic encodings of domain knowledge to automate parts of the design process. This approach has worked well when success can be written as an optimization problem, but useful evaluation criteria are often discovered during the design process. To study how these criteria are developed we built DreamRooms, a room layout prototyping tool with a tight interaction loop between the designer and a generative process that does not encode a priori objective measures of quality. DreamRooms consists of a VR environment wherein the user can set constraints and gradually lower the entropy of a generative process that produces alternative layouts for the user to consider and iterate on. In addition to the DreamRooms system, we present the results of an observational study which revealed benefits to rapid collaboration between a designer and the generative process in an embodied environment and points towards mechanisms for communicating design intent to the generative process.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.20380/gi2019.19
ER  - 

TY  - BOOK
AU  - Mohian, Soumik; Csallner, Christoph
TI  - MOBILESoft@ICSE - Doodle2App: native app code by freehand UI sketching
PY  - 2020
AB  - User interface development typically starts with freehand sketching, with pen on paper, which creates a big gap in the software development process. Recent advances in deep neural networks that have been trained on large sketch stroke sequence collections have enabled online sketch detection that supports many sketch element classes at high classification accuracy. This paper leverages the recent Google Quick, Draw! dataset of 50M sketch stroke sequences to pre-train a recurrent neural network and retrains it with sketch stroke sequences we collected via Amazon Mechanical Turk. The resulting Doodle2App website offers a paper substitute, i.e., a drawing interface with interactive UI preview and can convert sketches to a compilable single-page Android application. On 712 sketch samples Doodle2App achieved higher accuracy than the state-of-the-art tool Teleport. A video demo is at https://youtu.be/P4sb0pKTNEY
SP  - 81
EP  - 84
JF  - Proceedings of the IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3387905.3388607
ER  - 

TY  - JOUR
AU  - Vlah, D.; Žavbi, Roman; Vukašinović, Nikola
TI  - EVALUATION OF TOPOLOGY OPTIMIZATION AND GENERATIVE DESIGN TOOLS AS SUPPORT FOR CONCEPTUAL DESIGN
PY  - 2020
AB  - Nowadays, a large number of different tools that support early phases of design are available to engineers. In the past decade a specialized set of CAD-based tools were developed, that support the ideation process by generating different design alternatives according to the criteria given by the designer. Two types of tools are discussed in this paper: topology optimization and generative design tools. To investigate to what extent these tools are suitable for use in early design phases and what are the main differences between them, a study was conducted on an industrial case.
SP  - 451
EP  - 460
JF  - Proceedings of the Design Society: DESIGN Conference
VL  - 1
IS  - NA
PB  - 
DO  - 10.1017/dsd.2020.165
ER  - 

TY  - JOUR
AU  - ÖZDEMİR, Yusuf Burak; KARPUZCU, Yalçın; ÇAM, Serhat; GÜNPINAR, Erkan
TI  - Jant Tasarım Parametrizasyonu ve  Parametrizasyonun Optimizasyona Etkisi
PY  - 2022
AB  - <jats:p xml:lang="tr">Jantlar arabanın önemli bir parçasıdır ve tekerlekler ile birlikte arabanın ve yolcularının yükünü taşırlar. Bu yükü taşımak ve olası bir kazada can kaybını önlemek için jantın sağlam olması gerekli ve önemlidir. Diğer taraftan estetik açıdan da göze hitap etmelidir. Bu çalışmada öncelikle araba jantının sınır koşulları belirlenmiştir. Bu sınırlar içerisinde farklı jant tasarımları elde edebilmek için bir kullanıcı çalışması gerçekleştirilmiştir. Kullanıcı çalışmasındaki her bir katılımcı bir model tasarlamış ve parametrize etmiştir. Jant telinin sayısı, şekli ve göbek kalınlığı gibi tasarım parametreleri kullanıcı tarafından belirtilmiştir. Sonrasında kullanıcılardan bu parametreler kullanarak jeneratif tasarım yoluyla birbirinden farklı 20 tane jant modeli elde etmeleri istenmiştir. Durağan arabanın etki ettiği kuvvetler altında (parametrik olarak elde edilen) jantlar modellerinin gerilme ve yer değiştirme dağılımını bulmak için sonlu elemanlar yöntemi (FEM) kullanılmıştır. FEM kullanırken, ağ elemanlarının sayısına ve analiz edilen jantın yönüne dikkat edilmiştir. Jantlar tasarım kabiliyetine sahip gönüllü birisinin verdiği estetik puanlara ve FEM testlerinden elde edilen stres ve yer değiştirme değerlerine göre sıralanmıştır. Sıralamanın ardından genetik algoritma (GA) kullanılarak farklı ve estetik modeller elde edilip, kullanıcıya sunulmuş ve seçimi ile en uygun jant tasarım(lar)ı elde edilmiştir. Bu optimizasyon çalışması parametrizasyonu yapılmış iki farklı jant modeli kullanılarak yapılmıştır. Sonuçlar incelendiğinde parametrizasyon optimizasyon sonrası elde edilen modellerin performanslarını etkilemektedir.</jats:p>
SP  - 913
EP  - 926
JF  - Deu Muhendislik Fakultesi Fen ve Muhendislik
VL  - 24
IS  - 72
PB  - 
DO  - 10.21205/deufmd.2022247220
ER  - 

TY  - NA
AU  - Xue, Feng
TI  - ISSTA - Automated mobile apps testing from visual perspective
PY  - 2020
AB  - The current implementation of automated mobile apps testing generally relies on internal program information, such as reading code or GUI layout files, capturing event streams. This paper proposes an approach of automated mobile apps testing from a completely visual perspective. It uses computer vision technology to enable computer to judge the internal functions from the external GUI information of mobile apps as we humans do and generates test strategy for execution, which improves the interactivity, flexibility, and authenticity of testing. We believe that this vision-based testing approach will further help alleviate the contradiction between the current huge test requirements of mobile apps and the relatively lack of testers.
SP  - 577
EP  - 581
JF  - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3395363.3402644
ER  - 

TY  - NA
AU  - Lu, Yuwen; Zhang, Chengzhi; Zhang, Iris; Li, Toby Jia-Jun
TI  - Bridging the Gap Between UX Practitioners' Work Practices and AI-Enabled Design Support Tools
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519809
ER  - 

TY  - NA
AU  - Fok, Raymond; Zhong, Mingyuan; Ross, Anne Spencer; Fogarty, James; Wobbrock, Jacob O.
TI  - A Large-Scale Longitudinal Analysis of Missing Label Accessibility Failures in Android Apps
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502143
ER  - 

TY  - NA
AU  - Liu, Thomas F.; Craft, Mark; Situ, Jason; Yumer, Ersin; Mech, Radomir; Kumar, Ranjitha
TI  - UIST - Learning Design Semantics for Mobile Apps
PY  - 2018
AB  - Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78% of the total visible, non-redundant elements.
SP  - 569
EP  - 579
JF  - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3242587.3242650
ER  - 

TY  - NA
AU  - Pandian, Vinoth Pandian Sermuga; Suleri, Sarah; Jarke, Matthias
TI  - CHI - UISketch: A Large-Scale Dataset of UI Element Sketches
PY  - 2021
AB  - This paper contributes the first large-scale dataset of 17,979 hand-drawn sketches of 21 UI element categories collected from 967 participants, including UI/UX designers, front-end developers, HCI, and CS grad students, from 10 different countries. We performed a perceptual study with this dataset and found out that UI/UX designers can recognize the UI element sketches with ~96% accuracy. To compare human performance against computational recognition methods, we trained the state-of-the-art DNN-based image classification models to recognize the UI elements sketches. This study revealed that the ResNet-152 model outperforms other classification networks and detects unknown UI element sketches with 91.77% accuracy (chance is 4.76%). We have open-sourced the entire dataset of UI element sketches to the community intending to pave the way for further research in utilizing AI to assist the conversion of lo-fi UI sketches to higher fidelities.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445784
ER  - 

TY  - NA
AU  - Faust, Rebecca; Isaacs, Katherine E.; Bernstein, William Z.; Sharp, Michael; Scheidegger, Carlos
TI  - Anteater: Interactive Visualization of Program Execution Values in Context.
PY  - 2019
AB  - Debugging is famously one the hardest parts in programming. In this paper, we tackle the question: what does a debugging environment look like when we take interactive visualization as a central design principle? We introduce Anteater, an interactive visualization system for tracing and exploring the execution of Python programs. Existing systems often have visualization components built on top of an existing infrastructure. In contrast, Anteater's organization of trace data enables an intermediate representation which can be leveraged to automatically synthesize a variety of visualizations and interactions. These interactive visualizations help with tasks such as discovering important structures in the execution and understanding and debugging unexpected behaviors. To assess the utility of Anteater, we conducted a participant study where programmers completed tasks on their own python programs using Anteater. Finally, we discuss limitations and where further research is needed.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Keshavarzi, Mohammad; Hotson, Clayton; Cheng, Chin-Yi; Nourbakhsh, Mehdi; Bergin, Michael; Asl, Mohammad Rahmani
TI  - CHI Extended Abstracts - SketchOpt: Sketch-based Parametric Model Retrieval for Generative Design
PY  - 2021
AB  - Developing fully parametric building models for performance-based generative design tasks often requires proficiency in many advanced 3D modeling and visual programming software, limiting its use for many building designers. Moreover, iterations of such models can be time-consuming tasks and sometimes limiting depending on the the design stage, as major changes in the layout design may result in remodeling the entire parametric definition. To address these challenges, we introduce a novel automated generative design system, which takes a basic floor plan sketch as an input and provides a parametric model prepared for multi-objective building optimization as output. In addition, the user-designer can assign various design variables for its desired building elements by using simple annotations in the drawing. We take advantage of a asymmetric convolutional module combined with a parametrizer to allow real-time parametric sketch-retrieval for a performance-based generative workflow. The system would recognize the corresponding element and define variable constraints to prepare for a multi-objective optimization problem. We illustrate the the use case of our proposed system by running a real-time structural optimization form-finding study. Our findings indicate the system can be utilized as a promising generative design tool for novice users.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451620
ER  - 

TY  - NA
AU  - Yu, Geoffrey X.; Grossman, Tovi; Pekhimenko, Gennady
TI  - UIST - Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training
PY  - 2020
AB  - Training a state-of-the-art deep neural network (DNNs) is a computationally-expensive and time-consuming process, which incentivizes deep learning developers to debug their DNNs for computational performance. However, effectively performing this debugging requires intimate knowledge about the underlying software and hardware systems-something that the typical deep learning developer may not have. To help bridge this gap, we present Skyline: a new interactive tool for DNN training that supports in-editor computational performance profiling, visualization, and debugging. Skyline's key contribution is that it leverages special computational properties of DNN training to provide (i) interactive performance predictions and visualizations, and (ii) directly manipulatable visualizations that, when dragged, mutate the batch size in the code. As an in-editor tool, Skyline allows users to leverage these diagnostic features to debug the performance of their DNNs during development. An exploratory qualitative user study of Skyline produced promising results; all the participants found Skyline to be useful and easy to use.
SP  - 126
EP  - 139
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415890
ER  - 

TY  - NA
AU  - Rahman, Soliha; Pandian, Vinoth Pandian Sermuga; Jarke, Matthias
TI  - IUI Companion - RUITE: Refining UI Layout Aesthetics Using Transformer Encoder
PY  - 2021
AB  - In the User Interface (UI) design process, designers sketch the UI design (low fidelity prototype) with minimal focus on visual appearances before converting them to higher fidelities. Contrary to low-fidelity, higher fidelity prototypes require better layout and aesthetic quality, during which designers adhere to design laws and conventions. Numerous research studies attempt to automate this transformation of low fidelity sketches to higher fidelities using Deep Neural Networks. However, these studies seldom focus on the layout quality and aesthetics of the generated higher fidelity prototype. As a solution, this paper proposes RUITE, a UI layout refinement engine that optimizes layouts using a Transformer Encoder. We trained RUITE by adding noise to misalign 35,369 UI layouts from the RICO dataset as input and the original aligned layout annotation as ground-truth. Upon evaluation with 4,421 misaligned UI layouts, RUITE provides 77% accuracy in aligning them. RUITE improves the existing research on transforming low-fidelity sketches to higher fidelities by beautifying generated UI layouts.
SP  - 81
EP  - 83
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397482.3450716
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun
TI  - UIST (Adjunct Volume) - Multi-Modal Interactive Task Learning from Demonstrations and Natural Language Instructions
PY  - 2020
AB  - Interactive task learning (ITL) allows end users to 'teach' an intelligent agent new tasks, the corresponding task conditions,and the relevant concepts. This paper presents my research on expanding the applicability, generalizability, robustness, expressiveness, and script sharability of ITL systems usinga multi-modal approach. My research demonstrates that a multi-modal ITL approach that combines programming by demonstration and natural language instructions can empower users without significant programming expertise to extend intelligent agents for their own app-based computing tasks.
SP  - 162
EP  - 168
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415803
ER  - 

TY  - NA
AU  - Sereshkeh, Alborz Rezazadeh; Leung, Gary; Perumal, Krish; Phillips, Caleb; Zhang, Minfan; Fazly, Afsaneh; Mohomed, Iqbal
TI  - VASTA: A Vision and Language-assisted Smartphone Task Automation System
PY  - 2019
AB  - We present VASTA, a novel vision and language-assisted Programming By Demonstration (PBD) system for smartphone task automation. Development of a robust PBD automation system requires overcoming three key challenges: first, how to make a particular demonstration robust to positional and visual changes in the user interface (UI) elements; secondly, how to recognize changes in the automation parameters to make the demonstration as generalizable as possible; and thirdly, how to recognize from the user utterance what automation the user wishes to carry out. To address the first challenge, VASTA leverages state-of-the-art computer vision techniques, including object detection and optical character recognition, to accurately label interactions demonstrated by a user, without relying on the underlying UI structures. To address the second and third challenges, VASTA takes advantage of advanced natural language understanding algorithms for analyzing the user utterance to trigger the VASTA automation scripts, and to determine the automation parameters for generalization. We run an initial user study that demonstrates the effectiveness of VASTA at clustering user utterances, understanding changes in the automation parameters, detecting desired UI elements, and, most importantly, automating various tasks. A demo video of the system is available here: this http URL
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Xie, Mulong; Feng, Sidong; Xing, Zhenchang; Chen, Jieshan; Chen, Chunyang
TI  - ESEC/SIGSOFT FSE - UIED: a hybrid tool for GUI element detection
PY  - 2020
AB  - Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be further edited in popular UI design tools such as Sketch and Photoshop. UIED is evaluated to be capable of accurate detection and useful for downstream works. Tool URL: http://uied.online Github Link: https://github.com/MulongXie/UIED
SP  - 1655
EP  - 1659
JF  - Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3368089.3417940
ER  - 

TY  - JOUR
AU  - Gunpinar, Erkan; Khan, Shahroz
TI  - A multi-criteria based selection method using non-dominated sorting for genetic algorithm based design
PY  - 2019
AB  - The paper presents a generative design approach, particularly for simulation-driven designs, using a genetic algorithm (GA), which is structured based on a novel offspring selection strategy. The proposed selection approach commences while enumerating the offsprings generated from the selected parents. Afterwards, a set of eminent offsprings is selected from the enumerated ones based on the following merit criteria: space-fillingness to generate as many distinct offsprings as possible, resemblance/non-resemblance of offsprings to the good/bad individuals, non-collapsingness to produce diverse simulation results and constrain-handling for the selection of offsprings satisfying design constraints. The selection problem itself is formulated as a multi-objective optimization problem. A greedy technique is employed based on non-dominated sorting, pruning, and selecting the representative solution. According to the experiments performed using three different application scenarios, namely simulation-driven product design, mechanical design and user-centred product design, the proposed selection technique outperforms the baseline GA selection techniques, such as tournament and ranking selections.
SP  - 1319
EP  - 1357
JF  - Optimization and Engineering
VL  - 21
IS  - 4
PB  - 
DO  - 10.1007/s11081-019-09477-8
ER  - 

TY  - NA
AU  - Potluri, Venkatesh; Grindeland, Tadashi E; Froehlich, Jon E.; Mankoff, Jennifer
TI  - CHI - Examining Visual Semantic Understanding in Blind and Low-Vision Technology Users
PY  - 2021
AB  - Visual semantics provide spatial information like size, shape, and position, which are necessary to understand and efficiently use interfaces and documents. Yet little is known about whether blind and low-vision (BLV) technology users want to interact with visual affordances, and, if so, for which task scenarios. In this work, through semi-structured and task-based interviews, we explore preferences, interest levels, and use of visual semantics among BLV technology users across two device platforms (smartphones and laptops), and information seeking and interactions common in apps and web browsing. Findings show that participants could benefit from access to visual semantics for collaboration, navigation, and design. To learn this information, our participants used trial and error, sighted assistance, and features in existing screen reading technology like touch exploration. Finally, we found that missing information and inconsistent screen reader representations of user interfaces hinder learning. We discuss potential applications and future work to equip BLV users with necessary information to engage with visual semantics.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445040
ER  - 

TY  - JOUR
AU  - Tian, Deyu; Ma, Yun; Balasubramanian, Aruna; Liu, Yunxin; Huang, Gang; Liu, Xuanzhe
TI  - Characterizing Embedded Web Browsing in Mobile Apps
PY  - 2021
AB  - Modern mobile OSes support to display Web pages in the native apps, which we call embedded Web pages. In this paper, we conduct, to the best of our knowledge, the first measurement study on browsing embedded Web pages on Android. Our study on 22,521 popular Android apps shows that 57.9% and 73.8% of apps embed Web pages on two popular app markets: Google Play and Wandoujia, respectively. To analyze the embedded Web browsing performance at scale, we design and implement EWProfiler, a tool that can automatically search for embedded Web pages inside apps, trigger page loads, and retrieve performance metrics. Based on 445 embedded Web pages obtained by EWProfiler in 99 popular apps from the two app markets, we investigate the characteristics and performance of embedded Web pages, and find that embedded Web pages significantly impede the app user experience. To optimize the performance of embedded Web browsing, we investigate the effectiveness of three techniques, i.e., separating the browser kernel to a different process, loading pages from local storage, and pre-rendering. We believe that our findings could draw attentions to Web developers, browser vendors, app developers, and mobile OS vendors together towards better performance of embedded Web browsing.
SP  - 1
EP  - 1
JF  - IEEE Transactions on Mobile Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/tmc.2021.3065945
ER  - 

TY  - JOUR
AU  - Khan, Shahroz; Gunpinar, Erkan; Şener, Bekir
TI  - GenYacht : an interactive generative design system for computer-aided yacht hull design
PY  - 2019
AB  - Abstract In the present work, a new digital design system, GenYacht, is proposed for the creation of optimal and user-centred yacht hull forms. GenYacht is a hybrid system involving generative and interactive design approaches, which enables users to create a variety of design alternatives. Among them, a user can select a hull design with desirable characteristics based on its appearance and hydrostatics/hydrodynamic performance. GenYacht first explores a given design space using a generative design technique (GDT), which creates uniformly distributed designs satisfying the given design constraints. These designs are then presented to a user and single or multiple designs are selected based on the user’s requirements. Afterwards, based on the selections, the design space is refined using a novel space-shrinking technique (SST). In each interaction, SST shrinks the design space, which is then fed into GDT to create new designs in the shrank space for the next interaction. This shrinkage of design space guides the exploration process and focuses the computational efforts on user-preferred regions. The interactive and generative design steps are repeated until the user reaches a satisfactory design(s). The efficiency of GenYacht is demonstrated via experimental and user studies and its performance is compared with interactive genetic algorithms.
SP  - 106462
EP  - NA
JF  - Ocean Engineering
VL  - 191
IS  - NA
PB  - 
DO  - 10.1016/j.oceaneng.2019.106462
ER  - 

TY  - NA
AU  - Rolnick, David; Donti, Priya L.; Kaack, Lynn H.; Kochanski, K.; Lacoste, Alexandre; Sankaran, Kris; Ross, Andrew S.; Milojevic-Dupont, Nikola; Jaques, Natasha; Waldman-Brown, Anna; Luccioni, Alexandra; Maharaj, Tegan; Sherwin, Evan D.; Mukkavilli, S. Karthik; Kording, Konrad P.; Gomes, Carla P.; Ng, Andrew Y.; Hassabis, Demis; Platt, John; Creutzig, Felix; Chayes, Jennifer; Bengio, Yoshua
TI  - Tackling Climate Change with Machine Learning
PY  - 2019
AB  - Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.
SP  - NA
EP  - NA
JF  - arXiv: Computers and Society
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Krosnick, Rebecca
TI  - Tools for Creating UI Automation Macros
PY  - 2022
AB  - Automation macros enable users to perform digital tasks programmatically to save time or support hands-free interaction. For example, macros can be used to perform web scraping for a research project (e.g., scraping articles from a news site) or personal task automation via natural language (e.g., ordering food for delivery). Some kinds of automation are built into our devices and are readily available (e.g., via Siri [1] or Alexa [2] ), but this set is limited and often will not support a user&#x2019;s niche or complex needs. Users can create their own custom macros, but traditionally this requires writing program code which involves a significant amount of effort for programmers and is infeasible for non-programmers.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9832966
ER  - 

TY  - NA
AU  - Lin, Yuyu; Guo, Jiahao; Chen, Yang; Yao, Cheng; Ying, Fangtian
TI  - CHI - It Is Your Turn: Collaborative Ideation With a Co-Creative Robot through Sketch
PY  - 2020
AB  - Co-creative systems have been widely explored in the field of computational creativity. However, existing AI partners of these systems are mostly virtual agents. As sketching on paper with embodied robots could be more engaging for designers' early-stage ideation and collaborative practices, we envision the possibility of Cobbie, a mobile robot that ideates iteratively with designers by generating creative and diverse sketches. To evaluate the differences in co-creativity and user experience between the co-creative robots and virtual agents, we conducted a comparative experiment and analyzed the data collected from quantitative scales, observation, and semi-structured interview. The results reveal that Cobbie is more satisfying in motivating exploration, provoking unexpected ideas and engaging designers in the collaborative ideation process. Based on these findings, we discussed the prospects of co-creative robots for future developments of human-AI collaborative systems.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376258
ER  - 

TY  - NA
AU  - PILAGATTI, ADRIANO NICOLA; Atzeni, Eleonora; Salmi, Alessandro
TI  - Exploiting the generative design potential to select the best conceptual design of an aerospace component to be produced by additive manufacturing
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>Since the advent of Industry 4.0, the manufacturing sector has had to face new challenges, which have required the development of new skills and innovative tools. This scenario includes innovative production processes such as Additive Manufacturing (AM), a technology capable of producing a component layer-by-layer directly from the 3D model, without the need of specific tools. Generative Design (GD) may represent an opportunity to maximize the potential of AM techniques. GD is based on parametric computer-aided design (CAD) tools capable of generating multiple optimized outputs, among which the designer could select the most promising solution. This paper presents a general methodology for evaluating the GD outputs in the conceptual phase of design, to select the best possible solution through a series of criteria at several levels. The evaluation method is deployed in an aerospace field case study. The procedure demonstrates the benefits of adopting GD synergistically with AM in the early stages of product development. This indicates that the developed methodology could reduce the number of iterations during the design process, and the result is a decrease in the overall time spent on the project, avoiding problems during the final stages of the design.</jats:p>
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.21203/rs.3.rs-2330572/v1
ER  - 

TY  - NA
AU  - Ran, Dezhi; Li, Zongyang; Liu, Chenxu; Wang, Wenyu; Meng, Weizhi; Wu, Xionglin; Jin, Hui; Cui, Jing; Tang, Xing; Xie, Tao
TI  - Automated visual testing for mobile apps in an industrial seting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3510457.3513027
ER  - 

TY  - CHAP
AU  - Rui, Hu; Chen, Mingang; Cai, Lizhi; Chen, Wenjie
TI  - Detection and Segmentation of Graphical Elements on GUIs for Mobile Apps Based on Deep Learning
PY  - 2020
AB  - Recently, mobile devices are more popular than computers. However, mobile apps are not as thoroughly tested as desktop ones, especially for graphical user interface (GUI). In this paper, we study the detection and segmentation of graphical elements on GUIs for mobile apps based on deep learning. It is the preliminary work of GUI testing for mobile apps based on artificial intelligence. We create a dataset, which consists of 2,100 GUI screenshots (or pages) labeled with 42,156 graphic elements in 8 classes. Based on our dataset, we adopt Mask R-CNN to train the detection and segmentation of graphic elements on GUI screenshots. The experimental results show that the mAP value achieves 98%.
SP  - 187
EP  - 197
JF  - Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-64214-3_13
ER  - 

TY  - JOUR
AU  - Strazdas, Dominykas; Hintz, Jan; Felbberg, Anna-Maria; Al-Hamadi, Ayoub
TI  - Robots and Wizards: An Investigation Into Natural Human–Robot Interaction
PY  - 2020
AB  - The goal of the study was to research different communication modalities needed for intuitive Human-Robot Interaction. This study utilizes a Wizard of Oz prototyping method to enable a restriction-free, intuitive interaction with an industrial robot. The data from 36 test subjects suggests a high preference for speech input, automatic path planning and pointing gestures. The catalogue developed during this experiment contains intrinsic gestures suggesting that the two most popular gestures per action can be sufficient to cover the majority of users. The system scored an average of 74% in different user interface experience questionnaires, while containing forced flaws. These findings allow a future development of an intuitive Human-Robot interaction system with high user acceptance.
SP  - 207635
EP  - 207642
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.3037724
ER  - 

TY  - NA
AU  - Hutt, Stephen; D'Mello, Sidney K.
TI  - Evaluating Calibration-free Webcam-based Eye Tracking for Gaze-based User Modeling
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556580
ER  - 

TY  - BOOK
AU  - Todi, Kashyap
TI  - HTTF - Reimagining the Role of the Expert: From Interface Design to Interface Curation
PY  - 2019
AB  - User Interface (UI) design has been a core topic of HCI research for several decades. Equipped with design skills and knowledge, the expert interface designer meticulously analyses a design brief, conceptualises design ideas, and constructs viable solutions. The intended outcome of this tedious process is a usable and aesthetically-pleasing UI. Classical approaches in HCI have relied upon providing designers with guidelines, heuristics, and best practices for realising good designs. In recent years, computational approaches have turned towards formalising and automating parts of the design process. In this provocation, I claim that the future expert will hand over the task of creating design solutions entirely to the machine, and instead take on the role of an interface curator who inspects a set of feasible designs and picks out the best possible solutions for a given problem. I discuss the current state of computational interface design, and suggest a path forward towards realising this vision.
SP  - NA
EP  - NA
JF  - Proceedings of the Halfway to the Future Symposium 2019
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3363384.3363386
ER  - 

TY  - NA
AU  - Zhang, Dingwen; Wang, Bo; Wang, Gerong; Zhang, Qiang; Zhang, Jiajia; Han, Jungong; You, Zheng
TI  - Onfocus Detection: Identifying Individual-Camera Eye Contact from Unconstrained Images.
PY  - 2021
AB  - Onfocus detection aims at identifying whether the focus of the individual captured by a camera is on the camera or not. Based on the behavioral research, the focus of an individual during face-to-camera communication leads to a special type of eye contact, i.e., the individual-camera eye contact, which is a powerful signal in social communication and plays a crucial role in recognizing irregular individual status (e.g., lying or suffering mental disease) and special purposes (e.g., seeking help or attracting fans). Thus, developing effective onfocus detection algorithms is of significance for assisting the criminal investigation, disease discovery, and social behavior analysis. However, the review of the literature shows that very few efforts have been made toward the development of onfocus detector due to the lack of large-scale public available datasets as well as the challenging nature of this task. To this end, this paper engages in the onfocus detection research by addressing the above two issues. Firstly, we build a large-scale onfocus detection dataset, named as the OnFocus Detection In the Wild (OFDIW). It consists of 20,623 images in unconstrained capture conditions (thus called ``in the wild'') and contains individuals with diverse emotions, ages, facial characteristics, and rich interactions with surrounding objects and background scenes. On top of that, we propose a novel end-to-end deep model, i.e., the eye-context interaction inferring network (ECIIN), for onfocus detection, which explores eye-context interaction via dynamic capsule routing. Finally, comprehensive experiments are conducted on the proposed OFDIW dataset to benchmark the existing learning models and demonstrate the effectiveness of the proposed ECIIN. The project (containing both datasets and codes) is at this https URL.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Brückner, Lukas; Leiva, Luis A.; Oulasvirta, Antti
TI  - Learning GUI Completions with User-defined Constraints
PY  - 2022
AB  - <jats:p> A key objective in the design of <jats:bold>graphical user interfaces (GUIs)</jats:bold> is to ensure consistency across screens of the same product. However, designing a compliant layout is time-consuming and can distract designers from creative thinking. This paper studies layout recommendation methods that fulfill such consistency requirements using machine learning. Given a desired element type and size, the methods suggest element placements following real-world GUI design processes. Consistency requirements are given implicitly through previous layouts from which patterns are to be learned, comparable to existing screens of a software product. We adopt two recently proposed methods for this task, a <jats:bold>Graph Neural Network (GNN)</jats:bold> and a Transformer model, and compare them with a custom approach based on sequence alignment and <jats:bold>nearest neighbor search (kNN)</jats:bold> . The methods were tested on handcrafted datasets with explicit layout patterns, as well as large-scale public datasets of diverse mobile design layouts. Our results show that our instance-based learning algorithm outperforms both neural network approaches. Ultimately, this work contributes to establishing smarter design tools for professional designers with explainable algorithms that increase their efficacy. </jats:p>
SP  - 1
EP  - 40
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 12
IS  - 1
PB  - 
DO  - 10.1145/3490034
ER  - 

TY  - NA
AU  - Li, Zhengqing; Miyafuji, Shio; Sato, Toshiki; Kuzuoka, Hideaki; Koike, Hideki
TI  - UIST (Adjunct Volume) - OmniEyeball: Spherical Display Equipped With Omnidirectional Camera And Its Application For 360-Degree Video Communication
PY  - 2018
AB  - We propose OmniEyeball (OEB), which is a novel interactive 360° image I/O system. It integrates the spherical display system with an omnidirectional camera to enable both capturing the 360° panoramic live streaming video as well as displaying it. We also present its unique application for symmetric 360° video communication by utilizing two OEB terminals, which may solve the narrow field-of-view problem in video communication. In addition, we designed a vision-based touch detection technique as well as some features to support 360° video communication.
SP  - 33
EP  - 35
JF  - Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3266037.3266092
ER  - 

TY  - NA
AU  - Zhang, Xiaoyi; de Greef, Lilian; Swearngin, Amanda; White, Samuel; Murray, Kyle I.; Yu, Lisa; Shan, Qi; Nichols, Jeffrey; Wu, Jason; Fleizach, Chris; Everitt, Aaron; Bigham, Jeffrey P.
TI  - CHI - Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels
PY  - 2021
AB  - Many accessibility features available on mobile platforms require applications (apps) to provide complete and accurate metadata describing user interface (UI) components. Unfortunately, many apps do not provide sufficient metadata for accessibility features to work as expected. In this paper, we explore inferring accessibility metadata for mobile apps from their pixels, as the visual interfaces often best reflect an app’s full functionality. We trained a robust, fast, memory-efficient, on-device model to detect UI elements using a dataset of 77,637 screens (from 4,068 iPhone apps) that we collected and annotated. To further improve UI detections and add semantic information, we introduced heuristics (e.g., UI grouping and ordering) and additional models (e.g., recognize UI content, state, interactivity). We built Screen Recognition to generate accessibility metadata to augment iOS VoiceOver. In a study with 9 screen reader users, we validated that our approach improves the accessibility of existing mobile apps, enabling even previously inaccessible apps to be used.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445186
ER  - 

TY  - NA
AU  - Englmeier, David; Fan, Fan; Butz, Andreas
TI  - ISMAR - Rock or Roll – Locomotion Techniques with a Handheld Spherical Device in Virtual Reality
PY  - 2020
AB  - We investigate the use of a handheld spherical object as a controller for locomotion in VR. Rotating the object controls avatar movement in two different ways: As a zero order controller, it is continuously rotated to the target position as if rolling a ball on the floor. As a first order controller, it is tilted like a joystick to determine the direction and speed of movement. We describe how our prototype was built from low-cost commercially available hardware and discuss our design decisions. Then we evaluate both locomotion techniques in a user study (N=20) and compare them to established methods using handheld VR controllers. Our prototype matched and in some cases outperformed these methods regarding task time and accuracy. All results were obtained without any usage instructions, indicating easy learnability. Some of our insights may transfer to interaction with other naturally shaped objects in VR experiences.
SP  - 618
EP  - 626
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00089
ER  - 

TY  - JOUR
AU  - Khan, Shahroz; Awan, Muhammad Junaid
TI  - A generative design technique for exploring shape variations
PY  - 2018
AB  - Abstract Because innovative and creative design is essential to a successful product, this work brings the benefits of generative design in the conceptual phase of the product development process so that designers/engineers can effectively explore and create ingenious designs and make better design decisions. We proposed a state-of-the-art generative design technique (GDT), called Space-filling-GDT (Sf-GDT), for the creation of innovative designs. The proposed Sf-GDT has the ability to create variant optimal design alternatives for a given computer-aided design (CAD) model. An effective GDT should generate design alternatives that cover the entire design space. Toward that end, the criterion of space-filling is utilized, which uniformly distribute designs in the design space thereby giving a designer a better understanding of possible design options. To avoid creating similar designs, a weighted-grid-search approach is developed and integrated into the Sf-GDT. One of the core contributions of this work lies in the ability of Sf-GDT to explore hybrid design spaces consisting of both continuous and discrete parameters either with or without geometric constraints. A parameter-free optimization technique, called Jaya algorithm, is integrated into the Sf-GDT to generate optimal designs. Three different design parameterization and space formulation strategies; explicit, interactive, and autonomous, are proposed to set up a promising search region(s) for optimization. Two user interfaces; a web-based and a Windows-based, are also developed to utilize Sf-GDT with the existing CAD software having parametric design abilities. Based on the experiments in this study, Sf-GDT can generate creative design alternatives for a given model and outperforms existing state-of-the-art techniques.
SP  - 712
EP  - 724
JF  - Advanced Engineering Informatics
VL  - 38
IS  - NA
PB  - 
DO  - 10.1016/j.aei.2018.10.005
ER  - 

TY  - NA
AU  - Li, Yang; Li, Gang; He, Luheng; Zheng, Jingjie; Li, Hong; Guan, Zhiwei
TI  - Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements
PY  - 2020
AB  - Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.
SP  - NA
EP  - NA
JF  - arXiv: Learning
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Bunian, Sara; Li, Kai; Jemmali, Chaima; Harteveld, Casper; Fu, Yun; El-Nasr, Magy Seif
TI  - VINS: Visual Search for Mobile User Interface Design
PY  - 2021
AB  - Searching for relative mobile user interface (UI) design examples can aid interface designers in gaining inspiration and comparing design alternatives. However, finding such design examples is challenging, especially as current search systems rely on only text-based queries and do not consider the UI structure and content into account. This paper introduces VINS, a visual search framework, that takes as input a UI image (wireframe, high-fidelity) and retrieves visually similar design examples. We first survey interface designers to better understand their example finding process. We then develop a large-scale UI dataset that provides an accurate specification of the interface's view hierarchy (i.e., all the UI components and their specific location). By utilizing this dataset, we propose an object-detection based image retrieval framework that models the UI context and hierarchical structure. The framework achieves a mean Average Precision of 76.39\% for the UI detection and high performance in querying similar UI designs.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Tran, Minh; Sen, Taylan; Haut, Kurtis; Ali, Mohammad Rafayet; Hoque, Ehsan
TI  - Are You Really Looking at Me? A Feature-Extraction Framework for Estimating Interpersonal Eye Gaze From Conventional Video
PY  - 2022
AB  - NA
SP  - 912
EP  - 925
JF  - IEEE Transactions on Affective Computing
VL  - 13
IS  - 2
PB  - 
DO  - 10.1109/taffc.2020.2979440
ER  - 

TY  - JOUR
AU  - Campos, Cuauhtli; Sandak, Jakub; Kljun, Matjaž; Čopič Pucihar, Klen
TI  - The Hybrid Stylus: A Multi-Surface Active Stylus for Interacting with and Handwriting on Paper, Tabletop Display or Both.
PY  - 2022
AB  - The distinct properties and affordances of paper provide benefits that enabled paper to maintain an important role in the digital age. This is so much so, that some pen-paper interaction has been imitated in the digital world with touchscreens and stylus pens. Because digital medium also provides several advantages not available to physical paper, there is a clear benefit to merge the two mediums. Despite the plethora of concepts, prototypes and systems to digitise handwritten information on paper, these systems require specially prepared paper, complex setups and software, which can be used solely in combination with paper, and, most importantly, do not support the concurrent precise interaction with both mediums (paper and touchscreen) using one pen only. In this paper, we present the design, fabrication and evaluation of the <i>Hybrid Stylus.</i> The <i>Hybrid Stylus</i> is assembled with the infinity pencil tip (nib) made of graphite and a specially designed shielded tip holder that is attached to an active stylus. The stylus can be used for writing on a physical paper, while it still maintains all the features needed for tablet interaction. Moreover, the stylus also allows simultaneous digitisation of handwritten information on the paper when the paper is placed on the tablet screen. In order to evaluate the concept, we also add a user-friendly manual alignment of paper position on the underlying tablet computer The evaluation demonstrates that the system achieves almost perfect digitisation of strokes (98.6% of strokes were correctly registered with only 1.2% of ghost strokes) whilst maintaining excellent user experience of writing with a pencil on the paper.
SP  - 7058
EP  - 7058
JF  - Sensors (Basel, Switzerland)
VL  - 22
IS  - 18
PB  - 
DO  - 10.3390/s22187058
ER  - 

TY  - NA
AU  - Cheng, Yihua; Shiyao, Huang; Wang, Fei; Qian, Chen; Lu, Feng
TI  - A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation
PY  - 2020
AB  - Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarse-to-fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Patil, Akshay Gadi; Li, Manyi; Fisher, Matthew; Savva, Manolis; Zhang, Hao
TI  - CVPR - LayoutGMN: Neural Graph Matching for Structural Layout Similarity
PY  - 2021
AB  - We present a deep neural network to predict structural similarity between 2D layouts by leveraging Graph Matching Networks (GMN). Our network, coined LayoutGMN, learns the layout metric via neural graph matching, using an attention-based GMN designed under a triplet network setting. To train our network, we utilize weak labels obtained by pixel-wise Intersection-over-Union (IoUs) to define the triplet loss. Importantly, LayoutGMN is built with a structural bias which can effectively compensate for the lack of structure awareness in IoUs. We demonstrate this on two prominent forms of layouts, viz., floorplans and UI designs, via retrieval experiments on large-scale datasets. In particular, retrieval results by our network better match human judgement of structural layout similarity compared to both IoUs and other baselines including a state-of-the-art method based on graph neural networks and image convolution. In addition, LayoutGMN is the first deep model to offer both metric learning of structural layout similarity and structural matching between layout elements.
SP  - 11048
EP  - 11057
JF  - 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr46437.2021.01090
ER  - 

TY  - NA
AU  - Mahendran, Thivagar; Sharmilan, S
TI  - GAN Based Photo-Realistic Image Generation from Sketch using Nested U-Net
PY  - 2020
AB  - In computer vision generative Image modelling is a vast area of research that many studies have been carried out to address such problems as an image to image translation. In this study, we mainly discuss how we can bridge the gap between the industrial designer and their production workflow to reduce the cost of the time they spend on prototyping. We demonstrate an image synthesizing technique to generate a photo-realistic image of a real-world object from a sketch. We integrate a new network architecture to the existing network to improve the system in generating photo realistic images. Compared to the existing systems our system can generate images with more accuracy and more photo-realism.
SP  - 274
EP  - 280
JF  - 2020 Advanced Computing and Communication Technologies for High Performance Applications (ACCTHPA)
VL  - 2020
IS  - NA
PB  - 
DO  - 10.1109/accthpa49271.2020.9213230
ER  - 

TY  - JOUR
AU  - XUE, Feng; WU, Junsheng; ZHANG, Tao; WANG, Wei; CHENG, Jing
TI  - Visual judgment approach of isomorphic GUI for automated mobile app testing
PY  - 2022
AB  - <jats:p>Currently, the rapid growth of mobile apps requires automated testing technology to ensure their quality. Automated testing of mobile apps is usually closely related to the recognition and judgment of their graphical user interface (GUI), but there usually are numerous isomorphic GUIs with different styles and contents, and similar structure and function in mobile apps. In automatic testing, isomorphic GUI is easy to cause the issue of state space explosion, which leads to low efficiency or failure of testing. In view of the limitations of traditional automatic recognition of isomorphic GUI, this paper presents a GUI similarity judgment approach based on visual feature information. Firstly, the GUI component elements are identified by object detection, and then the GUI skeleton is abstracted. Secondly, the visual features of the GUI skeleton are extracted by a convolutional autoencoder. Finally, the isomorphic GUI judgment is completed by comparing the similarity of GUI visual features. The experimental results show that the proposed approach can effectively shield the influence of GUI style and content, complete the isomorphic GUI recognition more accurately and optimize the efficiency of automated mobile app testing.</jats:p>
SP  - 804
EP  - 811
JF  - Xibei Gongye Daxue Xuebao/Journal of Northwestern Polytechnical University
VL  - 40
IS  - 4
PB  - 
DO  - 10.1051/jnwpu/20224040804
ER  - 

TY  - NA
AU  - Xie, Mulong; Xing, Zhenchang; Feng, Sidong; Xu, Xiwei; Zhu, Liming; Chen, Chunyang
TI  - Psychologically-inspired, unsupervised inference of perceptual groups of GUI widgets from GUI images
PY  - 2022
AB  - Graphical User Interface (GUI) is not merely a collection of individual and unrelated widgets, but rather partitions discrete widgets into groups by various visual cues, thus forming higher-order perceptual units such as tab, menu, card or list. The ability to automatically segment a GUI into perceptual groups of widgets constitutes a fundamental component of visual intelligence to automate GUI design, implementation and automation tasks. Although humans can partition a GUI into meaningful perceptual groups of widgets in a highly reliable way, perceptual grouping is still an open challenge for computational approaches. Existing methods rely on ad-hoc heuristics or supervised machine learning that is dependent on specific GUI implementations and runtime information. Research in psychology and biological vision has formulated a set of principles (i.e., Gestalt theory of perception) that describe how humans group elements in visual scenes based on visual cues like connectivity, similarity, proximity and continuity. These principles are domain-independent and have been widely adopted by practitioners to structure content on GUIs to improve aesthetic pleasant and usability. Inspired by these principles, we present a novel unsupervised image-based method for inferring perceptual groups of GUI widgets. Our method requires only GUI pixel images, is independent of GUI implementation, and does not require any training data. The evaluation on a dataset of 1,091 GUIs collected from 772 mobile apps and 20 UI design mockups shows that our method significantly outperforms the state-of-the-art ad-hoc heuristics-based baseline. Our perceptual grouping method creates the opportunities for improving UI-related software engineering tasks.
SP  - NA
EP  - NA
JF  - Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3540250.3549138
ER  - 

TY  - NA
AU  - Wacker, Philipp; Wagner, Adrian; Voelker, Simon; Borchers, Jan
TI  - CHI - Heatmaps, Shadows, Bubbles, Rays: Comparing Mid-Air Pen Position Visualizations in Handheld AR
PY  - 2020
AB  - In Handheld Augmented Reality, users look at AR scenes through the smartphone held in their hand. In this setting, having a mid-air pointing device like a pen in the other hand greatly expands the interaction possibilities. For example, it lets users create 3D sketches and models while on the go. However, perceptual issues in Handheld AR make it difficult to judge the distance of a virtual object, making it hard to align a pen to it. To address this, we designed and compared different visualizations of the pen's position in its virtual environment, measuring pointing precision, task time, activation patterns, and subjective ratings of helpfulness, confidence, and comprehensibility of each visualization. While all visualizations resulted in only minor differences in precision and task time, subjective ratings of perceived helpfulness and confidence favor a 'heatmap' technique that colors the objects in the scene based on their distance to the pen.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376848
ER  - 

TY  - JOUR
AU  - Kang, Boseon; Jo, Minseok; Jeong, Chang-Sung
TI  - Clickable Object Detection Network for a Wide Range of Mobile Screen Resolutions
PY  - 2022
AB  - Recently, as the development cycle of applications has been shortened, it is important to develop rapid and accurate application testing technology. Since application testing requires a lot of cost, mobile component detection technology using deep learning is essential to prevent the use of expensive human resources. In this paper, we shall propose a Clickable Object Detection Network (CODNet) for mobile component detection in a wide range of mobile screen resolutions. CODNet consists of three modules: feature extraction, deconvolution and prediction modules in order to provide performance improvement and scalability. The Feature Extraction module uses squeeze and excitation blocks to efficiently extract features and change the ratio of the input image to 1:2 most close to that of mobile screen. Deconvolution module provides feature map of various sizes by upsampling feature map through top-down pathway and lateral connections. The prediction module selects an anchor size suitable for the mobile environment using the Anchor Transfer block, among the set of anchor candidates obtained through the analysis of mobile dataset. Moreover, we shall improve object detection performance by building a new mobile screen dataset consisting of data collected from various resolutions and operating systems. We shall show that our model achieves competitive performance in mean average precision on our dataset compared to the other models.
SP  - 115051
EP  - 115060
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3202222
ER  - 

TY  - CHAP
AU  - Zhang, Xucong; Park, Seonwook; Feit, Anna Maria
TI  - Eye Gaze Estimation and Its Applications
PY  - 2021
AB  - The human eye gaze is an important non-verbal cue that can unobtrusively provide information about the intention and attention of a user to enable intelligent interactive systems. Eye gaze can also be taken as input to systems as a replacement of the conventional mouse and keyboard, and can also be indicative of the cognitive state of the user. However, estimating and applying gaze in real-world applications poses significant challenges. In this chapter, we first review the development of gaze estimation methods in recent years. We especially focus on learning-based gaze estimation methods which benefit from large-scale data and deep learning methods that recently became available. Second, we discuss the challenges of using gaze estimation for real-world applications and our efforts toward making these methods easily usable for the Human-Computer Interaction community. At last, we provide two application examples, demonstrating the use of eye gaze to enable attentive and adaptive interfaces.
SP  - 99
EP  - 130
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_4
ER  - 

TY  - NA
AU  - Cheng, Yihua; Wang, Haofei; Bao, Yiwei; Lu, Feng
TI  - Appearance-based Gaze Estimation With Deep Learning: A Review and Benchmark.
PY  - 2021
AB  - Gaze estimation reveals where a person is looking. It is an important clue for understanding human intention. The recent development of deep learning has revolutionized many computer vision tasks, the appearance-based gaze estimation is no exception. However, it lacks a guideline for designing deep learning algorithms for gaze estimation tasks. In this paper, we present a comprehensive review of the appearance-based gaze estimation methods with deep learning. We summarize the processing pipeline and discuss these methods from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. Since the data pre-processing and post-processing methods are crucial for gaze estimation, we also survey face/eye detection method, data rectification method, 2D/3D gaze conversion method, and gaze origin conversion method. To fairly compare the performance of various gaze estimation approaches, we characterize all the publicly available gaze estimation datasets and collect the code of typical gaze estimation algorithms. We implement these codes and set up a benchmark of converting the results of different methods into the same evaluation metrics. This paper not only serves as a reference to develop deep learning-based gaze estimation methods but also a guideline for future gaze estimation research. Implemented methods and data processing codes are available at this http URL.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhang, Xucong; Huang, Michael Xuelin; Sugano, Yusuke; Bulling, Andreas
TI  - CHI - Training Person-Specific Gaze Estimators from User Interactions with Multiple Devices
PY  - 2018
AB  - Learning-based gaze estimation has significant potential to enable attentive user interfaces and gaze-based interaction on the billions of camera-equipped handheld devices and ambient displays. While training accurate person- and device-independent gaze estimators remains challenging, person-specific training is feasible but requires tedious data collection for each target device. To address these limitations, we present the first method to train person-specific gaze estimators across multiple devices. At the core of our method is a single convolutional neural network with shared feature extraction layers and device-specific branches that we train from face images and corresponding on-screen gaze locations. Detailed evaluations on a new dataset of interactions with five common devices (mobile phone, tablet, laptop, desktop computer, smart TV) and three common applications (mobile game, text editing, media center) demonstrate the significant potential of cross-device training. We further explore training with gaze locations derived from natural interactions, such as mouse or touch input.
SP  - 624
EP  - NA
JF  - Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3173574.3174198
ER  - 

TY  - CHAP
AU  - Yokokubo, Anna; Kato, Yuji; Siio, Itiro
TI  - HCI (42) - TracKenzan: Digital Flower Arrangement Using Trackpad and Stylus Pen
PY  - 2020
AB  - We propose “TracKenzan,” a training system for Ikebana (Japanese flower arrangement) in a 3D computer graphics space using a trackpad and a stylus pen. Ikebana includes a Kenzan, a flat metal base with hundreds of upward-pointing pins to hold flowers and branches in place. In the proposed system, a trackpad represents the Kenzan, and a stylus pen equipped with a 3D tracker resembles each flower stem. The users build the flower arrangement by selecting virtual flowers with adjustable length, whose position and orientation correspond to those of the pen, and then place the flowers by pressing the desired position on the trackpad. By using the trackpad and pen resembling the Kenzan and flower, respectively, TracKenzan provides an intuitive and straightforward interface for users to practice Ikebana. We present the system implementation and evaluation tests conducted by 11 beginner and 7 experienced users, as well as one Ikebana teacher. They verified the system usability and capabilities as a learning tool for Ikebana.
SP  - 332
EP  - 343
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-60117-1_25
ER  - 

TY  - JOUR
AU  - Zhang, Qi; Pan, Xiang; Liu, Fuchang; Lu, Shufang
TI  - A benchmark dataset for real-time detection of icons in mobile apps and a small-scale feature module
PY  - 2020
AB  - NA
SP  - 87
EP  - 93
JF  - Pattern Recognition Letters
VL  - 136
IS  - NA
PB  - 
DO  - 10.1016/j.patrec.2020.04.037
ER  - 

TY  - JOUR
AU  - Cheng, Yihua; Zhang, Xucong; Lu, Feng; Sato, Yoichi
TI  - Gaze Estimation by Exploring Two-Eye Asymmetry
PY  - 2020
AB  - Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study.
SP  - 5259
EP  - 5272
JF  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
VL  - 29
IS  - NA
PB  - 
DO  - 10.1109/tip.2020.2982828
ER  - 

TY  - CHAP
AU  - Li, Yang; Zhou, Xin; Li, Gang
TI  - Bridging Natural Language and Graphical User Interfaces
PY  - 2021
AB  - “Language as symbolic action” (https://en.wikipedia.org/wiki/Kenneth_Burke) has a natural connection with direct-manipulation interaction (e.g., via GUI or physical appliances) that is common for modern computers such as smartphones. In this chapter, we present our efforts for bridging the gap between natural language and graphical user interfaces, which can potentially enable a broad category of interaction scenarios. Specifically, we develop datasets and deep learning models that can ground natural language instructions or command into executable actions on GUIs, and on the other hand generate natural language descriptions of user interfaces such that a user knows how to control them in language. These projects resemble research efforts intersecting Natural Language Processing (NLP) and HCI, and produce datasets and opensource code that lay a foundation for future research in the area.
SP  - 463
EP  - 493
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_14
ER  - 

TY  - NA
AU  - Ghosh, Shreya; Hayat, Munawar; Dhall, Abhinav; Knibbe, Jarrod
TI  - MTGLS: Multi-Task Gaze Estimation with Limited Supervision
PY  - 2021
AB  - Robust gaze estimation is a challenging task, even for deep CNNs, due to the non-availability of large-scale labeled data. Moreover, gaze annotation is a time-consuming process and requires specialized hardware setups. We propose MTGLS: a Multi-Task Gaze estimation framework with Limited Supervision, which leverages abundantly available non-annotated facial image data. MTGLS distills knowledge from off-the-shelf facial image analysis models, and learns strong feature representations of human eyes, guided by three complementary auxiliary signals: (a) the line of sight of the pupil (i.e. pseudo-gaze) defined by the localized facial landmarks, (b) the head-pose given by Euler angles, and (c) the orientation of the eye patch (left/right eye). To overcome inherent noise in the supervisory signals, MTGLS further incorporates a noise distribution modelling approach. Our experimental results show that MTGLS learns highly generalized representations which consistently perform well on a range of datasets. Our proposed framework outperforms the unsupervised state-of-the-art on CAVE (by 6.43%) and even supervised state-of-the-art methods on Gaze360 (by 6.59%) datasets.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Chu, Eric; Zaman, Loutfouz
TI  - Exploring alternatives with Unreal Engine’s Blueprints Visual Scripting System
PY  - 2021
AB  - NA
SP  - 100388
EP  - NA
JF  - Entertainment Computing
VL  - 36
IS  - NA
PB  - 
DO  - 10.1016/j.entcom.2020.100388
ER  - 

TY  - JOUR
AU  - Ross, Anne Spencer
TI  - An epidemiology-inspired, large-scale analysis of mobile app accessibility
PY  - 2020
AB  - My research explores large-scale analysis of mobile application accessibility, with a goal of expanding the types of accessibility barriers for which applications are tested and the ways in which results are presented. I am basing my research in a novel epidemiology-inspired framework that we developed for structuring the examination of mobile application accessibility. It frames accessibility as a product of diverse factors within the rich ecosystem in which applications are created, maintained, and used. The framework proposes large-scale analysis as a method for understanding the state of accessibility. My completed work applies this framework by conducting a large-scale analysis of an existing dataset of Android applications, assessing them for label-based accessibility barriers on image-based buttons. My future work will extend this research to analyze current applications at scale and expand the accessibility barriers that are tested for.
SP  - 1
EP  - 1
JF  - ACM SIGACCESS Accessibility and Computing
VL  - 123
IS  - 123
PB  - 
DO  - 10.1145/3386402.3386408
ER  - 

TY  - NA
AU  - Dogan, Mustafa Doga; Taka, Ahmad; Lu, Michael; Zhu, Yunyi; Kumar, Akshat; Gupta, Aakar; Mueller, Stefanie
TI  - InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools
PY  - 2022
AB  - Existing approaches for embedding unobtrusive tags inside 3D objects require either complex fabrication or high-cost imaging equipment. We present InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost near-infrared cameras. We achieve this by printing objects from an infrared-transmitting filament, which infrared cameras can see through, and by having air gaps inside for the tag's bits, which appear at a different intensity in the infrared image. We built a user interface that facilitates the integration of common tags (QR codes, ArUco markers) with the object geometry to make them 3D printable as InfraredTags. We also developed a low-cost infrared imaging module that augments existing mobile devices and decodes tags using our image processing pipeline. Our evaluation shows that the tags can be detected with little near-infrared illumination (0.2lux) and from distances as far as 250cm. We demonstrate how our method enables various applications, such as object tracking and embedding metadata for augmented reality and tangible interactions.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501951
ER  - 

TY  - JOUR
AU  - Zhu, Penghua; Li, Ying; Li, Tongyu; Yang, Wei; Xu, Yihan
TI  - GUI Widget Detection and Intent Generation via Image Understanding
PY  - 2021
AB  - NA
SP  - 160697
EP  - 160707
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3131753
ER  - 

TY  - NA
AU  - Li, Nianlong; Han, Teng; Tian, Feng; Huang, Jin; Sun, Minghui; Irani, Pourang; Alexander, Jason
TI  - CHI - Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen
PY  - 2020
AB  - The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376698
ER  - 

TY  - JOUR
AU  - Li, Jianjun; Fei, Jialuo; Cheng, Shichao; Tang, Zheng; Hui, Guobao
TI  - TSG-net: a residual-based informing network for 3D Gaze estimation
PY  - 2021
AB  - NA
SP  - 3647
EP  - 3662
JF  - Multimedia Tools and Applications
VL  - 81
IS  - 3
PB  - 
DO  - 10.1007/s11042-021-11666-6
ER  - 

TY  - NA
AU  - Ang, Gary; Lim, Ee Peng
TI  - Learning User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 27th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490099.3511143
ER  - 

TY  - NA
AU  - He, Zecheng; Sunkara, Srinivas; Zang, Xiaoxue; Xu, Ying; Liu, Lijuan; Wichers, Nevan; Schubiner, Gabriel; Lee, Ruby B.; Chen, Jindong; Aguera y Arcas, Blaise
TI  - ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces
PY  - 2020
AB  - As mobile devices are becoming ubiquitous, regularly interacting with a variety of user interfaces (UIs) is a common aspect of daily life for many people. To improve the accessibility of these devices and to enable their usage in a variety of settings, building models that can assist users and accomplish tasks through the UI is vitally important. However, there are several challenges to achieve this. First, UI components of similar appearance can have different functionalities, making understanding their function more important than just analyzing their appearance. Second, domain-specific features like Document Object Model (DOM) in web pages and View Hierarchy (VH) in mobile applications provide important signals about the semantics of UI elements, but these features are not in a natural language format. Third, owing to a large diversity in UIs and absence of standard DOM or VH representations, building a UI understanding model with high coverage requires large amounts of training data. Inspired by the success of pre-training based approaches in NLP for tackling a variety of problems in a data-efficient way, we introduce a new pre-trained UI representation model called ActionBert. Our methodology is designed to leverage visual, linguistic and domain-specific features in user interaction traces to pre-train generic feature representations of UIs and their components. Our key intuition is that user actions, e.g., a sequence of clicks on different UI components, reveals important information about their functionality. We evaluate the proposed model on a wide variety of downstream tasks, ranging from icon classification to UI component retrieval based on its natural language description. Experiments show that the proposed ActionBert model outperforms multi-modal baselines across all downstream tasks by up to 15.5%.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wacker, Philipp; Nowak, Oliver; Voelker, Simon; Borchers, Jan
TI  - MobileHCI - Evaluating Menu Techniques for Handheld AR with a Smartphone & Mid-Air Pen
PY  - 2020
AB  - Adding a mid-air pen to Handheld Augmented Reality creates a new kind of bimanual interaction for which many fundamental interaction design questions have not been answered yet. In particular, menus are an essential component in most visual interfaces, but it is unclear how to best interact with them in this setting: using the pen in mid-air or on a surface, using the touchscreen, or by moving the smartphone itself. We compared basic menus for these methods by analyzing success rates, selection times, device movement, and subjective ratings. Our results indicate that interacting with a mid-air menu using the pen, and operating a menu with the hand holding the smartphone, are sufficiently competitive to the current standard of two-handed touchscreen interaction, so that interaction designers can freely choose among them based on the interaction context of their application.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403548
ER  - 

TY  - JOUR
AU  - Omori, Yuki; Shima, Yoshihiro
TI  - Image Augmentation for Eye Contact Detection Based on Combination of Pre-trained Alex-Net CNN and SVM
PY  - 2020
AB  - NA
SP  - 85
EP  - 97
JF  - Journal of Computers
VL  - 15
IS  - 3
PB  - 
DO  - 10.17706/jcp.15.3.85-97
ER  - 

TY  - NA
AU  - Feldman, Molly Q.; Wang, Yiting; Byrd, William E.; Guimbretière, François; Andersen, Erik
TI  - Towards answering “Am I on the right track?” automatically using program synthesis
PY  - 2019
AB  - Students learning to program often need help completing assignments and understanding why their code does not work as they expect it to. One common place where they seek such help is at teaching assistant office hours. We found that teaching assistants in introductory programming (CS1) courses frequently answer some variant of the question ``Am I on the Right Track?''. The goal of this work is to develop an automated tool that provides similar feedback for students in real-time from within an IDE as they are writing their program. Existing automated tools lack the generality that we seek, often assuming a single approach to a problem, using hand-coded error models, or applying sample fixes from other students. In this paper, we explore the use of program synthesis to provide less constrained automated answers to ``Am I on the Right Track'' (AIORT) questions. We describe an observational study of TA-student interactions that supports targeting AIORT questions, as well as the development of and design considerations behind a prototype integrated development environment (IDE). The IDE uses an existing program synthesis engine to determine if a student is on the right track and we present pilot user studies of its use.
SP  - NA
EP  - NA
JF  - Proceedings of the 2019 ACM SIGPLAN Symposium on SPLASH-E
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3358711.3361626
ER  - 

TY  - NA
AU  - Ghosh, Shreya; Hayat, Munawar; Dhall, Abhinav; Knibbe, Jarrod
TI  - MTGLS: Multi-Task Gaze Estimation with Limited Supervision
PY  - 2022
AB  - Robust gaze estimation is a challenging task, even for deep CNNs, due to the non-availability of large-scale labeled data. Moreover, gaze annotation is a time-consuming process and requires specialized hardware setups. We propose MTGLS: a Multi-Task Gaze estimation framework with Limited Supervision, which leverages abundantly available non-annotated facial image data. MTGLS distills knowledge from off-the-shelf facial image analysis models, and learns strong feature representations of human eyes, guided by three complementary auxiliary signals: (a) the line of sight of the pupil (i.e. pseudo-gaze) defined by the localized facial landmarks, (b) the head-pose given by Euler angles, and (c) the orientation of the eye patch (left/right eye). To overcome inherent noise in the supervisory signals, MT-GLS further incorporates a noise distribution modelling approach. Our experimental results show that MTGLS learns highly generalized representations which consistently perform well on a range of datasets. Our proposed framework outperforms the unsupervised state-of-the-art on CAVE (by ∼ 6.43%) and even supervised state-of-the-art methods on Gaze360 (by ∼ 6.59%) datasets.
SP  - NA
EP  - NA
JF  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/wacv51458.2022.00123
ER  - 

TY  - CHAP
AU  - Li, Toby Jia-Jun; Mitchell, Tom M.; Myers, Brad A.
TI  - Demonstration + Natural Language: Multimodal Interfaces for GUI-based Interactive Task Learning Agents
PY  - 2021
AB  - We summarize our past five years of work on designing, building, and studying Sugilite, an interactive task learning agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations leveraging the graphical user interfaces (GUIs) of third-party mobile apps. Through its multi-modal and mixed-initiative approaches for Human-AI interaction, Sugilite made important contributions in improving the usability, applicability, generalizability, flexibility, robustness, and shareability of interactive task learning agents. Sugilite also represents a new human-AI interaction paradigm for interactive task learning, where it uses existing app GUIs as a medium for users to communicate their intents with an AI agent instead of the interfaces for users to interact with the underlying computing services. In this chapter, we describe the Sugilite system, explain the design and implementation of its key features, and show a prototype in the form of a conversational assistant on Android.
SP  - 495
EP  - 537
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_15
ER  - 

TY  - NA
AU  - Serra, Gerard; Miralles, David
TI  - Coevo: a collaborative design platform with artificial agents.
PY  - 2019
AB  - We present Coevo, an online platform that allows both humans and artificial agents to design shapes that solve different tasks. Our goal is to explore common shared design tools that can be used by humans and artificial agents in a context of creation. This approach can provide a better knowledge transfer and interaction with artificial agents since a common language of design is defined. In this paper, we outline the main components of this platform and discuss the definition of a human-centered language to enhance human-AI collaboration in co-creation scenarios.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Nguyen, David D.; Nepal, Surya; Kanhere, Salil S.
TI  - ACM Multimedia - Diverse Multimedia Layout Generation with Multi Choice Learning
PY  - 2021
AB  - Designing visually appealing layouts for multimedia documents containing text, graphs and images requires a form of creative intelligence. Modelling the generation of layouts has recently gained attention due to its importance in aesthetics and communication style. In contrast to standard prediction tasks, there are a range of acceptable layouts which depend on user preferences. For example, a poster designer may prefer logos on the top-left while another prefers logos on the bottom-right. Both are correct choices yet existing machine learning models treat layouts as a single choice prediction problem. In such situations, these models would simply average over all possible choices given the same input forming a degenerate sample. In the above example, this would form an unacceptable layout with a logo in the centre. In this paper, we present an auto-regressive neural network architecture, called LayoutMCL, that uses multi-choice prediction and winner-takes-all loss to effectively stabilise layout generation. LayoutMCL avoids the averaging problem by using multiple predictors to learn a range of possible options for each layout object. This enables LayoutMCL to generate multiple and diverse layouts from a single input which is in contrast with existing approaches which yield similar layouts with minor variations. Through quantitative benchmarks on real data (magazine, document and mobile app layouts), we demonstrate that LayoutMCL reduces Frechet Inception Distance (FID) by 83-98% and generates significantly more diversity in comparison to existing approaches.
SP  - 218
EP  - 226
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3475525
ER  - 

TY  - NA
AU  - Ogusu, Reo; Yamanaka, Takao
TI  - FG - LPM: Learnable Pooling Module for Efficient Full-Face Gaze Estimation
PY  - 2019
AB  - Gaze tracking is an important technology in many domains. Techniques such as Convolutional Neural Networks (CNNs) have allowed the invention of the gaze tracking method that relies only on commodity hardware such as the camera on a personal computer. It has been shown that the full-face region for gaze estimation can provide a better performance than the one obtained from eye image alone. However, a problem with using the full-face image is the heavy computation due to the larger image size. This study tackles this problem through compression of the input full-face image by removing redundant information using a novel learnable pooling module. The module can be trained end-to-end by backpropagation to learn the size of the grid in the pooling filter. The learnable pooling module keeps the resolution of valuable regions high and vice versa. This proposed method preserved the gaze estimation accuracy at a certain level when the image was reduced to a smaller size.
SP  - 1
EP  - 5
JF  - 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/fg.2019.8756523
ER  - 

TY  - CHAP
AU  - Chhetri, Sujit Rokka; Faruque, Mohammad Abdullah Al
TI  - Non-euclidean Data-Driven Modeling Using Graph Convolutional Neural Networks
PY  - 2020
AB  - So far in the previous chapters, we have utilized euclidean data for performing data-driven modeling. In Part IV of this book, we focus our attention on data-driven modeling algorithms for non-euclidean data. We specifically focus on developing algorithms for handling non-euclidean data present in cyber-physical systems. To this end, in this chapter we present a novel non-euclidean data-driven modeling approach using graph convolutional neural network.
SP  - 185
EP  - 207
JF  - Data-Driven Modeling of Cyber-Physical Systems using Side-Channel Analysis
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-37962-9_9
ER  - 

TY  - CHAP
AU  - Cheng, Yihua; Lu, Feng; Zhang, Xucong
TI  - ECCV (14) - Appearance-Based Gaze Estimation via Evaluation-Guided Asymmetric Regression
PY  - 2018
AB  - Eye gaze estimation has been increasingly demanded by recent intelligent systems to accomplish a range of interaction-related tasks, by using simple eye images as input. However, learning the highly complex regression between eye images and gaze directions is nontrivial, and thus the problem is yet to be solved efficiently. In this paper, we propose the Asymmetric Regression-Evaluation Network (ARE-Net), and try to improve the gaze estimation performance to its full extent. At the core of our method is the notion of “two eye asymmetry” observed during gaze estimation for the left and right eyes. Inspired by this, we design the multi-stream ARE-Net; one asymmetric regression network (AR-Net) predicts 3D gaze directions for both eyes with a novel asymmetric strategy, and the evaluation network (E-Net) adaptively adjusts the strategy by evaluating the two eyes in terms of their performance during optimization. By training the whole network, our method achieves promising results and surpasses the state-of-the-art methods on multiple public datasets.
SP  - 105
EP  - 121
JF  - Computer Vision – ECCV 2018
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-01264-9_7
ER  - 

TY  - JOUR
AU  - Kikuchi, Kotaro; Otani, Mayu; Yamaguchi, Kota; Simo-Serra, Edgar
TI  - Modeling Visual Containment for Web Page Layout Optimization
PY  - 2021
AB  - NA
SP  - 33
EP  - 44
JF  - Computer Graphics Forum
VL  - 40
IS  - 7
PB  - 
DO  - 10.1111/cgf.14399
ER  - 

TY  - NA
AU  - Liu, Zhe
TI  - ASE - Discovering UI display issues with visual understanding
PY  - 2020
AB  - GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on difference devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues. We develop a heuristics-based data augmentation method and a GAN-based data augmentation method for boosting the performance of our OwlEye. At present, the evaluation demonstrates that our OwlEye can achieve 85% precision and 84% recall in detecting UI display issues, and 90% accuracy in localizing these issues.
SP  - 1373
EP  - 1375
JF  - Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3324884.3418917
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Chen, Jingya; Mitchell, Tom M.; Myers, Brad A.
TI  - Towards Effective Human-AI Collaboration in GUI-Based Interactive Task Learning Agents
PY  - 2020
AB  - We argue that a key challenge in enabling usable and useful interactive task learning for intelligent agents is to facilitate effective Human-AI collaboration. We reflect on our past 5 years of efforts on designing, developing and studying the SUGILITE system, discuss the issues on incorporating recent advances in AI with HCI principles in mixed-initiative interactions and multi-modal interactions, and summarize the lessons we learned. Lastly, we identify several challenges and opportunities, and describe our ongoing work
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Lüthi, Guy; Fender, Andreas Rene; Holz, Christian
TI  - DeltaPen: A Device with Integrated High-Precision Translation and Rotation Sensing on Passive Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545655
ER  - 

TY  - JOUR
AU  - Di Sorbo, Andrea; Panichella, Sebastiano
TI  - Exposed! A case study on the vulnerability-proneness of Google Play Apps
PY  - 2021
AB  - Mobile applications are used for accomplishing everyday life activities, such as shopping, banking, and social communications. To leverage the features of mobile apps, users often need to share sensitive information. However, recent research demonstrated that most of such apps present critical security and privacy defects. In this context, we define as vulnerability-proneness the risk level(s) that users meet in downloading specific apps, to better understand whether (1) users select apps with lower risk levels and if (2) vulnerability-proneness of an app might affect its success. We use as proxy to measure such risk level the “number of different types of potential security issues exhibited by the app”. We conjecture that the vulnerability-proneness levels may vary based on (i) the types of data handled by the app, and (ii) the operations for which the app is supposed to be used. Hence, we investigate how the vulnerability-proneness of apps varies when observing (i) different app categories, and (ii) apps with different success levels. Finally, to increase the awareness of both users and developers on the vulnerability-proneness of apps, we evaluate the extent to which contextual information provided by the app market can be exploited to estimate the vulnerability-proneness levels of mobile apps. Results of our study show that apps in the Medical category exhibit the lowest levels of vulnerability-proneness. Besides, while no strong relations between vulnerability-proneness and average rating are observed, apps with a higher number of downloads tend to have higher vulnerability-proneness levels, but lower vulnerability-proneness density. Finally, we found that apps’ contextual information can be used to predict, in the early stages, the vulnerability-proneness levels of mobile apps.
SP  - 78
EP  - NA
JF  - Empirical Software Engineering
VL  - 26
IS  - 4
PB  - 
DO  - 10.1007/s10664-021-09978-0
ER  - 

TY  - NA
AU  - Peng, Chao; Zhang, Zhao; Lv, Zhengwei; Yang, Ping
TI  - MUBot: Learning to Test Large-Scale Commercial Android Apps like a Human
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icsme55016.2022.00074
ER  - 

TY  - NA
AU  - Liu, Zhihao; Zhang, Fanxing; Cheng, Zhanglin
TI  - ISMAR - BuildingSketch: Freehand Mid-Air Sketching for Building Modeling
PY  - 2021
AB  - Advancements in virtual reality (VR) technology enable us to rethink the way of interactive 3D modeling - intuitively creating 3D content directly in 3D space. However, conventional VR-based modeling is laborious and tedious to generate a detailed 3D model in full manual mode since users need to carefully draw almost the entire surface. In this paper, we present a freehand mid-air sketching system with the aid of deep learning techniques for modeling structured buildings, where the user freely draws a few key strokes in mid-air using his/her fingers to represent the desired shapes and our system automatically interprets the strokes using a deep neural network and generates a detailed building model based on a procedural modeling method. After creating several building blocks one by one, the user can freely move, rotate, and combine the blocks to form a complex building model. We demonstrate the ease of use for novice users, effectiveness, and efficiency of our sketching system, BuildingSketch, by presenting a variety of building models.
SP  - 329
EP  - 338
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00049
ER  - 

TY  - JOUR
AU  - Yang, Eun Kyoung; Lee, Jee Hyun
TI  - Cognitive impact of virtual reality sketching on designers’ concept generation
PY  - 2020
AB  - This study focuses on identifying the cognitive impact of virtual reality (VR) sketching on the concept generation phase of the design process. For an empirical study to analyse the cognitive influ...
SP  - 82
EP  - 97
JF  - Digital Creativity
VL  - 31
IS  - 2
PB  - 
DO  - 10.1080/14626268.2020.1726964
ER  - 

TY  - NA
AU  - Faust, Rebecca; Isaacs, Katherine E.; Bernstein, William Z.; Sharp, Michael; Scheidegger, Carlos
TI  - Anteater: Interactive Visualization for Program Understanding
PY  - 2019
AB  - Understanding and debugging long, complex programs can be extremely difficult; it often includes significant, manual program instrumentation and searches through source files. In this paper, we present Anteater, an interactive visualization system for tracing and exploring the execution of a program. While existing debugging tools often have visualization components, these components are often added on top of an existing environment. In Anteater, in contrast, visualization is a driving concern. We answer the following question: what should a debugging tool look like if it were designed from the ground up to support interactive visualization principles? Anteater automatically instruments source code to capture execution behavior along with variables and expressions specified by the user. After generating the execution trace, Anteater presents the execution information through interactive visualizations. Anteater supports interactions that help with tasks such as discovering important structures in the execution, learning dependencies, and understanding and debugging unexpected behaviors. To assess the utility of Anteater, we present several real-world case studies that show how Anteater compares favorably to existing approaches. Finally, we discuss limitations of our system and where further research is needed.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Moran, Kevin; Yachnes, Ali; Purnell, George; Mahmud, Junayed; Tufano, Michele; Cardenas, Carlos Bernal; Poshyvanyk, Denys; H'Doubler, Zach
TI  - An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation
PY  - 2022
AB  - Existing automated techniques for software documentation typically attempt to reason between two main sources of information: code and natural language. However, this reasoning process is often complicated by the lexical gap between more abstract natural language and more structured programming languages. One potential bridge for this gap is the Graphical User Interface (GUI), as GUIs inherently encode salient information about underlying program functionality into rich, pixel-based data representations. This paper offers one of the first comprehensive empirical investigations into the connection between GUIs and functional, natural language descriptions of software. First, we collect, analyze, and open source a large dataset of functional GUI descriptions consisting of 45,998 descriptions for 10,204 screenshots from popular Android applications. The descriptions were obtained from human labelers and underwent several quality control mechanisms. To gain insight into the representational potential of GUIs, we investigate the ability of four Neural Image Captioning models to predict natural language descriptions of varying granularity when provided a screenshot as input. We evaluate these models quantitatively, using common machine translation metrics, and qualitatively through a large-scale user study. Finally, we offer learned lessons and a discussion of the potential shown by multimodal models to enhance future techniques for automated software documentation.
SP  - NA
EP  - NA
JF  - 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/saner53432.2022.00069
ER  - 

TY  - NA
AU  - Choi, Junyoung; Hyun, Minsung; Kwak, Nojun
TI  - Task-oriented Design through Deep Reinforcement Learning.
PY  - 2019
AB  - We propose a new low-cost machine-learning-based methodology which assists designers in reducing the gap between the problem and the solution in the design process. Our work applies reinforcement learning (RL) to find the optimal task-oriented design solution through the construction of the design action for each task. For this task-oriented design, the 3D design process in product design is assigned to an action space in Deep RL, and the desired 3D model is obtained by training each design action according to the task. By showing that this method achieves satisfactory design even when applied to a task pursuing multiple goals, we suggest the direction of how machine learning can contribute to the design process. Also, we have validated with product designers that this methodology can assist the creative part in the process of design.
SP  - NA
EP  - NA
JF  - arXiv: Learning
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Balaji, Ananta Narayanan; Kimber, Clayton; Li, David; Wu, Shengzhi; Du, Ruofei; Kim, David
TI  - RetroSphere
PY  - 2022
AB  - <jats:p>Advanced AR/VR headsets often have a dedicated depth sensor or multiple cameras, high processing power, and a high-capacity battery to track hands or controllers. However, these approaches are not compatible with the small form factor and limited thermal capacity of lightweight AR devices. In this paper, we present RetroSphere, a self-contained 6 degree of freedom (6DoF) controller tracker that can be integrated with almost any device. RetroSphere tracks a passive controller with just 3 retroreflective spheres using a stereo pair of mass-produced infrared blob trackers, each with its own infrared LED emitters. As the sphere is completely passive, no electronics or recharging is required. Each object tracking camera provides a tiny Arduino-compatible ESP32 microcontroller with the 2D position of the spheres. A lightweight stereo depth estimation algorithm that runs on the ESP32 performs 6DoF tracking of the passive controller. Also, RetroSphere provides an auto-calibration procedure to calibrate the stereo IR tracker setup. Our work builds upon Johnny Lee's Wii remote hacks and aims to enable a community of researchers, designers, and makers to use 3D input in their projects with affordable off-the-shelf components. RetroSphere achieves a tracking accuracy of about 96.5% with errors as low as ~3.5 cm over a 100 cm tracking range, validated with ground truth 3D data obtained using a LIDAR camera while consuming around 400 mW. We provide implementation details, evaluate the accuracy of our system, and demonstrate example applications, such as mobile AR drawing, 3D measurement, etc. with our Retrosphere-enabled AR glass prototype.</jats:p>
SP  - 1
EP  - 36
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569479
ER  - 

TY  - NA
AU  - Guo, Mengxi; Huang, Danqing; Xie, Xiaodong
TI  - The Layout Generation Algorithm of Graphic Design Based on Transformer-CVAE
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 2021 International Conference on Signal Processing and Machine Learning (CONF-SPML)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/conf-spml54095.2021.00049
ER  - 

TY  - CHAP
AU  - Yao, Xulu; Yap, Moi Hoon; Zhang, Yanlong
TI  - An Automatic GUI Generation Method Based on Generative Adversarial Network
PY  - 2022
AB  - AbstractAs a technique applied with artificial neural networks, deep learning is widely used in the field of image recognition. However, a lack of available datasets leads to imperfect model learning. By analysing the data scale requirements of deep learning and aiming at the application in graphical user interface (GUI) generation, it was found that the collection of GUI datasets is a time- and labour-consuming project. This makes it difficult to meet the dataset needs of current deep learning networks. To solve this problem, we propose the user interface generative adversarial network (UIGAN), a semi-supervised deep learning model, to produce a large number of reliable GUI datasets. By combining a cyclic neural network with a generated countermeasure network, UIGAN can learn the sequence relationship and characteristics of data, make the generated countermeasure network generate reasonable data, and then expand the selected Rico dataset. Relying on the network structure, the characteristics of collected data can be well analysed, and a large number of reasonable data can be generated according to them. After data processing, a reliable dataset for model training can be formed, which alleviates the problem of dataset shortage in deep learning.KeywordsGUIDeep learningGANData augmentation
SP  - 641
EP  - 653
JF  - Proceedings of Seventh International Congress on Information and Communication Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-19-2394-4_59
ER  - 

TY  - NA
AU  - Zhou, Hao; Chen, Ting; Wang, Haoyu; Yu, Le; Luo, Xiapu; Wang, Ting; Zhang, Wei
TI  - ASE - UI obfuscation and its effects on automated UI analysis for Android apps
PY  - 2020
AB  - The UI driven nature of Android apps has motivated the development of automated UI analysis for various purposes, such as app analysis, malicious app detection, and app testing. Although existing automated UI analysis methods have demonstrated their capability in dissecting apps' UI, little is known about their effectiveness in the face of app protection techniques, which have been adopted by more and more apps. In this paper, we take a first step to systematically investigate UI obfuscation for Android apps and its effects on automated UI analysis. In particular, we point out the weaknesses in existing automated UI analysis methods and design 9 UI obfuscation approaches. We implement these approaches in a new tool named UIObfuscator after tackling several technical challenges. Moreover, we feed 3 kinds of tools that rely on automated UI analysis with the apps protected by UIObfuscator, and find that their performances severely drop. This work reveals limitations of automated UI analysis and sheds light on app protection techniques.
SP  - 199
EP  - 210
JF  - Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3324884.3416642
ER  - 

TY  - NA
AU  - Kuznetsov, Konstantin; Fu, Chen; Gao, Song; Jansen, David N.; Zhang, Lijun; Zeller, Andreas
TI  - What do all these Buttons do? Statically Mining Android User Interfaces at Scale.
PY  - 2021
AB  - We introduce FRONTMATTER: a tool to automatically mine both user interface models and behavior of Android apps at a large scale with high precision. Given an app, FRONTMATTER statically extracts all declared screens, the user interface elements, their textual and graphical features, as well as Android APIs invoked by interacting with them. Executed on tens of thousands of real-world apps, FRONTMATTER opens the door for comprehensive mining of mobile user interfaces, jumpstarting empirical research at a large scale, addressing questions such as "How many travel apps require registration?", "Which apps do not follow accessibility guidelines?", "Does the user interface correspond to the description?", and many more. FRONTMATTER and the mined dataset are available under an open-source license.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Bâce, Mihai; Staal, Sander; Bulling, Andreas
TI  - Accurate and Robust Eye Contact Detection During Everyday Mobile Device Interactions.
PY  - 2019
AB  - Quantification of human attention is key to several tasks in mobile human-computer interaction (HCI), such as predicting user interruptibility, estimating noticeability of user interface content, or measuring user engagement. Previous works to study mobile attentive behaviour required special-purpose eye tracking equipment or constrained users' mobility. We propose a novel method to sense and analyse visual attention on mobile devices during everyday interactions. We demonstrate the capabilities of our method on the sample task of eye contact detection that has recently attracted increasing research interest in mobile HCI. Our method builds on a state-of-the-art method for unsupervised eye contact detection and extends it to address challenges specific to mobile interactive scenarios. Through evaluation on two current datasets, we demonstrate significant performance improvements for eye contact detection across mobile devices, users, or environmental conditions. Moreover, we discuss how our method enables the calculation of additional attention metrics that, for the first time, enable researchers from different domains to study and quantify attention allocation during mobile interactions in the wild.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Yang, Zhiyuan; Yang, Wenbo; Yang, Guang; Yang, Changyuan
TI  - A Co-creation Interaction Framework and Its Application for Intelligent Design System
PY  - 2022
AB  - NA
SP  - 343
EP  - 355
JF  - Human-Computer Interaction. Theoretical Approaches and Design Methods
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05311-5_24
ER  - 

TY  - NA
AU  - Lee, Bokyung; Lee, Michael S.; Mogk, Jeremy P.M.; Goldstein, Rhys; Bibliowicz, Jacobo; Tessier, Alexander
TI  - Conference on Designing Interactive Systems - Designing a Multi-Agent Occupant Simulation System to Support Facility Planning and Analysis for COVID-19
PY  - 2021
AB  - The COVID-19 pandemic changed our lives, forcing us to reconsider our built environment, architectural designs, and even behaviours. Multiple stakeholders, including designers, building facility managers, and policy makers, are making decisions to reduce SARS-CoV-2 virus transmission and make our environment safer; however, systems to effectively and interactively evaluate virus transmission in physical spaces are lacking. To help fill this gap, we propose OccSim, a system that automatically generates occupancy behaviours in a 3D model of a building and helps users analyze the potential effect of virus transmission from a large-scale and longitudinal perspective. Our participatory evaluation with four groups of stakeholders revealed that OccSim could enhance their decision making processes by identifying specific risks of virus transmission in advance, and illuminating how each risk relates to complex human-building interactions. We reflect on our design and discuss OccSim’s potential implications in the domains of ‘design evaluation,’ ‘generative design,’ and ‘digital twins.’
SP  - 15
EP  - 30
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462030
ER  - 

TY  - NA
AU  - Ross, Anne Spencer; Zhang, Xiaoyi; Fogarty, James; Wobbrock, Jacob O.
TI  - ASSETS - Examining Image-Based Button Labeling for Accessibility in Android Apps through Large-Scale Analysis
PY  - 2018
AB  - We conduct the first large-scale analysis of the accessibility of mobile apps, examining what unique insights this can provide into the state of mobile app accessibility. We analyzed 5,753 free Android apps for label-based accessibility barriers in three classes of image-based buttons: Clickable Images, Image Buttons, and Floating Action Buttons. An epidemiology-inspired framework was used to structure the investigation. The population of free Android apps was assessed for label-based inaccessible button diseases. Three determinants of the disease were considered: missing labels, duplicate labels, and uninformative labels. The prevalence, or frequency of occurrences of barriers, was examined in apps and in classes of image-based buttons. In the app analysis, 35.9% of analyzed apps had 90% or more of their assessed image-based buttons labeled, 45.9% had less than 10% of assessed image-based buttons labeled, and the remaining apps were relatively uniformly distributed along the proportion of elements that were labeled. In the class analysis, 92.0% of Floating Action Buttons were found to have missing labels, compared to 54.7% of Image Buttons and 86.3% of Clickable Images. We discuss how these accessibility barriers are addressed in existing treatments, including accessibility development guidelines.
SP  - 119
EP  - 130
JF  - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3234695.3236364
ER  - 

TY  - NA
AU  - da Cunha, Kelvin B.; Brito, Caio Jose dos Santos; Valenca, Luas; Francisco, Simoes; Teichrieb, Veronica
TI  - SIBGRAPI - A Study on the Impact of Domain Randomization for Monocular Deep 6DoF Pose Estimation
PY  - 2020
AB  - In this work, we apply domain randomization to synthetic images and train deep 6DoF monocular RGB pose estimation models to work on a real object. We compare 19 models trained with different combinations of synthetic and real data (fully synthetic, fully real, initially synthetic and supplemented with real, and a real-synthetic randomized mix). By gradually decreasing the amount of real data used, we show it is possible for deep 6DoF detection to obtain superior results while using less real data (which is harder to obtain) and suggest the best approach to train a model with synthetic data. Our method is validated using a textureless 3D printed object, as the textureless category is a challenging, common open problem in itself. A real and a synthetic dataset generated for this work, totalling over 24,800 annotated frames, are also made public. We also show that synthetic, randomized data can help generalize a model by training it to handle challenges such as illumination changes and fast motion. Finally, we also evaluate how a model trained for one camera sensor works with a different one, and show that synthetic simulations of real cameras can help overcoming this challenge.
SP  - 332
EP  - 339
JF  - 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sibgrapi51738.2020.00052
ER  - 

TY  - JOUR
AU  - Bessghaier, Narjes; Soui, Makram; Ghaibi, Nadia
TI  - Towards the automatic restructuring of structural aesthetic design of Android user interfaces
PY  - 2022
AB  - NA
SP  - 103598
EP  - NA
JF  - Computer Standards & Interfaces
VL  - 81
IS  - NA
PB  - 
DO  - 10.1016/j.csi.2021.103598
ER  - 

TY  - NA
AU  - Ghosh, Shreya; Dhall, Abhinav; Sharma, Garima; Gupta, Sarthak; Sebe, Nicu
TI  - Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver Gaze Zone Estimation Dataset
PY  - 2020
AB  - Labelling of human behavior analysis data is a complex and time consuming task. In this paper, a fully automatic technique for labelling an image based gaze behavior dataset for driver gaze zone estimation is proposed. Domain knowledge can be added to the data recording paradigm and later labels can be generated in an automatic manner using speech to text conversion. In order to remove the noise in STT due to different ethnicity, the speech frequency and energy are analysed. The resultant Driver Gaze in the Wild DGW dataset contains 586 recordings, captured during different times of the day including evening. The large scale dataset contains 338 subjects with an age range of 18-63 years. As the data is recorded in different lighting conditions, an illumination robust layer is proposed in the Convolutional Neural Network (CNN). The extensive experiments show the variance in the database resembling real-world conditions and the effectiveness of the proposed CNN pipeline. The proposed network is also fine-tuned for the eye gaze prediction task, which shows the discriminativeness of the representation learnt by our network on the proposed DGW dataset.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Sain, Aneeshan; Bhunia, Ayan Kumar; Yang, Yongxin; Xiang, Tao; Song, Yi-Zhe
TI  - Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval
PY  - 2020
AB  - Sketch as an image search query is an ideal alternative to text in capturing the fine-grained visual details. Prior successes on fine-grained sketch-based image retrieval (FG-SBIR) have demonstrated the importance of tackling the unique traits of sketches as opposed to photos, e.g., temporal vs. static, strokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a further trait of sketches that has been overlooked to date, that is, they are hierarchical in terms of the levels of detail -- a person typically sketches up to various extents of detail to depict an object. This hierarchical structure is often visually distinct. In this paper, we design a novel network that is capable of cultivating sketch-specific hierarchies and exploiting them to match sketch with photo at corresponding hierarchical levels. In particular, features from a sketch and a photo are enriched using cross-modal co-attention, coupled with hierarchical node fusion at every level to form a better embedding space to conduct retrieval. Experiments on common benchmarks show our method to outperform state-of-the-arts by a significant margin.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Tao; Liu, Ying; Gao, Jerry; Gao, Lipeng; Cheng, Jing
TI  - Deep Learning-Based Mobile Application Isomorphic GUI Identification for Automated Robotic Testing
PY  - 2020
AB  - Fully black-box robotic testing is needed given the popularity of mobile applications. A critical constraining issue for generating graphical user interface (GUI) models is identifying isomorphic GUIs. We present a deep learningbased end-to-end trainable model to determine the similarity between GUIs and identify isomorphic GUIs.
SP  - 67
EP  - 74
JF  - IEEE Software
VL  - 37
IS  - 4
PB  - 
DO  - 10.1109/ms.2020.2987044
ER  - 

TY  - NA
AU  - Kolthoff, Kristian; Bartelt, Christian; Ponzetto, Simone Paolo
TI  - ASE - GUI2WiRe: rapid wireframing with a mined and large-scale GUI repository using natural language requirements
PY  - 2020
AB  - High-fidelity Graphical User Interface (GUI) prototyping is a well-established and suitable method for enabling fruitful discussions, clarification and refinement of requirements formulated by customers. GUI prototypes can help to reduce misunderstandings between customers and developers, which may occur due to the ambiguity comprised in informal Natural Language (NL). However, a disadvantage of employing high-fidelity GUI prototypes is their time-consuming and expensive development. Common GUI prototyping tools are based on combining individual GUI components or manually crafted templates. In this work, we present GUI2WiRe, a tool that enables users to retrieve GUI prototypes from a semiautomatically created large-scale GUI repository for mobile applications matching user requirements specified in Natural Language (NLR). We extract multiple text segments from the GUI hierarchy data and employ various Information Retrieval (IR) models and Automatic Query Expansion (AQE) techniques to achieve ad-hoc GUI retrieval from NLR. Retrieved GUI prototypes mined from applications can be inserted in the graphical editor of GUI2WiRe to rapidly create wireframes. GUI components are extracted automatically from the GUI screenshots and basic editing functionality is provided to the user. Finally, a preview of the application is created from the wireframe to allow interactive exploration of the current design. We evaluated the applied IR and AQE approaches for their effectiveness in terms of GUI retrieval relevance on a manually annotated collection of NLR and discuss our planned user studies. Video presentation of GUI2WiRe: https://youtu.be/2nN-Xr2Hk7I
SP  - 1297
EP  - 1301
JF  - Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3324884.3415289
ER  - 

TY  - NA
AU  - Guo, Philip J.
TI  - UIST - Ten Million Users and Ten Years Later: Python Tutor’s Design Guidelines for Building Scalable and Sustainable Research Software in Academia
PY  - 2021
AB  - Research software is often built as prototypes that never get widespread usage and are left unmaintained after a few papers get published. To counteract this trend, we propose a method for building research software with scale and sustainability in mind so that it can organically grow a large userbase and enable longer-term research. To illustrate this method, we present the design and implementation of Python Tutor (pythontutor.com), a code visualization tool that is, to our knowledge, one of the most widely-used pieces of research software developed within a university lab. Over the past decade, it has been used by over ten million people in over 180 countries. It has also contributed to 55 publications from 35 research groups in 13 countries. We distilled lessons from working on Python Tutor into three sets of design guidelines: 1) user experience design for scale and sustainability, 2) software architecture design for long-term sustainability, and 3) designing a sustainable software development workflow within academia. These guidelines can enable a student to create long-lasting software that reaches many users and facilitates research from many independent groups.
SP  - 1235
EP  - 1251
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474819
ER  - 

TY  - NA
AU  - Li, Yuanchun; Yang, Ziyue; Guo, Yao; Chen, Xiangqun
TI  - Humanoid: A Deep Learning-based Approach to Automated Black-box Android App Testing
PY  - 2019
AB  - Automated input generators are widely used for large-scale dynamic analysis of mobile apps. Such input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. In this paper, we propose Humanoid, a deep learning-based approach to GUI test input generation by learning from human interactions. Our insight is that if we can learn from human-generated interaction traces, it is possible to automatically prioritize test inputs based on their importance as perceived by users. We design and implement a deep neural network model to learn how end-users would interact with an app (specifically, which UI elements to interact with and how). Our experiments showed that the interaction model can successfully prioritize user-preferred inputs for any new UI (with a top-1 accuracy of 51.2% and a top-10 accuracy of 85.2%). We implemented an input generator for Android apps based on the learned model and evaluated it on both open-source apps and market apps. The results indicated that Humanoid was able to achieve higher coverage than six state-of-the-art test generators. However, further analysis showed that the learned model was not the main reason of coverage improvement. Although the learned interaction pattern could drive the app into some important GUI states with higher probabilities, it had limited effect on the width and depth of GUI state search, which is the key to improve test coverage in the long term. Whether and how human interaction patterns can be used to improve coverage is still an unknown and challenging problem.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Li, Yang; He, Jiacong; Zhou, Xin; Zhang, Yuan; Baldridge, Jason
TI  - ACL - Mapping Natural Language Instructions to Mobile UI Action Sequences
PY  - 2020
AB  - We present a new problem: grounding natural language instructions to mobile user interface actions, and contribute three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action phrase tuples from long-range natural language instructions. A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions. Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp.
SP  - 8198
EP  - 8210
JF  - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2020.acl-main.729
ER  - 

TY  - JOUR
AU  - Serrano Lasa, Ibon; Mazmela Etxabe, Maitane; Ricondo Iriondo, Itziar
TI  - New application for sketching in a machine tool company
PY  - 2021
AB  - NA
SP  - 64
EP  - 94
JF  - Journal of Engineering Design
VL  - 33
IS  - 1
PB  - 
DO  - 10.1080/09544828.2021.1976398
ER  - 

TY  - NA
AU  - Lukes, Dylan; Sarracino, John; Coleman, Cora; Peleg, Hila; Lerner, Sorin; Polikarpova, Nadia
TI  - ESEC/SIGSOFT FSE - Synthesis of web layouts from examples
PY  - 2021
AB  - We present a new technique for synthesizing dynamic, constraint-based visual layouts from examples. Our technique tackles two major challenges of layout synthesis. First, realistic layouts, especially on the web, often contain hundreds of elements, so the synthesizer needs to scale to layouts of this complexity. Second, in common usage scenarios, examples contain noise, so the synthesizer needs to be tolerant to imprecise inputs. To address these challenges we propose a two-phase approach to synthesis, where a local inference phase rapidly generates a set of likely candidate constraints that satisfy the given examples, and then a global inference phase selects a subset of the candidates that generalizes to unseen inputs. This separation of concerns helps our technique tackle the two challenges: the local phase employs Bayesian inference to handle noisy inputs, while the global phase leverages the hierarchical nature of complex layouts to decompose the global inference problem into inference of independent sub-layouts. We implemented this technique in a tool called Mockdown and evaluated it on nine real-world web layouts, as well as a series of widespread layout components and an existing dataset of 644 Android applications. Our experiments show that Mockdown is able to synthesize a highly accurate layout for the majority of benchmarks from just three examples (two for Android layouts), and that it scales to layouts with over 600 elements, about 30x more than has been reported in prior work on layout synthesis.
SP  - 651
EP  - 663
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3468533
ER  - 

TY  - NA
AU  - Acornley, Christopher
TI  - ICMI - Using Generative Adversarial Networks to Create Graphical User Interfaces for Video Games
PY  - 2021
AB  - Designing and creating a Graphical User Interface (GUI) is a difficult and slow process. It requires a number of professions to all contribute to its development and it can be heavily detrimental to a product if implemented poorly. This research aims to investigate a method of using Generative Adversarial Networks (GANs) to generate new and usable designs for GUIs. GANs are a relatively new architecture for adversarial learning and have been used to good effect in replicating instances of a real dataset. The primary aim is to develop a GAN that is capable of processing a collection of existing GUIs and learn how to replicate these to allow for creation of further designs. These GUI designs need to be formatted in a manner that enables modification, allowing for them to be used by a development team to enhance their production process. Completed work demonstrates numerous approaches at using GANs to create text files that contain the component elements of a GUI. Their results and the release of a similar research paper (GUIGAN) has led to a new approach focusing on more abstract data representation, with a quality control system for ensuring the output data is properly formatted. It is hypothesised that the approach will develop a model capable of creating new, editable GUI designs.
SP  - 802
EP  - 806
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3481273
ER  - 

TY  - NA
AU  - Gupta, Kamal; Lazarow, Justin; Achille, Alessandro; Davis, Larry; Mahadevan, Vijay; Shrivastava, Abhinav
TI  - LayoutTransformer: Layout Generation and Completion with Self-attention
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccv48922.2021.00104
ER  - 

TY  - NA
AU  - Jiang, Zexun; Yin, Hao; Luo, Yan; Gong, Jiaying; Yang, Yuannan; Lin, Manshan
TI  - IPCCC - Quantitative Analysis of Mobile Application User Interface Design
PY  - 2019
AB  - With the development and proliferation of mobile devices, the industry of mobile applications has grown into a massive market. Well-designed user interfaces are essential for a high-quality and popular mobile application. To understand the basic principles of UI designing, this work employs a data-driven quantitative approach. In this work, we build a dataset of mobile UI and propose a systematic approach for quantitative analysis of UI design using five measurable metrics. Our dataset and analytical methods make it possible to correlate the quantitative metrics with the qualitative features so that the quality of UI designs can be reliably studied and optimized. The evaluation shows that these proposed metrics can be correlated with the qualitative features, and some UI designing suggestions or insights are generated.
SP  - 1
EP  - 8
JF  - 2019 IEEE 38th International Performance Computing and Communications Conference (IPCCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ipccc47392.2019.8958722
ER  - 

TY  - NA
AU  - Pandian, Vinoth Pandian Sermuga; Suleri, Sarah; Jarke, Matthias
TI  - IUI Companion - Blu: What GUIs are made of
PY  - 2020
AB  - UI designers look for inspirational examples from existing UI designs during the prototyping process. However, they have to reconstruct these example UI designs from scratch to edit content or apply styling. The existing solution attempts to make UI screens into editable vector graphics using image segmentation techniques. In this research, we aim to use deep learning and gestalt laws-based algorithms to convert UI screens to editable blueprints by identifying the constituent UI element categories, their location, dimension, text content, and layout hierarchy. In this paper, we present a proof-of-concept web application that uses the UI screens and annotations from the RICO dataset and generates an editable blueprint vector graphic, and a UI layout tree. With this research, we aim to support UX designers in reconstructing UI screens and communicating UI layout information to developers.
SP  - 81
EP  - 82
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces Companion
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379336.3381497
ER  - 

TY  - NA
AU  - Shrestha, Nischal; Barik, Titus; Parnin, Chris
TI  - It's Like Python But: Towards Supporting Transfer of Programming Language Knowledge
PY  - 2018
AB  - Expertise in programming traditionally assumes a binary novice-expert divide. Learning resources typically target programmers who are learning programming for the first time, or expert programmers for that language. An underrepresented, yet important group of programmers are those that are experienced in one programming language, but desire to author code in a different language. For this scenario, we postulate that an effective form of feedback is presented as a transfer from concepts in the first language to the second. Current programming environments do not support this form of feedback. In this study, we apply the theory of learning transfer to teach a language that programmers are less familiar with--such as R--in terms of a programming language they already know--such as Python. We investigate learning transfer using a new tool called Transfer Tutor that presents explanations for R code in terms of the equivalent Python code. Our study found that participants leveraged learning transfer as a cognitive strategy, even when unprompted. Participants found Transfer Tutor to be useful across a number of affordances like stepping through and highlighting facts that may have been missed or misunderstood. However, participants were reluctant to accept facts without code execution or sometimes had difficulty reading explanations that are verbose or complex. These results provide guidance for future designs and research directions that can support learning transfer when learning new programming languages.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ramanujam, Murali; Chen, Helen; Mardani, Shaghayegh; Netravali, Ravi
TI  - Floo
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3498361.3538929
ER  - 

TY  - JOUR
AU  - Nathasya, Rossevine Artha; Karnalim, Oscar; Ayub, Mewati
TI  - Integrating program and algorithm visualisation for learning data structure implementation
PY  - 2019
AB  - Abstract Algorithm Visualisation (AV) tool is commonly used to learn data structures. However, since that tool does not address technical details, some students may not know how to implement the data structures. This paper integrates the AV tool with Program Visualisation (PV) tool to help the students understanding the data structures’ implementation. The integration (which is implemented as a tool named DS-PITON) works similarly as a PV tool except that the data structures are visualised with the AV tool. Through quasi experiments, it can be stated that DS-PITON helps students to get better assessment score and to complete their assessment faster (even though the impact on completion time can work in reverse on slow-paced students). Further, according to a questionnaire survey, the students believe that DS-PITON helps them learning data structure materials.
SP  - 193
EP  - 204
JF  - Egyptian Informatics Journal
VL  - 20
IS  - 3
PB  - 
DO  - 10.1016/j.eij.2019.05.001
ER  - 

TY  - NA
AU  - Bai, Chongyang; Zang, Xiaoxue; Xu, Ying; Sunkara, Srinivas; Rastogi, Abhinav; Chen, Jindong; Aguera y Arcas, Blaise
TI  - UIBert: Learning Generic Multimodal Representations for UI Understanding
PY  - 2021
AB  - To improve the accessibility of smart devices and to simplify their usage, building models which understand user interfaces (UIs) and assist users to complete their tasks is critical. However, unique challenges are proposed by UI-specific characteristics, such as how to effectively leverage multimodal UI features that involve image, text, and structural metadata and how to achieve good performance when high-quality labeled data is unavailable. To address such challenges we introduce UIBert, a transformer-based joint image-text model trained through novel pre-training tasks on large-scale unlabeled UI data to learn generic feature representations for a UI and its components. Our key intuition is that the heterogeneous features in a UI are self-aligned, i.e., the image and text features of UI components, are predictive of each other. We propose five pretraining tasks utilizing this self-alignment among different features of a UI component and across various components in the same UI. We evaluate our method on nine real-world downstream UI tasks where UIBert outperforms strong multimodal baselines by up to 9.26% accuracy.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gupta, Kamal; Achille, Alessandro; Lazarow, Justin; Davis, Larry S.; Mahadevan, Vijay; Shrivastava, Abhinav
TI  - Layout Generation and Completion with Self-attention.
PY  - 2020
AB  - We address the problem of layout generation for diverse domains such as images, documents, and mobile applications. A layout is a set of graphical elements, belonging to one or more categories, placed together in a meaningful way. Generating a new layout or extending an existing layout requires understanding the relationships between these graphical elements. To do this, we propose a novel framework, LayoutTransformer, that leverages a self-attention based approach to learn contextual relationships between layout elements and generate layouts in a given domain. The proposed model improves upon the state-of-the-art approaches in layout generation in four ways. First, our model can generate a new layout either from an empty set or add more elements to a partial layout starting from an initial set of elements. Second, as the approach is attention-based, we can visualize which previous elements the model is attending to predict the next element, thereby providing an interpretable sequence of layout elements. Third, our model can easily scale to support both a large number of element categories and a large number of elements per layout. Finally, the model also produces an embedding for various element categories, which can be used to explore the relationships between the categories. We demonstrate with experiments that our model can produce meaningful layouts in diverse settings such as object bounding boxes in scenes (COCO bounding boxes), documents (PubLayNet), and mobile applications (RICO dataset).
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chen, Jieshan; Xie, Mulong; Xing, Zhenchang; Chen, Chunyang; Xu, Xiwei; Zhu, Liming; Li, Guoqiang
TI  - ESEC/SIGSOFT FSE - Object detection for graphical user interface: old fashioned or deep learning or a combination?
PY  - 2020
AB  - Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods. This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods. We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection.
SP  - 1202
EP  - 1214
JF  - Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3368089.3409691
ER  - 

TY  - JOUR
AU  - Wimmer, Michael; Kovacic, Iva; Ferschin, Peter; Rist, Florian; Hensel, Michael; Schinegger, Kristina; Rutzinger, Stefan; Kaufmann, Hannes; Kilian, Martin; Müller, Christian; Izmestiev, Ivan; Nawratil, Georg; Füssl, Josef; Stavric, Milena; Hahn, David; Suter, Georg
TI  - Advanced Computational Design – digitale Methoden für die frühe Entwurfsphase
PY  - 2022
AB  - NA
SP  - 720
EP  - 730
JF  - Bautechnik
VL  - 99
IS  - 10
PB  - 
DO  - 10.1002/bate.202200057
ER  - 

TY  - JOUR
AU  - Paredes, Luis; McMillan, Caroline; Chan, Wan Kyn; Chandrasegaran, Senthil; Singh, Ramyak; Ramani, Karthik; Wilde, Danielle
TI  - CHIMERA
PY  - 2021
AB  - <jats:p>Wearable technologies draw on a range of disciplines, including fashion, textiles, HCI, and engineering. Due to differences in methodology, wearables researchers can experience gaps or breakdowns in values, goals, and vocabulary when collaborating. This situation makes wearables development challenging, even more so when technologies are in the early stages of development and their technological and cultural potential is not fully understood. We propose a common ground to enhance the accessibility of wearables-related resources. The objective is to raise awareness and create a convergent space for researchers and developers to both access and share information across domains. We present CHIMERA, an online search interface that allows users to explore wearable technologies beyond their discipline. CHIMERA is powered by a Wearables Taxonomy and a database of research, tutorials, aesthetic approaches, concepts, and patents. To validate CHIMERA, we used a design task with multidisciplinary designers, an open-ended usability study with experts, and a usability survey with students of a wearables design class. Our findings suggest that CHIMERA assists users with different mindsets and skillsets to engage with information, expand and share knowledge when developing wearables. It forges common ground across divergent disciplines, encourages creativity, and affords the formation of inclusive, multidisciplinary perspectives in wearables development.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494974
ER  - 

TY  - JOUR
AU  - Li, Jianjun; Fei, Jialuo; Cheng, Shichao; Tang, Zheng; Hui, Guobao
TI  - TSG-net: a residual-based informing network for 3D Gaze estimation
PY  - 2021
AB  - The appearance-based method for gaze estimation has great potential to work well under various conditions, but current learning-based methods ignore inferior eye images in datasets caused by poor eye region locating, occlusions and abnormal head poses. These images badly impact the accuracy of estimation. In the study, inspired by binocular vision characteristics, we propose two cooperative sub-networks, True Gaze Consistency Network (TG-Net) and Single Gaze Inconsistency Network(SG-Net) which composes TSG-Net. TG-Net and SG-Net cooperate through a residual paradigm and Informing module. More specially, TG-Net explicitly extracts the consistency of paired eyes and weights high-level features from two paired-eye images utilizing SE-Block and an artificial gaze direction, named True Gaze. SG-Net outputs residual momentums based on True Gaze for better estimation of paired eyes. Experimental results on three benchmark datasets demonstrate that the proposed method performs competitively against the existed representative CNN-Based methods. TSG-Net improves on the state-of-the-art by 22% on MPIIGaze and shows more advantages in additional analysis.
SP  - 1
EP  - 16
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Su, Yuhui; Chen, Chunyang; Wang, Junjie; Liu, Zhe; Wang, Dandan; Li, Shoubin; Wang, Qing
TI  - The Metamorphosis: Automatic Detection of Scaling Issues for Mobile Apps
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 37th IEEE/ACM International Conference on Automated Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3551349.3556935
ER  - 

TY  - NA
AU  - Trinh, Mai; Padhan, Jhasketan; Navkar, Nikhil V.; Deng, Zhigang
TI  - Preliminary Design and Evaluation of an Interfacing Mechanism for Maneuvering Virtual Minimally Invasive Surgical Instruments
PY  - 2022
AB  - Augmenting the motion of virtual surgical instruments onto a minimally invasive surgical field acts as a visual cue for the operating surgeon. In this work we propose an interfacing mechanism to provide input for maneuvering such virtual surgical instruments. Specifically, an interface in the form of a 3D-printed dodecahedron pen with attached binary squared planar markers is employed. The proposed tracking mechanism computes the pose of the interface from a realtime video feed acquired from a camera. The system provides accurate pose estimation with mean errors of $0.27 \pm 0.06$ mm in translation and $0.37 \pm 0.04$ degrees in rotation. The object pose estimation takes &#x007E;6 ms. Utilized Azure Kinect camera with frame rate of 30 FPS and 1280 x 720 image resolution video, the tracking speed of the proposed system is &#x007E;25 FPS. The easy to integrate, cost effective setup makes the interfacing mechanism particularly suitable for remote surgical tele-mentoring applications.
SP  - NA
EP  - NA
JF  - 2022 International Symposium on Medical Robotics (ISMR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismr48347.2022.9807585
ER  - 

TY  - NA
AU  - Henley, Austin Z.; Ball, Julian; Klein, Benjamin; Rutter, Aiden; Lee, Dylan
TI  - An Inquisitive Code Editor for Addressing Novice Programmers' Misconceptions of Program Behavior
PY  - 2021
AB  - Novice programmers face numerous barriers while attempting to learn how to code that may deter them from pursuing a computer science degree or career in software development. In this work, we propose a tool concept to address the particularly challenging barrier of novice programmers holding misconceptions about how their code behaves. Specifically, the concept involves an inquisitive code editor that: (1) identifies misconceptions by periodically prompting the novice programmer with questions about their program's behavior, (2) corrects the misconceptions by generating explanations based on the program's actual behavior, and (3) prevents further misconceptions by inserting test code and utilizing other educational resources. We have implemented portions of the concept as plugins for the Atom code editor and conducted informal surveys with students and instructors. Next steps include deploying the tool prototype to students enrolled in introductory programming courses.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ang, Gary; Lim, Ee-Peng
TI  - IUI - Learning Network-Based Multi-Modal Mobile User Interface Embeddings
PY  - 2021
AB  - Rich multi-modal information - text, code, images, categorical and numerical data - co-exist in the user interface (UI) design of mobile applications. UI designs are composed of UI entities supporting different functions which together enable the application. To support effective search and recommendation applications over mobile UIs, we need to be able to learn UI representations that integrate latent semantics. In this paper, we propose a novel unsupervised model - Multi-modal Attention-based Attributed Network Embedding (MAAN) model. MAAN is designed to capture both multi-modal and structural network information. Based on the encoder-decoder framework, MAAN aims to learn UI representations that allow UI design reconstruction. The generated embedding can be applied to a variety of tasks: predicting UI elements associated with UI screens, inferring missing UI screen and element attributes, predicting UI user ratings, and retrieving UIs. Extensive experiments, including user evaluations, conducted on two datasets from RICO, a rich real-world mobile UI repository, demonstrates that MAAN out-performs other state-of-the-art models.
SP  - 366
EP  - 376
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450693
ER  - 

TY  - JOUR
AU  - Gunpinar, Erkan; Coskun, Umut Can; Ozsipahi, Mustafa; Gunpinar, Serkan
TI  - A Generative Design and Drag Coefficient Prediction System for Sedan Car Side Silhouettes based on Computational Fluid Dynamics
PY  - 2019
AB  - NA
SP  - 65
EP  - 79
JF  - Computer-Aided Design
VL  - 111
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2019.02.003
ER  - 

TY  - NA
AU  - Bao, Yiwei; Cheng, Yihua; Liu, Yunfei; Lu, Feng
TI  - Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets
PY  - 2021
AB  - Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Jiang, Yue; Lu, Yuwen; Nichols, Jeffrey; Stuerzlinger, Wolfgang; Yu, Chun; Lutteroth, Christof; Li, Yang; Kumar, Ranjitha; Li, Toby Jia-Jun
TI  - Computational Approaches for Understanding, Generating, and Adapting User Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3504030
ER  - 

TY  - NA
AU  - Mozaffari, Mohammad Amin; Zhang, Xinyuan; Cheng, Jinghui; Guo, Jin L.C.
TI  - GANSpiration: Balancing Targeted and Serendipitous Inspiration in User Interface Design with Style-Based Generative Adversarial Network
PY  - 2022
AB  - Inspiration from design examples plays a crucial role in the creative process of user interface design. However, current tools and techniques that support inspiration usually only focus on example browsing with limited user control or similarity-based example retrieval, leading to undesirable design outcomes such as focus drift and design fixation. To address these issues, we propose the GANSpiration approach that suggests design examples for both targeted and serendipitous inspiration, leveraging a style-based Generative Adversarial Network. A quantitative evaluation revealed that the outputs of GANSpiration-based example suggestion approaches are relevant to the input design, and at the same time include diverse instances. A user study with professional UI/UX practitioners showed that the examples suggested by our approach serve as viable sources of inspiration for overall design concepts and specific design elements. Overall, our work paves the road of using advanced generative machine learning techniques in supporting the creative design practice.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517511
ER  - 

TY  - NA
AU  - Li, Yuanchun; Yang, Ziyue; Guo, Yao; Chen, Xiangqun
TI  - ASE - Humanoid: a deep learning-based approach to automated black-box Android app testing
PY  - 2019
AB  - Automated input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most black-box input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. We propose Humanoid, an automated black-box Android app testing tool based on deep learning. The key technique behind Humanoid is a deep neural network model that can learn how human users choose actions based on an app's GUI from human interaction traces. The learned model can then be used to guide test input generation to achieve higher coverage. Experiments on both open-source apps and market apps demonstrate that Humanoid is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators. Humanoid is open-sourced at https://github.com/yzygitzh/Humanoid and a demo video can be found at https://youtu.be/PDRxDrkyORs.
SP  - 1070
EP  - 1073
JF  - 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ase.2019.00104
ER  - 

TY  - NA
AU  - Gupta, Kamal; Lazarow, Justin; Achille, Alessandro; Davis, Larry S.; Mahadevan, Vijay; Shrivastava, Abhinav
TI  - LayoutTransformer: Layout Generation and Completion with Self-attention
PY  - 2020
AB  - We address the problem of scene layout generation for diverse domains such as images, mobile applications, documents, and 3D objects. Most complex scenes, natural or human-designed, can be expressed as a meaningful arrangement of simpler compositional graphical primitives. Generating a new layout or extending an existing layout requires understanding the relationships between these primitives. To do this, we propose LayoutTransformer, a novel framework that leverages self-attention to learn contextual relationships between layout elements and generate novel layouts in a given domain. Our framework allows us to generate a new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout. Furthermore, our analyses show that the model is able to automatically capture the semantic properties of the primitives. We propose simple improvements in both representation of layout primitives, as well as training methods to demonstrate competitive performance in very diverse data domains such as object bounding boxes in natural images(COCO bounding box), documents (PubLayNet), mobile applications (RICO dataset) as well as 3D shapes (Part-Net). Code and other materials will be made available at this https URL.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Lasa, Ibon Serrano; Etxabe, Maitane Mazmela; Iriondo, Itziar Ricondo
TI  - New application for sketching in a machine tool company
PY  - 2021
AB  - The identification of new applications for sketching and freehand drawing in the field of engineering is a subject of interest for researchers and practitioners, especially after a significant drop...
SP  - 1
EP  - 31
JF  - Journal of Engineering Design
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ghosh, Shreya; Dhall, Abhinav; Hayat, Munawar; Knibbe, Jarrod; Ji, Qiang
TI  - Automatic Gaze Analysis: A Survey of Deep Learning based Approaches
PY  - 2021
AB  - Eye gaze analysis is an important research problem in the field of Computer Vision and Human-Computer Interaction. Even with notable progress in the last 10 years, automatic gaze analysis still remains challenging due to the uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and illumination conditions. There are several open questions including what are the important cues to interpret gaze direction in an unconstrained environment without prior knowledge and how to encode them in real-time. We review the progress across a range of gaze analysis tasks and applications to elucidate these fundamental questions; identify effective methods in gaze analysis and provide possible future directions. We analyze recent gaze estimation and segmentation methods, especially in the unsupervised and weakly supervised domain, based on their advantages and reported evaluation metrics. Our analysis shows that the development of a robust and generic gaze analysis method still needs to address real-world challenges such as unconstrained setup and learning with less supervision. We conclude by discussing future research directions for designing a real-world gaze analysis system that can propagate to other domains including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and Human Computer Interaction (HCI). Project Page: this https URL}{this https URL
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zhao, Yixue; Talebipour, Saghar; Baral, Kesina; Park, Hyojae; Yee, Leon; Khan, Safwat Ali; Brun, Yuriy; Medvidović, Nenad; Moran, Kevin
TI  - Avgust: automating usage-based test generation from videos of app executions
PY  - 2022
AB  - Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced automated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69% of the tests Avgust generates successfully execute the desired usage, and that Avgust's classifiers outperform the state of the art.
SP  - NA
EP  - NA
JF  - Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3540250.3549134
ER  - 

TY  - NA
AU  - Yamaguchi, Kota
TI  - CanvasVAE: Learning to Generate Vector Graphic Documents
PY  - 2021
AB  - Vector graphic documents present visual elements in a resolution free, compact format and are often seen in creative applications. In this work, we attempt to learn a generative model of vector graphic documents. We define vector graphic documents by a multi-modal set of attributes associated to a canvas and a sequence of visual elements such as shapes, images, or texts, and train variational auto-encoders to learn the representation of the documents. We collect a new dataset of design templates from an online service that features complete document structure including occluded elements. In experiments, we show that our model, named CanvasVAE, constitutes a strong baseline for generative modeling of vector graphic documents.
SP  - NA
EP  - NA
JF  - 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccv48922.2021.00543
ER  - 

TY  - NA
AU  - Tran O'Leary, Jasper; Jun, Eunice; Peek, Nadya
TI  - Improving Programming for Exploratory Digital Fabrication with Inline Machine Control and Styled Toolpath Visualizations
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3561998
ER  - 

TY  - NA
AU  - Gobert, Camille; Beaudouin-Lafon, Michel
TI  - i-LaTeX : Manipulating Transitional Representations between LaTeX Code and Generated Documents
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517494
ER  - 

TY  - NA
AU  - Novoa, Mauricio
TI  - Exploration on drawing from traditional representation to new embodied and immersive presentation of ideas
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhao, Shenghuan; Wang, Luo; Qian, Xueming; Chen, Jianping
TI  - Enhancing performance-based generative architectural design with sketch-based image retrieval: a pilot study on designing building facade fenestrations
PY  - 2021
AB  - By coupling parametric modelling, building performance (like energy efficiency) simulation, and algorithmic optimization, performance-based generative architectural design (PGAD) can automatically generate lots of high-performance architectural design solutions. Although it is ‘performance-based’, the final selection of a real design project still needs to consider the aesthetics of design choices. However, due to the overwhelming number of design choices generated by PGAD, it is difficult for designers to choose the most favourable one from them. Therefore, the current study tries to integrate the technology of sketch-based image retrieval (SBIR) into the selecting stage of PGAD. Rather than navigating alternatives one from another and getting lost, designers can directly find the most aesthetically preferred one by inputting his/her hand-drawn design. A design project of fenestrating a multiple-floor office building is used to demonstrate this method and test three SBIR algorithms: Angular radial partitioning (ARP), Angular radial orientation partitioning (AROP), and Sketch-A-Net model (SAN). Test results show that AROP performs the best among these three algorithms. Its retrievals are most similar to inquiry images drawn by architects. Meanwhile, performances of AROP with different template combinations are also rated. After that, AROP with the best template is also tested with incompletely drawn inquiry images. In the end, investigation results are validated by another building facade design case. The current study automates the PGAD process stepwise, making it more applicable to real design projects.
SP  - 1
EP  - 17
JF  - The Visual Computer
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - von Wangenheim, Christiane Gresse; Porto, João Vitor Araujo; Hauck, Jean Carlo Rossa; Borgatto, Adriano Ferreti
TI  - Do we agree on user interface aesthetics of Android apps
PY  - 2018
AB  - Context: Visual aesthetics is increasingly seen as an essential factor in perceived usability, interaction, and overall appraisal of user interfaces especially with respect to mobile applications. Yet, a question that remains is how to assess and to which extend users agree on visual aesthetics. Objective: This paper analyzes the inter-rater agreement on visual aesthetics of user interfaces of Android apps as a basis for guidelines and evaluation models. Method: We systematically collected ratings on the visual aesthetics of 100 user interfaces of Android apps from 10 participants and analyzed the frequency distribution, reliability and influencing design aspects. Results: In general, user interfaces of Android apps are perceived more ugly than beautiful. Yet, raters only moderately agree on the visual aesthetics. Disagreements seem to be related to subtle differences with respect to layout, shapes, colors, typography, and background images. Conclusion: Visual aesthetics is a key factor for the success of apps. However, the considerable disagreement of raters on the perceived visual aesthetics indicates the need for a better understanding of this software quality with respect to mobile apps.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Feng, Sidong; Chen, Chunyang
TI  - GIFdroid
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 44th International Conference on Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3510003.3510048
ER  - 

TY  - CONF
AU  - Li, Toby Jia-Jun; Popowski, Lindsay; Mitchell, Tom M.; Myers, Brad A.
TI  - CHI - Screen2Vec: Semantic Embedding of GUI Screens and GUI Components
PY  - 2021
AB  - Representing the semantics of GUI screens and components is crucial to data-driven computational methods for modeling user-GUI interactions and mining GUI designs. Existing GUI semantic representations are limited to encoding either the textual content, the visual design and layout patterns, or the app contexts. Many representation techniques also require significant manual data annotation efforts. This paper presents Screen2Vec, a new self-supervised technique for generating representations in embedding vectors of GUI screens and components that encode all of the above GUI features without requiring manual annotation using the context of user interaction traces. Screen2Vec is inspired by the word embedding method Word2Vec, but uses a new two-layer pipeline informed by the structure of GUIs and interaction traces and incorporates screen- and app-specific metadata. Through several sample downstream tasks, we demonstrate Screen2Vec’s key useful properties: representing between-screen similarity through nearest neighbors, composability, and capability to represent user tasks.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Tanaka, Hideyuki
TI  - Turning Any Object into an Input Device with a High-Accuracy Fiducial Marker
PY  - 2022
AB  - High-accuracy marker is a fiducial marker developed by the author, and its six degrees of freedom (6DoF) pose estimation error is less than 10% of that of conventional fiducial markers. This paper proposes a method to turn any object into an input device using only one camera and one high-accuracy marker. As an example, a prototype of passive stylus was achieved by simply attaching a high-accuracy marker to a pen. The results of the experiment showed that the position error of the pen tip was less than 1 mm without using any filtering technique.
SP  - NA
EP  - NA
JF  - 2022 IEEE/SICE International Symposium on System Integration (SII)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/sii52469.2022.9708752
ER  - 

TY  - NA
AU  - Evirgen, Noyan; Chen, Xiang 'Anthony'
TI  - GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks
PY  - 2022
AB  - Generative Adversarial Network (GAN) is widely adopted in numerous application areas, such as data preprocessing, image editing, and creativity support. However, GAN's 'black box' nature prevents non-expert users from controlling what data a model generates, spawning a plethora of prior work that focused on algorithm-driven approaches to extract editing directions to control GAN. Complementarily, we propose a GANzilla: a user-driven tool that empowers a user with the classic scatter/gather technique to iteratively discover directions to meet their editing goals. In a study with 12 participants, GANzilla users were able to discover directions that (i) edited images to match provided examples (closed-ended tasks) and that (ii) met a high-level goal, e.g., making the face happier, while showing diversity across individuals (open-ended tasks).
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545638
ER  - 

TY  - NA
AU  - Jennings, Nicholas; Nandy, Ananya; Zhu, Xinyi; Wang, Yuting; Sui, Fanping; Smith, James; Hartmann, Bjoern
TI  - GeneratiVR: Spatial Interactions in Virtual Reality to Explore Generative Design Spaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519616
ER  - 
