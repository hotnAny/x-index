
TY  - NA
AU  - Tajima, Yuki; Horiuchi, Toshiharu; Hattori, NA
TI  - ACM Multimedia - Sync Glass: Virtual Pouring and Toasting Experience with Multimodal Presentation
PY  - 2021
AB  - One of the challenges of non-face-to-face communication is the absence of the haptic dimension. To solve this, a haptic communication system via the Internet has been proposed. The system has to be designed in such a way that it does not create discomfort during general use. The "Sync Glass" that we have developed transmits and presents the feeling of pouring a drink and making a toast accompanied by haptic, sound and visual effects. The device is designed to resemble a glass cup and, moreover, each action, including drinking and making a toast is performed in the customary way, making its use more acceptable to users. In the internal user demonstrations we performed, the experience has been reviewed with participants saying that "the feeling of pouring is so realistic", "so enjoyable!", and similar affirmative statements.
SP  - 2768
EP  - 2770
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3478553
ER  - 

TY  - BOOK
AU  - Hirao, Yutaro; Takala, Tuukka M.; Lécuyer, Anatole
TI  - VR Workshops - Comparing Motion-based Versus Controller-based Pseudo-haptic Weight Sensations in VR
PY  - 2020
AB  - This work examines whether pseudo-haptic experiences can be achieved using a game controller without motion tracking. For this purpose, we implemented a virtual hand manipulation method that uses the controller’s analog stick. We compared the method’s pseudo-haptic experience to that of the conventional approach of using a hand-held motion controller. The results suggest that our analog stick manipulation can present pseudo-weight sensations in a similar way to the conventional approach. This means that interaction designers and users can also choose to utilize analog stick manipulation for pseudo-haptic experiences, as an alternative to motion controllers.
SP  - 305
EP  - 310
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00069
ER  - 

TY  - JOUR
AU  - Kaluarachchi, Tharindu; Reis, Andrew; Nanayakkara, Suranga
TI  - A Review of Recent Deep Learning Approaches in Human-Centered Machine Learning.
PY  - 2021
AB  - After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning (HCML). We present this review paper as an overview and analysis of existing work in HCML related to DL. Firstly, we collaborated with field domain experts to develop a working definition for HCML. Secondly, through a systematic literature review, we analyze and classify 162 publications that fall within HCML. Our classification is based on aspects including contribution type, application area, and focused human categories. Finally, we analyze the topology of the HCML landscape by identifying research gaps, highlighting conflicting interpretations, addressing current challenges, and presenting future HCML research opportunities.
SP  - 2514
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 21
IS  - 7
PB  - 
DO  - 10.3390/s21072514
ER  - 

TY  - NA
AU  - Daskalogrigorakis, Grigoris; McNamara, Ann; Mania, Katerina
TI  - SIGGRAPH Posters - Holo-Box: Level-of-Detail Glanceable Interfaces for Augmented Reality
PY  - 2021
AB  - Glanceable interfaces are Augmented Reality (AR) User Interfaces (UIs) for information retrieval ”at a glance” relying on eye gaze for implicit input. While they provide rapid information retrieval, they often occlude a large part of the real-world. This is compounded as the amount of virtual information increases. Interacting with complex glanceable interfaces often results in unintentional eye gaze interaction and selections due to the Midas Touch problem. In this work, we present Holo-box, an innovative AR UI design that combines 2D compact glanceable interfaces with 3D virtual ”Holo-boxes”. We can utilize the glanceable 2D interface to provide compact information at a glance while using Holo-box for explicit input such as hand tracking activated when necessary, surpassing the Midas Touch problem and resulting in Level-of-Detail(LOD) for AR glanceable UIs. We test our proposed system inside a real-world machine shop to provide on-demand virtual information while minimizing unintentional real-world occlusion.
SP  - NA
EP  - NA
JF  - Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450618.3469175
ER  - 

TY  - NA
AU  - Kimani, Everlyne; Parmar, Dhaval; Murali, Prasanth; Bickmore, Timothy
TI  - CHI Extended Abstracts - Sharing the Load Online: Virtual Presentations with Virtual Co-Presenter Agents
PY  - 2021
AB  - The pandemic has caused a significant increase in the use of videoconferencing for oral presentations. Prior work demonstrated that an embodied conversational agent that co-delivers an oral presentation could be used in face-to-face presentations to reduce public speaking anxiety and increase presentation quality. In this work, we evaluate the use of a co-presenter agent in the delivery of virtual presentations given over a videoconferencing system, comparing them to presentations given without the agent. We found that participants were satisfied with the co-presenter agent, and those who liked the agent (scoring above the mean on a composite self-report measure of satisfaction) rated the presentations they gave with the agent as having significantly higher quality compared to those given without the agent. There was evidence the agent helped participants feel less nervous about their talks. Interviews confirmed these findings, and identified additional advantages and disadvantages of using co-presenter agents in virtual presentations.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451670
ER  - 

TY  - NA
AU  - Matsumoto, Keigo; Langbehn, Eike; Narumi, Takuji; Steinicke, Frank
TI  - VR - Detection Thresholds for Vertical Gains in VR and Drone-based Telepresence Systems
PY  - 2020
AB  - Several redirected walking techniques have been introduced and analyzed in recent years, while the main focus was on manipulations in horizontal directions, in particular, by means of curvature, rotation, and translation gains. However, less research has been conducted on the manipulation of vertical movements and its possible use as a redirection technique. Actually, vertical movements are fundamentally important, e.g., for remotely steering a drone using a virtual reality headset.In this paper, we explored vertical gains, a novel redirection technique, which enables us to purposefully manipulate the mapping of the user’s physical vertical movements to movements in the virtual space and the remote space. This approach allows natural and more active physical control of a real drone. To demonstrate the usability of vertical gains, we implemented a telepresence drone and vertical redirection techniques for stretching and crouching actions using common VR devices. We conducted two user studies to investigate the effective manipulation ranges and its usability: one study using a virtual environment (VE), and one using a camera stream from a telepresence drone. The results revealed that our technique could manipulate a users vertical movement without her/his noticing.
SP  - 101
EP  - 107
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581262503135
ER  - 

TY  - JOUR
AU  - Takács, Barnabás; Vincze, Zsuzsanna
TI  - Deep authoring - an AI Tool set for creating immersive MultiMedia experiences
PY  - 2021
AB  - We introduce a fully automated 360° video processing pipeline using a hierarchical combination of Artificial Intelligence (AI) modules to create immersive volumetric XR experiences. Two critical productions tasks (person segmentation and depth estimation) are addressed with a parallel Deep Neural Network (DNN) pipeline that combines instance segmentation, person detection, pose estimation, camera stabilization, neural tracking, 3D face detection, hair masking, and monocular 360° depth computation in a single and robust tool set. To facilitate the rapid uptake of these techniques we provide a detailed review of AI-based methods to address these problems (complete with links to recommended open source implementations) as well as references to existing authoring tools in the market. Our key contributions include a method to create semi-synthetic data sets for data auto-augmentation and using this technique to generate over 3.8 m images as part of a concise evaluation and subsequent retraining of DNNs for person detection tasks. Furthermore, we apply the same techniques to develop a spherical DNN for monocular depth estimation with a Free Viewpoint Video (FVV) capture system and a novel method to generate 3D human shapes and pose mannequins for training. To evaluate the performance of our AI authoring tool set we address four challenging production tasks and demonstrate the practical use of our solution with videos showing processed output.
SP  - 31105
EP  - 31134
JF  - Multimedia Tools and Applications
VL  - 80
IS  - 20
PB  - 
DO  - 10.1007/s11042-020-10275-z
ER  - 

TY  - NA
AU  - Sayyad, Ehsan; Sra, Misha; Höllerer, Tobias
TI  - ISMAR - Walking and Teleportation in Wide-area Virtual Reality Experiences
PY  - 2020
AB  - Location-based or Out-of-Home Entertainment refers to experiences such as theme and amusement parks, laser tag and paintball arenas, roller and ice skating rinks, zoos and aquariums, or science centers and museums among many other family entertainment and cultural venues. More recently, location-based VR has emerged as a new category of out-of-home entertainment. These VR experiences can be likened to social entertainment options such as laser tag, where physical movement is an inherent part of the experience versus at-home VR experiences where physical movement often needs to be replaced by artificial locomotion techniques due to tracking space constraints. In this work, we present the first VR study to understand the impact of natural walking in a large physical space on presence and user preference. We compare it with teleportation in the same large space, since teleportation is the most commonly used locomotion technique for consumer, at-home VR. Our results show that walking was overwhelmingly preferred by the participants and teleportation leads to significantly higher self-reported simulator sickness. The data also shows a trend towards higher self-reported presence for natural walking.
SP  - 608
EP  - 617
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00088
ER  - 

TY  - JOUR
AU  - Liu, Xiangyu; Wei, Xuetao; Guo, Lei; Song, Song
TI  - DarkVLP: "Lights-Off" Visible-Light Positioning
PY  - 2022
AB  - NA
SP  - 11071
EP  - 11084
JF  - IEEE Internet of Things Journal
VL  - 9
IS  - 13
PB  - 
DO  - 10.1109/jiot.2021.3125795
ER  - 

TY  - JOUR
AU  - Ahuja, Karan; Ofek, Eyal; Gonzalez-Franco, Mar; Holz, Christian; Wilson, Andrew D.
TI  - CoolMoves: User Motion Accentuation in Virtual Reality
PY  - 2021
AB  - Current Virtual Reality (VR) systems are bereft of stylization and embellishment of the user's motion - concepts that have been well explored in animations for games and movies. We present CooIMoves, a system for expressive and accentuated full-body motion synthesis of a user's virtual avatar in real-time, from the limited input cues afforded by current consumer-grade VR systems, specifically headset and hand positions. We make use of existing motion capture databases as a template motion repository to draw from. We match similar spatio-temporal motions present in the database and then interpolate between them using a weighted distance metric. Joint prediction probability is then used to temporally smooth the synthesized motion, using human motion dynamics as a prior. This allows our system to work well even with very sparse motion databases (e.g., with only 3-5 motions per action). We validate our system with four experiments: a technical evaluation of our quantitative pose reconstruction and three additional user studies to evaluate the motion quality, embodiment and agency.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463499
ER  - 

TY  - NA
AU  - Müller, Leon; Pfeuffer, Ken; Gugenheimer, Jan; Pfleging, Bastian; Prange, Sarah; Alt, Florian
TI  - CHI - SpatialProto: Exploring Real-World Motion Captures for Rapid Prototyping of Interactive Mixed Reality
PY  - 2021
AB  - Spatial computing devices that blend virtual and real worlds have the potential to soon become ubiquitous. Yet, creating experiences for spatial computing is non-trivial and needs skills in programming and 3D content creation, rendering them inaccessible to a wider group of users. We present SpatialProto, an in-situ spatial prototyping system for lowering the barrier to engage in spatial prototyping. With a depth-sensing capable mixed reality headset, SpatialProto lets users record animated objects of the real-world environment (e.g. paper, clay, people, or any other prop), extract only the relevant parts, and directly place and transform these recordings in their physical environment. We describe the design and implementation of SpatialProto, a user study evaluating the system’s prototype with non-expert users (n = 9), and demonstrate applications where multiple captures are fused for compelling augmented reality experiences.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445560
ER  - 

TY  - NA
AU  - Saquib, Nazmus; Kazi, Rubaiat Habib; Wei, Li-Yi; Mark, Gloria; Roy, Deb
TI  - CHI - Constructing Embodied Algebra by Sketching
PY  - 2021
AB  - Mathematical models and expressions traditionally evolved as symbolic representations, with cognitively arbitrary rules of symbol manipulation. The embodied mathematics philosophy posits that abstract math concepts are layers of metaphors grounded in our intuitive arithmetic capabilities, such as categorizing objects and part-whole analysis. We introduce a design framework that facilitates the construction and exploration of embodied representations for algebraic expressions, using interactions inspired by innate arithmetic capabilities. We instantiated our design in a sketch interface that enables construction of visually interpretable compositions that are directly mappable to algebraic expressions and explorable through a ladder of abstraction [47]. The emphasis is on bottom-up construction, with the user sketching pictures while the system generates corresponding algebra. We present diverse examples created by our prototype system. A coverage of the US Common Core curriculum and playtesting studies with children point to the future direction and potential for a sketch-based design paradigm for mathematics.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445460
ER  - 

TY  - JOUR
AU  - Liu, Jingyang; Li, Yunzhi; Goel, Mayank
TI  - A semantic-based approach to digital content placement for immersive environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Visual Computer
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s00371-022-02707-8
ER  - 

TY  - NA
AU  - Li, Jingyi; Hashim, Sonia; Jacobs, Jennifer
TI  - CHI - What We Can Learn From Visual Artists About Software Development
PY  - 2021
AB  - This paper explores software’s role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists’ experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445682
ER  - 

TY  - JOUR
AU  - Shatilov, Kirill A.; Chatzopoulos, Dimitris; Lee, Lik Hang; Hui, Pan
TI  - Emerging ExG-based NUI Inputs in Extended Realities: A Bottom-up Survey
PY  - 2021
AB  - Incremental and quantitative improvements of two-way interactions with extended realities (XR) are contributing toward a qualitative leap into a state of XR ecosystems being efficient, user-friendly, and widely adopted. However, there are multiple barriers on the way toward the omnipresence of XR; among them are the following: computational and power limitations of portable hardware, social acceptance of novel interaction protocols, and usability and efficiency of interfaces. In this article, we overview and analyse novel natural user interfaces based on sensing electrical bio-signals that can be leveraged to tackle the challenges of XR input interactions. Electroencephalography-based brain-machine interfaces that enable thought-only hands-free interaction, myoelectric input methods that track body gestures employing electromyography, and gaze-tracking electrooculography input interfaces are the examples of electrical bio-signal sensing technologies united under a collective concept of ExG. ExG signal acquisition modalities provide a way to interact with computing systems using natural intuitive actions enriching interactions with XR. This survey will provide a bottom-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) ExG hardware solutions, (iii) ExG-enabled applications, (iv) discussion on social acceptance of such applications and technologies, as well as (v) research challenges, application directions, and open problems; evidencing the benefits that ExG-based Natural User Interfaces inputs can introduce to the area of XR.
SP  - 1
EP  - 49
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 11
IS  - 2
PB  - 
DO  - 10.1145/3457950
ER  - 

TY  - CHAP
AU  - Abtahi, Parastoo; Sharma, Neha; Landay, James A.; Follmer, Sean
TI  - Presenting and Exploring Challenges in Human-Robot Interaction Design Through Bodystorming
PY  - 2020
AB  - In the coming era of ubiquitous robotics we envision the need for the effortless design of contextually-aware interactions with robots. Ubiquitous robots create a number of challenges for designers. Firstly, due to their dynamic nature, prototyping requires skillful programming and is often time consuming. Moreover, these devices are often context-aware and their behavior is affected by people, objects, and their environment. Existing tools for human-robot interaction designers require programming expertise, do not leverage design methodologies such as iterative design, and do not support in-situ user testing. We propose that bodystorming can be used as an effective method in this process, to communicate needfinding results and to explore the design of situated interactions with robots. As a case study, we first conduct a series of interviews and observe the workflow of human-robot interaction designers, to better understand the challenges they face. We summarize the insights gathered from our needfinding, including challenges around data capture and information overload. We then describe how we used a mystery-game-style role-playing activity in an interactive workshop to communicate our findings, induce empathy, and initiate an effective ideation phase. Finally, we summarize the learnings from this workshop and how such bodystorming techniques can be used to communicate needfinding results at early stages of the human-robot interaction design process.
SP  - 327
EP  - 344
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-62037-0_15
ER  - 

TY  - CHAP
AU  - Turner, Phil
TI  - Imagination and Design
PY  - 2020
AB  - The central thesis of this book is that imagination is important to HCI and where better to begin this discussion than with the design of a digital artefact? When we design a new piece of technology (or engage in any design-related activity) there is an expectation that our imaginations will be at hand to generate ideas with respect to what the technology will do, and how it will look and how it will be experienced. This is imagination as mental time travel and creativity; and imagination as storytelling and as facilitating of social interaction. Having set ourselves what is a very broad target—the design process in all its stripes, and all four aspects of imagination, we will narrow our focus on how we use things (“design tools”) to crystallize our imaginations. Thus, the topic of this chapter may be characterised as variations on “thinking with things”. Firstly, consistent with current cognitive science (e.g. Clark in Being there: putting brain, body, and world together again. MIT press, 1998), we don’t just think with our brains as we routinely employ with all kinds of external representations and artefacts. These different forms of “thinking with things” have been, in turn, studied in a variety of ways, for example, through the use of props in make-believe. This is the major theme of not only this chapter but also for Chap. 4. Instead of (say) toys, imagine thinking with sketches and prototypes. Secondly, there is also evidence from the study of external cognition (Scaife and Rogers 1996) which is the “interaction between internal and external representations when performing cognitive tasks”. In general, they describe external cognition as a form of computational offloading, which is the extent to which external representations can reduce the amount of cognitive effort required to solve a problem or complete a task. Here we are specifically concerned with capturing our imaginations with the use of sketches, external representations and then thinking with these. In each of these instances we will see how something (whether it be psychological, material or cultural—or a mixture) serves to make or help make material what we imagine.
SP  - 49
EP  - 78
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-37348-1_3
ER  - 

TY  - JOUR
AU  - Coronado, Enrique; Itadera, Shunki; Ramirez-Alpizar, Ixchel G.
TI  - Integrating Virtual, Mixed, and Augmented Reality to Human–Robot Interaction Applications Using Game Engines: A Brief Review of Accessible Software Tools and Frameworks
PY  - 2023
AB  - <jats:p>This article identifies and summarizes software tools and frameworks proposed in the Human–Robot Interaction (HRI) literature for developing extended reality (XR) experiences using game engines. This review includes primary studies proposing Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) solutions where humans can control or interact with real robotic platforms using devices that extend the user’s reality. The objective of this article is not to present an extensive list of applications and tools. Instead, we present recent, relevant, common, and accessible frameworks and software tools implemented in research articles published in high-impact robotics conferences and journals. For this, we searched papers published during a seven-years period between 2015 and 2022 in relevant databases for robotics (Science Direct, IEEE Xplore, ACM digital library, Springer Link, and Web of Science). Additionally, we present and classify the application context of the reviewed articles in four groups: social robotics, programming of industrial robots, teleoperation of industrial robots, and Human–Robot collaboration (HRC).</jats:p>
SP  - 1292
EP  - NA
JF  - Applied Sciences
VL  - 13
IS  - 3
PB  - 
DO  - 10.3390/app13031292
ER  - 

TY  - JOUR
AU  - Heemsbergen, Luke; Bowtell, Greg; Vincent, Jordan Beth
TI  - Conceptualising Augmented Reality: From virtual divides to mediated dynamics:
PY  - 2021
AB  - Although Augmented Reality (AR) scholarship is largely defined through technocentric boundary work that delineates the virtual from the real, it is nevertheless vital to consider experiential conce...
SP  - 830
EP  - 846
JF  - Convergence: The International Journal of Research into New Media Technologies
VL  - 27
IS  - 3
PB  - 
DO  - 10.1177/1354856521989514
ER  - 

TY  - NA
AU  - Li, Jingyi; Hashim, Sonia; Jacobs, Jennifer
TI  - What We Can Learn From Visual Artists About Software Development
PY  - 2021
AB  - This paper explores software's role in visual art production by examining how artists use and develop software. We conducted interviews with professional artists who were collaborating with software developers, learning software development, and building and maintaining software. We found artists were motivated to learn software development for intellectual growth and access to technical communities. Artists valued efficient workflows through skilled manual execution and personal software development, but avoided high-level forms of software automation. Artists identified conflicts between their priorities and those of professional developers and computational art communities, which influenced how they used computational aesthetics in their work. These findings contribute to efforts in systems engineering research to integrate end-user programming and creativity support across software and physical media, suggesting opportunities for artists as collaborators. Artists' experiences writing software can guide technical implementations of domain-specific representations, and their experiences in interdisciplinary production can aid inclusive community building around computational tools.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wei, Tzu-Yun; Tsai, Hsin-Ruey; Liao, Yu-So; Tsai, Chieh; Chen, Yi-Shan; Wang, Chi; Chen, Bing-Yu
TI  - UIST - ElastiLinks: Force Feedback between VR Controllers with Dynamic Points of Application of Force
PY  - 2020
AB  - Force feedback is commonly used to enhance realism in virtual reality (VR). However, current works mainly focus on providing different force types or patterns, but do not investigate how a proper point of application of force (PAF), which means where the resultant force is applied to, affects users' experience. For example, users perceive resistive force without torque when pulling a virtual bow, but with torque when pulling a virtual slingshot. Therefore, we propose a set of handheld controllers, ElastiLinks, to provide force feedback between controllers with dynamic PAFs.A rotatable track on each controller provides a dynamic PAF, and two common types of force feedback, resistive force and impact, are produced by two links, respectively. We performed a force perception study to ascertain users' resistive and impact force level distinguishability between controllers. Based on the results, we conducted another perception study to understand users' distinguishability of PAF offset and rotation differences. Finally, we performed a VR experience study to prove that force feedback with dynamic PAFs enhances VR experience.
SP  - 1023
EP  - 1034
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415836
ER  - 

TY  - NA
AU  - Herdel, Viviane; Yamin, Lee J.; Cauchard, Jessica R.
TI  - Above and Beyond: A Scoping Review of Domains and Applications for Human-Drone Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501881
ER  - 

TY  - NA
AU  - Davis, Josh Urban; Wentzel, Johann
TI  - CHI Extended Abstracts - Font Your Friends and Loved Ones: On the Utility of Ugly Interfaces
PY  - 2021
AB  - User interface design often focuses so heavily on clean and minimal interface aesthetics that any deviation is often rejected as “ugly”. This tendency towards abstraction in UI design can be contextualized as a removal of the “human” or “physical world” from the aesthetic choices and design considerations for the system. To resist this techno-deterministic eradication of the human presence from UI design, as well as radically inject the human presence back into user interfaces, we present typeFACE, a web interface and generative adversarial network designed to create fonts from human faces. We provide an implementation and applications for such a system, as well as contextualize and analyze the history of “ugliness” and the “uncanny” in UI design history. We also discuss implications of such a system within the domains of data ownership, identity, and HCI design research.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3450371
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Chang, Yuan-Chia; Wei, Tzu-Yun; Tsao, Chih-An; Koo, Xander Chin-yuan; Wang, Hao-Chuan; Chen, Bing-Yu
TI  - CHI - GuideBand: Intuitive 3D Multilevel Force Guidance on a Wristband in Virtual Reality
PY  - 2021
AB  - For haptic guidance, vibrotactile feedback is a commonly-used mechanism, but requires users to interpret its complicated patterns especially in 3D guidance, which is not intuitive and increases their mental effort. Furthermore, for haptic guidance in virtual reality (VR), not only guidance performance but also realism should be considered. Since vibrotactile feedback interferes with and reduces VR realism, it may not be proper for VR haptic guidance. Therefore, we propose a wearable device, GuideBand, to provide intuitive 3D multilevel force guidance upon the forearm, which reproduces an effect that the forearm is pulled and guided by a virtual guider or telepresent person in VR. GuideBand uses three motors to pull a wristband at different force levels in 3D space. Such feedback usually requires much larger and heavier robotic arms or exoskeletons. We conducted a just-noticeable difference study to understand users’ force level distinguishability. Based on the results, we performed a study to verify that compared with state-of-the-art vibrotactile guidance, GuideBand is more intuitive, needs a lower level of mental effort, and achieves similar guidance performance. We further conducted a VR experience study to observe how users combine and complement visual and force guidance, and prove that GuideBand enhances realism in VR guidance.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445262
ER  - 

TY  - NA
AU  - Su, Yuning; Nai, Weizhi; Xiaoying, Sun; Sun, Zuowei
TI  - WHC - Design and Modeling of an Ungrounded Haptic Gun that Simulates Recoil Using Asymmetric Force
PY  - 2021
AB  - The lack of perception of recoil limits the immersion of virtual reality first-person shooting games in which users hold a gun in their hand. This paper presents the design and modeling of an ungrounded haptic gun that could simulate the recoil using asymmetric force, which is rendered by a voice coil actuator and produces directional force perception to users rather than vibration feedback. A magnet is driven by the electromagnetic force and moves forward inside the gun like a bullet does and the corresponding reaction force is perceived by users as a recoil. A spring is used to reset the position of the magnet, and a friction damper is used to prevent a collision when the magnet returns to its initial position that breaks the rendering of asymmetric force. We analyzed the dynamic model of the recoil. A user study was conducted to comparatively evaluate the perception of shooting recoil as well as the subjective preference of the proposed recoil rendering method. The results show that our proposed haptic gun with asymmetric force rendering could induce clear perception of shooting recoil, and could provide a more favored virtual shooting experience.
SP  - 361
EP  - 366
JF  - 2021 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc49131.2021.9517202
ER  - 

TY  - NA
AU  - Racca, Mattia; Kyrki, Ville; Cakmak, Maya
TI  - HRI - Interactive Tuning of Robot Program Parameters via Expected Divergence Maximization
PY  - 2020
AB  - Enabling diverse users to program robots for different applications is critical for robots to be widely adopted. Most of the new collaborative robot manipulators come with intuitive programming interfaces that allow novice users to compose robot programs and tune their parameters. However, parameters like motion speeds or exerted forces cannot be easily demonstrated and often require manual tuning, resulting in a tedious trial-and-error process. To address this problem, we formulate tuning of one-dimensional parameters as an Active Learning problem where the learner iteratively refines its estimate of the feasible range of parameter values, by selecting informative queries. By executing the parametrized actions, the learner gathers the user’s feedback, in the form of directional answers (“higher,” “lower,” or “fine”), and integrates it in the estimate. We propose an Active Learning approach based on Expected Divergence Maximization for this setting and compare it against two baselines with synthetic data. We further compare the approaches on a real-robot dataset obtained from programs written with a simple Domain-Specific Language for a robot arm and manually tuned by expert users (N=8) to perform four manipulation tasks. We evaluate the effectiveness and usability of our interactive tuning approach against manual tuning with a user study where novice users (N=8) tuned parameters of a human-robot hand-over program. CCS CONCEPTS • Computing methodologies → Active learning settings; • Human-centered computing → User centered design; • Computer systems organization → External interfaces for robotics. ACM Reference Format: Mattia Racca, Ville Kyrki, and Maya Cakmak. 2020. Interactive Tuning of Robot Program Parameters via Expected Divergence Maximization. In Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction (HRI ’20), March 23–26, 2020, Cambridge, United Kingdom. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3319502.3374784
SP  - 629
EP  - 638
JF  - Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3319502.3374784
ER  - 

TY  - NA
AU  - Song, Yunpeng; Huang, Yun; Cai, Zhongmin; Hong, Jason
TI  - CHI - I'm All Eyes and Ears: Exploring Effective Locators for Privacy Awareness in IoT Scenarios
PY  - 2020
AB  - With the proliferation of IoT devices, there are growing concerns about being sensed or monitored by these devices unawares, especially in places perceived as private. We explore the design space of IoT locators to help people physically find nearby IoT devices. We first conducted a survey to understand people's willingness, current practices, and challenges in finding IoT devices. Our survey findings motivated us to design and implement low-cost locators (visual, auditory, and contextualized pictures) to help people find nearby devices. Through an iterative design process and two rounds of experiments, we found that these locators greatly reduced people's search time over a baseline of no locators. Many participants found the visual and auditory locators enjoyable. Some participants also appropriated the use of our system for other purposes, e.g., to learn about new IoT devices, instead of for privacy awareness.
SP  - 3376585
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376585
ER  - 

TY  - NA
AU  - Nakamura, Fumihiko; Verhulst, Adrien; Sakurada, Kuniharu; Sugimoto, Maki
TI  - AHs - Virtual Whiskers: Spatial Directional Guidance using Cheek Haptic Stimulation in a Virtual Environment
PY  - 2021
AB  - Spatial cues are an important element of navigating people in physical/virtual spaces. In terms of spatial navigation, integrating vision with other modalities, such as haptics, can guide users more effectively. Haptic cues are presented on the body parts that are sensitive to stimuli such as hands and a head. The head is reported to be superior to the body for spatial directional perception. In this paper, we propose Virtual Whiskers, a spatial directional guidance technique by haptic stimulation of the cheeks using tiny robot arms attached to a Head-Mounted Display (HMD). We deploy photo reflective sensors attached to the tip of 2 robotic arms to detect the distance between the tip and the cheek surface. Using the robot arms, we stimulate a point on the cheek obtained by calculating an intersection between the cheek surface and the target direction. We experimentally investigated how accurately participants identify the target direction provided by our guidance method. We evaluated an error between the actual target direction and the participant’s pointed direction. The experimental result shows that our method achieves the average absolute directional error of 2.76 degrees in the azimuthal plane and 7.32 degrees in the elevation plane. We also conducted a spatial guidance experiment to evaluate task performance in a target search task. We compared the condition of only vision and vision with haptics for task completion time. The average of task completion time in visual-only condition was M=12.45 s, SD=14.51 s, and visual with haptic condition resulted in M=6.91 s, SD=5.48 s. Statistical test revealed a significant difference in task completion time between the visual condition and the visual+haptic condition.
SP  - 141
EP  - 151
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458987
ER  - 

TY  - NA
AU  - Ye, Hui; Fu, Hongbo
TI  - ProGesAR: Mobile AR Prototyping for Proxemic and Gestural Interactions with Real-world IoT Enhanced Spaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517689
ER  - 

TY  - NA
AU  - Fang, Cathy Mengying; Gu, Jianzhe; Yao, Lining; Harrison, Chris
TI  - ElectriPop: Low-Cost, Shape-Changing Displays Using Electrostatically Inflated Mylar Sheets
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501837
ER  - 

TY  - NA
AU  - Sun, Lingyun; Li, Jiaji; Ji, Junzhe; Pan, Deying; Li, Mingming; Zhu, Kuangqi; Fan, Yitao; Yang, Yue; Tao, Ye; Wang, Guanyun
TI  - X-Bridges: Designing Tunable Bridges to Enrich 3D Printed Objects' Deformation and Stiffness
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545710
ER  - 

TY  - NA
AU  - Cortes, Juan Pablo Forero; Suriyaarachchi, Hussel; Nassani, Alaeddin; Zhang, Haimo; Nanayakkara, Suranga
TI  - MobileHCI - OM: A Comprehensive Tool to Elicit Subjective Vibrotactile Expressions Associated with Contextualised Meaning in Our Everyday Lives
PY  - 2021
AB  - The sense of touch offers interesting possibilities as a robust and ubiquitous communication channel. In this paper, we present OM, a tool that enables users to design subjective vibrotactile expressions associated with contextualised information relevant to them. OM consists of a pair of wrist-worn devices that can reproduce complex vibrotactile symbols and a companion editor smartphone app that allows users to create, customise and store personalised expressions. We studied OM in real-world contexts by allowing 13 participants to explore the functionalities of OM throughout their daily interactions with complete autonomy. We highlight relevant scenarios, design considerations, and future directions towards a tool that can help people unveil an alternative, ubiquitous and private communication system accessible to all.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472028
ER  - 

TY  - BOOK
AU  - Hanaoka, Kohei; Shimizu, Masahiro; Umedachi, Takuya
TI  - RoboSoft - Development of 3D Printed Structure that Visualizes Bending and Compression Deformations for Soft-bodied Robots
PY  - 2021
AB  - This paper presents 3D printed structure and a measurement algorithm to sense bending and compression/extension deformations for a soft-bodied robot. The proposed method utilizes a 3D printed structure painted with multiple colors to visualize the deformation. Capturing the deformation state with a cheap web camera allows us to simultaneously measure the bending and compression/extension deformations with the proposed algorithm using the image processing library, OpenCV. The experiments with the prototype demonstrate that the method successfully and easily measures the multi-modal deformation distribution. The proposed method can be a powerful tool to measure the complex and rich multi-modal deformations of soft-bodied robots and design the feedback controllers.
SP  - 155
EP  - 162
JF  - 2021 IEEE 4th International Conference on Soft Robotics (RoboSoft)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/robosoft51838.2021.9479224
ER  - 

TY  - NA
AU  - Yoshida, Shigeo; Sun, Yuqian; Kuzuoka, Hideaki
TI  - CHI - PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality
PY  - 2020
AB  - We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376358
ER  - 

TY  - JOUR
AU  - Burden, Alan G.; Caldwell, Glenda Amayo; Guertler, Matthias R.
TI  - Towards human–robot collaboration in construction: current cobot trends and forecasts
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Construction Robotics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s41693-022-00085-0
ER  - 

TY  - JOUR
AU  - Stellmacher, Carolin; Bonfert, Michael; Kruijff, Ernst; Schöning, Johannes
TI  - Triggermuscle: Exploring Weight Perception for Virtual Reality Through Adaptive Trigger Resistance in a Haptic VR Controller
PY  - 2022
AB  - <jats:p>It is challenging to provide users with a haptic weight sensation of virtual objects in VR since current consumer VR controllers and software-based approaches such as pseudo-haptics cannot render appropriate haptic stimuli. To overcome these limitations, we developed a haptic VR controller named <jats:italic>Triggermuscle</jats:italic> that adjusts its trigger resistance according to the weight of a virtual object. Therefore, users need to adapt their index finger force to grab objects of different virtual weights. Dynamic and continuous adjustment is enabled by a spring mechanism inside the casing of an HTC Vive controller. In two user studies, we explored the effect on weight perception and found large differences between participants for sensing change in trigger resistance and thus for discriminating virtual weights. The variations were easily distinguished and associated with weight by some participants while others did not notice them at all. We discuss possible limitations, confounding factors, how to overcome them in future research and the pros and cons of this novel technology.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 2
IS  - NA
PB  - 
DO  - 10.3389/frvir.2021.754511
ER  - 

TY  - JOUR
AU  - Hung, Ching-Wen; Tsai, Hsin-Ruey; Su, Chi-Chun; Chiu, Jui-Cheng; Chen, Bing-Yu
TI  - OsciHead
PY  - 2022
AB  - <jats:p>Current haptic devices are usually designed to provide one type of force feedback; however, most VR scenarios require versatile force feedback, which may require the integration of different devices to provide various types of forces. In addition, besides the main haptic effects caused by the forces, multiple types of oscillation may also commonly accompany them, which are crucial for improving VR realism and immersion. Therefore, we simulate versatile force feedback by rendering the corresponding types of oscillation as the effects caused by those forces. We take inertia and impact forces as examples in this paper, and achieve versatility using the proposed device, OsciHead, on a head-mounted display (HMD), instead of integrating different devices. By controlling elastic bands' elasticity and stored power, OsciHead uses two rotatable oscillators on both sides of the HMD, in order to render various multilevel and multidimensional oscillation feedback in 2D translation and 2D rotation directions on a head. In an exploratory study, we explored different scenarios in which multiple types of oscillation could be simulated by OsciHead. We then observed oscillation level distinguishability in two just-noticeable difference (JND) studies, and evaluated the oscillation type recognition rates in a recognition study. Based on the results, we performed a VR study, which verified that the inertia and impact feedback simulated by OsciHead enhances realism and achieves versatility.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - MHCI
PB  - 
DO  - 10.1145/3546715
ER  - 

TY  - NA
AU  - Porfirio, David; Stegner, Laura; Cakmak, Maya; Sauppé, Allison; Albarghouthi, Aws; Mutlu, Bilge
TI  - CHI - Figaro: A Tabletop Authoring Environment for Human-Robot Interaction
PY  - 2021
AB  - Human-robot interaction designers and developers navigate a complex design space, which creates a need for tools that support intuitive design processes and harness the programming capacity of state-of-the-art authoring environments. We introduce Figaro, an expressive tabletop authoring environment for mobile robots, inspired by shadow puppetry, that provides designers with a natural, situated representation of human-robot interactions while exploiting the intuitiveness of tabletop and tangible programming interfaces. On the tabletop, Figaro projects a representation of an environment. Users demonstrate sequences of behaviors, or scenes, of an interaction by manipulating instrumented figurines that represent the robot and the human. During a scene, Figaro records the movement of figurines on the tabletop and narrations uttered by users. Subsequently, Figaro employs real-time program synthesis to assemble a complete robot program from all scenes provided. Through a user study, we demonstrate the ability of Figaro to support design exploration and development for human-robot interaction.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3446864
ER  - 

TY  - NA
AU  - Chung, John Joon Young; Shin, Hijung Valentina; Xia, Haijun; Wei, Li-Yi; Kazi, Rubaiat Habib
TI  - CHI - Beyond Show of Hands: Engaging Viewers via Expressive and Scalable Visual Communication in Live Streaming
PY  - 2021
AB  - Live streaming is gaining popularity across diverse application domains in recent years. A core part of the experience is streamer-viewer interaction, which has been mainly text-based. Recent systems explored extending viewer interaction to include visual elements with richer expression and increased engagement. However, understanding expressive visual inputs becomes challenging with many viewers, primarily due to the relative lack of structure in visual input. On the other hand, adding rigid structures can limit viewer interactions to narrow use cases or decrease the expressiveness of viewer inputs. To facilitate the sensemaking of many visual inputs while retaining the expressiveness or versatility of viewer interactions, we introduce a visual input management framework (VIMF) and a system, VisPoll, that help streamers specify, aggregate, and visualize many visual inputs. A pilot evaluation indicated that VisPoll can expand the types of viewer interactions. Our framework provides insights for designing scalable and expressive visual communication for live streaming.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445419
ER  - 

TY  - BOOK
AU  - Bektaş, Kenan
TI  - ETRA Adjunct - Toward A Pervasive Gaze-Contingent Assistance System: Attention and Context-Awareness in Augmented Reality
PY  - 2020
AB  - Mobile devices with high-speed connectivity provide us with access to gigabytes of high resolution images, videos, and graphics. For instance, a head-worn display can be used to augment the real view with digitized visual information (Figure 1). Eye tracking helps us to understand how we process visual information and it allows us to develop gaze-enabled interactive systems. For instance, foveated gaze-contingent displays (GCDs) dynamically adjust the level of detail according to the user’s point-of-interest. We propose that GCDs should take users’ attention and cognitive load into account, augment their vision with contextual information and provide personalized assistance in solving visual tasks. Grounded on existing literature, we identified several research questions that need to be discussed before developing such displays.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379157.3391657
ER  - 

TY  - JOUR
AU  - Liu, Guanhong; Yu, Tianyu; Yao, Zhihao; Xu, Haiqing; Zhang, Yunyi; Xu, Xuhai; Xu, Xiaomeng; Gao, Mingyue; Sun, Qirui; Zhang, Tingliang; Mi, Haipeng
TI  - ViviPaint: Creating Dynamic Painting with a Thermochromic Toolkit
PY  - 2022
AB  - <jats:p>New materials and technologies facilitate the design of thermochromic dynamic paintings. However, creating a thermochromic painting requires knowledge of electrical engineering and computer science, which is a barrier for artists and enthusiasts with non-technology backgrounds. Existing toolkits only support limited design space and fail to provide usable solutions for independent creation and for meeting the needs of the artists. We present ViviPaint, a toolkit that assists artists and enthusiasts in creating thermochromic paintings easily and conveniently. We summarized the pain points and challenges by observing a professional artist’s entire thermochromic painting creation process. We then designed ViviPaint consisting of a design tool and a set of hardware components. The design tool provides a GUI animation choreography interface, hardware assembly guidance, and assistance in assembly process. The hardware components comprise an augmented picture frame with a detachable structure and 24 temperature-changing units using Peltier elements. The results of our evaluation study (N = 8) indicate that our toolkit is easy to use and effectively assists users in creating thermochromic paintings.</jats:p>
SP  - 63
EP  - 63
JF  - Multimodal Technologies and Interaction
VL  - 6
IS  - 8
PB  - 
DO  - 10.3390/mti6080063
ER  - 

TY  - NA
AU  - Mahadevan, Karthik; Sousa, Maurício; Tang, Anthony; Grossman, Tovi
TI  - "Grip-that-there": An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration
PY  - 2021
AB  - In ad-hoc human-robot collaboration (HRC), humans and robots work on a task without pre-planning the robot's actions prior to execution; instead, task allocation occurs in real-time. However, prior research has largely focused on task allocations that are pre-planned - there has not been a comprehensive exploration or evaluation of techniques where task allocation is adjusted in real-time. Inspired by HCI research on territoriality and proxemics, we propose a design space of novel task allocation techniques including both explicit techniques, where the user maintains agency, and implicit techniques, where the efficiency of automation can be leveraged. The techniques were implemented and evaluated using a tabletop HRC simulation in VR. A 16-participant study, which presented variations of a collaborative block stacking task, showed that implicit techniques enable efficient task completion and task parallelization, and should be augmented with explicit mechanisms to provide users with fine-grained control.
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ogawa, Kumpei; Fujita, Kazuyuki; Takashima, Kazuki; Kitamura, Yoshifumi
TI  - PseudoJumpOn: Jumping onto Steps in Virtual Reality
PY  - 2022
AB  - Jumping onto steps is a promising action for creating an instant height movement in VR, but installing physical steps is impractical. We propose PseudoJumpOn, a novel locomotion technique using a common VR setup that allows the user to experience virtual step-up jumping motion by applying two types of viewpoint-manipulation methods to a physical jump on a flat floor. The core idea is to replicate the physical characteristics of ascending jumps, and thus we designed two viewpoint-manipulation methods: gain manipulation, which differentiates the ascent and descent height, and peak shifting, which delays the peak timing. We conducted a user study asking participants (N = 20) to experience two-legged step-up jumps onto 0.2&#x2013;0.8-m heights in VR as the two methods were applied in combination (gain manipulation: five conditions where the ascending gain was 1.0&#x2013;5.0; peak shifting: four conditions where the peak timing in VR was delayed by 0&#x2013;1.0 ratios of the original timing). The results showed that the participants in most conditions felt positively in terms of reality and naturalness of actually jumping onto steps, even though knowing no physical steps existed. In addition, subsequent analyses also derived practical guidelines for determining the appropriate gains and the potential use of peak shifting to achieve a natural step-up jumping experience.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00084
ER  - 

TY  - CHAP
AU  - Huang, Chien-Ming
TI  - HCI (37) - Contextual Programming of Collaborative Robots
PY  - 2020
AB  - Collaborative robots are envisioned to assist people in an increasing range of domains, from manufacturing to home care; however, due to the variable nature of these fields, such robots will inevitably face unfamiliar situations and unforeseen task requirements, and must be able to interact with users who possess diverse skill sets, backgrounds, and needs. Presently, robust, autonomous solutions for appropriately handling these vast possibilities and uncertainties are unattainable. End-user robot programming offers an alternative approach that lets end users provide task specifications and author robot skills to meet their own specific contextual constraints and custom task needs. Contextual information—such as task objects, environmental landmarks, and user preferences—is essential in realizing desirable, flexible, and reliable robot programs. However, most robot programming systems at present do not afford intuitive ways of specifying contextual information. In this paper, we draw on our prior work to illustrate the barriers to end-user robot programming when using a state-of-the-art programming interface. We then present two case studies that explore new approaches to providing a robot system with contextual information about the user, task, and environment, and how these methods can help improve task performance and user experience. We discuss our findings and future directions for building effective end-user programming tools to bring robotic assistance closer to everyday users.
SP  - 321
EP  - 338
JF  - Artificial Intelligence in HCI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50334-5_22
ER  - 

TY  - NA
AU  - Krings, Sarah; Yigitbas, Enes; Jovanovikj, Ivan; Sauer, Stefan; Engels, Gregor
TI  - EICS - Development framework for context-aware augmented reality applications
PY  - 2020
AB  - Augmented Reality (AR) is a technology on the rise. Due to its growing popularity and application in various domains, ways of increasing user friendliness and usability of AR are now becoming more important. Context-awareness is one of these ways, as it can make an AR application adjust to the user, their situation and their needs, making the application more ergonomic and easy to work with. Especially in mobile AR, context changes happen often, due to the user moving around and using their devices in different environments and situations. In order to ease the development of context-aware applications for mobile AR, the modular development framework AARCon is proposed. AARCon consists of context monitoring features to keep track of the context of use and adaptation features to react to context changes. To show the potential of AARCon, a case study is presented, where AARCon is used to create a context-aware AR printer maintenance application.
SP  - NA
EP  - NA
JF  - Companion Proceedings of the 12th ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3393672.3398640
ER  - 

TY  - NA
AU  - Caetano, Arthur; Sra, Misha
TI  - ARfy: A Pipeline for Adapting 3D Scenes to Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558697
ER  - 

TY  - NA
AU  - Liu, Jingyang
TI  - Intelligent Environments - Semantic Mapping: A Semantics-based Approach to Virtual Content Placement for Immersive Environments
PY  - 2021
AB  - Semantic Mapping is a semantics-based interactive system that enables intuitive virtual content placement for projection mapping in intelligent environments. Our semantic mapping system embeds semantic information of the environment to provide a user with an easy way to control and place projected virtual items in the physical world. In contrast to traditional projection mapping that involves manual adjustments, this semantic mapping system enables efficient manipulation of virtual content through inputs from users via speech or text. To build the system, we first use a commercial depth camera for scene reconstruction and an end-to-end deep learning framework for semantic segmentation at the instance level. We illustrate the system by developing a prototype for a set of proof-of-concept, room-scale applications. The accuracy study and user study results show that the system can provide users with accurate semantic information for effective virtual content placement.
SP  - 1
EP  - 8
JF  - 2021 17th International Conference on Intelligent Environments (IE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ie51775.2021.9486580
ER  - 

TY  - NA
AU  - Zhang, Tianyi; Lowmanstone, London; Wang, Xinyu; Glassman, Elena L.
TI  - UIST - Interactive Program Synthesis by Augmented Examples
PY  - 2020
AB  - Programming-by-example (PBE) has become an increasingly popular component in software development tools, human-robot interaction, and end-user programming. A long-standing challenge in PBE is the inherent ambiguity in user-provided examples. This paper presents an interaction model to disambiguate user intent and reduce the cognitive load of understanding and validating synthesized programs. Our model provides two types of augmentations to user-given examples: 1) semantic augmentation where a user can specify how different aspects of an example should be treated by a synthesizer via light-weight annotations, and 2) data augmentation where the synthesizer generates additional examples to help the user understand and validate synthesized programs. We implement and demonstrate this interaction model in the domain of regular expressions, which is a popular mechanism for text processing and data wrangling and is often considered hard to master even for experienced programmers. A within-subjects user study with twelve participants shows that, compared with only inspecting and annotating synthesized programs, interacting with augmented examples significantly increases the success rate of finishing a programming task with less time and increases users? confidence of synthesized programs.
SP  - 627
EP  - 648
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415900
ER  - 

TY  - JOUR
AU  - Ting, Yi-Wen; Lin, Po-Hsien; Lin, Chih-Long
TI  - The Transformation and Application of Virtual and Reality in Creative Teaching: A New Interpretation of the Triadic Ballet
PY  - 2023
AB  - <jats:p>Virtual imaging technology has been widely used in entertainment, medicine, engineering and other fields, and the advancement of virtual imaging has also provided new opportunities for art performances and exhibitions. This research uses live dance, screen dance and virtual dance displays to conduct the audience’s experience of watching dance performances and compares the three forms. A total of 30 students participated in this research. According to the results of this study, the content of the dance works includes route and trajectory, movements and expressions, costumes, and overall atmosphere; there is no obvious difference under the three viewing conditions, and the spatial performance of dance works is best in live performances. According to the research results, the physical stage performance still has the advantage of space performance, virtual image has the advantage of solving the distance between the audience and the stage, and the screen image is helpful for the performance of the stage color. The results of this study show that the presentation of different technologies can improve the audience’s viewing experience of dance, but how to create an impressive spatial experience using the screen image and a virtual environment for a live performance still needs technical improvement.</jats:p>
SP  - 61
EP  - NA
JF  - Education Sciences
VL  - 13
IS  - 1
PB  - 
DO  - 10.3390/educsci13010061
ER  - 

TY  - NA
AU  - McGill, Mark; Gugenheimer, Jan; Freeman, Euan
TI  - VRST - A Quest for Co-Located Mixed Reality: Aligning and Assessing SLAM Tracking for Same-Space Multi-User Experiences
PY  - 2020
AB  - Current solutions for creating co-located Mixed Reality (MR) experiences typically rely on platform-specific synchronisation of spatial anchors or Simultaneous Localisation and Mapping (SLAM) data across clients, often coupled to cloud services. This introduces significant costs (in development and deployment), constraints (with interoperability across platforms often limited), and privacy concerns. For practitioners, support is needed for creating platform-agnostic co-located MR experiences. This paper explores the utility of aligned SLAM solutions by 1) surveying approaches toward aligning disparate device coordinate spaces, formalizing their theoretical accuracy and limitations; 2) providing skeleton implementations for audience-based, small-scale and large-scale co-location using said alignment approaches; and 3) detailing how we can assess the accuracy and safety of 6DoF/SLAM tracking solutions for any arbitrary device and dynamic environment without the need for an expensive ground truth optical tracking, by using trilateration and a $30 laser distance meter. Through this, we hope to further democratise the creation of cross-platform co-located MR experiences.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418968
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Hedayati, Hooman; Zheng, Clement; Bohn, James L.; Szafir, Daniel; Yi-Luen, Ellen; Gross, Mark D.; Leithinger, Daniel
TI  - CHI - RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots
PY  - 2020
AB  - RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376523
ER  - 

TY  - JOUR
AU  - BermejoCarlos, ; HuiPan, 
TI  - A Survey on Haptic Technologies for Mobile Augmented Reality
PY  - 2021
AB  - Augmented reality (AR) applications have gained much research and industry attention. Moreover, the mobile counterpart—mobile augmented reality (MAR) is one of the most explosive growth areas for A...
SP  - 1
EP  - 35
JF  - ACM Computing Surveys
VL  - 54
IS  - 9
PB  - 
DO  - 10.1145/3465396
ER  - 

TY  - JOUR
AU  - Plopski, Alexander; Hirzle, Teresa; Norouzi, Nahal; Qian, Long; Bruder, Gerd; Langlotz, Tobias
TI  - The Eye in Extended Reality: A Survey on Gaze Interaction and Eye Tracking in Head-worn Extended Reality
PY  - 2022
AB  - <jats:p>With innovations in the field of gaze and eye tracking, a new concentration of research in the area of gaze-tracked systems and user interfaces has formed in the field of Extended Reality (XR). Eye trackers are being used to explore novel forms of spatial human–computer interaction, to understand human attention and behavior, and to test expectations and human responses. In this article, we review gaze interaction and eye tracking research related to XR that has been published since 1985, which includes a total of 215 publications. We outline efforts to apply eye gaze for direct interaction with virtual content and design of attentive interfaces that adapt the presented content based on eye gaze behavior and discuss how eye gaze has been utilized to improve collaboration in XR. We outline trends and novel directions and discuss representative high-impact papers in detail.</jats:p>
SP  - 1
EP  - 39
JF  - ACM Computing Surveys
VL  - 55
IS  - 3
PB  - 
DO  - 10.1145/3491207
ER  - 

TY  - JOUR
AU  - Ye, Xupeng
TI  - A Survey on Simulation for Weight Perception in Virtual Reality
PY  - 2021
AB  - Virtual reality (VR) technology can provide users with an immersive experience as if they are in the real world, which can be applied in the fields of entertainment, education and scientific research, etc. In order to improve the sense of presence and immersion in VR, the design of multimodal feedback is an important component. In particular, the simulation of weight of virtual objects poses many challenges due to the limitations of hardware and software. Many researchers focused on this issue in various ways. These methods are mainly divided into two categories: device-based simulation and software-based simulation. This paper investigates the focus of software-based simulation, particularly for the virtual feedback methods proposed by researchers in recent years. We introduce the background of these proposed methods, technical implementation principles, application scenarios, the advantages and disadvantages of these simulation methods, and the evaluation criteria. We also propose the future challenges and the development of simulation methods for weight perception of virtual objects in VR.
SP  - 1
EP  - 24
JF  - Journal of Computer and Communications
VL  - 9
IS  - 9
PB  - 
DO  - 10.4236/jcc.2021.99001
ER  - 

TY  - JOUR
AU  - Chakraborty, Samit; Biswas, Manik Chandra
TI  - 3D printing technology of polymer-fiber composites in textile and fashion industry: a potential roadmap of concept to consumer
PY  - 2020
AB  - NA
SP  - 112562
EP  - NA
JF  - Composite Structures
VL  - 248
IS  - NA
PB  - 
DO  - 10.1016/j.compstruct.2020.112562
ER  - 

TY  - BOOK
AU  - Le Minh, Khanh-Hoi; Le, Kim Hung
TI  - RTSI - AirGen: GAN-based synthetic data generator for air monitoring in Smart City
PY  - 2021
AB  - The past decade has seen a notable increase in air pollution that directly damages health, animals, and plants worldwide. To mitigate such negative effects, several research groups have been working on predicting air quality using deep learning. However, the lack of high-quality air quality datasets is a major obstacle encountered to achieve high accuracy prediction. In this paper, we introduce an air monitoring data generator powered by learning distributed real sequences using the generative adversarial network (GAN), namely AirGen. An unsupervised adversarial loss is also employed in the network to minimize the difference between generated synthetic and original data in the training process. Experiments on real datasets indicate that the data generated by Airgen could significantly increase the prediction accuracy performed by deep learning models. The mean square error (MSE) is remarkably reduced from 0.024 to 0.015.
SP  - 317
EP  - 322
JF  - 2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/rtsi50628.2021.9597364
ER  - 

TY  - JOUR
AU  - Karolus, Jakob; Kiss, Francisco; Eckerth, Caroline; Viot, Nicolas; Bachmann, Felix; Schmidt, Albrecht; Wozniak, Pawel W.
TI  - EMBody: A Data-Centric Toolkit for EMG-Based Interface Prototyping and Experimentation
PY  - 2021
AB  - Body movements, from a short smile to a marathon run, are driven by muscle activity. Despite the fact that measuring muscle activity with electromyography (EMG) is technically well established, it is highly complex and its use in interfaces has been limited. Easy access to muscle sensing can offer new opportunities to Human-Computer Interaction (HCI) research. Off-the-shelf sensors often only provide low-level access, hence requiring expertise in signal processing and widening the gulf of execution for users without engineering skills. To address this challenge, we introduce EMBody, a data-centric toolkit for EMG-based interface prototyping and experimentation. EMBody offers multiple levels of prototyping fidelity for EMG sensing, signal processing, and data interpretation. Our data-centric toolkit encapsulates the different data representation stages, offering a wide range of customization opportunities to experts while also allowing non-technical designers to focus on creating new interaction techniques. EMBody features a lightweight form factor and wireless connectivity. Additionally, the system leverages an exploration-centered workflow by allowing rapid access to measurement data via the accompanying software. Users define a set of motions to be recognized and interactively provide example data points. The toolkit then handles signal processing and classification. The recognized movements are streamed on the local network, ready to be used by interactive applications. This paper reports on how to use EMBody and its implementation. We iteratively developed the toolkit in a series of workshops and example applications. Users who had none or very limited knowledge of EMG could rapidly create engaging functional prototypes, while experts appreciated the modularity of the software component allowing for a high degree of customization. We contribute the software and hardware components of EMBody as a tool for the research community to stimulate creative exploration of EMG systems.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3457142
ER  - 

TY  - JOUR
AU  - Stefanidi, Zinovia; Margetis, George; Ntoa, Stavroula; Papagiannakis, George
TI  - Real-Time Adaptation of Context-Aware Intelligent User Interfaces, for Enhanced Situational Awareness
PY  - 2022
AB  - In this work, a novel computational approach for the dynamic adaptation of User Interfaces (UIs) is proposed, which aims at enhancing the Situational Awareness (SA) of users by leveraging the current context and providing the most useful information, in an optimal and efficient manner. By combining Ontology modeling and reasoning with Combinatorial Optimization, the system decides <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">what</i> information to present, <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">when</i> to present it, <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">where</i> to visualize it in the display - and <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">how</i> , taking into consideration contextual factors as well as placement constraints. The main objective of the proposed approach is to optimize the SA associated with the displayed UI <italic xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">at run-time</i> , while avoiding information overload and induced stress. In the context of this work, we have deployed our computational approach to the use case of an Augmented Reality (AR) system for Law Enforcement Agents (LEAs). To explore the benefits and limitations of the developed system, two evaluations have been conducted. The first one was an expert-based evaluation with LEAs and User Experience (UX) experts, assessing the appropriateness of the system’s decisions. The second one was a user-based evaluation involving LEAs from different agencies, estimating the SA, the mental workload and the overall UX associated with the system, through an AR simulation. The results indicate that the system enhances SA, and while not imposing workload, it provides an overall positive UX.
SP  - 23367
EP  - 23393
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3152743
ER  - 

TY  - NA
AU  - Cohn, Brian A.; Maselli, Antonella; Ofek, Eyal; Gonzalez-Franco, Mar
TI  - AIVR - SnapMove: Movement Projection Mapping in Virtual Reality
PY  - 2020
AB  - We present SnapMove a technique to reproject reaching movements inside Virtual Reality. SnapMove can be used to reduce the need of large, fatiguing or difficult motions. We designed multiple reprojection techniques, linear or planar, uni-manual, bi-manual or head snap, that can be used for reaching, throwing and virtual tool manipulation. In a user study (n=21) we explore if the self-avatar follower effect can be modulated depending on the cost of the motion introduced by remapping. SnapMove was successful in re-projecting user’s hand position from e.g. a lower area, to a higher avatar-hand position–a mapping which can be ideal for limiting fatigue. It was also successful in preserving avatar embodiment and gradually bring users to perform movements with higher cost energies, which have most interest for rehabilitation scenarios. We implemented applications for menu interaction, climbing, rowing, and throwing darts. Overall, SnapMove can make interactions in virtual environments easier. We discuss the potential impact of SnapMove for application in gaming, accessibility and therapy.
SP  - 74
EP  - 81
JF  - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr50618.2020.00024
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Dinc, Abdullatif; Obernolte, Jan; Rivu, Radiah; Abdrabou, Yasmeen; Shelter, Franziska; Abdelrahman, Yomna; Alt, Florian
TI  - UIST - Bi-3D: Bi-Manual Pen-and-Touch Interaction for 3D Manipulation on Tablets
PY  - 2021
AB  - Tablets are attractive for design work anywhere, but 3D manipulations are notoriously difficult. We explore how engaging the stylus and multi-touch in concert can render such tasks easier. We introduce Bi-3D, an interaction concept where touch gestures are combined with 2D pen commands for 3D manipulation. For example, for a fast and intuitive 3D drag & drop technique: the pen drags the object on-screen, and parallel pinch-to-zoom moves it in the third dimension. In this paper, we describe the Bi-3D design space, crossing two-handed input and the degrees-of-freedom (DOF) of 3D manipulation and navigation tasks. We demonstrate sketching and manipulation tools in a prototype 3D design application, where users can fluidly combine 3D operations through alternating and parallel use of the modalities. We evaluate the core technique, bi-manual 3DOF input, against widget and mid-air baselines in an object movement task. We find that Bi-3D is a fast and practical way for multi-dimensional manipulation of graphical objects, promising to facilitate 3D design on stylus and tablet devices.
SP  - 149
EP  - 161
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474741
ER  - 

TY  - NA
AU  - Ens, Barrett; Bach, Benjamin; Cordeil, Maxime; Engelke, Ulrich; Serrano, Marcos; Willett, Wesley; Prouzeau, Arnaud; Anthes, Christoph; Büschel, Wolfgang; Dunne, Cody; Dwyer, Tim; Grubert, Jens; Haga, Jason H.; Kirshenbaum, Nurit; Kobayashi, Dylan; Lin, Tica; Olaosebikan, Monsurat; Pointecker, Fabian; Saffo, David; Saquib, Nazmus; Schmalstieg, Dieter; Szafir, Danielle Albers; Whitlock, Matt; Yang, Yalong
TI  - CHI - Grand Challenges in Immersive Analytics
PY  - 2021
AB  - Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.
SP  - 459
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3446866
ER  - 

TY  - NA
AU  - Gupta, Meenu; Kumar, Rakesh; Yadav, Utkarsh; Tuteja, Devanshu
TI  - Augmented Reality based 3D Business Card implementing Virtual buttons
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Second International Conference on Computer Science, Engineering and Applications (ICCSEA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccsea54677.2022.9936445
ER  - 

TY  - NA
AU  - Senft, Emmanuel; Hagenow, Michael; Praveena, Pragathi; Radwin, Robert; Zinn, Michael; Gleicher, Michael; Mutlu, Bilge
TI  - A Method For Automated Drone Viewpoints to Support Remote Robot Manipulation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iros47612.2022.9982063
ER  - 

TY  - CHAP
AU  - Piening, Robin; Pfeuffer, Ken; Esteves, Augusto; Mittermeier, Tim; Prange, Sarah; Schröder, Philippe; Alt, Florian
TI  - INTERACT (1) - Looking for Info: Evaluation of Gaze Based Information Retrieval in Augmented Reality
PY  - 2021
AB  - This paper presents the results of an empirical study and a real-world deployment of a gaze-adaptive UI for Augmented Reality (AR). AR introduces an attention dilemma between focusing on the reality vs. on AR content. Past work suggested eye gaze as a technique to open information interfaces, however there is only little empirical work. We present an empirical study comparing gaze-adaptive to an always-on interface in tasks that vary focus between reality and virtual content. Across tasks, we find most participants prefer the gaze-adaptive UI and find it less distracting. When focusing on reality, the gaze UI is faster, perceived as easier and more intuitive. When focusing on virtual content, always-on is faster but user preferences are split. We conclude with the design and deployment of an interactive application in a public museum, demonstrating the promising potential in the real world.
SP  - 544
EP  - 565
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85623-6_32
ER  - 

TY  - NA
AU  - Nakagaki, Ken
TI  - UIST (Adjunct Volume) - Mechanical Shells: Physical Add-ons for Extending and Reconfiguring the Interactivities of Actuated TUIs
PY  - 2020
AB  - In this paper, I introduce a concept of mechanical shells, which are physical add-ons that can adaptively augment, extend, and reconfigure the interactivities of self-actuated tangible user interfaces (TUIs). While a variety of research explores actuated and shape-changing interfaces for providing dynamic physical affordance and tangible displays, the concept of mechanical shell intends to overcome the constraint of existing generic actuated TUI hardware thereby enabling greater versatility and expression. This paper overviews the mechanical shell concept, describes project examples, outlines a research framework, and suggests open space for future research.
SP  - 151
EP  - 156
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415801
ER  - 

TY  - JOUR
AU  - Yeasmin, Rubaya; Han, Seung-Ik; Duy, Le Thai; Ahn, Byungmin; Seo, Hyungtak
TI  - A Skin-like Self-healing and stretchable substrate for wearable electronics
PY  - 2022
AB  - NA
SP  - 140543
EP  - 140543
JF  - Chemical Engineering Journal
VL  - NA
IS  - NA
PB  - 
DO  - 10.1016/j.cej.2022.140543
ER  - 

TY  - BOOK
AU  - Ritchie, Jackie; Bontilao, Joselle; Kennelly, Sarah; Topliss, Jack; Dunn, Jessica; Renaud, Andre; Huber, Tim; de Gast, Barro W; Piumsomboon, Thammathip
TI  - AsianCHI@CHI - COMFlex: An Adaptive Haptic Interface with Shape-Changing and Weight-Shifting Mechanism for Immersive Virtual Reality
PY  - 2021
AB  - This work explores shape-changing and weight-shifting mechanisms for haptic interfaces to simulate various shapes and sizes in Virtual Reality (VR) for the industrial product design process. The COMFlex system offers haptic feedback in the form of a weight distribution changing (COM) and shape-changing (Flex) device while perceiving a visual representation in VR. Any state changes to the virtual representation are reflected by the COMFlex system, allowing live dynamic feedback to the user. We share initial findings from experimenting with COMFlex in several use cases for our follow up design improvements. Finally, future work is discussed, including physical changes to the device and potential applications.
SP  - 210
EP  - 214
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3468214
ER  - 

TY  - JOUR
AU  - Zarraonandia, Telmo; Díaz, Paloma; Aedo, Ignacio; Bellucci, Andrea
TI  - Engaging educators in the ideation of scenarios for cross-reality game-based learning experiences
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Cross-reality media technology creates alternate reality experiences in which the physical and the virtual world are interconnected and influence each other through a network of sensors and actuators. Despite technological advances, the landscape of cross-reality technology as an enabler of alternate reality educational experiences has not been explored yet. The technical expertise required to set up and program such mixed environments is too high to engage the problem owners (i.e. educational experts) in the design process and, hence, user-driven innovation remains challenging. In this paper we explore the co-creation of cross-reality experiences for educational games. We created a no-programming toolkit that provides a visual language and interface abstractions to quickly build prototypes of cross-reality interactions. The toolkit supports experience prototyping and allows designers to coproduce, with educational experts, meaningful scenarios while they create, try out and reconfigure their prototypes. We report on a workshop with 36 educators where the toolkit was used to ideate cross-reality games for education. We discuss use cases of game-based learning applications developed by the participants that follow different pedagogical strategies and combine different physical and virtual spaces and times. We outline implications for the design of cross-reality interactions in educational settings that trigger further research and technological developments.</jats:p>
SP  - NA
EP  - NA
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11042-022-13632-2
ER  - 

TY  - NA
AU  - Lu, Edward; Miller, John H.; Pereira, Nuno; Rowe, Anthony
TI  - ISMAR - FLASH: Video-Embeddable AR Anchors for Live Events
PY  - 2021
AB  - Public spaces like concert stadiums and sporting arenas are ideal venues for AR content delivery to crowds of mobile phone users. Unfortunately, these environments tend to be some of the most challenging in terms of lighting and dynamic staging for vision-based relocalization. In this paper, we introduce FLASH1, a system for delivering AR content within challenging lighting environments that uses active tags (i.e., blinking) with detectable features from passive tags (quads) for marking regions of interest and determining pose. This combination allows the tags to be detectable from long distances with significantly less computational overhead per frame, making it possible to embed tags in existing video displays like large jumbotrons. To aid in pose acquisition, we implement a gravity-assisted pose solver that removes the ambiguous solutions that are often encountered when trying to localize using standard passive tags. We show that our technique outperforms similarly sized passive tags in terms of range by 20-30% and is fast enough to run at 30 FPS even within a mobile web browser on a smartphone.
SP  - 489
EP  - 497
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00066
ER  - 

TY  - NA
AU  - Wang, Tianyi; Qian, Xun; He, Fengming; Hu, Xiyun; Huo, Ke; Cao, Yuanzhi; Ramani, Karthik
TI  - UIST - CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications
PY  - 2020
AB  - Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.
SP  - 328
EP  - 341
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415815
ER  - 

TY  - NA
AU  - Mannino, Miro; Abouzied, Azza
TI  - SIGMOD Conference - Synner: Generating Realistic Synthetic Data
PY  - 2020
AB  - Synner allows users to generate realistic-looking data. With Synner users can visually and declaratively specify properties of the dataset they wish to generate. Such properties include the domain, and statistical distribution of each field, and relationships between fields. User can also sketch custom distributions and relationships. Synner provides instant feedback on every user interaction by visualizing a preview of the generated data. It also suggests generation specifications from a few user-provided examples of data to generate, column labels and other user interactions. In this demonstration, we showcase Synner and summarize results from our evaluation of Synner's effectiveness at generating realistic-looking data.
SP  - 2749
EP  - 2752
JF  - Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3318464.3384696
ER  - 

TY  - NA
AU  - Achberger, Alexander; Arulrajah, Pirathipan; Sedlmair, Michael; Vidackovic, Kresimir
TI  - STROE: An Ungrounded String-Based Weight Simulation Device
PY  - 2022
AB  - We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user&#x2019;s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users&#x2019; perceived realism and immersion of VR scenes.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00029
ER  - 

TY  - NA
AU  - Liao, Chen-Chieh; Koike, Hideki; Nakamura, Takuto
TI  - CHI Extended Abstracts - Realtime Center of Mass Adjustment via Weight Switching Device inside a Golf Putter
PY  - 2020
AB  - In order to improve the performance in putting, we design a weight switching device that can provide various switching angles. This paper proposes a system that can change the center of mass by switching the weight to different positions around the head of a putter. The proposed system starts switching the weight at the beginning of a downswing and ends before the putter hits a ball. To verify the effectiveness of the proposed system, we conducted a user study and examined if the face to path angle of the putter changed when the weight was switched to different positions. In the user study, the participant performed putting with different switching angles. In the analysis, we focused on the differences in the face to path angle among the switching angles. The user study showed that the proposed system is effective in changing the face to path angle when switching the weight away from the putter's shaft. Based on the experimental results, the proposed system contributes to affecting the face to path angle dynamically in real-time.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383030
ER  - 

TY  - NA
AU  - Yang, Humphrey; Johnson, Tate; Zhong, Ke; Patel, Dinesh; Olson, Gina; Majidi, Carmel; Islam, Mohammad; Yao, Lining
TI  - ReCompFig: Designing Dynamically Reconfigurable Kinematic Devices Using Compliant Mechanisms and Tensioning Cables
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502065
ER  - 

TY  - NA
AU  - Wu, Yifan; Hellerstein, Joseph M.; Satyanarayan, Arvind
TI  - UIST - B2: Bridging Code and Interactive Visualization in Computational Notebooks
PY  - 2020
AB  - Data scientists have embraced computational notebooks to author analysis code and accompanying visualizations within a single document. Currently, although these media may be interleaved, they remain siloed: interactive visualizations must be manually specified as they are divorced from the analysis provenance expressed via dataframes, while code cells have no access to users' interactions with visualizations, and hence no way to operate on the results of interaction. To bridge this divide, we present B2, a set of techniques grounded in treating data queries as a shared representation between the code and interactive visualizations. B2 instruments data frames to track the queries expressed in code and synthesize corresponding visualizations. These visualizations are displayed in a dashboard to facilitate interactive analysis. When an interaction occurs, B2 reifies it as a data query and generates a history log in a new code cell. Subsequent cells can use this log to further analyze interaction results and, when marked as reactive, to ensure that code is automatically recomputed when new interaction occurs. In an evaluative study with data scientists, we find that B2 promotes a tighter feedback loop between coding and interacting with visualizations. All participants frequently moved from code to visualization and vice-versa, which facilitated their exploratory data analysis in the notebook.
SP  - 152
EP  - 165
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415851
ER  - 

TY  - NA
AU  - Sun, Wei; Chen, Yanjun; Zhan, Simon; Han, Teng; Tian, Feng; Wang, Hongan; Yang, Xing-Dong
TI  - CHI - RElectrode: A Reconfigurable Electrode For Multi-Purpose Sensing Based on Microfluidics
PY  - 2021
AB  - In this paper, we propose a reconfigurable electrode, RElectrode, using a microfluidic technique that can change the geometry and material properties of the electrode to satisfy the needs for sensing a variety of different types of user input through touch/touchless gestures, pressure, temperature, and distinguish between different types of objects or liquids. Unlike the existing approaches, which depend on the specific-shaped electrode for particular sensing (e.g., coil for inductive sensing), RElectrode enables capacity, inductance, resistance/pressure, temperature, pH sensings all in a single package. We demonstrate the design and fabrication of the microfluidic structure of our RElectrode, evaluate its sensing performance through several studies, and provide some unique applications. RElectrode demonstrates technical feasibility and application values of integrating physical and biochemical properties of microfluidics into novel sensing interfaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445652
ER  - 

TY  - NA
AU  - Honnet, Cedric; Perner-Wilson, Hannah; Teyssier, Marc; Fruchard, Bruno; Steimle, Jürgen; Baptista, Ana Catarina; Strohmeier, Paul
TI  - CHI - PolySense: Augmenting Textiles with Electrical Functionality using In-Situ Polymerization
PY  - 2020
AB  - We present a method for enabling arbitrary textiles to sense pressure and deformation: In-situ polymerization supports integration of piezoresistive properties at the material level, preserving a textile's haptic and mechanical characteristics. We demonstrate how to enhance a wide set of fabrics and yarns using only readily available tools. To further support customisation by the designer, we present methods for patterning, as needed to create circuits and sensors, and demonstrate how to combine areas of different conductance in one material. Technical evaluation results demonstrate the performance of sensors created using our method is comparable to off-the-shelf piezoresistive textiles. As application examples, we demonstrate rapid manufacturing of on-body interfaces, tie-dyed motion-capture clothing, and zippers that act as potentiometers.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376841
ER  - 

TY  - NA
AU  - Goudswaard, Maas; Abraham, Abel; da Rocha, Bruna Goveia; Andersen, Kristina; Liang, Rong-Hao
TI  - Conference on Designing Interactive Systems - FabriClick: Interweaving Pushbuttons into Fabrics Using 3D Printing and Digital Embroidery
PY  - 2020
AB  - Mechanical pushbuttons, which provide physical landmarks and clear tactile feedback, are easily accessible and highly reliable in eyes-free use. Potentially, their merits can improve the experiences of on-body or wearable HCI. However, they are not commonly adopted as a user interface of smart textiles because the physical mechanism of conventional pushbutton hardware requires further integration, which should be seamless enough to be comfortably worn. In this pictorial, we present a design exploration of the methodologies for interweaving mechanical pushbuttons into fabrics. The exploration used a frame system, which unifies the workflow of digital embroidery and 3D printing and enables the exploration of the physical design. Through the process, we investigated methods of integration and fabrication through making and presented our findings with proof-of-concept implementations. We also discussed the alternative designs and interaction methods as well as their implications to enlighten future research directions and opportunities.
SP  - 379
EP  - 393
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395569
ER  - 

TY  - NA
AU  - Lin, Richard; Ramesh, Rohit; Jain, Nikhil; Koe, Josephine; Nuqui, Ryan; Dutta, Prabal; Hartmann, Bjoern
TI  - UIST - Weaving Schematics and Code: Interactive Visual Editing for Hardware Description Languages
PY  - 2021
AB  - In many engineering disciplines such as circuit board, chip, and mechanical design, a hardware description language (HDL) approach provides important benefits over direct manipulation interfaces by supporting concepts like abstraction and generator meta-programming. While several such HDLs have emerged recently and promised power and flexibility, they also present challenges – especially to designers familiar with current graphical workflows. In this work, we investigate an IDE approach to provide a graphical editor for a board-level circuit design HDL. Unlike GUI builders which convert an entire diagram to code, we instead propose generating equivalent HDL from individual graphical edit actions. By keeping code as the primary design input, we preserve the full power of the underlying HDL, while remaining useful even to advanced users. We discuss our concept, design considerations such as performance, system implementation, and report on the results of an exploratory remote user study with four experienced hardware designers.
SP  - 1039
EP  - 1049
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474804
ER  - 

TY  - JOUR
AU  - Khamaisi, Riccardo Karim; Prati, Elisa; Peruzzini, Margherita; Raffaeli, Roberto; Pellicciari, Marcello
TI  - UX in AR-Supported Industrial Human–Robot Collaborative Tasks: A Systematic Review
PY  - 2021
AB  - The fourth industrial revolution is promoting the Operator 4.0 paradigm, originating from a renovated attention towards human factors, growingly involved in the design of modern, human-centered processes. New technologies, such as augmented reality or collaborative robotics are thus increasingly studied and progressively applied to solve the modern operators’ needs. Human-centered design approaches can help to identify user’s needs and functional requirements, solving usability issues, or reducing cognitive or physical stress. The paper reviews the recent literature on augmented reality-supported collaborative robotics from a human-centered perspective. To this end, the study analyzed 21 papers selected after a quality assessment procedure and remarks the poor adoption of user-centered approaches and methodologies to drive the development of human-centered augmented reality applications to promote an efficient collaboration between humans and robots. To remedy this deficiency, the paper ultimately proposes a structured framework driven by User eXperience approaches to design augmented reality interfaces by encompassing previous research works. Future developments are discussed, stimulating fruitful reflections and a decisive standardization process.
SP  - 10448
EP  - NA
JF  - Applied Sciences
VL  - 11
IS  - 21
PB  - 
DO  - 10.3390/app112110448
ER  - 

TY  - NA
AU  - Pezutti-Dyer, Franklin; Buechley, Leah
TI  - Extruder-Turtle: A Library for 3D Printing Delicate, Textured, and Flexible Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501312
ER  - 

TY  - NA
AU  - Boldu, Roger; Wijewardena, Mevan; Zhang, Haimo; Nanayakkara, Suranga
TI  - MobileHCI - MAGHair: A Wearable System to Create Unique Tactile Feedback by Stimulating Only the Body Hair
PY  - 2020
AB  - We present MAGHair, a novel wearable technique that provides subtle haptic sensation by stimulating the body hair without touching the skin. Our approach builds on previous research in magnetic hair stimulation and magnetic locomotion. We use magnetic cosmetics to augment the body hair, which can then be stimulated by a wearable apparatus that combines electromagnets and permanent magnets. We provide technical insights on the implementation of a fully functional wrist-worn form factor and early adaptations into other form factors. In addition, we provide a workflow for evaluating and characterizing the magnetic cosmetic recipes. Finally, we evaluate MAGHair, which demonstrated that users could detect the sensation of hair movement that they described as gentle and unique.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403545
ER  - 

TY  - JOUR
AU  - Sun, Wei; Chen, Yuwen; Chen, Yanjun; Zhang, Xiaopeng; Zhan, Simon; Li, Yixin; Wu, Jiecheng; Han, Teng; Mi, Haipeng; Wang, Jingxian; Tian, Feng; Yang, Xing-Dong
TI  - MicroFluID
PY  - 2022
AB  - <jats:p>RFID has been widely used for activity and gesture recognition in emerging interaction paradigms given its low cost, lightweight, and pervasiveness. However, current learning-based approaches on RFID sensing require significant efforts in data collection, feature extraction, and model training. To save data processing effort, we present MicroFluID, a novel RFID artifact based on a multiple-chip structure and microfluidic switches, which informs the input state by directly reading variable ID information instead of retrieving primitive signals. Fabricated on flexible substrates, four types of microfluidic switch circuits are designed to respond to external physical events, including pressure, bend, temperature, and gravity. By default, chips are disconnected into the circuit owing to the reserved gaps in transmission line. While external input or status change occurs, conductive liquid floating in the microfluidics channels will fill the gap(s), creating a connection to certain chip(s). In prototyping the device, we conducted a series of simulations and experiments to explore the feasibility of the multi-chip tag design, key fabrication parameters, interaction performance, and users' perceptions.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550296
ER  - 

TY  - NA
AU  - Chen, Yan; Lee, Sang Won; Oney, Steve
TI  - CHI - CoCapture: Effectively Communicating UI Behaviors on Existing Websites by Demonstrating and Remixing
PY  - 2021
AB  - User Interface (UI) mockups are commonly used as shared context during interface development collaboration. In practice, UI designers often use screenshots and sketches to create mockups of desired UI behaviors for communication. However, in the later stages of UI development, interfaces can be arbitrarily complex, making it labor-intensive to sketch, and static screenshots are limited in the types of interactive and dynamic behaviors they can express. We introduce CoCapture, a system that allows designers to easily create UI behavior mockups on existing web interfaces by demonstrating and remixing, and to accurately describe their requests to helpers by referencing the resulting mockups using hypertext. We showed that participants could more accurately describe UI behaviors with CoCapture than with existing sketch and communication tools and that the resulting descriptions were clear and easy to follow. Our approach can help teams develop UIs efficiently by bridging communication gaps with more accurate visual context.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445573
ER  - 

TY  - JOUR
AU  - Pruszko, Laura; Coutrix, Céline; Laurillau, Yann; Piranda, Benoît; Bourgeois, Julien
TI  - Molecular HCI
PY  - 2021
AB  - <jats:p>Shape-changing User Interfaces attract growing interest in Human-Computer Interaction. Modular robotics offer a great opportunity for their implementation. However, the current theoretical and technical advances of modular robotics are fragmented and little centered on the user. To unify existing work and center future research on the user, we perform a systematic literature review enabling us to build a unifying space for the design of modular shape-changing user interfaces.Our aim is to bridge the gap between HCI and robotics. We relate properties of different domains and identify inconsistencies to structure the design space. Towards this aim, we conduct a thorough cross-disciplinary survey to propose: 1) a set of design properties at the scale of the interface (macro-scale) and at the scale of the modules (micro-scale) and 2) the impact of these properties on each other. This paper can be used to describe and compare existing modular shape-changing UIs and generate new design ideas by building upon knowledge from robotics and HCI.</jats:p>
SP  - 1
EP  - 33
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3461733
ER  - 

TY  - NA
AU  - Reipschläger, Patrick; Brudy, Frederik; Dachselt, Raimund; Matejka, Justin; Fitzmaurice, George; Anderson, Fraser
TI  - AvatAR: An Immersive Analysis Environment for Human Motion Data Combining Interactive 3D Avatars and Trajectories
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517676
ER  - 

TY  - NA
AU  - Chu, Shao-Yu; Cheng, Yun-Ting; Lin, Shih Chin; Huang, Yung-Wen; Chen, Yi; Chen, Mike Y.
TI  - UIST - MotionRing: Creating Illusory Tactile Motion around the Head using 360° Vibrotactile Headbands
PY  - 2021
AB  - We present MotionRing, a vibrotactile headband that creates illusory tactile motion around the head by controlling the timing of a 1-D, 360° sparse array of vibration motors. Its unique ring shape enables symmetric and asymmetric haptic motion experiences, such as when users pass through a medium and when an object passes nearby in any direction. We first conducted a perception study to understand how factors such as vibration motor timing, spacing, duration, intensity, and head region affect the perception of apparent tactile motion. Results showed that illusory tactile motion around the head can be achieved with 12 and 16 vibration motors with angular speed between 0.5-4.9 revolutions per second. We developed a symmetric and an asymmetric tactile motion pattern to enhance the experience of teleportation in VR and dodging footballs, respectively. We conducted a user study to compare the experience of MotionRing vs. static vibration patterns and visual-only feedback. Results showed that illusory tactile motion significantly improved users’ perception of directionality and enjoyment of motion events, and was most preferred by users.
SP  - 724
EP  - 731
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474781
ER  - 

TY  - JOUR
AU  - Liu, Guanhong; Gu, Yizheng; Yin, Yiwen; Yu, Chun; Wang, Yuntao; Mi, Haipeng; Shi, Yuanchun
TI  - Keep the Phone in Your Pocket: Enabling Smartphone Operation with an IMU Ring for Visually Impaired People
PY  - 2020
AB  - Previous studies have shown that visually impaired users face a unique set of pain points in smartphone interaction including locating and removing the phone from a pocket, two-handed interaction while holding a cane, and keeping personal data private in a public setting. In this paper, we present a ring-based input interaction that enables in-pocket smartphone operation. By wearing a ring with an Inertial Measurement Unit on the index finger, users can perform gestures on any surface (e.g., tables, thighs) using subtle, one-handed gestures and receive auditory feedback via earphones. We conducted participatory studies to obtain a set of versatile commands and corresponding gestures. We subsequently trained an SVM model to recognize these gestures and achieved a mean accuracy of 95.5% on 15 classifications. Evaluation results showed that our ring interaction is more efficient than some baseline phone interactions and is easy, private, and fun to use.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397308
ER  - 

TY  - JOUR
AU  - Wu, Shuangqing; Zeng, Taotao; Liu, Zhenhua; Ma, Guozhi; Xiong, Zhengyu; Zuo, Lin; Zhou, Zeyan
TI  - 3D Printing Technology for Smart Clothing: A Topic Review.
PY  - 2022
AB  - Clothing is considered to be an important element of human social activities. With the increasing maturity of 3D printing technology, functional 3D printing technology can realize the perfect combination of clothing and electronic devices while helping smart clothing to achieve specific functions. Furthermore, the application of functional 3D printing technology in clothing not only provides people with the most comfortable and convenient wearing experience, but also completely subverts consumers' perception of traditional clothing. This paper introduced the progress of the application of 3D printing from the aspect of traditional clothing and smart clothing through two mature 3D printing technologies normally used in the field of clothing, and summarized the challenges and prospects of 3D printing technology in the field of smart clothing. Finally, according to the analysis of the gap between 3D-printed clothing and traditionally made clothing due to the material limitations, this paper predicted that the rise in intelligent materials will provide a new prospect for the development of 3D-printed clothing. This paper will provide some references for the application research of 3D printing in the field of smart clothing.
SP  - 7391
EP  - 7391
JF  - Materials (Basel, Switzerland)
VL  - 15
IS  - 20
PB  - 
DO  - 10.3390/ma15207391
ER  - 

TY  - CONF
AU  - Li, Jiannan; Balakrishnan, Ravin; Grossman, Tovi
TI  - Graphics Interface - StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation.
PY  - 2019
AB  - NA
SP  - 317
EP  - 326
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wang, Kevin; Ramanan, Deva; Bansal, Aayush
TI  - Video Exploration via Video-Specific Autoencoders.
PY  - 2021
AB  - We present simple video-specific autoencoders that enables human-controllable video exploration. This includes a wide variety of analytic tasks such as (but not limited to) spatial and temporal super-resolution, spatial and temporal editing, object removal, video textures, average video exploration, and correspondence estimation within and across videos. Prior work has independently looked at each of these problems and proposed different formulations. In this work, we observe that a simple autoencoder trained (from scratch) on multiple frames of a specific video enables one to perform a large variety of video processing and editing tasks. Our tasks are enabled by two key observations: (1) latent codes learned by the autoencoder capture spatial and temporal properties of that video and (2) autoencoders can project out-of-sample inputs onto the video-specific manifold. For e.g. (1) interpolating latent codes enables temporal super-resolution and user-controllable video textures; (2) manifold reprojection enables spatial super-resolution, object removal, and denoising without training for any of the tasks. Importantly, a two-dimensional visualization of latent codes via principal component analysis acts as a tool for users to both visualize and intuitively control video edits. Finally, we quantitatively contrast our approach with the prior art and found that without any supervision and task-specific knowledge, our approach can perform comparably to supervised approaches specifically trained for a task.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Behera, Ajit
TI  - Self-Healing Materials
PY  - 2021
AB  - In the current era, the self-healing properties found in some advanced materials have a resemblance with the biological wound healing process. This chapter describes the gradual development of metal, ceramic, polymer, and others in the direction of self-healing at room temperature as well as in high-temperature applications. Various classifications of materials such as autonomic, nonautonomic, intrinsic, and extrinsic processes of self-healing are described with their mechanism. Self-healing polymer is a step ahead of other materials in cent percent healing. Different concepts and thermodynamic approaches of thermoplastic, as well as thermosetting polymer, have been discussed broadly. Applications in aero industries, automobile, electrical and electronics, mechanical, medical, textile, and other industries have been discussed extensively.
SP  - 321
EP  - 358
JF  - Advanced Materials
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-80359-9_10
ER  - 

TY  - NA
AU  - Kery, Mary Beth; Ren, Donghao; Hohman, Fred; Moritz, Dominik; Wongsuphasawat, Kanit; Patel, Kayur
TI  - mage: Fluid Moves Between Code and Graphical Work in Computational Notebooks
PY  - 2020
AB  - We aim to increase the flexibility at which a data worker can choose the right tool for the job, regardless of whether the tool is a code library or an interactive graphical user interface (GUI). To achieve this flexibility, we extend computational notebooks with a new API mage, which supports tools that can represent themselves as both code and GUI as needed. We discuss the design of mage as well as design opportunities in the space of flexible code/GUI tools for data work. To understand tooling needs, we conduct a study with nine professional practitioners and elicit their feedback on mage and potential areas for flexible code/GUI tooling. We then implement six client tools for mage that illustrate the main themes of our study findings. Finally, we discuss open challenges in providing flexible code/GUI interactions for data workers.
SP  - 140
EP  - 151
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415842
ER  - 

TY  - NA
AU  - Moradi, Hedieh; Torres, César I.
TI  - Conference on Designing Interactive Systems - Siloseam: A Morphogenetic Workflow for the Design and Fabrication of Inflatable Silicone Bladders
PY  - 2020
AB  - Silicone is a transformative design material found within a variety of emerging HCI practices including shape-changing interfaces, soft robotics, and wearables. However, workflows for designing and fabricating silicone forms require a time-intensive mold-cast-cure pipeline that limits the experiential knowledge that can be gained from working directly with silicone. In this work, we conduct a material-centric exploration of silicone and develop designerly workflows for creating inflatable silicone bladders. We present Siloseam, a creative framework that streamlines a bladder design and fabrication process, collects tacit knowledge involved in recovering from errors, and introduces new workflows that reuse existing molds. A set of exemplar artifacts demonstrates an expanded repertoire of silicone forms that leverage various configurations of airtight seams to create playful, haptic interactions. We discuss the remaining challenges in integrating silicone with a broader range of materials and opportunities for developing designerly workflows for other mold-and-cast processes.
SP  - 1995
EP  - 2006
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395473
ER  - 

TY  - NA
AU  - Kuroki, Hinako; Baba, Tetsuaki
TI  - Calligraphy Z: A Fabricatable Pen Plotter for Handwritten Strokes with Z-Axis Pen Pressure
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558657
ER  - 

TY  - NA
AU  - Abe, Takehiro; Sakamoto, Daisuke
TI  - MobileHCI - MagneTrack: Magnetic Field Separation Method for Continuous and Simultaneous 1-DOF Tracking of Two-magnets
PY  - 2021
AB  - This paper presents a method to track two magnets using one magnetometer on a smart device (e.g., smartphone), which detects the locations of the magnets around the device. A magnetometer measures the overlapping data signals generated by the two magnets. Thus, a data-separation technique is required to separately track the continuous and simultaneous signals generated by the two magnets. In this study, we develop a separation algorithm based on nonnegative matrix factorization to continuously and simultaneously track two magnets, and then conduct an experiment to validate our concept. We then compare our method in both simulated and actual environments. In the actual environment, we observe real data signals arising from the two magnets on a magnetometer. Finally, we present example applications to demonstrate the use cases of our model in human–computer interaction systems.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472052
ER  - 

TY  - NA
AU  - Balaji, Ananta Narayanan; Peh, Li-Shiuan
TI  - CHI Extended Abstracts - AI-on-skin: Enabling On-body AI Inference for Wearable Artificial Skin Interfaces
PY  - 2021
AB  - Existing artificial skin interfaces suffer from the lack of on-skin compute that can provide fast neural network inference for time-critical application scenarios. In this paper, we propose AI-on-skin - a wearable artificial skin interface integrated with a neural network hardware accelerator that can be reconfigured across diverse neural network models and applications. AI-on-skin is designed to scale to the entire body, comprising tiny, low-power, accelerators distributed across the body. We built a prototype of AI-on-skin that covers the entire forearm (17 by 10 cm) based on off-the-shelf FPGAs. Our electronic skin based prototype can perform (a) handwriting recognition with 96% accuracy, (b) gesture recognition with 95% accuracy and (c) handwritten word recognition with 93.5% accuracy. AI-On-Skin achieves 20X and 35X speedup over off-body inference via bluetooth and on-body microcontroller based inference approach respectively. To the best of our knowledge, AI-On-Skin is the first ever wearable prototype to demonstrate skin interfaces with on-body neural network inference.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451689
ER  - 

TY  - NA
AU  - Yanhong, Jia
TI  - Novel Structure and Expression of Printing Path for 3D Printing in Rapid Prototyping
PY  - 2021
AB  - Additive manufacturing (AM), commonly known as three-dimensional (3D) printing, refers to a method of rapid prototyping through material stacking. In the field of human-computer interaction, many researchers have used technology to realise rapid prototyping, such as printing parts, mechanical structures and product shells. Some researchers have explored interactive product manufacturing based on AM, such as flexible electronic fabric, four-dimensional self-forming structure and conductive ink. Most of these studies reformed the printing nozzle and printing materials on the basis of the traditional fused deposition modelling 3D printer, but the research on the printing path is still in the initial stage in the exploration of new interactive product manufacturing. Accordingly, this paper focuses on interactive manufacturing technology and the innovations of printing paths and parameters. The results are hoped to provide inspiration for future interactive manufacturing.
SP  - 637
EP  - 640
JF  - 2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aiid51893.2021.9456511
ER  - 

TY  - NA
AU  - Gero, Katy Ilonka; Chilton, Lydia; Melancon, Chris; Cleron, Mike
TI  - Eliciting Gestures for Novel Note-taking Interactions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533480
ER  - 

TY  - JOUR
AU  - Lee, Lik Hang; Zhu, Yiming; Yau, Yui-Pan; Hui, Pan; Pirttikangas, Susanna
TI  - Press-n-Paste: Copy-and-Paste Operations with Pressure-sensitive Caret Navigation for Miniaturized Surface in Mobile Augmented Reality
PY  - 2021
AB  - Copy-and-paste operations are the most popular features on computing devices such as desktop computers, smartphones and tablets. However, the copy-and-paste operations are not sufficiently addressed on the Augmented Reality (AR) smartglasses designated for real-time interaction with texts in physical environments. This paper proposes two system solutions, namely Granularity Scrolling (GS) and Two Ends (TE), for the copy-and-paste operations on AR smartglasses. By leveraging a thumb-size button on a touch-sensitive and pressure-sensitive surface, both the multi-step solutions can capture the target texts through indirect manipulation and subsequently enables the copy-and-paste operations. Based on the system solutions, we implemented an experimental prototype named Press-n-Paste (PnP). After the eight-session evaluation capturing 1,296 copy-and-paste operations, 18 participants with GS and TE achieve the peak performance of 17,574 ms and 13,951 ms per copy-and-paste operation, with 93.21% and 98.15% accuracy rates respectively, which are as good as the commercial solutions using direct manipulation on touchscreen devices. The user footprints also show that PnP has a distinctive feature of miniaturized interaction area within 12.65 mm * 14.48 mm. PnP not only proves the feasibility of copy-and-paste operations with the flexibility of various granularities on AR smartglasses, but also gives significant implications to the design space of pressure widgets as well as the input design on smart wearables.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3457146
ER  - 

TY  - NA
AU  - Chilufya, Emma Mainza; Arvola, Mattias
TI  - HAI - Conceptual Designing of a Virtual Receptionist: Remote Desktop Walkthrough and Bodystorming in VR
PY  - 2021
AB  - Early user involvement in the design of intelligent virtual agents (IVAs) is fundamental for usability and otherwise good user experience. In this paper, we present a combination of methods used in the remote conceptual design of a virtual receptionist for a university department. The study builds on two workshops with potential users. The first was a bodystorming workshop in virtual reality (VR) with four researchers at the department, and the second was a desktop walkthrough workshop on an online whiteboard with five students at the department. Proposed solutions from the workshops were deconstructed using a morphological chart into a pentad of parameters: agent, act, scene, agency, and purpose. New design concepts were then composed by combining solutions. Sketching was used to further detail and present the generated concepts. Our analysis of the workshops indicates that the bodystorming workshop had an aesthetic perspective on embodied interaction while the desktop walkthrough workshop had a more instrumental perspective on usability. The combination of embodied but remote ideation methods with morphological chart structured by the pentad is novel to not only the IVA field, but also to interaction design in general. Finally, the conceptual design of a novel cross-platform IVA is proposed.
SP  - 112
EP  - 120
JF  - Proceedings of the 9th International Conference on Human-Agent Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472307.3484171
ER  - 

TY  - NA
AU  - Nisser, Martin; Liao, Christina Chen; Chai, Yuchen; Adhikari, Aradhana; Hodges, Steve; Mueller, Stefanie
TI  - CHI - LaserFactory: A Laser Cutter-based Electromechanical Assembly and Fabrication Platform to Make Functional Devices & Robots
PY  - 2021
AB  - LaserFactory is an integrated fabrication process that augments a commercially available fabrication machine to support the manufacture of fully functioning devices without human intervention. In addition to creating 2D and 3D mechanical structures, LaserFactory creates conductive circuit traces with arbitrary geometries, picks-and-places electronic and electromechanical components, and solders them in place. To enable this functionality, we make four contributions. First, we build a hardware add-on to the laser cutter head that can deposit silver circuit traces and assemble components. Second, we develop a new method to cure dispensed silver using a CO2 laser. Third, we build a motion-based signaling method that allows our system to be readily integrated with commercial laser cutters. Finally, we provide a design and visualization tool for making functional devices with LaserFactory. Having described the LaserFactory system, we demonstrate how it is used to fabricate devices such as a fully functioning quadcopter and a sensor-equipped wristband. Our evaluation shows that LaserFactory can assemble a variety of differently sized components (up to 65g), that these can be connected by narrow traces (down to 0.75mm) that become highly conductive after laser soldering (3.2Ω/m), and that our acceleration-based sensing scheme works reliably (to 99.5% accuracy).
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445692
ER  - 

TY  - NA
AU  - Subramonyam, Hariharan; Seifert, Colleen M.; Adar, Eytan
TI  - IUI - ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces
PY  - 2021
AB  - When prototyping AI experiences (AIX), interface designers seek useful and usable ways to support end-user tasks through AI capabilities. However, AI poses challenges to design due to its dynamic behavior in response to training data, end-user data, and feedback. Designers must consider AI’s uncertainties and offer adaptations such as explainability, error recovery, and automation vs. human task control. Unfortunately, current prototyping tools assume a black-box view of AI, forcing designers to work with separate tools to explore machine learning models, understand model performance, and align interface choices with model behavior. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow for AIX design that combines model exploration with UI prototyping tasks. Our system, ProtoAI, allows designers to directly incorporate model outputs into interface designs, evaluate design choices across different inputs, and iteratively revise designs by analyzing model breakdowns. We demonstrate how ProtoAI can readily operationalize human-AI design guidelines. Our user study finds that designers can effectively engage in MIP to create and evaluate AI-powered interfaces during AIX design.
SP  - 48
EP  - 58
JF  - 26th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397481.3450640
ER  - 

TY  - NA
AU  - Lam, Kit Yung; Lee, Lik Hang; Hui, Pan
TI  - ACM Multimedia - A2W: Context-Aware Recommendation System for Mobile Augmented Reality Web Browser
PY  - 2021
AB  - Augmented Reality (AR) offers new capabilities for blurring the boundaries between physical reality and digital media. However, the capabilities of integrating web contents and AR remain underexplored. This paper presents an AR web browser with an integrated context-aware AR-to-Web content recommendation service named as A2W browser, to provide continuously user-centric web browsing experiences driven by AR headsets. We implement the A2W browser on an AR headset as our demonstration application, demonstrating the features and performance of A2W framework. The A2W browser visualizes the AR-driven web contents to the user, which is suggested by the content-based filtering model in our recommendation system. In our experiments, 20 participants with the adaptive UIs and recommendation system in A2W browser achieve up to 30.69% time saving compared to smartphone conditions. Accordingly, A2W-supported web browsing on workstations facilitates the recommended information leading to 41.67% faster reaches to the target information than typical web browsing.
SP  - 2447
EP  - 2455
JF  - Proceedings of the 29th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474085.3475413
ER  - 

TY  - NA
AU  - Zheng, Clement; Yong, Zhen Zhou; Lin, Hongnan; Oh, HyunJoo; Yen, Ching Chiuan
TI  - Shape-Haptics: Planar & Passive Force Feedback Mechanisms for Physical Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501829
ER  - 

TY  - NA
AU  - Teyssier, Marc; Koelle, Marion; Strohmeier, Paul; Fruchard, Bruno; Steimle, Jürgen
TI  - CHI - Eyecam: Revealing Relations between Humans and Sensing Devices through an Anthropomorphic Webcam
PY  - 2021
AB  - We are surrounded by sensing devices. We are accustomed to them, appreciate their benefits, and even create affective bonds and might neglect the implications they might have for our daily life. By presenting Eyecam, an anthropomorphic webcam mimicking a human eye, we challenge conventional relationships with ubiquitous sensing devices and call to re-think how sensing devices might appear and behave. Inspired by critical design, Eyecam is an exaggeration of a familiar sensing device which allows for critical reflections on its perceived functionalities and its impact on human-human and human-device relations. We identify 5 different roles Eyecam can take: Mediator, Observer, Mirror, Presence, and Agent. Contributing design fictions and thinking prompts, we allow for articulation on privacy awareness and intrusion, affect in mediated communication, agency and self-perception along with speculation on potential futures. We envision this work to contribute to a bold and responsible design of ubiquitous sensing devices.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445491
ER  - 

TY  - NA
AU  - Ragozin, Kirill; Kunze, Kai; Marky, Karola; Pai, Yun Suen
TI  - CHI Extended Abstracts - MazeRunVR: An Open Benchmark for VR Locomotion Performance, Preference and Sickness in the Wild
PY  - 2020
AB  - Locomotion in virtual reality (VR) is one of the biggest problems for large scale adoption of VR applications. Yet, to our knowledge, there are few studies conducted in-the-wild to understand performance metrics and general user preference for different mechanics. In this paper, we present the first steps towards an open framework to create a VR locomotion benchmark. As a viability study, we investigate how well the users move in VR when using three different locomotion mechanics. It was played in over 124 sessions across 10 countries in a period of three weeks. The included prototype locomotion mechanics are arm swing,walk-in-place and trackpad movement. We found that over-all, users performed significantly faster using arm swing and trackpad when compared to walk-in-place. For subjective preference, arm swing was significantly more preferred over the other two methods. Finally for induced sickness, walk-in-place was the overall most sickness-inducing locomotion method.
SP  - 3383035
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383035
ER  - 

TY  - NA
AU  - Asai, Kentaro; Fukusato, Tsukasa; Igarashi, Takeo
TI  - CHI - Integrated Development Environment with Interactive Scatter Plot for Examining Statistical Modeling
PY  - 2020
AB  - The development of a statistical modeling program requires example data to observe and verify the behavior of the program. Such example data are either taken from an existing dataset or synthesized using commands. Programmers may want to directly design an arbitrary dataset or modify it interactively, but it is difficult to do so in current development environments. We therefore propose combining a code editor with an interactive scatter plot editor to efficiently understand the behavior of statistical modeling algorithms. The user interactively creates and modifies the dataset on the scatter plot editor, while the system continuously executes the code in the editor, taking the data as input, and shows the result in the editor. This paper presents the design rationale behind the system and introduces several usage scenarios.
SP  - 1
EP  - 7
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376455
ER  - 

TY  - NA
AU  - Cheng, Yifei; Yan, Yukang; Yi, Xin; Shi, Yuanchun; Lindlbauer, David
TI  - UIST - SemanticAdapt: Optimization-based Adaptation of Mixed Reality Layouts Leveraging Virtual-Physical Semantic Connections
PY  - 2021
AB  - We present an optimization-based approach that automatically adapts Mixed Reality (MR) interfaces to different physical environments. Current MR layouts, including the position and scale of virtual interface elements, need to be manually adapted by users whenever they move between environments, and whenever they switch tasks. This process is tedious and time consuming, and arguably needs to be automated for MR systems to be beneficial for end users. We contribute an approach that formulates this challenge as a combinatorial optimization problem and automatically decides the placement of virtual interface elements in new environments. To achieve this, we exploit the semantic association between the virtual interface elements and physical objects in an environment. Our optimization furthermore considers the utility of elements for users’ current task, layout factors, and spatio-temporal consistency to previous layouts. All those factors are combined in a single linear program, which is used to adapt the layout of MR interfaces in real time. We demonstrate a set of application scenarios, showcasing the versatility and applicability of our approach. Finally, we show that compared to a naive adaptive baseline approach that does not take semantic associations into account, our approach decreased the number of manual interface adaptations by 33%.
SP  - 282
EP  - 297
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474750
ER  - 

TY  - NA
AU  - Tadeja, Slawomir Konrad; Langdon, Patrick; Kristensson, Per Ola
TI  - ISMAR - Supporting Iterative Virtual Reality Analytics Design and Evaluation by Systematic Generation of Surrogate Clustered Datasets
PY  - 2021
AB  - Virtual Reality (VR) is a promising technology platform for immersive visual analytics. However, the design space of VR analytics interface design is vast and difficult to explore using traditional A/B comparisons in formal or informal controlled experiments— a fundamental part of an iterative design process. A key factor that complicates such comparisons is the dataset. Exposing participants to the same dataset in all conditions introduces an unavoidable learning effect. On the other hand, using different datasets for all experimental conditions introduces the dataset itself as an uncontrolled variable, which reduces internal validity to an unacceptable degree. In this paper, we propose to rectify this problem by introducing a generative process for synthesizing clustered datasets for VR analytics experiments. This process generates datasets that are distinct while simultaneously allowing systematic comparisons in experiments. A key advantage is that these datasets can then be used in iterative design processes. In a two-part experiment, we show the validity of the generative process and demonstrate how new insights in VR-based visual analytics can be gained using synthetic datasets.
SP  - 376
EP  - 385
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00054
ER  - 

TY  - NA
AU  - Oh, Seungjae; Park, Chaeyong; Jeon, Yo-Seb; Choi, Seungmoon
TI  - UIST - Identifying Contact Fingers on Touch Sensitive Surfaces by Ring-Based Vibratory Communication
PY  - 2021
AB  - As computing paradigms shift toward mobile and ubiquitous interaction, there is an increasing demand for wearable interfaces supporting multifaceted input in smart living environments. In this regard, we introduce a system that identifies contact fingers using vibration as a modality of communication. We investigate the vibration characteristics of the communication channels involved and simulate the transmission of vibration sequences. In the simulation, we test and refine modulation and demodulation methods to design vibratory communication protocols that are robust to environmental noises and can detect multiple simultaneous contact fingers. As a result, we encode an on-off keying sequence with a unique carrier frequency to each finger and demodulate the sequences by applying cross-correlation. We verify the communication protocols in two environments, laboratory and cafe, where the resulting highest accuracy was 93 % and 90.5 %, respectively. Our system achieves over 91 % accuracy in identifying seven contact states from three fingers while wearing only two actuator rings with the aid of a touch screen. Our findings shed light on diversifying touch interactions on rigid surfaces by means of vibratory communication.
SP  - 208
EP  - 222
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474745
ER  - 

TY  - NA
AU  - Jun, Eunice; Birchfield, Melissa; de Moura, Nicole; Heer, Jeffrey; Just, René
TI  - Hypothesis Formalization: Empirical Findings, Software Limitations, and Design Implications.
PY  - 2021
AB  - Data analysis requires translating higher level questions and hypotheses into computable statistical models. We present a mixed-methods study aimed at identifying the steps, considerations, and challenges involved in operationalizing hypotheses into statistical models, a process we refer to as hypothesis formalization. In a formative content analysis of research papers, we find that researchers highlight decomposing a hypothesis into sub-hypotheses, selecting proxy variables, and formulating statistical models based on data collection design as key steps. In a lab study, we find that analysts fixated on implementation and shaped their analysis to fit familiar approaches, even if sub-optimal. In an analysis of software tools, we find that tools provide inconsistent, low-level abstractions that may limit the statistical models analysts use to formalize hypotheses. Based on these observations, we characterize hypothesis formalization as a dual-search process balancing conceptual and statistical considerations constrained by data and computation, and discuss implications for future tools.
SP  - NA
EP  - NA
JF  - arXiv: Other Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Qian, Xun; He, Fengming; Hu, Xiyun; Wang, Tianyi; Ipsita, Ananya; Ramani, Karthik
TI  - ScalAR: Authoring Semantically Adaptive Augmented Reality Experiences in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517665
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Tappa, Jordan L; Zheng, Yi; Forman, Jack; Leong, Joanne; Koenig, Sven; Ishii, Hiroshi
TI  - (Dis)Appearables: A Concept and Method for Actuated Tangible UIs to Appear and Disappear based on Stages
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501906
ER  - 

TY  - JOUR
AU  - Kirsch, Louise P.; Job, Xavier E.; Auvray, Malika; Hayward, Vincent
TI  - Harnessing tactile waves to measure skin-to-skin interactions.
PY  - 2020
AB  - Skin-to-skin touch is an essential form of tactile interaction, yet there is no known method to quantify how we touch our own skin or someone else's skin. Skin-to-skin touch is particularly challenging to measure objectively, since interposing an instrumented sheet, no matter how thin and flexible, between the interacting skins is not an option. To fill this gap, we explored a technique that takes advantage of the propagation of vibrations from the locus of touch to pick up a signal that contains information about skin-to-skin tactile interactions. These "tactile waves" were measured by an accelerometer sensor placed on the touching finger. Applied pressure and speed had a direct influence on measured signal power when the target of touch was the self or another person. The measurements were insensitive to changes in the location of the sensor relative to the target. Our study suggests that this method has potential for probing behaviour during skin-to-skin tactile interactions and could be a valuable technique to study social touch, self-touch, and motor control. The method is non-invasive, easy to commission, inexpensive, and robust.
SP  - 1469
EP  - 1477
JF  - Behavior research methods
VL  - 53
IS  - 4
PB  - 
DO  - 10.3758/s13428-020-01492-3
ER  - 

TY  - JOUR
AU  - Liang, Chen; Hsia, Chi; Yu, Chun; Yan, Yukang; Wang, Yuntao; Shi, Yuanchun
TI  - DRG-Keyboard
PY  - 2022
AB  - <jats:p>We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.</jats:p>
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569463
ER  - 

TY  - JOUR
AU  - Baroroh, Dawi Karomati; Chu, Chih-Hsing; Wang, Lihui
TI  - Systematic literature review on augmented reality in smart manufacturing: Collaboration between human and computational intelligence
PY  - 2021
AB  - NA
SP  - 696
EP  - 711
JF  - Journal of Manufacturing Systems
VL  - 61
IS  - NA
PB  - 
DO  - 10.1016/j.jmsy.2020.10.017
ER  - 

TY  - JOUR
AU  - Zeng, Hong; Yu, Weijie; Chen, Dapeng; Hu, Xuhui; Zhang, Dingguo; Song, Aiguo
TI  - Exploring Biomimetic Stiffness Modulation and Wearable Finger Haptics for Improving Myoelectric Control of Virtual Hand.
PY  - 2022
AB  - The embodiment of virtual hand (VH) by the user is generally deemed to be important for virtual reality (VR) based hand rehabilitation applications, which may help to engage the user and promote motor skill relearning. In particular, it requires that the VH should produce task-dependent interaction behaviors from rigid to soft. While such a capability is inherent to humans via hand stiffness regulation and haptic interactions, yet it have not been successfully imitated by VH in existing studies. In this paper, we present a work which integrates biomimetic stiffness regulation and wearable finger force feedback in VR scenarios involving myoelectric control of VH. On one hand, the biomimetic stiffness modulation intuitively enables VH to imitate the stiffness profile of the user's hand in real time. On the other hand, the wearable finger force-feedback device elicits a natural and realistic sensation of external force on the fingertip, which provides the user a proper understanding of the environment for enhancing his/her stiffness regulation. The benefits of the proposed integrated system were evaluated with eight healthy subjects that performed two tasks with opposite stiffness requirements. The achieved performance is compared with reduced versions of the integrated system, where either biomimetic impedance control or wearable force feedback is excluded. The results suggest that the proposed integrated system enables the stiffness of VH to be adaptively regulated by the user through the perception of interaction torques and vision, resulting in task-dependent behaviors from rigid to soft for VH.
SP  - 1601
EP  - 1611
JF  - IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society
VL  - 30
IS  - NA
PB  - 
DO  - 10.1109/tnsre.2022.3181284
ER  - 

TY  - JOUR
AU  - Liu, Shi-Hong; Yen, Pai-Chien; Mao, Yi-Hsuan; Lin, Yu-Hsin; Chandra, Erick; Chen, Mike Y.
TI  - HeadBlaster: a wearable approach to simulating motion perception using head-mounted air propulsion jets
PY  - 2020
AB  - We present HeadBlaster, a novel wearable technology that creates motion perception by applying ungrounded force to the head to stimulate the vestibular and proprioception sensory systems. Compared to motion platforms that tilt the body, HeadBlaster more closely approximates how lateral inertial and centrifugal forces are felt during real motion to provide more persistent motion perception. In addition, because HeadBlaster only actuates the head rather than the entire body, it eliminates the mechanical motion platforms that users must be constrained to, which improves user mobility and enables room-scale VR experiences. We designed a wearable HeadBlaster system with 6 air nozzles integrated into a VR headset, using compressed air jets to provide persistent, lateral propulsion forces. By controlling multiple air jets, it is able to create the perception of lateral acceleration in 360 degrees. We conducted a series of perception and human-factor studies to quantify the head movement, the persistence of perceived acceleration, and the minimal level of detectable forces. We then explored the user experience of HeadBlaster through two VR applications: a custom surfing game, and a commercial driving simulator together with a commercial motion platform. Study results showed that HeadBlaster provided significantly longer perceived duration of acceleration than motion platforms. It also significantly improved realism and immersion, and was preferred by users compared to using VR alone. In addition, it can be used in conjunction with motion platforms to further augment the user experience.
SP  - 84
EP  - NA
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 4
PB  - 
DO  - 10.1145/3386569.3392482
ER  - 

TY  - NA
AU  - Ran, Mengyuan; Wang, Shanshan; Liao, Jun; Zhang, Yuhao; Liu, Li
TI  - SMC - STGauntlet: Recognizing Hand Gestures over Multiple Hand-Worn Motion Sensors
PY  - 2020
AB  - Hand gesture recognition with wearables typically focuses on the characteristics of a single point on hand, but ignores the diversity of motion information over hand skeleton. As a result, current methods suffer from two key challenges to manage multiple hand joints: displacement detection and motion representation. This leads us to define a spatio-temporal framework, named STGauntlet, that explicitly characterizes the hand motion context of spatio-temporal relations among multiple joints and detects hand gestures in real-time. The framework introduces the Lie algebra to capture the inherent structural varieties of hand motions with spatio-temporal dependencies among multiple joints. In addition, we developed a hand-worn prototype with multiple motion sensors respectively attached to various joints on hand and collected 7000 samples of seven gestures from nine subjects. Our in-lab study shows that STGauntlet is capable of detecting gesture types together with their 3D tracking trajectory with 97.35% and 95.17% accuracies for subject dependent and independent recognition, respectively.
SP  - 3340
EP  - 3345
JF  - 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smc42975.2020.9282843
ER  - 

TY  - NA
AU  - Yang, Willa Yunqi; Zhuang, Yumeng; Darcy, Luke Andre; Liu, Grace; Ion, Alexandra
TI  - Reconfigurable Elastic Metamaterials
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545649
ER  - 

TY  - JOUR
AU  - Zhang, Zhenning; Pan, Zhigeng; Li, Weiqing; Su, Zhiyong
TI  - X-Board: an egocentric adaptive AR assistant for perception in indoor environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Virtual Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10055-022-00742-3
ER  - 

TY  - NA
AU  - Murayama, Takumi; Yamaoka, Junichi; Kakehi, Yasuaki
TI  - CHI Extended Abstracts - Reflatables: A Tube-based Reconfigurable Fabrication of Inflatable 3D Objects
PY  - 2020
AB  - In this study, we propose a novel fabrication method to create large-scale 3D objects by bending and piling up an inflated single polyethylene tube. A unique point of our method is that we can redo the design and reconfigure it using the same tube after evacuating the air from the object. To provide detachable constraints on the tube for bending, we developed a novel technique with a hook-and-loop fastener attached on the tube. We also designed software to convert 3D targeted object shape to the bending pattern of the tube. Furthermore, we implemented a prototype of a machine for the automation of the attaching process of the fastener on the tube. In this paper, we describe the details of the design, implementation, results and future works.
SP  - 3382904
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382904
ER  - 

TY  - NA
AU  - Ryu, Neung; Jo, Hye-Young; Pahud, Michel; Sinclair, Mike; Bianchi, Andrea
TI  - CHI - GamesBond: Bimanual Haptic Illusion of Physically Connected Objects for Immersive VR Using Grip Deformation
PY  - 2021
AB  - Virtual Reality experiences, such as games and simulations, typically support the usage of bimanual controllers to interact with virtual objects. To recreate the haptic sensation of holding objects of various shapes and behaviors with both hands, previous researchers have used mechanical linkages between the controllers that render adjustable stiffness. However, the linkage cannot quickly adapt to simulate dynamic objects, nor it can be removed to support free movements. This paper introduces GamesBond, a pair of 4-DoF controllers without physical linkage but capable to create the illusion of being connected as a single device, forming a virtual bond. The two controllers work together by dynamically displaying and physically rendering deformations of hand grips, and so allowing users to perceive a single connected object between the hands, such as a jumping rope. With a user study and various applications we show that GamesBond increases the realism, immersion, and enjoyment of bimanual interaction.
SP  - 1
EP  - 10
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445727
ER  - 

TY  - NA
AU  - Achberger, Alexander; Aust, Fabian; Pohlandt, Daniel; Vidackovic, Kresimir; Sedlmair, Michael
TI  - UIST - STRIVE: String-Based Force Feedback for Automotive Engineering
PY  - 2021
AB  - The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice.
SP  - 841
EP  - 853
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474790
ER  - 

TY  - NA
AU  - Liu, Siyi; Lee, Gun A.; Li, Yi; Piumsomboon, Thammathip; Ens, Barrett
TI  - Force-Based Foot Gesture Navigation in Virtual Reality
PY  - 2021
AB  - Navigation is a primary interaction in virtual reality. Previous research has explored different forms of artificial locomotion techniques for navigation, including hand gestures and body motions. However, few studies have investigated force-based foot gestures as a locomotion technique. We present three force-based foot gestures (Foot Fly, Foot Step and Foot Teleportation) for navigation in a virtual environment, relying on surface electromyography sensors readings from leg muscles. A pilot study comparing our techniques with controller-based techniques indicates that force-based foot gestures can provide a fun and engaging alternative. Of all six input techniques evaluated, Foot Fly was often most preferred despite requiring more exertion than the Controller Fly technique.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489945
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Chiang, Chi-huan; Hsu, Shuo-wen; Yang, Chih-Yun; Huang, Da-Yuan; Chen, Bing-Yu
TI  - SUI - TanGo: Exploring Expressive Tangible Interactions on Head-Mounted Displays
PY  - 2020
AB  - We present TanGo, an always-available input modality on VR headset, which can be complementary to current VR accessories. TanGO is an active mechanical structure symmetrically equipped on Head-Mounted Display, enabling 3-dimensional bimanual sliding input with each degree of freedom furnished a brake system driven by micro servo generating totally 6 passive resistive force profiles. TanGo is an all-in-one structure that possess rich input and output while keeping compactness with the trade-offs between size, weight and usability. Users can actively gesture like pushing, shearing, or squeezing with specific output provided while allowing hands to rest in stationary experiences. TanGo also renders users flexibility to switch seamlessly between virtual and real usage in Augmented Reality without additional efforts and instruments. We demonstrate three applications to show the interaction space of TanGo and then discuss its limitation and show future possibilities based on preliminary user feedback.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418457
ER  - 

TY  - NA
AU  - Santolucito, Mark
TI  - FARM@ICFP - Human-in-the-loop program synthesis for live coding
PY  - 2021
AB  - Live Coding is a creative coding practice, where the act of programming itself constitutes a performance. The code written during a Live Coding performance often generates media, for example a continuous stream of music or video. One of the challenges of Live Coding is in finding a balance in the language design, such that the language is both expressive enough for the artist, as well as simple enough to be programmed in real-time. In order to reduce the overhead of manually coding every part of a Live Coding performance, we propose a tool for Live Coding that leverages program synthesis to simplify the process. Program synthesis retains the "show your code" ethos of Live Coding performances, while also lowering the barrier to entry to the performance practice.
SP  - 47
EP  - 53
JF  - Proceedings of the 9th ACM SIGPLAN International Workshop on Functional Art, Music, Modelling, and Design
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3471872.3472972
ER  - 

TY  - JOUR
AU  - Majeed, Abdul; Zhang, Xiaohan
TI  - On the Adoption of Modern Technologies to Fight the COVID-19 Pandemic: A Technical Synthesis of Latest Developments
PY  - 2023
AB  - <jats:p>In the ongoing COVID-19 pandemic, digital technologies have played a vital role to minimize the spread of COVID-19, and to control its pitfalls for the general public. Without such technologies, bringing the pandemic under control would have been tricky and slow. Consequently, exploration of pandemic status, and devising appropriate mitigation strategies would also be difficult. In this paper, we present a comprehensive analysis of community-beneficial digital technologies that were employed to fight the COVID-19 pandemic. Specifically, we demonstrate the practical applications of ten major digital technologies that have effectively served mankind in different ways during the pandemic crisis. We have chosen these technologies based on their technical significance and large-scale adoption in the COVID-19 arena. The selected technologies are the Internet of Things (IoT), artificial intelligence(AI), natural language processing(NLP), computer vision (CV), blockchain (BC), federated learning (FL), robotics, tiny machine learning (TinyML), edge computing (EC), and synthetic data (SD). For each technology, we demonstrate the working mechanism, technical applications in the context of COVID-19, and major challenges from the perspective of COVID-19. Our analysis can pave the way to understanding the roles of these digital COVID-19-fighting technologies that can be used to fight future infectious diseases to prevent global crises. Moreover, we discuss heterogeneous data that have significantly contributed to addressing multiple aspects of the ongoing pandemic when fed to the aforementioned technologies. To the best of the authors’ knowledge, this is a pioneering work on community-beneficial and transformative technologies in the context of COVID-19 with broader coverage of studies and applications.</jats:p>
SP  - 90
EP  - 123
JF  - COVID
VL  - 3
IS  - 1
PB  - 
DO  - 10.3390/covid3010006
ER  - 

TY  - NA
AU  - Jakubovic, Joel; Petricek, Tomas
TI  - Ascending the Ladder to Self-Sustainability: Achieving Open Evolution in an Interactive Graphical System
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3563835.3568736
ER  - 

TY  - NA
AU  - Fehr, Daniel; Sassenburg, Renske; Blunschi, Jacqueline; Lay-Ekuakille, Aime; Massaro, Alessandro; Bonmarin, Mathias; Spano, Fabrizio
TI  - MeMeA - A Capacitive Color-Changing Electronic Skin for Touch Sensing Applications
PY  - 2021
AB  - Robots are slowly becoming part of our civilization, or at least one of the main evolutions of the third millennium. Nowadays their integration is based on their aspects by looking more and more human. Additionally, not only considering the psychological aspects, our society will have to improve their interaction. Systems integrating a full spectrum of sensors will have to be implemented. In this framework, as a preliminary step, the implementation of a tactile robotic skin can be an interesting upgrade. To guarantee safety between robots and humans, it can be interesting to implement such robots with human-like tactile perception. In this work, we focus on the realization of innovative tactile skin model. This model allows to sense and indicate where the pressures have been applied by using a combination of a flexible polymeric capacitive skin model combined with a LED matrix.
SP  - 1
EP  - 6
JF  - 2021 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/memea52024.2021.9478674
ER  - 

TY  - NA
AU  - Scargill, Timothy; Chen, Ying; Eom, Sangjun; Dunn, Jessilyn; Gorlatova, Maria
TI  - Environmental, User, and Social Context-Aware Augmented Reality for Supporting Personal Development and Change
PY  - 2022
AB  - Robust pervasive context-aware augmented reality (AR) has the potential to enable a range of applications that support users in reaching their personal and professional goals. In such applications, AR can be used to deliver richer, more immersive, and more timely just in time adaptive interventions (JITAI) than conventional mo-bile solutions, leading to more effective support of the user. This position paper defines a research agenda centered on improving AR applications&#x0027; environmental, user, and social context awareness. Specifically, we argue for two key architectural approaches that will allow pushing AR context awareness to the next level: use of wearable and Internet of Things (IoT) devices as additional data streams that complement the data captured by the AR devices, and the development of edge computing-based mechanisms for enriching existing scene understanding and simultaneous localization and mapping (SLAM) algorithms. The paper outlines a collection of specific research directions in the development of such architectures and in the design of next-generation environmental, user, and social context awareness algorithms.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00042
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Li, Pengyu; Nith, Romain; Fonseca, Joshua; Lopes, Pedro
TI  - CHI - Touch&Fold: A Foldable Haptic Actuator for Rendering Touch in Mixed Reality
PY  - 2021
AB  - We propose a nail-mounted foldable haptic device that provides tactile feedback to mixed reality (MR) environments by pressing against the user's fingerpad when a user touches a virtual object. What is novel in our device is that it quickly tucks away when the user interacts with real-world objects. Its design allows it to fold back on top of the user's nail when not in use, keeping the user's fingerpad free to, for instance, manipulate handheld tools and other objects while in MR. To achieve this, we engineered a wireless and self-contained haptic device, which measures 24×24×41 mm and weighs 9.5 g. Furthermore, our foldable end-effector also features a linear resonant actuator, allowing it to render not only touch contacts (i.e., pressure) but also textures (i.e., vibrations). We demonstrate how our device renders contacts with MR surfaces, buttons, low- and high-frequency textures. In our first user study, we found that participants perceived our device to be more realistic than a previous haptic device that also leaves the fingerpad free (i.e., fingernail vibration). In our second user study, we investigated the participants’ experience while using our device in a real-world task that involved physical objects. We found that our device allowed participants to use the same finger to manipulate handheld tools, small objects, and even feel textures and liquids, without much hindrance to their dexterity, while feeling haptic feedback when touching MR interfaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445099
ER  - 

TY  - NA
AU  - Tahara, Tomu; Seno, Takashi; Narita, Gaku; Ishikawa, Tomoya
TI  - ISMAR Adjunct - Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph
PY  - 2020
AB  - We present Retargetable AR—a novel AR framework that yields an AR experience that is aware of scene contexts set in various real environments, achieving natural interaction between the virtual and real worlds. We characterize scene contexts with relationships among objects in 3D space. A context assumed by an AR content and a context formed by a real environment where users experience AR are represented as abstract graph representations, i.e. scene graphs. From RGB-D streams, our framework generates a volumetric map in which geometric and semantic information of a scene are integrated. Moreover, using the semantic map, we abstract scene objects as oriented bounding boxes and estimate their orientations. Then our framework constructs, in an online fashion, a 3D scene graph characterizing the context of a real environment for AR. The correspondence between the constructed graph and an AR scene graph denoting the context of AR content provides a semantically registered content arrangement, which facilitates natural interaction between the virtual and real worlds. We performed extensive evaluations on our prototype system through quantitative evaluation of the performance of the oriented bounding box estimation, subjective evaluation of the AR content arrangement based on constructed 3D scene graphs, and an online AR demonstration.
SP  - 249
EP  - 255
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct51615.2020.00072
ER  - 

TY  - NA
AU  - Chung, Michael Jae-Yoon; Cakmak, Maya
TI  - Authoring Human Simulators via Probabilistic Functional Reactive Program Synthesis
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/hri53351.2022.9889630
ER  - 

TY  - NA
AU  - Wang, Yu-Wei; Lin, Yu-Hsin; Ku, Pin-Sung; Miyatake, Yōko; Mao, Yi-Hsuan; Chen, Po Yu; Tseng, Chun-Miao; Chen, Mike Y.
TI  - CHI - JetController: High-speed Ungrounded 3-DoF Force Feedback Controllers using Air Propulsion Jets
PY  - 2021
AB  - JetController is a novel haptic technology capable of supporting high-speed and persistent 3-DoF ungrounded force feedback. It uses high-speed pneumatic solenoid valves to modulate compressed air to achieve 20-50Hz of full impulses at 4.0-1.0N, and combines multiple air propulsion jets to generate 3-DoF force feedback. Compared to propeller-based approaches, JetController supports 10-30 times faster impulse frequency, and its handheld device is significantly lighter and more compact. JetController supports a wide range of haptic events in games and VR experiences, from firing automatic weapons in games like Halo (15Hz) to slicing fruits in Fruit Ninja (up to 45Hz). To evaluate JetController, we integrated our prototype with two popular VR games, Half-life: Alyx and Beat Saber, to support a variety of 3D interactions. Study results showed that JetController significantly improved realism, enjoyment, and overall experience compared to commercial vibrating controllers, and was preferred by most participants.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445549
ER  - 

TY  - NA
AU  - Rossmy, Beat; Wiethoff, Alexander
TI  - Musical Grid Interfaces: Past, Present, and Future Directions
PY  - NA
AB  - This paper examines grid interfaces which are currently used in many musical devices and instruments. This type of interface concept has been rooted in the NIME community since the early 2000s. We provide an overview of research projects and commercial products and conducted an expert interview as well as an online survey. In summary this work shares: (1) an overview on grid controller research, (2) a set of three usability issues deduced by a multi method approach, and (3) an evaluation of user perceptions regarding persistent usability issues and common reasons for the use of grid interfaces.
SP  - NA
EP  - NA
JF  - NIME 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.21428/92fbeb44.6a2451e6
ER  - 

TY  - NA
AU  - Liu, Kao-Hua; Sasaki, Tomoya; Kajihara, Hiroyuki; Hiyama, Atsushi; Inami, Masahiko; Chen, Chien-Hsu
TI  - Designing WindCage- Unpacking the Thinking and Prototyping a Propeller-Based Haptic Unit
PY  - 2022
AB  - The topic of Augmented Reality Enabling Superhuman Sports (ARES) highlights the necessity of applying tangible game objects to achieve rich haptic feedback and realistic interactions. In this paper, we propose WindCage, a tetrakaidecahedron shape propeller-based haptic unit, and its two usages to provide different ways to experience &#x201C;holding the wind.&#x201D; Users are required to execute physical movements and activities during the Superhuman Sports and Serious Game (SSSG) content. However, it is foreseeable that users&#x0027; physical and cognitive condition varies. Therefore, we iteratively developed the system according to the Inclusive Design methodology to enable the maximum number of users with different abilities to adopt it in various scenarios. We aim to utilize the results to contribute insights into designing game objects in the Superhuman Sports and Serious Game community.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00038
ER  - 

TY  - JOUR
AU  - Di Gioia, Francesco Riccardo; Brasier, Eugenie; Pietriga, Emmanuel; Appert, Caroline
TI  - Investigating the Use of AR Glasses for Content Annotation on Mobile Devices
PY  - 2022
AB  - <jats:p>Mobile devices such as smartphones and tablets have limited display size and input capabilities that make a variety of tasks challenging. Coupling the mobile device with Augmented Reality eyewear such as smartglasses can help address some of these challenges. In the specific context of digital content annotation tasks, this combination has the potential to enhance the user experience on two fronts. First, annotations can be offloaded into the air around the mobile device, freeing precious screen real-estate. Second, as smartglasses often come equipped with a variety of sensors including a camera, users can annotate documents with pictures or videos of their environment, captured on the spot, hands-free, and from the wearer's perspective. We present AnnotAR, a prototype that we use as a research probe to assess the viability of this approach to digital content annotation. We use AnnotAR to gather users' preliminary feedback in a laboratory setting, and to showcase how it could support real-world use cases.</jats:p>
SP  - 430
EP  - 447
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567727
ER  - 

TY  - JOUR
AU  - Ujitoko, Yusuke; Ban, Yuki
TI  - Survey of Pseudo-haptics: Haptic Feedback Design and Application Proposals
PY  - 2021
AB  - In the last two decades, the design of pseudo-haptics as a haptic presentation method that does not require a mechanical feedback device has been proposed in various research papers. Moreover, applications using pseudo-haptics have been proposed and evaluated in various contexts. However, the findings from these studies have not yet been comprehensively organized in a survey paper in the recent times. In this paper, findings from a series of individual prior studies were summarized from the design through to the application proposals. First, we summarize visual stimuli designs based on the target haptic object properties to induce pseudo-haptics. Second, we summarize two special issues when designing pseudo-haptics; (1) workaround design for the visualized mismatch of visual stimuli and user input and (2) the combination design of pseudo-haptics and physical stimuli. Third, application proposals that use pseudo-haptics for training, assistance, and entertainment are presented. This survey paper would help not only researchers in academia but also application developers who intend to use pseudo-haptics as a haptic presentation method.
SP  - 1
EP  - 1
JF  - IEEE transactions on haptics
VL  - 14
IS  - 4
PB  - 
DO  - 10.1109/toh.2021.3077619
ER  - 

TY  - JOUR
AU  - Almahmoud, Jumana; Jahanbakhsh, Farnaz; Facciotti, Marc; Igo, Michele; Sripathi, Kamali; Gal, Kobi; Karger, David
TI  - Spotlights: Designs for Directing Learners' Attention in a Large-Scale Social Annotation Platform
PY  - 2022
AB  - <jats:p>A new approach to online discussion, which situates student discussions in the margins of the course content, can enhance student engagement with course materials. However, in high-enrollment classes, the large number of comments can overwhelm and intimidate students. Some become frustrated by the volume of potential online interactions and by a perceived lack of immediate relevance to their studies. Likewise, instructors are disappointed when outstanding discussions, that they deem valuable for all to see, get lost in the clutter.</jats:p> <jats:p>To address these challenges, we propose visual spotlighting mechanisms for increasing the saliency of selected comments. We piloted and deployed multiple designs in two high-enrollment biology courses at a large public university in the United States. Interviews, surveys, and a controlled experiment show that spotlighting relevant comments in heavily annotated texts positively affects students' engagement, measured in terms of their attention to comments, and their reported sense of validation and pride. Students also reported their preferences for certain spotlighting designs.</jats:p>
SP  - 1
EP  - 36
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555598
ER  - 

TY  - JOUR
AU  - Jeon, Sang-Bin; Kwon, Soon-Uk; Hwang, June-Young; Cho, Yong-Hun; Kim, Hayeon; Park, Jinhyung; Lee, In-Kwon
TI  - Dynamic optimal space partitioning for redirected walking in multi-user environment
PY  - 2022
AB  - <jats:p>In multi-user Redirected Walking (RDW), the space subdivision method divides a shared physical space into sub-spaces and allocates a sub-space to each user. While this approach has the advantage of precluding any collisions between users, the conventional space subdivision method suffers from frequent boundary resets due to the reduction of available space per user. To address this challenge, in this study, we propose a space subdivision method called Optimal Space Partitioning (OSP) that dynamically divides the shared physical space in real-time. By exploiting spatial information of the physical and virtual environment, OSP predicts the movement of users and divides the shared physical space into optimal sub-spaces separated with shutters. Our OSP framework is trained using deep reinforcement learning to allocate optimal sub-space to each user and provide optimal steering. Our experiments demonstrate that OSP provides higher sense of immersion to users by minimizing the total number of reset counts, while preserving the advantage of the existing space subdivision strategy: ensuring better safety to users by completely eliminating the possibility of any collisions between users beforehand. Our project is available at https://github.com/AppleParfait/OSP-Archive.</jats:p>
SP  - 1
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 4
PB  - 
DO  - 10.1145/3528223.3530113
ER  - 

TY  - JOUR
AU  - Liang, Chen; Yu, Chun; Qin, Yue; Wang, Yuntao; Shi, Yuanchun
TI  - DualRing: Enabling Subtle and Expressive Hand Interaction with Dual IMU Rings
PY  - 2021
AB  - We present DualRing, a novel ring-form input device that can capture the state and movement of the user's hand and fingers. With two IMU rings attached to the user's thumb and index finger, DualRing can sense not only the absolute hand gesture relative to the ground but also the relative pose and movement among hand segments. To enable natural thumb-to-finger interaction, we develop a high-frequency AC circuit for on-body contact detection. Based on the sensing information of DualRing, we outline the interaction space and divide it into three sub-spaces: within-hand interaction, hand-to-surface interaction, and hand-to-object interaction. By analyzing the accuracy and performance of our system, we demonstrate the informational advantage of DualRing in sensing comprehensive hand gestures compared with single-ring-based solutions. Through the user study, we discovered the interaction space enabled by DualRing is favored by users for its usability, efficiency, and novelty.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478114
ER  - 

TY  - NA
AU  - Adams, Jonathan; Murphy, Erin; Sutor, John; Dodd, Ava
TI  - Assessing the Qualities of Synthetic Visual Data Production
PY  - 2021
AB  - A literature review was conducted using journal articles and conference proceedings to examine emerging research practices, and applications of synthetic visual data over the past 5 years. The current research examined articles related to research trends in artificial intelligence training intended to improve computer vision and object detection. Search strings were developed and used to retrieve research articles from the ACM and IEEE databases. The resulting articles were examined for trends, general practices, disciplines where the greatest efforts have been made, advances, and relevant production processes. The research reveals that visual synthetic data encompasses filtering, augmentation, and object domain randomization techniques. Further, all of the research that included an evaluation of synthetic visual data suggest that there are noteworthy performance improvements in accuracy. Additionally, producing realistic synthetic data reduces the current limitations related to labeling, image quality, paucity of relevant data, and privacy issues.
SP  - NA
EP  - NA
JF  - 2021 9th International Conference on Information and Education Technology (ICIET)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iciet51873.2021.9419586
ER  - 

TY  - NA
AU  - Greenspan, Ben; Danielescu, Andreea
TI  - UIST (Adjunct Volume) - Designing Low-Cost Sports Prosthetics with Advanced 3D Printing Techniques
PY  - 2020
AB  - Prosthetic limbs are an extension of the human body and should allow for the same function and control that a natural limb provides. Most prosthetics aim to allow an individual to return to a baseline quality of life, but few allow the individual to return to sports. Sport prosthetics can be difficult to produce due to the specificity of the designs and associated costs. Some prosthetics are becoming more accessible with the availability of 3D-printing. In this work, we create a prototype basketball prosthetic hand to enable a more natural shot. This prototype uses 3D printed springs for energy return and 3D printed electronics for integrated strain sensing that can be mapped to haptic feedback for the user. Combining these technologies will lead to a sport prosthetic limb whose technology advancements can be applied to future prosthetic designs.
SP  - 126
EP  - 128
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416145
ER  - 

TY  - NA
AU  - Chung, Michael Jae-Yoon; Nakura, Mino; Neti, Sai Harshita; Lu, Anthony Y. H.; Hummel, Elana; Cakmak, Maya
TI  - RO-MAN - ConCodeIt! A Comparison of Concurrency Interfaces in Block-Based Visual Robot Programming
PY  - 2020
AB  - Concurrency makes robot programming challenging even for professional programmers, yet it is essential for rich, interactive social robot behaviors. Visual programming aims to lower the barrier for robot programming but does not support rich concurrent behavior for meaningful robotics applications. In this paper, we explore extensions to block-based visual languages to enable programming of concurrent behavior with (1) asynchronous procedure calls, which encourage imperative programming, (2) callbacks, which encourage event-driven programming, and (3) promise, which also encourages imperative programming by providing event synchronization utilities. We compare these approaches through a systematic analysis of social robot programs with representative concurrency patterns, as well as a user study (N=23) in which participants authored such programs. Our work identifies characteristic differences between these approaches and demonstrates that the promise-based concurrency interface enables more concise programs with fewer errors.
SP  - 245
EP  - 252
JF  - 2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ro-man47096.2020.9223337
ER  - 

TY  - JOUR
AU  - Reimer, Dennis; Podkosova, Iana; Scherzer, Daniel; Kaufmann, Hannes
TI  - Colocation for SLAM-Tracked VR Headsets with Hand Tracking
PY  - 2021
AB  - In colocated multi-user Virtual Reality applications, relative user positions in the virtual environment need to match their relative positions in the physical tracking space. A mismatch between virtual and real relative user positions might lead to harmful events such as physical user collisions. This paper examines three calibration methods that enable colocated Virtual Reality scenarios for SLAM-tracked head-mounted displays without the need for an external tracking system. Two of these methods—fixed-point calibration and marked-based calibration—have been described in previous research; the third method that uses hand tracking capabilities of head-mounted displays is novel. We evaluated the accuracy of these three methods in an experimental procedure with two colocated Oculus Quest devices. The results of the evaluation show that our novel hand tracking-based calibration method provides better accuracy and consistency while at the same time being easy to execute. The paper further discusses the potential of all evaluated calibration methods.
SP  - 58
EP  - NA
JF  - Computers
VL  - 10
IS  - 5
PB  - 
DO  - 10.3390/computers10050058
ER  - 

TY  - NA
AU  - Jiang, Weiwei; Wang, Chaofan; Sarsenbayeva, Zhanna; Irlitti, Andrew; Knibbe, Jarrod; Dingler, Tilman; Goncalves, Jorge; Kostakos, Vassilis
TI  - InfoPrint: Embedding Information into 3D Printed Objects.
PY  - 2021
AB  - We present a technique to embed information invisible to the eye inside 3D printed objects. The information is integrated in the object model, and then fabricated using off-the-shelf dual-head FDM (Fused Deposition Modeling) 3D printers. Our process does not require human intervention during or after printing with the integrated model. The information can be arbitrary symbols, such as icons, text,binary, or handwriting. To retrieve the information, we evaluate two different infrared-based imaging devices that are readily available-thermal cameras and near-infrared scanners. Based on our results, we propose design guidelines for a range of use cases to embed and extract hidden information. We demonstrate how our method can be used for different applications, such as interactive thermal displays, hidden board game tokens, tagging functional printed objects, and autographing non-fungible fabrication work.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Colley, Mark; Jansen, Pascal; Rukzio, Enrico; Gugenheimer, Jan
TI  - SwiVR-Car-Seat
PY  - 2021
AB  - <jats:p>Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494968
ER  - 

TY  - JOUR
AU  - Mallalieu, A.; Hajali, T.; Isaksson, O.; Panarotto, M.
TI  - The Role of Digital Infrastructure for the Industrialisation of Design for Additive Manufacturing
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>The use of Additive Manufacturing (AM) can bring opportunities for industry, but several challenges need to be addressed, specifically the digital infrastructure comprising the AM value chain. A combination of a systematic literature review and an industrial use case study concludes that there is low consideration of the digital infrastructure in Design for Additive Manufacturing (DfAM) methods and tools which has a negative impact on the industrialisation of AM. It is therefore recommended that further studies are to be made on how to manage the digital infrastructure in DfAM processes.</jats:p>
SP  - 1401
EP  - 1410
JF  - Proceedings of the Design Society
VL  - 2
IS  - NA
PB  - 
DO  - 10.1017/pds.2022.142
ER  - 

TY  - JOUR
AU  - Pfeuffer, Ken; Abdrabou, Yasmeen; Esteves, Augusto; Rivu, Radiah; Abdelrahman, Yomna; Meitner, Stefanie; Saadi, Amr; Alt, Florian
TI  - ARtention: A design space for gaze-adaptive user interfaces in augmented reality
PY  - 2021
AB  - NA
SP  - 1
EP  - 12
JF  - Computers & Graphics
VL  - 95
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.01.001
ER  - 

TY  - BOOK
AU  - Chhaglani, Bhawana; Anand, Abhay Sheel; Garg, Nakul; Ashok, Ashwin
TI  - LIOT@MOBICOM - Evaluating LED-camera communication for drones
PY  - 2020
AB  - This work explores the idea of using LED-camera communication for drone communication. In particular, this research positions the idea of using optical wireless links using LED transmitters and camera/image sensor receivers to communicate between ground and drones and between drones. While the concept opens an opportunity for a complementary line-of-sight (LOS) technology to radio frequency (RF) wireless to enable drone communication, it also raises fundamental research questions as to the ability to communicate under harsh mobile settings inherent to drones. To this end, in this paper, we present an empirical study of LED-camera communication performance for ground-drone communication under different mobile settings or trajectories of the drone. Through a bit-error-rate (BER) metric based evaluation of the performance, we evaluate the quality of drone-ground uplink and downlink LED-communication under real-world mobile conditions. Through insights from the BER evaluation, we highlight the fundamental challenges to be addressed that limit the practicality of drone visible light communication (VLC).
SP  - 18
EP  - 23
JF  - Proceedings of the Workshop on Light Up the IoT
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3412449.3412889
ER  - 

TY  - NA
AU  - Lu, Qian; Darnal, Aryabhat; Takahashi, Haruki; Muliana, Anastasia Hanifah; Kim, Jeeeun
TI  - User-Centered Property Adjustment with Programmable Filament
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519864
ER  - 

TY  - NA
AU  - Shi, Rongkai; Zhu, Nan; Liang, Hai-Ning; Zhao, Shengdong
TI  - ISMAR - Exploring Head-based Mode-Switching in Virtual Reality
PY  - 2021
AB  - Mode-switching supports multilevel operations using a limited number of input methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches for mode-switching use buttons, controllers, and users’ hands. However, they are inefficient and challenging to do with tasks that require both hands (e.g., when users need to use two hands during drawing operations). Using head gestures for mode-switching can be an efficient and cost-effective way, allowing for a more continuous and smooth transition between modes. In this paper, we explore the use of head gestures for mode-switching especially in scenarios when both users’ hands are performing tasks. We present a first user study that evaluated eight head gestures that could be suitable for VR HMD with a dual-hand line-drawing task. Results show that move forward, move backward, roll left, and roll right led to better performance and are preferred by participants. A second study integrating these four gestures in Tilt Brush, an open-source painting VR application, is conducted to further explore the applicability of these gestures and derive insights. Results show that Tilt Brush with head gestures allowed users to change modes with ease and led to improved interaction and user experience. The paper ends with a discussion on some design recommendations for using head-based mode-switching in VR HMD.
SP  - 118
EP  - 127
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00026
ER  - 

TY  - JOUR
AU  - Lindlbauer, David
TI  - The future of mixed reality is adaptive
PY  - 2022
AB  - <jats:p>In a future where we replace our smartphones and notebooks with mixed reality headsets, the way we create user interfaces will change drastically. Future interfaces will need to adapt automatically to users' context, guided by optimization-based methods and machine learning, to become beneficial for end-users.</jats:p>
SP  - 26
EP  - 31
JF  - XRDS: Crossroads, The ACM Magazine for Students
VL  - 29
IS  - 1
PB  - 
DO  - 10.1145/3558191
ER  - 

TY  - NA
AU  - Li, Haodong; Rosete, Sonali; Coyle, Jeremy; Phillips, Rachael V.; Hejazi, Nima S.; Malenica, Ivana; Arnold, Benjamin F.; Benjamin-Chung, Jade; Mertens, Andrew; Colford, John M.; van der Laan, Mark J.; Hubbard, Alan
TI  - Evaluating the Robustness of Targeted Maximum Likelihood Estimators via Realistic Simulations in Nutrition Intervention Trials
PY  - 2021
AB  - Several recently developed methods have the potential to harness machine learning in the pursuit of target quantities inspired by causal inference, including inverse weighting, doubly robust estimating equations and substitution estimators like targeted maximum likelihood estimation. There are even more recent augmentations of these procedures that can increase robustness, by adding a layer of cross-validation (cross-validated targeted maximum likelihood estimation and double machine learning, as applied to substitution and estimating equation approaches, respectively). While these methods have been evaluated individually on simulated and experimental data sets, a comprehensive analysis of their performance across ``real-world'' simulations have yet to be conducted. In this work, we benchmark multiple widely used methods for estimation of the average treatment effect using ten different nutrition intervention studies data. A realistic set of simulations, based on a novel method, highly adaptive lasso, for estimating the data-generating distribution that guarantees a certain level of complexity (undersmoothing) is used to better mimic the complexity of the true data-generating distribution. We have applied this novel method for estimating the data-generating distribution by individual study and to subsequently use these fits to simulate data and estimate treatment effects parameters as well as their standard errors and resulting confidence intervals. Based on the analytic results, a general recommendation is put forth for use of the cross-validated variants of both substitution and estimating equation estimators. We conclude that the additional layer of cross-validation helps in avoiding unintentional over-fitting of nuisance parameter functionals and leads to more robust inferences.
SP  - NA
EP  - NA
JF  - arXiv: Methodology
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Warner, Jeremy
TI  - UIST (Adjunct Volume) - Visual Design Reuse Through Style Recognition and Transfer
PY  - 2021
AB  - This work aims to transfer design attributes and styles within and across visual documents such as slides, graphic designs, and non-photorealistic renderings. Consistent style across elements is a hallmark of good graphic design. Many visual stylistic design patterns exist throughout visualizations, presentations, and interactive media experiences (games, visual novels). These patterns often exist in visual style guides, brand guides, and concept art. However, except for structured document layouts (e.g., HTML/CSS), design tools often do not enforce consistent style decisions or must be manually maintained. Synchronizing style guides and designs require significant effort, discouraging exploration and the mixing of new ideas. This work introduces algorithms that recognize implicit patterns and structures in visual documents along with interfaces that let designers operate on these patterns, specifically, to view and apply design changes across pattern instances flexibly. The key benefits of visual redesign through implicit patterns are: (1) removing any dependence on upfront formal style declarations, (2) enabling extraction and distribution of implicit visual patterns, and (3) facilitating the exploration of novel visual design concepts through the mixing of styles.
SP  - 175
EP  - 178
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3477591
ER  - 

TY  - NA
AU  - Cheng, Tingyu; Park, Jung Wook; Li, Jiachen; Ramey, Charles; Lin, Hongnan; Abowd, Gregory D.; Brum Medeiros, Carolina; Oh, HyunJoo; Giordano, Marcello
TI  - PITAS: Sensing and Actuating Embedded Robotic Sheet for Physical Information Communication
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517532
ER  - 

TY  - NA
AU  - Bonilla Fominaya, Angelica M.; Chew, Rong Kang (Ron); Komar, Matthew L.; Lo, Jeremia; Slabakis, Alexandra; Sun, Ningjing (Anita); Zhang, Yunyi (Joyce); Lindlbauer, David
TI  - MoonBuddy: A Voice-based Augmented Reality User Interface That Supports Astronauts During Extravehicular Activities
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558690
ER  - 

TY  - NA
AU  - Meier, Manuel; Streli, Paul; Fender, Andreas; Holz, Christian
TI  - VR - TaplD: Rapid Touch Interaction in Virtual Reality using Wearable Sensing
PY  - 2021
AB  - Current Virtual Reality systems typically use cameras to capture user input from controllers or free-hand mid-air interaction. In this paper, we argue that this is a key impediment to productivity scenarios in VR, which require continued interaction over prolonged periods of time-a requirement that controller or free-hand input in mid-air does not satisfy. To address this challenge, we bring rapid touch interaction on surfaces to Virtual Reality-the input modality that users have grown used to on phones and tablets for continued use. We present TapID, a wrist-based inertial sensing system that complements headset-tracked hand poses to trigger input in VR. TapID embeds a pair of inertial sensors in a flexible strap, one at either side of the wrist; from the combination of registered signals, TapID reliably detects surface touch events and, more importantly, identifies the finger used for touch. We evaluated TapID in a series of user studies on event-detection accuracy (F1 = 0.997) and hand-agnostic finger-identification accuracy (within-user: F1 = 0.93; across users: F1 = 0.91 after 10 refinement taps and F1 = 0.87 without refinement) in a seated table scenario. We conclude with a series of applications that complement hand tracking with touch input and that are uniquely enabled by TapID, including UI control, rapid keyboard typing and piano playing, as well as surface gestures.
SP  - 519
EP  - 528
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00076
ER  - 

TY  - CHAP
AU  - Singh, Abbey; Peachey, Matthew; Kaur, Ramanpreet; Haltner, Peter; Frederick, Shannon; Alnusayri, Mohammed; Manco, David Choco; Morris, Colton; Brownlee, Shannon; Malloch, Joseph; Reilly, Derek
TI  - Supporting Spatial Thinking in Augmented Reality Narrative: A Field Study
PY  - 2022
AB  - NA
SP  - 270
EP  - 291
JF  - Interactive Storytelling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-22298-6_17
ER  - 

TY  - NA
AU  - Li, Jiannan; Lyu, Jiahe; Sousa, Maurício; Balakrishnan, Ravin; Tang, Anthony; Grossman, Tovi
TI  - UIST - Route Tapestries: Navigating 360° Virtual Tour Videos Using Slit-Scan Visualizations
PY  - 2021
AB  - An increasingly popular way of experiencing remote places is by viewing 360° virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360° video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating 360°virtual tour videos.
SP  - 223
EP  - 238
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474746
ER  - 

TY  - BOOK
AU  - Mthunzi, Everett Mondliwethu; Echtler, Florian
TI  - ISS Companion - Artefact: A UML-based framework for model-driven development of interactive surface prototypes
PY  - 2021
AB  - While interactive surface prototypes may be highly application-specific, existing prototypes hint at common, recurring design considerations. Given the rapid accumulation of near-identical prototypes, there is a need to promote design reuse. In this context, existing research prototypes motivate abstracting generic structures, architectural views, and descriptions to inform future designs. This paper proposes Artefact: a UML-based framework for model-driven development of interactive surface prototypes. We define flexible base models using existing research prototypes: initial hardware and middleware abstractions to support developers in the early design stages. For validation, we use the proposed framework to capture existing research prototypes. We then conduct an interview study to learn expert perceptions towards the captured model representations. Our initial findings highlight three significant benefits: (1) an accessible graphical syntax with unambiguous model representation, (2) a system for capturing arbitrary technical specifications, and (3) flexible model representation with consistent notation. While we can not draw any absolute conclusions, initial results suggest benefits in the model-driven approach.
SP  - 16
EP  - 20
JF  - Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447932.3490523
ER  - 

TY  - JOUR
AU  - Park, Chaeyong; Kim, Jin-Soo; Choi, Seungmoon
TI  - Length Perception Model for Handheld Controllers: The Effects of Diameter and Inertia
PY  - 2021
AB  - Typical handheld controllers for interaction in virtual reality (VR) have fixed shapes and sizes, regardless of what visual objects they represent. Resolving this crossmodal incongruence with a shape-changing interface is our long-term goal. In this paper, we seek to find a length perception model that considers the moment of inertia (MOI) and diameter of a handheld object based on the concept of dynamic touch. Such models serve as a basis for computational algorithms for shape changing. We carried out two perceptual experiments. In Experiment 1, we measured the perceived lengths of 24 physical objects with different MOIs and diameters. Then we obtained a length perception model to reproduce the desired perceived length with a handheld controller. In Experiment 2, we validated our model in a crossmodal matching scenario, where a visual rod was matched to a haptic rod in terms of the perceived length. Our results contribute to understanding the relationship between the perceived length and physical properties of a handheld object and designing shape-changing algorithms to render equivalent visual and haptic sensory cues for length perception in VR.
SP  - 310
EP  - 315
JF  - IEEE transactions on haptics
VL  - 14
IS  - 2
PB  - 
DO  - 10.1109/toh.2021.3077709
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Braud, Tristan; Zhou, Pengyuan; Wang, Lin; Xu, Dianlei; Lin, Zijun; Kumar, Abhishek; Bermejo, Carlos; Hui, Pan
TI  - All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda
PY  - 2021
AB  - Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse.
SP  - NA
EP  - NA
JF  - arXiv: Computers and Society
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ko, Donghyeon; Bin Yim, Jee; Lee, Yujin; Pyun, Jaehoon; Lee, Woohun
TI  - CHI - Designing Metamaterial Cells to Enrich Thermoforming 3D Printed Object for Post-Print Modification
PY  - 2021
AB  - In this paper, we present a metamaterial structure called thermoformable cells, TF-Cells, to enrich thermoforming for post-print modification. So far, thermoforming is limitedly applied for modifying a 3D printed object due to its low thermal conductivity. TF-Cells consists of beam arrays that affluently pass hot air and have high heat transference. Through heating the embedded TF-Cells of the printed object, users can modify not only the deeper area of the object surface but also its form factor. With a series of technical experiments, we investigated TF-Cells’ thermoformability, depending on their structure’s parameters, orientations, and heating conditions. Next, we present a series of compound cells consisting of TF-Cells and solid structure to adjust stiffness or reduce undesirable shape deformation. Adapting the results from the experiments, we built a simple tool for embedding TF-Cells into a 3D model. Using the tool, we implemented examples under contexts of mechanical fitting, ergonomic fitting, and aesthetic tuning.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445229
ER  - 

TY  - NA
AU  - Chen, Linfeng; Takashima, Kazuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - CHI - PinpointFly: An Egocentric Position-control Drone Interface using Mobile AR
PY  - 2021
AB  - Accurate drone positioning is challenging because pilots only have a limited position and direction perception of a flying drone from their perspective. This makes conventional joystick-based speed control inaccurate and more complicated and significantly degrades piloting performance. We propose PinpointFly, an egocentric drone interface that allows pilots to arbitrarily position and rotate a drone using position-control direct interactions on a see-through mobile AR where the drone position and direction are visualized with a virtual cast shadow (i.e., the drone’s orthogonal projection onto the floor). Pilots can point to the next position or draw the drone’s flight trajectory by manipulating the virtual cast shadow and the direction/height slider bar on the touchscreen. We design and implement a prototype of PinpointFly for indoor and visual line of sight scenarios, which are comprised of real-time and predefined motion-control techniques. We conduct two user studies with simple positioning and inspection tasks. Our results demonstrate that PinpointFly makes the drone positioning and inspection operations faster, more accurate, simpler and fewer workload than a conventional joystick interface with a speed-control method.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445110
ER  - 

TY  - JOUR
AU  - Yau, Yui-Pan; Lee, Lik Hang; Li, Zheng; Braud, Tristan; Ho, Yi-Hsuan; Hui, Pan
TI  - How Subtle Can It Get?: A Trimodal Study of Ring-sized Interfaces for One-Handed Drone Control
PY  - 2020
AB  - Flying drones have become common objects in our daily lives, serving a multitude of purposes. Many of these purposes involve outdoor scenarios where the user combines drone control with another activity. Traditional interaction methods rely on physical or virtual joysticks that occupy both hands, thus restricting drone usability. In this paper, we investigate one-handed human-to-drone-interaction by leveraging three modalities: force, touch, and IMU. After prototyping three different combinations of these modalities on a smartphone, we evaluate them against the current commercial standard through two user experiments. These experiments help us to find the combination of modalities that strikes a compromise between user performance, perceived task load, wrist rotation, and interaction area size. Accordingly, we select a method that achieves faster task completion times than the two-handed commercial baseline by 16.54% with the merits of subtle user behaviours inside a small-size ring-form device and implements this method within the ring-form device. The last experiment involving 12 participants shows that thanks to its small size and weight, the ring device displays better performance than the same method implemented on a mobile phone. Furthermore, users unanimously found the device useful for controlling a drone in mobile scenarios (AVG = 3.92/5), easy to use (AVG = 3.58/5) and easy to learn (AVG = 3.58/5). Our findings give significant design clues in search of subtle and effective interaction through finger augmentation devices with drone control. The users with our prototypical system and a multi-modal on-finger device can control a drone with subtle wrist rotation (pitch gestures: 43.24° amplitude and roll gestures: 46.35° amplitude) and unnoticeable thumb presses within a miniature-sized area of (1.08 * 0.61 cm2).
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397319
ER  - 

TY  - JOUR
AU  - Palma, Gianpaolo; Perry, Sara; Cignoni, Paolo
TI  - Augmented Virtuality Using Touch-Sensitive 3D-Printed Objects
PY  - 2021
AB  - Virtual reality (VR) technologies have become more and more affordable and popular in the last five years thanks to hardware and software advancements. A critical issue for these technologies is finding paradigms that allow user interactions in ways that are as similar as possible to the real world, bringing physicality into the experience. Current literature has shown, with different experiments, that the mapping of real objects in virtual reality alongside haptic feedback significantly increases the realism of the experience and user engagement, leading to augmented virtuality. In this paper, we present a system to improve engagement in a VR experience using inexpensive, physical, and sensorized copies of real artefacts made with cheap 3D fabrication technologies. Based on a combination of hardware and software components, the proposed system gives the user the possibility to interact with the physical replica in the virtual environment and to see the appearance of the original cultural heritage artefact. In this way, we overcome one of the main limitations of mainstream 3D fabrication technologies: a faithful appearance reproduction. Using a consumer device for the real-time hand tracking and a custom electronic controller for the capacitive touch sensing, the system permits the creation of augmented experiences where the user with their hands can change the virtual appearance of the real replica object using a set of personalization actions selectable from a physical 3D-printed palette.
SP  - 2186
EP  - NA
JF  - Remote Sensing
VL  - 13
IS  - 11
PB  - 
DO  - 10.3390/rs13112186
ER  - 

TY  - NA
AU  - Achberger, Alexander; Heyen, Frank; Vidakovic, Kresimir; Sedlmair, Michael
TI  - VINCI - PropellerHand: A Hand-Mounted, Propeller-Based Force Feedback Device
PY  - 2021
AB  - Immersive analytics is a fast growing field that is often applied in virtual reality (VR). VR environments often lack immersion due to missing sensory feedback when interacting with data. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our final version through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality.
SP  - NA
EP  - NA
JF  - The 14th International Symposium on Visual Information Communication and Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3481549.3481563
ER  - 

TY  - NA
AU  - Liu, Jen-Shuo; Wang, Portia; Tversky, Barbara; Feiner, Steven
TI  - Adaptive Visual Cues for Guiding a Bimanual Unordered Task in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00059
ER  - 

TY  - BOOK
AU  - Skjøtt, Jonathan
TI  - Programming - Spreadsheets as notational environment for paper weaving
PY  - 2020
AB  - This work explores how spreadsheets can serve as the foundation for the creation of a hybrid design medium for paper weaving. The case for using spreadsheets as a model for the development of environments for designing paper weaving patterns is made. Firstly, because both spreadsheets as well as the woven form of paper weaving designs have cells as their elementary substrate. Secondly, because it allows for exploration of the design space. And thirdly, as it enables use by end-users with varying levels of experience. An environment, VisiWeave, implementing the points mentioned is presented and explored.
SP  - 158
EP  - 162
JF  - Conference Companion of the 4th International Conference on Art, Science, and Engineering of Programming
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3397537.3397545
ER  - 

TY  - NA
AU  - Huang, Gaoping; Qian, Xun; Wang, Tianyi; Patel, Fagun; Sreeram, Maitreya; Cao, Yuanzhi; Ramani, Karthik; Quinn, Alexander J.
TI  - CHI - AdapTutAR: An Adaptive Tutoring System for Machine Tasks in Augmented Reality
PY  - 2021
AB  - Modern manufacturing processes are in a state of flux, as they adapt to increasing demand for flexible and self-configuring production. This poses challenges for training workers to rapidly master new machine operations and processes, i.e. machine tasks. Conventional in-person training is effective but requires time and effort of experts for each worker trained and not scalable. Recorded tutorials, such as video-based or augmented reality (AR), permit more efficient scaling. However, unlike in-person tutoring, existing recorded tutorials lack the ability to adapt to workers’ diverse experiences and learning behaviors. We present AdapTutAR, an adaptive task tutoring system that enables experts to record machine task tutorials via embodied demonstration and train learners with different AR tutoring contents adapting to each user’s characteristics. The adaptation is achieved by continually monitoring learners’ tutorial-following status and adjusting the tutoring content on-the-fly and in-situ. The results of our user study evaluation have demonstrated that our adaptive system is more effective and preferable than the non-adaptive one.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445283
ER  - 

TY  - CHAP
AU  - Choi, Wonhyuk; Vazirani, Michel; Santolucito, Mark
TI  - Program Synthesis for Musicians: A Usability Testbed for Temporal Logic Specifications
PY  - 2021
AB  - In recent years, program synthesis research has made significant progress in creating user-friendly tools for Programming by example (PBE) and Programming by demonstration (PBD) environments. However, program synthesis from logical specifications, such as reactive synthesis, still faces large challenges in widespread adoption. In order to bring reactive synthesis to a wider audience, more research is necessary to explore different interface options. We present The SynthSynthesizer, a music-based tool for designing and testing specification interfaces. The tool enables researchers to prototype different interfaces for reactive synthesis and run user studies on them. The tool is accessible to both researchers and users by running on a browser on top of a docker-containerized synthesis toolchain. We show sample implementations with the tool by creating dropdown interfaces, and by running a user study with 21 users.
SP  - 47
EP  - 61
JF  - Programming Languages and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-89051-3_4
ER  - 

TY  - NA
AU  - Wang, Tianyi; Qian, Xun; He, Fengming; Ramani, Karthik
TI  - CHI Extended Abstracts - LightPaintAR: Assist Light Painting Photography with Augmented Reality
PY  - 2021
AB  - Light painting photos are created by moving light sources in mid-air while taking a long exposure photo. However, it is challenging for novice users to leave accurate light traces without any spatial guidance. Therefore, we present LightPaintAR, a novel interface that leverages augmented reality (AR) traces as a spatial reference to enable precise movement of the light sources. LightPaintAR allows users to draft, edit, and adjust virtual light traces in AR, and move light sources along the AR traces to generate accurate light traces on photos. With LightPaintAR, users can light paint complex patterns with multiple strokes and colors. We evaluate the effectiveness and the usability of our system with a user study and showcase multiple light paintings created by the users. Further, we discuss future improvements of LightPaintAR.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451672
ER  - 

TY  - NA
AU  - Forman, Jack; Dogan, Mustafa Doga; Forsythe, Hamilton; Ishii, Hiroshi
TI  - UIST - DefeXtiles: 3D Printing Quasi-Woven Fabric via Under-Extrusion
PY  - 2020
AB  - We present DefeXtiles, a rapid and low-cost technique to produce tulle-like fabrics on unmodified fused deposition modeling (FDM) printers. The under-extrusion of filament is a common cause of print failure, resulting in objects with periodic gap defects. In this paper, we demonstrate that these defects can be finely controlled to quickly print thinner, more flexible textiles than previous approaches allow. Our approach allows hierarchical control from micrometer structure to decameter form and is compatible with all common 3D printing materials. In this paper, we introduce the mechanism of DefeXtiles, establish the design space through a set of primitives with detailed workflows, and characterize the mechanical properties of DefeXtiles printed with multiple materials and parameters. Finally, we demonstrate the interactive features and new use cases of our approach through a variety of applications, such as fashion design prototyping, interactive objects, aesthetic patterning, and single-print actuators.
SP  - 1222
EP  - 1233
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415876
ER  - 

TY  - NA
AU  - Lerner, Sorin
TI  - UIST - Focused Live Programming with Loop Seeds
PY  - 2020
AB  - Live programming is a paradigm in which the programmer can visualize the runtime values of the program each time the program changes. The promise of live programming depends on using test cases to run the program and thereby provide these runtime values. In this paper we show that in some situations test cases are insufficient in a fundamental way, in that there are no test inputs that can drive certain incomplete loops to produce useful data, a problem we call the loop-datavoid problem. The problem stems from the fact that useful data inside the loop might only be produced after the loop has been fully written. To solve this problem, we propose a paradigm called Focused Live Programming with Loop Seeds, in which the programmer provides hypothetical values to start a loop iteration, and then the programming environment focuses the live visualization on this hypothetical loop iteration. We introduce the loop-datavoid problem, present our proposed solution, explain it in detail, and then present the results of a user study.
SP  - 607
EP  - 613
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415834
ER  - 

TY  - NA
AU  - Shi, Yilei; Zhang, Haimo; Cao, Jiashuo; Nanayakkara, Suranga
TI  - AHs - VersaTouch: A Versatile Plug-and-Play System that Enables Touch Interactions on Everyday Passive Surfaces
PY  - 2020
AB  - We present VersaTouch, a portable, plug-and-play system that uses active acoustic sensing to track fine-grained touch locations as well as touch force of multiple fingers on everyday surfaces without having to permanently instrument them or do extensive calibration. Our system is versatile in multiple aspects. First, with simple calibration, VersaTouch can be arranged in arbitrary layouts in order to fit into crowded surfaces while retaining its accuracy. Second, various modalities of touch input, such as distance and position, can be supported depending on the number of sensors used to suit the interaction scenario. Third, VersaTouch can sense multi-finger touch, touch force, as well as identify the touch source. Last, VersaTouch is capable of providing vibrotactile feedback to fingertips through the same actuators used for touch sensing. We conducted a series of studies and demonstrated that VersaTouch was able to track finger touch using various layouts with average error from 9.62mm to 14.25mm on different surfaces within a circular area of 400mm diameter centred around the sensors, as well as detect touch force. Finally, we discuss the interaction design space and interaction techniques enabled by VersaTouch.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384778
ER  - 

TY  - NA
AU  - O'Leary, Jasper Tran; Lee, Khang; Peek, Nadya
TI  - CHI Extended Abstracts - A Grammar of Digital Fabrication Machines
PY  - 2021
AB  - Digital fabrication tools for makers have increased access to manufacturing processes such as 3D printing and computer-controlled laser cutting or milling. Despite research advances in novel hardware and software tools for fabrication tasks, there is no formal way to reason about the fabrication machine itself. There is no standard format for representing the high-level features of machines and trade-offs between them; instead, this important information is relegated to folk knowledge. To make machine information explicit, we present Taxon, a machine specification language broad enough to represent many machines, while also allowing for enough expressivity to meaningfully compare and infer performance. We describe and detail the motivation behind the design of Taxon, as well as how Taxon programs compile to a simulation of physical machines. We discuss opportunities for future work in digital fabrication that requires a standard, formalized representation of machines.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451829
ER  - 

TY  - NA
AU  - Afsar, Ozgun Kilic; Shtarbanov, Ali; Mor, Hila; Nakagaki, Ken; Forman, Jack; Modrei, Karen; Jeong, Seung Hee; Hjort, Klas; Höök, Kristina; Ishii, Hiroshi
TI  - UIST - OmniFiber: Integrated Fluidic Fiber Actuators for Weaving Movement based Interactions into the ‘Fabric of Everyday Life’
PY  - 2021
AB  - Fiber – a primitive yet ubiquitous form of material – intertwines with our bodies and surroundings, from constructing our fibrous muscles that enable our movement, to forming fabrics that intimately interface with our skin. In soft robotics and advanced materials science research, actuated fibers are gaining interest as thin, flexible materials that can morph in response to external stimuli. In this paper, we build on fluidic artificial muscles research to develop OmniFiber - a soft, line-based material system for designing movement-based interactions. We devised actuated thin (oouter
SP  - 1010
EP  - 1026
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474802
ER  - 

TY  - NA
AU  - Zhu, Zhengzhe; Liu, Ziyi; Wang, Tianyi; Zhang, Youyou; Qian, Xun; Raja, Pashin Farsak; Villanueva, Ana; Ramani, Karthik
TI  - MechARspace: An Authoring System Enabling Bidirectional Binding of Augmented Reality with Toys in Real-time
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545668
ER  - 

TY  - NA
AU  - Cascaval, Dan; Shalah, Mira; Quinn, Phillip; Bodik, Rastislav; Agrawala, Maneesh; Schulz, Adriana
TI  - Differentiable 3D CAD Programs for Bidirectional Editing
PY  - 2021
AB  - Modern CAD tools represent 3D designs not only as geometry, but also as a program composed of geometric operations, each of which depends on a set of parameters. Program representations enable meaningful and controlled shape variations via parameter changes. However, achieving desired modifications solely through parameter editing is challenging when CAD models have not been explicitly authored to expose select degrees of freedom in advance. We introduce a novel bidirectional editing system for 3D CAD programs. In addition to editing the CAD program, users can directly manipulate 3D geometry and our system infers parameter updates to keep both representations in sync. We formulate inverse edits as a set of constrained optimization objectives, returning plausible updates to program parameters that both match user intent and maintain program validity. Our approach implements an automatically differentiable domain-specific language for CAD programs, providing derivatives for this optimization to be performed quickly on any expressed program. Our system enables rapid, interactive exploration of a constrained 3D design space by allowing users to manipulate the program and geometry interchangeably during design iteration. While our approach is not designed to optimize across changes in geometric topology, we show it is expressive and performant enough for users to produce a diverse set of design variants, even when the CAD program contains a large number of parameters.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Li, Haodong; Rosete, Sonali; Coyle, Jeremy; Phillips, Rachael V; Hejazi, Nima S; Malenica, Ivana; Arnold, Benjamin F; Benjamin-Chung, Jade; Mertens, Andrew; Colford, John M; van der Laan, Mark J; Hubbard, Alan E
TI  - Evaluating the robustness of targeted maximum likelihood estimators via realistic simulations in nutrition intervention trials.
PY  - 2022
AB  - Several recently developed methods have the potential to harness machine learning in the pursuit of target quantities inspired by causal inference, including inverse weighting, doubly robust estimating equations and substitution estimators like targeted maximum likelihood estimation. There are even more recent augmentations of these procedures that can increase robustness, by adding a layer of cross-validation (cross-validated targeted maximum likelihood estimation and double machine learning, as applied to substitution and estimating equation approaches, respectively). While these methods have been evaluated individually on simulated and experimental data sets, a comprehensive analysis of their performance across real data based simulations have yet to be conducted. In this work, we benchmark multiple widely used methods for estimation of the average treatment effect using ten different nutrition intervention studies data. A nonparametric regression method, undersmoothed highly adaptive lasso, is used to generate the simulated distribution which preserves important features from the observed data and reproduces a set of true target parameters. For each simulated data, we apply the methods above to estimate the average treatment effects as well as their standard errors and resulting confidence intervals. Based on the analytic results, a general recommendation is put forth for use of the cross-validated variants of both substitution and estimating equation estimators. We conclude that the additional layer of cross-validation helps in avoiding unintentional over-fitting of nuisance parameter functionals and leads to more robust inferences.
SP  - 2132
EP  - 2165
JF  - Statistics in medicine
VL  - 41
IS  - 12
PB  - 
DO  - 10.1002/sim.9348
ER  - 

TY  - JOUR
AU  - Oulasvirta, Antti; Dayama, Niraj Ramesh; Shiripour, Morteza; John, Maximilian; Karrenbauer, Andreas
TI  - Combinatorial Optimization of Graphical User Interface Designs
PY  - 2020
AB  - The graphical user interface (GUI) has become the prime means for interacting with computing systems. It leverages human perceptual and motor capabilities for elementary tasks such as command exploration and invocation, information search, and multitasking. For designing a GUI, numerous interconnected decisions must be made such that the outcome strikes a balance between human factors and technical objectives. Normally, design choices are specified manually and coded within the software by professional designers and developers. This article surveys combinatorial optimization as a flexible and powerful tool for computational generation and adaptation of GUIs. As recently as 15 years ago, applications were limited to keyboards and widget layouts. The obstacle has been the mathematical definition of design tasks, on the one hand, and the lack of objective functions that capture essential aspects of human behavior, on the other. This article presents definitions of layout design problems as integer programming tasks, a coherent formalism that permits identification of problem types, analysis of their complexity, and exploitation of known algorithmic solutions. It then surveys advances in formulating evaluative functions for common design-goal foci such as user performance and experience. The convergence of these two advances has expanded the range of solvable problems. Approaches to practical deployment are outlined with a wide spectrum of applications. This article concludes by discussing the position of this application area within optimization and human–computer interaction research and outlines challenges for future work.
SP  - 434
EP  - 464
JF  - Proceedings of the IEEE
VL  - 108
IS  - 3
PB  - 
DO  - 10.1109/jproc.2020.2969687
ER  - 

TY  - NA
AU  - Cao, Jacky; Lam, Kit-Yung; Lee, Lik Hang; Liu, Xiaoli; Hui, Pan; Su, Xiang
TI  - Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence.
PY  - 2021
AB  - Mobile Augmented Reality (MAR) integrates computer-generated virtual objects with physical environments for mobile devices. MAR systems enable users to interact with MAR devices, such as smartphones and head-worn wearables, and performs seamless transitions from the physical world to a mixed world with digital entities. These MAR systems support user experiences by using MAR devices to provide universal accessibility to digital contents. Over the past 20 years, a number of MAR systems have been developed, however, the studies and design of MAR frameworks have not yet been systematically reviewed from the perspective of user-centric design. This article presents the first effort of surveying existing MAR frameworks (count: 37) and further discusses the latest studies on MAR through a top-down approach: 1) MAR applications; 2) MAR visualisation techniques adaptive to user mobility and contexts; 3) systematic evaluation of MAR frameworks including supported platforms and corresponding features such as tracking, feature extraction plus sensing capabilities; and 4) underlying machine learning approaches supporting intelligent operations within MAR systems. Finally, we summarise the development of emerging research fields, current state-of-the-art, and discuss the important open challenges and possible theoretical and technical directions. This survey aims to benefit both researchers and MAR system developers alike.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Yang, Munseok; Yamaoka, Junichi
TI  - MultiJam: Fabricating Jamming User Interface using Multi-material 3D Printing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505565
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Tsai, Chieh; Liao, Yu-So; Chiang, Yi-Ting; Zhang, Zhong-Yi
TI  - FingerX: Rendering Haptic Shapes of Virtual Objects Augmented by Real Objects using Extendable and Withdrawable Supports on Fingers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517489
ER  - 

TY  - NA
AU  - Zhang, Zhuoming; Alvina, Jessalyn; Détienne, Françoise; Lecolinet, Eric
TI  - Pulling, Pressing, and Sensing with In-Flat: Transparent Touch Overlay for Smartphones
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3531073.3531111
ER  - 

TY  - NA
AU  - Liu, Guanhong; Xu, Haiqing; Ding, Xianghua(Sharon); Gao, Mingyue; Li, Bowen; Ruan, Fushen; Mi, Haipeng
TI  - "It Puts Life into My Creations": Understanding Fluid Fiber as a Media for Expressive Display
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501990
ER  - 

TY  - NA
AU  - Lin, Stephen Shiao-ru; Gamage, Nisal Menuka; Herath, Kithmini; Withana, Anusha
TI  - MyoSpring: 3D Printing Mechanomyographic Sensors for Subtle Finger Gesture Recognition
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501321
ER  - 

TY  - NA
AU  - Higuchi, Keita; Sano, Shotaro; Igarashi, Takeo
TI  - Conference on Designing Interactive Systems - Interactive Hyperparameter Optimization with Paintable Timelines
PY  - 2021
AB  - We propose a method to integrate more interactivity into automatic hyperparameter optimization systems to leverage the user’s prior knowledge on parameter distribution. In our method, the user continuously observes automatic optimization’s progress and dynamically specifies where to search in the parameter space. We present a prototype implementation of an interactive dashboard for an optimizer to show our method’s feasibility. The interactive dashboard’s main feature is “paintable timeline” where the user can not only observe the past parameter values tested as in standard timeline but also specify the range of future parameters to be tested with simple painting operations. We show three examples where user intervention might improve the performance of automatic optimizations. We run a user study with experts and the results show that, with prior knowledge about parameter distribution of the target problem, interactive optimization can reach better results compared to fully automatic optimization.
SP  - 1518
EP  - 1528
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462077
ER  - 

TY  - JOUR
AU  - Hoppe, Matthias; Oskina, Daria; Schmidt, Albrecht; Kosch, Thomas
TI  - Odin's Helmet: A Head-Worn Haptic Feedback Device to Simulate G-Forces on the Human Body in Virtual Reality
PY  - 2021
AB  - Virtual Reality (VR) experiences have massively improved in the mediation of feedback. However, the simulation of forces is still limited. This paper presents Odin's Helmet, a head-worn device to simulate g-forces that act on the human head in real-life situations. Odin's Helmet uses four head-mounted propellers as actuators to simulate g-forces through pushing and pulling the user's head while being immersed in VR. Odin's Helmet's goal is to increase presence and manipulate the user's perception of the otolith organ in the vestibular system. The user's perception will be tricked to experience a sensation of self-movement in VR. A technical evaluation shows Odin's Helmet's applicability to apply perceivable g-forces to the user's head. We conclude with future use cases of Odin's Helmet, such as redirected walking by controlling the user's head orientation, attention guidance, and wind simulations through Odin's Helmet.
SP  - 1
EP  - 15
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3461734
ER  - 

TY  - NA
AU  - Choi, Kyung Yun; Ishii, Hiroshi
TI  - TEI - Therms-Up!: DIY Inflatables and Interactive Materials by Upcycling Wasted Thermoplastic Bags
PY  - 2021
AB  - We introduce a DIY method of creating inflatables and prototyping interactive materials from wasted thermoplastic bags that easily found at home. We used a inexpensive FFF 3D printer, without any customization of the printer, to heat-seal and patterning different types of mono and multilayered thermoplastic bags. We characterized 8 different types of commonly-used product package’s plastic film which are mostly made of polypropylene and polyethylene, and provided 3D printer settings for re-purposing each material. In addition to heat-sealing, we explored a new design space of using a 3D printer to create embossing, origami creases, and textures on thermoplastic bags, and demonstrate examples of applying this technique to create various materials for rapid design and prototyping. To validate the durability of the inflatables, we evaluated 9 different thermoplastic air pouches’ heat-sealed bonding strength. Lastly, we show use-case scenarios of prototyping products and interface, and creating playful experience at home.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3442457
ER  - 

TY  - NA
AU  - Lerner, Sorin
TI  - CHI - Projection Boxes: On-the-fly Reconfigurable Visualization for Live Programming
PY  - 2020
AB  - Live programming is a regime in which the programming environment provides continual feedback, most often in the form of runtime values. In this paper, we present Projection Boxes, a novel visualization technique for displaying runtime values of programs. The key idea behind projection boxes is to start with a full semantics of the program, and then use projections to pick a subset of the semantics to display. By varying the projection used, projection boxes can encode both previously known visualization techniques, and also new ones. As such, projection boxes provide an expressive and configurable framework for displaying runtime information. Through a user study we demonstrate that (1) users find projection boxes and their configurability useful (2) users are not distracted by the always-on visualization (3) a key driving force behind the need for a configurable visualization for live programming lies with the wide variation in programmer preferences.
SP  - 1
EP  - 7
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376494
ER  - 

TY  - NA
AU  - Röddiger, Tobias; Beigl, Michael; Wolffram, Daniel; Budde, Matthias; Sun, Hongye
TI  - AHs - PDMSkin: On-Skin Gestures with Printable Ultra-Stretchable Soft Electronic Second Skin
PY  - 2020
AB  - Innovative enabling technologies are key drivers of human augmentation. In this paper, we explore a new, conductive, and configurable material made from Polydimethylsiloxane (PDMS) that is capillary doped with silver particles (Ag) using an immiscible secondary fluid to build ultra-stretchable, soft electronics. Bonding silver particles directly with PDMS enables inherently stretchable Ag-PDMS circuits. Compared to previous work, the reduced silver consumption creates significant advantages, e.g., better stretchability and lower costs. The secondary fluid ensures self-assembling conductivity networks. Sensors are 3D-printed ultra-thin (200%. Therefore, printed circuits can attach tightly onto the body. Due to biocompatibility, devices can be implanted (e.g., open wounds treatment). We present a proof of concept on-skin interface that uses the new material to provide six distinct input gestures. Our quantitative evaluation with ten participants shows that we can successfully classify the gestures with a low spatial-resolution circuit. With few training data and a gradient boosting classifier, we yield 83% overall accuracy. Our qualitative material study with twelve participants shows that usability and comfort are well perceived; however, the smooth but easy to adapt surface does not feel tissue-equivalent. For future work, the new material will likely serve to build robust and skin-like electronics.
SP  - 28
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384789
ER  - 

TY  - NA
AU  - Lao, Cheryl; Xia, Haijun; Wigdor, Daniel; Chevalier, Fanny
TI  - SUI - Attribute Spaces: Supporting Design Space Exploration in Virtual Reality
PY  - 2021
AB  - Exploring the design space of configurations for objects in virtual scenes is a challenge within virtual reality authoring tools due to the lack of visualization capabilities, non-destructive operations, suggestions, and flexibility. This work introduces Attribute Spaces, tools for visualizing and manipulating object attributes in virtual reality during 3D content generation. Attribute Spaces enable designers to systematically explore design spaces by supporting rapid comparisons between design alternatives and offering design suggestions. Custom combinations of attributes can be grouped and manipulated simultaneously for several objects. The grouping supports the creation of custom operation combinations that can be used as tools to edit multiple attribute, as well as snapshots of promising design decisions for later review. In an evaluation of Attribute Spaces by 3D design experts, our approach was found to enhance users’ understanding of their design space exploration progress and showed promise for integration into existing 3D workflows.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485290
ER  - 

TY  - JOUR
AU  - Lorenz, Michal
TI  - Anotační praktiky vědců: analýza kognitivní práce
PY  - 2021
AB  - Ucel – Anotacni praktiky vědců představuji jeden ze zakladnich prvků vědecke prace napřic vědnimi obory. Ve srovnani s praktikami cteni a psani textů velmi malo prozkouman. Studie je založena na formativnim paradigmatu designu HCI a sociokognitivnim paradigmatu informacniho chovani. Ve výzkumne studii představujeme výsledky zkoumani anotacnich praktik vědců pracujicich s tistěnými vědeckými texty. Výzkum zaměřujeme na interakce clověka s textem (Human–Text Interaction) s cilem popsat kognitivni praci vědců při anotovani a identifikovat jeji omezeni. Výzkumu se zucastnilo celkem 20 vědců. Zastupuji humanitni, spolecenske a přirodni vědy (lingvistiku, psychologii, geografii) a interdisciplinarni výzkum s různým stupněm odbornosti – novacci (doktorandi), pokrocili (postgradualni výzkumnici), odbornici (docenti a profesoři). Kognitivni prace byla analyzovana při anotovani vědeckých clanků, monografii a diplomových praci. Design/metodologie/přistup – Výzkum byl proveden metodou analýzy kognitivni prace. Design výzkumu byl přirozený – vědci cetli ve svem přirozenem prostředi texty dle vlastniho výběru s ukolem vytvařet si anotace pro svoji dalsi praci. Každý typ textu cetli 15 minut. Během interakce s textem nahlas mluvili o sve cinnosti. Ziskane zvukove a obrazove zaznamy byly zpracovany pomoci analýzy verbalnich protokolů a analýzy abstraktni hierarchie. Výsledky – Celkem bylo pomoci abstraktni hierarchie analyzovano 829 zakladnich anotacnich elementů odpovidajicich jednoduchemu aktu anotovani. Podle typu a mista interakce s textem byla vytvořena typologie anotaci s 13 kategoriemi. Analýza anotacnich praktik odhalila vzorce spoluvýskytu anotacnich elementů, některe z nich se vyskytuji pravidelně spolecně v jedne kombinaci. Akty anotovani se vyskytuji buď izolovaně, nebo se skladaji do souborů na sebe navazujicich cinnosti, ktere jsou stopami prohlubujiciho se porozuměni textu. Vedle aktů a cinnosti anotovani se vyskytuji take anotacni praktiky oznacovane jako metaanotace, ktere jsou anotacemi k již vytvořeným anotacim, a makroanotace uspořadavajici anotace napřic textem a sloužici orientacni funkci v textu. Ziskane výsledky kognitivni prace při anotovani naznacuji topologicke napěti mezi kognitivnimi funkcemi a nastroji pro psani poznamek, což ma důsledky pro design nastrojů pro cteni a poznamkovani elektronických odborných textů. Originalita/hodnota – Přispěvek přinasi vhled do anotacnich praktik vědců při interakci s odborným textem založený na empirickem výzkumu. Zjistěni lze využit při designovani anotacnich nastrojů elektronických textů. Design anotacniho editoru by měl zabranit narusovani kognitivnich procesů a soustředěni vědců přeorientovanim jejich pozornosti od textu k výběru anotacniho nastroje pro realizaci anotacniho aktu. Takový design zajisti přimou interakci clověka s textem misto interakce s anotacnim editorem.
SP  - NA
EP  - NA
JF  - ProInflow
VL  - 13
IS  - 1
PB  - 
DO  - 10.5817/proin2021-1-2
ER  - 

TY  - NA
AU  - Daskalogrigorakis, Grigoris; McNamara, Ann; Marinakis, Angelos; Antoniadis, Aristomenis; Mania, Katerina
TI  - Glance-Box: Multi-LOD Glanceable Interfaces for Machine Shop Guidance in Augmented Reality using Blink and Hand Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00070
ER  - 

TY  - JOUR
AU  - Digumarti, Krishna Manaswi; Gosden, Daniel; Le, Nguyen Hao; Rossiter, Jonathan
TI  - Toward Stimuli-Responsive Soft Robots with 3D Printed Self-Healing Konjac Glucomannan Gels
PY  - 2022
AB  - Significant progress in fabricating new multifunctional soft materials and the advances of additive manufacturing technologies have given birth to a new generation of soft robots with complex capab...
SP  - 425
EP  - 434
JF  - 3D Printing and Additive Manufacturing
VL  - 9
IS  - 5
PB  - 
DO  - 10.1089/3dp.2020.0289
ER  - 

TY  - JOUR
AU  - Steinbuss, Georg; Böhm, Klemens
TI  - Benchmarking Unsupervised Outlier Detection with Realistic Synthetic Data
PY  - 2021
AB  - Benchmarking unsupervised outlier detection is difficult. Outliers are rare, and existing benchmark data contains outliers with various and unknown characteristics. Fully synthetic data usually consists of outliers and regular instances with clear characteristics and thus allows for a more meaningful evaluation of detection methods in principle. Nonetheless, there have only been few attempts to include synthetic data in benchmarks for outlier detection. This might be due to the imprecise notion of outliers or to the difficulty to arrive at a good coverage of different domains with synthetic data. In this work, we propose a generic process for the generation of datasets for such benchmarking. The core idea is to reconstruct regular instances from existing real-world benchmark data while generating outliers so that they exhibit insightful characteristics. We propose and describe a generic process for the benchmarking of unsupervised outlier detection, as sketched so far. We then describe three instantiations of this generic process that generate outliers with specific characteristics, like local outliers. To validate our process, we perform a benchmark with state-of-the-art detection methods and carry out experiments to study the quality of data reconstructed in this way. Next to showcasing the workflow, this confirms the usefulness of our proposed process. In particular, our process yields regular instances close to the ones from real data. Summing up, we propose and validate a new and practical process for the benchmarking of unsupervised outlier detection.
SP  - 3441453
EP  - 20
JF  - ACM Transactions on Knowledge Discovery from Data
VL  - 15
IS  - 4
PB  - 
DO  - 10.1145/3441453
ER  - 

TY  - NA
AU  - Subramonyam, Hariharan; Seifert, Colleen M.; Adar, Eytan
TI  - Towards A Process Model for Co-Creating AI Experiences.
PY  - 2021
AB  - Thinking of technology as a design material is appealing. It encourages designers to explore the material's properties to understand its capabilities and limitations, a prerequisite to generative design thinking. However, as a material, AI resists this approach because its properties emerge as part of the design process itself. Therefore, designers and AI engineers must collaborate in new ways to create both the material and its application experience. We investigate the co-creation process through a design study with 10 pairs of designers and engineers. We find that design 'probes' with user data are a useful tool in defining AI materials. Through data probes, designers construct designerly representations of the envisioned AI experience (AIX) to identify desirable AI characteristics. Data probes facilitate divergent thinking, material testing, and design validation. Based on our findings, we propose a process model for co-creating AIX and offer design considerations for incorporating data probes in design tools.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chi, Peggy; Frey, Nathan; Panovich, Katrina; Essa, Irfan
TI  - UIST - Automatic Instructional Video Creation from a Markdown-Formatted Tutorial
PY  - 2021
AB  - We introduce HowToCut, an automatic approach that converts a Markdown-formatted tutorial into an interactive video that presents the visual instructions with a synthesized voiceover for narration. HowToCut extracts instructional content from a multimedia document that describes a step-by-step procedure. Our method selects and converts text instructions to a voiceover. It makes automatic editing decisions to align the narration with edited visual assets, including step images, videos, and text overlays. We derive our video editing strategies from an analysis of 125 web tutorials and apply Computer Vision techniques to the assets. To enable viewers to interactively navigate the tutorial, HowToCut’s conversational UI presents instructions in multiple formats upon user commands. We evaluated our automatically-generated video tutorials through user studies (N=20) and validated the video quality via an online survey (N=93). The evaluation shows that our method was able to effectively create informative and useful instructional videos from a web tutorial document for both reviewing and following.
SP  - 677
EP  - 690
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474778
ER  - 

TY  - NA
AU  - Kolvenbag, Jay; Bruns, Miguel; Winters, Amy
TI  - Rapid Prototyping Dynamic Robotic Fibers for Tunable Movement
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558696
ER  - 

TY  - JOUR
AU  - Li, Changjian; Pan, Hao; Bousseau, Adrien; Mitra, Niloy J.
TI  - Free2CAD
PY  - 2022
AB  - <jats:p>CAD modeling, despite being the industry-standard, remains restricted to usage by skilled practitioners due to two key barriers. First, the user must be able to mentally parse a final shape into a valid sequence of supported CAD commands; and second, the user must be sufficiently conversant with CAD software packages to be able to execute the corresponding CAD commands. As a step towards addressing both these challenges, we present Free2CAD wherein the user can simply sketch the final shape and our system parses the input strokes into a sequence of commands expressed in a simplified CAD language. When executed, these commands reproduce the sketched object. Technically, we cast sketch-based CAD modeling as a sequence-to-sequence translation problem, for which we leverage the powerful Transformers neural network architecture. Given the sequence of pen strokes as input, we introduce the new task of grouping strokes that correspond to individual CAD operations. We combine stroke grouping with geometric fitting of the operation parameters, such that intermediate groups are geometrically corrected before being reused, as context, for subsequent steps in the sequence inference. Although trained on synthetically-generated data, we demonstrate that Free2CAD generalizes to sketches created from real-world CAD models as well as to sketches drawn by novice users.</jats:p> <jats:p> <jats:italic>Code and data are at https://github.com/Enigma-li/Free2CAD.</jats:italic> </jats:p>
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 4
PB  - 
DO  - 10.1145/3528223.3530133
ER  - 

TY  - NA
AU  - Huynh, Brandon; Wysopal, Abby; Ross, Vivian; Orlosky, Jason; Hollerer, Tobias
TI  - Layerable Apps: Comparing Concurrent and Exclusive Display of Augmented Reality Applications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00104
ER  - 

TY  - JOUR
AU  - Hafner, Christian; Bickel, Bernd
TI  - The design space of plane elastic curves
PY  - 2021
AB  - Elastic bending of initially flat slender elements allows the realization and economic fabrication of intriguing curved shapes. In this work, we derive an intuitive but rigorous geometric characterization of the design space of plane elastic rods with variable stiffness. It enables designers to determine which shapes are physically viable with active bending by visual inspection alone. Building on these insights, we propose a method for efficiently designing the geometry of a flat elastic rod that realizes a target equilibrium curve, which only requires solving a linear program. We implement this method in an interactive computational design tool that gives feedback about the feasibility of a design, and computes the geometry of the structural elements necessary to realize it within an instant. The tool also offers an iterative optimization routine that improves the fabricability of a model while modifying it as little as possible. In addition, we use our geometric characterization to derive an algorithm for analyzing and recovering the stability of elastic curves that would otherwise snap out of their unstable equilibrium shapes by buckling. We show the efficacy of our approach by designing and manufacturing several physical models that are assembled from flat elements.
SP  - 1
EP  - 20
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3450626.3459800
ER  - 

TY  - NA
AU  - Lu, Feiyu; Davari, Shakiba; Lisle, Lee; Li, Yuan; Bowman, Doug A.
TI  - VR - Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality
PY  - 2020
AB  - Augmented reality head-worn displays (AR HWDs) have the potential to assist personal computing and the acquisition of everyday information. In this research, we propose Glanceable AR, an interaction paradigm for accessing information in AR HWDs. In Glanceable AR, secondary information resides at the periphery of vision to stay unobtrusive and can be accessed by a quick glance whenever needed. We propose two novel hands-free interfaces: "head-glance", in which virtual contents are fixed to the user’s body and can be accessed by head rotation, and "gaze-summon" in which contents can be "summoned" into central vision by eye-tracked gazing at the periphery. We compared these techniques with a baseline heads-up display (HUD), which we call "eye-glance" interface in two dual-task scenarios. We found that the head-glance and eye-glance interfaces are more preferred and more efficient than the gaze-summon interface for discretionary information access. For a continuous monitoring task, the eye-glance interface was preferred. We discuss the implications of our findings for designing Glanceable AR interfaces in AR HWDs.
SP  - 930
EP  - 939
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581100361198
ER  - 

TY  - JOUR
AU  - Martin, Daniel; Malpica, Sandra; Gutierrez, Diego; Masia, Belen; Serrano, Ana
TI  - Multimodality in VR: A Survey
PY  - 2022
AB  - <jats:p>Virtual reality (VR) is rapidly growing, with the potential to change the way we create and consume content. In VR, users integrate multimodal sensory information they receive to create a unified perception of the virtual world. In this survey, we review the body of work addressing multimodality in VR and its role and benefits in user experience, together with different applications that leverage multimodality in many disciplines. These works thus encompass several fields of research and demonstrate that multimodality plays a fundamental role in VR, enhancing the experience, improving overall performance, and yielding unprecedented abilities in skill and knowledge transfer.</jats:p>
SP  - 1
EP  - 36
JF  - ACM Computing Surveys
VL  - 54
IS  - 10s
PB  - 
DO  - 10.1145/3508361
ER  - 

TY  - JOUR
AU  - Young, Sierra N.; Lanciloti, Ryan J.; Peschel, Joshua M.
TI  - The Effects of Interface Views on Performing Aerial Telemanipulation Tasks Using Small UAVs
PY  - 2021
AB  - NA
SP  - 213
EP  - 228
JF  - International Journal of Social Robotics
VL  - 14
IS  - 1
PB  - 
DO  - 10.1007/s12369-021-00783-9
ER  - 

TY  - NA
AU  - Williams, Jack; Gordon, Andrew D.
TI  - VL/HCC - Where-Provenance for Bidirectional Editing in Spreadsheets
PY  - 2021
AB  - We explore the idea of adding bidirectionality to spreadsheet formulas, so that editing the output can directly affect the input. We introduce portals: a portal is a value paired with its where-provenance, that is, one or more links to its origin. When a portal is the result of a formula in a cell, that cell inherits the capability to edit the locations described by the provenance of the portal. The simplicity of portals makes them amenable to implementation in an existing spreadsheet system. We analyse the list of functions provided by a widely-used commercial spreadsheet system and find that many frequently used functions work with portals with no modification.
SP  - 1
EP  - 10
JF  - 2021 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc51201.2021.9576272
ER  - 

TY  - NA
AU  - Chow, Derrek; Xia, Gracie; Ou, Jasmine
TI  - FormSense: A Fabrication Method to Support Shape Exploration of Interactive Prototypes
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558730
ER  - 

TY  - NA
AU  - Krosnick, Rebecca; Oney, Steve
TI  - VL/HCC - Understanding the Challenges and Needs of Programmers Writing Web Automation Scripts
PY  - 2021
AB  - For web scraping and task automation purposes, programmers write scripts to interact with websites. This is similar to writing end-to-end user interface (UI) test automation suites for software, but on third-party websites that the programmer does not own, introducing new challenges. A programmer might know what semantic operations they want their script to perform, but translating this to code can be difficult. The programmer must investigate the website's internal structure, content, and how UI elements behave, and then write code to click, type, and otherwise interact with UI elements. Many tools and frameworks for creating web automation scripts exist but the challenges programmers face in using them remains understudied. We conducted two studies to study how programmers write web automation scripts. The first study focuses on understanding general challenges. The second focuses on the ways website UI context and script feedback can be helpful. We also provide a set of design findings that detail the kinds of context and feedback developers need while writing web automation scripts.
SP  - 1
EP  - 9
JF  - 2021 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc51201.2021.9576476
ER  - 

TY  - JOUR
AU  - Wirth, M.; Shea, K.; Chen, T.
TI  - 3D-printing textiles: multi-stage mechanical characterization of additively manufactured biaxial weaves
PY  - 2023
AB  - NA
SP  - 111449
EP  - NA
JF  - Materials & Design
VL  - 225
IS  - NA
PB  - 
DO  - 10.1016/j.matdes.2022.111449
ER  - 

TY  - NA
AU  - Degraen, Donald; Piovarči, Michal; Bickel, Bernd; Krüger, Antonio
TI  - UIST - Capturing Tactile Properties of Real Surfaces for Haptic Reproduction
PY  - 2021
AB  - Tactile feedback of an object’s surface enables us to discern its material properties and affordances. This understanding is used in digital fabrication processes by creating objects with high-resolution surface variations to influence a user’s tactile perception. As the design of such surface haptics commonly relies on knowledge from real-life experiences, it is unclear how to adapt this information for digital design methods. In this work, we investigate replicating the haptics of real materials. Using an existing process for capturing an object’s microgeometry, we digitize and reproduce the stable surface information of a set of 15 fabric samples. In a psychophysical experiment, we evaluate the tactile qualities of our set of original samples and their replicas. From our results, we see that direct reproduction of surface variations is able to influence different psychophysical dimensions of the tactile perception of surface textures. While the fabrication process did not preserve all properties, our approach underlines that replication of surface microgeometries benefits fabrication methods in terms of haptic perception by covering a large range of tactile variations. Moreover, by changing the surface structure of a single fabricated material, its material perception can be influenced. We conclude by proposing strategies for capturing and reproducing digitized textures to better resemble the perceived haptics of the originals.
SP  - 954
EP  - 971
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474798
ER  - 

TY  - JOUR
AU  - Li, Yi-Jun; Steinicke, Frank; Wang, Miao
TI  - A Comprehensive Review of Redirected Walking Techniques: Taxonomy, Methods, and Future Directions
PY  - 2022
AB  - NA
SP  - 561
EP  - 583
JF  - Journal of Computer Science and Technology
VL  - 37
IS  - 3
PB  - 
DO  - 10.1007/s11390-022-2266-7
ER  - 

TY  - BOOK
AU  - Broussard, David M; Rahman, Yitoshee; Kulshreshth, Arun K.; Borst, Christoph W.
TI  - VR Workshops - An Interface for Enhanced Teacher Awareness of Student Actions and Attention in a VR Classroom
PY  - 2021
AB  - Networked VR is gaining recognition as a way to provide remote presentations or classes when in-person meetings are difficult or risky to conduct. However, the tools do not provide as many cues about audience actions and attention as in-person meetings, for example, subtle face and body motion cues are missing. Furthermore, the field of view and visual detail are reduced, and there are added problems such as motion sickness, network disconnections, and relatively unrestricted avatar positioning. To help teachers understand and manage students in such an environment, we designed an interface to support teacher awareness of students and their actions, attention, and temperament in a social VR environment. This paper focuses on how different visual cues are integrated into an immersive VR interface that keeps relevant information about students within the teacher’s visual field of attention. Cues include floating indicators, centrally-arranged face icons with gaze information, tethers and other indicators of avatar location, and options to reduce the amount of presented information. We include a pilot study of user preferences for different cue types and their parameters (such as indicator style and placement with respect to the teacher).
SP  - 284
EP  - 290
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00058
ER  - 

TY  - NA
AU  - Chidambaram, Subramanian; Huang, Hank; He, Fengming; Qian, Xun; Villanueva, Ana M.; Redick, Thomas S.; Stuerzlinger, Wolfgang; Ramani, Karthik
TI  - Conference on Designing Interactive Systems - ProcessAR: An augmented reality-based tool to create in-situ procedural 2D/3D AR Instructions
PY  - 2021
AB  - Augmented reality (AR) is an efficient form of delivering spatial information and has great potential for training workers. However, AR is still not widely used for such scenarios due to the technical skills and expertise required to create interactive AR content. We developed ProcessAR, an AR-based system to develop 2D/3D content that captures subject matter expert’s (SMEs) environment-object interactions in situ. The design space for ProcessAR was identified from formative interviews with AR programming experts and SMEs, alongside a comparative design study with SMEs and novice users. To enable smooth workflows, ProcessAR locates and identifies different tools/objects through computer vision within the workspace when the author looks at them. We explored additional features such as embedding 2D videos with detected objects and user-adaptive triggers. A final user evaluation comparing ProcessAR and a baseline AR authoring environment showed that, according to our qualitative questionnaire, users preferred ProcessAR.
SP  - 234
EP  - 249
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462126
ER  - 

TY  - NA
AU  - Ferdowsifard, Kasra; Ordookhanians, Allen; Peleg, Hila; Lerner, Sorin; Polikarpova, Nadia
TI  - UIST - Small-Step Live Programming by Example
PY  - 2020
AB  - Live programming is a paradigm in which the programming environment continually displays runtime values. Program synthesis is a technique that can generate programs or program snippets from examples. \deltextThis paper presents a new programming paradigm called Synthesis-Aided Live Programming that combines these two prior ideas in a synergistic way. When using Synthesis-Aided Live Programming, programmers can change the runtime values displayed by the live \addtextPrevious works that combine the two have taken a holistic approach to the way examples describe the behavior of functions and programs. This paper presents a new programming paradigm called Small-Step Live Programming by Example that lets the user apply Programming by Example locally. When using Small-Step Live Programming by Example, programmers can change the runtime values displayed by the live visualization to generate local program snippets. % Live programming and program % synthesis work perfectly together because the live programming environment % reifies values, which makes it easy for programmers to provide the examples % needed by the synthesizer. We implemented this new paradigm in a tool called \toolname, and performed a user study on $13$ programmers. Our study finds that Small-Step Live Programming by Example with \toolname helps users solve harder problems faster, and that for certain types of queries, users prefer it to searching the web. Additionally, we identify the \usersynthgap, in which users' mental models of the tool do not match its ability, and needs to be taken into account in the design of future synthesis tools.
SP  - 614
EP  - 626
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415869
ER  - 

TY  - JOUR
AU  - Moghaddam, Mohsen Ebrahimi; Wilson, Nicholas; Modestino, Alicia Sasser; Jona, Kemi; Marsella, Stacy
TI  - Exploring augmented reality for worker assistance versus training
PY  - 2021
AB  - NA
SP  - 101410
EP  - NA
JF  - Advanced Engineering Informatics
VL  - 50
IS  - NA
PB  - 
DO  - 10.1016/j.aei.2021.101410
ER  - 

TY  - NA
AU  - Lu, Binghao; Fu, Jirui; Hosseini, Saba M.; Park, Joon-Hyuk
TI  - Modeling and Characterization of 3D Printed Flexible Mesh Structure for Wearable Interface
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/biorob52689.2022.9925405
ER  - 

TY  - JOUR
AU  - Viegas, Fabio; Barbosa, Jorge Luis Victória; Kunst, Rafael; Heckler, Wesllei Felipe
TI  - UFollower: A Model for Smart Cities Based on Ubiquitous Security and Surveillance
PY  - 2021
AB  - Crime is one of the most critical problems in urban centers, especially in large cities. In this sense, technological solutions are needed to provide security to citizens, contributing to the reduction of crime rates. The present work proposes the UFollower model (Ubiquitous Follower) which meets this scenario. The scientific contribution of this work consists of the use of Context Histories and User Profiles for data analysis focused on ubiquitous security. The use of prediction mechanisms and historical contexts allowed to reach up to 70.35% of inference rate for a particular crime (domestic violence). The comparison with the related works indicates that UFollower is the only proposal that presents the issue of public security with context histories and user profile management. The evaluation was conducted through scenarios, allowing to evaluate the related hypotheses. In this sense, a context simulator was built where twenty objects interacted to allow the evolution. Among them, there are people, vehicles, cameras, and wearables.
SP  - 2019
EP  - 2027
JF  - IEEE Latin America Transactions
VL  - 19
IS  - 12
PB  - 
DO  - 10.1109/tla.2021.9480143
ER  - 

TY  - NA
AU  - Günther, Sebastian; Rasch, Julian; Schön, Dominik; Müller, Florian; Schmitz, Martin; Riemann, Jan; Matviienko, Andrii; Mühlhäuser, Max
TI  - Smooth as Steel Wool: Effects of Visual Stimuli on the Haptic Perception of Roughness in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517454
ER  - 

TY  - JOUR
AU  - Biedermann, Manuel; Beutler, Patrick; Meboldt, Mirko
TI  - Automated design of additive manufactured flow components with consideration of overhang constraint
PY  - 2021
AB  - Abstract When designing parts for additive manufacturing (AM), users must consider manufacturing restrictions specific to the chosen AM production technology. For material extrusion or laser powder bed fusion ( L -PBF), users need to follow design rules such as avoiding geometries with critical overhangs. If users create the 3D part geometry manually using computer-aided design (CAD), the consideration of design rules can be challenging and time-consuming. Especially for complex-shaped parts, novice designers may need multiple loops to analyze and modify CAD features for manufacturability. This manual process prevents users to quickly transfer the layout design of a part into the corresponding production-ready 3D part geometry. Therefore, it is highly desirable to automate the manual CAD process, including the consideration of AM restrictions. This work aims to automate the CAD-based design of AM parts with fluid flow channels. For this purpose, the work presents automated procedures that generate and modify design features such that they automatically comply with the AM overhang constraint. The procedures focus on flow components such as hydraulic manifolds and include design features such as support-free flow channels, integrated and sacrificial supports, and other features such as boreholes, interfaces, ribs, and channel branches. To enforce the overhang constraint in the generation of features, users define the part orientation to the build direction, and values for the minimum build angle and maximum allowed diameter of horizontally oriented circular cross-sections. As a benchmark, the work demonstrates the procedures by revisiting an earlier AM hydraulic manifold. Given restrictions of L -PBF of stainless steel, the study uses the procedures to generate manufacturable 3D manifold designs for different part orientations. One of the manifold variants is fabricated to show the manufacturability of the generated 3D part designs. The work finishes by discussing the benefits of the procedures, their transferability, and enhancements to generate manufacturable and functionally optimized designs.
SP  - 102119
EP  - NA
JF  - Additive Manufacturing
VL  - 46
IS  - NA
PB  - 
DO  - 10.1016/j.addma.2021.102119
ER  - 

TY  - JOUR
AU  - Roels, Ellen; Terryn, Seppe; Iida, Fumiya; Bosman, Anton W.; Norvez, Sophie; Clemens, Frank; Van Assche, Guy; Vanderborght, Bram; Brancart, Joost
TI  - Processing of Self-Healing Polymers for Soft Robotics.
PY  - 2021
AB  - Soft robots are, due to their softness, inherently safe and adapt well to unstructured environments. However, they are prone to various damage types. Self-healing polymers address this vulnerability. Self-healing soft robots can recover completely from macroscopic damage, extending their lifetime. For developing healable soft robots, various formative and additive manufacturing methods have been exploited to shape self-healing polymers into complex structures. Additionally, several novel manufacturing techniques, noted as (re)assembly binding techniques that are specific to self-healing polymers, have been created. Herein, the wide variety of processing techniques of self-healing polymers for robotics available in the literature is reviewed, and limitations and opportunities discussed thoroughly. Based on defined requirements for soft robots, these techniques are critically compared and validated. A strong focus is drawn to the reversible covalent and (physico)chemical cross-links present in the self-healing polymers that do not only endow healability to the resulting soft robotic components, but are also beneficial in many manufacturing techniques. They solve current obstacles in soft robots, including the formation of robust multi-material parts, recyclability, and stress relaxation. This review bridges two promising research fields, and guides the reader toward selecting a suitable processing method based on a self-healing polymer and the intended soft robotics application.
SP  - 2104798
EP  - NA
JF  - Advanced materials (Deerfield Beach, Fla.)
VL  - 34
IS  - 1
PB  - 
DO  - 10.1002/adma.202104798
ER  - 

TY  - NA
AU  - Bohus, Dan; Andrist, Sean; Feniello, Ashley; Saw, Nick; Horvitz, Eric
TI  - Continual Learning about Objects in the Wild: An Interactive Approach
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556567
ER  - 

TY  - NA
AU  - Yang, Jackie; Banerjee, Gaurab; Gupta, Vishesh; Lam, Monica S.; Landay, James A.
TI  - CHI - Soundr: Head Position and Orientation Prediction Using a Microphone Array
PY  - 2020
AB  - Although state-of-the-art smart speakers can hear a user's speech, unlike a human assistant these devices cannot figure out users' verbal references based on their head location and orientation. Soundr presents a novel interaction technique that leverages the built-in microphone array found in most smart speakers to infer the user's spatial location and head orientation using only their voice. With that extra information, Soundr can figure out users references to objects, people, and locations based on the speakers' gaze, and also provide relative directions. To provide training data for our neural network, we collected 751 minutes of data (50x that of the best prior work) from human speakers leveraging a virtual reality headset to accurately provide head tracking ground truth. Our results achieve an average positional error of 0.31m and an orientation angle accuracy of 34.3° for each voice command. A user study to evaluate user preferences for controlling IoT appliances by talking at them found this new approach to be fast and easy to use.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376427
ER  - 

TY  - NA
AU  - Han, Changyo; Takahashi, Ryo; Yahagi, Yuchi; Naemura, Takeshi
TI  - CHI Extended Abstracts - 3D Printing Firm Inflatables with Internal Tethers
PY  - 2021
AB  - This paper presents a technique for 3D printing firm inflatables with consumer-grade fused-deposition modeling (FDM) 3D printers and flexible filaments. By printing bridges inside the inflatable to tie its walls, internal tethers can retain the shape of the surfaces when inflated. This internal structure gives extra stiffness to the inflatables while retaining them lightweight and portable; the inflatables can be squished down to reduce the volume and inflated back to a sturdy state. Compared to conventional drop-stitch fabrics, the length of internal tethers can be easily varied owing to 3D printing, allowing us to fabricate angled surfaces as well as parallel surfaces. We evaluate the physical properties of the 3D-printed inflatables with internal tethers made with diverse printing parameters. Finally, we demonstrate the feasibility of our technique in custom inflatable design with example applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451613
ER  - 

TY  - NA
AU  - Milani, Mostafa; Huang, Yu; Chiang, Fei
TI  - Diversifying Anonymized Data with Diversity Constraints.
PY  - 2020
AB  - Recently introduced privacy legislation has aimed to restrict and control the amount of personal data published by companies and shared to third parties. Much of this real data is not only sensitive requiring anonymization, but also contains characteristic details from a variety of individuals. This diversity is desirable in many applications ranging from Web search to drug and product development. Unfortunately, data anonymization techniques have largely ignored diversity in its published result. This inadvertently propagates underlying bias in subsequent data analysis. We study the problem of finding a diverse anonymized data instance where diversity is measured via a set of diversity constraints. We formalize diversity constraints and study their foundations such as implication and satisfiability. We show that determining the existence of a diverse, anonymized instance can be done in PTIME, and we present a clustering-based algorithm. We conduct extensive experiments using real and synthetic data showing the effectiveness of our techniques, and improvement over existing baselines. Our work aligns with recent trends towards responsible data science by coupling diversity with privacy-preserving data publishing.
SP  - NA
EP  - NA
JF  - arXiv: Databases
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Xavier, Matheus S.; Tawk, Charbel D.; Zolfagharian, Ali; Pinskier, Joshua; Howard, David; Young, Taylor; Lai, Jiewen; Harrison, Simon M.; Yong, Yuen K.; Bodaghi, Mahdi; Fleming, Andrew J.
TI  - Soft Pneumatic Actuators: A Review of Design, Fabrication, Modeling, Sensing, Control and Applications
PY  - 2022
AB  - Soft robotics is a rapidly evolving field where robots are fabricated using highly deformable materials and usually follow a bioinspired design. Their high dexterity and safety makes them ideal for applications such as gripping, locomotion, and biomedical devices, where the environment is highly dynamic and sensitive to physical interaction. Pneumatic actuation remains the dominant technology in soft robotics due to its low cost and mass, fast response time, and easy implementation. Given the significant number of publications in soft robotics over recent years, newcomers and even established researchers may have difficulty assessing the state of the art. To address this issue, this article summarizes the development of soft pneumatic actuators and robots up until the date of publication. The scope of this article includes the design, modeling, fabrication, actuation, characterization, sensing, control, and applications of soft robotic devices. In addition to a historical overview, there is a special emphasis on recent advances such as novel designs, differential simulators, analytical and numerical modeling methods, topology optimization, data-driven modeling and control methods, hardware control boards, and nonlinear estimation and control techniques. Finally, the capabilities and limitations of soft pneumatic actuators and robots are discussed and directions for future research are identified.
SP  - 59442
EP  - 59485
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3179589
ER  - 

TY  - JOUR
AU  - Wen, Zhen; Zeng, Wei; Weng, Luoxuan; Liu, Yihan; Xu, Mingliang; Chen, Wei
TI  - Effects of View Layout On Situated Analytics for Multiple-View Representations in Immersive Visualization.
PY  - 2022
AB  - Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209475
ER  - 

TY  - NA
AU  - Büschel, Wolfgang; Lehmann, Anke; Dachselt, Raimund
TI  - CHI - MIRIA: A Mixed Reality Toolkit for the In-Situ Visualization and Analysis of Spatio-Temporal Interaction Data
PY  - 2021
AB  - In this paper, we present MIRIA, a Mixed Reality Interaction Analysis toolkit designed to support the in-situ visual analysis of user interaction in mixed reality and multi-display environments. So far, there are few options to effectively explore and analyze interaction patterns in such novel computing systems. With MIRIA, we address this gap by supporting the analysis of user movement, spatial interaction, and event data by multiple, co-located users directly in the original environment. Based on our own experiences and an analysis of the typical data, tasks, and visualizations used in existing approaches, we identify requirements for our system. We report on the design and prototypical implementation of MIRIA, which is informed by these requirements and offers various visualizations such as 3D movement trajectories, position heatmaps, and scatterplots. To demonstrate the value of MIRIA for real-world analysis tasks, we conducted expert feedback sessions using several use cases with authentic study data.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445651
ER  - 

TY  - NA
AU  - Qian, Jing; Sun, Qi; Wigington, Curtis; Han, Han L.; Sun, Tong; Healey, Jennifer; Tompkin, James; Huang, Jeff
TI  - Dually Noted: Layout-Aware Annotations with Smartphone Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502026
ER  - 

TY  - NA
AU  - Do, Seungwon; Lee, Byungjoo
TI  - CHI - Improving Reliability of Virtual Collision Responses: A Cue Integration Technique
PY  - 2020
AB  - In virtual reality (VR), a user's virtual avatar can interact with a virtual object by colliding with it. If collision responses do not occur in the direction that the user expects, the user experiences degradation of accuracy and precision in applications such as VR sports games. In determining the response of a virtual collision, existing physics engines have not considered the direction in which the user perceived and estimated the collision. Based on the cue integration theory, this study presents a statistical model explaining how users estimate the direction of a virtual collision from their body's orientation and velocity vectors. The accuracy and precision of virtual collisions can be improved by 8.77% and 30.29%, respectively, by setting the virtual collision response in the direction that users perceive.
SP  - 3376819
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376819
ER  - 

TY  - NA
AU  - Dudley, John; Jacques, Jason T.; Kristensson, Per Ola
TI  - CHI - Crowdsourcing Design Guidance for Contextual Adaptation of Text Content in Augmented Reality
PY  - 2021
AB  - Augmented Reality (AR) can deliver engaging user experiences that seamlessly meld virtual content with the physical environment. However, building such experiences is challenging due to the developer’s inability to assess how uncontrolled deployment contexts may influence the user experience. To address this issue, we demonstrate a method for rapidly conducting AR experiments and real-world data collection in the user’s own physical environment using a privacy-conscious mobile web application. The approach leverages the large number of distinct user contexts accessible through crowdsourcing to efficiently source diverse context and perceptual preference data. The insights gathered through this method complement emerging design guidance and sample-limited lab-based studies. The utility of the method is illustrated by re-examining the design challenge of adapting AR text content to the user’s environment. Finally, we demonstrate how gathered design insight can be operationalized to provide adaptive text content functionality in an AR headset.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445493
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Karim, Adnan; Xia, Tian; Hedayati, Hooman; Marquardt, Nicolai
TI  - Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces
PY  - 2022
AB  - This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517719
ER  - 

TY  - JOUR
AU  - Wu, Mengjie; Zhi, Chao; Tu, Li; Yongzhen, Wang; Dai, Yang; Yu, Lingjie; Meng, Jiaguang; Xiaoyi, He
TI  - Cotton-containing printing wires based on the two-dimensional braiding method for three-dimensional printing of clothing:
PY  - 2021
AB  - There is a large somatosensory gap between the three-dimensional (3D) printing of clothing and traditional garments due to the limitations (e.g., air permeability and skin-friendliness) of the prin...
SP  - 004051752110592
EP  - 1393
JF  - Textile Research Journal
VL  - 92
IS  - 9-10
PB  - 
DO  - 10.1177/00405175211059208
ER  - 

TY  - JOUR
AU  - Kim, Jinwook; Kim, Seonghyeon; Lee, Jeongmi
TI  - The Effect of Multisensory Pseudo-Haptic Feedback on Perception of Virtual Weight
PY  - 2022
AB  - Providing realistic haptic feedback of virtual objects is critical for immersive VR experience, and there have been many approaches to simulate haptic properties. Most of them, however, are limited to a narrow modulation range of simulated perception. To overcome this limitation, the current paper examines the effect of multisensory pseudo-haptic feedback that combines control-to-display (C/D) ratio manipulation and electrical muscle stimulation (EMS) on simulated weight perception. In two experiments, we independently manipulated the C/D ratio and EMS status and observed the effects on the absolute and difference thresholds of simulated weight perception. From the absolute thresholds results, we specify the effective range of C/D ratio that can successfully induce weight perception and show that the range can be more than twice widened by multisensory pseudo-haptic feedback. Furthermore, we demonstrate that the sensitivity to weight difference increases as the standard C/D ratio decreases from the difference thresholds results, which provides practical design guidelines for assigning multiple levels of weight to virtual objects. This study contributes to understanding the psychological effects of multisensory pseudo-haptic feedback on simulated weight perception in virtual reality.
SP  - 5129
EP  - 5140
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3140438
ER  - 

TY  - BOOK
AU  - Buruk, Oğuz Turan; Hamari, Juho
TI  - MindTrek - Immersive Video Sketching: Low-Fidelity Extended Reality Prototyping for Everyone
PY  - 2021
AB  - As the extended reality (XR) field advance, the need for tools assisting designers in the early-design phases for these immersive environments also increases. Although several tools exist, we still need a method that allows non-experts to engage in designing for XR, especially for collaborative contexts such as participatory design (PD) workshops. In this paper, we introduce Immersive Video Sketching (IVS), a low-cost prototyping method for early-phase XR design that can be easily employed by novice and non-expert designers. IVS combines body storming, paper prototyping and video sketching for XR environments. We tested IVS with 23 participants in a PD session focusing on XR game wearables. Our results showed that IVS can help non-experts to grasp the immersive nature of XR environments easily. On the other hand, a 4-hour design session might not be enough for iterations on design ideas and different design skills might create discrepancies in the outcomes.
SP  - 165
EP  - 175
JF  - Academic Mindtrek 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3464327.3464330
ER  - 

TY  - NA
AU  - Je, Seungwoo; Lim, Hyunseung; Moon, Kongpyung; Teng, Shan-Yuan; Brooks, Jas; Lopes, Pedro; Bianchi, Andrea
TI  - CHI - Elevate: A Walkable Pin-Array for Large Shape-Changing Terrains
PY  - 2021
AB  - Current head-mounted displays enable users to explore virtual worlds by simply walking through them (i.e., real-walking VR). This led researchers to create haptic displays that can also simulate different types of elevation shapes. However, existing shape-changing floors are limited by their tabletop scale or the coarse resolution of the terrains they can display due to the limited number of actuators and low vertical resolution. To tackle this challenge, we introduce Elevate, a dynamic and walkable pin-array floor on which users can experience not only large variations in shapes but also the details of the underlying terrain. Our system achieves this by packing 1200 pins arranged on a 1.80 × 0.60m platform, in which each pin can be actuated to one of ten height levels (resolution: 15mm/level). To demonstrate its applicability, we present our haptic floor combined with four walkable applications and a user study that reported increased realism and enjoyment.
SP  - 1
EP  - 11
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445454
ER  - 

TY  - NA
AU  - Ulu, Erva; Ulu, Nurcan Gecer; Li, Jiahao; Hsiao, Walter
TI  - Curvy: An Interactive Design Tool for Varying Density Support Structures.
PY  - 2021
AB  - We introduce Curvy-an interactive design tool to generate varying density support structures for 3D printing. Support structures are essential for printing models with extreme overhangs. Yet, they often cause defects on contact areas, resulting in poor surface quality. Low-level design of support structures may alleviate such negative effects. However, it is tedious and unintuitive for novice users as it is hard to predict the impact of changes to the support structure on the final printed part. Curvy allows users to define their high-level preferences on the surface quality directly on the target object rather than explicitly designing the supports. These preferences are then automatically translated into low-level design parameters to generate the support structure. Underlying novel curvy zigzag toolpathing algorithm uses these instructions to generate varying density supports by altering the spacing between individual paths in order to achieve prescribed quality. Combined with the build orientation optimization, Curvy provides a practical solution to the design of support structures with minimal perceptual or functional impact on the target part to be printed.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - He, Liang
TI  - UIST (Adjunct Volume) - Designing, Controlling, and Fabricating In-Place Augmented Structures
PY  - 2020
AB  - Emerging 3D printing technology has enabled the rapid development of physical objects. However, 3D-printed objects are rarely interactive and adding interactivity to printed objects is inherently challenging. To boost 3D printing for a wider spectrum of applications, I introduce in-place augmented structures, a class of 3D printable parametric structures that can be integrated with physical objects and spaces for augmented behaviors. In my research, I explore how 3D printing can support interaction (e.g., sensing and actuation) by creating novel design techniques and building interactive design tools that enable end-users to design and control desired behaviors. With these techniques and tools, I fabricate the in-place structures with readily available fabrication techniques and demonstrate my approach with a suite of applications across different domains.
SP  - 169
EP  - 173
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415804
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles; Haliyo, Sinan; Frey, Pascal
TI  - "Can I Touch This?": Survey of Virtual Reality Interactions via Haptic Solutions
PY  - 2021
AB  - Haptic feedback has become crucial to enhance the user experiences in Virtual Reality (VR). This justifies the sudden burst of novel haptic solutions proposed these past years in the HCI community. This article is a survey of Virtual Reality interactions, relying on haptic devices. We propose two dimensions to describe and compare the current haptic solutions: their degree of physicality, as well as their degree of actuation. We depict a compromise between the user and the designer, highlighting how the range of required or proposed stimulation in VR is opposed to the haptic interfaces flexibility and their deployment in real-life use-cases. This paper (1) outlines the variety of haptic solutions and provides a novel perspective for analysing their associated interactions, (2) highlights the limits of the current evaluation criteria regarding these interactions, and finally (3) reflects the interaction, operation and conception potentials of "encountered-type of haptic devices".
SP  - NA
EP  - NA
JF  - 32e Conférence Francophone sur l'Interaction Homme-Machine
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450522.3451323
ER  - 

TY  - CHAP
AU  - Pham, Tracy; Tezza, Dante; Andujar, Marvin
TI  - HCI (2) - Enhancing Drone Pilots’ Engagement Through a Brain-Computer Interface
PY  - 2020
AB  - Drones are becoming ubiquitous in society, and as their use continues to grow, it becomes important to research new approaches to provide a better user experience and safer flights. In this paper, we propose the use of brain-computer interfaces (BCI) to measure the drone pilot’s engagement from the brain while piloting drones. We hypothesize that relaying on a quantified measurement, the pilot will be encouraged to increase their focus, leading to higher engagement and possible safer flights. Our first contribution is a technical description of the system, which allows the pilots to control a virtual first-person view (FPV) drone while receiving feedback on their engagement level measured with a BCI. Secondly, we present the results of a user study with 10 participants, in which their feedback stated that receiving engagement level feedback increased their engagement as they tried to raise their focus in the activity.
SP  - 706
EP  - 718
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_49
ER  - 

TY  - BOOK
AU  - John, Brendan; Kalyanaraman, Sriram; Jain, Eakta
TI  - VR Workshops - Look Out! A Design Framework for Safety Training Systems A Case Study on Omnidirectional Cinemagraphs
PY  - 2020
AB  - Accidents occur when a person’s attention is distracted. A key aspect of safety training is directing people’s attention to potential hazards. Unfortunately, creating such hazards also puts people at risk, especially during safety training in manufacturing and construction. Virtual reality provides a training mechanism by which hazardous training scenarios can be created without putting the trainee at risk. We present a general framework for safety training systems and also present results from a case study where we create and evaluate a novel safety training environment, namely, omnidirectional cinemagraphs.
SP  - 147
EP  - 153
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00031
ER  - 

TY  - NA
AU  - Scargill, Tim; Chen, Ying; Marzen, Nathan; Gorlatova, Maria
TI  - Integrated Design of Augmented Reality Spaces Using Virtual Environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00045
ER  - 

TY  - NA
AU  - Lo, Wei Hong; Regenbrecht, Holger; Ens, Barrett; Zollmann, Stefanie
TI  - A Context-aware Interface for Immersive Sports Spectating
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00125
ER  - 

TY  - NA
AU  - Merino, Mauricio Verano; Thomas van Binsbergen, L.; Seraj, Mazyar
TI  - Making the Invisible Visible in Computational Notebooks
PY  - 2022
AB  - Notebooks are increasingly popular programming tools adopted by a diverse range of users, including professional and novice users, from various fields not necessarily skilled in software engineering, to experiment with programming and develop software. Notebooks are often used within interactive and exploratory programming settings; however, some of their main use cases are not naturally supported by their design. For example, users can only get insights into the program’s state by executing program fragments and updating one’s mental model. This paper discusses the possibility of defining widgets to improve notebooks by providing direct insights into the program state. The widgets are developed upon previous work in which a novel approach to incremental programming is suggested based on the notion of an exploring interpreter. As example, we present widgets for visualizing execution history and variable assignments, thereby reducing the cognitive load on users.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833148
ER  - 

TY  - NA
AU  - Wang, Chenglong; Feng, Yu; Bodik, Rastislav; Dillig, Isil; Cheung, Alvin; Ko, Amy J.
TI  - Falx: Synthesis-Powered Visualization Authoring
PY  - 2021
AB  - Modern visualization tools aim to allow data analysts to easily create exploratory visualizations. When the input data layout conforms to the visualization design, users can easily specify visualizations by mapping data columns to visual channels of the design. However, when there is a mismatch between data layout and the design, users need to spend significant effort on data transformation. We propose Falx, a synthesis-powered visualization tool that allows users to specify visualizations in a similarly simple way but without needing to worry about data layout. In Falx, users specify visualizations using examples of how concrete values in the input are mapped to visual channels, and Falx automatically infers the visualization specification and transforms the data to match the design. In a study with 33 data analysts on four visualization tasks involving data transformation, we found that users can effectively adopt Falx to create visualizations they otherwise cannot implement.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445249
ER  - 

TY  - NA
AU  - Arawjo, Ian; DeArmas, Anthony; Roberts, Michael; Basu, Shrutarshi; Parikh, Tapan
TI  - Notational Programming for Notebook Environments: A Case Study with Quantum Circuits
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545619
ER  - 

TY  - NA
AU  - Cheng, Jen-Hao; Chen, Yi; Chang, Ting-Yi; Lin, Hsu-En; Wang, Po-Yao Cosmos; Cheng, Lung-Pan
TI  - VR - Impossible Staircase: Vertically Real Walking in an Infinite Virtual Tower
PY  - 2021
AB  - We present Impossible Staircase, a real-walking virtual reality system that allows users to climb an infinite virtual tower. Our set-up consists of an one-level scaffold and a lifter. A user climbs up the scaffold by real walking on a stairway while wearing a head-mounted display, and gets reset to the ground level by a lifter imperceptibly. By repeating this process, the user perceives an illusion of climbing an infinite number of levels. Our system achieves the illusion by (1) controlling the movement of the lifter to generate reverse and imperceptible motion, (2) guiding the user through the scaffold with delay mechanisms to reset the lifter in time, and (3) procedural generating overlapping structures to enlarge perceived height of each level. We built a working system and demonstrated it with a 15-min experience. With the working system, we conducted user studies to gain deeper insights into vertical motion simulation and vertical real walking in virtual reality.
SP  - 50
EP  - 56
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00025
ER  - 

TY  - NA
AU  - Du, Ruofei; Turner, Eric; Dzitsiuk, Maksym; Prasso, Luca; Duarte, Ivo; Dourgarian, Jason; Afonso, Joao; Pascoal, Jose; Gladstone, Josh; Cruces, Nuno; Izadi, Shahram; Kowdle, Adarsh; Tsotsos, Konstantine; Kim, David
TI  - UIST - DepthLab: Real-time 3D Interaction with Depth Maps for Mobile Augmented Reality
PY  - 2020
AB  - Mobile devices with passive depth sensing capabilities are ubiquitous, and recently active depth sensors have become available on some tablets and AR/VR devices. Although real-time depth data is accessible, its rich value to mainstream AR applications has been sorely under-explored. Adoption of depth-based UX has been impeded by the complexity of performing even simple operations with raw depth data, such as detecting intersections or constructing meshes. In this paper, we introduce DepthLab, a software library that encapsulates a variety of depth-based UI/UX paradigms, including geometry-aware rendering (occlusion, shadows), surface interaction behaviors (physics-based collisions, avatar path planning), and visual effects (relighting, 3D-anchored focus and aperture effects). We break down the usage of depth into localized depth, surface depth, and dense depth, and describe our real-time algorithms for interaction and rendering tasks. We present the design process, system, and components of DepthLab to streamline and centralize the development of interactive depth features. We have open-sourced our software at https://github.com/googlesamples/arcore-depth-lab to external developers, conducted performance evaluation, and discussed how DepthLab can accelerate the workflow of mobile AR designers and developers. With DepthLab we aim to help mobile developers to effortlessly integrate depth into their AR experiences and amplify the expression of their creative vision.
SP  - 829
EP  - 843
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415881
ER  - 

TY  - NA
AU  - Muender, Thomas; Bonfert, Michael; Reinschluessel, Anke Verena; Malaka, Rainer; Döring, Tanja
TI  - Haptic Fidelity Framework: Defining the Factors of Realistic Haptic Feedback for Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501953
ER  - 

TY  - JOUR
AU  - Rout, Sangram K; Bisram, Marisa Ravena; Cao, Jian
TI  - Methods for numerical simulation of knit based morphable structures: knitmorphs.
PY  - 2022
AB  - Shape morphing behavior has applications in many fields such as soft robotics, actuators and sensors, solar cells, tight packaging, flexible electronics, and biomedicine. The most common approach to achieve shape morphing structures is through shape memory alloys or hydrogels. These two materials undergo differential strains which generate a variety of shapes. In this work, we demonstrate the novel concept that 2D knits comprising of yarns from different materials can be morphed into different three-dimensional shapes thereby forming a bridge between traditional knitting and shape changing structures. This concept is referred to as Knitmorphs. Our computational analysis acts as the proof of concept revealing that knitted patterns of varying materials morph into complex shapes, such as saddle, axisymmetric cup, and a plate with waves when subjected to thermal loads. Two-dimensional circular models of plain and rib developed on CAD packages are imported to the finite element analysis software Abaqus, followed by post-processing into wires and assigning fiber material properties of different thermal coefficients of expansion and stiffness. We also propose potential applications for the concept of programmable knits for developing robots based upon jellyfish like locomotion, and complex structures similar to wind turbine blades. This novel concept is meant to introduce a new field for design when considering morphable structures.
SP  - 6630
EP  - NA
JF  - Scientific reports
VL  - 12
IS  - 1
PB  - 
DO  - 10.1038/s41598-022-09422-3
ER  - 

TY  - NA
AU  - Cheng, Yi Fei; Yin, Hang; Yan, Yukang; Gugenheimer, Jan; Lindlbauer, David
TI  - Towards Understanding Diminished Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517452
ER  - 

TY  - JOUR
AU  - Rakkolainen, Ismo; Freeman, Euan; Sand, Antti; Raisamo, Roope; Brewster, Stephen
TI  - A Survey of Mid-Air Ultrasound Haptics and Its Applications
PY  - 2021
AB  - Ultrasound haptics is a contactless haptic technology that enables novel mid-air interactions with rich multisensory feedback. This article surveys recent advances in ultrasound haptic technology. We discuss the fundamentals of this haptic technology, how a variety of perceptible sensations are rendered, and how it is currently being used to enable novel interaction techniques. We summarize its strengths, weaknesses, and potential applications across various domains. We conclude with our perspective on key directions for this promising haptic technology.
SP  - 2
EP  - 19
JF  - IEEE transactions on haptics
VL  - 14
IS  - 1
PB  - 
DO  - 10.1109/toh.2020.3018754
ER  - 

TY  - NA
AU  - Zhao, Xiaoliang; Chen, Jia; Li, Jinhui; Jin, Mengmeng; Deng, Zihan; Li, Xiaopeng; Zhang, Wei; Jin, Minghua; Ying, Fangtian; Wang, Qi; Wang, Guanyun
TI  - MultiPneu: Inflatable Deformation of Multilayered Film Materials
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - The Ninth International Symposium of Chinese CHI
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490355.3490517
ER  - 

TY  - JOUR
AU  - Yao, Yuan; Ding, Cheng; Aburaia, Mohamed; Lackner, Maximilian; He, Lanlan
TI  - A 3D weaving infill pattern for fused filament fabrication
PY  - 2021
AB  - The fused filament fabrication process is the most used additive manufacturing process due to its simplicity and low operating costs. In this process, a thermoplastic filament is led through an extruder, melted, and applied to a building platform by the axial movements of an automated Cartesian system in such a way that a three-dimensional object is created layer by layer. Compared to other additive manufacturing technologies, the components produced have mechanical limitations and are often not suitable for functional applications. To reduce the anisotropy of mechanical strength in fused filament fabrication (FFF), this paper proposes a 3D weaving deposit path planning method that utilizes a 5-layer repetitive structure to achieve interlocking and embedding between neighbor slicing planes to improve the mechanical linkage within the layers. The developed algorithm extends the weaving path as an infill pattern to fill different structures and makes this process feasible on a standard three-axis 3D printer. Compared with 3D weaving printed parts by layer-to-layer deposit, the anisotropy of mechanical properties inside layers is significantly reduced to 10.21% and 0.98%.
SP  - 1
EP  - 14
JF  - NA
VL  - 117
IS  - 9
PB  - 
DO  - 10.21203/rs.3.rs-295081/v1
ER  - 

TY  - NA
AU  - Lilija, Klemen; Pohl, Henning; Hornbæk, Kasper
TI  - CHI - Who Put That There? Temporal Navigation of Spatial Recordings by Direct Manipulation
PY  - 2020
AB  - Spatial recordings allow viewers to move within them and freely choose their viewpoint. However, such recordings make it easy to miss events and difficult to follow moving objects when skipping through the recording. To alleviate these problems we present the Who Put That There system that allows users to navigate through time by directly manipulating objects in the scene. By selecting an object, the user can navigate to moments where the object changed. Users can also view trajectories of objects that changed location and directly manipulate them to navigate. We evaluated the system with a set of sensemaking questions in a think-aloud study. Participants understood the system and found it useful for finding events of interest, while being present and engaged in the recording.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376604
ER  - 

TY  - NA
AU  - Gong, Jun; Gupta, Aakar; Benko, Hrvoje
TI  - UIST - Acustico: Surface Tap Detection and Localization using Wrist-based Acoustic TDOA Sensing
PY  - 2020
AB  - In this paper, we present Acustico, a passive acoustic sensing approach that enables tap detection and 2D tap localization on uninstrumented surfaces using a wrist-worn device. Our technique uses a novel application of acoustic time differences of arrival (TDOA) analysis. We adopt a sensor fusion approach by taking both 'surface waves' (i.e., vibrations through surface) and 'sound waves' (i.e., vibrations through air) into analysis to improve sensing resolution. We carefully design a sensor configuration to meet the constraints of a wristband form factor. We built a wristband prototype with four acoustic sensors, two accelerometers and two microphones. Through a 20-participant study, we evaluated the performance of our proposed sensing technique for tap detection and localization. Results show that our system reliably detects taps with an F1-score of 0.9987 across different environmental noises and yields high localization accuracies with root-mean-square-errors of 7.6mm (X-axis) and 4.6mm (Y-axis) across different surfaces and tapping techniques.
SP  - 406
EP  - 419
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415901
ER  - 

TY  - NA
AU  - Lu, Jasmine; Liu, Ziwei; Brooks, Jas; Lopes, Pedro
TI  - UIST - Chemical Haptics: Rendering Haptic Sensations via Topical Stimulants
PY  - 2021
AB  - We propose a new class of haptic devices that provide haptic sensations by delivering liquid-stimulants to the user's skin; we call this chemical haptics. Upon absorbing these stimulants, which contain safe and small doses of key active ingredients, receptors in the user's skin are chemically triggered, rendering distinct haptic sensations. We identified five chemicals that can render lasting haptic sensations: tingling (sanshool), numbing (lidocaine), stinging (cinnamaldehyde), warming (capsaicin), and cooling (menthol). To enable the application of our novel approach in a variety of settings (such as VR), we engineered a self-contained wearable that can be worn anywhere on the user's skin (e.g., face, arms, legs). Implemented as a soft silicone patch, our device uses micropumps to push the liquid stimulants through channels that are open to the user's skin, enabling topical stimulants to be absorbed by the skin as they pass through. Our approach presents two unique benefits. First, it enables sensations, such as numbing, not possible with existing haptic devices. Second, our approach offers a new pathway, via the skin's chemical receptors, for achieving multiple haptic sensations using a single actuator, which would otherwise require combining multiple actuators (e.g., Peltier, vibration motors, electro-tactile stimulation). We evaluated our approach by means of two studies. In our first study, we characterized the temporal profiles of sensations elicited by each chemical. Using these insights, we designed five interactive VR experiences utilizing chemical haptics, and in our second user study, participants rated these VR experiences with chemical haptics as more immersive than without. Finally, as the first work exploring the use of chemical haptics on the skin, we offer recommendations to designers for how they may employ our approach for their interactive experiences.
SP  - 239
EP  - 257
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474747
ER  - 

TY  - JOUR
AU  - Lee, Hyelip; Bianchi, Andrea
TI  - YourSphere: Supporting Flexible Environmental Switching in Open-plan Workspaces via Augmented Reality
PY  - 2020
AB  - NA
SP  - 43
EP  - 53
JF  - Archives of Design Research
VL  - 33
IS  - 4
PB  - 
DO  - 10.15187/adr.2020.11.33.4.43
ER  - 

TY  - JOUR
AU  - Xiao, Ya-Qian; Kan, Chi-Wai
TI  - Review on Development and Application of 3D-Printing Technology in Textile and Fashion Design
PY  - 2022
AB  - <jats:p>Three-dimensional printing (3DP) allows for the creation of highly complex products and offers customization for individual users. It has generated significant interest and shows great promise for textile and fashion design. Here, we provide a timely and comprehensive review of 3DP technology for the textile and fashion industries according to recent advances in research. We describe the four 3DP methods for preparing textiles; then, we summarize three routes to use 3DP technology in textile manufacturing, including printing fibers, printing flexible structures and printing on textiles. In addition, the applications of 3DP technology in fashion design, functional garments and electronic textiles are introduced. Finally, the challenges and prospects of 3DP technology are discussed.</jats:p>
SP  - 267
EP  - 267
JF  - Coatings
VL  - 12
IS  - 2
PB  - 
DO  - 10.3390/coatings12020267
ER  - 

TY  - NA
AU  - Subramonyam, Hariharan; Seifert, Colleen M.; Adar, Eytan
TI  - Conference on Designing Interactive Systems - Towards A Process Model for Co-Creating AI Experiences
PY  - 2021
AB  - Thinking of technology as a design material is appealing. It encourages designers to explore the material’s properties to understand its capabilities and limitations—a prerequisite to generative design thinking. However, as a material, AI resists this approach because its properties only emerge as part of the user experience design. Therefore, designers and AI engineers must collaborate in new ways to create both the material and its application experience. We investigate the co-creation process through a design study with 10 pairs of designers and engineers. We find that design ‘probes’ with user data are a useful tool in defining AI materials. Through data probes, designers construct designerly representations of the envisioned AI experience (AIX) to identify desirable AI characteristics. Data probes facilitate divergent design thinking, material testing, and design validation. Based on our findings, we propose a process model for co-creating AIX and offer design considerations for incorporating data probes in AIX design tools.
SP  - 1529
EP  - 1543
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462012
ER  - 

TY  - NA
AU  - Sagheb, Shahabedin; Liu, Frank Wencheng; Vuong, Alex; Dai, Shiling; Wirjadi, Ryan; Bao, Yueming; Likamwa, Robert
TI  - Demonstration of Geppetteau: Enabling haptic perceptions of virtual fluids in various vessel profiles using a string-driven haptic interface
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558611
ER  - 

TY  - JOUR
AU  - Yin, Jessica; Hinchet, Ronan; Shea, Herbert; Majidi, Carmel
TI  - Wearable Soft Technologies for Haptic Sensing and Feedback
PY  - 2020
AB  - NA
SP  - 2007428
EP  - NA
JF  - Advanced Functional Materials
VL  - 31
IS  - 39
PB  - 
DO  - 10.1002/adfm.202007428
ER  - 

TY  - NA
AU  - Tokuda, Yutaka; Sahoo, Deepak Ranjan; Jones, Matt; Subramanian, Sriram; Withana, Anusha
TI  - TEI - Flowcuits: Crafting Tangible and Interactive Electrical Components with Liquid Metal Circuits
PY  - 2021
AB  - We present Flowcuits, a DIY fabrication method to prototype tangible, interactive and functional electrical components by manipulating liquid metals. The prototypes afford both physical and visual interactions to demonstrate the inner working mechanics of fundamental electronic elements, which enables tangible and playful learning. The fabrication process follows simple imprinting and sealing of fluidic circuits with a 3D-printed stamp on an accessible moldable-substrates such as ‘Blu Tack’. Utilizing conductive gallium indium liquid metal, we demonstrated interactive and re-configurable electronic components such as switches, variable resistors, variable capacitors, logic gates and pressure sensors. In this paper, we present the design analogy of Flowcuits, DIY fabrication approach including a parametric 3D stamp design toolkit and results from a technical evaluation. The stamps are printed with a low-cost 3D printer and all the materials are inexpensive and reusable, enabling Flowcuits to be easily used without any advanced lab facilities.
SP  - 35
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440654
ER  - 

TY  - NA
AU  - Mazursky, Alex; Teng, Shan-Yuan; Nith, Romain; Lopes, Pedro
TI  - CHI - MagnetIO: Passive yet Interactive Soft Haptic Patches Anywhere
PY  - 2021
AB  - We propose a new type of haptic actuator, which we call MagnetIO, that is comprised of two parts: one battery-powered voice-coil worn on the user's fingernail and any number of interactive soft patches that can be attached onto any surface (everyday objects, user's body, appliances, etc.). When the user's finger wearing our voice-coil contacts any of the interactive patches it detects its magnetic signature via magnetometer and vibrates the patch, adding haptic feedback to otherwise input-only interactions. To allow these passive patches to vibrate, we make them from silicone with regions doped with polarized neodymium powder, resulting in soft and stretchable magnets. This stretchable form-factor allows them to be wrapped to the user's body or everyday objects of various shapes. We demonstrate how these add haptic output to many situations, such as adding haptic buttons to the walls of one's home. In our technical evaluation, we demonstrate that our interactive patches can be excited across a wide range of frequencies (0-500 Hz) and can be tuned to resonate at specific frequencies based on the patch's geometry. Furthermore, we demonstrate that MagnetIO's vibration intensity is as powerful as a typical linear resonant actuator (LRA); yet, unlike these rigid actuators, our passive patches operate as springs with multiple modes of vibration, which enables a wider band around its resonant frequency than an equivalent LRA.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445543
ER  - 

TY  - CHAP
AU  - Shah, Syed Hammad Hussain; Longva, Bjørnar; Hameed, Ibrahim A.; Solberg, Mads; Karlsen, Anniken
TI  - HCI (38) - Health Data Management for Nursing Practice: An Intelligent, Holographic Mixed-Reality System
PY  - 2021
AB  - The elderly population in need of long-term care is rising, adding to the workload of healthcare workers in nursing homes. Throughout a work day health workers often must perform the time consuming process of collecting various types of health data from the residents. A recent study has revealed that frontline nurses and caregivers spend over half of their time accessing and updating patient records instead of providing care. New technology could enhance these data-oriented operations, and free up time so that staff can care for patients in other ways. Intelligent human-computer interaction paradigms can enhance data retrieval, visualization, and communication processes efficiently in a way to improve the overall productivity during work. In this study, we present an intelligent holographic mixed reality (MR) system that leverages Microsoft Hololens for work practices of healthcare work in nursing homes. Supported by computer vision, this system provides automatic data retrieval, in addition to interactive data visualization and mid-air data entry mechanism. Using face recognition through the Hololens’ visual sensors, the system recognizes individual patients, and automatically retrieves relevant health data and supports data entry.
SP  - 329
EP  - 336
JF  - HCI International 2021 - Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78642-7_45
ER  - 

TY  - BOOK
AU  - Yu, Tianhong Catherine; McCann, James
TI  - SCF - Coupling Programs and Visualization for Machine Knitting
PY  - 2020
AB  - To effectively program knitting machines, like any fabrication machine, users must be able to place the code they write in correspondence with the output the machine produces. This mapping is used in the code-to-output direction to understand what their code will produce, and in the output-to-code direction to debug errors in the finished product. In this paper, we describe and demonstrate an interface that provides two-way coupling between high- or low-level knitting code and a topological visualization of the knitted output. Our system allows the user to locate the knitting machine operations generated by any selected code, as well as the code that generates any selected knitting machine operation. This link between the code and visualization has the potential to reduce the time spent in design, implementation, and debugging phases, and save material costs by catching errors before actually knitting the object. We show examples of patterns designed using our tool and describe common errors that the tool catches when used in an academic lab setting and an undergraduate course.
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3424630.3425410
ER  - 

TY  - NA
AU  - Ceyssens, Jeroen; Di Fiore, Fabian; Luyten, Kris
TI  - Context-Aware Support of Dexterity Skills in Cross-Reality Environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00214
ER  - 

TY  - NA
AU  - Ng, Alexander; Medeiros, Daniel; McGill, Mark; Williamson, Julie R.; Brewster, Stephen
TI  - ISMAR - The Passenger Experience of Mixed Reality Virtual Display Layouts in Airplane Environments
PY  - 2021
AB  - Augmented / Mixed Reality headsets will in-time see adoption and use in a variety of mobility and transit contexts, allowing users to view and interact with virtual content and displays for productivity and entertainment. However, little is known regarding how multi-display virtual workspaces should be presented in a transit context, nor to what extent the unique affordances of transit environments (e.g. the social presence of others) might influence passenger perception of virtual display layouts. Using a simulated VR passenger airplane environment, we evaluated three different AR-driven virtual display configurations (Horizontal, Vertical, and Focus main display with smaller secondary windows) at two different depths, exploring their usability, user preferences, and the underlying factors that influenced those preferences. We found that the perception of invading other’s personal space significantly influenced preferred layouts in transit contexts. Based on our findings, we reflect on the unique challenges posed by passenger contexts, provide recommendations regarding virtual display layout in the confined airplane environment, and expand on the significant benefits that AR offers over physical displays in said environments.
SP  - 265
EP  - 274
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00042
ER  - 

TY  - NA
AU  - Yakura, Hiromu; Goto, Masataka
TI  - ISMAR - Enhancing Participation Experience in VR Live Concerts by Improving Motions of Virtual Audience Avatars
PY  - 2020
AB  - While participating in live concerts is a promising application of virtual reality (VR), it falls short of our participation experience in the real world. In particular, to increase the engagement of participants, previous studies emphasized the importance of social experience among audience members, such as the sense of co-presence elicited by sharing physical reactions or body movements synchronized with music. In this respect, a common strategy in existing platforms is to present avatars of remote human participants in a VR venue and make every avatar imitate movements of the corresponding participant. However, this strategy implicitly assumes that a not small number of users connect simultaneously to watch the same content and thus is not applicable when only a few users gather or a user is watching alone. Therefore, with the aim of providing better experience to a user who participates in live concerts as one of the audience, we examine computational approaches to enhancing the sense of co-presence through virtual audience avatars. We propose four methods of presenting avatar movements: copying the user’s own movements, copying other users’ movements, repeating beat-synchronous movements, and synthesizing machine-learning-based movements. We compare their effectiveness in a user experiment and discuss application scenarios and design implications that open up new ways of active media consumption in VR environments.
SP  - 555
EP  - 565
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00083
ER  - 

TY  - JOUR
AU  - Senft, Emmanuel; Hagenow, Michael; Welsh, Kevin M.; Radwin, Robert G.; Zinn, Michael R.; Gleicher, Michael; Mutlu, Bilge
TI  - Task-Level Authoring for Remote Robot Teleoperation
PY  - 2021
AB  - Remote teleoperation of robots can broaden the reach of domain specialists across a wide range of industries such as home maintenance, health care, light manufacturing, and construction. However, current direct control methods are impractical, and existing tools for programming robot remotely have focused on users with significant robotic experience. Extending robot remote programming to end users, i.e., users who are experts in a domain but novices in robotics, requires tools that balance the rich features necessary for complex teleoperation tasks with ease of use. The primary challenge to usability is that novice users are unable to specify complete and robust task plans to allow a robot to perform duties autonomously, particularly in highly variable environments. Our solution is to allow operators to specify shorter sequences of high-level commands, which we call task-level authoring, to create periods of variable robot autonomy. This approach allows inexperienced users to create robot behaviors in uncertain environments by interleaving exploration, specification of behaviors, and execution as separate steps. End users are able to break down the specification of tasks and adapt to the current needs of the interaction and environments, combining the reactivity of direct control to asynchronous operation. In this paper, we describe a prototype system contextualized in light manufacturing and its empirical validation in a user study where 18 participants with some programming experience were able to perform a variety of complex telemanipulation tasks with little training. Our results show that our approach allowed users to create flexible periods of autonomy and solve rich manipulation tasks. Furthermore, participants significantly preferred our system over comparative more direct interfaces, demonstrating the potential of our approach for enabling end users to effectively perform remote robot programming.
SP  - 707149
EP  - NA
JF  - Frontiers in robotics and AI
VL  - 8
IS  - NA
PB  - 
DO  - 10.3389/frobt.2021.707149
ER  - 

TY  - NA
AU  - Russell, Daniel M.; Neustaedter, Carman; Tang, John C.; Judge, Tejinder K.; Olson, Gary M.
TI  - CHI Extended Abstracts - Videoconferencing in the Age of COVID: How Well Has It Worked Out?
PY  - 2021
AB  - During the past year we've all spent many hours on videoconference calls, sometimes more than was comfortable. While CHI might not have anticipated a viral-driven surge in videoconferencing, online meetings has been a topic of CHI research for the past 25 years. This is a good time to assess how well our research has matched what this natural experiment is telling us. What did we get right? And what did the field get wrong? The panel, comprised of people who directly witnessed much of this history, will reflect on these questions. We don't expect all to agree with each panelist's conclusions, and we will invite reactions and contributions from the audience as well..
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3450398
ER  - 

TY  - BOOK
AU  - Mthunzi, Everett Mondliwethu; Getschmann, Christopher; Echtler, Florian
TI  - ISS Companion - Fast 3D point-cloud segmentation for interactive surfaces
PY  - 2021
AB  - Easily accessible depth sensors have enabled using point-cloud data to augment tabletop surfaces in everyday environments. However, point-cloud operations are computationally expensive and challenging to perform in real-time, particularly when targeting embedded systems without a dedicated GPU. In this paper, we propose mitigating the high computational costs by segmenting candidate interaction regions near real-time. We contribute an open-source solution for variable depth cameras using CPU-based architectures. For validation, we employ Microsoft’s Azure Kinect and report achieved performance. Our initial findings show that our approach takes under to segment candidate interaction regions on a tabletop surface and reduces the data volume by up to 70%. We conclude by contrasting the performance of our solution against a model-fitting approach implemented by the SurfaceStreams toolkit. Our approach outperforms the RANSAC-based strategy within the context of our test scenario, segmenting a tabletop’s interaction region up to 94% faster. Our results show promise for point-cloud-based approaches, even when targeting embedded solutions with limited resources.
SP  - 33
EP  - 37
JF  - Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447932.3491141
ER  - 

TY  - BOOK
AU  - Judd, Euan; Digumarti, Krishna Manaswi; Rossiter, Jonathan; Hauser, Helmut
TI  - RoboSoft - NeatSkin: A Discrete Impedance Tomography Skin Sensor
PY  - 2020
AB  - In this paper we present NeatSkin, a novel artificial skin sensor based on electrical impedance tomography. The key feature is a discrete network of fluidic channels which is used to infer the location of touch. Change in resistance of the conductive fluid within these channels during deformation is used to construct sensitivity maps. We present a method to simulate touch using this unique network-based, low output dimensionality approach. The efficacy is demonstrated by fabricating a NeatSkin sensor. This paves the way for the development of more complex channel networks and a higher resolution soft skin sensor with potential applications in soft robotics, wearable devices and safe human-robot interaction.
SP  - 33
EP  - 38
JF  - 2020 3rd IEEE International Conference on Soft Robotics (RoboSoft)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/robosoft48309.2020.9115979
ER  - 

TY  - NA
AU  - Lin, Hongnan; He, Liang; Song, Fangli; Li, Yifan; Cheng, Tingyu; Zheng, Clement; Wang, Wei; Oh, HyunJoo
TI  - FlexHaptics: A Design Method for Passive Haptic Inputs Using Planar Compliant Structures
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502113
ER  - 

TY  - NA
AU  - Huang, Hsin-Yu; Ning, Chih-Wei; Wang, Po-Yao; Cheng, Jen-Hao; Cheng, Lung-Pan
TI  - CHI Extended Abstracts - Haptic-go-round: A Surrounding Platform for Encounter-type Haptics in Virtual Reality Experiences
PY  - 2020
AB  - We present Haptic-go-round, a surrounding platform that allows deploying props and devices to provide haptic feedbacks in any direction in virtual reality experiences. The key component of Haptic-go-round is a motorized turntable that rotates the correct haptic device to the right direction at the right time to match what users are about to touch. We implemented a working platform including plug-and-play prop cartridges and a software interface that allow experience designers to agilely add their haptic components and use the platform for their applications. We conducted technical experiments and two user studies on Haptic-go-round to evaluate its performance. We report the results and discuss our insights and limitations.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376476
ER  - 

TY  - JOUR
AU  - Yan, Zihan; Zhou, Jiayi; Wu, Yufei; Liu, Guanhong; Luo, Danli; Zhou, Zihong; Mi, Haipeng; Sun, Lingyun; Chen, Xiang 'Anthony'; Tao, Ye; Zhang, Yang; Wang, Guanyun
TI  - Shoes++
PY  - 2022
AB  - <jats:p>Feet are the foundation of our bodies that not only perform locomotion but also participate in intent and emotion expression. Thus, foot gestures are an intuitive and natural form of expression for interpersonal interaction. Recent studies have mostly introduced smart shoes as personal gadgets, while foot gestures used in multi-person foot interactions in social scenarios remain largely unexplored. We present Shoes++, which includes an inertial measurement unit (IMU)-mounted sole and an input vocabulary of social foot-to-foot gestures to support foot-based interaction. The gesture vocabulary is derived and condensed by a set of gestures elicited from a participatory design session with 12 users. We implement a machine learning model in Shoes++ which can recognize two-person and three-person social foot-to-foot gestures with 94.3% and 96.6% accuracies (N=18). In addition, the sole is designed to easily attach to and detach from various daily shoes to support comfortable social foot interaction without taking off the shoes. Based on users' qualitative feedback, we also found that Shoes++ can support team collaboration and enhance emotion expression, thus making social interactions or interpersonal dynamics more engaging in an expanded design space.</jats:p>
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534620
ER  - 

TY  - JOUR
AU  - Knitter, Laura; Jagusch, Konrad; Scharr, Christian; Heinze, Christoph; Sender, Jan; Flügge, Wilko
TI  - Human-in-the-Loop-Ansatz vereinfacht maschinelles Lernen für das Störungsmanagement
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>Die Auswertung sensorischer Daten von stationären Betriebsmitteln in der Produktion bietet Potenziale zur Reduktion störungsbedingter Kosten. Vernetzte Sensoren dienen dabei zur datengetriebenen Fehlerursachenanalysen. In diesem Beitrag wird ein generalisierter Ansatz zur Detektion von anomalen Zuständen auf Basis maschineller Lernalgorithmen und die Lokalisierung der Ursache vorgestellt. Um Aufwände zur datentechnischen Abbildung der Ausfälle zu minimieren, wird eine Datengenerierung durch den Bediener herangezogen. Eine Validierung des Ansatzes zeigt, dass unüberwachte Lernmethoden Potenziale zur Erkennung von Störungen selbst auf Basis kleiner Datensätze aufweisen.</jats:p>
SP  - 623
EP  - 628
JF  - Zeitschrift für wirtschaftlichen Fabrikbetrieb
VL  - 117
IS  - 10
PB  - 
DO  - 10.1515/zwf-2022-1131
ER  - 

TY  - NA
AU  - Bala, Paulo; Oakley, Ian; Nisi, Valentina; Nunes, Nuno Jardim
TI  - CHI - Dynamic Field of View Restriction in 360° Video: Aligning Optical Flow and Visual SLAM to Mitigate VIMS
PY  - 2021
AB  - Head-Mounted Display based Virtual Reality is proliferating. However, Visually Induced Motion Sickness (VIMS), which prevents many from using VR without discomfort, bars widespread adoption. Prior work has shown that limiting the Field of View (FoV) can reduce VIMS at a cost of also reducing presence. Systems that dynamically adjust a user’s FoV may be able to balance these concerns. To explore this idea, we present a technique for standard 360° video that shrinks FoVs only during VIMS inducing scenes. It uses Visual Simultaneous Localization and Mapping and peripheral optical flow to compute camera movements and reduces FoV during rapid motion or optical flow. A user study (N=23) comparing 360° video with unrestricted-FoVs (90°), reduced fixed-FoVs (40°) and dynamic-FoVs (40°-90°) revealed that dynamic-FoVs mitigate VIMS while maintaining presence. We close by discussing the user experience of dynamic-FoVs and recommendations for how they can help make VR comfortable and immersive for all.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445499
ER  - 

TY  - NA
AU  - Singh, Abbey; Kaur, Ramanpreet; Haltner, Peter; Peachey, Matthew; Gonzalez-Franco, Mar; Malloch, Joseph; Reilly, Derek
TI  - VR - Story CreatAR: a Toolkit for Spatially-Adaptive Augmented Reality Storytelling
PY  - 2021
AB  - Headworn Augmented Reality (AR) and Virtual Reality (VR) displays are an exciting new medium for locative storytelling. Authors face challenges planning and testing the placement of story elements when the story is experienced in multiple locations or the environment is large or complex. We present Story CreatAR, the first locative AR/VR authoring tool that integrates spatial analysis techniques. Story CreatAR is designed to help authors think about, experiment with, and reflect upon spatial relationships between story elements, and between their story and the environment. We motivate and validate our design through developing different locative AR/VR stories with several authors.
SP  - 713
EP  - 722
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00098
ER  - 

TY  - NA
AU  - Lu, Feiyu; Davari, Shakiba; Bowman, Doug A.
TI  - SUI - Exploration of Techniques for Rapid Activation of Glanceable Information in Head-Worn Augmented Reality
PY  - 2021
AB  - Future augmented reality (AR) glasses may provide pervasive and continuous access to everyday information. However, it remains unclear how to address the issue of virtual information overlaying and occluding real-world objects and information that are of interest to users. One approach is to keep virtual information sources inactive until they are explicitly requested, so that the real world remains visible. In this research, we explored the design of interaction techniques with which users can activate virtual information sources in AR. We studied this issue in the context of Glanceable AR, in which virtual information resides at the periphery of the user’s view. We proposed five techniques and evaluated them in both sitting and walking scenarios. Our results demonstrate the usability, user preference, and social acceptance of each technique, as well as design recommendations to achieve optimal performance. Our findings can inform the design of lightweight techniques to activate virtual information displays in future everyday AR interfaces.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485286
ER  - 

TY  - CHAP
AU  - Kastner, Marvin; Grasse, Ole; Jahn, Carlos
TI  - Container Flow Generation for Maritime Container Terminals
PY  - 2022
AB  - NA
SP  - 133
EP  - 143
JF  - Dynamics in Logistics
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05359-7_11
ER  - 

TY  - NA
AU  - Kienzle, Wolf; Whitmire, Eric; Rittaler, Chris; Benko, Hrvoje
TI  - CHI - ElectroRing: Subtle Pinch and Touch Detection with a Ring
PY  - 2021
AB  - We present ElectroRing, a wearable ring-based input device that reliably detects both onset and release of a subtle finger pinch, and more generally, contact of the fingertip with the user’s skin. ElectroRing addresses a common problem in ubiquitous touch interfaces, where subtle touch gestures with little movement or force are not detected by a wearable camera or IMU. ElectroRing’s active electrical sensing approach provides a step-function-like change in the raw signal, for both touch and release events, which can be easily detected using only basic signal processing techniques. Notably, ElectroRing requires no second point of instrumentation, but only the ring itself, which sets it apart from existing electrical touch detection methods. We built three demo applications to highlight the effectiveness of our approach when combined with a simple IMU-based 2D tracking system.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445094
ER  - 

TY  - CHAP
AU  - Krishna, Ranjay; Gordon, Mitchell; Fei-Fei, Li; Bernstein, Michael S.
TI  - Visual Intelligence through Human Interaction
PY  - 2021
AB  - Over the last decade, Computer Vision, the branch of Artificial Intelligence aimed at understanding the visual world, has evolved from simply recognizing objects in images to describing pictures, answering questions about images, aiding robots maneuver around physical spaces, and even generating novel visual content. As these tasks and applications have modernized, so too has the reliance on more data, either for model training or for evaluation. In this chapter, we demonstrate that novel interaction strategies can enable new forms of data collection and evaluation for Computer Vision. First, we present a crowdsourcing interface for speeding up paid data collection by an order of magnitude, feeding the data-hungry nature of modern vision models. Second, we explore a method to increase volunteer contributions using automated social interventions. Third, we develop a system to ensure human evaluation of generative vision models are reliable, affordable, and grounded in psychophysics theory. We conclude with future opportunities for Human–Computer Interaction to aid Computer Vision.
SP  - 257
EP  - 314
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_9
ER  - 

TY  - JOUR
AU  - Young, Sierra N.; Lanciloti, Ryan J.; Peschel, Joshua M.
TI  - The Effects of Interface Views on Performing Aerial Telemanipulation Tasks Using Small UAVs
PY  - 2021
AB  - This paper presents a human-robot interaction (HRI) study of a dedicated Mission Specialist interface for performing telemanipulation tasks using a small unoccupied aerial vehicle (UAV). Current literature suggests that the successful completion of aerial manipulation tasks in real-world environments requires human input due to challenges in autonomous perception and control. Visual information of the remote environment in a telemanipulation interface can significantly affect performance under direct control; however, the effects of interface visualizations on task performance have not been studied for UAV telemanipulation. This work evaluated the effects of interface viewpoint on aerial manipulation task performance. The interfaces evaluated in this study included video streams from cameras located onboard the UAV, including: (i) a manipulator egocentric view, (ii) a manipulator exocentric view, and (iii) a combination of egocentric and exocentric views. A total of 36 participants completed three different manipulation tasks using all three interface conditions. The observations and results showed that both the exocentric and mixed view configurations contributed to improved task performance over an egocentric-only interface. Further, this study resulted in data regarding view use, view effectiveness, and task type that can be used for further developing interfacing for aerial manipulators that change and adapt to the environment and task.
SP  - 1
EP  - 16
JF  - International Journal of Social Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mahadevan, Karthik; Sousa, Maurício; Tang, Anthony; Grossman, Tovi
TI  - CHI - “Grip-that-there”: An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration
PY  - 2021
AB  - In ad-hoc human-robot collaboration (HRC), humans and robots work on a task without pre-planning the robot's actions prior to execution; instead, task allocation occurs in real-time. However, prior research has largely focused on task allocations that are pre-planned - there has not been a comprehensive exploration or evaluation of techniques where task allocation is adjusted in real-time. Inspired by HCI research on territoriality and proxemics, we propose a design space of novel task allocation techniques including both explicit techniques, where the user maintains agency, and implicit techniques, where the efficiency of automation can be leveraged. The techniques were implemented and evaluated using a tabletop HRC simulation in VR. A 16-participant study, which presented variations of a collaborative block stacking task, showed that implicit techniques enable efficient task completion and task parallelization, and should be augmented with explicit mechanisms to provide users with fine-grained control.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445355
ER  - 

TY  - JOUR
AU  - Gu, Yizheng; Yu, Chun; Li, Zhipeng; Li, Zhaoheng; Xiaoying, Wei; Shi, Yuanchun
TI  - QwertyRing: Text Entry on Physical Surfaces Using a Ring
PY  - 2020
AB  - The software keyboard is widely used on digital devices such as smartphones, computers, and tablets. The software keyboard operates via touch, which is efficient, convenient, and familiar to users. However, some emerging technology devices such as AR/VR headsets and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing, a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement Unit) ring. Users wear the ring on the middle phalanx of the index finger and type on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing, users do not focus on monitoring the hand motions. They receive text feedback on a separate screen, e.g., an AR/VR headset or a digital device display, such as a computer monitor. The basic idea of QwertyRing is to detect touch events and predict users' desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing through a five-day user study. Participants achieved a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based techniques [24, 30, 45, 68] and is 86.48% of the speed of typing on a smartphone with an index finger. The results show that QwertyRing enables efficient touch-based text entry on physical surfaces.
SP  - 1
EP  - 29
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432204
ER  - 

TY  - NA
AU  - Endow, Shreyosi; Moradi, Hedieh; Srivastava, Anvay; Noya, Esau G; Torres, César I.
TI  - Conference on Designing Interactive Systems - Compressables: A Haptic Prototyping Toolkit for Wearable Compression-based Interfaces
PY  - 2021
AB  - Compression-based haptic feedback has been used in wearables to issue notifications, provide therapeutic effects, and create immersive storytelling environments. Such worn devices are well studied on the wrists, arms, and head, however, many unconventional yet context-rich areas of the body remain underexplored. Current haptic prototyping techniques have large instrumentation costs, requiring the design of bespoke embedded devices that do not have the flexibility to be applied to other body sites. In this work, we introduce an open-source prototyping toolkit for designing, fabricating, and programming wearable compression-based interfaces, or compressables. Our approach uses a lost-PVA technique for making custom inflatable silicone bladders, an off-the-shelf pneumatics controller, and a mobile app to explore and tune haptic interactions through sketch gestures. We validate the toolkit’s open-endedness through a user study and heuristic evaluation. We use exemplar artifacts to annotate the design space of compressables and discuss opportunities to further expand haptic expression on the body.
SP  - 1101
EP  - 1114
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462057
ER  - 

TY  - NA
AU  - Cao, Yuanzhi; Qian, Xun; Wang, Tianyi; Lee, Rachel; Huo, Ke; Ramani, Karthik
TI  - CHI - An Exploratory Study of Augmented Reality Presence for Tutoring Machine Tasks
PY  - 2020
AB  - Machine tasks in workshops or factories are often a compound sequence of local, spatial, and body-coordinated human-machine interactions. Prior works have shown the merits of video-based and augmented reality (AR) tutoring systems for local tasks. However, due to the lack of a bodily representation of the tutor, they are not as effective for spatial and body-coordinated interactions. We propose avatars as an additional tutor representation to the existing AR instructions. In order to understand the design space of tutoring presence for machine tasks, we conduct a comparative study with 32 users. We aim to explore the strengths/limitations of the following four tutor options: video, non-avatar-AR, half-body+AR, and full-body+AR. The results show that users prefer the half-body+AR overall, especially for the spatial interactions. They have a preference for the full-body+AR for the body-coordinated interactions and the non-avatar-AR for the local interactions. We further discuss and summarize design recommendations and insights for future machine task tutoring systems.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376688
ER  - 

TY  - NA
AU  - Purnendu, NA; Novack, Sasha M; Acome, Eric; Keplinger, Christoph; Alistar, Mirela; Gross, Mark D.; Bruns, Carson J.; Leithinger, Daniel
TI  - Conference on Designing Interactive Systems - Electriflow: Soft Electrohydraulic Building Blocks for Prototyping Shape-changing Interfaces
PY  - 2021
AB  - We present Electriflow: a new class of soft electrohydraulic actuators as building blocks for prototyping shape-changing interfaces. These actuators are silent and fast in operation and can be fabricated with commodity materials. Electriflow generates an immediate hydraulic force upon electrostatic activation without an external fluid supply source, enabling a simple and compact self-contained design. This paper describes the materials and mechanisms of these shape-changing building blocks, as well as the underlying fabrication process, which includes a software tool that assists in their design, shape visualization and construction. Finally, we explore four classes of application prototypes: tangible animation, actuating origami creases, shape-changing phone, and shape-changing bowl.
SP  - 1280
EP  - 1290
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462093
ER  - 

TY  - NA
AU  - Shimizu, Shuntaro; Hashimoto, Takeru; Yoshida, Shigeo; Matsumura, Reo; Narumi, Takuji; Kuzuoka, Hideaki
TI  - VR - Unident: Providing Impact Sensations on Handheld Objects via High-Speed Change of the Rotational Inertia
PY  - 2021
AB  - Several virtual reality (VR) proxies have been developed that can emulate impact sensations by generating actual forces on the hand. Although these proxies contribute to increasing the reality of VR, they still have some limitations, such as high latency, high power consumption, and low frequency to provide impact sensations. To overcome these limitations, we first propose a method to provide an impact sensation without actual force generation by quickly changing the rotational inertia of a handheld proxy while users are swinging it. Then, we developed Unident, a handheld proxy capable of changing its rotational inertia by moving a weight along one axis at a high speed. Two experiments were conducted to evaluate the ability of Unident to provide users with impact sensations. In the first experiment, we demonstrate that Unident can physically provide an impact sensation applied to a handheld object by analyzing the pressure on the user's palm. The second experiment shows that Unident can provide an impact sensation with various magnitudes depending on the amount of rotational inertia to be changed. Finally, we present an application that can be enabled by Unident.
SP  - 11
EP  - 20
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00021
ER  - 

TY  - JOUR
AU  - Assaf, Tareq
TI  - Assessing a new Frequency Modulation architecture for artificial large area skin-like sensory array deployment in robotic platforms
PY  - 2022
AB  - This work presents a new architecture to deploy large-area artificial skin to enhance the spatial awareness of robotic and prosthetic platforms. Adding sensing information will improve robots’ safety both around humans and in unstructured environments. Unlike biological systems, current artificial platforms lack a fully-integrated skin-like layer that would significantly improve dexterity and capabilities. However, artificial skin is limited by the technical issues of scaling power to and communication from many sensors. Many current solutions are either relatively low-power with limited sensor type e.g. contact only, or capable of multiple sensor type readings at the expense of requiring higher power. Their data acquisition and sampling rates are also constrained by the technique used to transfer them. The solution to address these problems uses frequency modulation (FM) to encode many signals on a single wire. This solution is applied as an internal robotic communication protocol. This work describes the design rational and experimentally validates the concept within the frequency range of tactile mechanoreceptors. The proposed architecture has the potential to enable large-area skin deployment.
SP  - 100252
EP  - 100252
JF  - Array
VL  - 16
IS  - NA
PB  - 
DO  - 10.1016/j.array.2022.100252
ER  - 

TY  - NA
AU  - Qin, Fang; Cheng, Huai-Yu; Sneeringer, Rachel; Vlachostergiou, Maria; Acharya, Sampada; Liu, Haolin; Majidi, Carmel; Islam, Mohammad; Yao, Lining
TI  - CHI Extended Abstracts - ExoForm: Shape Memory and Self-Fusing Semi-Rigid Wearables
PY  - 2021
AB  - Semi-rigid and rigid structures have been utilized in many on-body applications including musculoskeletal support (e.g., braces and splints). However, most of these support structures are not very compliant, so effortless custom fitting becomes a unique design challenge. Furthermore, the weight and space needed to transport these structures impede adoption in mobile environments. Here, we introduce ExoForm, a compact, customizable, and semi-rigid wearable material system with self-fusing edges that can semi-autonomously assemble on-demand while providing integrated sensing, control, and mobility. We present a comprehensive and holistic engineering strategy that includes optimized material composition, computationally-guided design and fabrication, semi-autonomous self-morphing assembly and fusing steps, heating control, and sensing for our easy-to-wear ExoForm. Finally, we fabricate wearable braces using the ExoForm method as a demonstration along with preliminary evaluation of ExoForm's performance.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451818
ER  - 

TY  - NA
AU  - Ipsita, Ananya; Li, Hao; Duan, Runlin; Cao, Yuanzhi; Chidambaram, Subramanian; Liu, Min; Ramani, Karthik
TI  - CHI Extended Abstracts - VRFromX: From Scanned Reality to Interactive Virtual Experience with Human-in-the-Loop
PY  - 2021
AB  - There is an increasing trend of Virtual-Reality (VR) applications found in education, entertainment, and industry. Many of them utilize real world tools, environments, and interactions as bases for creation. However, creating such applications is tedious, fragmented, and involves expertise in authoring VR using programming and 3D-modelling softwares. This hinders VR adoption by decoupling subject matter experts from the actual process of authoring while increasing cost and time. We present VRFromX, an in-situ Do-It-Yourself (DIY) platform for content creation in VR that allows users to create interactive virtual experiences. Using our system, users can select region(s) of interest (ROI) in scanned point cloud or sketch in mid-air using a brush tool to retrieve virtual models and then attach behavioral properties to them. We ran an exploratory study to evaluate usability of VRFromX and the results demonstrate feasibility of the framework as an authoring tool. Finally, we implemented three possible use-cases to showcase potential applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451747
ER  - 

TY  - JOUR
AU  - Fan, Neil Xu; Xiao, Robert
TI  - Reducing the Latency of Touch Tracking on Ad-hoc Surfaces
PY  - 2022
AB  - <jats:p>Touch sensing on ad-hoc surfaces has the potential to transform everyday surfaces in the environment - desks, tables and walls - into tactile, touch-interactive surfaces, creating large, comfortable interactive spaces without the cost of large touch sensors. Depth sensors are a promising way to provide touch sensing on arbitrary surfaces, but past systems have suffered from high latency and poor touch detection accuracy. We apply a novel state machine-based approach to analyzing touch events, combined with a machine-learning approach to predictively classify touch events from depth data with lower latency and higher touch accuracy than previous approaches. Our system can reduce end-to-end touch latency to under 70ms, comparable to conventional capacitive touchscreens. Additionally, we open-source our dataset of over 30,000 touch events recorded in depth, infrared and RGB for the benefit of future researchers.</jats:p>
SP  - 489
EP  - 499
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567730
ER  - 

TY  - NA
AU  - Yan, Xin; Lu, Lin; Sharf, Andrei; Yu, Xing; Sun, Yulu
TI  - Man-made by Computer: On-the-Fly Fine Texture 3D Printing
PY  - 2021
AB  - Applying textures to 3D models are means for creating realistic looking objects. This is especially important in the 3D manufacturing domain as manufactured models should ideally comprise a natural and realistic appearance. Nevertheless, natural material textures usually consist of dense patterns and fine details. Their embedding onto 3D models is typically cumbersome, requiring large processing time and resulting in large size meshes. This paper presents a novel approach for direct embedding of fine scale geometric textures onto 3D printed models by on-the-fly modification of the 3D printer’s head. Our idea is to embed 3D textures by revising the 3D printer’s G-code, i.e., incorporating texture details through modification of the printer’s path. Direct manipulation of the printer’s head movement allows for fine-scale texture mapping and editing on-the-fly in the 3D printing process. Thus, our method avoids the computationally expensive texture mapping, mesh processing and manufacturing preprocessing. This allows embedding detailed geometric textures of unlimited density which can model manual manufacturing artifacts and natural material properties. Results demonstrate that our direct G-code textured models are printed robustly and efficiently in both space and time compared to traditional methods.
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485114.3485119
ER  - 

TY  - NA
AU  - Gruen, Robert; Ofek, Eyal; Steed, Anthony; Gal, Ran; Sinclair, Mike; Gonzalez-Franco, Mar
TI  - VR - Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices
PY  - 2020
AB  - Measuring Visual Latency in VR and AR devices has become increasingly complicated as many of the components will influence others in multiple loops and ultimately affect the human cognitive and sensory perception. In this paper we present a new method based on the idea that the performance of humans on a rapid motor task will remain constant, and that any added delay will correspond to the system latency. We ask users to perform a task inside different video see-through devices and also in front of a computer. We also calculate the latency of the systems using a hardware instrumentation-based measurement technique for bench-marking. Results show that this new form of latency measurement through human cognitive performance can be reliable and comparable to hardware instrumentation-based measurement. Our method is adaptable to many forms of user interaction. It is particularly suitable for systems, such as AR and VR, where externalizing signals is difficult, or where it is important to measure latency while the system is in use by a user.
SP  - 791
EP  - 799
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1580498468656
ER  - 

TY  - NA
AU  - Terzioglu, Yunus; Murali, Prasanth; Kimani, Everlyne; Bickmore, Timothy
TI  - Sharing the Spotlight: Co-presenting with a Humanoid Robot
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/hri53351.2022.9889400
ER  - 

TY  - NA
AU  - O'Leary, Jasper Tran; Nandi, Chandrakana; Lee, Khang; Peek, Nadya
TI  - UIST - Taxon: a Language for Formal Reasoning with Digital Fabrication Machines
PY  - 2021
AB  - Digital fabrication machines for makers have expanded access to manufacturing processes such as 3D printing, laser cutting, and milling. While digital models encode the data necessary for a machine to manufacture an object, understanding the trade-offs and limitations of the machines themselves is crucial for successful production. Yet, this knowledge is not codified and must be gained through experience, which limits both adoption of and creative exploration with digital fabrication tools. To formally represent machines, we present Taxon, a language that encodes a machine’s high-level characteristics, physical composition, and performable actions. With this programmatic foundation, makers can develop rules of thumb that filter for appropriate machines for a given job and verify that actions are feasible and safe. We integrate the language with a browser-based system for simulating and experimenting with machine workflows. The system lets makers engage with rules of thumb and enrich their understanding of machines. We evaluate Taxon by representing several machines from both common practice and digital fabrication research. We find that while Taxon does not exhaustively describe all machines, it provides a starting point for makers and HCI researchers to develop tools for reasoning about and making decisions with machines.
SP  - 691
EP  - 709
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474779
ER  - 

TY  - NA
AU  - Kovacs, Robert; Ofek, Eyal; Franco, Mar Gonzalez; Siu, Alexa F.; Marwecki, Sebastian; Holz, Christian; Sinclair, Mike
TI  - UIST - Haptic PIVOT: On-Demand Handhelds in VR
PY  - 2020
AB  - We present PIVOT, a wrist-worn haptic device that renders virtual objects into the user's hand on demand. Its simple design comprises a single actuated joint that pivots a haptic handle into and out of the user's hand, rendering the haptic sensations of grasping, catching, or throwing an object anywhere in space. Unlike existing hand-held haptic devices and haptic gloves, PIVOT leaves the user's palm free when not in use, allowing users to make unencumbered use of their hand. PIVOT also enables rendering forces acting on the held virtual objects, such as gravity, inertia, or air-drag, by actively driving its motor while the user is firmly holding the handle. When wearing a PIVOT device on both hands, they can add haptic feedback to bimanual interaction, such as lifting larger objects. In our user study, participants (n=12) evaluated the realism of grabbing and releasing objects of different shape and size with mean score 5.19 on a scale from 1 to 7, rated the ability to catch and throw balls in different directions with different velocities (mean=5.5), and verified the ability to render the comparative weight of held objects with 87% accuracy for ~100g increments.
SP  - 1046
EP  - 1059
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415854
ER  - 

TY  - NA
AU  - Winters, Amy; Bekkers, Iris; Nayeb Ghanbar Hosseini, Dorsa; Vekemans, Verindi; Weima, Samuël; Bruns, Miguel
TI  - Dynamic Robotic Fibers: Liquid Crystal Elastomers for Programmable and Reversible Shape-Changing Behaviors
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519769
ER  - 

TY  - JOUR
AU  - Helfer, Sven; Kümmel, Michéle; Bathelt, Franziska; Sedlmayr, Martin
TI  - GMDS - Generating Enriched Synthetic German Hospital Claims Data - A Use Case Driven Approach.
PY  - 2021
AB  - Clinical data and above all individual patient data are highly sensitive. All the more it is important to protect these critical information while analyzing and exploring their specifics for further research. However, in order to enable students and other researchers to develop decision support systems and to use modern data analysis methods such as intelligent pattern recognition, the provision of clinical data is essential. In order to allow this while completely protecting the privacy of a patient, we present a mixed approach to generate semantically and clinically realistic data: (1) We use available synthetic data, extract information on patient visits and diagnoses and adapt them to the encoding systems of German claims data; (2) based on a statistical analysis of real German hospital data, we identify distributions of procedures, laboratory data and other measurements and transfer them to the synthetic patient's visits and diagnoses in a semi-automated way. This enabled us to provide students a data set that is as semantically and clinically realistic as possible to apply patient-level prediction algorithms within the development of clinical decision support systems without putting patient data at any risk.
SP  - 58
EP  - 65
JF  - Studies in health technology and informatics
VL  - 278
IS  - NA
PB  - 
DO  - 10.3233/shti210051
ER  - 

TY  - JOUR
AU  - Kim, You-Jin; Kumaran, Radha; Sayyad, Ehsan; Milner, Anne; Bullock, Tom; Giesbrecht, Barry; Hollerer, Tobias
TI  - Investigating Search Among Physical and Virtual Objects Under Different Lighting Conditions.
PY  - 2022
AB  - By situating computer-generated content in the physical world, mobile augmented reality (AR) can support many tasks that involve effective search and inspection of physical environments. Currently, there is limited information regarding the viability of using AR in realistic wide-area outdoor environments and how AR experiences affect human behavior in these environments. Here, we conducted a wide-area outdoor AR user study ($n=48$) using a commercially available AR headset (Microsoft Hololens 2) to compare (1) user interactions with physical and virtual objects in the environment (2) the effects of different lighting conditions on user behavior and AR experience and (3) the impact of varying cognitive load on AR task performance. Participants engaged in a treasure hunt task where they searched for and classified virtual target items (green "gems") in an augmented outdoor courtyard scene populated with physical and virtual objects. Cognitive load was manipulated so that in half the search trials users were required to monitor an audio stream and respond to specific target sounds. Walking paths, head orientation and eye gaze information were measured, and users were queried about their memory of encountered objects and provided feedback on the experience. Key findings included (1) Participants self-reported significantly lower comfort in the ambient natural light condition, with virtual objects more visible and participants more likely to walk into physical objects at night; (2) recall for physical objects was worse than for virtual objects, (3) participants discovered more gems hidden behind virtual objects than physical objects, implying higher attention on virtual objects and (4) dual-tasking modified search behavior. These results suggest there are important technical, perceptual and cognitive factors that must be considered if the full potential of "anywhere and anytime mobile AR" is to be realized.
SP  - 3788
EP  - 3798
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203093
ER  - 

TY  - NA
AU  - Li, Yi-Jun; Jin, De-Rong; Wang, Miao; Chen, Jun-Long; Steinicke, Frank; Hu, Shi-Min; Zhao, Qinping
TI  - VR - Detection Thresholds with Joint Horizontal and Vertical Gains in Redirected Jumping
PY  - 2021
AB  - Redirected jumping (RDJ) is a locomotion technique that allows users to explore a virtual space that is larger than the available physical space by imperceptibly manipulating users' virtual viewpoints according to different gains. In previous redirected jumping work, different types of gains were imposed separately, without considering the possible interaction effects of horizontal and vertical gains on the jumping distance perception. To figure out how humans perceive distance manipulation when more than one gain is used, in this paper, we explored joint horizontal and vertical gains that manipulate horizontal and vertical distances at the same time during two-legged takeoff jumping in the virtual space. We estimated and analyzed horizontal and vertical detection thresholds by conducting a user study, fitting the data to two-dimensional psychometric functions, and visualizing the fitted 3D plots. We provided quantitative insights into the effects of joint gains on detection thresholds, where the imperceptible range for one gain can be affected by the variation of the other gain. Finally, we designed redirected jumping-based games as applications with joint horizontal and vertical gains and demonstrated the effectiveness of the redirected jumping technique.
SP  - 95
EP  - 102
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00030
ER  - 

TY  - NA
AU  - Li, Tianyu; Liu, Yue; Ma, Shining; Hu, Mingwei; Liu, Tong; Song, Weitao
TI  - NailRing: An Intelligent Ring for Recognizing Micro-gestures in Mixed Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00032
ER  - 

TY  - NA
AU  - Davari, Shakiba; Lu, Feiyu; Bowman, Doug A.
TI  - Validating the Benefits of Glanceable and Context-Aware Augmented Reality for Everyday Information Access Tasks
PY  - 2022
AB  - Glanceable Augmented Reality interfaces have the potential to provide fast and efficient information access for the user. However, where to place the virtual content and how to access them depend on the user context. We designed a Context-Aware AR interface that can intelligently adapt for two different contexts: solo and social. We evaluated information access using Context-Aware AR compared to current mobile phones and non-adaptive Glanceable AR interfaces. We found that in a solo scenario, compared to a mobile phone, the Context-Aware AR interface was preferred, easier, and significantly faster; it improved the user experience; and it allowed the user to better focus on their primary task. In the social scenario, we discovered that the mobile phone was slower, more intrusive, and perceived as the most difficult. Meanwhile, Context-Aware AR was faster for responding to information needs triggered by the conversation; it was preferred and perceived as the easiest for resuming conversation after information access; and it improved the user&#x2019;s awareness of the other person&#x2019;s facial expressions.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00063
ER  - 

TY  - NA
AU  - Shen, Junxiao; Hu, Jinghui; Dudley, John J.; Kristensson, Per Ola
TI  - Personalization of a Mid-Air Gesture Keyboard using Multi-Objective Bayesian Optimization
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00088
ER  - 

TY  - NA
AU  - Odoemelem, Henry; Van Laerhoven, Kristof
TI  - UbiComp/ISWC Adjunct - A low-cost prototyping framework for human-robot desk interaction
PY  - 2020
AB  - Many current human-robot interactive systems tend to use accurate and fast - but also costly - actuators and tracking systems to establish working prototypes that are safe to use and deploy for user studies. This paper presents an embedded framework to build a desktop space for human-robot interaction, using an open-source robot arm, as well as two RGB cameras connected to a Raspberry Pi-based controller that allow a fast yet low-cost object tracking and manipulation in 3D. We show in our evaluations that this facilitates prototyping a number of systems in which user and robot arm can commonly interact with physical objects.
SP  - 191
EP  - 194
JF  - Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410530.3414323
ER  - 

TY  - NA
AU  - Nakagaki, Ken; Leong, Joanne; Tappa, Jordan L.; Wilbert, João; Ishii, Hiroshi
TI  - UIST - HERMITS: Dynamically Reconfiguring the Interactivity of Self-propelled TUIs with Mechanical Shell Add-ons
PY  - 2020
AB  - We introduce HERMITS, a modular interaction architecture for self-propelled Tangible User Interfaces (TUIs) that incorporates physical add-ons, referred to as mechanical shells. The mechanical shell add-ons are intended to be dynamically reconfigured by utilizing the locomotion capability of self-propelled TUIs (e.g. wheeled TUIs, swarm UIs). We developed a proof-of-concept system that demonstrates this novel architecture using two-wheeled robots and a variety of mechanical shell examples. These mechanical shell add-ons are passive physical attatchments that extend the primitive interactivities (e.g. shape, motion and light) of the self-propelled robots. The paper proposes the architectural design, interactive functionality of HERMITS as well as design primitives for mechanical shells. The paper also introduces the prototype implementation that is based on an off-the-shelf robotic toy with a modified docking mechanism. A range of applications is demonstrated with the prototype to motivate the collective and dynamically reconfigurable capability of the modular architecture, such as an interactive mobility simulation, an adaptive home/desk environment, and a story-telling narrative. Lastly, we discuss the future research opportunity of HERMITS to enrich the interactivity and adaptability of actuated and shape changing TUIs.
SP  - 882
EP  - 896
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415831
ER  - 

TY  - JOUR
AU  - Cofer, Savannah; Chen, Tyler N.; Yang, Jackie; Follmer, Sean
TI  - Detecting Touch and Grasp Gestures Using a Wrist-Worn Optical and Inertial Sensing Network
PY  - 2022
AB  - Freehand gesture-based interaction promises to enable rich interaction in applications such as augmented reality, virtual reality, human-robot interaction, and robotic prosthetic devices. However, current sensing approaches are limited to mid-air whole hand gestures and fail to identify small-scale tactile interactions with unsensorized environments. Detecting tactile interactions may unlock potential new applications in augmented reality and human-robot interaction in which unsensorized surfaces are used as touch input devices. This work presents a novel wrist-worn sensing device that combines near-infrared and inertial measurement unit sensing to enable high-accuracy detection of surface touch and grasp interactions. Two convolutional neural networks were used to map device inputs to detect touch events, and classify them by gesture type or direction. We evaluated the accuracy and temporal precision of our system for event detection and classification. Results from an in-lab user study of 12 participants showed an average of 97% touch detection accuracy and 98% grasp detection accuracy. In our study, we found that near-infrared and inertial sensing are complementary and can be used in tandem to effectively address both touch event detection and directionality classification.
SP  - 10842
EP  - 10849
JF  - IEEE Robotics and Automation Letters
VL  - 7
IS  - 4
PB  - 
DO  - 10.1109/lra.2022.3191173
ER  - 

TY  - JOUR
AU  - Nakamura, Fumihiko; Verhulst, Adrien; Sakurada, Kuniharu; Fukuoka, Masaaki; Sugimoto, Maki
TI  - Evaluation of Spatial Directional Guidance Using Cheek Haptic Stimulation in a Virtual Environment
PY  - 2022
AB  - <jats:p>Spatial cues play an important role in navigating people in both physical and virtual spaces. In spatial navigation, visual information with additional cues, such as haptic cues, enables effective guidance. Most haptic devices are applied to various body parts to make mechanical stimuli, while few devices stimulate a head despite the excellent sensitivity. This article presents Virtual Whiskers, a spatial directional guidance technique by cheek haptic stimulation using tiny robot arms attached to a Head-Mounted Display (HMD). The tip of the robotic arm has photo reflective sensors to detect the distance between the tip and the cheek surface. Using the robot arms, we stimulate a point on the cheek obtained by calculating an intersection between the cheek surface and the target direction. In the directional guidance experiment, we investigated how accurately participants identify the target direction provided by our guidance method. We evaluated an error between the actual target direction and the participant's pointed direction. The experimental result shows that our method achieves the average absolute directional error of 2.54° in the azimuthal plane and 6.54° in the elevation plane. We also conducted a spatial guidance experiment to evaluate task performance in a target search task. We compared the condition of visual information, visual and audio information, and visual information and cheek haptics for task completion time, System Usability Scale (SUS) score, NASA-TLX score. The averages of task completion time were M = 6.39 s, SD = 3.34 s, and M = 5.62 s, SD = 3.12 s, and M = 4.35 s, SD = 2.26 s, in visual-only condition, visual+audio condition, and visual+haptic condition, respectively. In terms of the SUS score, visual condition, visual+audio condition, and visual+haptic condition achieved M = 55.83, SD = 20.40, and M = 47.78, SD = 20.09, and M = 80.42, SD = 10.99, respectively. As for NASA-TLX score, visual condition, visual+audio condition, and visual+haptic condition resulted in M = 75.81, SD = 16.89, and M = 67.57, SD = 14.96, and M = 38.83, SD = 18.52, respectively. Statistical tests revealed significant differences in task completion time, SUS score, and NASA-TLX score between the visual and the visual+haptic condition and the visual+audio and the visual+haptic condition.</jats:p>
SP  - NA
EP  - NA
JF  - Frontiers in Computer Science
VL  - 4
IS  - NA
PB  - 
DO  - 10.3389/fcomp.2022.733844
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Braud, Tristan; Hosio, Simo; Hui, Pan
TI  - Towards Augmented Reality-driven Human-City Interaction: Current Research on Mobile Headsets and Future Challenges
PY  - 2020
AB  - Interaction design for Augmented Reality (AR) is gaining increasing attention from both academia and industry. This survey discusses 260 articles (68.8% of articles published between 2015 - 2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by a review of the latest trends of information visualization, constrained interfaces, and embodied interaction for AR headsets. We highlight under-explored issues in interface design and input techniques that warrant further research, and conjecture that AR with complementary Conversational User Interfaces (CUIs) is a key enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of AR in Human-City Interaction.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Kadomoto, Junichiro; Sasatani, Takuya; Narumi, Koya; Usami, Naoto; Irie, Hidetsugu; Sakai, Shuichi; Kawahara, Yoshihiro
TI  - Toward Wirelessly Cooperated Shape-Changing Computing Particles
PY  - 2021
AB  - Widespread research is being conducted on computational materials. The challenge for the realization of computational materials is how to weave computers into everyday objects consisting of various shapes and form factors. One idea to achieve this vision is to build physical computational particles that can cooperatively communicate among others and, thereby, change the whole shape like clay. Wireless communication and power transfer are keys to making such computing particles happen. Thus, we introduce an approach to embody these particles using multiple tiny IC chips, which wirelessly cooperate with each other. This article shows the current state-of-the-art wireless communication and power transfer technologies integrated into IC chips to achieve fine-grained shape deformation of computers. Moreover, this article also presents recent trends and future directions in shape-changing human–computer interfaces research.
SP  - 9
EP  - 17
JF  - IEEE Pervasive Computing
VL  - 20
IS  - 03
PB  - 
DO  - 10.1109/mprv.2021.3086035
ER  - 

TY  - NA
AU  - Ryu, Neung; Lee, Woojin; Kim, Myung Jin; Bianchi, Andrea
TI  - UIST - ElaStick: A Handheld Variable Stiffness Display for Rendering Dynamic Haptic Response of Flexible Object
PY  - 2020
AB  - Haptic controllers have an important role in providing rich and immersive Virtual Reality (VR) experiences. While previous works have succeeded in creating handheld devices that simulate dynamic properties of rigid objects, such as weight, shape, and movement, recreating the behavior of flexible objects with different stiffness using ungrounded controllers remains an open challenge. In this paper we present ElaStick, a variable-stiffness controller that simulates the dynamic response resulting from shaking or swinging flexible virtual objects. This is achieved by dynamically changing the stiffness of four custom elastic tendons along a joint that effectively increase and reduce the overall stiffness of a perceived object in 2-DoF. We show that with the proposed mechanism, we can render stiffness with high precision and granularity in a continuous range between 10.8 and 71.5Nmm/degree. We estimate the threshold of the human perception of stiffness with a just-noticeable difference (JND) study and investigate the levels of immersion, realism and enjoyment using a VR application.
SP  - 1035
EP  - 1045
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415862
ER  - 

TY  - CHAP
AU  - Zhang, Xucong; Park, Seonwook; Feit, Anna Maria
TI  - Eye Gaze Estimation and Its Applications
PY  - 2021
AB  - The human eye gaze is an important non-verbal cue that can unobtrusively provide information about the intention and attention of a user to enable intelligent interactive systems. Eye gaze can also be taken as input to systems as a replacement of the conventional mouse and keyboard, and can also be indicative of the cognitive state of the user. However, estimating and applying gaze in real-world applications poses significant challenges. In this chapter, we first review the development of gaze estimation methods in recent years. We especially focus on learning-based gaze estimation methods which benefit from large-scale data and deep learning methods that recently became available. Second, we discuss the challenges of using gaze estimation for real-world applications and our efforts toward making these methods easily usable for the Human-Computer Interaction community. At last, we provide two application examples, demonstrating the use of eye gaze to enable attentive and adaptive interfaces.
SP  - 99
EP  - 130
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_4
ER  - 

TY  - NA
AU  - Lee, Seol-Yee; Molla, Tahmidul Islam; Kao, Cindy Hsin-Liu
TI  - ISWC - A 10-Year Review of the Methods and Purposes of On-Skin Interface Research in ACM SIGCHI
PY  - 2021
AB  - This paper aims to organize research from Association for Computing Machinery Special Interest Group on Computer–Human Interaction (ACM SIGCHI) associated conferences that have significant impact on on-skin interfaces. We performed a systematic review of the ACM database and identified 68 on-skin related publications between January 2010 to May 2021. The publications are categorized, evaluated, and compared over time and by conference. Citation impact of each paper is assessed revealing trends in the field and reflected onto the general community. Through extensive data collection, we present a thorough analysis of on-skin interfaces’ development in the past decade.
SP  - 84
EP  - 90
JF  - 2021 International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460421.3480424
ER  - 

TY  - NA
AU  - Lee, Jaewook; Jin, Fanjie; Kim, Younsoo; Lindlbauer, David
TI  - User Preference for Navigation Instructions in Mixed Reality
PY  - 2022
AB  - Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users&#x2019; visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users&#x2019; contexts.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00102
ER  - 

TY  - JOUR
AU  - Assaf, Tareq
TI  - A Frequency Modulation-Based Taxel Array: A Bio-Inspired Architecture for Large-Scale Artificial Skin.
PY  - 2021
AB  - This work introduces an array prototype based on a Frequency Modulation (FM) encoding architecture to transfer multiple sensor signals on a single wire. The use case presented adopts Hall-effect sensors as an example to represent a much larger range of sensor types (e.g., proximity and temperature). This work aims to contribute to large area artificial skin systems which are a key element to enhance robotic platforms. Artificial skin will allow robotic platforms to have spatial awareness which will make interaction with objects and users safe. The FM-based architecture has been developed to address limitations in large-scale artificial skin scalability. Scalability issues include power requirements; number of wires needed; as well as frequency, density, and sensitivity bottlenecks. In this work, eight sensor signals are simultaneously acquired, transferred on a single wire and decoded in real-time. The overall taxel array current consumption is 36 mA. The work experimentally validates and demonstrates that different input signals can be effectively transferred using this approach minimizing wiring and power consumption of the taxel array. Four different tests using single as well as multiple stimuli are presented. Observations on performances, noise, and taxel array behaviour are reported. The results show that the taxel array is reliable and effective in detecting the applied stimuli.
SP  - 5112
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 21
IS  - 15
PB  - 
DO  - 10.3390/s21155112
ER  - 

TY  - JOUR
AU  - Chen, Taizhou; Li, Tianpei; Yang, Xingyu; Zhu, Kening
TI  - EFRing
PY  - 2022
AB  - <jats:p>We present EFRing, an index-finger-worn ring-form device for detecting thumb-to-index-finger (T2I) microgestures through the approach of electric-field (EF) sensing. Based on the signal change induced by the T2I motions, we proposed two machine-learning-based data-processing pipelines: one for recognizing/classifying discrete T2I microgestures, and the other for tracking continuous 1D T2I movements. Our experiments on the EFRing microgesture classification showed an average within-user accuracy of 89.5% and an average cross-user accuracy of 85.2%, for 9 discrete T2I microgestures. For the continuous tracking of 1D T2I movements, our method can achieve the mean-square error of 3.5% for the generic model and 2.3% for the personalized model. Our 1D-Fitts'-Law target-selection study shows that the proposed tracking method with EFRing is intuitive and accurate for real-time usage. Lastly, we proposed and discussed the potential applications for EFRing.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569478
ER  - 

TY  - NA
AU  - Shultz, Craig; Kim, Daehwa; Ahuja, Karan; Harrison, Chris
TI  - TriboTouch: Micro-Patterned Surfaces for Low Latency Touchscreens
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502069
ER  - 

TY  - JOUR
AU  - Gkoulalas-Divanis, Aris; Vatsalan, Dinusha; Karapiperis, Dimitrios; Kantarcioglu, Murat
TI  - Modern Privacy-Preserving Record Linkage Techniques: An Overview
PY  - 2021
AB  - Record linkage is the challenging task of deciding which records, coming from disparate data sources, refer to the same entity. Established back in 1946 by Halbert L. Dunn, the area of record linkage has received tremendous attention over the years due to its numerous real-world applications, and has led to a plethora of technologies, methods, metrics, and systems. A major direction in record linkage regards methods for linking records in a privacy-preserving manner, where sensitive and personally identifiable information in the records is not leaked as part of the linkage process. In this article, we provide an overview of the large body of research literature in privacy-preserving record linkage, discuss the different generations of techniques that have been proposed, their advantages and limitations, and present a taxonomy as well as an extensive survey on the latest generation of methods. We conclude this work with a roadmap to the new generation of analytics-driven techniques that aims to address some of the major challenges in the field.
SP  - 4966
EP  - 4987
JF  - IEEE Transactions on Information Forensics and Security
VL  - 16
IS  - NA
PB  - 
DO  - 10.1109/tifs.2021.3114026
ER  - 

TY  - JOUR
AU  - Biswas, Nilotpal; Banerjee, Debangshu; Bhattacharya, Samit
TI  - Realistic walking experience for system-automated virtual reality tour
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11042-022-14035-z
ER  - 

TY  - NA
AU  - Muthukumarana, Sachith; Nassani, Alaeddin; Park, Noel; Steimle, Jurgen; Billinghurst, Mark; Nanayakkara, Suranga
TI  - XRtic: A Prototyping Toolkit for XR Applications using Cloth Deformation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00071
ER  - 

TY  - NA
AU  - Cao, Yuanzhi; Fuste, Anna; Heun, Valentin
TI  - MobileTutAR: a Lightweight Augmented Reality Tutorial System using Spatially Situated Human Segmentation Videos
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519639
ER  - 

TY  - NA
AU  - Hempel, Brian; Chugh, Ravi
TI  - VL/HCC - Tiny Structure Editors for Low, Low Prices! (Generating GUIs from toString Functions)
PY  - 2020
AB  - Writing toString functions to display custom data values is straightforward, but building custom interfaces to manipulate such values is more difficult. Though tolerable in many scenarios, this difficulty is acute in emerging value-centric IDEs—such as those that provide programming by examples (PBE) or bidirectional transformation (BX) modalities, in which users manipulate output values to specify programs.We present an approach that automatically generates custom GUIs from ordinary toString functions. By tracing the execution of the toString function on an input value, our technique overlays a tiny structure editor upon the output string: UI widgets for selecting, adding, removing, and modifying elements of the original value are displayed atop appropriate substrings.We implement our technique—in a tool called TSE—for a simple functional language with custom algebraic data types (ADTs), and evaluate the tiny structure editors produced by TSE on a selection of existing and custom toString functions.
SP  - 1
EP  - 5
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127256
ER  - 

TY  - NA
AU  - McNutt, Andrew; Chugh, Ravi
TI  - Integrated Visualization Editing via Parameterized Declarative Templates
PY  - 2021
AB  - Interfaces for creating visualizations typically embrace one of several common forms. Textual specification enables fine-grained control, shelf building facilitates rapid exploration, while chart choosing promotes immediacy and simplicity. Ideally these approaches could be unified to integrate the user- and usage-dependent benefits found in each modality, yet these forms remain distinct. We propose parameterized declarative templates, a simple abstraction mechanism over JSON-based visualization grammars, as a foundation for multimodal visualization editors. We demonstrate how templates can facilitate organization and reuse by factoring the more than 160 charts that constitute Vega-Lite's example gallery into approximately 40 templates. We exemplify the pliability of abstracting over charting grammars by implementing -- as a template -- the functionality of the shelf builder Polestar (a simulacra of Tableau) and a set of templates that emulate the Google Sheets chart chooser. We show how templates support multimodal visualization editing by implementing a prototype and evaluating it through an approachability study.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445356
ER  - 

TY  - NA
AU  - Mahadevan, Karthik; Chen, Yan; Cakmak, Maya; Tang, Anthony; Grossman, Tovi
TI  - Mimic: In-Situ Recording and Re-Use of Demonstrations to Support Robot Teleoperation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545639
ER  - 

TY  - NA
AU  - Wu, Yifan
TI  - Interaction History for Building Human-Data Interfaces
PY  - 2021
AB  - Author(s): Wu, Yifan | Advisor(s): Hellerstein, Joseph M | Abstract: History provides context for the present. In the same way, past user interactions provide context for present explorations. This thesis investigates ways to reify user interaction history to address emerging challenges in the design and programming of human-data interfaces.We leverage interaction history in three different but connected designs. The first is to enhance the design of interactions that suffer from delays, such as when working with remote databases. We use interaction history as a visual anchor to facilitate concurrent interactions, which ameliorate the cognitive burdens caused by delays. The second is to facilitate the programming of interactive visualizations involving asynchronous communication with remote databases. We capture event histories as a first-class programming construct, allowing the developer to declaratively specify what data to compute and how to update the state of the user interface. This way, developers avoid the low-level details of accessing remote data and coordinating events. The third and last design is to use interaction history to create a bridge between interaction design and programming. We capture and reify interaction history in both computational notebooks and interactive visualizations. Affordances on these reified histories help data scientists move fluidly between the two mediums.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Nikolov, Ivan Adriyanov
TI  - Mindtrek - Testing VR headset cameras for capturing written content
PY  - 2020
AB  - Virtual reality (VR) has become an important tool for providing immersive and collaborative teleprensence experiences. The technology has the possibility of bringing people in different geographical places together in an immersive way and makes sharing knowledge, ideas and experience easier. An important evolution in the immersive experience is the introduction of items from the real world into the virtual environment and the creation of a mixed reality experience. We present a pilot study in the use of the built-in head mounted display (HMD) cameras, for capturing parts of the environment and presenting them in VR. Particular emphasis is put on capturing physical written content on flat surfaces and visualizing it in VR for sharing with other users. We test the HTC Vive and Valve Index cameras and compare them to two external solutions by applying optical character recognition (OCR) on captured and rectified images from them. We demonstrate that the Valve Index camera has the potential to be used for capturing readable text for use in shared VR.
SP  - 153
EP  - 156
JF  - Proceedings of the 23rd International Conference on Academic Mindtrek
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377290.3377315
ER  - 

TY  - JOUR
AU  - Hafner, Christian; Bickel, Bernd
TI  - The design space of plane elastic curves
PY  - 2021
AB  - NA
SP  - 1
EP  - 20
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3476576.3476697
ER  - 

TY  - NA
AU  - Yan, Zeyu; Peng, Huaishu
TI  - UIST - FabHydro: Printing Interactive Hydraulic Devices with an Affordable SLA 3D Printer
PY  - 2021
AB  - We introduce FabHydro, a set of rapid and low-cost methods to prototype interactive hydraulic devices based on an off-the-shelf 3D printer and flexible photosensitive resin. We first present printer settings and custom support structures to warrant the successful print of flexible and deformable objects. We then demonstrate two printing methods to seal the transmission fluid inside these deformable structures: the Submerged Printing process that seals the liquid resin without manual assembly, and the Printing with Plugs method that allows the use of different transmission fluids without modification to the printer. Following the printing methods, we report a design space with a range of 3D printable primitives, including the hydraulic generator, transmitter, and actuator. To demonstrate the feasibility of our approaches and the breadth of new designs that they enable, we showcase a set of examples from a printed robotic gripper that can be operated at a distance to a mobile phone stand that serves as a status reminder by repositioning the user’s phone. We conclude with a discussion of our approach’s limitations and possible future improvements.
SP  - 298
EP  - 311
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474751
ER  - 

TY  - NA
AU  - Zhang, Zhong-Yi; Chen, Hong-Xian; Wang, Shih-Hao; Tsai, Hsin-Ruey
TI  - ELAXO : Rendering Versatile Resistive Force Feedback for Fingers Grasping and Twisting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545677
ER  - 

TY  - BOOK
AU  - Medathati, Naga Venkata Kartheek; Desai, Ruta; Hillis, James M.
TI  - ETRA - Towards inferring cognitive state changes from pupil size variations in real world conditions
PY  - 2020
AB  - The ability to infer cognitive state from pupil size provides an opportunity to reduce friction in human-computer interaction. For example, the computer could automatically turn off notifications when it detects, using pupil size, that the user is deeply focused on a task. However, our ability to do so has been limited. A principal reason for this is that pupil size varies with multiple factors (e.g., luminance and vergence), so isolating variations due to cognitive processes is challenging. In particular, rigorous benchmarks to detect cognitively-driven pupillary event from continuous stream of data in real-world settings have not been well-established. Motivated by these challenges, we first performed visual search experiments at room scale, with natural indoor conditions with real stimuli where the timing of the detection event was user-controlled. In spite of the natural experimental conditions, we found that the mean pupil dilation response to a cognitive state change (i.e., search target detected) was qualitatively similar and consistent with more controlled laboratory studies. Next, to address the challenge of detecting state changes from continuous data, we fit discriminant models using Support Vector Machine (SVM) computed on short epochs of 1-2 seconds extracted using rolling windows. We tested three different features (descriptive statistics, baseline corrected pupil size, and local Z-score) with our models. We obtained best performance using local Z-score as a feature (mean Area under the Curve (AUC) of 0.6). Our naturalistic experiments and modeling results provide a baseline for future research aimed at leveraging pupillometry for real-world applications.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391319
ER  - 

TY  - NA
AU  - Wang, Chiu-Hsuan; Yong, Seraphina; Chen, Hsin-Yu; Ye, Yuan-Syun; Chan, Liwei
TI  - UIST - HMD Light: Sharing In-VR Experience via Head-Mounted Projector for Asymmetric Interaction
PY  - 2020
AB  - We present HMD Light, a proof-of-concept Head-Mounted Display (HMD) implementation that reveals the Virtual Reality (VR) user's experience in the physical environment to facilitate communication between VR and external users in a mobile VR context. While previous work externalized the VR user's experience through an on-HMD display, HMD Light places the display into the physical environment to enable larger display and interaction area. This work explores the interaction design space of HMD Light and presents four applications to demonstrate its versatility. Our exploratory user study observed participant pairs experience applications with HMD Light and evaluated usability, accessibility and social presence between users. From the results, we distill design insights for HMD Light and asymmetric VR collaboration.
SP  - 472
EP  - 486
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415847
ER  - 

TY  - BOOK
AU  - Chung, SeungA; Park, Soobin; Park, Sohyeon; Lee, Kyungyeon; Oh, Uran
TI  - W4A - Improving mealtime experiences of people with visual impairments
PY  - 2021
AB  - A number of studies have been conducted to understand the accessibility issues that people with visual impairments experience. However, most of these are limited to navigation and object recognition tasks. In this study, we focused on providing mealtime assistance. We first conducted a preliminary online survey with 91 participants and an in-depth interview study with eight participants with visual impairments and two social workers. Based on the findings, we identified various difficulties that people with visual impairments face during mealtimes, and types of dish-related information they wish to get before and during meals. To understand the implications for designing the interaction of a meal assistance system for people with visual impairments, we then implemented a prototype in a virtual environment and conducted another user study with 7 participants for evaluation. Reflecting the findings, we suggest design recommendations for a future meal assistance system for people with visual impairments so that they can enjoy their meals independently.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Web for All Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430263.3452421
ER  - 

TY  - NA
AU  - Lu, Chin-Yuan; Hsieh, Han-Wei; Liang, Rong-Hao; Lee, Chi-Jung; Yang, Ling-Chien; Xue, Mengru; Guo, Jr-Ling; Hsieh, Meng-Ju; Chen, Bing-Yu
TI  - CHI - Combining Touchscreens with Passive Rich-ID Building Blocks to Support Context Construction in Touchscreen Interactions
PY  - 2021
AB  - This research investigates the design space of combining touchscreens with passive rich-ID building block systems to support the physical construction of contexts in touchscreen interactions. With two proof-of-concept systems, RFIPillars and RFITiles, we explore various schemes for using tangible inputs for context enrichment in touchscreen interactions. Instead of incorporating an electronic touchscreen module that requires per-module maintenance, this work intentionally makes each tangible object passive. We explore rear-projection solutions to integrate touchscreen interactions into these passive building blocks with capacitive touch sensing techniques and deliberate physical forgiving to retain the merits of being both batteryless and wireless. The presented research artifacts embody the interaction designs and elucidate scalability challenges in integrating touchscreen interactions into this emerging tangible user interface.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445722
ER  - 

TY  - NA
AU  - Tsai, Ching-Yi; Tsai, I-Lun; Lai, Chao-Jung; Chow, Derrek; Wei, Lauren; Cheng, Lung-Pan; Chen, Mike Y.
TI  - AirRacket: Perceptual Design of Ungrounded, Directional Force Feedback to Improve Virtual Racket Sports Experiences
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502034
ER  - 

TY  - NA
AU  - Purnendu, NA; Acome, Eric; Keplinger, Christoph; Gross, Mark D.; Bruns, Carson J.; Leithinger, Daniel
TI  - CHI Extended Abstracts - Soft Electrohydraulic Actuators for Origami Inspired Shape-Changing Interfaces
PY  - 2021
AB  - In this paper, we present electrohydraulic actuators for origami inspired shape-changing interfaces, which are capable of producing sharp hinge-like bends. These compliant actuators generate an immediate hydraulic force upon electrostatic activation without an external fluid supply source, are silent and fast in operation, and can be fabricated with commodity materials. We experimentally investigate the characteristics of these actuators and present application scenarios for actuating existing objects as well as origami folds. In addition, we present a software tool for the design and fabrication of shape-changing interfaces using these electrohydraulic actuators. We also discuss how this work opens avenues for other possible applications in Human Computer Interaction (HCI).
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451590
ER  - 

TY  - JOUR
AU  - Medeiros, Daniel; McGill, Mark; Ng, Alexander; McDermid, Robert; Pantidi, Nadia; Williamson, Julie; Brewster, Stephen
TI  - From Shielding to Avoidance: Passenger Augmented Reality and the Layout of Virtual Displays for Productivity in Shared Transit.
PY  - 2022
AB  - Passengers spend considerable periods of time in shared transit spaces, relying on smartphones and laptops for work. However, these displays are limited in size and ergonomics compared to typical multi-monitor setups used in the office, impairing productivity. Augmented Reality (AR) headsets could provide large, flexible virtual workspaces during travel, enabling passengers to work more efficiently. This paper investigates the factors affecting how passengers choose to layout virtual displays in car, train, subway and plane environments, studying the affordances of each mode of transport and the presence of others. Results from our experiment showed: significant usage of the physical environment to align displays; strong social effects meant avoiding placing displays over other passengers or their belongings; and use of displays for shielding oneself from others. Our findings show the unique challenges posed by the mode of transport and presence of others on the use of AR for mobile productivity in the future.
SP  - 3640
EP  - 3650
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203002
ER  - 

TY  - JOUR
AU  - Xu, Kai; Ottley, Alvitta; Walchshofer, Conny; Streit, Marc; Chang, Remco; Wenskovitch, John
TI  - Survey on the Analysis of User Interactions and Visualization Provenance
PY  - 2020
AB  - There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.
SP  - 757
EP  - 783
JF  - Computer Graphics Forum
VL  - 39
IS  - 3
PB  - 
DO  - 10.1111/cgf.14035
ER  - 

TY  - NA
AU  - Wang, Tianyi; Qian, Xun; He, Fengming; Hu, Xiyun; Cao, Yuanzhi; Ramani, Karthik
TI  - UIST - GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications
PY  - 2021
AB  - Freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR. Further, we demonstrate multiple application scenarios enabled by GesturAR, such as interactive virtual objects, robots, and avatars, room-level interactive AR spaces, embodied AR presentations, etc. Finally, we evaluate the performance and usability of GesturAR through a user study.
SP  - 552
EP  - 567
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474769
ER  - 

TY  - NA
AU  - Wang, Xian; Monteiro, Diego; Lee, Lik-Hang; Hui, Pan; Liang, Hai-Ning
TI  - VibroWeight: Simulating Weight and Center of Gravity Changes of Objects in Virtual Reality for Enhanced Realism
PY  - 2022
AB  - Haptic feedback in virtual reality (VR) allows users to perceive the physical properties of virtual objects (e.g., their weight and motion patterns). However, the lack of haptic sensations deteriorates users' immersion and overall experience. In this work, we designed and implemented a low-cost hardware prototype with liquid metal, VibroWeight, which can work in complementarity with commercial VR handheld controllers. VibroWeight is characterized by bimodal feedback cues in VR, driven by adaptive absolute mass (weights) and gravity shift. To our knowledge, liquid metal is used in a VR haptic device for the first time. Our 29 participants show that VibroWeight delivers significantly better VR experiences in realism and comfort.
SP  - NA
EP  - NA
JF  - 2022 IEEE Haptics Symposium (HAPTICS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/haptics52432.2022.9765609
ER  - 

TY  - NA
AU  - Niiyama, Ryuma; Sato, Hiroki; Tsujimura, Kazzmasa; Narumi, Koya; Seong, Young ah; Yamamura, Ryosuke; Kakehi, Yasuaki; Kawahara, Yoshihiro
TI  - UIST - poimo: Portable and Inflatable Mobility Devices Customizable for Personal Physical Characteristics
PY  - 2020
AB  - Despite the recent growth in popularity of personal mobility devices (e.g., e-scooters and e-skateboards), they still suffer from limited safety and narrow design form factors, due to their rigid structures. On the other hand, inflatable interfaces studied in human-computer interaction can achieve large volume change by simple inflation/deflation. Inflatable structure also offers soft and safe interaction owing to material compliance and diverse fabrication methods that lead to a wide range of forms and aesthetics. In this paper, we propose poimo, a new family of POrtable and Inflatable MObility devices, which consists of inflatable frames, inflatable wheels, and inflatable steering mechanisms made of a mass-manufacturable material called drop-stitch fabric. First, we defined the basic material properties of a drop-stitch inflatable structure that is sufficiently strong to carry a person while simultaneously allowing soft deformation and deflation for storage and portability. We then implemented an interactive design system that can scan the user's desired riding posture to generate a customized personal mobility device and can add the user's shape and color preferences. To demonstrate the custom-design capability and mobility, we designed several 3D models using our system and built physical samples for two basic templates: a motorcycle and a wheelchair. Finally, we conducted an online user study to examine the usability of the design system and share lessons learned for further improvements in the design and fabrication of poimo.
SP  - 912
EP  - 923
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415894
ER  - 

TY  - JOUR
AU  - Luo, Shijian; Cui, Zhitong; Gu, Jiancheng; Chen, Ting; Shen, Chengyi; Lu, Yang; Wang, Yanan; Hansen, Preben
TI  - RoamFab: A Design Tool for Reconfiguring Parameterized Mechanisms to 3D Models With Structural Optimization
PY  - 2022
AB  - NA
SP  - 1
EP  - 15
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2102600
ER  - 

TY  - NA
AU  - Rossmy, Beat; Rümelin, Sonja; Wiethoff, Alexander
TI  - TEI - StringTouch - From String Instruments towards new Interface Morphologies
PY  - 2021
AB  - We present StringTouch, a user interface design exploration translating the expressive resource of string instruments to a new interface morphology. StringTouch transfers the string as a tactile element of interaction to the touch surface, resulting in a tactilely experienceable interface. In this paper we discuss our research through design centered approach, which focused on the exploration of musical string instruments and their translation to the UI context. To investigate this specific design space, we analyzed the systematic and handling of string instruments as well as common HCI principles to develop the interaction concept. The resulting experience prototype demonstrates the idea’s potential for haptic UI design and provides insights into the prototyping process. We present: (1) the investigation of string instruments as a resource for TUI design and (2) the transfer to a generic UI context to inform new hybrid interface morphologies that combine features of tangible, touch, and flexible interaction.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440628
ER  - 

TY  - NA
AU  - Kari, Mohamed; Grosse-Puppendahl, Tobias; Coelho, Luis Falconeri; Fender, Andreas; Bethge, David; Schütte, Reinhard; Holz, Christian
TI  - ISMAR - TransforMR: Pose-Aware Object Substitution for Composing Alternate Mixed Realities
PY  - 2021
AB  - Despite the advances in machine perception, semantic scene understanding is still a limiting factor in mixed reality scene composition. In this paper, we present TransforMR, a video see-through mixed reality system for mobile devices that performs 3D-pose-aware object substitution to create meaningful mixed reality scenes. In real-time and for previously unseen and unprepared real-world environments, TransforMR composes mixed reality scenes so that virtual objects assume behavioral and environment-contextual properties of replaced real-world objects. This yields meaningful, coherent, and humaninterpretable scenes, not yet demonstrated by today’s augmentation techniques. TransforMR creates these experiences through our novel pose-aware object substitution method building on different 3D object pose estimators, instance segmentation, video inpainting, and pose-aware object rendering. TransforMR is designed for use in the real-world, supporting the substitution of humans and vehicles in everyday scenes, and runs on mobile devices using just their monocular RGB camera feed as input. We evaluated TransforMR with eight participants in an uncontrolled city environment employing different transformation themes. Applications of TransforMR include real-time character animation analogous to motion capturing in professional film making, however without the need for preparation of either the scene or the actor, as well as narrative-driven experiences that allow users to explore fictional parallel universes in mixed reality. We make all of our source code and assets available1.1TransforMR code release: https://github.com/MohamedKari/transformr
SP  - 69
EP  - 79
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00021
ER  - 

TY  - JOUR
AU  - Enea, Sacco; Moon, Seung Ki
TI  - Guidelines for 3D printed springs using material extrusion
PY  - 2021
AB  - <jats:sec> <jats:title content-type="abstract-subheading">Purpose</jats:title> <jats:p>Springs are an integral part of mechanisms and can benefit from additive manufacturing’s (AM) increased design freedom. Given the limited literature on the subject, the purpose of this paper is to develop guidelines for fabricating helical springs using three-dimensional (3D) printing.</jats:p> </jats:sec> <jats:sec> <jats:title content-type="abstract-subheading">Design/methodology/approach</jats:title> <jats:p>Polylactic acid (PLA) is the main material investigated, with ULTEM™ 9085 used as a comparison. The experimental procedure is to vary the spring parameters, print the springs and test them in tension or compression using constant velocity. Plots of the force and displacement are used to measure the linear and post-deformation spring constants. Loading of the springs is done both to breakage and cyclically. Cyclic loading is also used to observe the plastic behaviour of the springs. Parameters that are varied include wire and coil diameters, pitch, wire cross-section, in-fill and layer height.</jats:p> </jats:sec> <jats:sec> <jats:title content-type="abstract-subheading">Findings</jats:title> <jats:p>A square wire cross-section is used, instead of a circle because it produces more consistent coils. In-fills make no significant difference in the elastic stiffness of the springs but the mono in-fill breaks at a greater extension, so it is recommended. Tension and compression springs are confirmed to behave the same when in the elastic regime. ULTEM™ 9085 produces consistently weaker springs compared to PLA. Variation of layer height shows that thinner layers increase the stiffness of the springs.</jats:p> </jats:sec> <jats:sec> <jats:title content-type="abstract-subheading">Originality/value</jats:title> <jats:p>This study investigates the behaviour of 3D printed helical springs in tension and compression. Three guidelines are created: square wire cross-section, mono-directional in-fill and thin layers are recommended.</jats:p> </jats:sec>
SP  - 409
EP  - 427
JF  - Rapid Prototyping Journal
VL  - 28
IS  - 3
PB  - 
DO  - 10.1108/rpj-04-2020-0078
ER  - 

TY  - NA
AU  - Onishi, Yuki; Takashima, Kazuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - VR - BouncyScreen: Physical Enhancement of Pseudo-Force Feedback
PY  - 2021
AB  - We explore BouncyScreen, an actuated 1D display system that enriches indirect interaction with a virtual object by pseudo-haptic feedback mechanics enhanced through the screen's physical movements. We configured a proof-of-concept prototype of BouncyScreen with a flat-screen mounted on a mobile robot. When the user manipulates a virtual object using virtual reality (VR) controllers, the screen moves in accordance with the virtual object. We conducted psychophysical studies to examine how BouncyScreen's physical movements would affect users' pseudo-haptic perceptions and interaction experiences. Our preliminary study using a weight discrimination task for object pushing interaction showed that BouncyScreen offers identical pseudo-force feedback to the vision-based pseudo-haptic technique. We conducted a follow-up psychophysical study using a weight magnitude estimation task for object pushing and bumping interactions. The results reveal that a users' perceived weight magnitude is enhanced by the screen's physical motion under different characteristics depending on interaction styles (i.e., pushing and bumping). Our study also confirmed that the screen's synchronous physical motions significantly enhance the reality of the interaction and the sense of presence. We close this paper with some applications and use suggestions for BouncyScreen in future HMD-free flat-screen 3D user interface systems.
SP  - 363
EP  - 372
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00059
ER  - 

TY  - JOUR
AU  - Peleg, Hila; Gabay, Roi; Itzhaky, Shachar; Yahav, Eran
TI  - Programming with a read-eval-synth loop
PY  - 2020
AB  - A frequent programming pattern for small tasks, especially expressions, is to repeatedly evaluate the program on an input as its editing progresses. The Read-Eval-Print Loop (REPL) interaction model has been a successful model for this programming pattern. We present the new notion of Read-Eval-Synth Loop (RESL) that extends REPL by providing in-place synthesis on parts of the expression marked by the user. RESL eases programming by synthesizing parts of a required solution. The underlying synthesizer relies on a partial solution from the programmer and a few examples. RESL hinges on bottom-up synthesis with general predicates and sketching, generalizing programming by example. To make RESL practical, we present a formal framework that extends observational equivalence to non-example specifications. We evaluate RESL by conducting a controlled within-subjects user-study on 19 programmers from 8 companies, where programmers are asked to solve a small but challenging set of competitive programming problems. We find that programmers using RESL solve these problems with far less need to edit the code themselves and by browsing documentation far less. In addition, they are less likely to leave a task unfinished and more likely to be correct.
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - OOPSLA
PB  - 
DO  - 10.1145/3428227
ER  - 

TY  - JOUR
AU  - Lee, Hyunjin; Bang, Sunyoung; Woo, Woontack
TI  - Effects of coordinate system and position of AR notification while walking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Virtual Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10055-022-00693-9
ER  - 

TY  - NA
AU  - Popovici, Irina
TI  - Experimental Results on the Accuracy of the Myo Armband for Short-Range Pointing Tasks
PY  - 2020
AB  - We analyze the accuracy of a Thalmic Myo armband in regards to short-range pointing interfaces. Through an experiment with multiple trials, we investigate the factors that could have an impact on the recognition rate. We found that removing the armband between sessions of use and a small spatial distance between pointing locations have a negative impact on the accuracy. By applying an exploratory approach, we found a configuration with a 97.3% recognition rate.
SP  - NA
EP  - NA
JF  - 2020 International Conference on Development and Application Systems (DAS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/das49615.2020.9108916
ER  - 

TY  - NA
AU  - Kovacs, Robert; Rambold, Lukas; Fritzsche, Lukas; Meier, Dominik; Shigeyama, Jotaro; Katakura, Shohei; Zhang, Ran; Baudisch, Patrick
TI  - UIST - Trusscillator: a System for Fabricating Human-Scale Human-Powered Oscillating Devices
PY  - 2021
AB  - Trusscillator is an end-to-end system that allows non-engineers to create human-scale human-powered devices that perform oscillatory movements, such as playground equipment, workout devices, and interactive kinetic installations. While recent research has been focusing on generating mechanisms that produce specific movement-path, without considering the required energy for the motion (kinematic approach), Trusscillator supports users in designing mechanisms that recycle energy in the system in the form of oscillating mechanisms (dynamic approach), specifically with the help of coil-springs. The presented system features a novel set of tools tailored for designing the dynamic experience of the motion. These tools allow designers to focus on user experience-specific aspects, such as motion range, tempo, and effort while abstracting away the underlying technicalities of eigenfrequencies, spring constants, and energy. Since the forces involved in the resulting devices can be high, Trusscillator helps users to fabricate from steel by picking out appropriate steal springs, generating part lists, and producing stencils and welding jigs that help weld with precision. To validate our system, we designed, built, and tested a series of unique playground equipment featuring 2-4 degrees of movement.
SP  - 1074
EP  - 1088
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474807
ER  - 

TY  - JOUR
AU  - Jun, Eunice; Birchfield, Melissa; De Moura, Nicole; Heer, Jeffrey; Just, René
TI  - Hypothesis Formalization: Empirical Findings, Software Limitations, and Design Implications
PY  - 2022
AB  - <jats:p> Data analysis requires translating higher level questions and hypotheses into computable statistical models. We present a mixed-methods study aimed at identifying the steps, considerations, and challenges involved in operationalizing hypotheses into statistical models, a process we refer to as <jats:italic>hypothesis formalization</jats:italic> . In a formative content analysis of 50 research papers, we find that researchers highlight decomposing a hypothesis into sub-hypotheses, selecting proxy variables, and formulating statistical models based on data collection design as key steps. In a lab study, we find that analysts fixated on implementation and shaped their analyses to fit familiar approaches, even if sub-optimal. In an analysis of software tools, we find that tools provide inconsistent, low-level abstractions that may limit the statistical models analysts use to formalize hypotheses. Based on these observations, we characterize hypothesis formalization as a dual-search process balancing conceptual and statistical considerations constrained by data and computation and discuss implications for future tools. </jats:p>
SP  - 1
EP  - 28
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 1
PB  - 
DO  - 10.1145/3476980
ER  - 

TY  - CHAP
AU  - Yigitbas, Enes; Jovanovikj, Ivan; Sauer, Stefan; Engels, Gregor
TI  - INTERACT Workshops - On the Development of Context-Aware Augmented Reality Applications
PY  - 2020
AB  - Augmented Reality (AR) is a technique that enables users to interact with their physical environment through the overlay of digital information. With the spread of AR applications in various domains (e.g. product design, manufacturing or maintenance) and the introduction of concepts such as Pervasive Augmented Reality (PAR), the aspect context-awareness started to play an important role. By sensing the user’s current context and adapting the AR application accordingly, an adequate user experience can be achieved. Due to the complex structure and composition of AR applications, their development is a challenging task. Although, context-awareness for AR systems was addressed to some extent, a systematic method for development of context-aware AR applications is not fully covered yet. Therefore, in this paper, we identify the main challenges for development of context-aware AR applications and sketch our solution idea for a model-based development framework. The benefit and applicability of our solution idea is shown on the basis of two example mobile AR applications supporting context-awareness.
SP  - 107
EP  - 120
JF  - Beyond Interactions
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-46540-7_11
ER  - 

TY  - NA
AU  - Subbaraman, Blair; Peek, Nadya
TI  - p5.fab: Direct Control of Digital Fabrication Machines from a Creative Coding Environment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533496
ER  - 

TY  - JOUR
AU  - Lee, Lik Hang; Braud, Tristan; Hosio, Simo; Hui, Pan
TI  - Towards Augmented Reality Driven Human-City Interaction: Current Research on Mobile Headsets and Future Challenges
PY  - 2021
AB  - Interaction design for Augmented Reality (AR) is gaining attention from both academia and industry. This survey discusses 260 articles (68.8% of articles published between 2015–2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by reviewing the latest trends of information visualization, constrained interfaces, and embodied interaction for AR headsets. We highlight under-explored issues in interface design and input techniques that warrant further research and conjecture that AR with complementary Conversational User Interfaces (CUIs) is a crucial enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of AR in Human-City Interaction.
SP  - 1
EP  - 38
JF  - ACM Computing Surveys
VL  - 54
IS  - 8
PB  - 
DO  - 10.1145/3467963
ER  - 

TY  - NA
AU  - Monteiro, Diego; Liang, Hai-Ning; Wang, Xian; Xu, Wenge; Tu, Huawei
TI  - ICMI - Design and Development of a Low-cost Device for Weight and Center of Gravity Simulation in Virtual Reality
PY  - 2021
AB  - With rapid advances in virtual reality (VR) technology, the use of haptics has become important to allow users to feel the physical properties of virtual objects. Current research has focused mainly on either weight variation or changing the center of gravity, which limits the simulation potential and may affect the feeling of immersion. This research explores the design and development of a device that can simulate both weight and center of gravity using low-cost components. Through an iterative design process and continuous testing with users, we arrived at a final prototype, FluidWeight, a device that can be attached to a typical VR handheld controller. FluidWeight uses fluid, which is transported from a central storage to a receptacle attached to the controller. A final experiment shows that users enjoyed using it because it could help increase the sense of realism in VR applications.
SP  - 453
EP  - 460
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3479907
ER  - 

TY  - NA
AU  - González, Juan Carlos; Boronat, Fernando; Sapena, Almanzor; Pastor, Javier Redondo
TI  - Key Technologies for Networked Virtual Environments
PY  - 2021
AB  - Thanks to the improvements experienced in technology in the last few years, most especially in virtual reality systems, the number and potential of networked virtual environments or NVEs and their users are increasing. NVEs aim to give distributed users a feeling of immersion in a virtual world and the possibility of interacting with other users or with virtual objects inside it, like when they interact in the real world. Being able to provide that feeling and natural interactions when the users are geographically separated is one of the goals of these systems. Nevertheless, this goal is especially sensitive to different issues, such as different connections with heterogeneous throughput or different network latencies, which can lead to consistency and synchronization problems and, thus, to a worsening of the users' quality of experience or QoE. With the purpose of solving these issues, researchers have proposed and evaluated numerous technical solutions, in fields like network architectures, data distribution and filtering, resource balancing, computing models, predictive modeling and synchronization in NVEs. This paper gathers and classifies them, summarizing their advantages and disadvantages, using a new way of classification. With the current increase of the number of NVEs and the multiple solutions proposed so far, this work aims to become a useful tool and a starting point not only for future researchers in this field but also for those who are new in NVEs development, in which guaranteeing a good users' QoE is essential.
SP  - NA
EP  - NA
JF  - arXiv: Networking and Internet Architecture
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Ajaykumar, Gopika; Steele, Maureen; Huang, Chien-Ming
TI  - A Survey on End-User Robot Programming
PY  - 2021
AB  - As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.
SP  - 1
EP  - 36
JF  - ACM Computing Surveys
VL  - 54
IS  - 8
PB  - 
DO  - 10.1145/3466819
ER  - 

TY  - NA
AU  - Petricek, Tomas
TI  - The Gamma: Programmatic Data Exploration for Non-programmers
PY  - 2022
AB  - Data exploration tools based on code can access any data source, result in reproducible scripts and encourage users to verify, reuse and modify existing code. Unfortunately, they are hard to use and require expert coding skills. Can we make data exploration tools based on code accessible to non-experts?We present The Gamma, a novel text-based data exploration environment that answers the question in the affirmative. The Gamma takes the idea of code completion to the limit. Users create transparent and reproducible scripts without writing code, by repeatedly choosing from offered code completions.The Gamma is motivated by the needs of data journalists and shows that we may not need to shy away from code for building accessible, reproducible and transparent tools that allow a broad public to benefit from the rise of open data.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833134
ER  - 

TY  - NA
AU  - Lyu, Zhuoyue; Yang, Jackie (Junrui); Lam, Monica S.; Landay, James A.
TI  - HomeView: Automatically Building Smart Home Digital Twins With Augmented Reality Headsets
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558709
ER  - 

TY  - NA
AU  - Feit, Anna Maria; Vordemann, Lukas; Park, Seonwook; Bérubé, Caterina; Hilliges, Otmar
TI  - Detecting Relevance during Decision-Making from Eye Movements for UI Adaptation
PY  - 2020
AB  - This paper proposes an approach to detect information relevance during decision-making from eye movements in order to enable user interface adaptation. This is a challenging task because gaze behavior varies greatly across individual users and tasks and groundtruth data is difficult to obtain. Thus, prior work has mostly focused on simpler target-search tasks or on establishing general interest, where gaze behavior is less complex. From the literature, we identify six metrics that capture different aspects of the gaze behavior during decision-making and combine them in a voting scheme. We empirically show, that this accounts for the large variations in gaze behavior and out-performs standalone metrics. Importantly, it offers an intuitive way to control the amount of detected information, which is crucial for different UI adaptation schemes to succeed. We show the applicability of our approach by developing a room-search application that changes the visual saliency of content detected as relevant. In an empirical study, we show that it detects up to 97% of relevant elements with respect to user self-reporting, which allows us to meaningfully adapt the interface, as confirmed by participants. Our approach is fast, does not need any explicit user input and can be applied independent of task and user.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391321
ER  - 

TY  - JOUR
AU  - Achberger, Alexander; Heyen, Frank; Vidackovic, Kresimir; Sedlmair, Michael
TI  - Touching data with PropellerHand
PY  - 2022
AB  - <jats:sec> <jats:title>Abstract</jats:title> <jats:p>Immersive analytics often takes place in virtual environments which promise the users immersion. To fulfill this promise, sensory feedback, such as haptics, is an important component, which is however not well supported yet. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our prototype through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality. Additionally, we conducted a second user study in the field of immersive visualization to investigate the potential benefits of PropellerHand there.</jats:p> </jats:sec><jats:sec> <jats:title>Graphical abstract</jats:title> </jats:sec>
SP  - NA
EP  - NA
JF  - Journal of Visualization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s12650-022-00859-2
ER  - 

TY  - JOUR
AU  - Cascaval, D.; Shalah, M.; Quinn, P.; Bodik, R.; Agrawala, M.; Schulz, A.
TI  - Differentiable 3D CAD Programs for Bidirectional Editing
PY  - 2022
AB  - NA
SP  - 309
EP  - 323
JF  - Computer Graphics Forum
VL  - 41
IS  - 2
PB  - 
DO  - 10.1111/cgf.14476
ER  - 

TY  - CHAP
AU  - Ng, Wasabii; Barati, Bahareh; Karana, Elvin
TI  - Healing with Fungi: Unique Aesthetic Expressions for Mycelium-Based Materials Through Patch and Mend
PY  - 2022
AB  - NA
SP  - 3253
EP  - 3267
JF  - [ ] With Design: Reinventing Design Modes
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-19-4472-7_210
ER  - 

TY  - JOUR
AU  - Jansen, Pascal; Colley, Mark; Rukzio, Enrico
TI  - A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review
PY  - 2022
AB  - <jats:p>Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.</jats:p>
SP  - 1
EP  - 51
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534617
ER  - 

TY  - NA
AU  - Kalkofen, Denis; Mori, Shohei; Ladinig, Tobias; Daling, Lea M.; Abdelrazeq, Anas; Ebner, Markus; Ortega, Manuel Labrador; Feiel, Susanne; Gabl, Sebastian; Shepel, Taras; Tibbett, James; Laine, Teemu H.; Hitch, Michael; Drebenstedt, Carsten; Moser, Peter
TI  - VR Workshops - Tools for Teaching Mining Students in Virtual Reality based on 360° Video Experiences
PY  - 2020
AB  - In recent years, Virtual Reality (VR) technology has found their way into higher education. Its power lays in its ability to provide immersive three-dimensional (3D) experiences that help conveying educational content whilst providing rich interaction possibilities. Especially in mining engineering education, VR has high potential to reshape the provided learning content. Field trips, i.e. mine visits, are an integral part of the education and necessary to transfer knowledge to students. However, field trips are time and cost intensive and mines often have tight entry regulations. As a result, the number of field trips is limited. VR-based field trips offer a considerable alternative presupposed they replicate the complex mining environment realistically. In addition, VR mines have the advantage of taking students close to events (e.g. explosions) that are impossible to demonstrate in a real mine. However, generating realistic 3D content for VR still involves complex, and thus time consuming tasks. Therefore, we present the design of a VR Framework for teaching mining students based on 360° video data, its evaluation in three different lectures, and its extension based on the feedback we received from students and teachers from four different universities.
SP  - 455
EP  - 459
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00096
ER  - 

TY  - JOUR
AU  - Sra, Misha; Danry, Valdemar; Maes, Pattie; Johnsen, Kyle; Billinghurst, Mark
TI  - Situated VR: Toward a Congruent Hybrid Reality Without Experiential Artifacts.
PY  - 2022
AB  - The vision of extended reality (XR) systems is living in a world where real and virtual elements seamlessly and contextually augment experiences of ourselves and the worlds we inhabit. While this integration promises exciting opportunities for the future of XR, it comes with the risk of experiential distortions and feelings of dissociation, especially related to virtual reality (VR). When transitioning from a virtual world to the real world, users report of experiential structures that linger on, as sort of after images, causing disruptions in their daily life. In this work, we define these atypical experiences as experiential artifacts (EAs) and present preliminary results from an informal survey conducted online with 76 VR users to highlight different types of artifacts and their durations. To avoid disruptions caused by these artifacts and simultaneously increase the user's sense of presence, we propose the idea of situated VR, which blends the real and virtual in novel ways that can reduce incongruencies between the two worlds. We discuss the implications of EAs, and through examples from our own work in building hybrid experiences, we demonstrate the potential and relevance of situated VR in the design of a future, more immersive, artifact-free hybrid reality.
SP  - 7
EP  - 18
JF  - IEEE computer graphics and applications
VL  - 42
IS  - 3
PB  - 
DO  - 10.1109/mcg.2022.3154358
ER  - 

TY  - NA
AU  - Fender, Andreas Rene; Holz, Christian
TI  - Causality-preserving Asynchronous Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501836
ER  - 

TY  - CHAP
AU  - Memmesheimer, Vera Marie; Ebert, Achim
TI  - Towards Advanced Evaluation of Collaborative XR Spaces
PY  - 2022
AB  - AbstractExtended Reality (XR) technologies such as head-mounted displays are deemed beneficial for the collaboration of co-located as well as distributed people. As such, XR technologies appear particularly promising for supporting distant and hybrid teaching which became highly relevant during the Covid-19 pandemic. Despite the potential awarded to such technologies, practical applications are still very rare. In order to investigate the impediments to the practical adoption of XR technologies, the respective systems should be evaluated in real-world settings. Existing evaluation tools are, however, not suited for this purpose. In this paper, we explain why today’s evaluation tools such as questionnaires, observation, and performance measurements are not sufficient for evaluating long-time, exploratory, and collaborative tasks that are typical in educational settings. To address this gap, we follow a top-down approach: Based on an existing model of user acceptance, we specify the variables that are to be optimized by HCI research and outline the potential of wearable-based measuring instruments to quantitatively assess these parameters. Eventually, we point out related research gaps that should be addressed by future research.KeywordsWearable-based evaluationExtended Reality (XR)Collaborative learningUser acceptanceEase of useTask-technology fit
SP  - 443
EP  - 452
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-98388-8_40
ER  - 

TY  - JOUR
AU  - Ku, Pin-Sung; Molla, Md. Tahmidul Islam; Huang, Kunpeng; Kattappurath, Priya; Ranjan, Krithik; Kao, Hsin-Liu Cindy
TI  - SkinKit
PY  - 2021
AB  - <jats:p>The emergence of on-skin interfaces has created an opportunity for seamless, always-available on-body interactions. However, developing a new fabrication process for on-skin interfaces can be time-consuming, challenging to incorporate new features, and not available for quick form-factor preview through prototyping. We introduce SkinKit, the first construction toolkit for on-skin interfaces, which enables fast, low-fidelity prototyping with a slim form factor directly applicable to the skin. SkinKit comprises modules consisting of skin-conformable base substrates and reusable Flexible Printed Circuits Board (FPCB) blocks. They are easy to attach and remove under tangible plug-and-play construction but still offer robust conductive connections in a slim form. Further, SkinKit aims to lower the barrier to entry in building on-skin interfaces without demanding technical expertise. It leverages a variety of preprogrammed modules connected in unique sequences to achieve various function customizations. We describe our iterative design and development process of SkinKit, comparing materials, connection mechanisms, and modules reflecting on its capability. We report results from single- and multi- session workshops with 34 maker participants spanning STEM and design backgrounds. Our findings reveal how diverse maker populations engage in on-skin interface design, what types of applications they choose to build, and what challenges they faced.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494989
ER  - 

TY  - NA
AU  - Lee, Lik Hang; Braud, Tristan; Hosio, Simo; Hui, Pan
TI  - Towards Augmented Reality-driven Human-City Interaction: Current Research and Future Challenges.
PY  - 2020
AB  - Interaction design for Augmented Reality (AR) is gaining increasing attention from both academia and industry. This survey discusses 205 articles (75% of articles published between 2015 - 2019) to review the field of human interaction in connected cities with emphasis on augmented reality-driven interaction. We provide an overview of Human-City Interaction and related technological approaches, followed by a review of the latest trends of information visualization, constrained interfaces, and embodied interaction for AR headsets. We highlight under-explored issues in interface design and input techniques that warrant further research, and conjecture that AR with complementary Conversational User Interfaces (CUIs) is a key enabler for ubiquitous interaction with immersive systems in smart cities. Our work helps researchers understand the current potential and future needs of AR in Human-City Interaction.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Reilly, Derek; Mahajan, Shivam; Singh, Abbey; Moore, Jake; Fresia, Isaac; Peachey, Matt; Malloch, Joseph
TI  - ISMAR Adjunct - Using Space Syntax to Enable Walkable AR Experiences
PY  - 2020
AB  - "Walkable" Augmented Reality (AR) experiences span floors of a building or involve exploring city neighbourhoods. In these cases setting greatly impacts object placement, interactive events, and narrative flow: in a zombie game for example, a standoff might best occur in an open foyer while a chase might be most effective in a narrow hallway. Spatial attributes are important when experiences are designed for a specific setting but also when settings are not known at design time. In this paper we explore how generic spatial attributes can facilitate design decisions in both cases. We conduct game design through the lens of space syntax, illustrating how attributes like openness, connectivity, and visual complexity can assist placement of walkable AR content in a site-specific narrative-driven scavenger hunt called ScavengAR and a "site-agnostic" game called Adventure AR. We contribute a Unity3D plugin that resolves design constraints expressed in terms of space syntax attributes to place AR content for a single setting or for multiple settings dynamically.
SP  - 289
EP  - 294
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct51615.2020.00080
ER  - 

TY  - NA
AU  - Evangelista Belo, João Marcelo; Lystbæk, Mathias N.; Feit, Anna Maria; Pfeuffer, Ken; Kán, Peter; Oulasvirta, Antti; Grønbæk, Kaj
TI  - AUIT – the Adaptive User Interfaces Toolkit for Designing XR Applications
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545651
ER  - 

TY  - NA
AU  - Barnaby, Gareth; Roudaut, Anne
TI  - UIST - Mantis: A Scalable, Lightweight and Accessible Architecture to Build Multiform Force Feedback Systems
PY  - 2019
AB  - Mantis is a highly scalable system architecture that democratizes haptic devices by enabling designers to create accurate, multiform and accessible force feedback systems. Mantis uses brushless DC motors, custom electronic controllers, and an admittance control scheme to achieve stable high-quality haptic rendering. It enables common desktop form factors but also: large workspaces (multiple arm lengths), multiple arm workspaces, and mobile workspaces. It also uses accessible components and costs significantly less than typical high-fidelity force feedback solutions which are often confined to research labs. We present our design and show that Mantis can reproduce the haptic fidelity of common robotic arms. We demonstrate its multiform ability by implementing five systems: a single desktop-sized device, a single large workspace device, a large workspace system with four points of feedback, a mobile system and a wearable one.
SP  - 937
EP  - 948
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347909
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Liao, Yu-So; Tsai, Chieh
TI  - ImpactVest: Rendering Spatio-Temporal Multilevel Impact Force Feedback on Body in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501971
ER  - 

TY  - NA
AU  - Qian, Xun; He, Fengming; Hu, Xiyun; Wang, Tianyi; Ramani, Karthik
TI  - ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545663
ER  - 

TY  - NA
AU  - Gohlke, Kristian; Sattler, Wolfgang; Hornecker, Eva
TI  - AirPinch – An Inflatable Touch Fader with Pneumatic Tactile Feedback
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505568
ER  - 

TY  - JOUR
AU  - Regenbrecht, Holger; Zwanenburg, Sander; Langlotz, Tobias
TI  - Pervasive Augmented Reality—Technology and Ethics
PY  - 2022
AB  - In the foreseeable future, mobile and wearable computing technology with an augmented reality (AR) interface can provide an omnipresent, environmentally adaptive, and everyday reality augmentation. This new pervasive AR technology will lead to a continuous moderation of experienced reality with the potential to support better and faster decision-making, the exploration of new information, and novel ways of communication, interaction, and collaboration. However, pervasive AR technology will also have undesired consequences, e.g., in the areas of privacy, commercial exploitation, distractions, digital inequality, and our perception of what is true and real. Little is known about how severe these effects will be when AR has become pervasive and how they can be prevented or mitigated. We draw on current developments in research and the market, sketch a near-time future of pervasive AR technology, identify ethical considerations, and discuss the development of pervasive AR systems.
SP  - 84
EP  - 91
JF  - IEEE Pervasive Computing
VL  - 21
IS  - 3
PB  - 
DO  - 10.1109/mprv.2022.3152993
ER  - 

TY  - NA
AU  - McGuffin, Michael J.; Fuhrman, C.P.
TI  - AVI - Categories and Completeness of Visual Programming and Direct Manipulation
PY  - 2020
AB  - Recent innovations in visual programming and the use of direct manipulation for programming have demonstrated promise, but also raise questions about how far these approaches can be generalized. To clarify these issues, we present a categorization of systems for visual programming, programming-by-example, and similar systems. By examining each category, we elucidate the advantages, limitations, and ways to extend systems in each category. Our work makes it easier for researchers and designers to understand how visual programming languages (VPLs) and similar systems relate to each other, and how to extend them. We also indicate directions for future research.
SP  - NA
EP  - NA
JF  - Proceedings of the International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3399715.3399821
ER  - 

TY  - NA
AU  - Martin, Daniel; Malpica, Sandra; Gutierrez, Diego; Masia, Belen; Serrano, Ana
TI  - Multimodality in VR: A Survey
PY  - 2021
AB  - Virtual reality (VR) is rapidly growing, with the potential to change the way we create and consume content. In VR, users integrate multimodal sensory information they receive, to create a unified perception of the virtual world. In this survey, we review the body of work addressing multimodality in VR, and its role and benefits in user experience, together with different applications that leverage multimodality in many disciplines. These works thus encompass several fields of research, and demonstrate that multimodality plays a fundamental role in VR; enhancing the experience, improving overall performance, and yielding unprecedented abilities in skill and knowledge transfer.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wang, Sijia; Fang, Cathy Mengying; Yang, Yiyao; Lu, Kexin; Vlachostergiou, Maria; Yao, Lining
TI  - Morphace: An Integrated Approach for Designing Customizable and Transformative Facial Prosthetic Makeup
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519406
ER  - 

TY  - NA
AU  - Chung, John Joon Young; Kim, Wooseok; Yoo, Kang Min; Lee, Hwaran; Adar, Eytan; Chang, Minsuk
TI  - TaleBrush: Sketching Stories with Generative Pretrained Language Models
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501819
ER  - 

TY  - NA
AU  - Tsai, Hsin-Ruey; Hung, Ching-Wen; Wu, Tzu-Chun; Chen, Bing-Yu
TI  - CHI - ElastOscillation: 3D Multilevel Force Feedback for Damped Oscillation on VR Controllers
PY  - 2020
AB  - Force feedback from damped oscillation is a common effect in our daily lives, especially when shaking an elastic object, an object hanging or containing other stuff, or a container with liquid, e.g., casting with a fishing pole or wine-swirling. Such a force, affected by complex physical variations and collisions, is difficult to properly simulate using current force feedback methods. Therefore, we propose ElastOscillation on a virtual reality (VR) controller to provide 3D multilevel force feedback for damped oscillation to enhance VR experiences. ElastOscillation consists of a proxy, six elastic bands and DC motors. It leverages the motors to control the bands' elasticity to restrain the movement of the proxy, which is connected with the bands. Therefore, when users shake the ElastOscillation device, the proxy shakes or moves in corresponding ranges of movement. The users then perceive the force from oscillation at different levels. In addition, elastic force from the bands further reinforces the oscillation force feedback. We conducted a force perception study to understand users' distinguishability for perceiving oscillation forces in 1D and 2D movement, respectively. Based on the results, we performed a VR experience study to show that the force feedback provided by ElastOscillation enhances VR realism.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376408
ER  - 

TY  - JOUR
AU  - Borgos-Rodriguez, Katya; Das, Maitraye; Piper, Anne Marie
TI  - Melodie: A Design Inquiry into Accessible Crafting through Audio-enhanced Weaving
PY  - 2021
AB  - Despite the promise of the maker movement as empowering individuals and democratizing design, people with disabilities still face many barriers to participation. Recent work has highlighted the inaccessible nature of making and introduced more accessible maker technologies, practices, and workspaces. One less explored area of accessible making involves supporting more traditional forms of craftwork, such as weaving and fiber arts. The present study reports an analysis of existing practices at a weaving studio within a residential community for people with vision impairments and explores the creation of an audio-enhanced loom to support this practice. Our iterative design process began with 60 hours of field observations at the weaving studio, complemented by 15 interviews with residents and instructors at the community. These insights informed the design of Melodie, an interactive floor loom that senses and provides audio feedback during weaving. Our design exploration of Melodie revealed four scenarios of use among this community: promoting learning among novice weavers, raising awareness of system state, enhancing the aesthetics of weaving, and supporting artistic performance. We identify recommendations for designing audio-enhanced technologies that promote accessible crafting and reflect on the role of technology in predominantly manual craftwork.
SP  - 1
EP  - 30
JF  - ACM Transactions on Accessible Computing
VL  - 14
IS  - 1
PB  - 
DO  - 10.1145/3444699
ER  - 

TY  - NA
AU  - Zhao, Lizhi; Lu, Xuequan; Bao, Qianyue; Wang, Meili
TI  - In-Place Gestures Classification via Long-term Memory Augmented Network
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00037
ER  - 

TY  - NA
AU  - Hartmann, Jeremy; DiVerdi, Stephen; Nguyen, Cuong; Vogel, Daniel
TI  - UIST - View-Dependent Effects for 360° Virtual Reality Video
PY  - 2020
AB  - "View-dependent effects'' have parameters that change with the user's view and are rendered dynamically at runtime. They can be used to simulate physical phenomena such as exposure adaptation, as well as for dramatic purposes such as vignettes. We present a technique for adding view-dependent effects to 360 degree video, by interpolating spatial keyframes across an equirectangular video to control effect parameters during playback. An in-headset authoring tool is used to configure effect parameters and set keyframe positions. We evaluate the utility of view-dependent effects with expert 360 degree filmmakers and the perception of the effects with a general audience. Results show that experts find view-dependent effects desirable for their creative purposes and that these effects can evoke novel experiences in an audience.
SP  - 354
EP  - 364
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415846
ER  - 

TY  - NA
AU  - Ryu, Neung; Kim, Myung Jin; Bianchi, Andrea
TI  - SIGGRAPH ASIA Emerging Technologies - Demonstration of ElaStick: A Variable Stiffness Display for Rendering Handheld Flexible Object
PY  - 2020
AB  - We present ElaStick, a handheld variable stiffness controller capable of simulating the kinesthetic sensation of deformable and flexible objects when swung or shaken. ElaStick is capable of rendering gradual changes of stiffness along two independent axes over a wide continuous range. Two trackers on the controller enable a closed-loop feedback that allows to accurately map the device’s deformations to the visuals of a Virtual Reality application.
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2020 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3415255.3422894
ER  - 

TY  - NA
AU  - Tanaka, Yoshihito; Kubota, Akira
TI  - Evaluation of Pseudo-Haptics system feedbacking muscle activity
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565682
ER  - 

TY  - NA
AU  - Dalgic, Omer; Puccinelli, Daniele; Zuniga, Marco
TI  - Enabling Body-Centric Computing Applications with LED-to-Camera Communication
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 Workshop on Body-centric Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3539489.3539588
ER  - 

TY  - BOOK
AU  - Li, Yi-Jun; Wang, Miao; Jin, De-Rong; Steinicke, Frank; Hu, Shi-Min; Zhao, Qinping
TI  - VR Workshops - Effects of Virtual Environments and Self-representations on Redirected Jumping
PY  - 2021
AB  - We design experiments to measure the perception (detection thresholds for gains, presence, embodiment, intrinsic motivation, and cybersickness) and physical performance (heart rate intensity, preparation time, and actual jumping distance) of redirected jumping (RDJ), under six different combinations of virtual environments (VEs) (low and high visual richness) and self-representations (SRs) (invisible, shoes, human-like). Results suggested that both VEs and SRs influence users’ perception and performance in RDJ, and have to be taken into account when designing locomotion techniques.
SP  - 464
EP  - 465
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00114
ER  - 

TY  - NA
AU  - Zhang, Zhuoming; Alvina, Jessalyn; Héron, Robin; Safin, Stéphane; Détienne, Françoise; Lecolinet, Eric
TI  - CHI - Touch without Touching: Overcoming Social Distancing in Semi-Intimate Relationships with SansTouch
PY  - 2021
AB  - Social distancing may force people to restrict social touch practices. Our survey (N=136) highlighted substantial social touch breakdowns during the COVID-19 pandemic for semi-intimate relationships (e.g., friends, colleagues), with handshakes being the most reduced, and frustrations at having to re-establish social touch habits. We then designed SansTouch, a multi-modal hand sleeve used together along with a smartphone to enable mediated hand-to-hand interactions such as handshakes or holding hands. To invoke the mediated touch, users synchronously mimic the hand position as in real life while holding SansTouch. Users can feel the touch sensation in real time without touching. Participants from our observational study (N=12) quickly adopted the hand-to-hand interactions of SansTouch for exchanging greetings face-to-face with colleagues and reported stronger preferences towards using SansTouch as opposed to mid-air gestures (e.g., waving). We discuss design implications, including the trade-offs of multi-modality for touch devices in face-to-face communication.
SP  - 1
EP  - 13
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445612
ER  - 

TY  - NA
AU  - Schmitz, Martin; Günther, Sebastian; Schön, Dominik; Müller, Florian
TI  - Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501981
ER  - 

TY  - NA
AU  - Takahashi, Haruki; Punpongsanon, Parinya; Kim, Jeeeun
TI  - UIST - Programmable Filament: Printed Filaments for Multi-material 3D Printing
PY  - 2020
AB  - From full-color objects to functional capacitive artifacts, 3D printing multi-materials became essential to broaden the application areas of digital fabrication. We present Programmable Filament, a novel technique that enables multi-material printing using a commodity FDM 3D printer, requiring no hardware upgrades. Our technique builds upon an existing printing technique in which multiple filament segments are printed and spliced into a single threaded filament. We propose an end-to-end pipeline for 3D printing an object in multi-materials, with an introduction of the design systems for end-users. Optimized for low-cost, single-nozzle FDM 3D printers, the system is built upon our computational analysis and experiments to enhance its validity over various printers and materials to design and produce a programmable filament. Finally, we discuss application examples and speculate the future with its potential, such as custom filament manufacturing on-demand.
SP  - 1209
EP  - 1221
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415863
ER  - 

TY  - NA
AU  - Cheng, Yi Fei; Luong, Tiffany; Fender, Andreas Rene; Streli, Paul; Holz, Christian
TI  - ComforTable User Interfaces: Surfaces Reduce Input Error, Time, and Exertion for Tabletop and Mid-air User Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00029
ER  - 

TY  - JOUR
AU  - Fajardo, Jorge I; Farez, Marco V; Paltán, César A
TI  - Experimental Analysis of the Relationship between Textile Structure, Tensile Strength and Comfort in 3D Printed Structured Fabrics.
PY  - 2022
AB  - In this article, an experimental investigation was conducted to study the effects of 3D printed structured fabrics on the tensile strength of two additive manufacturing technologies: (i) fused deposition modeling (FDM); and (ii) stereolithography (SLA). Three types of structured fabrics were designed in a linked fabric structure, which resembled the main characteristics of a conventional textile. Through computer-aided design (CAD), the textile structures were sketched, which, in a STL format, were transferred to 3D printing software, and consequently, they were printed. The specimens were subjected to tensile tests to analyse the behaviour of the linked structures under tensile loads. The results obtained indicated that the elements structured in a linked fabric pattern showed a statistically significant effect between the design of the 3D printed structured fabric and its tensile strength. Some important properties in textiles, fabric areal density, fineness (tex) and fabric flexibility were also analysed. This study opens an important field of research on the mechanical resistance of textile structures manufactured by 3D printing, oriented for applications in wearables that have a promising future in the fields of medicine, aerospace, sports, fashion, etc.
SP  - 152
EP  - NA
JF  - Polymers
VL  - 15
IS  - 1
PB  - 
DO  - 10.3390/polym15010152
ER  - 

TY  - NA
AU  - Shi, Rongkai; Zhu, Nan; Liang, Hai-Ning; Zhao, Shengdong
TI  - Exploring Head-based Mode-Switching in Virtual Reality
PY  - 2021
AB  - Mode-switching supports multilevel operations using a limited number of input methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches for mode-switching use buttons, controllers, and users' hands. However, they are inefficient and challenging to do with tasks that require both hands (e.g., when users need to use two hands during drawing operations). Using head gestures for mode-switching can be an efficient and cost-effective way, allowing for a more continuous and smooth transition between modes. In this paper, we explore the use of head gestures for mode-switching especially in scenarios when both users' hands are performing tasks. We present a first user study that evaluated eight head gestures that could be suitable for VR HMD with a dual-hand line-drawing task. Results show that move forward, move backward, roll left, and roll right led to better performance and are preferred by participants. A second study integrating these four gestures in Tilt Brush, an open-source painting VR application, is conducted to further explore the applicability of these gestures and derive insights. Results show that Tilt Brush with head gestures allowed users to change modes with ease and led to improved interaction and user experience. The paper ends with a discussion on some design recommendations for using head-based mode-switching in VR HMD.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Belo, João; Feit, Anna Maria; Feuchtner, Tiare; Grønbæk, Kaj
TI  - CHI - XRgonomics: Facilitating the Creation of Ergonomic 3D Interfaces
PY  - 2021
AB  - Arm discomfort is a common issue in Cross Reality applications involving prolonged mid-air interaction. Solving this problem is difficult because of the lack of tools and guidelines for 3D user interface design. Therefore, we propose a method to make existing ergonomic metrics available to creators during design by estimating the interaction cost at each reachable position in the user’s environment. We present XRgonomics, a toolkit to visualize the interaction cost and make it available at runtime, allowing creators to identify UI positions that optimize users’ comfort. Two scenarios show how the toolkit can support 3D UI design and dynamic adaptation of UIs based on spatial constraints. We present results from a walkthrough demonstration, which highlight the potential of XRgonomics to make ergonomics metrics accessible during the design and development of 3D UIs. Finally, we discuss how the toolkit may address design goals beyond ergonomics.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445349
ER  - 

TY  - JOUR
AU  - Song, Jean Y.; Chung, John Joon Young; Fouhey, David F.; Lasecki, Walter S.
TI  - C-Reference: Improving 2D to 3D Object Pose Estimation Accuracy via Crowdsourced Joint Object Estimation
PY  - 2020
AB  - Converting widely-available 2D images and videos, captured using an RGB camera, to 3D can help accelerate the training of machine learning systems in spatial reasoning domains ranging from in-home assistive robots to augmented reality to autonomous vehicles. However, automating this task is challenging because it requires not only accurately estimating object location and orientation, but also requires knowing currently unknown camera properties (e.g., focal length). A scalable way to combat this problem is to leverage people's spatial understanding of scenes by crowdsourcing visual annotations of 3D object properties. Unfortunately, getting people to directly estimate 3D properties reliably is difficult due to the limitations of image resolution, human motor accuracy, and people's 3D perception (i.e., humans do not "see" depth like a laser range finder). In this paper, we propose a crowd-machine hybrid approach that jointly uses crowds' approximate measurements of multiple in-scene objects to estimate the 3D state of a single target object. Our approach can generate accurate estimates of the target object by combining heterogeneous knowledge from multiple contributors regarding various different objects that share a spatial relationship with the target object. We evaluate our joint object estimation approach with 363 crowd workers and show that our method can reduce errors in the target object's 3D location estimation by over 40%, while requiring only $35$% as much human time. Our work introduces a novel way to enable groups of people with different perspectives and knowledge to achieve more accurate collective performance on challenging visual annotation tasks.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW1
PB  - 
DO  - 10.1145/3392858
ER  - 

TY  - JOUR
AU  - Adilkhanov, Adilzhan; Rubagotti, Matteo; Kappassov, Zhanat
TI  - Haptic Devices: Wearability-Based Taxonomy and Literature Review
PY  - 2022
AB  - In the last decade, several new haptic devices have been developed, contributing to the definition of more realistic virtual environments. An overview on this topic requires a description of the various technologies employed in building such devices, and of their application domains. This survey describes the current technology underlying haptic devices, based on the concept of &#x201C;wearability level&#x201D;. More than 90 devices, newly developed and described in scientific papers published in the period 2010-2021, are reviewed, which provide either haptic illusions or novel haptic feedback for teleoperation, entertainment, training, education, guidance and notification. As a result, the analyzed systems are divided into grounded, hand-held and wearable devices; the latter are further split into exoskeletons and gloves, finger-worn devices, and arm-worn devices. For the systems in each of these categories, descriptions and tables are provided that analyze their structure, including device mass and employed actuators, their applications, and other characteristics such as type of haptic feedback and tactile illusions. The paper also provides an overview of devices worn in parts of the human body other than arms and hands, and precisely haptic vests, jackets and belts, and haptic devices for head, legs and feet. Based on this analysis, the survey also provides a discussion on research gaps and challenges, and potential future directions.
SP  - 91923
EP  - 91947
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3202986
ER  - 

TY  - JOUR
AU  - Zhang, Shuo; Ke, Xingxing; Jiang, Qin; Chai, Zhiping; Wu, Zhigang; Ding, Han
TI  - Fabrication and Functionality Integration Technologies for Small-Scale Soft Robots.
PY  - 2022
AB  - Small-scale soft robots are attracting increasing interest for visible and potential applications owing to their safety and tolerance resulting from their intrinsic soft bodies or compliant structures. However, it is not sufficient that the soft bodies merely provide support or system protection. More importantly, to meet the increasing demands of controllable operation and real-time feedback in unstructured/complicated scenarios, these robots are required to perform simplex and multimodal functionalities for sensing, communicating, and interacting with external environments during large or dynamic deformation with the risk of mismatch or delamination. Challenges are encountered during fabrication and integration, including the selection and fabrication of composite/materials and structures, integration of active/passive functional modules with robust interfaces, particularly with highly deformable soft/stretchable bodies. Here, methods and strategies of fabricating structural soft bodies and integrating them with functional modules for developing small-scale soft robots are investigated. Utilizing templating, 3D printing, transfer printing, and swelling, small-scale soft robots can be endowed with several perceptual capabilities corresponding to diverse stimulus, such as light, heat, magnetism, and force. The integration of sensing and functionalities effectively enhances the agility, adaptability, and universality of soft robots when applied in various fields, including smart manufacturing, medical surgery, biomimetics, and other interdisciplinary sciences.
SP  - e2200671
EP  - 2200671
JF  - Advanced materials (Deerfield Beach, Fla.)
VL  - 34
IS  - 52
PB  - 
DO  - 10.1002/adma.202200671
ER  - 

TY  - NA
AU  - Hartmann, Jeremy; Vogel, Daniel
TI  - Enhanced Videogame Livestreaming by Reconstructing an Interactive 3D Game View for Spectators
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517521
ER  - 

TY  - NA
AU  - Liu, Jingjing; Liane, Wei; Ning, Bing; Mao, Ting
TI  - VR - Work Surface Arrangement Optimization Driven by Human Activity
PY  - 2021
AB  - In this paper, we aim at guiding people to accomplish a personalized task, work surface organizing, in mixed reality environment, which can also be applied to intelligent robots. Through the cameras mounted in a MR device, e.g., Hololens, we firstly capture a person's daily activities in real scene when he uses the work surface. From such activities, we model the individual behavior habits and apply them to optimize the arrangement of the work surface. A cost function is defined for the optimization, considering general arrangement rules and human habitual behavior. The optimized arrangement is suggested to the user by augmenting the virtual arrangement on the real scene. To evaluate the effectiveness of our approach, we conducted experiments on a variety of scenes.
SP  - 270
EP  - 278
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00049
ER  - 

TY  - NA
AU  - Li, Jiannan; Sousa, Maurício; Balakrishnan, Ravin; Grossman, Tovi
TI  - HAI - Constellation: a Multi-User Interface for Remote Drone Tours
PY  - 2021
AB  - Remotely controlled camera drones can support live, dynamic, and interactive virtual tours for travelers to overcome distance, expense, and health barriers. Yet, assigning one drone to one traveler may incur unnecessary waste of resources, and an abundance of concurrent drones raises safety concerns. While sharing the input and output of a single drone among multiple concurrent users can alleviate these limitations, standard control sharing protocols, such as turn-taking, are often inefficient. We present Constellation, a multi-user drone control system that synthesizes diverse user goals and generates efficient flight paths for the group. It supports point-of-interest specification on both static 3D environmental maps and live camera views. The generated paths minimize all users’ total extra waiting time. A web-based study with 16 participants show that Constellation could help groups navigate to their points-of-interest faster in comparison to the turn-taking baseline.
SP  - 277
EP  - 282
JF  - Proceedings of the 9th International Conference on Human-Agent Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472307.3484685
ER  - 

TY  - NA
AU  - Deshpande, Himani; Takahashi, Haruki; Kim, Jeeeun
TI  - CHI - EscapeLoom: Fabricating New Affordances for Hand Weaving
PY  - 2021
AB  - Hand-weaving is a beloved craft in history, holding promise for many opportunities in making from flat sheet fabrics to smart textiles. To afford new weaving experiences, we explore how 3D printed custom weaving tools interplay with different materiality, augmenting the design space of weaving. We propose novel weaving techniques enabled by 3D printed custom tools: (1) water-soluble draft to synchronize design intention and practice, (2) flexible warps to guide complex patterns and to shape resulting object, and (3) rigid global geometry for woven artifacts in 3D. EscapeLoom as a computational design tool enables users to employ various parameters in their computational design, and showcases many creative possibilities that move away from the traditional definition of a loom to dive into what more it can be.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445600
ER  - 

TY  - NA
AU  - Beever, Lee; John, Nigel W.
TI  - LevelEd SR: A Substitutional Reality Level Design Workflow
PY  - 2022
AB  - Virtual reality (VR) and augmented reality (AR) have continued to increase in popularity over the past decade. However, there are still issues with how much space is required for room-scale VR and experiences are still lacking from haptic feedback. We present LevelEd SR, a substitutional reality level design workflow that combines AR and VR systems and is built for consumer devices. The system enables passive haptics through the inclusion of physical objects from within a space into a virtual world. A validation study (17 participants) has produced quantitative data that suggests players benefit from passive haptics in entertainment VR games with an improved game experience and increased levels of presence. Including objects, such as real-world furniture that is paired with a digital proxy in the virtual world, also opens up more spaces to be used for room-scale VR. We evaluated the workflow and found that participants were accepting of the system, rating it positively using the System Usability Scale questionnaire and would want to use it again to experience substitutional reality.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00031
ER  - 

TY  - NA
AU  - Muehlhaus, Marie; Steimle, Jürgen; Koelle, Marion
TI  - Feather Hair: Interacting with Sensorized Hair in Public Settings
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533527
ER  - 

TY  - JOUR
AU  - Mango, Joseph; Claramunt, Christophe; Ngondo, Jamila; Zhang, Di; Xu, Dong; Colak, EbruHusniye; Li, Xiang
TI  - Multipurpose temporal GIS model for cadastral data management
PY  - 2021
AB  - NA
SP  - 1205
EP  - 1230
JF  - International Journal of Geographical Information Science
VL  - 36
IS  - 6
PB  - 
DO  - 10.1080/13658816.2021.2009483
ER  - 

TY  - NA
AU  - Maio, Rafael; Santos, André; Marques, Bernardo; Almeida, Duarte; Dias, Paulo; Sousa-Santos, Beatriz
TI  - Pervasive Augmented Reality (AR) for Assistive Production: Comparing the use of a Head-Mounted Display (HMD) versus a Hand-Held Device (HHD)
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3570591
ER  - 

TY  - JOUR
AU  - Parilusyan, Brice; Teyssier, Marc; Martinez-Missir, Valentin; Duhart, Clément; Serrano, Marcos
TI  - Sensurfaces
PY  - 2022
AB  - <jats:p>Ubiquitous touch sensing surfaces are largely influenced by touchscreens' look and feel and fail to express the physical richness of existing surrounding materials. We introduce Sensurfaces, a plug-and-play electronic module that allows to rapidly experiment with touch-sensitive surfaces while preserving the original appearance of materials. Sensurfaces is composed of plug-and-play modules that can be connected together to expand the size and number of materials composing a sensitive surface. The combination of Sensurfaces modules allows the creation of small or large multi-material sensitive surfaces that can detect multi-touch but also body proximity, pose, pass, or even human steps. In this paper, we present the design and implementation of Sensurfaces. We propose a design space describing the factors of Sensurfaces interfaces. Then, through a series of technical evaluations, we demonstrate the capabilities of our system. Finally, we report on two workshops validating the usability of our system.</jats:p>
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534616
ER  - 

TY  - JOUR
AU  - McNutt, Andrew M
TI  - No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization.
PY  - 2022
AB  - There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209460
ER  - 

TY  - NA
AU  - Tanaka, Yudai; Horie, Arata; Chen, Xiang 'Anthony'
TI  - VRST - DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback
PY  - 2020
AB  - We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418964
ER  - 

TY  - JOUR
AU  - Herskovitz, Jaylin; Cheng, Yi Fei; Guo, Anhong; Sample, Alanson P.; Nebeling, Michael
TI  - XSpace: An Augmented Reality Toolkit for Enabling Spatially-Aware Distributed Collaboration
PY  - 2022
AB  - <jats:p>Augmented Reality (AR) has the potential to leverage environmental information to better facilitate distributed collaboration, however, such applications are difficult to develop. We present XSpace, a toolkit for creating spatially-aware AR applications for distributed collaboration. Based on a review of existing applications and developer tools, we design XSpace to support three methods for creating shared virtual spaces, each emphasizing a different aspect: shared objects, user perspectives, and environmental meshes. XSpace implements these methods in a developer toolkit, and also provides a set of complimentary visual authoring tools to allow developers to preview a variety of configurations for a shared virtual space. We present five example applications to illustrate that XSpace can support the development of a rich set of collaborative AR experiences that are difficult to produce with current solutions. Through XSpace, we discuss implications for future application design, including user space customization and privacy and safety concerns when sharing users' environments.</jats:p>
SP  - 277
EP  - 302
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567721
ER  - 

TY  - JOUR
AU  - Lee, Jiwon; Chun, Jaehoon
TI  - Development of 3D Printed Bags Using Roll-Type Printing Method
PY  - 2022
AB  - NA
SP  - 505
EP  - 518
JF  - Fashion & Textile Research Journal
VL  - 24
IS  - 5
PB  - 
DO  - 10.5805/sfti.2022.24.5.505
ER  - 

TY  - JOUR
AU  - Winkle, Katie; Senft, Emmanuel; Lemaignan, Séverin
TI  - LEADOR: A Method for End-To-End Participatory Design of Autonomous Social Robots.
PY  - 2021
AB  - Participatory design (PD) has been used to good success in human-robot interaction (HRI) but typically remains limited to the early phases of development, with subsequent robot behaviours then being hardcoded by engineers or utilised in Wizard-of-Oz (WoZ) systems that rarely achieve autonomy. In this article, we present LEADOR (Led-by-Experts Automation and Design Of Robots), an <i>end-to-end</i> PD methodology for domain expert co-design, automation, and evaluation of social robot behaviour. This method starts with typical PD, working with the domain expert(s) to co-design the interaction specifications and state and action space of the robot. It then replaces the traditional offline programming or WoZ phase by an <i>in situ</i> and online teaching phase where the domain expert can live-program or teach the robot how to behave whilst being embedded in the interaction context. We point out that this live teaching phase can be best achieved by adding a learning component to a WoZ setup, which captures implicit knowledge of experts, as they intuitively respond to the dynamics of the situation. The robot then progressively learns an appropriate, expert-approved policy, ultimately leading to full autonomy, even in sensitive and/or ill-defined environments. However, LEADOR is agnostic to the exact technical approach used to facilitate this learning process. The extensive inclusion of the domain expert(s) in robot design represents established responsible innovation practice, lending credibility to the system both during the teaching phase and when operating autonomously. The combination of this expert inclusion with the focus on <i>in situ</i> development also means that LEADOR supports a mutual shaping approach to social robotics. We draw on two previously published, foundational works from which this (generalisable) methodology has been derived to demonstrate the feasibility and worth of this approach, provide concrete examples in its application, and identify limitations and opportunities when applying this framework in new environments.
SP  - 704119
EP  - NA
JF  - Frontiers in robotics and AI
VL  - 8
IS  - NA
PB  - 
DO  - 10.3389/frobt.2021.704119
ER  - 

TY  - NA
AU  - Teyssier, Marc; Parilusyan, Brice; Roudaut, Anne; Steimle, Jürgen
TI  - ICRA - Human-Like Artificial Skin Sensor for Physical Human-Robot Interaction
PY  - 2021
AB  - Physical Human-Robot-Interaction (pHRI) is beneficial for communication in social interaction or to perform collaborative tasks but is also crucial for safety. While robotic devices embed sensors for this sole purpose, their design often is the results of a trade-off between technical capabilities and rarely considers human factors. We propose a novel approach to design and fabricate compliant Human-like artificial skin sensors for robots, with similar mechanical properties as human skin and capable of precisely detecting touch. Our artificial skin relies on the use of different silicone elastomers to replicate the human skin layers and comprises an embedded electrode matrix to perform mutual capacitance sensing. We present the sensor and describe its fabrication process which is scalable, low-cost and ensures flexibility, compliance and robustness. We introduce Muca, an open-source sensing development board and then evaluate the performance of the sensor.
SP  - 3626
EP  - 3633
JF  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra48506.2021.9561152
ER  - 

TY  - NA
AU  - Winkle, Katie; Senft, Emmanuel; Lemaignan, Séverin
TI  - LEADOR: A Method for End-to-End Participatory Design of Autonomous Social Robots.
PY  - 2021
AB  - Participatory Design (PD) in Human-Robot Interaction (HRI) typically remains limited to the early phases of development, with subsequent robot behaviours then being hardcoded by engineers or utilised in Wizard-of-Oz (WoZ) systems that rarely achieve autonomy. We present LEADOR (Led-by-Experts Automation and Design Of Robots) an end-to-end PD methodology for domain expert co-design, automation and evaluation of social robots. LEADOR starts with typical PD to co-design the interaction specifications and state and action space of the robot. It then replaces traditional offline programming or WoZ by an in-situ, online teaching phase where the domain expert can live-program or teach the robot how to behave while being embedded in the interaction context. We believe that this live teaching can be best achieved by adding a learning component to a WoZ setup, to capture experts' implicit knowledge, as they intuitively respond to the dynamics of the situation. The robot progressively learns an appropriate, expert-approved policy, ultimately leading to full autonomy, even in sensitive and/or ill-defined environments. However, LEADOR is agnostic to the exact technical approach used to facilitate this learning process. The extensive inclusion of the domain expert(s) in robot design represents established responsible innovation practice, lending credibility to the system both during the teaching phase and when operating autonomously. The combination of this expert inclusion with the focus on in-situ development also means LEADOR supports a mutual shaping approach to social robotics. We draw on two previously published, foundational works from which this (generalisable) methodology has been derived in order to demonstrate the feasibility and worth of this approach, provide concrete examples in its application and identify limitations and opportunities when applying this framework in new environments.
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Li, Yijun; Wang, Miao; Jin, Derong; Steinicke, Frank; Zhao, Qinping
TI  - Effects of virtual environment and self-representations on perception and physical performance in redirected jumping
PY  - 2021
AB  - Redirected jumping (RDJ) allows users to explore virtual environments (VEs) naturally by scaling a small real-world jump to a larger virtual jump with virtual camera motion manipulation, thereby addressing the problem of limited physical space in VR applications. Previous RDJ studies have mainly focused on detection threshold estimation. However, the effect VE or selfrepresentation (SR) has on the perception or performance of RDJs remains unclear. In this paper, we report experiments to measure the perception (detection thresholds for gains, presence, embodiment, intrinsic motivation, and cybersickness) and physical performance (heart rate intensity, preparation time, and actual jumping distance) of redirected forward jumping under six different combinations of VE (low and high visual richness) and SRs (invisible, shoes, and human-like). Our results indicated that the detection threshold ranges for horizontal translation gains were significantly smaller in the VE with high rather than low visual richness. When different SRs were applied, our results did not suggest significant differences in detection thresholds, but it did report longer actual jumping distances in the invisible body case compared with the other two SRs. In the high visual richness VE, the preparation time for jumping with a human-like avatar was significantly longer than that with other SRs. Finally, some correlations were found between perception and physical performance measures. All these findings suggest that both VE and SRs influence users' perception and performance in RDJ and must be considered when designing locomotion techniques.
SP  - 451
EP  - 469
JF  - Virtual Reality & Intelligent Hardware
VL  - 3
IS  - 6
PB  - 
DO  - 10.1016/j.vrih.2021.06.003
ER  - 

TY  - NA
AU  - Faltaous, Sarah; Prochazka, Marvin; Auda, Jonas; Keppel, Jonas; Wittig, Nick; Gruenefeld, Uwe; Schneegass, Stefan
TI  - Give Weight to VR: Manipulating Users' Perception of Weight in Virtual Reality with Electric Muscle Stimulation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3547571
ER  - 

TY  - CHAP
AU  - Hao, Chenxu; Dwivedi, Anany; Beckerle, Philipp
TI  - A Literature-Based Perspective on Human-Centered Design and Evaluation of Interfaces for Virtual Reality in Robotics
PY  - 2023
AB  - NA
SP  - 1
EP  - 13
JF  - Human-Friendly Robotics 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-22731-8_1
ER  - 

TY  - NA
AU  - Tahara, Tomu; Seno, Takashi; Narita, Gaku; Ishikawa, Tomoya
TI  - Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph
PY  - 2020
AB  - In this paper, we present Retargetable AR, a novel AR framework that yields an AR experience that is aware of scene contexts set in various real environments, achieving natural interaction between the virtual and real worlds. To this end, we characterize scene contexts with relationships among objects in 3D space, not with coordinates transformations. A context assumed by an AR content and a context formed by a real environment where users experience AR are represented as abstract graph representations, i.e. scene graphs. From RGB-D streams, our framework generates a volumetric map in which geometric and semantic information of a scene are integrated. Moreover, using the semantic map, we abstract scene objects as oriented bounding boxes and estimate their orientations. With such a scene representation, our framework constructs, in an online fashion, a 3D scene graph characterizing the context of a real environment for AR. The correspondence between the constructed graph and an AR scene graph denoting the context of AR content provides a semantically registered content arrangement, which facilitates natural interaction between the virtual and real worlds. We performed extensive evaluations on our prototype system through quantitative evaluation of the performance of the oriented bounding box estimation, subjective evaluation of the AR content arrangement based on constructed 3D scene graphs, and an online AR demonstration. The results of these evaluations showed the effectiveness of our framework, demonstrating that it can provide a context-aware AR experience in a variety of real scenes.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mor, Hila; Yu, Tianyu; Nakagaki, Ken; Miller, Benjamin Harvey; Jia, Yichen; Ishii, Hiroshi
TI  - CHI - Venous Materials: Towards Interactive Fluidic Mechanisms
PY  - 2020
AB  - Venous Materials is a novel concept and approach of an interactive material utilizing fluidic channels. We present a design method for fluidic mechanisms that respond to deformation by mechanical inputs from the user, such as pressure and bending. We designed a set of primitive venous structures that act as embedded analog fluidic sensors, displaying flow and color change. In this paper, we consider the fluid as the medium to drive tangible information triggered by deformation, and at the same time, to function as a responsive display of that information. To provide users with a simple way to create and validate designs of fluidic structures, we built a software platform and design tool UI. This design tool allows users to quickly design the geometry, and simulate the flow with intended mechanical force dynamically. We present a range of applications that demonstrate how Venous Materials can be utilized to augment interactivity of everyday physical objects.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376129
ER  - 

TY  - CHAP
AU  - Saric, Isad; Colic, Mirsad; Muratovic, Enis; Delic, Muamer; Muminovic, Adis J.
TI  - Design and Development of Compression and Torsion Springs Using CAD/CAE
PY  - 2021
AB  - In this paper we displayed the problematic of spring design, with usage of CAD/CAE (Computer Aided Design/Computer Aided Engineering) methods. Spring 3D (Three-dimensional) parametric modeling is performed in CATIA (Computer Aided Three-dimensional Interactive Application) software for purposes of achieving fast manipulation of spring geometry, so various spring variants can be made. Using the same software, FEM (Finite Element Method) analysis is used to determine maximum stresses and deflections. Utility of developed parametrized spring system is best reflected for cases of new spring design, where new geometric and FEM models are created with simple parameter application and update feature. This research resulted in spring prototyping with utilization of 3D printing techniques and spring manufacturing.
SP  - 635
EP  - 644
JF  - Lecture Notes in Networks and Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90055-7_50
ER  - 

TY  - NA
AU  - Xue, Tong; Ali, Abdallah El; Zhang, Tianyi; Ding, Gangyi; Cesar, Pablo
TI  - CHI - RCEA-360VR: Real-time, Continuous Emotion Annotation in 360° VR Videos for Collecting Precise Viewport-dependent Ground Truth Labels
PY  - 2021
AB  - Precise emotion ground truth labels for 360° virtual reality (VR) video watching are essential for fine-grained predictions under varying viewing behavior. However, current annotation techniques either rely on post-stimulus discrete self-reports, or real-time, continuous emotion annotations (RCEA) but only for desktop/mobile settings. We present RCEA for 360° VR videos (RCEA-360VR), where we evaluate in a controlled study (N=32) the usability of two peripheral visualization techniques: HaloLight and DotSize. We furthermore develop a method that considers head movements when fusing labels. Using physiological, behavioral, and subjective measures, we show that (1) both techniques do not increase users’ workload, sickness, nor break presence (2) our continuous valence and arousal annotations are consistent with discrete within-VR and original stimuli ratings (3) users exhibit high similarity in viewing behavior, where fused ratings perfectly align with intended labels. Our work contributes usable and effective techniques for collecting fine-grained viewport-dependent emotion labels in 360° VR.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445487
ER  - 

TY  - JOUR
AU  - Yao, Yuan; Ding, Cheng; Aburaia, Mohamed; Lackner, Maximilian; He, Lanlan
TI  - A 3D weaving infill pattern for fused filament fabrication
PY  - 2021
AB  - The fused filament fabrication process is the most used additive manufacturing process due to its simplicity and low operating costs. In this process, a thermoplastic filament is led through an extruder, melted, and applied to a building platform by the axial movements of an automated Cartesian system in such a way that a three-dimensional object is created layer by layer. Compared to other additive manufacturing technologies, the components produced have mechanical limitations and are often not suitable for functional applications. To reduce the anisotropy of mechanical strength in fused filament fabrication (FFF), this paper proposes a 3D weaving deposit path planning method that utilizes a 5-layer repetitive structure to achieve interlocking and embedding between neighbor slicing planes to improve the mechanical linkage within the layers. The developed algorithm extends the weaving path as an infill pattern to fill different structures and makes this process feasible on a standard three-axis 3D printer. Compared with 3D weaving printed parts by layer-to-layer deposit, the anisotropy of mechanical properties inside layers is significantly reduced to 10.21% and 0.98%.
SP  - 3101
EP  - 3114
JF  - The International Journal of Advanced Manufacturing Technology
VL  - 117
IS  - 9-10
PB  - 
DO  - 10.1007/s00170-021-07694-z
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Steimle, Jürgen
TI  - Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517668
ER  - 

TY  - JOUR
AU  - Liu, Xiaolong; Wang, Lili
TI  - Redirected jumping in virtual scenes with alleys
PY  - 2021
AB  - The redirected jumping (RDJ) technique is a new locomotion method that saves physical tracking area and enhances the body movement experience of users in virtual reality. In a previous study, the range of imperceptible manipulation gains in RDJ was discussed in an empty virtual environment (VE). In this study, we conducted three tasks to investigate the influence of alley width on the detection threshold of jump redirection in a VE. The results demonstrated that the imperceptible distance gain range in RDJ was not associated with the width of the alleys. The imperceptible height and rotation gain ranges in RDJ are related to the width of the alleys. We preliminarily summarized the relationship between the occlusion distance and manipulation range of the three gains in a complex environment. Simultaneously, the guiding principle for choosing three gains in RDJ according to the occlusion distance in a complex environment is provided.
SP  - 470
EP  - 483
JF  - Virtual Reality & Intelligent Hardware
VL  - 3
IS  - 6
PB  - 
DO  - 10.1016/j.vrih.2021.06.004
ER  - 

TY  - NA
AU  - Hashimoto, Takeru; Yoshida, Shigeo; Narumi, Takuji
TI  - MetamorphX: An Ungrounded 3-DoF Moment Display that Changes its Physical Properties through Rotational Impedance Control
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545650
ER  - 

TY  - NA
AU  - Han, Han L.; Yu, Junhang; Bournet, Raphael; Ciorascu, Alexandre; Mackay, Wendy E.; Beaudouin-Lafon, Michel
TI  - Passages: Interacting with Text Across Documents
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502052
ER  - 

TY  - JOUR
AU  - Gu, Jiseong; Lee, Geehyuk
TI  - Towards More Direct Text Editing With Handwriting Interfaces
PY  - 2022
AB  - NA
SP  - 233
EP  - 248
JF  - International Journal of Human–Computer Interaction
VL  - 39
IS  - 1
PB  - 
DO  - 10.1080/10447318.2022.2041893
ER  - 

TY  - CHAP
AU  - Tiersen, Federico Julien; Calvo, Rafael A.
TI  - HCI (2) - Holdable Devices: Supporting Mindfulness, Psychological Autonomy and Self-Regulation During Smartphone Use
PY  - 2021
AB  - It has been argued that consuming social and micro-targeted digital content rapidly and continuously arouses the brain into an impulsive, dopamine-fueled, ‘automatic’ flow state that leads to excessive and unhealthy smartphone use. The ubiquity of advertising-based products that exploit users’ vulnerabilities to maximize engagement is leading to detrimental impacts on well-being and widespread addiction symptoms. In the UK, about 40% of adults think they spend too much time online, 60% consider themselves ‘hooked’ and 33% find disconnecting difficult.
SP  - 476
EP  - 495
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78465-2_35
ER  - 

TY  - NA
AU  - Vielman, Pablo; Akleman, Ergun
TI  - SIGGRAPH Posters - Animated Futurist Sculpting as Dynamic Implicit Shapes
PY  - 2021
AB  - In this work, we present an approach to obtain futurist sculptures. Our approach is inspired by the works of Italian Futurist artists such as Umberto Boccioni. Futurism, as an art movement, aims to achieve to define forms that are a product of time but is permanent in space. In this work, we have developed a methodology to produce a set of futurist sculptures from any given animation of any object that is defined as a triangular mesh. Each produced futurist sculpture is a still frame of what can be rendered as a sculpture animation. Our method is based on the conversion of a given polygonal mesh and its motion into an implicit shape in 4D space which consists of 3-spatial and one temporal dimension. To create each specific futurist sculpture, we compute a subset of this 4D implicit shape in a given time interval. The resulting immersion of 4D structure into the 3D spatial domain provides us desired futurist sculpture for the given time interval. The most important aspect of our methodology is the conversion of animated polygonal mesh into a 4D implicit shape. We first convert a polygonal mesh into a set of particles. Each particle can have its own color. All the points that are closer to the trajectory of the particle form an implicitly defined swept volume [Kim et al. 2004]. These swept volumes appear to be similar to the extrusion of a circle along a curve, but they are guaranteed to be free of artifacts caused by intersections.
SP  - NA
EP  - NA
JF  - Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450618.3469148
ER  - 

TY  - JOUR
AU  - Shi, Yilei; Zhang, Haimo; Zhao, Kaixing; Cao, Jiashuo; Sun, Mengmeng; Nanayakkara, Suranga
TI  - Ready, Steady, Touch!: Sensing Physical Contact with a Finger-Mounted IMU
PY  - 2020
AB  - A finger held in the air exhibits microvibrations, which are reduced when it touches a static object. When a finger moves along a surface, the friction between them produces vibrations, which can not be produced with a free-moving finger in the air. With an inertial measurement unit (IMU) capturing such motion characteristics, we demonstrate the feasibility to detect contact between the finger and static objects. We call our technique ActualTouch. Studies show that a single nail-mounted IMU on the index finger provides sufficient data to train a binary touch status classifier (i.e., touch vs. no-touch), with an accuracy above 95%, generalised across users. This model, trained on a rigid tabletop surface, was found to retain an average accuracy of 96% for 7 other types of everyday surfaces with varying rigidity, and in walking and sitting scenarios where no touch occurred. ActualTouch can be combined with other interaction techniques, such as in a uni-stroke gesture recogniser on arbitrary surfaces, where touch status from ActualTouch is used to delimit the motion gesture data that feed into the recogniser. We demonstrate the potential of ActualTouch in a range of scenarios, such as interaction for augmented reality applications, and leveraging daily surfaces and objects for ad-hoc interactions.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397309
ER  - 

TY  - NA
AU  - Rao, Anyi; Xu, Linning; Lin, Dahua
TI  - Shoot360: Normal View Video Creation from City Panorama Footage
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3528233.3530702
ER  - 

TY  - NA
AU  - Hube, Natalie; Achberger, Alexander; Liepert, Philipp; Vogelsang, Jonas; Vidackovic, Kresimir; Sedlmair, Michael
TI  - Study on the Influence of Upper Limb Representations and Haptic Feedback in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00172
ER  - 

TY  - JOUR
AU  - Franček, Petar; Jambrošić, Kristian; Horvat, Marko; Planinec, Vedran
TI  - The Performance of Inertial Measurement Unit Sensors on Various Hardware Platforms for Binaural Head-Tracking Applications
PY  - 2023
AB  - <jats:p>Binaural synthesis with head tracking is often used in spatial audio systems. The devices used for head tracking must provide data on the orientation of the listener’s head. These data need to be highly accurate, and they need to be provided as fast and as frequently as possible. Therefore, head-tracking devices need to be equipped with high-quality inertial measurement unit (IMU) sensors. Since IMUs readily include triaxial accelerometers, gyroscopes, and magnetometers, it is crucial that all of these sensors perform well, as the head orientation is calculated from all sensor outputs. This paper discusses the challenges encountered in the process of the performance assessment of IMUs through appropriate measurements. Three distinct hardware platforms were investigated: five IMU sensors either connected to Arduino-based embedded systems or being an integral part of one, five smartphones across a broad range of overall quality with integrated IMUs, and a commercial virtual reality unit that utilizes a headset with integrated IMUs. An innovative measurement method is presented and proposed for comparing the performance of sensors on all three platforms. The results of the measurements performed using the proposed method show that all three investigated platforms are adequate for the acquisition of the data required for calculating the orientation of a device as the input to the binaural synthesis process. Some limitations that have been observed during the measurements, regarding data acquisition and transfer, are discussed.</jats:p>
SP  - 872
EP  - NA
JF  - Sensors
VL  - 23
IS  - 2
PB  - 
DO  - 10.3390/s23020872
ER  - 

TY  - NA
AU  - Lu, Feiyu; Xu, Yan
TI  - Exploring Spatial UI Transition Mechanisms with Head-Worn Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517723
ER  - 

TY  - NA
AU  - Tran O'Leary, Jasper; Jun, Eunice; Peek, Nadya
TI  - Improving Programming for Exploratory Digital Fabrication with Inline Machine Control and Styled Toolpath Visualizations
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3561998
ER  - 

TY  - NA
AU  - Li, Jiahao; Samoylov, Alexis; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - Roman: Making Everyday Objects Robotically Manipulable with 3D-Printable Add-on Mechanisms
PY  - 2022
AB  - One important vision of robotics is to provide physical assistance by manipulating different everyday objects, e.g., hand tools, kitchen utensils. However, many objects designed for dexterous hand-control are not easily manipulable by a single robotic arm with a generic parallel gripper. Complementary to existing research on developing grippers and control algorithms, we present Roman, a suite of hardware design and software tool support for robotic engineers to create 3D printable mechanisms attached to everyday handheld objects, making them easier to be manipulated by conventional robotic arms. The Roman hardware comes with a versatile magnetic gripper that can snap on/off handheld objects and drive add-on mechanisms to perform tasks. Roman also provides software support to register and author control programs. To validate our approach, we designed and fabricated Roman mechanisms for 14 everyday objects/tasks presented within a design space and conducted expert interviews with robotic engineers indicating that Roman serves as a practical alternative for enabling robotic manipulation of everyday objects.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501818
ER  - 

TY  - NA
AU  - Davari, Shakiba
TI  - [DC] Context-Aware Inference and Adaptation in Augmented Reality
PY  - 2022
AB  - Augmented Reality(AR) offers the potential for easy and efficient information access, reinforcing the wide belief that AR Glasses are the next-generation of personal computing devices. However, to realize this all-day AR vision, the AR interface must be able to address the challenges that constant and pervasive presence of virtual content can cause for the user. The optimal interface, that is the most efficient yet least intrusive, in one context may be the worst interface for another context. Throughout the day, as the user switches context, an optimal all-day interface must adapts its virtual content display and interactions as well. This work aims to propose a research agenda to design and validate different adaptation techniques and context-aware AR interfaces and introduce a framework for the design of such intelligent interfaces.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00320
ER  - 

TY  - NA
AU  - Gobert, Camille; Beaudouin-Lafon, Michel
TI  - i-LaTeX : Manipulating Transitional Representations between LaTeX Code and Generated Documents
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517494
ER  - 

TY  - CONF
AU  - McNutt, Andrew; Chugh, Ravi
TI  - CHI - Integrated Visualization Editing via Parameterized Declarative Templates
PY  - 2021
AB  - Interfaces for creating visualizations typically embrace one of several common forms. Textual specification enables fine-grained control, shelf building facilitates rapid exploration, while chart choosing promotes immediacy and simplicity. Ideally these approaches could be unified to integrate the user- and usage-dependent benefits found in each modality, yet these forms remain distinct. We propose parameterized declarative templates, a simple abstraction mechanism over JSON-based visualization grammars, as a foundation for multimodal visualization editors. We demonstrate how templates can facilitate organization and reuse by factoring the more than 160 charts that constitute Vega-Lite’s example gallery into approximately 40 templates. We exemplify the pliability of abstracting over charting grammars by implementing—as a template—the functionality of the shelf builder Polestar (a simulacra of Tableau) and a set of templates that emulate the Google Sheets chart chooser. We show how templates support multimodal visualization editing by implementing a prototype and evaluating it through an approachability study.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Ke, Pingchuan; Cai, Shaoyu; Xu, Lantian; Zhu, Kening
TI  - Weighted Walking: Propeller-based On-leg Force Simulation of Walking in Fluid Materials in VR
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2021 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3476122.3484842
ER  - 

TY  - JOUR
AU  - Park, K; Yuk, H; Yang, M; Cho, J; Lee, H; Kim, J
TI  - A biomimetic elastomeric robot skin using electrical impedance and acoustic tomography for tactile sensing.
PY  - 2022
AB  - Human skin perceives physical stimuli applied to the body and mitigates the risk of physical interaction through its soft and resilient mechanical properties. Social robots would benefit from whole-body robotic skin (or tactile sensors) resembling human skin in realizing a safe, intuitive, and contact-rich human-robot interaction. However, existing soft tactile sensors show several drawbacks (complex structure, poor scalability, and fragility), which limit their application in whole-body robotic skin. Here, we introduce biomimetic robotic skin based on hydrogel-elastomer hybrids and tomographic imaging. The developed skin consists of a tough hydrogel and a silicone elastomer forming a skin-inspired multilayer structure, achieving sufficient softness and resilience for protection. The sensor structure can also be easily repaired with adhesives even after severe damage (incision). For multimodal tactile sensation, electrodes and microphones are deployed in the sensor structure to measure local resistance changes and vibration due to touch. The ionic hydrogel layer is deformed owing to an external force, and the resulting local conductivity changes are measured via electrodes. The microphones also detect the vibration generated from touch to determine the location and type of dynamic tactile stimuli. The measurement data are then converted into multimodal tactile information through tomographic imaging and deep neural networks. We further implement a sensorized cosmetic prosthesis, demonstrating that our design could be used to implement deformable or complex-shaped robotic skin.
SP  - eabm7187
EP  - NA
JF  - Science robotics
VL  - 7
IS  - 67
PB  - 
DO  - 10.1126/scirobotics.abm7187
ER  - 

TY  - JOUR
AU  - Sharma, Adwait; Salchow-Hömmen, Christina; Mollyn, Vimal Suresh; Nittala, Aditya Shekhar; Hedderich, Michael A.; Koelle, Marion; Seel, Thomas; Steimle, Jürgen
TI  - SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures
PY  - 2022
AB  - <jats:p> Gestural interaction with freehands and while grasping an everyday object enables <jats:italic>always-available input</jats:italic> . To sense such gestures, minimal instrumentation of the user’s hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present <jats:italic>SparseIMU</jats:italic> , a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios. </jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569894
ER  - 

TY  - JOUR
AU  - Tabrizian, Seyedreza Kashef; Sahraeeazartamar, Fatemeh; Brancart, Joost; Roels, Ellen; Ferrentino, Pasquale; Legrand, Julie; Van Assche, Guy; Vanderborght, Bram; Terryn, Seppe
TI  - A Healable Resistive Heater as a Stimuli-Providing System in Self-Healing Soft Robots
PY  - 2022
AB  - NA
SP  - 4574
EP  - 4581
JF  - IEEE Robotics and Automation Letters
VL  - 7
IS  - 2
PB  - 
DO  - 10.1109/lra.2022.3150033
ER  - 

TY  - NA
AU  - Yasu, Kentaro
TI  - MagneShape: A Non-electrical Pin-Based Shape-Changing Display
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545645
ER  - 

TY  - BOOK
AU  - Feit, Anna Maria; Vordemann, Lukas; Park, Seonwook; Bérubé, Caterina; Hilliges, Otmar
TI  - ETRA - Detecting Relevance during Decision-Making from Eye Movements for UI Adaptation
PY  - 2020
AB  - This paper proposes an approach to detect information relevance during decision-making from eye movements in order to enable user interface adaptation. This is a challenging task because gaze behavior varies greatly across individual users and tasks and ground-truth data is difficult to obtain. Thus, prior work has mostly focused on simpler target-search tasks or on establishing general interest, where gaze behavior is less complex. From the literature, we identify six metrics that capture different aspects of the gaze behavior during decision-making and combine them in a voting scheme. We empirically show, that this accounts for the large variations in gaze behavior and out-performs standalone metrics. Importantly, it offers an intuitive way to control the amount of detected information, which is crucial for different UI adaptation schemes to succeed. We show the applicability of our approach by developing a room-search application that changes the visual saliency of content detected as relevant. In an empirical study, we show that it detects up to 97% of relevant elements with respect to user self-reporting, which allows us to meaningfully adapt the interface, as confirmed by participants. Our approach is fast, does not need any explicit user input and can be applied independent of task and user.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Biener, Verena; Ofek, Eyal; Pahud, Michel; Kristensson, Per Ola; Grubert, Jens
TI  - Extended Reality for Knowledge Work in Everyday Environments
PY  - 2021
AB  - Virtual and Augmented Reality have the potential to change information work. The ability to modify the workers senses can transform everyday environments into a productive office, using portable head-mounted displays combined with conventional interaction devices, such as keyboards and tablets. While a stream of better, cheaper and lighter HMDs have been introduced for consumers in recent years, there are still many challenges to be addressed to allow this vision to become reality. This chapter summarizes the state of the art in the field of extended reality for knowledge work in everyday environments and proposes steps to address the open challenges.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Beier, Sofie; Berlow, Sam; Boucaud, Esat; Bylinskii, Zoya; Cai, Tianyuan; Cohn, Jenae; Crowley, Kathy; Day, Stephanie L.; Dingler, Tilman; Dobres, Jonathan; Healey, Jennifer; Jain, Rajiv; Jordan, Marjorie; Kerr, Bernard; Li, Qisheng; Miller, Dave B.; Nobles, Susanne; Papoutsaki, Alexandra; Qian, Jing; Rezvanian, Tina; Rodrigo, Shelley; Sawyer, Ben D.; Sheppard, Shannon M.; Stein, Bram; Treitman, Rick; Vanek, Jen; Wallace, Shaun; Wolfe, Benjamin E.
TI  - Readability Research: An Interdisciplinary Approach
PY  - 2021
AB  - Readability is on the cusp of a revolution. Fixed text is becoming fluid as a proliferation of digital reading devices rewrite what a document can do. As past constraints make way for more flexible opportunities, there is great need to understand how reading formats can be tuned to the situation and the individual. We aim to provide a firm foundation for readability research, a comprehensive framework for modern, multi-disciplinary readability research. Readability refers to aspects of visual information design which impact information flow from the page to the reader. Readability can be enhanced by changes to the set of typographical characteristics of a text. These aspects can be modified on-demand, instantly improving the ease with which a reader can process and derive meaning from text. We call on a multi-disciplinary research community to take up these challenges to elevate reading outcomes and provide the tools to do so effectively.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Moro, Satoshi; Komuro, Takashi
TI  - Generation of Virtual Reality Environment Based on 3D Scanned Indoor Physical Space
PY  - 2021
AB  - NA
SP  - 492
EP  - 503
JF  - Advances in Visual Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90439-5_39
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Ofek, Eyal; Sinclair, Mike; Leithinger, Daniel; Gonzalez-Franco, Mar
TI  - HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots.
PY  - 2021
AB  - HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability -- these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user's hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.
SP  - 1269
EP  - 1281
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474821
ER  - 

TY  - NA
AU  - Bouzbib, Elodie; Bailly, Gilles; Haliyo, Sinan; Frey, Pascal
TI  - UIST - CoVR: A Large-Scale Force-Feedback Robotic Interface for Non-Deterministic Scenarios in VR
PY  - 2020
AB  - We present CoVR, a novel robotic interface providing strong kinesthetic feedback (100 N) in a room-scale VR arena. It consists of a physical column mounted on a 2D Cartesian ceiling robot (XY displacements) with the capacity of (1) resisting to body-scaled users' actions such as pushing or leaning; (2) acting on the users by pulling or transporting them as well as (3) carrying multiple potentially heavy objects (up to 80kg) that users can freely manipulate or make interact with each other. We describe its implementation and define a trajectory generation algorithm based on a novel user intention model to support non-deterministic scenarios, where the users are free to interact with any virtual object of interest with no regards to the scenarios' progress. A technical evaluation and a user study demonstrate the feasibility and usability of CoVR, as well as the relevance of whole-body interactions involving strong forces, such as being pulled through or transported.
SP  - 209
EP  - 222
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415891
ER  - 

TY  - NA
AU  - Chang, Joseph Chee; Hahn, Nathan; Kittur, Aniket
TI  - UIST - Mesh: Scaffolding Comparison Tables for Online Decision Making
PY  - 2020
AB  - While there is an enormous amount of information online for making decisions such as choosing a product, restaurant, or school, it can be costly for users to synthesize that information into confident decisions. Information for users' many different criteria needs to be gathered from many different sources into a structure where they can be compared and contrasted. The usefulness of each criterion for differentiating potential options can be opaque to users, and evidence such as reviews may be subjective and conflicting, requiring users to interpret each under their personal context. We introduce Mesh, which scaffolds users to iteratively build up a better understanding of both their criteria and options by evaluating evidence gathered across sources in the context of consumer decision-making. Mesh bridges the gap between decision support systems that typically have rigid structures and the fluid and dynamic process of exploratory search, changing the cost structure to provide increasing payoffs with greater user investment. Our lab and field deployment studies found evidence that Mesh significantly reduces the costs of gathering and evaluating evidence and scaffolds decision-making through personalized criteria enabling users to gain deeper insights from data.
SP  - 391
EP  - 405
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415865
ER  - 

TY  - JOUR
AU  - Lovreglio, Ruggiero; Kinateder, Max
TI  - Augmented reality for pedestrian evacuation research: Promises and limitations
PY  - 2020
AB  - NA
SP  - 104750
EP  - NA
JF  - Safety Science
VL  - 128
IS  - NA
PB  - 
DO  - 10.1016/j.ssci.2020.104750
ER  - 

TY  - NA
AU  - Qian, Jing; Young-Ng, Meredith; Li, Xiangyu; Cheung, Angel; Yang, Fumeng; Huang, Jeff
TI  - CHI Extended Abstracts - Portalware: A Smartphone-Wearable Dual-Display System for Expanding the Free-Hand Interaction Region in Augmented Reality
PY  - 2020
AB  - Free-hand manipulation in smartphone augmented reality (AR) enables users to directly interact with virtual contents using their hands. However, human hands can ergonomically move in a broader range than a smartphone's field of view (FOV) can capture, requiring users to be aware of the limited usable interaction and viewing regions at all times. We present Portalware, a smartphone-wearable dual-display system that expands the usable interaction region for free-hand manipulation and enables users to receive visual feedback outside the smartphone's view. The wearable is a lightweight, low-cost display that shares the same AR environment in real-time with the smartphone.This setup empowers AR applications such as mid-air drawing and object manipulation by providing a 180-degree horizontal interaction region in front of the user. Other potential applications include wearing the smartphone like a pendant while using Portalware to continue interacting with AR objects. Without having to hold their phone up with their hand, users can benefit from resting their arms as needed. Finally, we discuss usability explorations, potential interactions, and future plans for empirical studies.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383079
ER  - 

TY  - NA
AU  - Onishi, Yuki; Takashima, Kazuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - CHI Extended Abstracts - Self-actuated Stretchable Partitions for Dynamically Creating Secure Workplaces
PY  - 2021
AB  - The configuration of office environments is related to worker satisfaction and can improve work efficiency. Although open offices promote communication among co-workers, privacy issues surface as well as increased risks from such health concerns as viral infections and pandemics. On the other hand, territorial offices protect worker privacy and reduce infection risks. Unfortunately, such arrangements often hinder communication among co-workers. Although a physical office must satisfy different needs under various scenarios with limited space, the lack of flexibility in furniture designs prevents dynamic space management. In this paper, we propose a self-actuated stretchable partition whose physical height, width, and position can dynamically change to create secure workplaces (e.g., against privacy and infectious risks) without inhibiting group collaboration. To support secure workplace layouts and space reconfigurations, the partitions’ height, length, and position are adapted automatically and dynamically. First, we consider the design space of a self-actuated stretchable partition and implement a proof-of concept prototype with a height-adjustable stand, a roll-up screen, and a mobile robot. We then show some example application scenarios to discuss potential future actuated-territorial offices.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451607
ER  - 

TY  - NA
AU  - Arimatsu, Kazuyuki; Mori, Hideki
TI  - CHI - Evaluation of Machine Learning Techniques for Hand Pose Estimation on Handheld Device with Proximity Sensor
PY  - 2020
AB  - Tracking finger movement for natural interaction using hand is commonly studied. For vision-based implementations of finger tracking in virtual reality (VR) application, finger movement is occluded by a handheld device which is necessary for auxiliary input, thus tracking finger movement using cameras is still challenging. Finger tracking controllers using capacitive proximity sensors on the surface are starting to appear. However, research on estimating articulated hand pose from curved capacitance sensing electrodes is still immature. Therefore, we built a prototype with 62 electrodes and recorded training datasets using an optical tracking system. We have introduced 2.5D representation to apply convolutional neural network methods on a capacitive image of the curved surface, and two types of network architectures based on recent achievements in the computer vision field were evaluated with our dataset. We also implemented real-time interactive applications using the prototype and demonstrated the possibility of intuitive interaction using fingers in VR applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376712
ER  - 

TY  - NA
AU  - Liu, Michael Xieyang; Kuznetsov, Andrew; Kim, Yongsung; Chang, Joseph Chee; Kittur, Aniket; Myers, Brad A.
TI  - Wigglite: Low-cost Information Collection and Triage
PY  - 2022
AB  - Consumers conducting comparison shopping, researchers making sense of competitive space, and developers looking for code snippets online all face the challenge of capturing the information they find for later use without interrupting their current flow. In addition, during many learning and exploration tasks, people need to externalize their mental context, such as estimating how urgent a topic is to follow up on, or rating a piece of evidence as a "pro" or "con," which helps scaffold subsequent deeper exploration. However, current approaches incur a high cost, often requiring users to select, copy, context switch, paste, and annotate information in a separate document without offering specific affordances that capture their mental context. In this work, we explore a new interaction technique called "wiggling," which can be used to fluidly collect, organize, and rate information during early sensemaking stages with a single gesture. Wiggling involves rapid back-and-forth movements of a pointer or up-and-down scrolling on a smartphone, which can indicate the information to be collected and its valence, using a single, light-weight gesture that does not interfere with other interactions that are already available. Through implementation and user evaluation, we found that wiggling helped participants accurately collect information and encode their mental context with a 58% reduction in operational cost while being 24% faster compared to a common baseline.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545661
ER  - 

TY  - NA
AU  - Garcia, Roderico; Cuellar-Mejia, Maria Jose; Cruz-Ortiz, David; Ballesteros, Mariana; Huegel, Joel C.
TI  - Development of a joystick with a two-degree-of-freedom mechanism based on cable-capstan transmission
PY  - 2022
AB  - This work presents the development of a joystick with two degrees of freedom. The joystick is designed considering a cable-capstan mechanism allowing the movement of the central axis for Pitch and Roll angles, where the central axis has a workspace in a plane (XY). The joystick design and instrumentation consider future haptic tasks and applications in practical engineering education courses. This device was designed based on fast prototype manufacturing, including 3D printed parts. The design is small and portable, and the development of the joystick includes a user interface to display and save data and graphs of different signals such as angular positions, central axis position, control signals, current of the motors, and errors, and allows tuning different parameters for the control implementation. In order to show the functionality of the designed mechanism, we show its performance in tracking trajectory tasks implementing a proportional, integral, derivative controller (PIDC) with a Super Twisting algorithm (STA). The results showed the joystick with the complete instrumentation and communication with the user interface in a personal computer. We compared the PIDC using an Euler's derivative vs. the PIDC with the STA's derivative estimated. The results of the trajectory tracking control with the PIDC-STA algorithm showed a better performance of the joystick with an average error norm of 2.73 degrees, taking the Pitch and Roll errors, compared with the PIDC- Euler controller with an average error norm of 3.22 degrees.
SP  - NA
EP  - NA
JF  - 2022 8th International Conference on Control, Decision and Information Technologies (CoDIT)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/codit55151.2022.9804116
ER  - 

TY  - NA
AU  - Dobinson, Rhett; Teyssier, Marc; Steimle, Jürgen; Fruchard, Bruno
TI  - MicroPress: Detecting Pressure and Hover Distance in Thumb-to-Finger Interactions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567698
ER  - 

TY  - NA
AU  - Du, Pei; Bulusu, Nirupama
TI  - ASSETS - An automated AR-based annotation tool for indoor navigation for visually impaired people
PY  - 2021
AB  - Low vision people face many daily encumbrances. Traditional visual enhancements do not suffice to navigate indoor environments, or recognize objects efficiently. In this paper, we explore how Augmented Reality (AR) can be leveraged to design mobile applications to improve visual experience and unburden low vision persons. Specifically, we propose a novel automated AR-based annotation tool for detecting and labeling salient objects for assisted indoor navigation applications like NearbyExplorer. NearbyExplorer, which issues audio descriptions of nearby objects to the users, relies on a database populated by large teams of volunteers and map-a-thons to manually annotate salient objects in the environment like desks, chairs, low overhead ceilings. This has limited widespread and rapid deployment. Our tool builds on advances in automated object detection, AR labeling and accurate indoor positioning to provide an automated way to upload object labels and user position to a database, requiring just one volunteer. Moreover, it enables low vision people to detect and notice surrounding objects quickly using smartphones in various indoor environments.
SP  - NA
EP  - NA
JF  - The 23rd International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3441852.3476561
ER  - 

TY  - NA
AU  - Lee, DoYoung; Kim, Jiwan; Oakley, Ian
TI  - CHI - FingerText: Exploring and Optimizing Performance for Wearable, Mobile and One-Handed Typing
PY  - 2021
AB  - Typing on wearables while situationally impaired, such as while walking, is challenging. However, while HCI research on wearable typing is diverse, existing work focuses on stationary scenarios and fine-grained input that will likely perform poorly when users are on-the-go. To address this issue we explore single-handed wearable typing using intra-hand touches between the thumb and fingers, a modality we argue will be robust to the physical disturbances inherent to input while mobile. We first examine the impact of walking on performance of these touches, noting no significant differences in accuracy or speed, then feed our study data into a multi-objective optimization process in order to design keyboard layouts (for both five and ten keys) capable of supporting rapid, accurate, comfortable, and unambiguous typing. A final study tests these layouts against QWERTY baselines and reports performance improvements of up to 10.45% WPM and 39.44% WER when users type while walking.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445106
ER  - 

TY  - CHAP
AU  - , 
TI  - Summary
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544578
ER  - 

TY  - JOUR
AU  - Weir, Kurtis; Loizides, Fernando; Nahar, Vinita; Aggoun, Amar; Pollard, Andrew
TI  - I see therefore i read: improving the reading capabilities of individuals with visual disabilities through immersive virtual reality
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - Universal Access in the Information Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10209-021-00854-8
ER  - 

TY  - NA
AU  - Baldi, Tommaso Lisini; Paolocci, Gianluca; Barcelli, Davide; Prattichizzo, Domenico
TI  - MED - Mobile Augmented Reality Integrating Fingertip Haptic Devices and Wrist-Worn Visual Displays
PY  - 2021
AB  - In the context of Mobile Augmented Reality (MAR), hand-based interaction with virtual environments is still an open challenge. Virtual objects projected on the smartphone screen are on a different layer than the camera-captured images, hence the user’s hand cannot get in contact with virtual entities. In this work, we present an improvement to a previously published proof-of-concept that aims at overcoming current MAR limitations by mapping the hand in the virtual reference. To this end, we developed an ad hoc ring interface for finger tracking and haptic feedback, designed with wearability in mind. A wrist-worn display compacts the hardware on a single-arm, leaving the other completely free. The overall system is affordable thanks to the use of the smart-phone as core technology and provides "on-demand" augmented reality, tailored for daily activities scenarios. Eighteen users were enrolled to compare the wristband based system and the standard hand-held smartphone montage. Quantitative and qualitative results show that our system was positively received by the subjects, that completed the experimental task achieving similar performance in the two conditions but expressed their preference for the wrist-worn display.
SP  - 578
EP  - 583
JF  - 2021 29th Mediterranean Conference on Control and Automation (MED)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/med51440.2021.9480285
ER  - 

TY  - NA
AU  - Feick, Martin; Bateman, Scott; Tang, Anthony; Miede, André; Marquardt, Nicolai
TI  - ISMAR - Tangi: Tangible Proxies For Embodied Object Exploration And Manipulation In Virtual Reality
PY  - 2020
AB  - Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi’s Manipulators and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi’s proxies outperform traditional controllers and were generally favored by participants.
SP  - 195
EP  - 206
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00042
ER  - 

TY  - BOOK
AU  - Wang, Liwen; Sandor, Christian
TI  - EuroXR - Can You Perceive the Size Change? Discrimination Thresholds for Size Changes in Augmented Reality.
PY  - 2021
AB  - NA
SP  - 25
EP  - 36
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Lei, Wentao; Fan, Mingming; Thang, Juliann
TI  - "I Shake The Package To Check If It's Mine"
PY  - 2022
AB  - With about 230 million packages delivered per day in 2020, fetching packages has become a routine for many city dwellers in China. When fetching packages, people usually need to go to collection sites of their apartment complexes or a KuaiDiGui, an increasingly popular type of self-service package pickup machine. However, little is known whether such processes are accessible to blind and low vision (BLV) city dwellers. We interviewed BLV people (N=20) living in a large metropolitan area in China to understand their practices and challenges of fetching packages. Our findings show that participants encountered difficulties in finding the collection site and localizing and recognizing their packages. When fetching packages from KuaiDiGuis, they had difficulty in identifying the correct KuaiDiGui, interacting with its touch screen, navigating the complex on-screen workflow, and opening the target compartment. We discuss design considerations to make the package fetching process more accessible to the BLV community.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502063
ER  - 

TY  - JOUR
AU  - Caeiro-Rodriguez, Manuel; Otero-González, Iván; Mikic-Fonte, Fernando A.; Llamas-Nistal, Martin
TI  - A Systematic Review of Commercial Smart Gloves: Current Status and Applications.
PY  - 2021
AB  - Smart gloves have been under development during the last 40 years to support human-computer interaction based on hand and finger movement. Despite the many devoted efforts and the multiple advances in related areas, these devices have not become mainstream yet. Nevertheless, during recent years, new devices with improved features have appeared, being used for research purposes too. This paper provides a review of current commercial smart gloves focusing on three main capabilities: (i) hand and finger pose estimation and motion tracking, (ii) kinesthetic feedback, and (iii) tactile feedback. For the first capability, a detailed reference model of the hand and finger basic movements (known as degrees of freedom) is proposed. Based on the PRISMA guidelines for systematic reviews for the period 2015-2021, 24 commercial smart gloves have been identified, while many others have been discarded because they did not meet the inclusion criteria: currently active commercial and fully portable smart gloves providing some of the three main capabilities for the whole hand. The paper reviews the technologies involved, main applications and it discusses about the current state of development. Reference models to support end users and researchers comparing and selecting the most appropriate devices are identified as a key need.
SP  - 2667
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 21
IS  - 8
PB  - 
DO  - 10.3390/s21082667
ER  - 

TY  - NA
AU  - Sato, Hiroki; Seong, Young ah; Yamamura, Ryosuke; Hayashi, Hiromasa; Hata, Katsuhiro; Ogata, Hisato; Niiyama, Ryuma; Kawahara, Yoshihiro
TI  - CHI Extended Abstracts - Soft yet Strong Inflatable Structures for a Foldable and Portable Mobility
PY  - 2020
AB  - We envision "Soft Mobility," which is a new type of personal mobility made of soft, lightweight, and inflatable materials. A soft body enables safer user interactions with pedestrians and drivers; the lightweight and inflatable properties allow the users to easily carry it as a "portable device," by deflating, folding, and packing it into, for example, a backpack. As embodiment of such soft mobility, we prototyped "poimo" (POrtable and Inflatable MObility). To evaluate the poimo, we first conducted mechanical tests to verify that it can endure the weight of a human. We also measured the time needed to inflate / deflate it to demonstrate its property. Finally, we reported on the preliminary results of riding on it, and clarified the requirements to further investigate and implement soft mobility transport.
SP  - 1
EP  - 4
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383147
ER  - 

TY  - CHAP
AU  - Tsukuda, Yuga; Tagami, Daichi; Sadasue, Masaaki; Suzuki, Shieru; Lu, Jun-Li; Ochiai, Yoichi
TI  - Calmbots: Exploring Madagascar Cockroaches as Living Ubiquitous Interfaces
PY  - 2022
AB  - NA
SP  - 522
EP  - 541
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05028-2_35
ER  - 

TY  - NA
AU  - Gupta, Aakar; Samad, Majed; Kin, Kenrick; Kristensson, Per Ola; Benko, Hrvoje
TI  - ISMAR - Investigating Remote Tactile Feedback for Mid-Air Text-Entry in Virtual Reality
PY  - 2020
AB  - In this paper, we investigate the utility of remote tactile feedback for freehand text-entry on a mid-air Qwerty keyboard in VR. To that end, we use insights from prior work to design a virtual keyboard along with different forms of tactile feedback, both spatial and non-spatial, for fingers and for wrists. We report on a multi-session text-entry study with 24 participants where we investigated four vibrotactile feedback conditions: on-fingers, on-wrist spatialized, on-wrist non-spatialized, and audio-visual only. We use micro-metrics analyses and participant interviews to analyze the mechanisms underpinning the observed performance and user experience. The results show comparable performance across feedback types. However, participants overwhelmingly prefer the tactile feedback conditions and rate on-fingers feedback as significantly lower in mental demand, frustration, and effort. Results also show that spatialization of vibrotactile feedback on the wrist as a way to provide finger-specific feedback is comparable in performance and preference to a single vibration location. The micro-metrics analyses suggest that users compensated for the lack of tactile feedback with higher visual and cognitive attention, which ensured similar performance but higher user effort.
SP  - 350
EP  - 360
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00062
ER  - 

TY  - JOUR
AU  - Moran-Ledesma, Marco; Schneider, Oliver; Hancock, Mark
TI  - User-Defined Gestures with Physical Props in Virtual Reality
PY  - 2021
AB  - When interacting with virtual reality (VR) applications like CAD and open-world games, people may want to use gestures as a means of leveraging their knowledge from the physical world. However, people may prefer physical props over handheld controllers to input gestures in VR. We present an elicitation study where 21 participants chose from 95 props to perform manipulative gestures for 20 CAD-like and open-world game-like referents. When analyzing this data, we found existing methods for elicitation studies were insufficient to describe gestures with props, or to measure agreement with prop selection (i.e., agreement between sets of items). We proceeded by describing gestures as context-free grammars, capturing how different props were used in similar roles in a given gesture. We present gesture and prop agreement scores using a generalized agreement score that we developed to compare multiple selections rather than a single selection. We found that props were selected based on their resemblance to virtual objects and the actions they afforded; that gesture and prop agreement depended on the referent, with some referents leading to similar gesture choices, while others led to similar prop choices; and that a small set of carefully chosen props can support multiple gestures.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - ISS
PB  - 
DO  - 10.1145/3486954
ER  - 

TY  - NA
AU  - Li, Jiahao; Cui, Meilin; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - UIST - Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities
PY  - 2020
AB  - Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.
SP  - 897
EP  - 911
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415826
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Tan, Lu; Zhang, Yuji; Seyed, Teddy; Yang, Xing-Dong
TI  - UIST - Capacitivo: Contact-Based Object Recognition on Interactive Fabrics using Capacitive Sensing
PY  - 2020
AB  - We present Capacitivo, a contact-based object recognition technique developed for interactive fabrics, using capacitive sensing. Unlike prior work that has focused on metallic objects, our technique recognizes non-metallic objects such as food, different types of fruits, liquids, and other types of objects that are often found around a home or in a workplace. To demonstrate our technique, we created a prototype composed of a 12 x 12 grid of electrodes, made from conductive fabric attached to a textile substrate. We designed the size and separation between the electrodes to maximize the sensing area and sensitivity. We then used a 10-person study to evaluate the performance of our sensing technique using 20 different objects, which yielded a 94.5% accuracy rate. We conclude this work by presenting several different application scenarios to demonstrate unique interactions that are enabled by our technique on fabrics.
SP  - 649
EP  - 661
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415829
ER  - 

TY  - NA
AU  - Pang, Yuren; Reinecke, Katharina; Just, René
TI  - Apéritif: Scaffolding Preregistrations to Automatically Generate Analysis Code and Methods Descriptions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517707
ER  - 

TY  - BOOK
AU  - Auda, Jonas; Gruenefeld, Uwe; Schneegass, Stefan
TI  - Mensch und Computer - Enabling Reusable Haptic Props for Virtual Reality by Hand Displacement
PY  - 2021
AB  - Virtual Reality (VR) enables compelling visual experiences. However, providing haptic feedback is still challenging. Previous work suggests utilizing haptic props to overcome such limitations and presents evidence that props could function as a single haptic proxy for several virtual objects. In this work, we displace users’ hands to account for virtual objects that are smaller or larger. Hence, the used haptic prop can represent several differently-sized virtual objects. We conducted a user study (N = 12) and presented our participants with two tasks during which we continuously handed them the same haptic prop but they saw in VR differently-sized virtual objects. In the first task, we used a linear hand displacement and increased the size of the virtual object to understand when participants perceive a mismatch. In the second task, we compare the linear displacement to logarithmic and exponential displacements. We found that participants, on average, do not perceive the size mismatch for virtual objects up to 50% larger than the physical prop. However, we did not find any differences between the explored different displacement. We conclude our work with future research directions.
SP  - 412
EP  - 417
JF  - Mensch und Computer 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3473856.3474000
ER  - 

TY  - JOUR
AU  - Yang, Ciyuan; Xu, Shuchang; Yu, Tianyu; Liu, Guanhong; Yu, Chun; Shi, Yuanchun
TI  - LightGuide: Directing Visually Impaired People along a Path Using Light Cues
PY  - 2021
AB  - Precise and reliable directional feedback is crucial for electronic traveling aids that guide visually impaired people along safe paths. A large proportion of visually impaired people can determine light position using their light perception. This work presents LightGuide, a directional feedback solution that indicates a safe direction of travel via the position of a light within the user's visual field. We prototyped LightGuide using an LED strip attached to the brim of a cap, and conducted three user studies to explore the effectiveness of LightGuide compared to HapticBag, a state-of-the-art baseline solution that indicates directions through on-shoulder vibrations. Results showed that, with LightGuide, participants turned to target directions in place more quickly and smoothly, and navigated along basic and complex paths more efficiently, smoothly, and accurately than HapticBag. Users' subjective feedback implied that LightGuide was easy to learn and intuitive to use. The potential limitations of using LightGuide in real environments are subsequently discussed.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463524
ER  - 

TY  - JOUR
AU  - do Rêgo, Beatriz Brito; Amorim, Caique Yan Conceição de; Sodré, Suyane Miranda; Garrido, Filipe Adeodato; Matos, Ecivaldo de Souza
TI  - Investigation on Equity and Otherness in the Interaction Design Process: A Systematic Mapping
PY  - 2022
AB  - NA
SP  - 1
EP  - 13
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2109251
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Chen, Jingya; Xia, Haijun; Mitchell, Tom M.; Myers, Brad A.
TI  - UIST - Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs
PY  - 2020
AB  - A major problem in task-oriented conversational agents is the lack of support for the repair of conversational breakdowns. Prior studies have shown that current repair strategies for these kinds of errors are often ineffective due to: (1) the lack of transparency about the state of the system's understanding of the user's utterance; and (2) the system's limited capabilities to understand the user's verbal attempts to repair natural language understanding errors. This paper introduces SOVITE, a new multi-modal speech plus direct manipulation interface that helps users discover, identify the causes of, and recover from conversational breakdowns using the resources of existing mobile app GUIs for grounding. SOVITE displays the system's understanding of user intents using GUI screenshots, allows users to refer to third-party apps and their GUI screens in conversations as inputs for intent disambiguation, and enables users to repair breakdowns using direct manipulation on these screenshots. The results from a remote user study with 10 users using SOVITE in 7 scenarios suggested that SOVITE's approach is usable and effective.
SP  - 1094
EP  - 1107
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415820
ER  - 

TY  - JOUR
AU  - Triantafyllidis, Eleftherios; McGreavy, Christopher; Gu, Jiacheng; Li, Zhibin
TI  - Study of Multimodal Interfaces and the Improvements on Teleoperation
PY  - 2020
AB  - Research in multimodal interfaces aims to provide immersive solutions and to increase overall human performance. A promising direction is to combine auditory, visual and haptic interaction between the user and the simulated environment. However, no extensive comparison exists to show how combining audiovisuohaptic interfaces would affect human perception and by extent reflected on task performance. Our paper explores this idea and presents a thorough, full-factorial comparison of how all combinations of audio, visual and haptic interfaces affect performance during manipulation. We evaluated how each combination affects the performance in a study ( $N=25$ ) consisting of manipulation tasks with various difficulties. The overall performance was assessed using both subjective measures, by assessing cognitive workload and system usability, and objective measurements, by incorporating time and spatial accuracy-based metrics. The results showed that regardless of task complexity, the combination of stereoscopic-vision with the virtual reality headset increased performance across all measurements by 40%, compared to monocular-vision from a generic display monitor. Besides, using haptic feedback improved outcomes by 10% and auditory feedback accounted for approximately 5% improvement.
SP  - 78213
EP  - 78227
JF  - IEEE Access
VL  - 8
IS  - NA
PB  - 
DO  - 10.1109/access.2020.2990080
ER  - 

TY  - JOUR
AU  - Liu, Michael Xieyang; Kittur, Aniket; Myers, Brad A.
TI  - To Reuse or Not To Reuse? A Framework and System for Evaluating Summarized Knowledge
PY  - 2021
AB  - As the amount of information online continues to grow, a correspondingly important opportunity is for individuals to reuse knowledge which has been summarized by others rather than starting from scratch. However, appropriate reuse requires judging the relevance, trustworthiness, and thoroughness of others' knowledge in relation to an individual's goals and context. In this work, we explore augmenting judgements of the appropriateness of reusing knowledge in the domain of programming, specifically of reusing artifacts that result from other developers' searching and decision making. Through an analysis of prior research on sensemaking and trust, along with new interviews with developers, we synthesized a framework for reuse judgements. The interviews also validated that developers express a desire for help with judging whether to reuse an existing decision. From this framework, we developed a set of techniques for capturing the initial decision maker's behavior and visualizing signals calculated based on the behavior, to facilitate subsequent consumers' reuse decisions, instantiated in a prototype system called Strata. Results of a user study suggest that the system significantly improves the accuracy, depth, and speed of reusing decisions. These results have implications for systems involving user-generated content in which other users need to evaluate the relevance and trustworthiness of that content.
SP  - 1
EP  - 35
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
PB  - 
DO  - 10.1145/3449240
ER  - 

TY  - JOUR
AU  - Crain, Patrick; Lee, Jaewook; Yen, Yu-Chun (Grace); Kim, Joy; Aiello, Alyssa; Bailey, Brian
TI  - Visualizing Topics and Opinions Helps Students Interpret Large Collections of Peer Feedback for Creative Projects
PY  - 2022
AB  - <jats:p>We deployed a feedback visualization tool to learn how students used the tool for interpreting feedback from peers and teaching assistants. The tool visualizes the topic and opinion structure in a collection of feedback and provides interaction for reviewing providers’ backgrounds. Eighteen teams engaged with the tool to interpret feedback for course projects. We surveyed students (N=69) to learn about their sensemaking goals, use of the tool to accomplish those goals, and perceptions of specific features. We interviewed students (N=12) and TAs (N=2) to assess the tool’s impact on students’ review processes and course instruction. Students discovered valuable feedback, assessed project quality, and justified design decisions to teammates by exploring specific icon patterns in the visualization. The interviews revealed that students mimicked strategies implemented in the tool when reviewing new feedback without the tool. Students found the benefits of the visualization outweighed the cost of labeling feedback.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3571817
ER  - 

TY  - NA
AU  - Kim, Lawrence H.; Drew, Daniel S.; Domova, Veronika; Follmer, Sean
TI  - CHI - User-defined Swarm Robot Control
PY  - 2020
AB  - A swarm of robots can accomplish more than the sum of its parts, and swarm systems will soon see increased use in applications ranging from tangible interfaces to search and rescue teams. However, effective human control of robot swarms has been shown to be demonstrably more difficult than controlling a single robot, and swarm-specific interactions methodologies are relatively underexplored. As we envision even non-expert users will have more daily in-person encounters with different numbers of robots in the future, we present a user-defined set of control interactions for tabletop swarm robots derived from an elicitation study. We investigated the effects of number of robots and proximity on the user's interaction and found significant effects. For instance, participants varied between using 1-2 fingers, one hand, and both hands depending on the group size. We also provide general design guidelines such as preferred interaction modality, common strategies, and a high-agreement interaction set.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376814
ER  - 

TY  - JOUR
AU  - Ma, Xiao-Ming; Xing, Yan; Zheng, Jia-Chen; Li, Xiaowei; Wang, Qiong-Hua
TI  - A real-time interactive rendering method for 360° tabletop integral imaging 3D display
PY  - 2021
AB  - NA
SP  - 679
EP  - 688
JF  - Journal of the Society for Information Display
VL  - 29
IS  - 9
PB  - 
DO  - 10.1002/jsid.1017
ER  - 

TY  - NA
AU  - Pamparău, Cristian; Aiordachioae, Adrian; Vatavu, Radu-Daniel
TI  - MUM - From Do You See What I See? to Do You Control What I See? Mediated Vision, From a Distance, for Eyewear Users
PY  - 2020
AB  - We discuss engineering aspects for shifting from “do you see what I see?” applications that stream the user’s field of view to remote viewers toward “do you control what I see?” features in which remote viewers are given the opportunity and tool to control the primary user’s field of view. To this end, we present two applications for (1) smartglasses with embedded video camera for live video streaming and (2) the HoloLens HMD that presents users with mediated versions of the visual world controlled by remote viewers.
SP  - 326
EP  - 328
JF  - 19th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3428361.3432089
ER  - 

TY  - JOUR
AU  - Thomas, Bruce H.
TI  - Examining User Perception of the Size of Multiple Objects in Virtual Reality
PY  - 2020
AB  - This article presents a user study into user perception of an object’s size when presented in virtual reality. Critical for users understanding of virtual worlds is their perception of the size of virtual objects. This article is concerned with virtual objects that are within arm’s reach of the user. Examples of such virtual objects could be virtual controls such as buttons, dials and levers that the users manipulate to control the virtual reality application. This article explores the issue of a user’s ability to judge the size of an object relative to a second object of a different colour. The results determined that the points of subjective equality for height and width judgement tasks ranging from 10 to 90 mm were all within an acceptable value. That is to say, participants were able to perceive height and width judgements very close to the target values. The results for height judgement task for just-noticeable difference were all less than 1.5 mm and for the width judgement task less than 2.3 mm.
SP  - 4049
EP  - NA
JF  - Applied Sciences
VL  - 10
IS  - 11
PB  - 
DO  - 10.3390/app10114049
ER  - 

TY  - CHAP
AU  - Krosl, Katharina
TI  - Simulating Cataracts in Virtual Reality
PY  - 2021
AB  - Vision impairments, such as cataracts, affect the visual perception of numerous people worldwide, but are hardly ever considered in architectural or lighting design, due to a lack of suitable tools. In this chapter, we address this issue by presenting a method to simulate vision impairments, in particular cataracts, graphically in virtual reality (VR), for people with normal sight. Such simulations can help train medical personnel, allow relatives of people with vision impairments to better understand the challenges they face in their everyday lives and also help architects and lighting designers to test their designs for accessibility. There have been different approaches and devices used for such simulations in the past. The boom of VR and augmented reality (AR) devices, following the release of the Oculus Rift and HTC Vive headsets, has provided new opportunities to create more immersive and more realistic simulations than ever before. However, the development of a vision impairment simulation is dependent on multiple factors: the designated application area, the impacts of the used hardware on a user’s vision, and of course the impact of the respective vision impairment on different aspects of the human visual system. We will discuss these factors in this chapter and also introduce some basic knowledge on human vision and how to measure it. Then we will illustrate how to simulate vision impairments in VR on the example of cataracts and explain how the presented methodology can be used to calibrate simulated symptoms to the same level of severity for different users. This methodology allows for the first time to conduct quantitative user studies to investigate the impact of certain vision impairments on perception and gain insight that can inform the design process of architects and lighting designers in the future.
SP  - 257
EP  - 283
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-61905-3_14
ER  - 

TY  - CHAP
AU  - Murray, John T.; Marino, Mark C.
TI  - Transfordance: The Decentering Effect of Transformative Affordances in Virtual Reality in The Hollow Reach
PY  - 2021
AB  - NA
SP  - 393
EP  - 398
JF  - Interactive Storytelling
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-92300-6_38
ER  - 

TY  - NA
AU  - Weigel, Martin; Schön, Oliver; Janssen, Herbert
TI  - UbiComp/ISWC Adjunct - Evaluation of body-worn FPCBs with bluetooth low energy, capacitive touch, and resistive flex sensing
PY  - 2020
AB  - Commercially available flexible printed circuit boards (FPCBs) have the potential to embed electronics, connectivity, and interactivity into the same surface. This makes them an ideal platform for untethered and interactive wearable devices. However, we lack an understanding how well FPCB-based antennas and sensors perform when worn directly on the body. This work contributes an understanding by studying body-worn FPCBs in three technical evaluations: First, we study the integration of Bluetooth Low Energy and compare the signal strength of our body-worn FPCB with a rigid BLE developer board. Second, we study the accuracy of capacitive touch sensing with two electrode sizes. Finally, we develop a resistive flex sensor based on commercially available FPCB materials and compare its accuracy with a state-of-the-art flex sensor. Taken together, our results demonstrate a high usability of FPCB-based wearable devices.
SP  - 147
EP  - 150
JF  - Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410530.3414380
ER  - 

TY  - NA
AU  - Peng, Tyler; Pochettino, Mora; Mueller, Stefanie
TI  - CircuitAssist: Automatically Dispensing Electronic Components to Facilitate Circuit Building
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558671
ER  - 

TY  - NA
AU  - van Doleweerd, Elzelinde; Altarriba Bertran, Ferran; Bruns, Miguel
TI  - Incorporating Shape-Changing Food Materials Into Everyday Culinary Practices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501315
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Ofek, Eyal; Sinclair, Mike; Leithinger, Daniel; Gonzalez-Franco, Mar
TI  - UIST (Adjunct Volume) - Demonstrating HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots
PY  - 2021
AB  - HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability—these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in-time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various applications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.
SP  - 131
EP  - 133
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480202
ER  - 

TY  - NA
AU  - Feick, Martin; Kleer, Niko; Zenner, André; Tang, Anthony; Krüger, Antonio
TI  - CHI - Visuo-haptic Illusions for Linear Translation and Stretching using Physical Proxies in Virtual Reality
PY  - 2021
AB  - Providing haptic feedback when manipulating virtual objects is an essential part of immersive virtual reality experiences; however, it is challenging to replicate all of an object's properties and characteristics. We propose the use of visuo-haptic illusions alongside physical proxies to enhance the scope of proxy-based interactions with virtual objects. In this work, we focus on two manipulation techniques, linear translation and stretching across different distances, and investigate how much discrepancy between the physical proxy and the virtual object may be introduced without participants noticing. In a study with 24 participants, we found that manipulation technique and travel distance significantly affect the detection thresholds, and that visuo-haptic illusions impact performance and accuracy. We show that this technique can be used to enable functional proxy objects that act as stand-ins for multiple virtual objects, illustrating the technique through a showcase VR-DJ application.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445456
ER  - 

TY  - NA
AU  - Gong, Hebo; Cui, Zhitong; Wang, Yanan; Shen, Chengyi; Zhang, Deyin; Luo, Shijian
TI  - CHI Extended Abstracts - eGlove: Designing Interactive Fabric Sensor for Enhancing Contact-Based Interactions
PY  - 2021
AB  - We present eGlove, a wearable and low-cost fabric sensor for recognizing a rich context of objects by touching them, including daily necessities, fruits, plants, as well as different body parts. Our sensing approach utilizes Swept frequency Capacitive Sensing (SFCS) to provide consistent sensor readings even when the fabric electrode is under varying deformation and stretching degrees. Our work proposes an easy fabrication method and hardware configuration for prototyping the interactive fabric sensor. We evaluated our system’s classification accuracy through per-user training and found a real-time classification of 96.3%. We also demonstrated novel contextual interactions enabled by our technical approach with several applications.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451824
ER  - 

TY  - JOUR
AU  - Nedelchev, Simeon; Skvortsova, Valeria; Guryev, Boris; Gaponov, Igor; Ryu, Jee-Hwan
TI  - On Energy-Preserving Motion in Twisted String Actuators
PY  - 2021
AB  - Many applications require robotic end-effectors and mechanisms to move along periodic trajectories of given amplitude and frequency. If motion parameters are known in advance, it might be beneficial to design the mechanism in such a way that its natural frequency is close to that of the desired trajectory so that controller needs to provide minimal effort to generate sustained oscillations. In this letter, we investigate natural nonlinear oscillatory behavior in twisted string actuators (TSA), describing its mathematical model and providing experimental verification of this phenomenon based on observations of dynamics and energy. We also design an energy-preserving controller that is capable of generating undamped oscillations of desired magnitude even under severe constraints on actuator torque. Experimental study has demonstrated that it was possible to induce undamped oscillatory response of TSA with a 2-kg payload while applying a maximum of 6 mNm motor torque, which can be used in robotic applications that require periodic motions and high controller efficiency, like legged robots.
SP  - 7406
EP  - 7412
JF  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 4
PB  - 
DO  - 10.1109/lra.2021.3097655
ER  - 

TY  - NA
AU  - Kong, Junhan; Zhong, Mingyuan; Fogarty, James; Wobbrock, Jacob O.
TI  - Quantifying Touch: New Metrics for Characterizing What Happens During a Touch
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 24th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517428.3544804
ER  - 

TY  - NA
AU  - Henderson, Jay; Ceha, Jessy; Lank, Edward
TI  - MobileHCI - STAT: Subtle Typing Around the Thigh for Head-Mounted Displays
PY  - 2020
AB  - In head-mounted display (HMD) interaction, text entry is frequently supported via some form of virtual touch, controller, or ray casting keyboard. While these options effectively support text entry, they often incur costs of additional external hardware, awkward movements, and hand encumbrance. We propose STAT, a low-cost, mobile, touch typing technique that leverages a smartphone screen located at the thigh, to support both tap and word gesture text input for HMDs. Through a controlled laboratory study, we explore the efficacy of our technique – including a comparison of typing in and out of an enclosed pocket – and present design recommendations for the opportunistic use of a personal touchscreen device positioned at a user’s thigh for HMD text entry.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403549
ER  - 

TY  - JOUR
AU  - Villa, Steeven; Mayer, Sven; Hartcher-O'Brien, Jess; Schmidt, Albrecht; Machulla, Tonja-Katrin
TI  - Extended Mid-air Ultrasound Haptics for Virtual Reality
PY  - 2022
AB  - <jats:p>Mid-air haptics allow bare-hand tactile stimulation; however, it has a constrained workspace, making it unsuitable for room-scale haptics. We present a novel approach to rendering mid-air haptic sensations in a large rendering volume by turning a static array into a dynamic array following the user's hand. We used a 6DOF robot to drive a haptic ultrasound array over a large 3D space. Our system enables rendering room-scale mid-air experiences while preserving bare-hand interaction, thus, providing tangibility for virtual environments. To evaluate our approach, we performed three evaluations. First, we performed a technical system evaluation, showcasing the feasibility of such a system. Next, we conducted three psychophysical experiments, showing that the motion does not affect the user's perception with high likelihood. Lastly, we explored seven use cases that showcase our system's potential using a user study. We discuss challenges and opportunities in how large-scale mid-air haptics can contribute toward room-scale haptic feedback. Thus, with our system, we contribute to general haptic mid-air feedback on a large scale.</jats:p>
SP  - 500
EP  - 524
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567731
ER  - 

TY  - NA
AU  - Guo, Anhong; Kamar, Ece; Vaughan, Jennifer Wortman; Wallach, Hanna; Morris, Meredith Ringel
TI  - Toward Fairness in AI for People with Disabilities: A Research Roadmap
PY  - 2019
AB  - AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.
SP  - NA
EP  - NA
JF  - arXiv: Computers and Society
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wu, Tony; Fukuhara, Shiho; Gillian, Nicholas; Sundara-Rajan, Kishore; Poupyrev, Ivan
TI  - UIST - ZebraSense: A Double-sided Textile Touch Sensor for Smart Clothing
PY  - 2020
AB  - ZebraSense is a novel dual-sided woven touch sensor that can recognize and differentiate interactions on the top and bottom surfaces of the sensor. ZebraSense is based on an industrial multi-layer textile weaving technique, yet it enables a novel capacitive sensing paradigm, where each sensing element contributes to touch detection on both surfaces of the sensor simultaneously. Unlike the common "sensor sandwich" approach used in previous work, ZebraSense inherently minimizes the number of sensing elements, which drastically simplifies both sensor construction and its integration into soft goods, while preserving maximum sensor resolution. The experimental evaluation confirmed the validity of our approach and demonstrated that ZebraSense is a reliable, efficient, and accurate solution for detecting user gestures in various dual-sided interaction scenarios, allowing for new use cases in smart apparel, home decoration, toys, and other textile objects.
SP  - 662
EP  - 674
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415886
ER  - 

TY  - NA
AU  - Danilova, Anastasia; Naiakshina, Alena; Horstmann, Stefan; Smith, Matthew
TI  - ICSE - Do you really code?: Designing and Evaluating Screening Questions for Online Surveys with Programmers
PY  - 2021
AB  - Recruiting professional programmers in sufficient numbers for research studies can be challenging because they often cannot spare the time, or due to their geographical distribution and potentially the cost involved. Online platforms such as Clickworker or Qualtrics do provide options to recruit participants with programming skill; however, misunderstandings and fraud can be an issue. This can result in participants without programming skill taking part in studies and surveys. If these participants are not detected, they can cause detrimental noise in the survey data. In this paper, we develop screener questions that are easy and quick to answer for people with programming skill but difficult to answer correctly for those without. In order to evaluate our questionnaire for efficacy and efficiency, we recruited several batches of participants with and without programming skill and tested the questions. In our batch 42% of Clickworkers stating that they have programming skill did not meet our criteria and we would recommend filtering these from studies. We also evaluated the questions in an adversarial setting. We conclude with a set of recommended questions which researchers can use to recruit participants with programming skill from online platforms.
SP  - 537
EP  - 548
JF  - 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icse43902.2021.00057
ER  - 

TY  - NA
AU  - Fang, Cathy; Harrison, Chris
TI  - UIST - Retargeted Self-Haptics for Increased Immersion in VR without Instrumentation
PY  - 2021
AB  - Today’s consumer virtual reality (VR) systems offer immersive graphics and audio, but haptic feedback is rudimentary – delivered through controllers with vibration feedback or is non-existent (i.e., the hands operating freely in the air). In this paper, we explore an alternative, highly mobile and controller-free approach to haptics, where VR applications utilize the user’s own body to provide physical feedback. To achieve this, we warp (retarget) the locations of a user’s hands such that one hand serves as a physical surface or prop for the other hand. For example, a hand holding a virtual nail can serve as a physical backstop for a hand that is virtually hammering, providing a sense of impact in an air-borne and uninstrumented experience. To illustrate this rich design space, we implemented twelve interactive demos across three haptic categories. We conclude with a user study from which we draw design recommendations.
SP  - 1109
EP  - 1121
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474810
ER  - 

TY  - JOUR
AU  - Oh, Uran; Joh, Hwayeon; Lee, Yunjung
TI  - Image Accessibility for Screen Reader Users: A Systematic Review and a Road Map
PY  - 2021
AB  - A number of studies have been conducted to improve the accessibility of images using touchscreen devices for screen reader users. In this study, we conducted a systematic review of 33 papers to get a holistic understanding of existing approaches and to suggest a research road map given identified gaps. As a result, we identified types of images, visual information, input device and feedback modalities that were studied for improving image accessibility using touchscreen devices. Findings also revealed that there is little study how the generation of image-related information can be automated. Moreover, we confirmed that the involvement of screen reader users is mostly limited to evaluations, while input from target users during the design process is particularly important for the development of assistive technologies. Then we introduce two of our recent studies on the accessibility of artwork and comics, AccessArt and AccessComics, respectively. Based on the identified key challenges, we suggest a research agenda for improving image accessibility for screen reader users.
SP  - 953
EP  - NA
JF  - Electronics
VL  - 10
IS  - 8
PB  - 
DO  - 10.3390/electronics10080953
ER  - 

TY  - NA
AU  - Veldhuis, Annemiek; Liang, Rong-Hao; Bekker, Tilde
TI  - Tangible and Embedded Interaction - CoDa: Collaborative Data Interpretation Through an Interactive Tangible Scatterplot
PY  - 2020
AB  - Tangibles can model abstract structures. One educational subject where this can be utilized is instruction on data visualization inter- pretation. Data physicalizations, tangible representations of data, offer graspable handles for the users to manipulate data visualiza- tions directly so that they can better understand what information they hold. However, investigations on the applicability of interac- tive data physicalizations in educational settings are still sparse. In this paper, we explore how students reason with an interactive tangible scatterplot through a collaborative data interpretation tool, CoDa. We report the design, development, and the user experiences in an exploratory study where 11 students, in groups of 2 to 4, completed a data analysis task with CoDa. The qualitative results show insights in the process of data interpretation, how interaction with the tangibles influenced these data interpretations, how the system aided collaboration and, overall user experience. We believe the results and implications offer a step towards nurturing future educational applications on interactive data physicalizations.
SP  - 323
EP  - 336
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374934
ER  - 

TY  - BOOK
AU  - Stellmacher, Carolin
TI  - VR Workshops - Haptic-Enabled Buttons Through Adaptive Trigger Resistance
PY  - 2021
AB  - While commercial controllers for virtual reality (VR) offer a variety of components to register user input, their ability to generate meaningful haptic feedback during the interaction lags behind. This prevents users from experiencing the virtual world through their haptic sense. For example, grabbing a light virtual object feels identical to grabbing a heavy virtual object. In this workshop paper, we enrich an established input component available in any commercial VR controller with appropriate haptic rendering capabilities. As a proof of concept, we present our haptic VR controller Triggermuscle and its adaptive trigger to simulate weight in VR. The trigger dynamically adapts its resistance according to the weight of a grabbed virtual object: The heavier the virtual object, the higher the trigger resistance and the more force users need to apply. Our system is built into the casing of an HTC Vive controller and connects the original trigger component to an extension spring for variable resistance. We envision for the future, that VR input devices can evolve more into input-output technologies and provide meaningful and versatile haptic feedback.
SP  - 201
EP  - 204
JF  - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw52623.2021.00044
ER  - 

TY  - NA
AU  - Hemmert, Fabian; Görts, Alexander; Horst, Jana; Park, So Jeong; Sion, Tom
TI  - Life-Death Interfaces: Tangible Ways of Legacy-Making, Grief, and Remembrance
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3547533
ER  - 

TY  - NA
AU  - Aigner, Roland; Pointner, Andreas; Preindl, Thomas; Danner, Rainer; Haller, Michael J.
TI  - CHI - TexYZ: Embroidering Enameled Wires for Three Degree-of-Freedom Mutual Capacitive Sensing
PY  - 2021
AB  - In this paper, we present TexYZ, a method for rapid and effortless manufacturing of textile mutual capacitive sensors using a commodity embroidery machine. We use enameled wire as a bobbin thread to yield textile capacitors with high quality and consistency. As a consequence, we are able to leverage the precision and expressiveness of projected mutual capacitance for textile electronics, even when size is limited. Harnessing the assets of machine embroidery, we implement and analyze five distinct electrode patterns, examine the resulting electrical features with respect to geometrical attributes, and demonstrate the feasibility of two promising candidates for small-scale matrix layouts. The resulting sensor patches are further evaluated in terms of capacitance homogeneity, signal-to-noise ratio, sensing range, and washability. Finally, we demonstrate two use case scenarios, primarily focusing on continuous input with up to three degrees-of-freedom.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445479
ER  - 

TY  - JOUR
AU  - Li, Tianshi; Neundorfer, Elijah B.; Agarwal, Yuvraj; Hong, Jason
TI  - Honeysuckle: Annotation-Guided Code Generation of In-App Privacy Notices
PY  - 2021
AB  - In-app privacy notices can help smartphone users make informed privacy decisions. However, they are rarely used in real-world apps, since developers often lack the knowledge, time, and resources to design and implement them well. We present Honeysuckle, a programming tool that helps Android developers build in-app privacy notices using an annotation-based code generation approach facilitated by an IDE plugin, a build system plugin, and a library. We conducted a within-subjects study with 12 Android developers to evaluate Honeysuckle. Each participant was asked to implement privacy notices for two popular open-source apps using the Honeysuckle library as a baseline as well as the annotation-based approach. Our results show that the annotation-based approach helps developers accomplish the task faster with significantly lower cognitive load. Developers preferred the annotation-based approach over the library approach because it was much easier to learn and use and allowed developers to achieve various types of privacy notices using a unified code format, which can enhance code readability and benefit team collaboration.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478097
ER  - 

TY  - JOUR
AU  - Khan, Sulaiman; Nazir, Shah; Khan, Habib Ullah
TI  - Analysis of Navigation Assistants for Blind and Visually Impaired People: A Systematic Review
PY  - 2021
AB  - Over the last few decades, the development in the field of navigation and routing devices has become a hindering task for the researchers to develop smart and intelligent guiding mechanism at indoor and outdoor locations for blind and visually impaired people (BVIPs). The existing research need to be analysed from a historical perception including early research on the first electronic travel aids to the use of modern artificial vision models for the navigation of BVIPs. Diverse approaches such as: e-cane or guide dog, infrared-based cane, laser based walker and many others are proposed for the navigation of BVIPs. But most of these techniques have limitations such as: infrared and ultrasonic based assistance has short range capacities for object detection. While laser based assistance can harm other people if it directly hit them on their eyes or any other part of the body. These trade-offs are critical to bring this technology in practice.To systematically assess, analyze, and identify the primary studies in this specialized field and provide an overview of the trends and empirical evidence in the proposed field. This systematic research work is performed by defining a set of relevant keywords, formulating four research questions, defining selection criteria for the articles, and synthesizing the empirical evidence in this area. Our pool of studies include 191 most relevant articles to the proposed field reported between 2011 and 2020 (a portion of 2020 is included). This systematic mapping will help the researchers, engineers, and practitioners to make more authentic decisions for finding gaps in the available navigation assistants and suggest a new and enhanced smart assistant application accordingly to ensure safety and accurate guidance of the BVIPs. This research work have several implications in particular the impact of reducing fatalities and major injuries of BVIPs.
SP  - 26712
EP  - 26734
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3052415
ER  - 

TY  - NA
AU  - Arsan, Deniz; Zaidi, Ali; Sagar, Aravind; Kumar, Ranjitha
TI  - UIST - App-Based Task Shortcuts for Virtual Assistants
PY  - 2021
AB  - Virtual assistants like Google Assistant and Siri often interface with external apps when they cannot directly perform a task. Currently, developers must manually expose the capabilities of their apps to virtual assistants, using App Actions on Android or Shortcuts on iOS. This paper presents savant, a system that automatically generates task shortcuts for virtual assistants by mapping user tasks to relevant UI screens in apps. For a given natural language task (e.g., “send money to Joe”), savant leverages text and semantic information contained within UIs to identify relevant screens, and intent modeling to parse and map entities (e.g., “Joe”) to required UI inputs. Therefore, savant allows virtual assistants to interface with apps and handle new tasks without requiring any developer effort. To evaluate savant, we performed a user study to identify common tasks users perform with virtual assistants. We then demonstrate that savant can find relevant app screens for those tasks and autocomplete the UI inputs.
SP  - 1089
EP  - 1099
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474808
ER  - 

TY  - JOUR
AU  - Vatavu, Radu-Daniel; Rusu, Petruţa-Paraschiva; Schipor, Ovidiu Andrei; Schipor, Maria Doina
TI  - Preferences of people with visual impairments for augmented and mediated vision: A vignette experiment
PY  - 2021
AB  - We examine in this work the desirability and preferences of people with visual impairments for assistive vision, i.e., vision rehabilitation and enhancement, delivered by smart eyewear devices. We present results from a vignette experiment with N = 17 participants with visual impairments, who reported their preferences regarding 32 hypothetical scenarios that we formulated for assistive vision, e.g., long-distance vision, peripheral vision, highly sensitive perception of colors, thermal vision, night vision, and others. Our results show higher desirability (average score of 4.21 out of 5) for assistive vision scenarios addressing rehabilitation of lost vision functions compared to scenarios that propose Augmented Reality-based enhancements of human vision (3.76) or visual perception in other regions of the electromagnetic spectrum, such as thermal or infrared vision (3.36). To understand these results, we conduct a second vignette study involving N = 178 participants without visual impairments, for which we report lower desirability for vision augmentation (3.44/5) compared to participants with visual impairments (3.75/5). We discuss implications of our results for augmented and mediated vision delivered by smart eyewear devices.
SP  - 1
EP  - 26
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11042-021-11498-4
ER  - 

TY  - JOUR
AU  - Liu, Zhenbin; He, Chang; Guo, Chaofan; Chen, Fengying; Bhandari, Bhesh; Zhang, Min
TI  - Dehydration-triggered shape transformation of 4D printed edible gel structure affected by material property and heating mechanism
PY  - 2021
AB  - NA
SP  - 106608
EP  - NA
JF  - Food Hydrocolloids
VL  - 115
IS  - NA
PB  - 
DO  - 10.1016/j.foodhyd.2021.106608
ER  - 

TY  - JOUR
AU  - Sarmadi, Hamid; Muñoz-Salinas, Rafael; Olivares-Mendez, Miguel A.; Medina-Carnicer, Rafael
TI  - Detection of Binary Square Fiducial Markers Using an Event Camera
PY  - 2021
AB  - Event cameras are a new type of image sensors that output changes in light intensity (events) instead of absolute intensity values. They have a very high temporal resolution and a high dynamic range. In this paper, we propose a method to detect and decode binary square markers using an event camera. We detect the edges of the markers by detecting line segments in an image created from events in the current packet. The line segments are combined to form marker candidates. The bit value of marker cells is decoded using the events on their borders. To the best of our knowledge, no other approach exists for detecting square binary markers directly from an event camera. Experimental results show that the performance of our proposal is much superior to the one from the RGB ArUco marker detector. Additionally, the proposed method can run on a single CPU thread in real-time.
SP  - 27813
EP  - 27826
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3058423
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Xu, Zheer; Yang, Xing-Dong; Hodges, Steve; Seyed, Teddy
TI  - CHI - Project Tasca: Enabling Touch and Contextual Interactions with a Pocket-based Textile Sensor
PY  - 2021
AB  - We present Project Tasca, a pocket-based textile sensor that detects user input and recognizes everyday objects that a user carries in the pockets of a pair of pants (e.g., keys, coins, electronic devices, or plastic items). By creating a new fabric-based sensor capable of detecting in-pocket touch and pressure, and recognizing metallic, non-metallic, and tagged objects inside the pocket, we enable a rich variety of subtle, eyes-free, and always-available input, as well as context-driven interactions in wearable scenarios. We developed our prototype by integrating four distinct types of sensing methods, namely: inductive sensing, capacitive sensing, resistive sensing, and NFC in a multi-layer fabric structure into the form factor of a jeans pocket. Through a ten-participant study, we evaluated the performance of our prototype across 11 common objects including hands, 8 force gestures, and 30 NFC tag placements. We yielded a 92.3% personal cross-validation accuracy for object recognition, 96.4% accuracy for gesture recognition, and a 100% accuracy for detecting NFC tags at close distance . We conclude by demonstrating the interactions enabled by our pocket-based sensor in several applications.
SP  - 1
EP  - 13
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445712
ER  - 

TY  - NA
AU  - Gonzalez, Eric J.; Ofek, Eyal; Gonzalez-Franco, Mar; Sinclair, Mike
TI  - UIST - X-Rings: A Hand-mounted 360° Shape Display for Grasping in Virtual Reality
PY  - 2021
AB  - X-Rings is a novel hand-mounted 360° shape display for Virtual Reality that renders objects in 3D and responds to user-applied touch and grasping force. Designed as a modular stack of motor-driven expandable rings (5.7-7.7 cm diameter), X-Rings renders radially-symmetric surfaces graspable by the user’s whole hand. The device is strapped to the palm, allowing the fingers to freely make and break contact with the device. Capacitance sensors and motor current sensing provide estimates of finger touch states and gripping force. We present the results of a user study evaluating participants’ ability to associate device-rendered shapes with visually-rendered objects as well as a demo application that allows users to freely interact with a variety of objects in a virtual environment.
SP  - 732
EP  - 742
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474782
ER  - 

TY  - NA
AU  - Marques, Arthur; Bradley, Nick C.; Murphy, Gail C.
TI  - ICSME - Characterizing Task-Relevant Information in Natural Language Software Artifacts
PY  - 2020
AB  - To complete a software development task, a software developer often consults artifacts which mostly consist of natural language text, such as API documentation, bug reports, and Q&A forums. Not all information within these artifacts is relevant to a developer’s current task, forcing them to filter through large amounts of irrelevant information, a frustrating and time-consuming activity. Since failing to locate relevant information may lead developers to incorrect or incomplete solutions, many approaches attempt to automatically extract relevant information from natural language artifacts. However, existing approaches are able to identify relevant text only for certain types of tasks and artifacts. To explore how these limitations could be relaxed, we conducted a controlled experiment in which we asked 20 software developers to examine 20 natural language artifacts consisting of 1,874 sentences and highlight the text they considered relevant to six software development tasks. Although the 2,463 distinct highlights participants created indicate variability in the perceived relevance of the text, the information considered key to completing the tasks was consistent. We observe consistency in the text using frame semantics, an approach that captures the key meaning of sentences, suggesting that frame semantics can be used in the future to automatically identify task-relevant information in natural language artifacts.
SP  - 476
EP  - 487
JF  - 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icsme46990.2020.00052
ER  - 

TY  - JOUR
AU  - Thangavel, Gomathi; Memedi, Mevludin; Hedström, Karin
TI  - Customized Information and Communication Technology for Reducing Social Isolation and Loneliness Among Older Adults: Scoping Review.
PY  - 2022
AB  - <AbstractText Label="BACKGROUND" NlmCategory="BACKGROUND">Advancements in science and various technologies have resulted in people having access to better health care, a good quality of life, and better economic situations, enabling humans to live longer than ever before. Research shows that the problems of loneliness and social isolation are common among older adults, affecting psychological and physical health. Information and communication technology (ICT) plays an important role in alleviating social isolation and loneliness.</AbstractText> <AbstractText Label="OBJECTIVE" NlmCategory="OBJECTIVE">The aim of this review is to explore ICT solutions for reducing social isolation or loneliness among older adults, the purpose of ICT solutions, and the evaluation focus of these solutions. This study particularly focuses on customized ICT solutions that either are designed from scratch or are modifications of existing off-the-shelf products that cater to the needs of older adults.</AbstractText> <AbstractText Label="METHODS" NlmCategory="METHODS">A scoping literature review was conducted. A search across 7 databases, including ScienceDirect, Association for Computing Machinery, PubMed, IEEE Xplore, PsycINFO, Scopus, and Web of Science, was performed, targeting ICT solutions for reducing and managing social isolation and loneliness among older adults. Articles published in English from 2010 to 2020 were extracted and analyzed.</AbstractText> <AbstractText Label="RESULTS" NlmCategory="RESULTS">From the review of 39 articles, we identified 5 different purposes of customized ICT solutions focusing on reducing social isolation and loneliness. These were social communication, social participation, a sense of belonging, companionship, and feelings of being seen. The mapping of purposes of ICT solutions with problems found among older adults indicates that increasing social communication and social participation can help reduce social isolation problems, whereas fulfilling emotional relationships and feeling valued can reduce feelings of loneliness. In terms of customized ICT solution types, we found the following seven different categories: social network, messaging services, video chat, virtual spaces or classrooms with messaging capabilities, robotics, games, and content creation and management. Most of the included studies (30/39, 77%) evaluated the usability and acceptance aspects, and few studies (11/39, 28%) focused on loneliness or social isolation outcomes.</AbstractText> <AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">This review highlights the importance of discussing and managing social isolation and loneliness as different but related concepts and emphasizes the need for future research to use suitable outcome measures for evaluating ICT solutions based on the problem. Even though a wide range of customized ICT solutions have been developed, future studies need to explore the recent emerging technologies, such as the Internet of Things and augmented or virtual reality, to tackle social isolation and loneliness among older adults. Furthermore, future studies should consider evaluating social isolation or loneliness while developing customized ICT solutions to provide more robust data on the effectiveness of the solutions.</AbstractText> <CopyrightInformation>©Gomathi Thangavel, Mevludin Memedi, Karin Hedström. Originally published in JMIR Mental Health (https://mental.jmir.org), 07.03.2022.</CopyrightInformation>
SP  - e34221
EP  - e34221
JF  - JMIR mental health
VL  - 9
IS  - 3
PB  - 
DO  - 10.2196/34221
ER  - 

TY  - NA
AU  - Wang, Liwen; Sandor, Christian
TI  - ISMAR Adjunct - An Empirical Study of Size Discrimination in Augmented Reality
PY  - 2021
AB  - Existing psychophysical experiments show that size perception can influence the human identification of object properties (e.g., shape or weight) in augmented reality (AR). Some recent studies have revealed the detection threshold of object size in real physical objects. However, the users’ absolute detection threshold of object size augmentation is not clear, which limits the further evaluation of AR design. In this paper, we present two two-alternative forced-choice-based experiments on size perception of virtual objects in AR to explore the detection threshold of size difference in object augmentation. Our experimental results demonstrate that the user’s point of subjective equality (PSE) is 4.00%, and the size difference could be easily detected when the virtual object is larger than 5.18%.
SP  - 384
EP  - 386
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct54149.2021.00087
ER  - 

TY  - JOUR
AU  - Kim, Huhn; Baek, Mi-Seon
TI  - Moment Controller: VR Controller raises Awareness of the Difference in Weight between Virtual Objects
PY  - 2021
AB  - NA
SP  - 133
EP  - 151
JF  - Archives of Design Research
VL  - 34
IS  - 2
PB  - 
DO  - 10.15187/adr.2021.05.34.2.133
ER  - 

TY  - CHAP
AU  - Bansal, Ridhi; Hauser, Helmut; Rossiter, Jonathan
TI  - A Scalable Soft Robotic Cellbot
PY  - 2022
AB  - NA
SP  - 199
EP  - 211
JF  - Biomimetic and Biohybrid Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-20470-8_21
ER  - 

TY  - NA
AU  - Krosnick, Rebecca; Oney, Steve
TI  - ParamMacros: Creating UI Automation Leveraging End-User Natural Language Parameterization
PY  - 2022
AB  - Prior work in programming-by-demonstration (PBD) has explored ways to enable end-users to create custom automation without needing to write code. We propose a new end-user specification model &#x2013; asking the end-user to explicitly identify parts of their natural language query that can be generalized. We built a PBD system, ParamMacros, where users first generalize a concrete natural language question &#x2013; identifying parameters and their possible values &#x2013; and then create a demonstration of how to answer the question on the website of interest. ParamMacros then infers a generalized program by using the user-provided parameter values to identify relevant patterns in the website&#x2019;s structure. In a lab study we found that participants were able to meaningfully parameterize natural language queries and felt such a parameterization and demonstration process would be useful for creating custom automation.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833005
ER  - 

TY  - NA
AU  - Dutta, Senjuti; Linder, Rhema; Lowe, Doug; Rosenbalm, Richard; Kuzminykh, Anastasia; Williams, Alex C
TI  - Mobilizing Crowdwork:A Systematic Assessment of the Mobile Usability of HITs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501876
ER  - 

TY  - NA
AU  - Kono, Michinari; Kabashima, Osamu; Sasaki, Norio; Yamaoka, Junichi
TI  - CHI PLAY - Pre-Eating Play: Fabrication Experiences for Playful Human-Food Interaction
PY  - 2020
AB  - Recent progress of human--food interaction (HFI) has led to a growing interest in the merger of digital technologies for playful experiences in eating. Although these interactive technologies have been primarily integrated with eating experiences, we find that progress has also been made in digital food fabrication process in the human--computer interaction (HCI) field. Food fabrication techniques are used to expand the design space of food preparation, and it is accomplished before the eating stage. Many products have introduced playful experiences to this preparation stage. However, we have not yet explored food fabrication techniques in the context of play in depth. This paper discusses the food preparation stage and how it can be designed to be playful by introducing several product cases. Using this knowledge, we further discuss how such methods can be applied to previous digital food fabrication techniques with our own example of a playful food fabrication approach. We aim to enable various stages of human--food interaction to be more playful by understanding food as a toy-like object that can be independent from or related to the eating stage.
SP  - 160
EP  - 168
JF  - Proceedings of the Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3410404.3414228
ER  - 

TY  - NA
AU  - Friedel, Marcus
TI  - HapticLever: Kinematic Force Feedback using a 3D Pantograph
PY  - 2022
AB  - HapticLever is a new kinematic approach for VR haptics which uses a 3D pantograph to stiffly render large-scale surfaces using small-scale proxies. The HapticLever approach does not consume power to render forces, but rather puts a mechanical constraint on the end effector using a small-scale proxy surface. The HapticLever approach provides stiff force feedback when the user interacts with a static virtual surface, but allows the user to move their arm freely when moving through free virtual space. We present the problem space, the related work, and the HapticLever design approach.
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558736
ER  - 

TY  - JOUR
AU  - Genay, Adélaïde; Lécuyer, Anatole; Hachet, Martin
TI  - Being an Avatar "for Real": a Survey on Virtual Embodiment in Augmented Reality.
PY  - 2022
AB  - Virtual self-avatars have been increasingly used in Augmented Reality (AR) where one can see virtual content embedded into physical space. However, little is known about the perception of self-avatars in such a context. The possibility that their embodiment could be achieved in a similar way as in Virtual Reality opens the door to numerous applications in education, communication, entertainment, or the medical field. This article aims to review the literature covering the embodiment of virtual self-avatars in AR. Our goal is (i) to guide readers through the different options and challenges linked to the implementation of AR embodiment systems, (ii) to provide a better understanding of AR embodiment perception by classifying the existing knowledge, and (iii) to offer insight on future research topics and trends for AR and avatar research. To do so, we introduce a taxonomy of virtual embodiment experiences by defining a "body avatarization" continuum. The presented knowledge suggests that the sense of embodiment evolves in the same way in AR as in other settings, but this possibility has yet to be fully investigated. We suggest that, whilst it is yet to be well understood, the embodiment of avatars has a promising future in AR and conclude by discussing possible directions for research.
SP  - 1
EP  - 20
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2021.3099290
ER  - 

TY  - JOUR
AU  - Chen, Taizhou; Xu, Lantian; Xu, Xianshan; Zhu, Kening
TI  - GestOnHMD: Enabling Gesture-based Interaction on Low-cost VR Head-Mounted Display
PY  - 2021
AB  - Low-cost virtual-reality (VR) head-mounted displays (HMDs) with the integration of smartphones have brought the immersive VR to the masses, and increased the ubiquity of VR. However, these systems are often limited by their poor interactivity. In this paper, we present GestOnHMD, a gesture-based interaction technique and a gesture-classification pipeline that leverages the stereo microphones in a commodity smartphone to detect the tapping and the scratching gestures on the front, the left, and the right surfaces on a mobile VR headset. Taking the Google Cardboard as our focused headset, we first conducted a gesture-elicitation study to generate 150 user-defined gestures with 50 on each surface. We then selected 15, 9, and 9 gestures for the front, the left, and the right surfaces respectively based on user preferences and signal detectability. We constructed a data set containing the acoustic signals of 18 users performing these on-surface gestures, and trained the deep-learning classification pipeline for gesture detection and recognition. Lastly, with the real-time demonstration of GestOnHMD, we conducted a series of online participatory-design sessions to collect a set of user-defined gesture-referent mappings that could potentially benefit from GestOnHMD.
SP  - 2597
EP  - 2607
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067689
ER  - 

TY  - NA
AU  - Zhao, Wei; Go, Kentaro; Kinoshita, Yuichiro
TI  - Performance Analysis of Smartwatch Text Input Using Different Hand Postures and Models
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 International Conference on Cyberworlds (CW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cw55638.2022.00033
ER  - 

TY  - NA
AU  - Matthies, Denys J.C.; Urban, Bodo; Wolf, Katrin; Schmidt, Albrecht
TI  - OZCHI - Reflexive Interaction: Extending the concept of Peripheral Interaction
PY  - 2019
AB  - Human-computer interaction (HCI) continues to evolve and interaction scenarios have to fulfill mobility, flexibility, and ad-hoc interaction where ever users are. To address this, traditional interaction concepts are being extended. While Peripheral Interaction was previously introduced, it still remains as a rather broad concept, intersecting with others, and thus creating space for further definitions. Therefore, this paper introduces the concept of Reflexive Interaction, which can be viewed as a specific manifestation of Peripheral Interaction. In contrast, Reflexive Interaction is envisioned to be executed at a secondary task without involving substantial cognitive effort. It allows the user to perform very short interactions, shorter than Microinteractions, without straining the user's main interaction channels occupied with the primary task. To clearly classify Reflexive Interaction in respect to previous interaction concepts, we use a taxonomy relying on an attention-based HCI model.
SP  - 266
EP  - 278
JF  - Proceedings of the 31st Australian Conference on Human-Computer-Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3369457.3369478
ER  - 

TY  - JOUR
AU  - Choi, Inrak; Gonzalez, Eric J.; Follmer, Sean
TI  - Hybrid Actuation With Unidirectional Clutches for Handheld Haptic Controllers
PY  - 2021
AB  - Realizing high performance force feedback through a handheld haptic interface is challenging due to the limitations on actuator selection imposed by form factor requirements including size, weight, and cost. Here, we introduce a hybrid actuation mechanism composed of a geared motor and two active unidirectional clutches to achieve an increased impedance range and improve performance. One clutch is used to selectively couple a free end-effector to the geared motor only during haptic rendering, eliminating the high inertia of the geared motor in free space. A second clutch is used to ground the end-effector directly to the device body, allowing the device to render large force without stalling the motor. This hybrid mechanism renders free space and solid objects effectively in a passive manner and extends the impedance rendering capability of a haptic interface driven by a small motor while keeping the device lightweight, energy-efficient, safe, and low-cost.
SP  - 4827
EP  - 4834
JF  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 3
PB  - 
DO  - 10.1109/lra.2021.3068700
ER  - 

TY  - NA
AU  - Luo, Yiyue; Wu, Kui; Spielberg, Andrew; Foshey, Michael; Rus, Daniela; Palacios, Tomás; Matusik, Wojciech
TI  - Digital Fabrication of Pneumatic Actuators with Integrated Sensing by Machine Knitting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517577
ER  - 

TY  - NA
AU  - Lu, Xueshi; Yu, Difeng; Liang, Hai-Ning; Goncalves, Jorge
TI  - UIST - iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems
PY  - 2021
AB  - Text entry is an important and frequent task in interactive devices including augmented reality head-mounted displays (AR HMDs). In current AR HMDs, there are still two main open challenges to overcome for efficient and usable text entry: arm fatigue due to mid-air input and visual occlusion because of their small see-through displays. To address these challenges, we present iText, a technique for AR HMDs that is hands-free and is based on an imaginary (invisible) keyboard. We first show that it is feasible and practical to use an imaginary keyboard on AR HMDs. Then, we evaluated its performance and usability with three hands-free selection mechanisms: eye blinks (E-Type), dwell (D-Type), and swipe gestures (G-Type). Our results show that users could achieve an average text entry speed of 11.95, 9.03 and 9.84 words per minutes (WPM) with E-Type, D-Type, and G-Type, respectively. Given that iText with E-Type outperformed the other two selection mechanisms in text entry rate and subjective feedback, we ran a third, 5-day study. Our results show that iText with E-Type can achieve an average text entry rate of 13.76 WPM with a mean word error rate of 1.5%. In short, iText can enable efficient eyes-free text entry and can be useful for various application scenarios in AR HMDs.
SP  - 815
EP  - 825
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474788
ER  - 

TY  - CHAP
AU  - Tanaka, Toshimitsu; Shibata, Yuri; Sagawa, Yuji
TI  - HCI (4) - One-Handed Character Input Method for Smart Glasses
PY  - 2020
AB  - A character input method optimized for smart glasses is presented. The user grabs the input device with one hand and taps or strokes with the thumb. The entered text is displayed on the smart glass. A dynamic input guide is also provided to help users enter characters. The feature of this method is that the user can enter characters without looking at the fingertip. This allows the user to enter text while paying attention to the surroundings. Characters are selected by the movement of the thumb, so when a user enters text on a crowded street, people around him/her will not notice. Of course, the entered text will not be peeped.
SP  - 393
EP  - 406
JF  - Human Interface and the Management of Information. Designing Information
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50020-7_28
ER  - 

TY  - NA
AU  - Louis, Thibault; Troccaz, Jocelyne; Rochet-Capellan, Amélie; Bérard, François
TI  - UIST - GyroSuite: General-Purpose Interactions for Handheld Perspective Corrected Displays
PY  - 2020
AB  - Handheld Perspective-Corrected Displays (HPCDs) are physical objects that have a notable volume and that display a virtual 3D scene on their entire surface. Being handheld, they create the illusion of holding the scene in a physical container (the display). This has strong benefits for the intuitiveness of 3D interaction: manipulating objects of the virtual scene amounts to physical manipulations of the display. HPCDs have been limited so far to technical demonstrators and experimental tools to assess their merits. However, they show great potential as interactive systems for actual 3D applications. This requires that novel interactions be created to go beyond object manipulation and to offer general-purpose services such as menu command selection and continuous parameter control. Working with a two-handed spherical HPCD, we report on the design and informal evaluations of various interaction techniques for distant object selection, scene scaling, menu interaction and continuous parameter control. In particular, our design leverages the efficient two-handed control of the rotations of the display. We demonstrate how some of these techniques can be assemble in a self-contained anatomy learning application. Novice participants used the application in a qualitative user experiment. Most participants used the application effortlessly without any training or explanations.
SP  - 1248
EP  - 1260
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415893
ER  - 

TY  - JOUR
AU  - Hedayati, Hooman; Suzuki, Ryo; Rees, Wyatt; Leithinger, Daniel; Szafir, Daniel
TI  - Designing Expandable-Structure Robots for Human-Robot Interaction.
PY  - 2022
AB  - In this paper, we survey the emerging design space of expandable structures in robotics, with a focus on how such structures may improve human-robot interactions. We detail various implementation considerations for researchers seeking to integrate such structures in their own work and describe how expandable structures may lead to novel forms of interaction for a variety of different robots and applications, including structures that enable robots to alter their form to augment or gain entirely new capabilities, such as enhancing manipulation or navigation, structures that improve robot safety, structures that enable new forms of communication, and structures for robot swarms that enable the swarm to change shape both individually and collectively. To illustrate how these considerations may be operationalized, we also present three case studies from our own research in expandable structure robots, sharing our design process and our findings regarding how such structures enable robots to produce novel behaviors that may capture human attention, convey information, mimic emotion, and provide new types of dynamic affordances.
SP  - 719639
EP  - NA
JF  - Frontiers in robotics and AI
VL  - 9
IS  - NA
PB  - 
DO  - 10.3389/frobt.2022.719639
ER  - 

TY  - CHAP
AU  - Hopkins, Torin; Bae, S. Sandra; Uhr, Julia; Zheng, Clement; Banić, Amy; Do, Ellen Yi-Luen
TI  - User Interfaces in Smart Cities
PY  - 2021
AB  - NA
SP  - 687
EP  - 719
JF  - Handbook of Smart Cities
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-69698-6_94
ER  - 

TY  - NA
AU  - Dube, Tafadzwa Joseph; Johnson, Kevin; Arif, Ahmed Sabbir
TI  - Shapeshifter: Gesture Typing in Virtual Reality with a Force-based Digital Thimble
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519679
ER  - 

TY  - JOUR
AU  - Sutton, Jonathan; Langlotz, Tobias; Plopski, Alexander
TI  - Seeing Colours: Addressing Colour Vision Deficiency with Vision Augmentations using Computational Glasses
PY  - 2022
AB  - <jats:p>Colour vision deficiency is a common visual impairment that cannot be compensated for using optical lenses in traditional glasses, and currently remains untreatable. In our work, we report on research on Computational Glasses for compensating colour vision deficiency. While existing research only showed corrected images within the periphery or as an indirect aid, Computational Glasses build on modified standard optical see-through head-mounted displays and directly modulate the user’s vision, consequently adapting their perception of colours. In this work, we present an exhaustive literature review of colour vision deficiency compensation and subsequent findings; several prototypes with varying advantages—from well-controlled bench prototypes to less controlled but higher application portable prototypes; and a series of studies evaluating our approach starting with proving its efficacy, comparing to the state-of-the-art, and extending beyond static lab prototypes looking at real world applicability. Finally, we evaluated directions for future compensation methods for computational glasses.</jats:p>
SP  - 1
EP  - 53
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3486899
ER  - 

TY  - JOUR
AU  - Li, Yifan; Kim, Kangsoo; Erickson, Austin; Norouzi, Nahal; Jules, Jonathan; Bruder, Gerd; Welch, Gregory F.
TI  - A Scoping Review of Assistance and Therapy with Head-Mounted Displays for People Who Are Visually Impaired
PY  - 2022
AB  - <jats:p>Given the inherent visual affordances of Head-Mounted Displays (HMDs) used for Virtual and Augmented Reality (VR/AR), they have been actively used over many years as assistive and therapeutic devices for the people who are visually impaired. In this article, we report on a scoping review of literature describing the use of HMDs in these areas. Our high-level objectives included detailed reviews and quantitative analyses of the literature, and the development of insights related to emerging trends and future research directions.</jats:p> <jats:p>Our review began with a pool of 1,251 articles collected through a variety of mechanisms. Through a structured screening process, we identified 61 English research articles employing HMDs to enhance the visual sense of people with visual impairments for more detailed analyses. Our analyses reveal that there is an increasing amount of HMD-based research on visual assistance and therapy, and there are trends in the approaches associated with the research objectives. For example, AR is most often used for visual assistive purposes, whereas VR is used for therapeutic purposes. We report on eight existing survey articles, and present detailed analyses of the 61 research articles, looking at the mitigation objectives of the researchers (assistive versus therapeutic), the approaches used, the types of HMDs, the targeted visual conditions, and the inclusion of user studies. In addition to our detailed reviews and analyses of the various characteristics, we present observations related to apparent emerging trends and future research directions.</jats:p>
SP  - 1
EP  - 28
JF  - ACM Transactions on Accessible Computing
VL  - 15
IS  - 3
PB  - 
DO  - 10.1145/3522693
ER  - 

TY  - NA
AU  - Vermette, Laton; McGrenere, Joanna; Chilana, Parmit K.
TI  - Conference on Designing Interactive Systems - Peek-through Customization: Example-Based In-Context Sharing for Learning Management Systems
PY  - 2020
AB  - Learning to use a learning management system (LMS) can often be complex and challenging for instructors who have little time to explore the application interface. We ran formative interviews (N=10) suggesting that instructors often prefer to consult colleagues to seek examples of customizations and their explanations. Based on our findings, we designed and developed Customizer, an in-context example-based customization sharing platform that runs atop a widely-used LMS and facilitates discovery of relevant customizations shared by peers. Customizer allows instructors to experiment with shared customizations in familiar contexts by copying their course content into on-the-fly testing environments, minimizing any risk of breaking their live course setups. Our usability evaluation (N=10) showed that most users found Customizer intuitive and useful, and that its exploratory interface was helpful in a variety of use cases. Furthermore, many participants saw potential for Customizer to improve their workflows in other applications beyond educational contexts.
SP  - 1155
EP  - 1167
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395507
ER  - 

TY  - JOUR
AU  - Boopathy, Bhavadharini; Rajan, Anbarasan; Stephen, Jaspin; Radhakrishnan, Mahendran
TI  - Development and characterisation of structurally reforming engineered flat‐rice xerogel for hot water cooking
PY  - 2022
AB  - NA
SP  - 502
EP  - 511
JF  - International Journal of Food Science & Technology
VL  - 58
IS  - 1
PB  - 
DO  - 10.1111/ijfs.16128
ER  - 

TY  - NA
AU  - Hedayati, Hooman; Suzuki, Ryo; Leithinger, Daniel; Szafir, Daniel
TI  - PufferBot: Actuated Expandable Structures for Aerial Robots
PY  - 2020
AB  - We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone's propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures. The supplementary video can be found at: this https URL
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Glenn, Terrell; Ipsita, Ananya; Carithers, Caleb; Peppler, Kylie; Ramani, Karthik
TI  - CHI - StoryMakAR: Bringing Stories to Life With An Augmented Reality & Physical Prototyping Toolkit for Youth
PY  - 2020
AB  - Makerspaces can support educational experiences in prototyping for children. Storytelling platforms enable high levels of creativity and expression, but have high barriers of entry. We introduce StoryMakAR, which combines making and storytelling. StoryMakAR is a new AR-IoT system for children that uses block programming, physical prototyping, and event-based storytelling to bring stories to life. We reduce the barriers to entry for youth (Age=14-18) by designing an accessible, plug-and-play system through merging both electro-mechanical devices and virtual characters to create stories. We describe our initial design process, the evolution and workflow of StoryMakAR, and results from multiple single-session workshops with 33 high school students. Our preliminary studies led us to understand what students want to make. We provide evidence of how students both engage and have difficulties with maker-based storytelling. We also discuss the potential for StoryMakAR to be used as a learning environment for classrooms and younger students.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376790
ER  - 

TY  - NA
AU  - Suzuki, Ryo
TI  - UIST (Adjunct Volume) - Collective Shape-changing Interfaces
PY  - 2019
AB  - In this paper, I introduce collective shape-changing interfaces, a class of shape-changing interfaces that consist of a set of discrete collective elements. Through massively parallel transformation, locomotion, and connection of individual building blocks, the overall physical structure can be dynamically changed. Given this parallel change of individual elements, I propose three approaches for user interaction: dynamic, improvised, and actuated collective shape transformation. I exemplify each approach through my own work and possible future work.
SP  - 154
EP  - 157
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356877
ER  - 

TY  - CHAP
AU  - , 
TI  - Years of Referenced Manuscripts
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544581
ER  - 

TY  - NA
AU  - Zhang, Amy X.; Muller, Michael; Wang, Dakuo
TI  - How do Data Science Workers Collaborate? Roles, Workflows, and Tools
PY  - 2020
AB  - Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Li, Jiahao; Cui, Meilin; Kim, Jeeeun; Chen, Xiang 'Anthony'
TI  - Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to Robotically Augment Default Functionalities
PY  - 2020
AB  - Reconfiguring shapes of objects enables transforming existing passive objects with robotic functionalities, e.g., a transformable coffee cup holder can be attached to a chair's armrest, a piggy bank can reach out an arm to 'steal' coins. Despite the advance in end-user 3D design and fabrication, it remains challenging for non-experts to create such 'transformables' using existing tools due to the requirement of specific engineering knowledge such as mechanisms and robotic design. We present Romeo -- a design tool for creating transformables to robotically augment objects' default functionalities. Romeo allows users to transform an object into a robotic arm by expressing at a high level what type of task is expected. Users can select which part of the object to be transformed, specify motion points in space for the transformed part to follow and the corresponding action to be taken. Romeo then automatically generates a robotic arm embedded in the transformable part ready for fabrication. A design session validated this tool where participants used Romeo to accomplish controlled design tasks and to open-endedly create coin-stealing piggy banks by transforming 3D objects of their own choice.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - CrovariPietro, ; PidòSara, ; PinoliPietro, ; BernasconiAnna, ; CanakogluArif, ; GarzottoFranca, ; CeriStefano, 
TI  - GeCoAgent: A Conversational Agent for Empowering Genomic Data Extraction and Analysis
PY  - 2021
AB  - With the availability of reliable and low-cost DNA sequencing, human genomics is relevant to a growing number of end-users, including biologists and clinicians. Typical interactions require applyin...
SP  - 1
EP  - 29
JF  - ACM Transactions on Computing for Healthcare
VL  - 3
IS  - 1
PB  - 
DO  - 10.1145/3464383
ER  - 

TY  - NA
AU  - Fujita, Kazuyuki; Suzuki, Aoi; Takashima, Kazuki; Ikematsu, Kaori; Kitamura, Yoshifumi
TI  - CHI - TiltChair: Manipulative Posture Guidance by Actively Inclining the Seat of an Office Chair
PY  - 2021
AB  - We propose TiltChair, an actuated office chair that physically manipulates the user’s posture by actively inclining the chair’s seat to address problems associated with prolonged sitting. The system controls the inclination angle and motion speed with the aim of achieving manipulative but unobtrusive posture guidance. To demonstrate its potential, we first built a prototype of TiltChair with a seat that could be tilted by pneumatic control. We then investigated the effects of the seat’s inclination angle and motions on task performance and overall sitting experience through two experiments. The results show that the inclination angle mainly affects the difficulty of maintaining one’s posture, while the motion speed affected the conspicuousness and subjective acceptability of the motion. However, these seating conditions did not affect objective task performance. Based on these results, we propose a design space for facilitating effective seat-inclination behavior using the three dimensions of angle, speed, and continuity. Furthermore, we discuss promising applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445151
ER  - 

TY  - NA
AU  - Kim, Yoonji; Lee, Hyein; Prasad, Ramkrishna; Je, Seungwoo; Choi, Youngkyung; Ashbrook, Daniel; Oakley, Ian; Bianchi, Andrea
TI  - UIST - SchemaBoard: Supporting Correct Assembly of Schematic Circuits using Dynamic In-Situ Visualization
PY  - 2020
AB  - Assembling circuits on breadboards using reference designs is a common activity among makers. While tools like Fritzing offer a simplified visualization of how components and wires are connected, such pictorial depictions of circuits are rare in formal educational materials and the vast bulk of online technical documentation. Electronic schematics are more common but are perceived as challenging and confusing by novice makers. To improve access to schematics, we propose SchemaBoard, a system for assisting makers in assembling and inspecting circuits on breadboards from schematic source materials. SchemaBoard uses an LED matrix integrated underneath a working breadboard to visualize via light patterns where and how components should be placed, or to highlight elements of circuit topology such as electrical nets and connected pins. This paper presents a formative study with 16 makers, the SchemaBoard system, and a summative evaluation with an additional 16 users. Results indicate that SchemaBoard is effective in reducing both the time and the number of errors associated with building a circuit based on a reference schematic, and for inspecting the circuit for correctness after its assembly.
SP  - 987
EP  - 998
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415887
ER  - 

TY  - NA
AU  - Gonzalez Penuela, Ricardo E.; Poremba, Wren; Trice, Christina; Azenkot, Shiri
TI  - Hands-On: Using Gestures to Control Descriptions of a Virtual Environment for People with Visual Impairments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558669
ER  - 

TY  - NA
AU  - Zhou, Jingbo; Tang, Zhenwei; Zhao, Min; Ge, Xiang; Zhuang, Fuzhen; Zhou, Meng; Zou, Liming; Yang, Chenglei; Xiong, Hui
TI  - KDD - Intelligent Exploration for User Interface Modules of Mobile App with Collective Learning
PY  - 2020
AB  - A mobile app interface usually consists of a set of user interface modules. How to properly design these user interface modules is vital to achieving user satisfaction for a mobile app. However, there are few methods to determine design variables for user interface modules except for relying on the judgment of designers. Usually, a laborious post-processing step is necessary to verify the key change of each design variable. Therefore, there is only a very limited amount of design solutions that can be tested. It is time-consuming and almost impossible to figure out the best design solutions as there are many modules. To this end, we introduce FEELER, a framework to fast and intelligently explore design solutions of user interface modules with a collective machine learning approach. FEELER can help designers quantitatively measure the preference score of different design solutions, aiming to facilitate the designers to conveniently and quickly adjust user interface module. We conducted extensive experimental evaluations on two real-life datasets to demonstrate its applicability in real-life cases of user interface module design in the Baidu App, which is one of the most popular mobile apps in China.
SP  - 3346
EP  - 3355
JF  - Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3394486.3403387
ER  - 

TY  - NA
AU  - Sereshkeh, Alborz Rezazadeh; Leung, Gary; Perumal, Krish; Phillips, Caleb; Zhang, Minfan; Fazly, Afsaneh; Mohomed, Iqbal
TI  - IUI - VASTA: a vision and language-assisted smartphone task automation system
PY  - 2020
AB  - We present VASTA, a novel vision and language-assisted Programming By Demonstration (PBD) system for smartphone task automation. Development of a robust PBD automation system requires overcoming three key challenges: first, how to make a particular demonstration robust to positional and visual changes in the user interface (UI) elements; secondly, how to recognize changes in the automation parameters to make the demonstration as generalizable as possible; and thirdly, how to recognize from the user utterance what automation the user wishes to carry out. To address the first challenge, VASTA leverages state-of-the-art computer vision techniques, including object detection and optical character recognition, to accurately label interactions demonstrated by a user, without relying on the underlying UI structures. To address the second and third challenges, VASTA takes advantage of advanced natural language understanding algorithms for analyzing the user utterance to trigger the VASTA automation scripts, and to determine the automation parameters for generalization. We run an initial user study that demonstrates the effectiveness of VASTA at clustering user utterances, understanding changes in the automation parameters, detecting desired UI elements, and, most importantly, automating various tasks. A demo video of the system is available here: http://y2u.be/kr2xE-FixjI.
SP  - 22
EP  - 32
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377515
ER  - 

TY  - CHAP
AU  - Minagawa, Tatsuya; Ochiai, Yoichi
TI  - HCI (19) - A Case Study of Augmented Physical Interface by Foot Access with 3D Printed Attachment.
PY  - 2021
AB  - We propose an attachment creation framework that allows foot access to existing physical interfaces designed to use hands such as doorknobs. The levers, knobs, and switches of furniture and electronic devices are designed for the human hand. These interfaces may not be accessible for hygienic and physical reasons. Due to the high cost of parts and initial installation, sensing or automation is not preferable. Therefore, there is a need for a low-cost way to access physical interfaces without hands. We have enabled foot access by extending the hand-accessible interface with 3D-printed attachments. Finally, we proposed a mechanism (component set) that transmits movement from a foot-accessed pedal to an interface with attachments. And we attached it to the doorknob, water faucet, and lighting switch interface. A case study was conducted to verify the system’s effectiveness, which consisted of 3D-printed attachments and pedals.
SP  - 315
EP  - 332
JF  - Design, User Experience, and Usability: Design for Diversity, Well-being, and Social Development
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78224-5_22
ER  - 

TY  - NA
AU  - Lee, Yujin; Kim, Myeongseong; Kim, Hyunjung
TI  - Tangible and Embedded Interaction - Rolling Pixels: Robotic Steinmetz Solids for Creating Physical Animations
PY  - 2020
AB  - This article introduces Rolling Pixels, that are essentially robotic Steinmetz solids, for constructing frame-by-frame physical animations. As a bicylinder-shaped Rolling Pixel rolls back and forth or left and right, the shape and color of the top view of the pixel changes repeatedly without using any additional shape- or color-changing techniques. Implemented using off-the-shelf products and technologies, Rolling Pixels are easy-to-build, reproducible, and customizable kinetic design material. We describe the design and implementation of the current prototype of Rolling Pixels. We also illustrate the potential of the Rolling Pixels as building blocks for physical animations through a set of simulated examples.
SP  - 557
EP  - 564
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374987
ER  - 

TY  - NA
AU  - Yixian, Yan; Takashima, Kazuki; Tang, Anthony; Tanno, Takayuki; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - UIST - ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World
PY  - 2020
AB  - We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.
SP  - 223
EP  - 235
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415859
ER  - 

TY  - NA
AU  - Chang, Zekun; Ta, Tung D.; Narumi, Koya; Kim, Heeju; Okuya, Fuminori; Li, Dongchi; Kato, Kunihiro; Qi, Jie; Miyamoto, Yoshinobu; Saito, Kazuya; Kawahara, Yoshihiro
TI  - CHI Extended Abstracts - Demonstrating Kirigami Haptic Swatches for Cut-and-Fold Haptic Feedback Mechanisms
PY  - 2020
AB  - Kirigami Haptic Swatches demonstrates how kirigami and origami based structures enable sophisticated haptic feedback through simple cut-and-fold fabrication techniques. We leverage four types of geometric patterns: rotational erection system (RES), split-fold waterbomb (SFWB), the overlaid structure of SFWB and RES (SFWB+RES), and cylindrical origami, to render different sets of haptic feedback (i.e., linear, bistable, snap-through, and angular rotational force behaviors, respectively). In each structure, form factor and force feedback properties can be tuned through geometric parameters. Based on the experimental results and analysis, we implemented software to automatically generate 2D patterns for desired haptic properties. We also showcased five example applications: assistive input interfaces, rotational switch, multi-sensory toy, task checklist, and phone accessories. We believe the Kirigami Haptic Swatches helps tinkerers, designers, and even researchers to create interactions that enrich our haptic experience.
SP  - 3383162
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383162
ER  - 

TY  - NA
AU  - Chang, Joseph Chee; Kim, Yongsung; Miller, Victor; Liu, Michael Xieyang; Myers, Brad A.; Kittur, Aniket
TI  - UIST - Tabs.do: Task-Centric Browser Tab Management
PY  - 2021
AB  - Despite the increasing complexity and scale of people’s online activities, browser interfaces have stayed largely the same since tabs were introduced in major browsers nearly 20 years ago. The gap between simple tab-based browser interfaces and the complexity of users’ tasks can lead to serious adverse effects – commonly referred to as “tab overload.” This paper introduces a Chrome extension called Tabs.do, which explores bringing a task-centric approach to the browser, helping users to group their tabs into tasks and then organize, prioritize, and switch between those tasks fluidly. To lower the cost of importing, Tabs.do uses machine learning to make intelligent suggestions for grouping users’ open tabs into task bundles by exploiting behavioral and semantic features. We conducted a field deployment study where participants used Tabs.do with their real-life tasks in the wild, and showed that Tabs.do can decrease tab clutter, enabled users to create rich task structures with lightweight interactions, and allowed participants to context-switch among tasks more efficiently.
SP  - 663
EP  - 676
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474777
ER  - 

TY  - NA
AU  - Onishi, Yuki; Takashima, Kazuki; Higashiyama, Shoi; Fujita, Kazuyuki; Kitamura, Yoshifumi
TI  - WaddleWalls: Room-scale Interactive Partitioning System using a Swarm of Robotic Partitions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545615
ER  - 

TY  - NA
AU  - Li, Jingyi; Hirsch, Linda; Lu, Tianyang; Mayer, Sven; Butz, Andreas
TI  - A Touch of Realities: Car-Interior-Based Haptic Interaction Supports In-Car VR Recovery from Interruptions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3543768
ER  - 

TY  - JOUR
AU  - Ambe, Aloha Hufana; Soro, Alessandro; Johnson, Daniel; Brereton, Margot
TI  - From Collaborative Habituation to Everyday Togetherness: A Long-Term Study of Use of the Messaging Kettle
PY  - 2022
AB  - <jats:p> We present a long-term study of use of the Messaging Kettle, an Internet of Things (IOT) research prototype that augments an everyday kettle with both sensing and messaging capability and a beautiful light display in order to investigate connecting geographically distant loved ones to their family through the routine of boiling the kettle. Connection at a distance has been of sustained interest to the CHI community, and the social connection of older people is of increasing importance in recognition of ageing populations globally. However, very few novel designs in this domain have been investigated in situ or over the long term to examine whether their use sustains, and if so, how they impact communication in a relationship. The Messaging Kettle was trialled with four pairs of dispersed older mothers and adult daughters over timeframes that lasted between two months to more than two years. We observed the phenomenon of <jats:italic>collaborative habituation</jats:italic> wherein each party creatively made the technology work for them both through a combination of the gradual transformation of their everyday practices, arrangements, and living. Through developing these joint practices over time, participants expressed feelings of <jats:italic>everyday togetherness</jats:italic> that nurture their relationship at a distance. Three of the four couples continued to use the prototype for years, beyond the initial trial. We reflect on the artful integration of features of the Messaging Kettle and the way in which these features supported <jats:italic>collaborative habituation</jats:italic> . We also reflect on lessons and implications for the design of such relational technologies. </jats:p>
SP  - 1
EP  - 47
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 1
PB  - 
DO  - 10.1145/3470973
ER  - 

TY  - JOUR
AU  - Robe, Peter; Kuttal, Sandeep Kaur
TI  - Designing PairBuddy—A Conversational Agent for Pair Programming
PY  - 2022
AB  - <jats:p>From automated customer support to virtual assistants, conversational agents have transformed everyday interactions, yet despite phenomenal progress, no agent exists for programming tasks. To understand the design space of such an agent, we prototyped PairBuddy—an interactive pair programming partner—based on research from conversational agents, software engineering, education, human-robot interactions, psychology, and artificial intelligence. We iterated PairBuddy’s design using a series of Wizard-of-Oz studies. Our pilot study of six programmers showed promising results and provided insights toward PairBuddy’s interface design. Our second study of 14 programmers was positively praised across all skill levels. PairBuddy’s active application of soft skills—adaptability, motivation, and social presence—as a navigator increased participants’ confidence and trust, while its technical skills—code contributions, just-in-time feedback, and creativity support—as a driver helped participants realize their own solutions. PairBuddy takes the first step towards an Alexa-like programming partner.</jats:p>
SP  - 1
EP  - 44
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 4
PB  - 
DO  - 10.1145/3498326
ER  - 

TY  - NA
AU  - Yen, Yu-Chun Grace; Kim, Joy O.; Bailey, Brian P.
TI  - CHI - Decipher: An Interactive Visualization Tool for Interpreting Unstructured Design Feedback from Multiple Providers
PY  - 2020
AB  - Feedback from diverse audiences can vary in focus, differ in structure, and contradict each other, making it hard to interpret and act on. While prior work has explored generating quality feedback, our work helps a designer interpret that feedback. Through a formative study with professional designers (N=10), we discovered that the interpretation process includes categorizing feedback, identifying valuable feedback, and prioritizing which feedback to incorporate in a revision. We also found that designers leverage feedback topic and sentiment, and the status of the provider to aid interpretation. Based on the findings, we created a new tool (Decipher) that enables designers to visualize and navigate a collection of feedback using its topic and sentiment structure. In a preliminary evaluation (N=20), we found that Decipher helped users feel less overwhelmed during feedback interpretation tasks and better attend to critical issues and conflicting opinions compared to using a typical document-editing tool.
SP  - 3376380
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376380
ER  - 

TY  - NA
AU  - Glake, Daniel; Ritter, Norbert; Ocker, Florian; Ahmady-Moghaddam, Nima; Osterholz, Daniel; Lenfers, Ulfia A.; Clemen, Thomas
TI  - CIKM - Hierarchical Semantics Matching For Heterogeneous Spatio-temporal Sources
PY  - 2021
AB  - Spatio-temporal data are semantically valuable information used for various analytical tasks to identify spatially relevant and temporally limited correlations within a domain. The increasing availability and data acquisition from multiple sources with their typically high heterogeneity are getting more and more attention. However, these sources often lack interconnecting shared keys, making their integration a challenging problem. For example, publicly available parking data that consist of point data on parking facilities with fluctuating occupancy and static location data on parking spaces cannot be directly correlated. Both data sets describe two different aspects from distinct sources in which parking spaces and fluctuating occupancy are part of the same semantic model object. Especially for ad hoc analytical tasks on integrated models, these missing relationships cannot be handled using join operations as usual in relational databases. The reason lies in the lack of equijoin relationships, comparing for equality of strings and additional overhead in loading data up before processing. This paper addresses the optimization problem of finding suitable partners in the absence of equijoin relations for heterogeneous spatio-temporal data, applicable to ad hoc analytics. We propose a graph-based approach that achieves good recall and performance scaling via hierarchically separating the semantics along spatial, temporal, and domain-specific dimensions. We evaluate our approach using public data, showing that it is suitable for many standard join scenarios and highlighting its limitations.
SP  - 565
EP  - 575
JF  - Proceedings of the 30th ACM International Conference on Information & Knowledge Management
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3459637.3482350
ER  - 

TY  - JOUR
AU  - Seo, MinYeong; Kang, HyeongYeop
TI  - Toward virtual stair walking
PY  - 2021
AB  - This paper presents a motion remapping-based locomotion technique. Our technique can provide a realistic sensation of climbing and descending stairs when users navigate the virtual environment on foot. The main contribution is to provide users a realistic experience of walking up and down virtual stairs while in reality, they are walking on a flat surface. When a user lifts their real foot, our technique controls the position of virtual foot in order to match the timing of real foot touching the floor with that of virtual foot touching the stairs. The avatar’s head and waist are also controlled to mimic the height change movements of stair walking. To achieve this, we collected the actual motion data beforehand and then designed our locomotion technique using the data. Then, we conducted an experiment and an application test. In the experiment, we identified how much visual gain should be applied to foot motion to induce a realistic sensation of stair walking. The results demonstrated that applying visual gains of 1.193 and 0.822 to motions of climbing and descending the stairs were accepted as the most realistic, respectively. In the application test, we investigated whether the proposed technique successfully increases the user’s perceived presence and provides a positive user experience. The results demonstrated that the user’s perceived presence was significantly enhanced when we applied visual gains. The results also showed that participants felt as if they were walking on the stairs in the virtual environment without experiencing discomfort or postural instability. As the proposed technique only needs visual cue control, we expect that it can easily be applied to commercial applications .
SP  - 2783
EP  - 2795
JF  - The Visual Computer
VL  - 37
IS  - 9
PB  - 
DO  - 10.1007/s00371-021-02179-2
ER  - 

TY  - CHAP
AU  - Cruz, Christian Arzate; Igarashi, Takeo
TI  - Interactive Reinforcement Learning for Autonomous Behavior Design
PY  - 2021
AB  - Reinforcement Learning (RL) is a machine learning approach based on how humans and animals learn new behaviors by actively exploring their environment that provides them positive and negative rewards. The interactive RL approach incorporates a human-in-the-loop that can guide a learning RL-based agent to personalize its behavior and/or speed up its learning process. To enable HCI researchers to make advances in this area, we introduce an interactive RL framework that outlines HCI challenges in the domain. By following this taxonomy, HCI researchers can (1) design new interaction techniques and (2) propose new applications. To help the role (1) researchers, we describe how different types of human feedback can adapt an RL model to perform as the users intend. We help researchers perform the role (2) by proposing generic design principles to create effective RL applications. Finally, we list current open challenges in interactive RL and what we consider the most promising research directions in this research area.
SP  - 345
EP  - 375
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_11
ER  - 

TY  - NA
AU  - Kaimoto, Hiroki; Monteiro, Kyzyl; Faridan, Mehrad; Li, Jiatong; Farajian, Samin; Kakehi, Yasuaki; Nakagaki, Ken; Suzuki, Ryo
TI  - Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI
PY  - 2022
AB  - This paper introduces Sketched Reality, an approach that combines AR sketching and actuated tangible user interfaces (TUI) for bidirectional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to "affect" each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional -- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545626
ER  - 

TY  - NA
AU  - Chang, Zekun; Ta, Tung D.; Narumi, Koya; Kim, Heeju; Okuya, Fuminori; Li, Dongchi; Kato, Kunihiro; Qi, Jie; Miyamoto, Yoshinobu; Saito, Kazuya; Kawahara, Yoshihiro
TI  - CHI - Kirigami Haptic Swatches: Design Methods for Cut-and-Fold Haptic Feedback Mechanisms
PY  - 2020
AB  - Kirigami Haptic Swatches demonstrate how kirigami and origami based structures enable sophisticated haptic feedback through simple cut-and-fold fabrication techniques. We leverage four types of geometric patterns: rotational erection system (RES), split-fold waterbomb (SFWB), the overlaid structure of SFWB and RES (SFWB+RES), and cylindrical origami, to render different sets of haptic feedback (i.e. linear, bistable, bouncing snap-through, and rotational force behaviors, respectively). In each structure, not only the form factor but also the force feedback properties can be tuned through geometric parameters. We experimentally analyzed and modeled the structures, and implemented software to automatically generate 2D patterns for desired haptic properties. We also demonstrate five example applications including an assistive custom keyboard, rotational switch, multi-sensory toy, task checklist, and phone accessories. We believe the Kirigami Haptic Swatches helps tinkerers, designers, and even researchers to create interactions that enrich our haptic experience.
SP  - 3376655
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376655
ER  - 

TY  - NA
AU  - Endo, Isamu; Takashima, Kazuki; Inoue, Maakito; Fujita, Kazuyuki; Kiyokawa, Kiyoshi; Kitamura, Yoshifumi
TI  - UIST - ModularHMD: A Reconfigurable Mobile Head-Mounted Display Enabling Ad-hoc Peripheral Interactions with the Real World
PY  - 2021
AB  - We propose ModularHMD, a new mobile head-mounted display concept, which adopts a modular mechanism and allows a user to perform ad-hoc peripheral interaction with real-world devices or people during VR experiences. ModularHMD is comprised of a central HMD and three removable module devices installed in the periphery of the HMD cowl. Each module has four main states: occluding, extended VR view, video see-through (VST), and removed/reused. Among different combinations of module states, a user can quickly setup the necessary HMD forms, functions, and real-world visions for ad-hoc peripheral interactions without removing the headset. For instance, an HMD user can see her surroundings by switching a module into the VST mode. She can also physically remove a module to obtain direct peripheral visions of the real world. The removed module can be reused as an instant interaction device (e.g., touch keyboards) for subsequent peripheral interactions. Users can end the peripheral interaction and revert to a full VR experience by re-mounting the module. We design ModularHMD’s configuration and peripheral interactions with real-world objects and people. We also implement a proof-of-concept prototype of ModularHMD to validate its interactions capabilities through a user study. Results show that ModularHMD is an effective solution that enables both immersive VR and ad-hoc peripheral interactions.
SP  - 100
EP  - 117
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474738
ER  - 

TY  - NA
AU  - Riva, Oriana; Kace, Jason
TI  - UIST - Etna: Harvesting Action Graphs from Websites
PY  - 2021
AB  - Knowledge bases, such as Google knowledge graph, contain millions of entities (people, places, etc.) and billions of facts about them. While much is known about entities, little is known about the actions these entities relate to. On the other hand, the Web has lots of information about human tasks. A website for restaurant reservations, for example, implicitly knows about various restaurant-related actions (making reservations, delivering food, etc.), the inputs these actions require and their expected output; it can also be automated to execute those actions. To harvest action knowledge from websites, we propose Etna. Users demonstrate how to accomplish various tasks in a website, and Etna constructs an action-state model of the website visualized as an action graph. An action graph includes definitions of tasks and actions, knowledge about their start/end states, and execution scripts for their automation. We report on our experience in building action-state models of many commercial websites and use cases that leveraged them.
SP  - 312
EP  - 331
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474752
ER  - 

TY  - NA
AU  - Chang, Joseph Chee; Hahn, Nathan; Kim, Yongsung; Coupland, Julina; Breneisen, Bradley; Kim, Hannah S; Hwong, John; Kittur, Aniket
TI  - CHI - When the Tab Comes Due:Challenges in the Cost Structure of Browser Tab Usage
PY  - 2021
AB  - Tabs have become integral to browsing the Web yet have changed little since their introduction nearly 20 years ago. In contrast, the internet has gone through dramatic changes, with users increasingly moving from navigating to websites to exploring information across many sources to support online sensemaking. This paper investigates how tabs today are overloaded with a diverse set of functionalities and issues users face when managing them. We interviewed ten information workers asking about their tab management strategies and walk through each open tab on their work computers four times over two weeks. We uncovered competing pressures pushing for keeping tabs open (ranging from interaction to emotional costs) versus pushing for closing them (such as limited attention and resources). We then surveyed 103 participants to estimate the frequencies of these pressures at scale. Finally, we developed design implications for future browser interfaces that can better support managing these pressures.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445585
ER  - 

TY  - NA
AU  - Yang, Humphrey; Luo, Danli; Qian, Kuanren; Yao, Lining
TI  - CHI - Freeform Fabrication of Fluidic Edible Materials
PY  - 2021
AB  - From providing nutrition to facilitating social exchanges, food plays an essential role in our daily lives and cultures. In HCI, we are interested in using food as an interaction medium and a context of personal fabrication. Yet, the design space of available food printing methods is limited to shapes with minimal overhangs and materials that have a paste-like consistency. In this work, we seek to expand this design space by adapting support bath-assisted printing to the food context. The bath scaffolds the embedded materials and preserves shapes during the printing processes, enabling us to create freeform food with fluid-like materials. We provide users guidelines for choosing the appropriate support bath type and processing methods depending on the printing material's properties. A design tool suite and application examples, including confectionery arts, 4D printed food, and edible displays are also offered to demonstrate the enabled interaction design space.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445097
ER  - 

TY  - NA
AU  - Yang, Humphrey; Qian, Kuanren; Liu, Haolin; Yu, Yuxuan; Gu, Jianzhe; McGehee, Matthew; Zhang, Yongjie Jessica; Yao, Lining
TI  - SimuLearn: Fast and Accurate Simulator to Support Morphing Materials Design and Workflows
PY  - 2020
AB  - Morphing materials allow us to create new modalities of interaction and fabrication by leveraging dynamic behaviors of materials. Yet, despite the ongoing rapid growth of computational tools within this realm, current developments are bottlenecked by the lack of an effective simulation method. As a result, existing design tools must trade-off between speed and accuracy to support a real-time interactive design scenario. In response, we introduce SimuLearn, a data-driven method that combines finite element analysis and machine learning to create real-time (0.61 seconds) and truthful (97% accuracy) morphing material simulators. We use mesh-like 4D printed structures to contextualize this method and prototype design tools to exemplify the design workflows and spaces enabled by a fast and accurate simulation method. Situating this work among existing literature, we believe SimuLearn is a timely addition to the HCI CAD toolbox that can enable the proliferation of morphing materials.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wilhelm, Mathias; Krakowczyk, Daniel; Albayrak, Sahin
TI  - PeriSense: Ring-Based Multi-Finger Gesture Interaction Utilizing Capacitive Proximity Sensing.
PY  - 2020
AB  - Rings are widely accepted wearables for gesture interaction. However, most rings can sense only the motion of one finger or the whole hand. We present PeriSense, a ring-shaped interaction device enabling multi-finger gesture interaction. Gestures of the finger wearing ring and its adjacent fingers are sensed by measuring capacitive proximity between electrodes and human skin. Our main contribution is the determination of PeriSense's interaction space involving the evaluation of capabilities and limitations. We introduce a prototype named PeriSense, analyze the sensor resolution at different distances, and evaluate finger gestures and unistroke gestures based on gesture sets allowing the determination of the strengths and limitations. We show that PeriSense is able to sense the change of conductive objects reliably up to 2.5 cm. Furthermore, we show that this capability enables different interaction techniques such as multi-finger gesture recognition or two-handed unistroke input.
SP  - 3990
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 14
PB  - 
DO  - 10.3390/s20143990
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Popowski, Lindsay; Mitchell, Tom M.; Myers, Brad A.
TI  - Screen2Vec: Semantic Embedding of GUI Screens and GUI Components
PY  - 2021
AB  - Representing the semantics of GUI screens and components is crucial to data-driven computational methods for modeling user-GUI interactions and mining GUI designs. Existing GUI semantic representations are limited to encoding either the textual content, the visual design and layout patterns, or the app contexts. Many representation techniques also require significant manual data annotation efforts. This paper presents Screen2Vec, a new self-supervised technique for generating representations in embedding vectors of GUI screens and components that encode all of the above GUI features without requiring manual annotation using the context of user interaction traces. Screen2Vec is inspired by the word embedding method Word2Vec, but uses a new two-layer pipeline informed by the structure of GUIs and interaction traces and incorporates screen- and app-specific metadata. Through several sample downstream tasks, we demonstrate Screen2Vec's key useful properties: representing between-screen similarity through nearest neighbors, composability, and capability to represent user tasks.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445049
ER  - 

TY  - JOUR
AU  - Park, Hyung In; Lee, Seung-Mi; Kim, Yoo Na
TI  - Psychological Reattachment to Work: Its Nomological Network and Role in the Relationship between Role Ambiguity and Work Engagement
PY  - 2021
AB  - NA
SP  - 43
EP  - 67
JF  - Journal of Social Science
VL  - 32
IS  - 2
PB  - 
DO  - 10.16881/jss.2021.04.32.2.43
ER  - 

TY  - NA
AU  - Chen, Yan; Lasecki, Walter S.; Dong, Tao
TI  - Towards Supporting Programming Education at Scale via Live Streaming
PY  - 2020
AB  - Live streaming, which allows streamers to broadcast their work to live viewers, is an emerging practice for teaching and learning computer programming. Participation in live streaming is growing rapidly, despite several apparent challenges, such as a general lack of training in pedagogy among streamers and scarce signals about a stream's characteristics (e.g., difficulty, style, and usefulness) to help viewers decide what to watch. To understand why people choose to participate in live streaming for teaching or learning programming, and how they cope with both apparent and non-obvious challenges, we interviewed 14 streamers and 12 viewers about their experience with live streaming programming. Among other results, we found that the casual and impromptu nature of live streaming makes it easier to prepare than pre-recorded videos, and viewers have the opportunity to shape the content and learning experience via real-time communication with both the streamer and each other. Nonetheless, we identified several challenges that limit the potential of live streaming as a learning medium. For example, streamers voiced privacy and harassment concerns, and existing streaming platforms do not adequately support viewer-streamer interactions, adaptive learning, and discovery and selection of streaming content. Based on these findings, we suggest specialized tools to facilitate knowledge sharing among people teaching and learning computer programming online, and we offer design recommendations that promote a healthy, safe, and engaging learning environment.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Lang, Florian; Schmidt, Albrecht; Machulla, Tonja
TI  - ICCHP (1) - Augmented Reality for People with Low Vision: Symbolic and Alphanumeric Representation of Information.
PY  - 2020
AB  - Many individuals with visual impairments have residual vision that often remains underused by assistive technologies. Head-mounted augmented reality (AR) devices can provide assistance, by recoding difficult-to-perceive information into a visual format that is more accessible. Here, we evaluate symbolic and alphanumeric information representations for their efficiency and usability in two prototypical AR applications: namely, recognizing facial expressions of conversational partners and reading the time. We find that while AR provides a general benefit, the complexity of the visual representations has to be matched to the user’s visual acuity.
SP  - 146
EP  - 156
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58796-3_19
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Nakayama, Ryosuke; Liu, Dan; Kakehi, Yasuaki; Gross, Mark D.; Leithinger, Daniel
TI  - UIST (Adjunct Volume) - LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators
PY  - 2019
AB  - This paper introduces LiftTiles, a modular and reconfigurable room-scale shape display. LiftTiles consist of an array of retractable and inflatable actuator that is compact (e.g., 15cm tall) and light (e.g., 1.8kg), while extending up to 1.5m to allow for large-scale shape transformation. Inflatable actuation also provides a robust structure that can support heavy objects (e.g., 10 kg weight). This paper describes the design and implementation of LiftTiles and explores the application space for reconfigurable room-scale shape displays.
SP  - 30
EP  - 32
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3357105
ER  - 

TY  - NA
AU  - Wang, Bryan; Li, Gang; Zhou, Xin; Chen, Zhourong; Grossman, Tovi; Li, Yang
TI  - Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning
PY  - 2021
AB  - Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across $\sim$22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Wagner, Tobias; Gugenheimer, Jan; Rukzio, Enrico
TI  - ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact Retrieval using Mixed Reality
PY  - 2022
AB  - Personal fabrication is made more accessible through repositories like Thingiverse, as they replace modeling with retrieval. However, they require users to translate spatial requirements to keywords, which paints an incomplete picture of physical artifacts: proportions or morphology are non-trivially encoded through text only. We explore a vision of in-situ spatial search for (future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to search for 3D models using in-situ sketches blended with textual queries. With ShapeFindAR, users search for geometry, and not necessarily precise labels, while coupling the search process to the physical environment (e.g., by sketching in-situ, extracting search terms from objects present, or tracing them). We developed ShapeFindAR for HoloLens 2, connected to a database of 3D-printable artifacts. We specify in-situ spatial search, describe its advantages, and present walkthroughs using ShapeFindAR, which highlight novel ways for users to articulate their wishes, without requiring complex modeling tools or profound domain knowledge.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517682
ER  - 

TY  - NA
AU  - Kim, Myung Jin; Bianchi, Andrea
TI  - AHs - Exploring Pseudo Hand-Eye Interaction on the Head-Mounted Display
PY  - 2021
AB  - Virtual and augmented reality devices and applications have enabled the user to experience a variety of simulated real-life experiences through first-person visual, auditory, and haptic feedback. However, among the numerous everyday interactions that have been emulated, the familiar interaction of touching or rubbing the eyes is yet to be explored and remains to be understood. In this paper, we aim to understand the components of natural hand-eye interaction, propose an interaction technique through a proof-of-concept prototype head-mounted display, and evaluate the user experience of the prototype through a user study. In addition, we share insights emerged from the studies with suggestions for further development of interaction techniques based on combinations of hardware and software.
SP  - 251
EP  - 258
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458960
ER  - 

TY  - JOUR
AU  - Avellino, Ignacio; Nozari, Sheida; Canlorbe, Geoffroy; Jansen, Yvonne
TI  - Surgical Video Summarization
PY  - 2021
AB  - <jats:p>While surgical videos are valuable support material for activities around surgery, their summarization demands great amounts of time from surgeons, resulting in the production of very few videos. We study the practices involving surgical video to motivate and inform the future design of tools for their summarization. Through interviews and observations in a field study, we find that (1) video summaries provide an important support for surgery, being used for self-improvement, education, discussing cases, scientific research, patient communication and as legal resources; (2) video summarization follows a process hindered by the loss of knowledge that originates during recording; and, (3) surgeons develop ad-hoc coordination strategies which involve using the video itself for articulation work, making it both the field of work and coordination artifact. We discuss ways in which tools can facilitate capturing knowledge during live action using these strategies.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
PB  - 
DO  - 10.1145/3449214
ER  - 

TY  - CHAP
AU  - , 
TI  - Paths Forward: Aspirations for TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544577
ER  - 

TY  - CHAP
AU  - Lang, Florian; Schmidt, Albrecht; Machulla, Tonja
TI  - Mixed Reality as Assistive Technology: Guidelines Based on an Assessment of Residual Functional Vision in Persons with Low Vision
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Residual visual capabilities and the associated phenomenological experience can differ significantly between persons with similar visual acuity and similar diagnosis. There is a substantial variance in situations and tasks that persons with low vision find challenging. Smartglasses provide the opportunity of presenting individualized visual feedback targeted to each user’s requirements. Here, we interviewed nine persons with low vision to obtain insight into their subjective perceptual experience associated with factors such as illumination, color, contrast, and movement, as well as context factors. Further, we contribute a collection of everyday activities that rely on visual perception as well as strategies participants employ in their everyday lives. We find that our participants rely on their residual vision as the dominant sense in many different everyday activities. They prefer vision to other modalities if they can perceive the information visually, which highlights the need for assistive devices with visual feedback.</jats:p>
SP  - 484
EP  - 493
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-08645-8_57
ER  - 

TY  - CHAP
AU  - Rizk, Yara; Venkateswaran, Praveen; Isahagian, Vatche; Muthusamy, Vinod; Talamadupula, Kartik
TI  - Can You Teach Robotic Process Automation Bots New Tricks?
PY  - 2022
AB  - AbstractOver the past decade, robotic process automation (RPA) has emerged as a lightweight paradigm for automation in business enterprises, making automation more accessible to non-techie business users. In the industry, RPA vendors have not only provided out-of-the-box RPA bots to automate manual tasks on legacy software; they have also provided users a recorder to create their own bots for specialized tasks. However, if these recorders do not create generalizable bots, users risk facing a “bot sprawl” and governance problem. Building generalizable bots currently requires intervention from IT departments which are typically oversubscribed given their limited resources. Furthermore, the generalization process is typically long and tedious; it does not scale to cover the expansive needs of business users. We thus need a tool that can empower business users to act as citizen developers and build generalized bots themselves. In this work, we argue that the next generation of RPA bots must leverage artificial intelligence to learn from user interactions (through natural language or other modalities intuitive to citizen developers) and generalize to unseen settings. To achieve this, we first survey and assess the current state of the art in the RPA field for enabling citizen developers; and identify several key research challenges at the intersection of AI, RPA, and interactive task learning that must be addressed to realize the vision of RPA bots that continually learn new automation solutions from user interactions.KeywordsRobotic process automationArtificial intelligenceLearningTeaching by instruction
SP  - 246
EP  - 259
JF  - Business Process Management: Blockchain, Robotic Process Automation, and Central and Eastern Europe Forum
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-16168-1_16
ER  - 

TY  - JOUR
AU  - Fombona-Pascual, Alba; Fombona, Javier; Vicente, Rubén
TI  - Augmented Reality, a Review of a Way to Represent and Manipulate 3D Chemical Structures.
PY  - 2022
AB  - Augmented reality (AR) is a mixed technology that superimposes three-dimensional (3D) digital data onto an image of reality. This technology enables users to represent and manipulate 3D chemical structures. In spite of its potential, the use of these tools in chemistry is still scarce. The aim of this work is to identify the real situation of AR developments and its potential for 3D visualization of molecules. A descriptive analysis of a selection of 143 research publications (extracted from Web of Science between 2018 and 2020) highlights some significant AR examples that had been implemented in chemistry, in both education and research environments. Although the traditional 2D screen visualization is still preferred when teaching chemistry, the application of AR in early education has shown potential to facilitate the understanding and visualization of chemical structures. The increasing connectivity of the AR technology to web platforms and scientific networks should translate into new opportunities for teaching and learning strategies.
SP  - 1863
EP  - 1872
JF  - Journal of chemical information and modeling
VL  - 62
IS  - 8
PB  - 
DO  - 10.1021/acs.jcim.1c01255
ER  - 

TY  - NA
AU  - Matulic, Fabrice; Ganeshan, Aditya; Fujiwara, Hiroshi; Vogel, Daniel
TI  - CHI - Phonetroller: Visual Representations of Fingers for Precise Touch Input with Mobile Phones in VR
PY  - 2021
AB  - Smartphone touch screens are potentially attractive for interaction in virtual reality (VR). However, the user cannot see the phone or their hands in a fully immersive VR setting, impeding their ability for precise touch input. We propose mounting a mirror above the phone screen such that the front-facing camera captures the thumbs on or near the screen. This enables the creation of semi-transparent overlays of thumb shadows and inference of fingertip hover points with deep learning, which help the user aim for targets on the phone. A study compares the effect of visual feedback on touch precision in a controlled task and qualitatively evaluates three example applications demonstrating the potential of the technique. The results show that the enabled style of feedback is effective for thumb-size targets, and that the VR experience can be enriched by using smartphones as VR controllers supporting precise touch input.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445583
ER  - 

TY  - NA
AU  - Li, Xiang; Tang, Xiaohang; Tong, Xin; Patibanda, Rakesh; Mueller, Florian 'Floyd'; Liang, Hai-Ning
TI  - CHI PLAY - Myopic Bike and Say Hi: Games for Empathizing with The Myopic
PY  - 2021
AB  - Myopia is an eye condition that makes it difficult for people to focus on faraway objects. It has become one of the most serious eye conditions worldwide and negatively impacts the quality of life of those who suffer from it. Although myopia is prevalent, many non-myopic people have misconceptions about it and encounter challenges empathizing those who suffer from it. In this research, we developed two virtual reality (VR) games, (1) ”Myopic Bike” and (2) ”Say Hi”, to provide a means for the non-myopic population to experience the difficulties of myopic people. Our two games simulate two inconvenient daily life scenarios (riding a bicycle and greeting friends on the street) that myopic people encounter when not wearing glasses. The goal is to facilitate empathy in people with non-myopia for those who suffer from myopia. We evaluated four participants’ game experiences through questionnaires and semi-structured interviews. Overall, our two VR games can create an engaging and non-judgmental experience for the non-myopic people that has potential to facilitate empathizing with those who suffer from myopia.
SP  - 333
EP  - 338
JF  - Extended Abstracts of the 2021 Annual Symposium on Computer-Human Interaction in Play
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450337.3483505
ER  - 

TY  - NA
AU  - Guo, Kaiwen; Zhou, Hao; Tian, Ye; Zhou, Wangqiu; Ji, Yusheng; Li, Xiang-Yang
TI  - Mudra: A Multi-Modal Smartwatch Interactive System with Hand Gesture Recognition and User Identification
PY  - 2022
AB  - The great popularity of smartwatches leads to a growing demand for smarter interactive systems. Hand gesture is suitable for interaction due to its unique features. However, the existing single-modal gesture interactive systems have different biases in diverse scenarios, which makes it intractable to be applied in real life. In this paper, we propose a multi-modal smartwatch interactive system named Mudra, which fuses vision and Inertial Measurement Unit (IMU) signals to recognize and identify hand gestures for convenient and robust interaction. We carefully design a parallel attention multi-task model for different modals, and fuse classification results at the decision level with an adaptive weight adjustment algorithm. We implement a prototype of Mudra and collect data from 25 volunteers to evaluate its effectiveness. Extensive experiments demonstrate that Mudra can achieve 95.4% and 92.3% F1-scores on recognition and identification tasks, respectively. Meanwhile, Mudra can maintain stability and robustness under different experimental settings.
SP  - NA
EP  - NA
JF  - IEEE INFOCOM 2022 - IEEE Conference on Computer Communications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/infocom48880.2022.9796879
ER  - 

TY  - JOUR
AU  - Wang, Guanyun; Qin, Fang; Liu, Haolin; Tao, Ye; Zhang, Yang; Zhang, Yongjie Jessica; Yao, Lining
TI  - MorphingCircuit: An Integrated Design, Simulation, and Fabrication Workflow for Self-morphing Electronics
PY  - 2020
AB  - Manufacturing nonplanar electronics often requires the integration of functions and forms through embedding circuit boards into three-dimensional (3D) shapes. While most popular solutions rely on cavities where electronics reside in forms of rigid circuit boards, other alternative approaches leverage 3D printing or layer lamination to create 3D electronics that often require expensive manufacturing processes and materials. Furthermore, many conventional methods are incompatible with complex geometries (e.g., surfaces that twist or have local minima). In response, we introduce MorphingCircuit, an integrated design, simulation, and fabrication workflow that combines electronic functions with forms through four-dimensional (4D) printing, which effectively reduces cost, production time, and e-waste. Specifically, we start by printing a flat substrate and assembling functional electronics on top of it. The flat structure will then self-morph into a preprogrammed 3D shape when triggered by external heating. Overall, our comprehensive 3D electronics fabrication pipeline encompasses the design, simulation, fabrication, and transformation, with which we hope to inspire designers, researchers, and makers to create conformal electronics on complex substrate geometries that were previously difficult or impossible to design or manufacture.
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 4
PB  - 
DO  - 10.1145/3432232
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Mitchell, Tom M.; Myers, Brad A.
TI  - ACL (demo) - Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations
PY  - 2020
AB  - We summarize our past five years of work on designing, building, and studying Sugilite, an interactive task learning agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations leveraging the graphical user interfaces (GUIs) of third-party mobile apps. Through its multi-modal and mixed-initiative approaches for Human- AI interaction, Sugilite made important contributions in improving the usability, applicability, generalizability, flexibility, robustness, and shareability of interactive task learning agents. Sugilite also represents a new human-AI interaction paradigm for interactive task learning, where it uses existing app GUIs as a medium for users to communicate their intents with an AI agent instead of the interfaces for users to interact with the underlying computing services. In this chapter, we describe the Sugilite system, explain the design and implementation of its key features, and show a prototype in the form of a conversational assistant on Android.
SP  - 215
EP  - 223
JF  - Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations
VL  - NA
IS  - NA
PB  - 
DO  - 10.18653/v1/2020.acl-demos.25
ER  - 

TY  - NA
AU  - Feick, Martin; Regitz, Kora Persephone; Tang, Anthony; Krüger, Antonio
TI  - Designing Visuo-Haptic Illusions with Proxies in Virtual Reality: Exploration of Grasp, Movement Trajectory and Object Mass
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517671
ER  - 

TY  - JOUR
AU  - Hu, Fang; He, Peng; Xu, Songlin; Li, Yin; Zhang, Cheng
TI  - FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist
PY  - 2020
AB  - In this paper, we present FingerTrak, a minimal-obtrusive wristband that enables continuous 3D finger tracking and hand pose estimation with four miniature thermal cameras mounted closely on a form-fitting wristband. FingerTrak explores the feasibility of continuously reconstructing the entire hand postures (20 finger joints positions) without the needs of seeing all fingers. We demonstrate that our system is able to estimate the entire hand posture by observing only the outline of the hand, i.e., hand silhouettes from the wrist using low-resolution (32 x 24) thermal cameras. A customized deep neural network is developed to learn to "stitch" these multi-view images and estimate 20 joints positions in 3D space. Our user study with 11 participants shows that the system can achieve an average angular error of 6.46° when tested under the same background, and 8.06° when tested under a different background. FingerTrak also shows encouraging results with the re-mounting of the device and has the potential to reconstruct some of the complicated poses. We conclude this paper with further discussions of the opportunities and challenges of this technology.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397306
ER  - 

TY  - JOUR
AU  - Maris, Elena; Wagman, Kelly B.; Bergmann, Rachel; Bragg, Danielle
TI  - Tech Worker Perspectives on Considering the Interpersonal Implications of Communication Technologies
PY  - 2022
AB  - <jats:p>Communication technologies, from social media to video conferencing, are used by billions of people globally and contribute to shaping relationships between people. As these technologies become increasingly ubiquitous, the tech workers building them are increasingly making product decisions that can have far-reaching interpersonal ramifications. At the same time, few workplace tools and support exist to help tech workers understand and navigate these potential ramifications, and tech worker perspectives on such tools are not fully understood. In this work, we explore the needs, challenges, and opportunities encountered by tech workers in thinking through the interpersonal implications of their products. To do this, we ran a semi-structured interview study with 10 diverse tech workers. To ground the discussion, study participants interacted with a design probe prototype, InterAct, which provides research-grounded information about interpersonal implications of product features. Our findings suggest a desire by tech workers to consider the social implications of the technologies they build, and the potential for structured tooling to help provide the required knowledge and build organizational support. Based on these findings, we provide design considerations for creating future workplace tools to support thinking about the social implications of technologies.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 7
IS  - GROUP
PB  - 
DO  - 10.1145/3567566
ER  - 

TY  - JOUR
AU  - Jin, Xiaofu; Hu, Xiaozhu; Wei, Xiaoying; Fan, Mingming
TI  - Synapse
PY  - 2022
AB  - <jats:p>As smartphones are widely adopted, mobile applications (apps) are emerging to provide critical services such as food delivery and telemedicine. While bring convenience to everyday life, this trend may create barriers for older adults who tend to be less tech-savvy than young people. In-person or screen sharing support is helpful but limited by the help-givers' availability. Video tutorials can be useful but require users to switch contexts between watching the tutorial and performing the corresponding actions in the app, which is cumbersome to do on a mobile phone. Although interactive tutorials have been shown to be promising, none was designed for older adults. Furthermore, the trial-and-error approach has been shown to be beneficial for older adults, but they often lack support to use the approach. Inspired by both interactive tutorials and trial-and-error approach, we designed an app-independent mobile service, Synapse, for help-givers to create a multimodal interactive tutorial on a smartphone and for help-receivers (e.g., older adults) to receive interactive guidance with trial-and-error support when they work on the same task. We conducted a user study with 18 older adults who were 60 and over. Our quantitative and qualitative results show that Synapse provided better support than the traditional video approach and enabled participants to feel more confident and motivated. Lastly, we present further design considerations to better support older adults with trial-and-error on smartphones.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550321
ER  - 

TY  - CHAP
AU  - Dutta, Saurav Kumar; Balathil, Jithin Krishnan; Reddy, Paluchani Anirudh; Reddy, Annem Narayana
TI  - Design of a Torque Controller with Application to a Single Degree of Freedom Haptic Device
PY  - 2020
AB  - The main aim of the work is to design a torque controller which is a critical element in a haptic device. Since, the torque is related to the current in a DC motor, the paper comes up with a current control circuit by which current can be controlled across a branch in the circuit. From experiments, an empirical current–torque relationship is obtained for the DC motor using a constant current power source. The DC motor is then integrated with the current control circuit so as to generate a theoretical relation between the voltage of the current control circuit and the torque of the DC motor. This theoretical voltage–torque relationship is also validated experimentally. The working of the entire set-up with reference to a single degree of freedom haptic device is also explained. The proposed work would draw the attention of more researchers for the development of affordable haptic devices.
SP  - 551
EP  - 559
JF  - Lecture Notes in Mechanical Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-981-15-7779-6_50
ER  - 

TY  - NA
AU  - Kim, Daehwa; Park, Keunwoo; Lee, Geehyuk
TI  - CHI - AtaTouch: Robust Finger Pinch Detection for a VR Controller Using RF Return Loss
PY  - 2021
AB  - Handheld controllers are an essential part of VR systems. Modern sensing techniques enable them to track users’ finger movements to support natural interaction using hands. The sensing techniques, however, often fail to precisely determine whether two fingertips touch each other, which is important for the robust detection of a pinch gesture. To address this problem, we propose AtaTouch, which is a novel, robust sensing technique for detecting the closure of a finger pinch. It utilizes a change in the coupled impedance of an antenna and human fingers when the thumb and finger form a loop. We implemented a prototype controller in which AtaTouch detects the finger pinch of the grabbing hand. A user test with the prototype showed a finger-touch detection accuracy of 96.4%. Another user test with the scenarios of moving virtual blocks demonstrated low object-drop rate (2.75%) and false-pinch rate (4.40%). The results and feedback from the participants support the robustness and sensitivity of AtaTouch.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445442
ER  - 

TY  - NA
AU  - Jain, Harshika; Chen, Melinda; Collins, Alisha; Yao, Lining; Liu, Sunniva; Bobde, Riya; Liu, Cindy
TI  - Demonstrating DIY Methods for Actuating Morphing Matter
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3535196
ER  - 

TY  - BOOK
AU  - Da Chung Yi, NA; Chang, Kuan-Ning; Tai, Yun-Hsuan; Chen, I-Cheng; Hung, Yi-Ping
TI  - VR Workshops - Elastic-Move: Passive Haptic Device with Force Feedback for Virtual Reality Locomotion
PY  - 2020
AB  - With advances in virtual reality, users can experience in the immersive virtual environment by a head-mounted display. One of the most widely used interaction techniques in VR application is locomotion. Most VR applications currently use teleportation as a locomotion method. Because teleportation does not involve any visible translational motion, it can avoid VR sickness. However, teleportation still has some disadvantages. Compared to the joystick which uses dash to move in the virtual environment, teleportation would reduce user’s immersion and make them lose the sense of direction due to the lack of moving process. In this paper, we introduce the “Elastic-Move”, an approach for cost-effectively using passive force feedback to reduce VR sickness and allow users to maintain immersion and sense of direction. We propose two haptic devices with force feedback: “Elastic-Rope” and “Elastic-Box”. ElasticRope is consisting of elastic ropes and controllers, and Elastic-Box is consisting of eight springs and a steel frame. We conducted an experiment that compared the degree of VR sickness by simulator sickness questionnaire(SSQ) survey while moving in the virtual environment among Dash, Teleport, Elastic-Rope, and Elastic-Box. As a result, Elastic-Rope and Elastic-Box reduce the VR sickness of users. Besides, Elastic-Box can be transposed in numerous applications because it is a low-cost and versatile device. This work suggests that passive force feedback can effectively reduce VR sickness.
SP  - 40
EP  - 45
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00015
ER  - 

TY  - NA
AU  - Pareddy, Sujeath; Guo, Anhong; Bigham, Jeffrey P.
TI  - ASSETS - X-Ray: Screenshot Accessibility via Embedded Metadata
PY  - 2019
AB  - Screenshots are frequently shared on social media, via personal communications, and in academic papers. Unfortunately, existing screenshot tools strip away semantics useful for making the content accessible, leaving only pixels. For example, a screenshot of a table removes the structural information useful for conveying it. We quantify the scale of the problem via a study of academic papers, showing that a large number of images included in academic papers are screenshots, and validate this via qualitative interviews with researchers about their figure generation process. We then introduce X-Ray, a system that captures and embeds the semantics of the underlying content into images. Using the X-Ray screenshot tool, semantic information is captured and stored in the Exif data of the resulting image, allowing it to "tag along" as the image is shared and reposted. We demonstrate that our approach retains accessibility for screen reader users via a study with five blind participants. More generally, our approach suggests a method for embedding accessibility metadata into otherwise inaccessible formats, enabling them to retain the more accessible representations that are present at capture time.
SP  - 389
EP  - 395
JF  - The 21st International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3308561.3353808
ER  - 

TY  - NA
AU  - Langerak, Thomas; Zarate, Juan Jose; Lindlbauer, David; Holz, Christian; Hilliges, Otmar
TI  - UIST - Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback
PY  - 2020
AB  - We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.
SP  - 594
EP  - 606
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415589
ER  - 

TY  - NA
AU  - Yan, Litao; Kim, Miryung; Hartmann, Bjoern; Zhang, Tianyi; Glassman, Elena L.
TI  - Concept-Annotated Examples for Library Comparison
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545647
ER  - 

TY  - NA
AU  - Suzuki, Ryo; Nakayama, Ryosuke; Liu, Dan; Kakehi, Yasuaki; Gross, Mark D.; Leithinger, Daniel
TI  - Tangible and Embedded Interaction - LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces
PY  - 2020
AB  - Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.
SP  - 143
EP  - 151
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374941
ER  - 

TY  - NA
AU  - Lee, Woojin; Prasad, Ramkrishna; Je, Seungwoo; Kim, Yoonji; Oakley, Ian; Ashbrook, Daniel; Bianchi, Andrea
TI  - TEI - VirtualWire: Supporting Rapid Prototyping with Instant Reconfigurations of Wires in Breadboarded Circuits
PY  - 2021
AB  - Assembling circuits is a challenging and time consuming activity for novice makers, frequently resulting in incorrect placements of wires and components into breadboards. This results in errors that are difficult to identify and debug, and delays that hinder creating, exploring or reconfiguring circuit layouts. This paper presents VirtualWire, a tool that allows users to rapidly design and modify circuits in software and have these changes instantiated in real-time as electrical connections on a physical breadboard. To achieve this, VirtualWire dynamically translates circuit design files into physical connections inside a hardware switching matrix, which handles wiring across breadboard rows and to/from an embedded Arduino. The user can interactively test, tune, and share different circuit layouts for an Arduino shield, and once satisfied, can fabricate the circuit on a permanent substrate. Quantitative and qualitative user studies demonstrate that VirtualWire significantly reduces the time taken for (by 37%), and the number of errors made during (by 53%) circuit assembly, while also supporting users in creating readable, space-efficient and flexible layouts.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440623
ER  - 

TY  - NA
AU  - Chowdhury, Nabila; Latulipe, Celine; Young, James E.
TI  - CSCW Companion - Listening Together while Apart: Intergenerational Music Listening
PY  - 2021
AB  - Collaborative music listening can support intergenerational connectedness. We conducted an environmental scan of video conferencing and music listening platforms and cognitive walkthroughs to investigate how these platforms can support collaborative music listening and conversation between a grandparent and teen grandchild. Our results indicate that common video conferencing platforms not only lack a convenient way to share music, but typically block audio from music listening apps played on the same device, preventing sharing. Further, while music streaming platforms enable connected friends to share songs and others’ playlists, they do not provide a means for smooth synchronous conversing about the music. Thus, this highlights opportunities for technology improvement to support intergenerational relationships by enabling distributed, intergenerational co-listening of music.
SP  - 36
EP  - 39
JF  - Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462204.3481765
ER  - 

TY  - NA
AU  - Luo, Yiyue; Wu, Kui; Palacios, Tomas; Matusik, Wojciech
TI  - CHI - KnitUI: Fabricating Interactive and Sensing Textiles with Machine Knitting
PY  - 2021
AB  - With the recent interest in wearable electronics and smart garments, digital fabrication of sensing and interactive textiles is in increasing demand. Recently, advances in digital machine knitting offer opportunities for the programmable, rapid fabrication of soft, breathable textiles. In this paper, we present KnitUI, a novel, accessible machine-knitted user interface based on resistive pressure sensing. Employing conductive yarns and various machine knitting techniques, we computationally design and automatically fabricate the double-layered resistive sensing structures as well as the coupled conductive connection traces with minimal manual post-processing. We present an interactive design interface for users to customize KnitUI’s colors, sizes, positions, and shapes. After investigating design parameters for the optimized sensing and interactive performance, we demonstrate KnitUI as a portable, deformable, washable, and customizable interactive and sensing platform. It obtains diverse applications, including wearable user interfaces, tactile sensing wearables, and artificial robot skin.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445780
ER  - 

TY  - NA
AU  - Qiu, Jianing; Chen, Lipeng; Gu, Xiao; Lo, Frank P.-W.; Tsai, Ya-Yen; Sun, Jiankai; Liu, Jiaqi; Lo, Benny
TI  - Egocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion.
PY  - 2021
AB  - In this paper, we address the problem of forecasting the trajectory of an egocentric camera wearer (ego-person) in crowded spaces. The trajectory forecasting ability learned from the data of different camera wearers walking around in the real world can be transferred to assist visually impaired people in navigation, as well as to instill human navigation behaviours in mobile robots, enabling better human-robot interactions. To this end, a novel egocentric human trajectory forecasting dataset was constructed, containing real trajectories of people navigating in crowded spaces wearing a camera, as well as extracted rich contextual data. We extract and utilize three different modalities to forecast the trajectory of the camera wearer, i.e., his/her past trajectory, the past trajectories of nearby people, and the environment such as the scene semantics or the depth of the scene. A Transformer-based encoder-decoder neural network model, integrated with a novel cascaded cross-attention mechanism that fuses multiple modalities, has been designed to predict the future trajectory of the camera wearer. Extensive experiments have been conducted, and the results have shown that our model outperforms the state-of-the-art methods in egocentric human trajectory forecasting.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Herskovitz, Jaylin; Wu, Jason; White, Samuel; Pavel, Amy; Reyes, Gabriel; Guo, Anhong; Bigham, Jeffrey P.
TI  - ASSETS - Making Mobile Augmented Reality Applications Accessible
PY  - 2020
AB  - Augmented Reality (AR) technology creates new immersive experiences in entertainment, games, education, retail, and social media. AR content is often primarily visual and it is challenging to enable access to it non-visually due to the mix of virtual and real-world content. In this paper, we identify common constituent tasks in AR by analyzing existing mobile AR applications for iOS, and characterize the design space of tasks that require accessible alternatives. For each of the major task categories, we create prototype accessible alternatives that we evaluate in a study with 10 blind participants to explore their perceptions of accessible AR. Our study demonstrates that these prototypes make AR possible to use for blind users and reveals a number of insights to move forward. We believe our work sets forth not only exemplars for developers to create accessible AR applications, but also a roadmap for future research to make AR comprehensively accessible.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Herskovitz, Jaylin; Wu, Jason; White, Samuel; Pavel, Amy; Reyes, Gabriel; Guo, Anhong; Bigham, Jeffrey P.
TI  - Making Mobile Augmented Reality Applications Accessible
PY  - 2020
AB  - Augmented Reality (AR) technology creates new immersive experiences in entertainment, games, education, retail, and social media. AR content is often primarily visual and it is challenging to enable access to it non-visually due to the mix of virtual and real-world content. In this paper, we identify common constituent tasks in AR by analyzing existing mobile AR applications for iOS, and characterize the design space of tasks that require accessible alternatives. For each of the major task categories, we create prototype accessible alternatives that we evaluate in a study with 10 blind participants to explore their perceptions of accessible AR. Our study demonstrates that these prototypes make AR possible to use for blind users and reveals a number of insights to move forward. We believe our work sets forth not only exemplars for developers to create accessible AR applications, but also a roadmap for future research to make AR comprehensively accessible.
SP  - NA
EP  - NA
JF  - The 22nd International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3373625.3417006
ER  - 

TY  - NA
AU  - Tao, Yujie; Teng, Shan-Yuan; Lopes, Pedro
TI  - UIST - Altering Perceived Softness of Real Rigid Objects by Restricting Fingerpad Deformation
PY  - 2021
AB  - We propose a haptic device that alters the perceived softness of real rigid objects without requiring to instrument the objects. Instead, our haptic device works by restricting the user's fingerpad lateral deformation via a hollow frame that squeezes the sides of the fingerpad. This causes the fingerpad to become bulgier than it originally was—when users touch an object's surface with their now-restricted fingerpad, they feel the object to be softer than it is. To illustrate the extent of softness illusion induced by our device, touching the tip of a wooden chopstick will feel as soft as a rubber eraser. Our haptic device operates by pulling the hollow frame using a motor. Unlike most wearable haptic devices, which cover up the user's fingerpad to create force sensations, our device creates softness while leaving the center of the fingerpad free, which allows the users to feel most of the object they are interacting with. This makes our device a unique contribution to altering the softness of everyday objects, creating “buttons” by softening protrusions of existing appliances or tangibles, or even, altering the softness of handheld props for VR. Finally, we validated our device through two studies: (1) a psychophysics study showed that the device brings down the perceived softness of any object between 50A-90A to around 40A (on Shore A hardness scale); and (2) a user study demonstrated that participants preferred our device for interactive applications that leverage haptic props, such as making a VR prop feel softer or making a rigid 3D printed remote control feel softer on its button.
SP  - 985
EP  - 996
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474800
ER  - 

TY  - JOUR
AU  - Liu, Zongjian; He, Jieling; Feng, Jianjiang; Zhou, Jie
TI  - PrinType
PY  - 2022
AB  - <jats:p>We present PrinType, a fingerprint recognition based typing technique for virtual reality. Different regions of fingers covered by friction ridges are assigned to different keys (i.e. letters, numbers, punctuation, or functions). Once the thumb-worn fingerprint sensor touches a finger, the contact region (and its key) is recognized by matching the current image with registered templates. Using only a small sensor, PrinType turns each segment of all fingers into a touchable key. We designed keyboard layouts corresponding to three interaction sub-spaces: whole finger keyboard, fingertip keyboard, and single-handed keyboard. A 12-person user study was conducted to evaluate the performance of different strategies. Our user evaluation showed that participants achieved an average of 29.56, 32.38, and 34.22 WPM with 0.79%, 0.20%, and 0.21% not corrected error rate in the three strategies. In addition, we provided a detailed analysis of various micro metrics to further understand user performance and technical characteristics. Overall, PrinType is favored by users for its usability, efficiency, and novelty.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569491
ER  - 

TY  - NA
AU  - Myers, Brad A.
TI  - IUI4EUD: intelligent user interfaces for end-user development
PY  - 2020
AB  - End-User Developers program to meet some goal other than the code itself. This includes scientists, data analysts, and the general public when they write code. We have been working for many years on various ways to make end-user development more successful. In this talk, I will focus on two new projects where we are applying intelligent user interfaces to this long-standing challenge. In Sugilite, the user can teach an intelligent agent new skills interactively with the user interfaces of relevant smartphone apps through a combination of programming by example (PBE) and natural language instructions. For instance, a user can teach Sugilite how to order the cheaper car between Uber and Lyft, even though Sugilite has no access to their APIs, no knowledge about the task domain, and no understanding of the concept "cheap" in advance. Another project, called Verdant, is focusing on helping data scientists, including those using Machine Learning and AI, to do exploratory programming. Verdant supports micro-versioning in computational notebooks for understanding the difference among the output and code of different versions, backtracking, provenance of output to its code, and searching the history. A goal for Verdant is to intelligently organize and summarize the raw history data to help data scientists make effective choices from it.
SP  - NA
EP  - NA
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3380622
ER  - 

TY  - NA
AU  - Qian, Jing
TI  - UIST (Adjunct Volume) - Personalizing 3D Free-Hand Input for Intuitive Smartphone Augmented Reality Interactions
PY  - 2020
AB  - My research goal is to build practical and intuitive 3D free-hand interactions for augmented reality (AR) on smartphones and to explore relevant behavioral-data-driven interaction techniques. In service of that goal, I have developed two preliminary AR systems on the smartphone: a Portal-ble system powered by visual, auditory, and haptic feedback and a set of grabbing accommodations that allows the user's hand to interact with AR contents directly. I have also developed a Portalware system that incorporates a smartphone-wearable interaction scheme to expand visual feedback beyond the smartphone's display and have leveraged this system to improve 3D mid-air sketching. My next steps are to use behavioral data such as device motion, hand motion and user postures to create dynamic and personalized interaction systems that facilitate intuitive AR interaction. Ultimately, these individually-tailored systems unlock new interaction possibilities for the general public and expand the usage scenarios for smartphone AR applications.
SP  - 179
EP  - 182
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415806
ER  - 

TY  - NA
AU  - Bae, S. Sandra; Zheng, Clement; West, Mary Etta; Do, Ellen Yi-Luen; Huron, Samuel; Szafir, Danielle Albers
TI  - Making Data Tangible: A Cross-disciplinary Design Space for Data Physicalization
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501939
ER  - 

TY  - CONF
AU  - Lee, Kyungjun; Sato, Daisuke; Asakawa, Saki; Asakawa, Chieko; Kacorri, Hernisa
TI  - ASSETS - Accessing Passersby Proxemic Signals through a Head-Worn Camera: Opportunities and Limitations for the Blind
PY  - 2021
AB  - The spatial behavior of passersby can be critical to blind individuals to initiate interactions, preserve personal space, or practice social distancing during a pandemic. Among other use cases, wearable cameras employing computer vision can be used to extract proxemic signals of others and thus increase access to the spatial behavior of passersby for blind people. Analyzing data collected in a study with blind (N=10) and sighted (N=40) participants, we explore: (i) visual information on approaching passersby captured by a head-worn camera; (ii) pedestrian detection algorithms for extracting proxemic signals such as passerby presence, relative position, distance, and head pose; and (iii) opportunities and limitations of using wearable cameras for helping blind people access proxemics related to nearby people. Our observations and findings provide insights into dyadic behaviors for assistive pedestrian detection and lead to implications for the design of future head-worn cameras and interactions.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - , 
TI  - Mediating Technologies
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544574
ER  - 

TY  - NA
AU  - Albaugh, Lea; McCann, James; Hudson, Scott E.; Yao, Lining
TI  - CHI - Engineering Multifunctional Spacer Fabrics Through Machine Knitting
PY  - 2021
AB  - Machine knitting is an increasingly accessible fabrication technology for producing custom soft goods. However, recent machine knitting research has focused on knit shaping, or on adapting hand-knitting patterns. We explore a capability unique to machine knitting: producing multilayer spacer fabrics. These fabrics consist of two face layers connected by a monofilament filler yarn which gives the structure stiffness and volume. We show how to vary knit patterning and yarn parameters in spacer fabrics to produce tactile materials with embedded functionality for forming soft actuated mechanisms and sensors with tunable density, stiffness, material bias, and bristle properties. These soft mechanisms can be rapidly produced on a computationally-controlled v-bed knitting machine and integrated directly into soft objects.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445564
ER  - 

TY  - NA
AU  - Wang, Li-Yang; Han, Ping-Hsuan; Chan, Liwei
TI  - Push-Ups: Enhancing Kinesthetic Experience with Shape-Forming Devices on the Feet Soles
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501333
ER  - 

TY  - NA
AU  - Deng, Jialin; Olivier, Patrick; Mueller, Florian 'Floyd'
TI  - CHI Extended Abstracts - Design of Cyber Food:: Beginning to Understand Food as Computational Artifact
PY  - 2021
AB  - With a growing interest in HCI around food, there is a trend to combine computing technology and food to facilitate novel eating experiences. However, most current systems tend to superimpose the technology over food rather than consider food itself as a focal interaction material. This paper proposes a more direct computational food integration by conceptualizing the notion of “Cyber Food”, accentuating “food as computational artifact”, where food embodies digital computation that can be ultimately consumed and digested by the human body. With this work, we attempt to open a new pathway to enrich human-food interactions beyond the traditional boundaries between the physical (edible) and digital realms.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451687
ER  - 

TY  - NA
AU  - Wataru, Yamada; Manabe, Hiroyuki; Ikeda, Daizo; Rekimoto, Jun
TI  - SUI - RayGraphy: Aerial Volumetric Graphics Rendered Using Lasers in Fog
PY  - 2020
AB  - We present RayGraphy display technology that renders volumetric graphics by superimposing the trajectories of lights in indoor space filled with fog. Since the traditional FogScreen approach requires the shaping of a thin layer of fog, it can only show two-dimensional images in a narrow range that is close to the fog-emitting nozzle. Although a method that renders volumetric graphics with plasma generated using high-power laser was also proposed, its operation in a public space is considered quite dangerous. The proposed system mainly comprises dozens of laser projectors circularly arranged in a fog-filled space, and renders volumetric graphics in a fog by superimposing weak laser beams from the projectors. Compared to the conventional methods, this system employing weak laser beams and the non-shaped innocuous fog is more scalable and safer. We aim to construct a new spatial augmented reality platform where computer-generated images can be drawn directly in the real world. We implement a prototype that consists of 32 laser projectors and a fog machine. Moreover, we evaluate and discuss the system performance and characteristics in experiments.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418446
ER  - 

TY  - JOUR
AU  - Teng, Xiuxiu; Zhang, Min; Mujumdar, Arun S.
TI  - 4D printing: Recent advances and proposals in the food sector
PY  - 2021
AB  - NA
SP  - 349
EP  - 363
JF  - Trends in Food Science & Technology
VL  - 110
IS  - NA
PB  - 
DO  - 10.1016/j.tifs.2021.01.076
ER  - 

TY  - NA
AU  - Wang, Jingxian; Zhang, Junbo; Li, Ke; Pan, Chengfeng; Majidi, Carmel; Kumar, Swarun
TI  - IPSN - Locating Everyday Objects using NFC Textiles
PY  - 2021
AB  - This paper builds a Near-field Communication (NFC) based localization system that allows ordinary surfaces to locate surrounding objects with high accuracy in the near-field. While there is rich prior work on device-free localization using far-field wireless technologies, the near-field is less explored. Prior work in this space operates at extremely small ranges (a few centimeters), leading to designs that sense close proximity rather than location. We propose TextileSense, a near-field beamforming system which can track everyday objects made of conductive materials (e.g., a human hand) even if they are a few tens of centimeters away. We use multiple flexible NFC coil antennas embedded in ordinary and irregularly shaped surfaces we interact with in smart environments - furniture, carpets, etc. We design and fabricate specialized textile coils woven into the fabric of the furniture and easily hidden by acrylic paint. We then develop a near-field blind beamforming algorithm to efficiently detect surrounding objects, and use a data-driven approach to further infer their location. A detailed experimental evaluation of TextileSense shows an average accuracy of 3.5 cm in tracking the location of objects of interest within a few tens of centimeters from the furniture.
SP  - 15
EP  - 30
JF  - Proceedings of the 20th International Conference on Information Processing in Sensor Networks (co-located with CPS-IoT Week 2021)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3412382.3458254
ER  - 

TY  - NA
AU  - Palani, Srishti; Ding, Zijian; Nguyen, Austin Anhkhoi; Chuang, Andrew; MacNeil, Stephen; Dow, Steven
TI  - CHI - CoNotate: Suggesting Queries Based on Notes Promotes Knowledge Discovery
PY  - 2021
AB  - When exploring a new domain through web search, people often struggle to articulate queries because they lack domain-specific language and well-defined informational goals. Perhaps search tools rely too much on the query to understand what a searcher wants. Towards expanding this contextual understanding of a user during exploratory search, we introduce a novel system, CoNotate, which offers query suggestions based on analyzing the searcher’s notes and previous searches for patterns and gaps in information. To evaluate this approach, we conducted a within-subjects study where participants (n=38) conducted exploratory searches using a baseline system (standard web search) and the CoNotate system. The CoNotate approach helped searchers issue significantly more queries, and discover more terminology than standard web search. This work demonstrates how search can leverage user-generated content to help people get started when exploring complex, multi-faceted information spaces.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445618
ER  - 

TY  - NA
AU  - Cho, Hyungjun; Kim, Han-Jong; Lee, Jiyeon; Kim, Chang Min; Bae, Jinseong; Nam, Tek-Jin
TI  - Conference on Designing Interactive Systems - IoTIZER: A Versatile Mechanical Hijacking Device for Creating Internet of Old Things
PY  - 2021
AB  - Mechanical hijacking of physical interfaces is a cost-eff method for providing an Internet of Things (IoT) experience. However, existing mechanical hijacking devices (MHD) have limited applicability and usability. This pictorial introduces a research through design project on IoTIZER, which is easy to use and versatile MHD. We report on a design process from identifying a design space, conducting an iterative design and prototyping. We present the final design's hardware, software, usage scenario and implementation details. We also share lessons from a user study with eight households. The potential value of IoTIZER were appreciated as a versatile MHD. We discuss improvement areas of IoTIZER and implications for creating an IoT environment while preserving the existing conventions.
SP  - 90
EP  - 103
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3461996
ER  - 

TY  - NA
AU  - Harrington, Christina; Martin-Hammond, Aqueasha; Bray, Kirsten E
TI  - Examining Identity as a Variable of Health Technology Research for Older Adults: A Systematic Review
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517621
ER  - 

TY  - NA
AU  - Kuznetsov, Andrew; Chang, Joseph Chee; Hahn, Nathan; Rachatasumrit, Napol; Breneisen, Bradley; Coupland, Julina; Kittur, Aniket
TI  - Fuse: In-Situ Sensemaking Support in the Browser
PY  - 2022
AB  - People spend a significant amount of time trying to make sense of the internet, collecting content from a variety of sources and organizing it to make decisions and achieve their goals. While humans are able to fluidly iterate on collecting and organizing information in their minds, existing tools and approaches introduce significant friction into the process. We introduce Fuse, a browser extension that externalizes users' working memory by combining low-cost collection with lightweight organization of content in a compact card-based sidebar that is always available. Fuse helps users simultaneously extract key web content and structure it in a lightweight and visual way. We discuss how these affordances help users externalize more of their mental model into the system (e.g., saving, annotating, and structuring items) and support fast reviewing and resumption of task contexts. Our 22-month public deployment and follow-up interviews provide longitudinal insights into the structuring behaviors of real-world users conducting information foraging tasks.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545693
ER  - 

TY  - CHAP
AU  - , 
TI  - Authors Cited
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544580
ER  - 

TY  - NA
AU  - Miyatake, Mako; Narumi, Koya; Sekiya, Yuji; Kawahara, Yoshihiro
TI  - CHI - Flower Jelly Printer: Slit Injection Printing for Parametrically Designed Flower Jelly
PY  - 2021
AB  - Flower jellies, a delicate dessert in which a flower-shaped jelly floats inside another clear jelly, fascinate people with both their beauty and elaborate construction. In efforts to simplify the challenging fabrication and enrich the design space of this dessert, we present Flower Jelly Printer: a printing device and design software for digitally fabricating flower jellies. Our design software lets users play with parameters and preview the resulting forms until achieving their desired shapes. We also developed slit injection printing that directly injects colored jelly into a base jelly, and shared several design examples to show the breadth of design possibilities. Finally, the user study with novice and experienced users demonstrates that our system benefits creators of all experience levels by iterative design and precise fabrication. We hope to enable more people to design and create their own flower jellies while expanding access and the design space for digitally fabricated foods.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445346
ER  - 

TY  - BOOK
AU  - Wu, Jason; Reyes, Gabriel; White, Sam C.; Zhang, Xiaoyi; Bigham, Jeffrey P.
TI  - W4A - When can accessibility help?: an exploration of accessibility feature recommendation on mobile devices
PY  - 2021
AB  - Numerous accessibility features have been developed and included in consumer operating systems to provide people with a variety of disabilities additional ways to access computing devices. Unfortunately, many users, especially older adults who are more likely to experience ability changes, are not aware of these features or do not know which combination to use. In this paper, we first quantify this problem via a survey with 100 participants, demonstrating that very few people are aware of built-in accessibility features on their phones. These observations led us to investigate accessibility recommendation as a way to increase awareness and adoption. We developed four prototype recommenders that span different accessibility categories, which we used to collect insights from 20 older adults. Our work demonstrates the need to increase awareness of existing accessibility features on mobile devices, and shows that automated recommendation could help people find beneficial accessibility features.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Fischer, Michael H.; Campagna, Giovanni; Choi, Euirim; Lam, Monica S.
TI  - PLDI - DIY assistant: a multi-modal end-user programmable virtual assistant
PY  - 2021
AB  - While Alexa can perform over 100,000 skills, its capability covers only a fraction of what is possible on the web. Individuals need and want to automate a long tail of web-based tasks which often involve visiting different websites and require programming concepts such as function composition, conditional, and iterative evaluation. This paper presents DIYA (Do-It-Yourself Assistant), a new system that empowers users to create personalized web-based virtual assistant skills that require the full generality of composable control constructs, without having to learn a formal programming language. With DIYA, the user demonstrates their task of interest in the browser and issues a few simple voice commands, such as naming the skills and adding conditions on the action. DIYA turns these multi-modal specifications into voice-invocable skills written in the ThingTalk 2.0 programming language we designed for this purpose. DIYA is a prototype that works in the Chrome browser. Our user studies show that 81% of the proposed routines can be expressed using DIYA. DIYA is easy to learn, and 80% of users surveyed find DIYA useful.
SP  - 312
EP  - 327
JF  - Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3453483.3454046
ER  - 

TY  - NA
AU  - Arabi, Abul Al; Kim, Jeeeun
TI  - Augmenting Everyday Objects into Personal Robotic Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2022 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3550471.3564763
ER  - 

TY  - JOUR
AU  - Chen, Taizhou; Xu, Lantian; Zhu, Kening
TI  - FritzBot: A data-driven conversational agent for physical-computing system design
PY  - 2021
AB  - NA
SP  - 102699
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 155
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2021.102699
ER  - 

TY  - NA
AU  - Cruz, Christian Arzate; Igarashi, Takeo
TI  - Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 2021 IEEE Conference on Games (CoG)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cog52621.2021.9618999
ER  - 

TY  - NA
AU  - Lang, Florian; Grootjen, Jesse W.; Chuang, Lewis L.; Machulla, Tonja
TI  - IDeA: A Demonstration of a Mixed Reality System to Support Living with Central Field Loss
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3547521
ER  - 

TY  - NA
AU  - Zeng, Chong; Li, Weihua; Guo, Hualong; Wu, Tung-Lung; Kim, Dennis Bumsoo
TI  - QRS Companion - Development of A Real 3D Display System
PY  - 2020
AB  - This paper introduces a three-dimensional light field display system, which is composed of a high-speed projector, a directional scattering mirror, a circular stainless-steel bearing plate, a rotating shaft and a high-speed micro motor. The system reduces information redundancy and computational complexity by reconstructing the light intensity distribution of the observed object, thus generating a real three-dimensional suspended image. The experimental results show that the suspension three-dimensional image can be generated by properly adjusting the projection rate of the image and the rotation speed of the rotating mirror (i.e. the motor speed). The clarity and accuracy of a three-dimension display depending on the number of slices selected, meaning that the more slices can be projected per minute, the finer the three-dimension display. Finally, this study provides the basic parameter matching, which shows the feasibility of developing a real light field 3D display system with stable performance, strong portability, easy implementation, and low cost. In short, naked-eye 3D allows the observer to view objects from any angle and direction, without the need for any auxiliary tools (such as glasses, helmets, etc.).
SP  - 644
EP  - 649
JF  - 2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/qrs-c51114.2020.00109
ER  - 

TY  - NA
AU  - Kim, Myung Jin; Ryu, Neung; Chang, Wooje; Pahud, Michel; Sinclair, Mike; Bianchi, Andrea
TI  - SpinOcchio: Understanding Haptic-Visual Congruency of Skin-Slip in VR with a Dynamic Grip Controller
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517724
ER  - 

TY  - JOUR
AU  - Tao, Ye; Lee, Yi-Chin; Liu, Haolin; Zhang, Xiaoxiao; Cui, Jianxun; Catherine, Mondoa; Babaei, Mahnoush; Santillan, Jasio; Wang, Guanyun; Luo, Danli; Liu, Di; Yang, Humphrey; Do, Youngwook; Sun, Lingyun; Wen, Wang; Zhang, Teng; Yao, Lining
TI  - Morphing pasta and beyond.
PY  - 2021
AB  - Morphing structures are often engineered with stresses introduced into a flat sheet by leveraging structural anisotropy or compositional heterogeneity. Here, we identify a simple and universal diffusion-based mechanism to enable a transient morphing effect in structures with parametric surface grooves, which can be realized with a single material and fabricated using low-cost manufacturing methods (e.g., stamping, molding, and casting). We demonstrate from quantitative experiments and multiphysics simulations that parametric surface grooving can induce temporary asynchronous swelling or deswelling and can transform flat objects into designed, three-dimensional shapes. By tuning the grooving pattern, we can achieve both zero (e.g., helices) and nonzero (e.g., saddles) Gaussian curvature geometries. This mechanism allows us to demonstrate approaches that could improve the efficiency of certain food manufacturing processes and facilitate the sustainable packaging of food, for instance, by creating morphing pasta that can be flat-packed to reduce the air space in the packaging.
SP  - NA
EP  - NA
JF  - Science advances
VL  - 7
IS  - 19
PB  - 
DO  - 10.1126/sciadv.abf4098
ER  - 

TY  - NA
AU  - Jeong, Yunwoo; Kim, Han-Jong; Yun, Gyeongwon; Nam, Tek-Jin
TI  - UIST - WIKA: A Projected Augmented Reality Workbench for Interactive Kinetic Art
PY  - 2020
AB  - Iterative artistic exploration, mechanism building, and interaction programming are essential processes of prototyping interactive kinetic art (IKA). However, scattered tools and interwoven workflows across digital and physical worlds make the task difficult. We present WIKA, an integrated environment supporting the whole creation process of IKA in the form of a layered picture frame in a single workspace. A projected AR system with a mobile device efficiently makes an interactive tabletop. The projected information connected with physical components (e.g. sensors and motors) enables the programming and simulation on the workspace. Physical components are applied from the initial phase of prototyping using an AR plate, and this supports the iterative trial-and-error process by bridging the workflow. A user study shows that WIKA enabled non-experts to create diverse IKA with their ideas. A tangible interaction and projected information enable the iterative and rapid creation. The method that integrates the hardware and software in the physical environment can be applied to other prototyping tools that support the creation of interactive and kinetic elements.
SP  - 999
EP  - 1009
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415880
ER  - 

TY  - NA
AU  - Swearngin, Amanda; Iqbal, Shamsi T.; Poznanski, Victor; Encarnación, Mark J.; Bennett, Paul N.; Teevan, Jaime
TI  - CHI - Scraps: Enabling Mobile Capture, Contextualization, and Use of Document Resources
PY  - 2021
AB  - People often capture photos or notes from their phones to integrate later into a document. But current mobile capture tools can make this hard, with the captured information ending up fragmented and decontextualized. This paper explores how to help document authors capture, contextualize, and use document-related information. A survey of 66 information workers reveals that document-focused information capture differs from other types of mobile information capture, and that while people capture a broad range of information types while mobile, most document-related capture comes in the form of photos, notes, and bookmarks. Based on this survey we built Scraps, which consists of two parts: 1) a mobile app that makes it easy for people to capture and add context to information from their phone, and 2) a Word sidebar that helps them later link that information to a document on their desktop. In a field study with 11 information workers, we find that Scraps streamlined the process of capturing and using document-related information, and enabled people to focus on writing over integrating captured information.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445185
ER  - 

TY  - CHAP
AU  - Hommaru, Katsuya; Tanaka, Jiro
TI  - HCI (8) - Walking support for visually impaired using ar/mr and virtual braille block
PY  - 2020
AB  - In recent years, the number of visually impaired people has been increasing, and supporting the movement of visually impaired people will be indispensable for the future society. At present, for visually impaired people, a general-purpose walking support is a combination of a braille block and a white cane; however, it is not enough. In this research, we provide a system that expands the above combination by utilizing the technologies of a see-through head-mounted display (HMD) and Augmented Reality/Mixed Reality (AR/MR). Specifically, utilizing the features of AR/MR, a virtual 3D object is projected as a braille block (virtual braille block) on the walking surface of visually impaired people via an HMD. Subsequently, when the white cane waved by visually impaired people and the virtual braille block intersect (collision), the guidance of forwarding, left, right, and turn is returned as feedback by voice and vibration. By realizing these, the goal is to provide a system that enables visually impaired people to move freely in the walking space.
SP  - 336
EP  - 354
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49282-3_24
ER  - 

TY  - NA
AU  - Pointner, Andreas; Preindl, Thomas; Mlakar, Sara; Aigner, Roland; Haberfellner, Mira Alida; Haller, Michael
TI  - Knitted Force Sensors
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558656
ER  - 

TY  - NA
AU  - Miyatake, Mako; Narumi, Koya; Sekiya, Yuji; Kawahara, Yoshihiro
TI  - CHI Extended Abstracts - Demonstrating Flower Jelly Printer for Parametrically Designed Flower Jelly
PY  - 2021
AB  - Flower jellies, a delicate dessert in which a flower-shaped jelly floats inside another clear jelly, fascinate people with both their beauty and elaborate construction. In efforts to simplify the challenging fabrication and enrich the design space of this dessert, we present Flower Jelly Printer: a printing device and design software for digitally fabricating flower jellies. Our design software lets users play with parameters and preview the resulting forms until achieving their desired shapes. We also developed slit injection printing that directly injects colored jelly into a base jelly, and shared several design examples to show the breadth of design possibilities. We hope to enable more people to design and create their own flower jellies while expanding access and the design space for digitally fabricated foods.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451541
ER  - 

TY  - CHAP
AU  - Hopkins, Torin; Bae, S. Sandra; Uhr, Julia; Banic, Amy; Zheng, Clement; Yi-Luen, Ellen
TI  - Handbook of Smart Cities - User Interfaces in Smart Cities
PY  - 2021
AB  - NA
SP  - 687
EP  - 719
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gonzalez-Franco, Mar; Sinclair, Mike; Ofek, Eyal
TI  - SAP - Asymmetry of Grasp in Haptic Perception
PY  - 2020
AB  - In this paper we present evidence that human perception of grasp might be most dependent on the information retrieved during the inward latch rather than the release of objects. This research is motivated by a number of haptic simulations and devices and grounded in perception science. We ran a user study (n=12) with two devices one capable of delivering compliant simulations for both grip and release (CLAW), i.e. symmetric device; the other only capable of delivering adaptive grip simulations (CapstanCrunch), i.e. asymmetric device. We fund that both performed similarly well for realism scores in a grasping task with objects of different stiffness. That similar performance was despite CapstanCrunch release was delivered by a constant spring independently of the compliance of the object. Our results show preliminary evidence that when simulating haptic grasp the release might be less important. And we propose a new theory of asymmetry of grasp in haptic perception.
SP  - NA
EP  - NA
JF  - ACM Symposium on Applied Perception 2020
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385955.3407934
ER  - 

TY  - JOUR
AU  - Chen, Xiaohuan; Zhang, Min; Teng, Xiuxiu; Mujumdar, Arun S.
TI  - Recent Progress in Modeling 3D/4D Printing of Foods
PY  - 2021
AB  - NA
SP  - 120
EP  - 133
JF  - Food Engineering Reviews
VL  - 14
IS  - 1
PB  - 
DO  - 10.1007/s12393-021-09297-6
ER  - 

TY  - JOUR
AU  - Jiang, Haiyan; Weng, Dongdong; Dongye, Xiaonuo; Liu, Yue
TI  - PinchText: One-Handed Text Entry Technique Combining Pinch Gestures and Hand Positions for Head-Mounted Displays
PY  - 2022
AB  - NA
SP  - 1
EP  - 17
JF  - International Journal of Human–Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/10447318.2022.2115333
ER  - 

TY  - CHAP
AU  - , 
TI  - TEI in the Wild
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544571
ER  - 

TY  - NA
AU  - Siu, Alexa F.; Sinclair, Mike; Kovacs, Robert; Ofek, Eyal; Holz, Christian; Cutrell, Edward
TI  - CHI - Virtual Reality Without Vision: A Haptic and Auditory White Cane to Navigate Complex Virtual Worlds
PY  - 2020
AB  - Current Virtual Reality (VR) technologies focus on rendering visuospatial effects, and thus are inaccessible for blind or low vision users. We examine the use of a novel white cane controller that enables navigation without vision of large virtual environments with complex architecture, such as winding paths and occluding walls and doors. The cane controller employs a lightweight three-axis brake mechanism to provide large-scale shape of virtual objects. The multiple degrees-of-freedom enables users to adapt the controller to their preferred techniques and grip. In addition, surface textures are rendered with a voice coil actuator based on contact vibrations; and spatialized audio is determined based on the progression of sound through the geometry around the user. We design a scavenger hunt game that demonstrates how our device enables blind users to navigate a complex virtual environment. Seven out of eight users were able to successfully navigate the virtual room (6x6m) to locate targets while avoiding collisions. We conclude with design consideration on creating immersive non-visual VR experiences based on user preferences for cane techniques, and cane material properties.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376353
ER  - 

TY  - NA
AU  - Elbehery, Mostafa; Weidner, Florian; Broll, Wolfgang
TI  - MUM - Haptic Space: The Effect of a Rigid Hand Representation on Presence when Interacting with Passive Haptics Controls in VR
PY  - 2020
AB  - Many virtual reality (VR) applications rely on passive haptics where virtual objects have a real counterpart that provides tactile feedback. In addition to that, many VR applications do not provide accurate hand representations, especially when there is a high chance of occlusion as this makes vision-based tracking problematic. Hence, we investigated how a simple hand representation affects user experience and presence when interacting with passive haptic controls in a virtual environment. We report on a between-subject user study where N = 45 participants experienced one of three conditions (no hands at all, hands represented as a rigid 3D model, and hands represented as a rigid 3D model with a snapping mechanism). Our results indicate that a simple hand representation using a 3D model of hands paired with a snapping mechanism significantly increases presence and user experience. That indicates that this simple and low-cost technique is effective to improve the VE as a whole. This, in return, provides a chance for improvement for many VR applications with passive haptics.
SP  - 245
EP  - 253
JF  - 19th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3428361.3428388
ER  - 

TY  - NA
AU  - Ma, Hua; Yamaoka, Junichi
TI  - Smart Textile Using 3D Printed Conductive Sequins
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505577
ER  - 

TY  - NA
AU  - Zhao, Luyang; Wu, Yijia; Blanchet, Julien; Perroni-Scharf, Maxine; Huang, Xiaonan; Booth, Joran W.; Kramer-Bottiglio, Rebecca; Balkcom, Devin
TI  - Soft Lattice Modules that Behave Independently and Collectively.
PY  - 2021
AB  - Natural systems integrate the work of many sub-units (cells) toward a large-scale unified goal (morphological and behavioral), which can counteract the effects of unexpected experiences, damage, or simply changes in tasks demands. In this paper, we exploit the opportunities presented by soft, modular, and tensegrity robots to introduce soft lattice modules that parallel the sub-units seen in biological systems. The soft lattice modules are comprised of 3D printed plastic "skeletons", linear contracting shape memory alloy spring actuators, and permanent magnets that enable adhesion between modules. The soft lattice modules are capable of independent locomotion, and can also join with other modules to achieve collective, self-assembled, larger scale tasks such as collective locomotion and moving an object across the surface of the lattice assembly. This work represents a preliminary step toward soft modular systems capable of independent and collective behaviors, and provide a platform for future studies on distributed control.
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Takahashi, Ryo; Fukumoto, Masaaki; Han, Changyo; Sasatani, Takuya; Narusue, Yoshiaki; Kawahara, Yoshihiro
TI  - UIST - TelemetRing: A Batteryless and Wireless Ring-shaped Keyboard using Passive Inductive Telemetry
PY  - 2020
AB  - TelemetRing is a batteryless and wireless ring-shaped keyboard that supports command and text entry in daily lives by detecting finger typing on various surfaces. The proposed inductive telemetry approach eliminates bulky batteries or capacitors from the ring part. Each ring consists of a sensor coil (the ring part itself), 1-DoF piezoelectric accelerometer, and varactor diode; moreover, it has different resonant frequencies. Typing shocks slightly shift the resonant frequency, and these are detected by a wrist-mounted readout coil. 5-bit chord keyboard is realized by attaching five sensor rings on five fingers. Our evaluation shows that the prototype achieved the tiny (6 g, 3.5 cm^3) ring sensor and 89.7% of typing detection ratio.
SP  - 1161
EP  - 1168
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415873
ER  - 

TY  - NA
AU  - Ma, Hua; Yamaoka, Junichi
TI  - SenSequins: Smart Textile Using 3D Printed Conductive Sequins
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545688
ER  - 

TY  - NA
AU  - Esmaeili, Shaghayegh; Benda, Brett; Ragan, Eric D.
TI  - VR - Detection of Scaled Hand Interactions in Virtual Reality: The Effects of Motion Direction and Task Complexity
PY  - 2020
AB  - In virtual reality (VR), natural physical hand interaction allows users to interact with virtual content using physical gestures. While the most straightforward use of tracked hand motion maintains a one-to-one mapping between the physical and virtual world, some cases might benefit from changing this mapping through scaled or redirected interactions that modify the mapping between user’s physical movements and the magnitude of corresponding virtual movements. However, large deviations in interaction fidelity may potentially provide distractions or a loss of perceived realism. Therefore, it is important to know the extent to which remapping techniques can be applied to scaled interactions in VR without users detecting the difference. In this paper, we extend prior research on redirected hand techniques by investigating user perception of scaled hand movements and estimating detection thresholds for different types of hand motion in VR. We conducted two experiments with a two-alternative forced-choice (2AFC) design to estimate the detection thresholds of remapped interaction. The first experiment tested the perception of motion scaling for simple hand movements, and the second experiment involved more complex reaching motions in a cognitively demanding game scenario. We present estimated detection thresholds for scale values that can be applied to virtual hand movements without users noticing the difference. Our findings show that detection thresholds differ significantly based on the type of hand movement (horizontal, vertical, and depth).
SP  - 453
EP  - 462
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581285352835
ER  - 

TY  - JOUR
AU  - Zhang, Li; Liu, Yizhe; Bai, Huidong; Zou, Qianyuan; Chang, Zhuang; He, Weiping; Wang, Shuxia; Billinghurst, Mark
TI  - Robot-enabled tangible virtual assembly with coordinated midair object placement
PY  - 2023
AB  - NA
SP  - 102434
EP  - 102434
JF  - Robotics and Computer-Integrated Manufacturing
VL  - 79
IS  - NA
PB  - 
DO  - 10.1016/j.rcim.2022.102434
ER  - 

TY  - JOUR
AU  - Alhafnawi, Merihan A H M; Hauert, Sabine; O'Dowd, Paul
TI  - Self-Organised Saliency Detection and Representation in Robot Swarms
PY  - 2021
AB  - Self-assembly of shapes using robot swarms has applications spanning architecture, functional materials, and art. Typically, the amount of robotic material needed to represent the shape is relative to the shape's area, with robots filling in the whole shape. To save in robotic material, we explore the ability to automatically aggregate on the frontier of shapes, essentially forming outlines, using a self-organised decentralised mechanism. An image is projected on top of a swarm of robots, and the robots sense the light colour and communicate this information to their neighbours to detect salient features. Results in simulation show the swarm can detect salient features, draw with different line thicknesses, adapt to changes in the feature over time, and scale to 300 robots in reality and up to 1000 robots in simulation. We then show a dynamic performance where robots continuously reconfigure in simulation to form a “video”.
SP  - 1487
EP  - 1494
JF  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 2
PB  - 
DO  - 10.1109/lra.2021.3057567
ER  - 

TY  - NA
AU  - Jain, Harshika; Collins, Alisha; Chen, Melinda; Yao, Lining
TI  - Morphing Matter for Girls
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3535211
ER  - 

TY  - NA
AU  - Choi, Dongjin; Evensen, Sara; Demiralp, Çağatay; Hruschka, Estevam R.
TI  - TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration
PY  - 2021
AB  - Despite rapid developments in the field of machine learning research, collecting high-quality labels for supervised learning remains a bottleneck for many applications. This difficulty is exacerbated by the fact that state-of-the-art models for NLP tasks are becoming deeper and more complex, often increasing the amount of training data required even for fine-tuning. Weak supervision methods, including data programming, address this problem and reduce the cost of label collection by using noisy label sources for supervision. However, until recently, data programming was only accessible to users who knew how to program. To bridge this gap, the Data Programming by Demonstration framework was proposed to facilitate the automatic creation of labeling functions based on a few examples labeled by a domain expert. This framework has proven successful for generating high-accuracy labeling models for document classification. In this work, we extend the DPBD framework to span-level annotation tasks, arguably one of the most time-consuming NLP labeling tasks. We built a novel tool, TagRuler, that makes it easy for annotators to build span-level labeling functions without programming and encourages them to explore trade-offs between different labeling models and active learning strategies. We empirically demonstrated that an annotator could achieve a higher F1 score using the proposed tool compared to manual labeling for different span-level annotation tasks.
SP  - NA
EP  - NA
JF  - arXiv: Computation and Language
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Yang, Tae-Heon; Son, Hyungki; Byeon, Sangkyu; Gil, Hyunjae; Hwang, Inwook; Jo, Gwanghyun; Choi, Seungmoon; Kim, Sang-Youn; Kim, Jin Ryong
TI  - Magnetorheological Fluid Haptic Shoes for Walking in VR
PY  - 2021
AB  - In this article, present RealWalk, a pair of haptic shoes for HMD-based VR, designed to create realistic sensations of ground surface deformation, and texture using Magnetorheological fluid (MR fluid). RealWalk offers a novel interaction scheme through the physical interaction between the shoes, and the ground surfaces while walking in VR. Each shoe consists of two MR fluid actuators, an insole pressure sensor, and a foot position tracker. The MR fluid actuators are designed in the form of multi-stacked disc structure with a long flow path to maximize the flow resistance. With changing the magnetic field intensity in MR fluid actuators based on the ground material in the virtual scene, the viscosity of MR fluid is changed accordingly. When a user steps on the ground with the shoes, the two MR fluid actuators are pressed down, creating a variety of ground material deformation such as snow, mud, and dry sand. We built an interactive VR application, and compared RealWalk with vibrotactile-based haptic shoes in four different VR scenes: grass, sand, mud, and snow. We report that, compared to vibrotactile-haptic shoes, RealWalk provides higher ratings in all scenes for discrimination, realism, and satisfaction. We also report qualitative user feedback for their experiences.
SP  - 83
EP  - 94
JF  - IEEE transactions on haptics
VL  - 14
IS  - 1
PB  - 
DO  - 10.1109/toh.2020.3017099
ER  - 

TY  - NA
AU  - Chung, SeungA; Joh, Hwayeon; Lee, Eunji; Oh, Uran
TI  - ISMAR Adjunct - PanoCue: An Efficient Visual Cue With a Omnidirectional Panoramic View for Finding a Target in 3D Space
PY  - 2021
AB  - Finding a specific object is one of the basic tasks in both physical and virtual environments. However, visually scanning large scenes can be inefficient and frustrating, especially when the target is outside of one’s field of view. As a result, there have been several studies for supporting target finding tasks in a three-dimensional environment with various visual cues. However, most studies focused on conveying either the relative position or the distance of the target to users. In this study, we propose PanoCue, which is a visual cue that overlays a panorama view of the surroundings at the center of the user’s field of view for conveying both the position and the distance of a target in a 3D space with respect to the user’s location and head orientation. For evaluation, we conducted a user study with 20 participants where they were asked to find a target under different visual cue conditions: PanoCue, Radar, Arrow, and a baseline without cues. As a result, we found that the presence of visual cues improves task performance and that PanoCue significantly reduces the travel distance. Findings also showed that our feedback design received positive ratings in terms of easiness, fatigue, and satisfaction.
SP  - 218
EP  - 223
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct54149.2021.00052
ER  - 

TY  - NA
AU  - Leen, Danny; Peek, Nadya; Ramakers, Raf
TI  - UIST - LamiFold: Fabricating Objects with Integrated Mechanisms Using a Laser cutter Lamination Workflow
PY  - 2020
AB  - We present LamiFold, a novel design and fabrication workflow for making functional mechanical objects using a laser cutter. Objects fabricated with LamiFold embed advanced rotary, linear, and chained mechanisms, including linkages that support fine-tuning and locking position. Laser cutting such mechanisms without LamiFold requires designing for and embedding off-the-shelf parts such as springs, bolts, and axles for gears. The key to laser cutting our functional mechanisms is the selective cutting and gluing of stacks of sheet material. Designing mechanisms for this workflow is non-trivial, therefore we contribute a set of mechanical primitives that are compatible with our lamination workflow and can be combined to realize advanced mechanical systems. Our software design environment facilitates the process of inserting and composing our mechanical primitives and realizing functional laser-cut objects.
SP  - 304
EP  - 316
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415885
ER  - 

TY  - NA
AU  - Saenz, Juan Pablo; De Russis, Luigi
TI  - Dear Diary: On Documenting Novices' Development Process
PY  - 2022
AB  - In the development projects implemented by novices, the usefulness of the documentation in the form of comments on the final working code is minimal to guide future implementations. Such documentation does not account for novices&#x2019; development process, including their choices, the errors they faced, the solutions they found, the sources they consulted, the lessons learned, and the advice to remember or give to someone else. Indeed, novices do not usually rely on their documentation to keep track of the successes and errors they find during the development process. Nevertheless, if enabled to capture various moments of the process seamlessly, novices can produce documentation that has the potential to become a valuable asset for them and other developers. This paper presents Dear Diary, a tool to support non-expert programmers in straightforwardly creating documentation artifacts directly from the IDE.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9833003
ER  - 

TY  - NA
AU  - Jiang, Ying; Zhang, Congyi; Fu, Hongbo; Cannavo, Alberto; Lamberti, Fabrizio; Lau, Henry Y. K.; Wang, Wenping
TI  - CHI - HandPainter - 3D Sketching in VR with Hand-based Physical Proxy
PY  - 2021
AB  - 3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index finger of the other hand as a 3D pen. To this end, we first perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the findings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the effectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445302
ER  - 

TY  - BOOK
AU  - Paolocci, Gianluca; Baldi, Tommaso Lisini; Barcelli, Davide; Prattichizzo, Domenico
TI  - VR Workshops - Combining Wristband Display and Wearable Haptics for Augmented Reality
PY  - 2020
AB  - Taking advantages of widely distributed hardware such as smartphones and tablets, Mobile Augmented Reality (MAR) market is rapidly growing. Major improvements can be envisioned in increasing the realism of virtual interaction and providing multimodal experiences. We propose a novel system prototype that locates the display on the forearm using a rigid support to avoid constraints due to hand-holding, and is equipped with hand tracking and cutaneous feedback. The hand tracking enables the manipulation of virtual objects, while the haptic rendering enhances the user’s perception of the virtual entities. The experimental setup has been tested by ten participants, that expressed their impressions about usability and functionality of the wrist-mounted system w.r.t. the traditional hand-held condition. Subjects’ personal evaluations suggest that the AR experience provided by the wrist-based approach is more engaging and immersive.
SP  - 632
EP  - 633
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00167
ER  - 

TY  - JOUR
AU  - Chen, Yang; Fennedy, Katherine; Fogel, Anna; Zhao, Shengdong; Zhang, Chao; Liu, Lijuan; Yen, Chingchiuan
TI  - SSpoon
PY  - 2022
AB  - <jats:p>One key strategy of combating obesity is to slow down eating; however, this is difficult to achieve due to people's habitual nature. In this paper, we explored the feasibility of incorporating shape-changing interface into an eating spoon to directly intervene in undesirable eating behaviour. First, we investigated the optimal dimension (i.e., Z-depth) and ideal range of spoon transformation for different food forms that could affect bite size while maintaining usability. Those findings allowed the development of SSpoon prototype through a series of design explorations that are optimised for user's adoption. Then, we applied two shape-changing strategies: instant transformations based on food forms and subtle transformations based on food intake) and examined in two comparative studies involving a full course meal using Wizard-of-Oz approach. The results indicated that SSpoon could achieve comparable effects to a small spoon (5ml) in reducing eating rate by 13.7-16.1% and food consumption by 4.4-4.6%, while retaining similar user satisfaction as a normal eating spoon (10ml). These results demonstrate the feasibility of a shape-changing eating utensil as a promising alternative to combat the growing concern of obesity.</jats:p>
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550312
ER  - 

TY  - NA
AU  - Nishihara, Yumi; Kakehi, Yasuaki
TI  - magashi: Fabrication of Shape-Changing Edible Structures by Extrusion-Based Printing and Baking
PY  - 2021
AB  - In the field of HCI, methods for creating morphing food have required the process of making sheets and cutting out the shapes by hand. In order to simplify the fabrication process and further expand the customization and design of shape-changing foods, we have developed magashi, a new method which enables users to design food in different shapes with intricate patterns by utilizing extrusion-based food printing and baking. With our technique, the printed food with mesh patterns can form into curved structures when baked. In order to realize this method, the material's ingredients, printing patterns, and thickness were optimized to control the drying process of the material, consequently inducing it to bend upward or downward during baking. We also propose several applications for creating shape-changing food and cutlery in different shapes, patterns and curvatures.
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450741.3465388
ER  - 

TY  - NA
AU  - Raeburn, Gideon; Tokarchuk, Laurissa
TI  - ISMAR - Varying user agency and interaction opportunities in a home mobile augmented virtuality story
PY  - 2021
AB  - New opportunities for immersive storytelling experiences have arrived through the technology in mobile phones, including the ability to overlay or register digital content on a user’s real world surroundings, to greater immerse the user in the world of the story. This raises questions around the methods and freedom to interact with the digital elements, that will lead to a more immersive and engaging experience. To investigate these areas the Augmented Virtuality (AV) mobile phone application Home Story was developed for iOS devices. It allows a user to move and interact with objects in a virtual environment displayed on their phone, by physically moving in the real world, completing particular actions to progress a story. A mixed methods study with Home Story either guided participants to the next interaction, or offered them increased agency to choose what object to interact with next. Virtual objects could also be interacted with in one of three ways; imagining the interaction, an embodied interaction using the user’s free hand, or a virtual interaction performed on the phone’s touchscreen. Similar levels of immersion were recorded across both study conditions suggesting both can be effective, though highlighting different issues in each case. The embodied free hand interactions proved particularly memorable, though further work is required to improve their implementation, arising from their novelty and lack of familiarity.
SP  - 347
EP  - 356
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00051
ER  - 

TY  - NA
AU  - Devrio, Nathan; Harrison, Chris
TI  - DiscoBand: Multiview Depth-Sensing Smartwatch Strap for Hand, Body and Environment Tracking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545634
ER  - 

TY  - JOUR
AU  - Kourtesis, Panagiotis; Argelaguet, Ferran; Vizcay, Sebastian; Marchal, Maud; Pacchierotti, Claudio
TI  - Electrotactile Feedback Applications for Hand and Arm Interactions: A Systematic Review, Meta-Analysis, and Future Directions.
PY  - 2022
AB  - Haptic feedback is critical in a broad range of human-machine/computer-interaction applications. However, the high cost and low portability/wearability of haptic devices remain unresolved issues, severely limiting the adoption of this otherwise promising technology. Electrotactile interfaces have the advantage of being more portable and wearable due to their reduced actuators' size, as well as their lower power consumption and manufacturing cost. The applications of electrotactile feedback have been explored in human-computer interaction and human-machine-interaction for facilitating hand-based interactions in applications, such as prosthetics, virtual reality, robotic teleoperation, surface haptics, portable devices, and rehabilitation. This article presents a technological overview of electrotactile feedback, as well a systematic review and meta-analysis of its applications for hand-based interactions. We discuss the different electrotactile systems according to the type of application. We also discuss over a quantitative congregation of the findings, to offer a high-level overview into the state-of-art and suggest future directions. Electrotactile feedback systems showed increased portability/wearability, and they were successful in rendering and/or augmenting most tactile sensations, eliciting perceptual processes, and improving performance in many scenarios. However, knowledge gaps (e.g., embodiment), technical (e.g., recurrent calibration, electrodes' durability) and methodological (e.g., sample size) drawbacks were detected, which should be addressed in future studies.
SP  - 479
EP  - 496
JF  - IEEE transactions on haptics
VL  - 15
IS  - 3
PB  - 
DO  - 10.1109/toh.2022.3189866
ER  - 

TY  - BOOK
AU  - Choi, Dongjin; Evensen, Sara; Demiralp, Çağatay; Hruschka, Estevam R.
TI  - WWW (Companion Volume) - TagRuler: Interactive Tool for Span-Level Data Programming by Demonstration
PY  - 2021
AB  - Despite rapid developments in the field of machine learning research, collecting high quality labels for supervised learning remains a bottleneck for many applications. This difficulty is exacerbated by the fact that state-of-the art models for NLP tasks are becoming deeper and more complex, often increasing the amount of training data required even for fine-tuning. Weak supervision methods, including data programming, address this problem and reduce the cost of label collection by using noisy label sources for supervision. However until recently, data programming was only accessible to users who knew how to program. In order to bridge this gap, the Data Programming by Demonstration framework was proposed to facilitate the automatic creation of labeling functions based on a few examples labeled by a domain expert. This framework has proven successful for generating high accuracy labeling models for document classification. In this work, we extend the DPBD framework to span-level annotation tasks, arguably one of the most time consuming NLP labeling tasks. We built a novel tool, TagRuler, that makes it easy for annotators to build span-level labeling functions without programming and encourages them to explore trade-offs between different labeling models and active learning strategies. We empirically demonstrated that an annotator could achieve a higher F1 score using the proposed tool compared to manual labeling for different span-level annotation tasks.
SP  - 673
EP  - 677
JF  - Companion Proceedings of the Web Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3442442.3458602
ER  - 

TY  - NA
AU  - Mott, Martez E.; Tang, John C.; Kane, Shaun K.; Cutrell, Edward; Morris, Meredith Ringel
TI  - ASSETS - “I just went into it assuming that I wouldn't be able to have the full experience”: Understanding the Accessibility of Virtual Reality for People with Limited Mobility
PY  - 2020
AB  - Virtual reality (VR) has the potential to transform many aspects of our daily lives, including work, entertainment, communication, and education. However, there has been little research into understanding the usability of VR for people with mobility limitations. In this paper, we present the results of an exploration to understand the accessibility of VR for people with limited mobility. We conducted semi-structured interviews with 16 people with limited mobility about their thoughts on, and experiences with, VR systems. We identified 7 barriers related to the physical accessibility of VR devices that people with limited mobility might encounter, ranging from the initial setup of a VR system to keeping VR controllers in view of cameras embedded in VR headsets. We also elicited potential improvements to VR systems that would address some accessibility concerns. Based on our findings, we discuss the importance of considering the abilities of people with limited mobility when designing VR systems, as the abilities of many participants did not match the assumptions embedded in the design of current VR systems.
SP  - NA
EP  - NA
JF  - The 22nd International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3373625.3416998
ER  - 

TY  - NA
AU  - Xu, Frank F.; Vasilescu, Bogdan; Neubig, Graham
TI  - In-IDE Code Generation from Natural Language: Promise and Challenges.
PY  - 2021
AB  - A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. We perform the first comprehensive investigation of the promise and challenges of using such technology inside the IDE, asking "at the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?" We first develop a plugin for the IDE that implements a hybrid of code generation and code retrieval functionality, and orchestrate virtual environments to enable collection of many user events. We ask developers with various backgrounds to complete 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Analysis identifies several pain points that could improve the effectiveness of future machine learning based code generation/retrieval developer assistants, and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies and development of better models.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Takada, Hideaki
TI  - 360-degree Personal Glassless 3D Display using Retroreflection and Narrow-angle Diffusion
PY  - 2022
AB  - We proposed a method to realize a high-quality 360-degree personal 3D display with a simple optical structure without 3D glasses. The proposed method applies retroreflection and narrow-angle diffusion, which is advantageous for image quality and structure. We have shown the feasibility of our method from projection verification and luminance distribution measurement using a prototype.
SP  - NA
EP  - NA
JF  - 2022 IEEE Industry Applications Society Annual Meeting (IAS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ias54023.2022.9940011
ER  - 

TY  - NA
AU  - Kim, Jin Hee (Heather); Patil, Shreyas Dilip; Matson, Sarina; Conroy, Melissa; Kao, Cindy Hsin-Liu
TI  - KnitSkin: Machine-Knitted Scaled Skin for Locomotion
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502142
ER  - 

TY  - NA
AU  - Triantafyllidis, Eleftherios; McGreavy, Christopher; Gu, Jiacheng; Li, Zhibin
TI  - Multimodal Interfaces for Effective Teleoperation.
PY  - 2020
AB  - Research in multi-modal interfaces aims to provide solutions to immersion and increase overall human performance. A promising direction is combining auditory, visual and haptic interaction between the user and the simulated environment. However, no extensive comparisons exist to show how combining audiovisuohaptic interfaces affects human perception reflected on task performance. Our paper explores this idea. We present a thorough, full-factorial comparison of how all combinations of audio, visual and haptic interfaces affect performance during manipulation. We evaluate how each interface combination affects performance in a study (N=25) consisting of manipulating tasks of varying difficulty. Performance is assessed using both subjective, assessing cognitive workload and system usability, and objective measurements, incorporating time and spatial accuracy-based metrics. Results show that regardless of task complexity, using stereoscopic-vision with the VRHMD increased performance across all measurements by 40% compared to monocular-vision from the display monitor. Using haptic feedback improved outcomes by 10% and auditory feedback accounted for approximately 5% improvement.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kang, Bumsoo; Kang, Seungwoo; Hwang, Inseok
TI  - CHI - MomentMeld: AI-augmented Mobile Photographic Memento towards Mutually Stimulatory Inter-generational Interaction
PY  - 2021
AB  - Aging often comes with declining social interaction, a known adversarial factor impacting the life satisfaction of senior population. Such decline appears even in family–a permanent social circle, as their adult children eventually go independent. We present MomentMeld, an AI-powered, cloud-backed mobile application that blends with everyday routine and naturally encourages rich and frequent inter-generational interactions in a family, especially those between the senior generation and their adult children. Firstly, we design a photographic interaction aid called mutually stimulatory memento, which is a cross-generational juxtaposition of semantically related photos to bring natural arousal of context-specific inter-generational empathy and reminiscence. Secondly, we build comprehensive ensemble AI models consisting of various deep neural networks and a runtime system that automates the creation of mutually stimulatory memento on top of the user’s usual photo-taking routines. We deploy MomentMeld in-the-wild with six families for an eight-week period, and discuss the key findings and further implications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445688
ER  - 

TY  - NA
AU  - Thangavel, Gomathi; Memedi, Mevludin; Hedström, Karin
TI  - Customized Information and Communication Technology for Reducing Social Isolation and Loneliness Among Older Adults: Scoping Review (Preprint)
PY  - 2021
AB  - <sec> <title>BACKGROUND</title> <p>Advancements in science and various technologies have resulted in people having access to better health care, a good quality of life, and better economic situations, enabling humans to live longer than ever before. Research shows that the problems of loneliness and social isolation are common among older adults, affecting psychological and physical health. Information and communication technology (ICT) plays an important role in alleviating social isolation and loneliness.</p> </sec> <sec> <title>OBJECTIVE</title> <p>The aim of this review is to explore ICT solutions for reducing social isolation or loneliness among older adults, the purpose of ICT solutions, and the evaluation focus of these solutions. This study particularly focuses on customized ICT solutions that either are designed from scratch or are modifications of existing off-the-shelf products that cater to the needs of older adults.</p> </sec> <sec> <title>METHODS</title> <p>A scoping literature review was conducted. A search across 7 databases, including ScienceDirect, Association for Computing Machinery, PubMed, IEEE Xplore, PsycINFO, Scopus, and Web of Science, was performed, targeting ICT solutions for reducing and managing social isolation and loneliness among older adults. Articles published in English from 2010 to 2020 were extracted and analyzed.</p> </sec> <sec> <title>RESULTS</title> <p>From the review of 39 articles, we identified 5 different purposes of customized ICT solutions focusing on reducing social isolation and loneliness. These were &lt;i&gt;social communication&lt;/i&gt;, &lt;i&gt;social participation&lt;/i&gt;, a &lt;i&gt;sense of belonging&lt;/i&gt;, &lt;i&gt;companionship&lt;/i&gt;, and &lt;i&gt;feelings of being seen&lt;/i&gt;. The mapping of purposes of ICT solutions with problems found among older adults indicates that increasing social communication and social participation can help reduce social isolation problems, whereas fulfilling emotional relationships and feeling valued can reduce feelings of loneliness. In terms of customized ICT solution types, we found the following seven different categories: &lt;i&gt;social network&lt;/i&gt;, &lt;i&gt;messaging services&lt;/i&gt;, &lt;i&gt;video chat&lt;/i&gt;, &lt;i&gt;virtual spaces or classrooms with messaging capabilities&lt;/i&gt;, &lt;i&gt;robotics&lt;/i&gt;, &lt;i&gt;games&lt;/i&gt;, and &lt;i&gt;content creation and management&lt;/i&gt;. Most of the included studies (30/39, 77%) evaluated the usability and acceptance aspects, and few studies (11/39, 28%) focused on loneliness or social isolation outcomes.</p> </sec> <sec> <title>CONCLUSIONS</title> <p>This review highlights the importance of discussing and managing social isolation and loneliness as different but related concepts and emphasizes the need for future research to use suitable outcome measures for evaluating ICT solutions based on the problem. Even though a wide range of customized ICT solutions have been developed, future studies need to explore the recent emerging technologies, such as the Internet of Things and augmented or virtual reality, to tackle social isolation and loneliness among older adults. Furthermore, future studies should consider evaluating social isolation or loneliness while developing customized ICT solutions to provide more robust data on the effectiveness of the solutions.</p> </sec>
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.2196/preprints.34221
ER  - 

TY  - JOUR
AU  - Faleel, Shariff A. M.; Gammon, Michael; Fan, Kevin; Huang, Da-Yuan; Li, Wei; Irani, Pourang
TI  - HPUI: Hand Proximate User Interfaces for One-Handed Interactions on Head Mounted Displays
PY  - 2021
AB  - We explore the design of Hand Proximate User Interfaces (HPUIs) for head-mounted displays (HMDs) to facilitate near-body interactions with the display directly projected on, or around the user's hand. We focus on single-handed input, while taking into consideration the hand anatomy which distorts naturally when the user interacts with the display. Through two user studies, we explore the potential for discrete as well as continuous input. For discrete input, HPUIs favor targets that are directly on the fingers (as opposed to off-finger) as they offer tactile feedback. We demonstrate that continuous interaction is also possible, and is as effective on the fingers as in the off-finger space between the index finger and thumb. We also find that with continuous input, content is more easily controlled when the interaction occurs in the vertical or horizontal axes, and less with diagonal movements. We conclude with applications and recommendations for the design of future HPUIs.
SP  - 4215
EP  - 4225
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2021.3106493
ER  - 

TY  - JOUR
AU  - Borowski, Marcel; Fog, Bjarke V.; Griggio, Carla F.; Eagan, James R.; Klokmose, Clemens N.
TI  - Between Principle and Pragmatism: Reflections on Prototyping Computational Media with Webstrates
PY  - 2022
AB  - <jats:p>Computational media describes a vision of software, which, in contrast to application-centric software, is (1) malleable, so users can modify existing functionality, (2) computable, so users can run custom code, (3) distributable, so users can open documents across different devices, and (4) shareable, so users can easily share and collaborate on documents. Over the last ten years, the Webstrates and Codestrates projects aimed to realize this vision of computational media. Webstrates is a server application that synchronizes the DOM of websites. Codestrates builds on top of Webstrates and adds an authoring environment, which blurs the use and development of applications. Grounded in a chronology of the development of Webstrates and Codestrates, we present eight tensions that we needed to balance during their development. We use these tensions as an analytical lens in three case studies and a game challenge in which participants created games using Codestrates. We discuss the results of the game challenge based on these tensions and present key takeaways for six of them. Finally, we present six lessons learned from our endeavor to realize the vision of computational media, demonstrating the balancing act of weighing the vision against the pragmatics of implementing a working system.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3569895
ER  - 

TY  - NA
AU  - Preindl, Thomas; Honnet, Cedric; Pointner, Andreas; Aigner, Roland; Paradiso, Joseph A.; Haller, Michael J.
TI  - UIST - Sonoflex: Embroidered Speakers Without Permanent Magnets
PY  - 2020
AB  - We present Sonoflex, a thin-form, embroidered dynamic speaker made without using a permanent magnet. Our design consists of two flat spiral coils, stacked on top of each other, and is based on an isolated, thin (0.15 mm) enameled copper wire. Our approach allows for thin, lightweight, and textile speakers and does not require high voltage as in electrostatic speakers. We show how the speaker can be designed and fabricated and evaluate its acoustic properties as a function of manufacturing parameters (size, turn counts, turn spacing, and substrate materials). The experiment results revealed that we can produce audible sound with a broad frequency range (1.5 kHz - 20 kHz) with the embroidered speaker with a diameter of 50 mm. We conclude the paper by presenting several applications such as audible notifications and near-ultrasound communication.
SP  - 675
EP  - 685
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415888
ER  - 

TY  - NA
AU  - Maehigashi, Akihiro; Sasada, Akira; Matsumuro, Miki; Shibata, Fumihisa; Kimura, Asako; Niida, Sumaru
TI  - CHI Extended Abstracts - Virtual Weight Illusion: Weight Perception of Virtual Objects Using Weight Illusions
PY  - 2021
AB  - This study investigated whether weight illusions in virtual reality (VR) without haptic feedback occur as in the real world. In the experiment, we set up three scenarios to cause three different weight illusions in VR: the size–weight illusion (smaller objects look lighter but feel heavier than larger ones), brightness–weight illusion (brighter objects look lighter but feel heavier than darker ones), and material–weight illusion (lighter-looking materials, such as wood, look lighter but feel heavier than heavier-looking materials, such as metal). The experimental results indicated that the weight perceptions of the brightness–weight and material–weight illusions in VR were opposite to those in the real world. However, the weight perception of the size–weight illusion in VR was the same as in the real world. This study demonstrates how weight illusions occur in VR without haptic feedback, and classifies weight perceptions and the robustness of the illusions.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451842
ER  - 

TY  - JOUR
AU  - Yang, Xiaoying; Sayono, Jacob; Xu, Jess; Li, Jiahao Nick; Hester, Josiah; Zhang, Yang
TI  - MiniKers
PY  - 2022
AB  - <jats:p>Automating operations of objects has made life easier and more convenient for billions of people, especially those with limited motor capabilities. On the other hand, even able-bodied users might not always be able to perform manual operations (e.g., both hands are occupied), and manual operations might be undesirable for hygiene purposes (e.g., contactless devices). As a result, automation systems like motion-triggered doors, remote-control window shades, contactless toilet lids have become increasingly popular in private and public environments. Yet, these systems are hampered by complex building wiring or short battery lifetimes, negating their positive benefits for accessibility, energy saving, healthcare, and other domains. In this paper we explore how these types of objects can be powered in perpetuity by the energy generated from a unique energy source - user interactions, specifically, the manual manipulations of objects by users who can afford them when they can afford them. Our assumption is that users' capabilities for object operations are heterogeneous, there are desires for both manual and automatic operations in most environments, and that automatic operations are often not needed as frequently - for example, an automatic door in a public space is often manually opened many times before a need for automatic operation shows up. The energy harvested by those manual operations would be sufficient to power that one automatic operation. We instantiate this idea by upcycling common everyday objects with devices which have various mechanical designs powered by a general-purpose backbone embedded system. We call these devices, MiniKers. We built a custom driver circuit that can enable motor mechanisms to toggle between generating powers (i.e., manual operation) and actuating objects (i.e., automatic operation). We designed a wide variety of mechanical mechanisms to retrofit existing objects and evaluated our system with a 48-hour deployment study, which proves the efficacy of MiniKers as well as shedding light into this people-as-power approach as a feasible solution to address energy needed for smart environment automation.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550287
ER  - 

TY  - CHAP
AU  - Hopkins, Torin; Bae, S. Sandra; Uhr, Julia; Banić, Amy; Zheng, Clement; Do, Ellen Yi-Luen
TI  - User Interfaces in Smart Cities
PY  - 2021
AB  - NA
SP  - 1
EP  - 33
JF  - Handbook of Smart Cities
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-15145-4_94-1
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Wong, Andrew; Qi, Yini; Blumberg, Lotta-Gili; Kim, Yoonji; Mueller, Stefanie
TI  - Conference on Designing Interactive Systems - Adapt2Learn: A Toolkit for Configuring the Learning Algorithm for Adaptive Physical Tools for Motor-Skill Learning
PY  - 2021
AB  - A recent study on motor-skill training showed that adaptive training tools that use shape-change to adapt the training difficulty based on learners’ performance can lead to higher learning gains. However, to date, no support tools exist to help designers create adaptive learning tools. Our formative study shows that developing the adaptive learning algorithm poses a particular challenge. To address this, we built Adapt2Learn, a toolkit that auto-generates the learning algorithm for adaptive tools. Designers choose their tool’s sensors and actuators, Adapt2Learn then configures the learning algorithm and generates a microcontroller script that designers can deploy on the tool. Once uploaded, the script assesses the learner’s performance via the sensors, computes the training difficulty, and actuates the tool to adapt the difficulty. Adapt2Learn’s visualization tool then lets designers visualize their tool’s adaptation and evaluate the learning algorithm. To validate that Adapt2Learn can generate adaptation algorithms for different tools, we built several application examples that demonstrate successful deployment.
SP  - 1301
EP  - 1312
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462128
ER  - 

TY  - NA
AU  - Faleel, Shariff A. M.; Gammon, Michael; Sakamoto, Yumiko; Menon, Carlo; Irani, Pourang
TI  - AH - User gesture elicitation of common smartphone tasks for hand proximate user interfaces
PY  - 2020
AB  - The ubiquity of smartphone interactions along with the advancements made in mixed reality applications and gesture recognition present an intriguing space for novel interaction techniques using the hand as an interface. This paper explores the idea of using hand proximate user interfaces (UI), i.e. interactions with and display of interface elements on and around the hand. We conducted two user studies to gain a better understanding of the design space for such interactions. The first study identifies the possible ways in which various elements can be displayed on and around the hand in the context of common smartphone applications. We conduct a second study to build a gesture set for interactions with elements displayed on and around the hand. We contribute an analysis of the data and observations collected from the two studies, resulting in a layout set and a gesture set for interactions with hand proximate UIs.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3396339.3396363
ER  - 

TY  - JOUR
AU  - Wambecke, Jérémy; Goguey, Alix; Nigay, Laurence; Dargent, Lauren; Hauret, Daniel; Lafon, Stephanie; de Visme, Jean-Samuel Louis
TI  - M[eye]cro: Eye-gaze+Microgestures for Multitasking and Interruptions
PY  - 2021
AB  - We present M[eye]cro an interaction technique to select on-screen objects and navigate menus through the synergistic use of eye-gaze and thumb-to-finger microgestures. Thumb-to-finger microgestures are gestures performed with the thumb of a hand onto the fingers of the same hand. The active body of research on microgestures highlights expected properties including speed, availability and eye-free interaction. Such properties make microgestures a good candidate for multitasking. However, while praised, the state-of-the-art hypothesis stating that microgestures could be beneficial for multitasking has never been quantitatively verified. We study and compare M[eye]cro to a baseline, i.e., a technique based on physical controllers, in a cockpit-based context. This context allows us to design a controlled experiment involving multitasking with low- and high-priority tasks in parallel. Our results show that performances of the two techniques are similar when participants only perform the selection task. However, M[eye]cro tends to yield better time performance when participants additionally need to treat high-priority tasks in parallel. Results also show that M[eye]cro induces less fatigue and is mostly preferred.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - EICS
PB  - 
DO  - 10.1145/3461732
ER  - 

TY  - JOUR
AU  - Nilsson, Niels Christian; Zenner, André; Simeone, Adalberto L.; Johnsen, Kyle; Sandor, Christian; Billinghurst, Mark
TI  - Propping Up Virtual Reality With Haptic Proxies
PY  - 2021
AB  - Physical props serving as proxies for virtual objects (haptic proxies) offer a cheap, convenient, and compelling way of delivering a sense of touch in virtual reality (VR). To successfully use haptic proxies for VR, they have to be both similar to and colocated with their virtual counterparts. In this article, we introduce a taxonomy organizing techniques using haptic proxies for VR into eight categories based on when the techniques are deployed (offline or real-time), what reality is being manipulated (physical or virtual reality), and the purpose of the techniques (to affect object perception or the mapping between real and virtual objects). Finally, we discuss key advantages and limitations of the different categories of techniques.
SP  - 104
EP  - 112
JF  - IEEE computer graphics and applications
VL  - 41
IS  - 5
PB  - 
DO  - 10.1109/mcg.2021.3097671
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun
TI  - UIST (Adjunct Volume) - Multi-Modal Interactive Task Learning from Demonstrations and Natural Language Instructions
PY  - 2020
AB  - Interactive task learning (ITL) allows end users to 'teach' an intelligent agent new tasks, the corresponding task conditions,and the relevant concepts. This paper presents my research on expanding the applicability, generalizability, robustness, expressiveness, and script sharability of ITL systems usinga multi-modal approach. My research demonstrates that a multi-modal ITL approach that combines programming by demonstration and natural language instructions can empower users without significant programming expertise to extend intelligent agents for their own app-based computing tasks.
SP  - 162
EP  - 168
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415803
ER  - 

TY  - JOUR
AU  - Lu, Xiong; Yan, Yuxing; Qi, Beibei; Qian, Huang; Sun, Junbin; Quigley, Aaron
TI  - Contactless Haptic Display Through Magnetic Field Control.
PY  - 2022
AB  - Haptic rendering enables people to touch, perceive, and manipulate virtual objects in a virtual environment. Using six cascaded identical hollow disk electromagnets and a small permanent magnet attached to an operator's finger, this paper proposes and develops an untethered haptic interface through magnetic field control. The concentric hole inside the six cascaded electromagnets provides the workspace, where the 3D position of the permanent magnet is tracked with a Microsoft Kinect sensor. The driving currents of six cascaded electromagnets are calculated in real-time for generating the desired magnetic force. Offline data from an FEA (finite element analysis) based simulation, determines the relationship between the magnetic force, the driving currents, and the position of the permanent magnet. A set of experiments including the virtual object recognition experiment, the virtual surface identification experiment, and the user perception evaluation experiment were conducted to demonstrate the proposed system, where Microsoft HoloLens holographic glasses are used for visual rendering. The proposed magnetic haptic display leads to an untethered and non-contact interface for natural haptic rendering applications, which overcomes the constraints of mechanical linkages in tool-based traditional haptic devices.
SP  - 328
EP  - 338
JF  - IEEE transactions on haptics
VL  - 15
IS  - 2
PB  - 
DO  - 10.1109/toh.2022.3151673
ER  - 

TY  - NA
AU  - Sereshkeh, Alborz Rezazadeh; Leung, Gary; Perumal, Krish; Phillips, Caleb; Zhang, Minfan; Fazly, Afsaneh; Mohomed, Iqbal
TI  - VASTA: A Vision and Language-assisted Smartphone Task Automation System
PY  - 2019
AB  - We present VASTA, a novel vision and language-assisted Programming By Demonstration (PBD) system for smartphone task automation. Development of a robust PBD automation system requires overcoming three key challenges: first, how to make a particular demonstration robust to positional and visual changes in the user interface (UI) elements; secondly, how to recognize changes in the automation parameters to make the demonstration as generalizable as possible; and thirdly, how to recognize from the user utterance what automation the user wishes to carry out. To address the first challenge, VASTA leverages state-of-the-art computer vision techniques, including object detection and optical character recognition, to accurately label interactions demonstrated by a user, without relying on the underlying UI structures. To address the second and third challenges, VASTA takes advantage of advanced natural language understanding algorithms for analyzing the user utterance to trigger the VASTA automation scripts, and to determine the automation parameters for generalization. We run an initial user study that demonstrates the effectiveness of VASTA at clustering user utterances, understanding changes in the automation parameters, detecting desired UI elements, and, most importantly, automating various tasks. A demo video of the system is available here: this http URL
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Mäkelä, Ville; Kleine, Johannes; Hood, Maxine; Alt, Florian; Schmidt, Albrecht
TI  - CHI - Hidden Interaction Techniques: Concealed Information Acquisition and Texting on Smartphones and Wearables
PY  - 2021
AB  - There are many situations where using personal devices is not socially acceptable, or where nearby people present a privacy risk. For these situations, we explore the concept of hidden interaction techniques through two prototype applications. HiddenHaptics allows users to receive information through vibrotactile cues on a smartphone, and HideWrite allows users to write text messages by drawing on a dimmed smartwatch screen. We conducted three user studies to investigate whether, and how, these techniques can be used without being exposed. Our primary findings are (1) users can effectively hide their interactions while attending to a social situation, (2) users seek to interact when another person is speaking, and they also tend to hide the interaction using their body or furniture, and (3) users can sufficiently focus on the social situation despite their interaction, whereas non-users feel that observing the user hinders their ability to focus on the social activity.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445504
ER  - 

TY  - CHAP
AU  - , 
TI  - Vignettes
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544565
ER  - 

TY  - NA
AU  - Aragon Bartsch, Sarah; Schneegass, Christina; Bemmann, Florian; Buschek, Daniel
TI  - A Day in the Life: Exploring the Use of Scheduled Mobile Chat Messages for Career Guidance
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3490637
ER  - 

TY  - NA
AU  - Streli, Paul; Jiang, Jiaxi; Fender, Andreas Rene; Meier, Manuel; Romat, Hugo; Holz, Christian
TI  - TapType: Ten-finger text entry on everyday surfaces via Bayesian inference
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501878
ER  - 

TY  - JOUR
AU  - Candoğan, Kezban; Bulut, Elvan Gökçen
TI  - 3D GIDA BASKISI: GÜNCEL DURUM VE GELECEK EĞİLİMLERİ
PY  - 2021
AB  - NA
SP  - 152
EP  - 167
JF  - GIDA / THE JOURNAL OF FOOD
VL  - 46
IS  - 1
PB  - 
DO  - 10.15237/gida.gd20130
ER  - 

TY  - NA
AU  - Yang, Humphrey; Qian, Kuanren; Liu, Haolin; Yu, Yuxuan; Gu, Jianzhe; McGehee, Matthew; Zhang, Yongjie Jessica; Yao, Lining
TI  - UIST - SimuLearn: Fast and Accurate Simulator to Support Morphing Materials Design and Workflows
PY  - 2020
AB  - Morphing materials allow us to create new modalities of interaction and fabrication by leveraging the materials? dynamic behaviors. Yet, despite the ongoing rapid growth of computational tools within this realm, current developments are bottlenecked by the lack of an effective simulation method. As a result, existing design tools must trade-off between speed and accuracy to support a real-time interactive design scenario. In response, we introduce SimuLearn, a data-driven method that combines finite element analysis and machine learning to create real-time (0.61 seconds) and truthful (97% accuracy) morphing material simulators. We use mesh-like 4D printed structures to contextualize this method and prototype design tools to exemplify the design workflows and spaces enabled by a fast and accurate simulation method. Situating this work among existing literature, we believe SimuLearn is a timely addition to the HCI CAD toolbox that can enable the proliferation of morphing materials.
SP  - 71
EP  - 84
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415867
ER  - 

TY  - CHAP
AU  - , 
TI  - Aesthetics of TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544575
ER  - 

TY  - NA
AU  - Nisser, Martin; Cheng, Leon; Makaram, Yashaswini; Suzuki, Ryo; Mueller, Stefanie
TI  - UIST (Adjunct Volume) - Programmable Polarities: Actuating Interactive Prototypes with Programmable Electromagnets
PY  - 2021
AB  - This demo introduces a framework that uses programmable electromagnets as a method to rapidly prototype interactive objects. Our approach allows users to to quickly and inexpensively embed actuation mechanisms into otherwise static prototypes in order to make them dynamic and interactive. Underpinning the technique is the insight of using electromagnets to interchangeably create attractive and repulsive forces between adjacent parts, and programmatically setting their polarities in a way that allows objects to translate rotationally and linearly, respond haptically, assemble, and locomote.
SP  - 121
EP  - 123
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480198
ER  - 

TY  - CHAP
AU  - , 
TI  - Authors' Biographies
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544583
ER  - 

TY  - BOOK
AU  - Ullmer, Brygg; Shaer, Orit; Mazalek, Ali; Hummels, Caroline
TI  - Weaving Fire into Form
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564
ER  - 

TY  - NA
AU  - Krosnick, Rebecca
TI  - Tools for Creating UI Automation Macros
PY  - 2022
AB  - Automation macros enable users to perform digital tasks programmatically to save time or support hands-free interaction. For example, macros can be used to perform web scraping for a research project (e.g., scraping articles from a news site) or personal task automation via natural language (e.g., ordering food for delivery). Some kinds of automation are built into our devices and are readily available (e.g., via Siri [1] or Alexa [2] ), but this set is limited and often will not support a user&#x2019;s niche or complex needs. Users can create their own custom macros, but traditionally this requires writing program code which involves a significant amount of effort for programmers and is infeasible for non-programmers.
SP  - NA
EP  - NA
JF  - 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc53370.2022.9832966
ER  - 

TY  - NA
AU  - Geslain, Benoit; Besga, Simon; Lebrun, Flavien; Bailly, Gilles
TI  - Generalizing the Hand Redirection Function in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3531073.3531100
ER  - 

TY  - NA
AU  - Clarence, Aldrich; Knibbe, Jarrod; Cordeil, Maxime; Wybrow, Michael
TI  - VR - Unscripted Retargeting: Reach Prediction for Haptic Retargeting in Virtual Reality
PY  - 2021
AB  - Research is exploring novel ways of adding haptics to VR. One popular technique is haptic retargeting, where real and virtual hands are decoupled to enable the reuse of physical props. However, this technique requires the system to know the users' intended interaction target, or requires additional hardware for prediction. We explore software-based reach prediction as a means of facilitating responsive, unscripted retargeting. We trained a Long Short-Term Memory network on users' reach trajectories to predict intended targets. We achieved an accuracy of 81.1 % at approximately 65% of movement. This could enable haptic retargeting during the last 35% of movement. We discuss the implications for possible physical proxy locations.
SP  - 150
EP  - 159
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00036
ER  - 

TY  - JOUR
AU  - Zhang, Amy X.; Muller, Michael; Wang, Dakuo
TI  - How do Data Science Workers Collaborate? Roles, Workflows, and Tools
PY  - 2020
AB  - Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW1
PB  - 
DO  - 10.1145/3392826
ER  - 

TY  - NA
AU  - Lang, Florian; Machulla, Tonja
TI  - Pressing a Button You Cannot See: Evaluating Visual Designs to Assist Persons with Low Vision through Augmented Reality
PY  - 2021
AB  - Partial vision loss occurs in several medical conditions and affects persons of all ages. It compromises many daily activities, such as reading, cutting vegetables, or identifying and accurately pressing buttons, e.g., on ticket machines or ATMs. Touchscreen interfaces pose a particular challenge because they lack haptic feedback from interface elements and often require people with impaired vision to rely on others for help. We propose a smartglasses-based solution to utilize the user’s residual vision. Together with visually-impaired individuals, we designed assistive augmentations for touchscreen interfaces and evaluated their suitability to guide attention towards interface elements and to increase the accuracy of manual inputs. We show that augmentations improve interaction performance and decrease cognitive load, particularly for unfamiliar interface layouts.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489873
ER  - 

TY  - CHAP
AU  - Tanaka, Toshimitsu; Ogawa, Natsumi; Tsuboi, Ryota; Sagawa, Yuji
TI  - One-Handed Character Input Method for Smart Glasses that Does Not Require Visual Confirmation of Fingertip Position
PY  - 2022
AB  - NA
SP  - 165
EP  - 179
JF  - Human-Computer Interaction. Technological Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05409-9_13
ER  - 

TY  - NA
AU  - Hoeve, Maartje ter; Sim, Robert; Nouri, Elnaz; Fourney, Adam; de Rijke, Maarten; White, Ryen W.
TI  - Conversations with Documents. An Exploration of Document-Centered Assistance
PY  - 2020
AB  - The role of conversational assistants has become more prevalent in helping people increase their productivity. Document-centered assistance, for example to help an individual quickly review a document, has seen less significant progress, even though it has the potential to tremendously increase a user's productivity. This type of document-centered assistance is the focus of this paper. Our contributions are three-fold: (1) We first present a survey to understand the space of document-centered assistance and the capabilities people expect in this scenario. (2) We investigate the types of queries that users will pose while seeking assistance with documents, and show that document-centered questions form the majority of these queries. (3) We present a set of initial machine learned models that show that (a) we can accurately detect document-centered questions, and (b) we can build reasonably accurate models for answering such questions. These positive results are encouraging, and suggest that even greater results may be attained with continued study of this interesting and novel problem space. Our findings have implications for the design of intelligent systems to support task completion via natural interactions with documents.
SP  - 43
EP  - 52
JF  - Proceedings of the 2020 Conference on Human Information Interaction and Retrieval
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3343413.3377971
ER  - 

TY  - NA
AU  - Wu, Jason; Reyes, Gabriel; White, Sam C.; Zhang, Xiaoyi; Bigham, Jeffrey P.
TI  - When Can Accessibility Help?: An Exploration of Accessibility Feature Recommendation on Mobile Devices
PY  - 2021
AB  - Numerous accessibility features have been developed and included in consumer operating systems to provide people with a variety of disabilities additional ways to access computing devices. Unfortunately, many users, especially older adults who are more likely to experience ability changes, are not aware of these features or do not know which combination to use. In this paper, we first quantify this problem via a survey with 100 participants, demonstrating that very few people are aware of built-in accessibility features on their phones. These observations led us to investigate accessibility recommendation as a way to increase awareness and adoption. We developed four prototype recommenders that span different accessibility categories, which we used to collect insights from 20 older adults. Our work demonstrates the need to increase awareness of existing accessibility features on mobile devices, and shows that automated recommendation could help people find beneficial accessibility features.
SP  - NA
EP  - NA
JF  - Proceedings of the 18th International Web for All Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430263.3452434
ER  - 

TY  - NA
AU  - Weng, Yueting; Chun, Yu; Shi, Yingtian; Zhao, Yuhang; Yan, Yukang; Shi, Yuanchun
TI  - CHI - FaceSight: Enabling Hand-to-Face Gesture Interaction on AR Glasses with a Downward-Facing Camera Vision
PY  - 2021
AB  - We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fixes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors. We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefits through five AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fingertip movement. Our algorithm achieves classification accuracy of all gestures at 83.06%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445484
ER  - 

TY  - JOUR
AU  - Zhao, Linlin; Zhang, Min; Chitrakar, Bimal; Adhikari, Benu
TI  - Recent advances in functional 3D printing of foods: a review of functions of ingredients and internal structures
PY  - 2020
AB  - Three-dimensional (3D) food printing technology combines 3D printing and food manufacturing. Rapidly increasing number of publications on various aspects of 3D food printing indicate the importance of this technology to food industry. The potential of delivering personalized products tailored to meet the taste preferences and specific dietary needs is one of the reasons for increasing researches in this technology. Currently there is an absence of a systematic review on the functional 3D printing. Also, there is no review on four-dimensional (4D) food printing concept that has emerged recently. This paper systematically reviews the functional ingredients used for creating printable food formula and their functions, including physiological functions, beneficial for health and physico-chemical functions, affecting the quality of 3D printing. In addition, it analyzes the functions of internal structures used or developed during 3D printing (infill structure and infill density) and their effects on texture properties of 3D printed food. Finally, it also introduces the concept of 4D food printing and summarizes the current advances in this novel technology.
SP  - 1
EP  - 15
JF  - Critical reviews in food science and nutrition
VL  - 61
IS  - 21
PB  - 
DO  - 10.1080/10408398.2020.1799327
ER  - 

TY  - NA
AU  - Li, Jiannan; Sousa, Maurício; Li, Chu; Liu, Jessie; Chen, Yan; Balakrishnan, Ravin; Grossman, Tovi
TI  - ASTEROIDS: Exploring Swarms of Mini-Telepresence Robots for Physical Skill Demonstration
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501927
ER  - 

TY  - JOUR
AU  - He, Liang; Wittkopf, Jarrid A.; Jun, Ji Won; Erickson, Kris; Ballagas, Rafael Tico
TI  - ModElec
PY  - 2021
AB  - <jats:p>Integrating electronics with highly custom 3D designs for the physical fabrication of interactive prototypes is traditionally cumbersome and requires numerous iterations of manual assembly and debugging. With the new capabilities of 3D printers, combining electronic design and 3D modeling workflows can lower the barrier for achieving interactive functionality or iterating on the overall design. We present ModElec---an interactive design tool that enables the coordinated expression of electronic and physical design intent by allowing designers to integrate 3D-printable circuits with 3D forms. With ModElec, the user can arrange electronic parts in a 3D body, modify the model design with embedded circuits updated, and preview the auto-generated 3D traces that can be directly printed with a multi-material-based 3D printer. We demonstrate the potential of ModElec with four example applications, from a set of game controls to reconfigurable devices. Further, the tool was reported as easy to use through a preliminary evaluation with eight designers.</jats:p>
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3495000
ER  - 

TY  - CHAP
AU  - , 
TI  - Tangible and Embodied Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544570
ER  - 

TY  - NA
AU  - Kong, Junhan; Guo, Anhong; Bigham, Jeffrey P.
TI  - ASSETS - Supporting Older Adults in Using Complex User Interfaces with Augmented Reality
PY  - 2019
AB  - Using complex interfaces has been shown to be challenging for older adults. Existing tutorial systems can be cumbersome, and sometimes difficult to use. To solve this problem, we present a system to support older adults in using visual interfaces by providing step-by-step visual guidance with augmented reality. Using the Apple ARKit platform, our system detects the interface in a phone camera view, and provides visual guidance for users to access the interface following a generated sequence of interactions based on pre-specified tasks and prior knowledge of the interface.
SP  - 661
EP  - 663
JF  - The 21st International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3308561.3354593
ER  - 

TY  - JOUR
AU  - Wenqiang, Chen; Lin, Chen; Ma, Meiyi; Parizi, Farshid Salemi; Patel, Shwetak N.; Stankovic, John A.
TI  - ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch
PY  - 2021
AB  - Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 1
PB  - 
DO  - 10.1145/3448119
ER  - 

TY  - NA
AU  - Borowski, Marcel; Klokmose, Clemens Nylandsted
TI  - Webstrates, Codestrates v2, and Varv: A Software Stack for Computational Media
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3547522.3547714
ER  - 

TY  - NA
AU  - Hedayati, Hooman; Suzuki, Ryo; Leithinger, Daniel; Szafir, Daniel
TI  - IROS - PufferBot: Actuated Expandable Structures for Aerial Robots
PY  - 2020
AB  - We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures.
SP  - 1338
EP  - 1343
JF  - 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iros45743.2020.9341088
ER  - 

TY  - JOUR
AU  - Paredes, Luis; Reddy, Sai Swarup; Chidambaram, Subramanian; Vagholkar, Devashri; Zhang, Yunbo; Benes, Bedrich; Ramani, Karthik
TI  - FabHandWear: An End-to-End Pipeline from Design to Fabrication of Customized Functional Hand Wearables
PY  - 2021
AB  - Current hand wearables have limited customizability, they are loose-fit to an individual's hand and lack comfort. The main barrier in customizing hand wearables is the geometric complexity and size variation in hands. Moreover, there are different functions that the users can be looking for; some may only want to detect hand's motion or orientation; others may be interested in tracking their vital signs. Current wearables usually fit multiple functions and are designed for a universal user with none or limited customization. There are no specialized tools that facilitate the creation of customized hand wearables for varying hand sizes and provide different functionalities. We envision an emerging generation of customizable hand wearables that supports hand differences and promotes hand exploration with additional functionality. We introduce FabHandWear, a novel system that allows end-to-end design and fabrication of customized functional self-contained hand wearables. FabHandWear is designed to work with off-the-shelf electronics, with the ability to connect them automatically and generate a printable pattern for fabrication. We validate our system by using illustrative applications, a durability test, and an empirical user evaluation. Overall, FabHandWear offers the freedom to create customized, functional, and manufacturable hand wearables.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463518
ER  - 

TY  - NA
AU  - Lam, Kit Yung; Lee, Lik-Hang; Hui, Pan
TI  - 3DeformR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 13th ACM Multimedia Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3524273.3528180
ER  - 

TY  - BOOK
AU  - Qian, Jing; Shamma, David A.; Avrahami, Daniel; Biehl, Jacob
TI  - IMX - Modality and Depth in Touchless Smartphone Augmented Reality Interactions
PY  - 2020
AB  - Augmented reality (AR) on smartphone devices allows people to interact with virtually placed objects anchored in the real world through the device’s viewport. Typically, smartphone AR interactions work with the device’s 2D touchscreen disconnected from the modality and depth of the virtual objects. In this paper, we studied 15 participants’ preferences, performances, and cognitive loads on a set of common tasks performed on smartphones at two interaction depths (close-range and distant) with two touchless modalities (hand tracking and screen dwell). We find that distant AR interactions, strongly preferred by the participants, were significantly faster and took less cognitive effort. We observed that within both interaction depths, modalities performed equally. When designing touchless modalities on smartphones, we suggest using distant interactions when overall performance is the top priority, otherwise, using hand tracking or screen dwell as back-ups for each other can be equally effective.
SP  - 74
EP  - 81
JF  - ACM International Conference on Interactive Media Experiences
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3391614.3393648
ER  - 

TY  - NA
AU  - Sarwar, Saquib; Wilson, David
TI  - Systematic Literature Review on Making and Accessibility
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 24th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517428.3550377
ER  - 

TY  - JOUR
AU  - Ishino, Masaya; Matsumoto, Mitsuharu
TI  - A Portable Transformable Device
PY  - 2022
AB  - Although there have been many studies on transforming robots in recent years, there are few reports of using transforming robots for daily use. In this research, we developed a portable transformable device using shape memory resin with the aim of daily use of the deformation devices. The developed device is lightweight and easy to carry. In spite of its simple structure, it is possible to realize a more complicated structure by combining them with each other. In order to clarify the effectiveness of the proposed device, we show some usage examples of the device and show the potential of the device as various tools for our daily life. © 2022 Institute of Electrical Engineers of Japan. Published by Wiley Periodicals LLC.
SP  - 153
EP  - 155
JF  - IEEJ Transactions on Electrical and Electronic Engineering
VL  - 18
IS  - 1
PB  - 
DO  - 10.1002/tee.23709
ER  - 

TY  - NA
AU  - Tsukuda, Yuga; Tagami, Daichi; Sadasue, Masaaki; Suzuki, Shieru; Lu, Jun-Li; Ochiai, Yoichi
TI  - Calmbots: Exploring Possibilities of Multiple Insects with On-hand Devices and Flexible Controls as Creation Interfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3516387
ER  - 

TY  - NA
AU  - Sermarini, John; Michlowitz, Robert A.; LaViola, Joseph J.; Walters, Lori C.; Azevedo, Roger; Kider Jr., Joseph T.
TI  - BIM Driven Retrofitting Design Evaluation of Building Facades
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567688
ER  - 

TY  - NA
AU  - Yamato, Yuki; Suzuki, Yutaro; Sekimori, Kodai; Shizuki, Buntarou; Takahashi, Shin
TI  - AVI - Hand Gesture Interaction with a Low-Resolution Infrared Image Sensor on an Inner Wrist
PY  - 2020
AB  - We propose a hand gesture interaction method using a low-resolution infrared image sensor on an inner wrist. We attach the sensor to the strap of a wrist-worn device, on the palmar side, and apply machine-learning techniques to recognize the gestures made by the opposite hand. As the sensor is placed on the inner wrist, the user can naturally control its direction to reduce privacy invasion. Our method can recognize four types of hand gestures: static hand poses, dynamic hand gestures, finger motion, and the relative hand position. We developed a prototype that does not invade surrounding people's privacy using an 8 x 8 low-resolution infrared image sensor. Then we conducted experiments to validate our prototype, and our results imply that the low-resolution sensor has sufficient capabilities for recognizing a rich array of hand gestures. In this paper, we introduce an implementation of a mapping application that can be controlled by our specified hand gestures, including gestures that use both hands.
SP  - NA
EP  - NA
JF  - Proceedings of the International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3399715.3399858
ER  - 

TY  - NA
AU  - Zhang, Mingrui Ray; Zhai, Shumin; Wobbrock, Jacob O.
TI  - TypeAnywhere: A QWERTY-Based Text Entry Solution for Ubiquitous Computing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517686
ER  - 

TY  - JOUR
AU  - Rodin, Ivan; Furnari, Antonino; Mavroeidis, Dimitrios; Farinella, Giovanni Maria
TI  - Predicting the future from first person (egocentric) vision: A survey
PY  - 2021
AB  - Abstract Egocentric videos can bring a lot of information about how humans perceive the world and interact with the environment, which can be beneficial for the analysis of human behaviour. The research in egocentric video analysis is developing rapidly thanks to the increasing availability of wearable devices and the opportunities offered by new large-scale egocentric datasets. As computer vision techniques continue to develop at an increasing pace, the tasks related to the prediction of future are starting to evolve from the need of understanding the present. Predicting future human activities, trajectories and interactions with objects is crucial in applications such as human–robot interaction, assistive wearable technologies for both industrial and daily living scenarios, entertainment and virtual or augmented reality. This survey summarizes the evolution of studies in the context of future prediction from egocentric vision making an overview of applications, devices, existing problems, commonly used datasets, models and input modalities. Our analysis highlights that methods for future prediction from egocentric vision can have a significant impact in a range of applications and that further research efforts should be devoted to the standardization of tasks and the proposal of datasets considering real-world scenarios such as the ones with an industrial vocation.
SP  - 103252
EP  - NA
JF  - Computer Vision and Image Understanding
VL  - 211
IS  - NA
PB  - 
DO  - 10.1016/j.cviu.2021.103252
ER  - 

TY  - NA
AU  - Vatavu, Radu-Daniel
TI  - Sensorimotor Realities: Formalizing Ability-Mediating Design for Computer-Mediated Reality Environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00086
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Qi, Shutong; Chen, Junchi; Shang, MuJie; Gong, Jun; Seyed, Teddy; Yang, Xing-Dong
TI  - CHI - Fabriccio: Touchless Gestural Input on Interactive Fabrics
PY  - 2020
AB  - We present Fabriccio, a touchless gesture sensing technique developed for interactive fabrics using Doppler motion sensing. Our prototype was developed using a pair of loop antennas (one for transmitting and the other for receiving), made of conductive thread that was sewn onto a fabric substrate. The antenna type, configuration, transmission lines, and operating frequency were carefully chosen to balance the complexity of the fabrication process and the sensitivity of our system for touchless hand gestures, performed at a 10 cm distance. Through a ten-participant study, we evaluated the performance of our proposed sensing technique across 11 touchless gestures as well as 1 touch gesture. The study result yielded a 92.8% cross-validation accuracy and 85.2% leave-one-session-out accuracy. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique on soft objects.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376681
ER  - 

TY  - NA
AU  - Cruz, Christian Arzate; Igarashi, Takeo
TI  - Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors.
PY  - 2021
AB  - Reinforcement learning techniques successfully generate convincing agent behaviors, but it is still difficult to tailor the behavior to align with a user's specific preferences. What is missing is a communication method for the system to explain the behavior and for the user to repair it. In this paper, we present a novel interaction method that uses interactive explanations using templates of natural language as a communication method. The main advantage of this interaction method is that it enables a two-way communication channel between users and the agent; the bot can explain its thinking procedure to the users, and the users can communicate their behavior preferences to the bot using the same interactive explanations. In this manner, the thinking procedure of the bot is transparent, and users can provide corrections to the bot that include a suggested action to take, a goal to achieve, and the reasons behind these decisions. We tested our proposed method in a clone of the video game named \textit{Super Mario Bros.}, and the results demonstrate that our interactive explanation approach is effective at diagnosing and repairing bot behaviors.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Pamparău, Cristian; Vatavu, Radu-Daniel
TI  - FlexiSee: flexible configuration, customization, and control of mediated and augmented vision for users of smart eyewear devices
PY  - 2021
AB  - Smart eyewear and Augmented Reality technology have been examined closely in the scientific community to provide vision rehabilitation to people with visual impairments as well as augmented vision to people with and without visual impairments for various application scenarios and contexts of use. However, current systems lack flexibility in the configuration and customization of the features and functionalities they present to their users, as we show in this paper by means of a thorough literature review and categorization of prior work on augmented and mediated vision for smart eyewear devices. To address the flexibility aspect that has been missing in prior work, we introduce FlexiSee, an application for smart eyewear devices, such as see-through Augmented Reality glasses and Head-Mounted Mixed Reality Displays, specifically designed to enable flexible configuration, customization, and control of both augmented and mediated vision. FlexiSee achieves this desiderata by implementing visual filters (e.g., color correction, edge highlighting, contrast adjustment, and others) that are coupled with a web-based interface, readily accessible from smartphones, tablets, smartwatches, and other devices with web browsers, where authorized users can specify and apply custom parameters for the visual filters implemented by FlexiSee. We also introduce FlexiSee-DS, a three-dimensional design space for FlexiSee-like applications, that includes mediation & augmentation, user categories, and control design dimensions to specify a variety of FlexiSee-like systems. We show how the dimensions of FlexiSee-DS were applied to inform the design of our FlexiSee system, and we highlight and focus on the distinction between primary users and vision monitors and assistants, where the latter two categories represent new types of users for augmented and mediated vision that have various degrees of control, from their remote locations, over the visual reality delivered to and perceived by the primary users of smart eyewear devices. We conduct a user study to understand the perception of vision monitors and assistants regarding our new FlexiSee concept and system, and we report empirical results about usability aspects (e.g., we found an average SUS score of 75.3 and high ratings for the perceived usefulness of FlexiSee) as well as user feedback and suggestions to inform further developments of FlexiSee-like systems and applications.
SP  - 30943
EP  - 30968
JF  - Multimedia Tools and Applications
VL  - 80
IS  - 20
PB  - 
DO  - 10.1007/s11042-020-10164-5
ER  - 

TY  - CHAP
AU  - , 
TI  - Framing TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544572
ER  - 

TY  - CHAP
AU  - Fu, Zongkai; Li, Huiyong; Ouyang, Zhenchao; Liu, Xuefeng; Niu, Jianwei
TI  - ICA3PP (1) - Typing Everywhere with an EMG Keyboard: A Novel Myo Armband-Based HCI Tool
PY  - 2020
AB  - To enhance users’ experience of inputting characters on mobile devices with small screens, this paper designed a novel virtual keyboard used on mobile devices. In particular, we introduce a novel virtual keyboard based on the MYO armband which is able to capture the electromyogram (EMG) signals of users when typing on any surfaces (such as human body or normal desktop). The actions of the three fingers are mapped to the nine keys of the T9 keyboard. After that, the signals of finger motions are translated into key sequences of the T9 keyboard. However, the identification of continuous finger motions is a critical challenge. To address the challenge, we convert the EMG signals in time domain into a 3D time-frequency map (each channel corresponds to the EMG unit of a frequency-domain feature), and extract the convolutional features with a 4-layer CNN (Convolutional Neural Network) module, an im2col module of Optical Character Recognition (OCR) and a Long Short-Term Memory (LSTM) module, and the final result is achieved as a probability graph of finger gestures. The Connection Temporal Classification (CTC) algorithm is adopted to find the best gesture sequence from the probability map. Experimental results show that our method can effectively identify different key sequences at three different input speeds with an average accuracy of 85.9%, and the integration testing with different volunteers shows that our method can achieve an average typing speed of 15.7 Word-Per-Minute (WPM).
SP  - 247
EP  - 261
JF  - Algorithms and Architectures for Parallel Processing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-60245-1_17
ER  - 

TY  - NA
AU  - Ye, Huizhong; Janssen, Charlaine; Noordman, Daan; Liang, Rong-Hao
TI  - Understanding How to Support Remote Co-Design with a Conceptual Modular Shape-Changing Interface Toolkit
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3505563
ER  - 

TY  - NA
AU  - Calacci, Dan
TI  - Organizing in the End of Employment: Information Sharing, Data Stewardship, and Digital Workerism
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Human-Computer Interaction for Work
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3533406.3533424
ER  - 

TY  - JOUR
AU  - Chen, Yan; Lasecki, Walter S.; Dong, Tao
TI  - Towards Supporting Programming Education at Scale via Live Streaming
PY  - 2021
AB  - Live streaming, which allows streamers to broadcast their work to live viewers, is an emerging practice for teaching and learning computer programming. Participation in live streaming is growing rapidly, despite several apparent challenges, such as a general lack of training in pedagogy among streamers and scarce signals about a stream's characteristics (e.g., difficulty, style, and usefulness) to help viewers decide what to watch. To understand why people choose to participate in live streaming for teaching or learning programming, and how they cope with both apparent and non-obvious challenges, we interviewed 14 streamers and viewers about their experience with live streaming programming. Among other results, we found that the casual and impromptu nature of live streaming makes it easier to prepare than pre-recorded videos, and viewers have the opportunity to shape the content and learning experience via real-time communication with both the streamer and each other. Nonetheless, we identified several challenges that limit the potential of live streaming as a learning medium. For example, streamers voiced privacy and harassment concerns, and existing streaming platforms do not adequately support viewer-streamer interactions, adaptive learning, and discovery and selection of streaming content. Based on these findings, we suggest specialized tools to facilitate knowledge sharing among people teaching and learning computer programming online, and we offer design recommendations that promote a healthy, safe, and engaging learning environment.
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW3
PB  - 
DO  - 10.1145/3434168
ER  - 

TY  - NA
AU  - Kshirsagar, Abhay; Patil, Neeraj
TI  - ICCCNT - IoT based smart lock with predictive maintenance
PY  - 2021
AB  - Security has been an essential matter of concern worldwide. Recently, cases of vehicle and consignment thefts have increased immensely. Security-based problems, as in goods' transportation through trucks, railways, private vehicles and other shipments, are vulnerable to frequent theft and security checks are done manually, as noted. It has been observed that physical locks are used for security that needs to be checked manually at regular intervals of time or at every halt or station. However, these systems are unreliable. The proposed circuit consists of a microcontroller, a tamper detection circuit, and a GPS module. Predictive maintenance will include monitoring the performance of the circuit components and predicting the likelihood of failures in the future. The aim is to provide security, safety monitoring, data tracking, real-time operations, etc. reducing theft cases, human efforts among other examples. The aim is to identify cargo robbery cases that are at risk and to provide security for different product types at various coordinates for transportation. The Internet of connected Things has enabled the devices in our environment to be and decisive devices, i.e., they exchange information or data with other devices or users of the network; wired/wireless, through a common Internet Protocol (IP) that connects with the Internet. In this way, we ensure that the devices are secure and capable of understanding unlawful events and changes in nearby areas and are acting and reacting autonomously largely without human intervention.
SP  - 1
EP  - 6
JF  - 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icccnt51525.2021.9579965
ER  - 

TY  - NA
AU  - Tran, Thanh-Hai; Tran, Hoang-Nhat; Nguyen, Hong-Quan; Le, Trung-Hieu; Nguyen, Van-Thang; Tran, Trung-Kien; Pham, Cuong; Le, Thi-Lan; Vu, Hai; Nguyen, Thanh Phuong; Thanh, Nguyen Huu
TI  - A pilot study on hand posture recognition from wrist-worn camera for human machine interaction
PY  - 2021
AB  - Hand gestures have been shown to be an efficient way for human-machine interaction. Existing approaches usually utilize ambient or head/chest-mounted cameras to capture hand images. This paper presents a new way to capture hand gestures using the wrist-worn camera. The wrist-worn device is designed as a watch with an integrated camera that is much easier and comfortable to wear in daily life context. We then collect a dataset of ten hand postures using the designed prototype by ten subjects. In addition, we deploy state-of-the-art lite CNN models (YOLO family, Single Shot Detector-SSD) as posture detectors and classifiers. Experimental results show that with limited camera angles, the postures are highly distinctive and easily discriminated with the highest performance of 98.85% and 97.40% in terms of precision and recall, which motivates a wide range of applications and new research directions for human-machine interaction, wearables, the Internet of Things (IoT) and so on.
SP  - NA
EP  - NA
JF  - 2021 International Conference on Advanced Technologies for Communications (ATC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/atc52653.2021.9598223
ER  - 

TY  - NA
AU  - August, Tal; Iqbal, Shamsi T.; Gamon, Michael; Encarnación, Mark J.
TI  - MobileHCI - Characterizing the Mobile Microtask Writing Process
PY  - 2020
AB  - The unique limitations of mobile environments make content creation and editing difficult. Microtasking—breaking down complex tasks into subtasks—requires shorter attention spans and quick interactions, making it suitable for mobile usage scenarios. Writing is an ideal process for mobile microtasking because of its many subgoals, but little is known about how writers can use this decomposition through the evolution of a document. In this paper we present findings from a controlled, week long study to characterize how writers use mobile microtasks while authoring a document. We found that writers created microtasks for editing and inserting information that generally required minimal writing. These tasks were especially well suited for mobile devices with writers completing tasks on commutes or while waiting for meetings. Writers who microtasked found it easy to interact with their document and complete tasks, writing and editing their document more overall compared to writers who instead edited their document directly on their phone.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403541
ER  - 

TY  - JOUR
AU  - Guo, Jia; Zhang, Min; Li, Jingyuan; Fang, Zhongxiang
TI  - Using Soy Protein Isolate to Improve the Deformation Properties of 4D-Printed Oat Flour Butterfly
PY  - 2023
AB  - NA
SP  - NA
EP  - NA
JF  - Food and Bioprocess Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11947-023-02994-6
ER  - 

TY  - JOUR
AU  - Villanueva, Ana; Zhu, Zhengzhe; Liu, Ziyi; Wang, Feiyang; Chidambaram, Subramanian; Ramani, Karthik
TI  - ColabAR: A Toolkit for Remote Collaboration in Tangible Augmented Reality Laboratories
PY  - 2022
AB  - <jats:p>Current times are accelerating new technologies to provide high-quality education for remote collaboration, as well as hands-on learning. This is particularly important in the case of laboratory-based classes, which play an essential role in STEM education. In this paper, we introduce ColabAR, a toolkit that uses physical proxies to manipulate virtual objects in Tangible Augmented Reality (TAR) laboratories. ColabAR introduces haptic-based customizable interaction techniques to promote remote collaboration between students. Our toolkit provides hardware and software that enable haptic feedback to improve user experience and promote collaboration during learning. Also, we present the architecture of our cloud platform for haptic interaction that supports information sharing between students in a TAR laboratory. We performed two user studies (N=40) to test the effect of our toolkit in enriching local and remote collaborative experiences. Finally, we demonstrated that our TAR laboratory enables students' performance (i.e., lab completion rate, lab scores) to be similar to their performance in an in-person laboratory.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512928
ER  - 

TY  - NA
AU  - Hong, Jonggi; Gandhi, Jaina; Mensah, Ernest Essuah; Zeraati, Farnaz Zamiri; Jarjue, Ebrima; Lee, Kyungjun; Kacorri, Hernisa
TI  - Blind Users Accessing Their Training Images in Teachable Object Recognizers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 24th International ACM SIGACCESS Conference on Computers and Accessibility
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517428.3544824
ER  - 

TY  - NA
AU  - Wu, Erwin; Yuan, Ye; Yeo, Hui-Shyong; Quigley, Aaron; Koike, Hideki; Kitani, Kris M.
TI  - UIST - Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-worn Camera via Dorsum Deformation Network
PY  - 2020
AB  - The automatic recognition of how people use their hands and fingers in natural settings -- without instrumenting the fingers -- can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fingers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress finger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the first vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81 degree for user-specific models and 9.77 degree for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer.
SP  - 1147
EP  - 1160
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415897
ER  - 

TY  - BOOK
AU  - Crovari, Pietro; Pidò, Sara; Garzotto, Franca; Ceri, Stefano
TI  - CONVERSATIONS - Show, Don’t Tell. Reflections on the Design of Multi-modal Conversational Interfaces
PY  - 2021
AB  - Conversational Agents are the future of Human-Computer Interaction. Technological advancements in Artificial Intelligence and Natural Language Processing allow the development of Conversational Agents that support increasingly complex tasks. When the complexity increases, the conversation alone is no more sufficient to support the interaction effectively, but other modalities must be integrated to relieve the cognitive burden for the final user. To this aim, we define and discuss a set of design principles to create effective multi-modal Conversational Agents. We start from the best practices in literature for multi-modal interaction and uni-modal Conversational Interfaces to see how they apply in our context. Then, we validate our results with an empirical evaluation. Our work sheds light on a largely unexplored field and inspires the future design of such interfaces.
SP  - 64
EP  - 77
JF  - Chatbot Research and Design
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-68288-0_5
ER  - 

TY  - JOUR
AU  - Leng, Jiaye; Wang, Lili; Liu, Xiaolong; Shi, Xuehuai; Wang, Miao
TI  - Efficient Flower Text Entry in Virtual Reality.
PY  - 2022
AB  - Text entry is a frequently used task in virtual reality (VR) applications, and controller is the most common interactive device in current VR systems. However, in terms of typing speed, there is still a gap between the existing controller-based text entry techniques and using a physical keyboard in reality, so it is important to improve the efficiency of the controller-based text entry. In this paper, we introduce Flower Text Entry, a single-controller text entry method based on a newly designed flower-shaped keyboard using hand 3D translation interaction for letters selection. We conduct user studies to optimize the keyboard design and the mapping between the interaction and selection, so as to evaluate our method. The results show that our method has high typing speed, lower error rate, and is very friendly to novices compared with the state-of-the-art controller-based text entry methods. After a short training, the novice group can type at 17.65 words per minute (WPM), and the potential expert group can type at 22.97 WPM. The highest typing speed is up to 30.80 WPM achieved by a potential expert participant.
SP  - 3662
EP  - 3672
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203101
ER  - 

TY  - NA
AU  - Abtahi, Parastoo; Hough, Sidney Q.; Landay, James A.; Follmer, Sean
TI  - Beyond Being Real: A Sensorimotor Control Perspective on Interactions in Virtual Reality
PY  - 2022
AB  - We can create Virtual Reality (VR) interactions that have no equivalent in the real world by remapping spacetime or altering users' body representation, such as stretching the user's virtual arm for manipulation of distant objects or scaling up the user's avatar to enable rapid locomotion. Prior research has leveraged such approaches, what we call beyond-real techniques, to make interactions in VR more practical, efficient, ergonomic, and accessible. We present a survey categorizing prior movement-based VR interaction literature as reality-based, illusory, or beyond-real interactions. We survey relevant conferences (CHI, IEEE VR, VRST, UIST, and DIS) while focusing on selection, manipulation, locomotion, and navigation in VR. For beyond-real interactions, we describe the transformations that have been used by prior works to create novel remappings. We discuss open research questions through the lens of the human sensorimotor control system and highlight challenges that need to be addressed for effective utilization of beyond-real interactions in future VR applications, including plausibility, control, long-term adaptation, and individual differences.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517706
ER  - 

TY  - CHAP
AU  - Li, Toby Jia-Jun; Mitchell, Tom M.; Myers, Brad A.
TI  - Demonstration + Natural Language: Multimodal Interfaces for GUI-based Interactive Task Learning Agents
PY  - 2021
AB  - We summarize our past five years of work on designing, building, and studying Sugilite, an interactive task learning agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations leveraging the graphical user interfaces (GUIs) of third-party mobile apps. Through its multi-modal and mixed-initiative approaches for Human-AI interaction, Sugilite made important contributions in improving the usability, applicability, generalizability, flexibility, robustness, and shareability of interactive task learning agents. Sugilite also represents a new human-AI interaction paradigm for interactive task learning, where it uses existing app GUIs as a medium for users to communicate their intents with an AI agent instead of the interfaces for users to interact with the underlying computing services. In this chapter, we describe the Sugilite system, explain the design and implementation of its key features, and show a prototype in the form of a conversational assistant on Android.
SP  - 495
EP  - 537
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_15
ER  - 

TY  - NA
AU  - Qian, Jing; Zhou, Tongyu; Young-Ng, Meredith; Ma, Jiaju; Cheung, Angel; Li, Xiangyu; Gonsher, Ian; Huang, Jeff
TI  - Conference on Designing Interactive Systems - Portalware: Exploring Free-Hand AR Drawing with a Dual-Display Smartphone-Wearable Paradigm
PY  - 2021
AB  - Free-hand interaction enables users to directly create artistic augmented reality content using a smartphone, but lacks natural spatial depth information due to the small 2D display’s limited visual feedback. Through an autobiographical design process, three authors explored free-hand drawing over a total of 14 weeks. During this process, they expanded the design space from a single-display smartphone format to a dual-display smartphone-wearable format (Portalware). This new configuration extends the virtual content from a smartphone to a wearable display and enables multi-display free-hand interactions. The authors documented experiences where 1) the display extends the smartphone’s canvas perceptually, allowing the authors to work beyond the smartphone screen view; 2) the additional perspective mitigates the difficulties of depth perception and improves the usability of direct free-hand manipulation; 3) the wearable use cases depend on the nature of the drawing, such as: replicating physical objects, “in-situ” mixed reality pieces, and multi-planar drawings.
SP  - 205
EP  - 219
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462098
ER  - 

TY  - JOUR
AU  - Guo, Anhong; Kamar, Ece; Vaughan, Jennifer Wortman; Wallach, Hanna; Morris, Meredith Ringel
TI  - Toward fairness in AI for people with disabilities SBG@a research roadmap
PY  - 2020
AB  - AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.
SP  - 1
EP  - 1
JF  - ACM SIGACCESS Accessibility and Computing
VL  - 125
IS  - 125
PB  - 
DO  - 10.1145/3386296.3386298
ER  - 

TY  - JOUR
AU  - Kornienko, V. Y.; Minaev, M. Y.
TI  - TRENDS IN THE DEVELOPMENT OF 3D FOOD PRINTING
PY  - 2022
AB  - This article reviews the currently existing schemes of 3D printers assigned for printing with edible components. The main types of extruders used for printing with food mixtures are considered in the article. Promising components are described, which serve as a base for creating the mixtures for 3D food printing. Examples of the successful application of 3D printing to create food products are given. The concept of 4D printing is considered.
SP  - 23
EP  - 29
JF  - Food systems
VL  - 5
IS  - 1
PB  - 
DO  - 10.21323/2618-9771-2022-5-1-23-29
ER  - 

TY  - NA
AU  - Xu, Zheer; Chen, Weihao; Zhao, Dongyang; Luo, Jiehui; Wu, Te-Yen; Gong, Jun; Yin, Sicheng; Zhai, Jialun; Yang, Xing-Dong
TI  - CHI - BiTipText: Bimanual Eyes-Free Text Entry on a Fingertip Keyboard
PY  - 2020
AB  - We present a bimanual text input method on a miniature fingertip keyboard, that invisibly resides on the first segment of a user's index finger on both hands. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The design of our keyboard layout followed an iterative process, where we first conducted a study to understand the natural expectation of the handedness of the keys in a QWERTY layout for users. Among a choice of 67,108,864 design variations, we identified 1295 candidates offering a good satisfaction for user expectations. Based on these results, we computed an optimized bimanual keyboard layout, while considering the joint optimization problems of word ambiguity and movement time. Our user evaluation revealed that participants achieved an average text entry speed of 23.4 WPM.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376306
ER  - 

TY  - NA
AU  - Arabi, Abul Al; Li, Jiahao; Chen, Xiang 'Anthony; Kim, Jeeeun
TI  - Mobiot: Augmenting Everyday Objects into Moving IoT Devices Using 3D Printed Attachments Generated by Demonstration
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517645
ER  - 

TY  - JOUR
AU  - Barhoush, Yazan A M; Nanjappan, Vijayakumar; Thiel, Felix J.; Georgiev, Georgi V.; Swapp, David; Loudon, Brian
TI  - A NOVEL EXPERIMENTAL DESIGN OF A REAL-TIME VR TRACKING DEVICE
PY  - 2021
AB  - Virtual Reality (VR) is progressively adopted at different stages of design and product development. Consequently, evolving interaction requirements in engineering design and development for VR are essential for technology adoption. One of these requirements is real-time positional tracking. This paper aims to present an experimental design of a new real-time positional tracking device (tracker), that is more compact than the existing solution, while addressing factors such as wearability and connectivity. We compare the simulation of the proposed device and the existing solution, discuss the results, and the limitations. The new experimental shape of the device is tailored towards research, allowing the engineering designer to take advantage of a new tracker alternative in new ways, and opens the door to new VR applications in research and product development.
SP  - 171
EP  - 180
JF  - Proceedings of the Design Society
VL  - 1
IS  - NA
PB  - 
DO  - 10.1017/pds.2021.18
ER  - 

TY  - NA
AU  - Lilija, Klemen; Kyllingsbæk, Søren; Hornbæk, Kasper
TI  - VR - Correction of Avatar Hand Movements Supports Learning of a Motor Skill
PY  - 2021
AB  - Learning to move the hands in particular ways is essential in many training andleisure virtual reality applications, yet challenging. Existing techniques that support learning of motor movement in virtual reality rely on external cues such as arrows showing where to move or transparent hands showing the target movement. We propose a technique where the avatar's hand movement is corrected to be closer to the target movement. This embeds guidance in the user's avatar, instead of in external cues and minimizes visual distraction. Through two experiments, we found that such movement guidance improves the short-term retention of the target movement when compared to a control condition without guidance.
SP  - 455
EP  - 462
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00069
ER  - 

TY  - NA
AU  - Luo, Danli; Yang, Humphrey; Khurana, Malika; Qian, Kuanren; Yao, Lining
TI  - CHI Extended Abstracts - Demonstrating Freeform Fabrication of Fluidic Edible Materials
PY  - 2021
AB  - From providing nutrition to being social platforms, food plays an essential role in our daily lives and cultures. In this work, we are interested in using food as an interaction medium and a context of personal fabrication with an expanded design space enabled by support bath-assisted printing. The bath scaffolds the embedded materials while preserving shapes during the printing processes and allows us to create freeform food with fluid-like materials. Coupled with different post-processing and cooking methods, this technique grants the versatility of food printing with fluidic materials. We will demo confectionery arts and dishes designed by our software tool.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451538
ER  - 

TY  - CHAP
AU  - , 
TI  - Acknowledgments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544569
ER  - 

TY  - JOUR
AU  - Lee, Ahreum; Jo, Wonse; Kannan, Shyam Sundar; Min, Byung-Cheol
TI  - Investigating the Effect of Deictic Movements of a Multi-Robot
PY  - 2020
AB  - While research on human-robot interaction is ongoing as robots become more readily available and easier to use, the study of interactions between a human and a team of multiple robots represents a ...
SP  - 197
EP  - 210
JF  - International Journal of Human–Computer Interaction
VL  - 37
IS  - 3
PB  - 
DO  - 10.1080/10447318.2020.1812908
ER  - 

TY  - NA
AU  - Davis, Josh Urban; Wu, Te-Yen; Shi, Bo; Lu, Hanyi; Panotopoulou, Athina; Whiting, Emily; Yang, Xing-Dong
TI  - CHI - TangibleCircuits: An Interactive 3D Printed Circuit Education Tool for People with Visual Impairments
PY  - 2020
AB  - We present a novel haptic and audio feedback device that allows blind and visually impaired (BVI) users to understand circuit diagrams. TangibleCircuits allows users to interact with a 3D printed tangible model of a circuit which provides audio tutorial directions while being touched. Our system comprises an automated parsing algorithm which extracts 3D printable models as well as an audio interfaces from a Fritzing diagram. To better understand the requirements of designing technology to assist BVI users in learning hardware computing, we conducted a series of formative inquiries into the accessibility limitations of current circuit tutorial technologies. In addition, we derived insights and design considerations gleaned from conducting a formal comparative user study to understand the effectiveness of TangibleCircuits as a tutorial system. We found that BVI users were better able to understand the geometric, spatial and structural circuit information using TangibleCircuits, as well as enjoyed learning with our tool.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376513
ER  - 

TY  - JOUR
AU  - Stemasov, Evgeny; Rukzio, Enrico; Gugenheimer, Jan
TI  - The Road to Ubiquitous Personal Fabrication: Modeling-Free Instead of Increasingly Simple
PY  - 2021
AB  - The tools for personal digital fabrication (DF) are on the verge of reaching mass-adoption beyond technology enthusiasts, empowering consumers to fabricate personalized artifacts. We argue that to achieve similar outreach and impact as personal computing, personal fabrication research may have to venture beyond ever-simpler interfaces for creation, toward lowest-effort workflows for remixing. We surveyed novice-friendly DF workflows from the perspective of HCI. Through this survey, we found two distinct approaches for this challenge: 1) simplifying expert modeling tools (AutoCAD →Tinkercad) and 2) enriching tools not involving primitive-based modeling with powerful customization (e.g., Thingiverse). Drawing parallel to content creation domains such as photography, we argue that the bulk of content is created via remixing (2). In this article, we argue that to be able to include the majority of the population in DF, research should embrace omission of workflow steps, shifting toward automation, remixing, and templates, instead of modeling from the ground up.
SP  - 19
EP  - 27
JF  - IEEE Pervasive Computing
VL  - 20
IS  - 1
PB  - 
DO  - 10.1109/mprv.2020.3029650
ER  - 

TY  - NA
AU  - Misztal, Sebastian; Carbonell, Guillermo; Schild, Jonas
TI  - Experiencing Age-Related Movement Impairment Through Visual Delegation in VR Can Substitute Haptic Impairments of an Age Simulation Suit
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE 10th International Conference on Serious Games and Applications for Health(SeGAH)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/segah54908.2022.9978590
ER  - 

TY  - NA
AU  - Kiaghadi, Ali; Huang, Jin; Homayounfar, Seyedeh Zohreh; Andrew, Trisha; Ganesan, Deepak
TI  - FabToys
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3498361.3538931
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Chen, Jingya; Mitchell, Tom M.; Myers, Brad A.
TI  - Towards Effective Human-AI Collaboration in GUI-Based Interactive Task Learning Agents
PY  - 2020
AB  - We argue that a key challenge in enabling usable and useful interactive task learning for intelligent agents is to facilitate effective Human-AI collaboration. We reflect on our past 5 years of efforts on designing, developing and studying the SUGILITE system, discuss the issues on incorporating recent advances in AI with HCI principles in mixed-initiative interactions and multi-modal interactions, and summarize the lessons we learned. Lastly, we identify several challenges and opportunities, and describe our ongoing work
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Patras, Cristian; Cibulskis, Mantas; Nilsson, Niels Christian
TI  - Body Warping Versus Change Blindness Remapping: A Comparison of Two Approaches to Repurposing Haptic Proxies for Virtual Reality
PY  - 2022
AB  - When using tangible props as proxies for virtual objects, it is important that these haptic proxies are similar to and co-located with their virtual counterparts. This makes it challenging to scale virtual scenarios because more proxies are needed as scenarios grow more complex. Haptic retargeting, or virtual remapping, makes it possible to repurpose the same physical prop as a proxy for multiple virtual objects. This paper details a user study comparing two techniques for repurposing haptic proxies; namely haptic retargeting based on body warping and change blindness remapping. Participants performed a simple button-pressing task, and 24 virtual buttons were mapped onto four haptic proxies with varying degrees of misalignment. Body warping and change blindness remapping were used to realign the real and virtual buttons, and the results indicate that users failed to reliably detect realignment of up to 7.9 cm for body warping and up to 9.7 cm for change blindness remapping. Moreover, change blindness remapping yielded significantly higher self-reported agency, and marginally higher ownership. Taken together these results suggest that this less explored technique has potential when it comes to repurposing haptic proxies for virtual reality.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00039
ER  - 

TY  - NA
AU  - Houben, Maarten; Brankaert, Rens; Kenning, Gail; Bongers, Inge; Eggen, Berry
TI  - Designing for Everyday Sounds at Home with People with Dementia and their Partners
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501993
ER  - 

TY  - JOUR
AU  - Li, Toby Jia-Jun; Chen, Jingya; Canfield, Brandon; Myers, Brad A.
TI  - Privacy-Preserving Script Sharing in GUI-based Programming-by-Demonstration Systems
PY  - 2020
AB  - An important concern in end user development (EUD) is accidentally embedding personal information in program artifacts when sharing them. This issue is particularly important in GUI-based programming-by-demonstration (PBD) systems due to the lack of direct developer control of script contents. Prior studies reported that these privacy concerns were the main barrier to script sharing in EUD. We present a new approach that can identify and obfuscate the potential personal information in GUI-based PBD scripts based on the uniqueness of information entries with respect to the corresponding app GUI context. Compared with the prior approaches, ours supports broader types of personal information beyond explicitly pre-specified ones, requires minimal user effort, addresses the threat of re-identification attacks, and can work with third-party apps from any task domain. Our approach also recovers obfuscated fields locally on the script consumer's side to preserve the shared scripts' transparency, readability, robustness, and generalizability. Our evaluation shows that our approach (1) accurately identifies the potential personal information in scripts across different apps in diverse task domains; (2) allows end-user developers to feel comfortable sharing their own scripts; and (3) enables script consumers to understand the operation of shared scripts despite the obfuscated fields.
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW1
PB  - 
DO  - 10.1145/3392869
ER  - 

TY  - JOUR
AU  - Ruffieux, Simon; Hwang, Chiwoong; Junod, Vincent; Caldara, Roberto; Lalanne, Denis; Ruffieux, Nicolas
TI  - Tailoring assistive smart glasses according to pathologies of visually impaired individuals: an exploratory investigation on social needs and difficulties experienced by visually impaired individuals
PY  - 2021
AB  - <jats:title>Abstract</jats:title><jats:p>Recent advances in the field of assistive devices technology represent a great opportunity for improving the quality of life of people with moderate to severe visual impairment. However, it is still unclear what are the precise daily difficulties, needs and expectations of the smart glasses technology for visually impaired individuals. To this aim, we conducted a survey based on three questionnaires to provide qualitative and quantitative insights on those questions across five groups suffering from various visual pathologies (<jats:inline-formula><jats:alternatives><jats:tex-math>$$N=50$$</jats:tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"> <mml:mrow> <mml:mi>N</mml:mi> <mml:mo>=</mml:mo> <mml:mn>50</mml:mn> </mml:mrow> </mml:math></jats:alternatives></jats:inline-formula>). The results clearly showed the importance of developing tailored solutions to fulfill the heterogeneous daily difficulties and needs identified across pathologies. Overall, groups shared similar expectations regarding the assistive smart glasses functionalities in order to improve social interactions.</jats:p>
SP  - NA
EP  - NA
JF  - Universal Access in the Information Society
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10209-021-00857-5
ER  - 

TY  - NA
AU  - Van Brummelen, Jessica; Weng, Kevin; Lin, Phoebe; Yeo, Catherine
TI  - Convo: What does conversational programming need? An exploration of machine learning interface design.
PY  - 2020
AB  - Vast improvements in natural language understanding and speech recognition have paved the way for conversational interaction with computers. While conversational agents have often been used for short goal-oriented dialog, we know little about agents for developing computer programs. To explore the utility of natural language for programming, we conducted a study ($n$=45) comparing different input methods to a conversational programming system we developed. Participants completed novice and advanced tasks using voice-based, text-based, and voice-or-text-based systems. We found that users appreciated aspects of each system (e.g., voice-input efficiency, text-input precision) and that novice users were more optimistic about programming using voice-input than advanced users. Our results show that future conversational programming tools should be tailored to users' programming experience and allow users to choose their preferred input mode. To reduce cognitive load, future interfaces can incorporate visualizations and possess custom natural language understanding and speech recognition models for programming.
SP  - 1
EP  - 5
JF  - 2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vl/hcc50065.2020.9127277
ER  - 

TY  - NA
AU  - Miyatake, Yamato; Punpongsanon, Parinya; Iwai, Daisuke; Sato, Kosuke
TI  - interiqr: Unobtrusive Edible Tags using Food 3D Printing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545669
ER  - 

TY  - JOUR
AU  - Gonzalez, Jesse T.; Hudson, Scott E.
TI  - Layer by Layer, Patterned Valves Enable Programmable Soft Surfaces
PY  - 2022
AB  - <jats:p>Programmable surfaces, which can be instructed to alter their shape or texture, may one day serve as a platform for tangible interfaces and adaptive environments. But so far, these structures have been constrained in scale by a challenging fabrication process, as the numerous constituent actuators must be built and assembled individually. We look towards emerging trends in mechanical engineering and consider an alternate framework --- layer-driven design, which enables the production of dynamic, discretely-actuated surfaces at multiple scales. By centering the construction around patterning and stacking, forgoing individual assembly in favor of bulk processes such as photo-etching and laser cutting, we avoid the need for multiple manufacturing steps that are repeated for each of the many actuators that compose the surface. As an instance of this layer-driven model, we build an array of electrostatic valves, and use this composite material (which we refer to as Stoma-Board) to drive four types of pneumatic transducers. We also show how this technique may be readily industrialized, through integration with the highly mature and automated manufacturing processes of modern electronics.</jats:p>
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 1
PB  - 
DO  - 10.1145/3517251
ER  - 

TY  - NA
AU  - Wu, Te-Yen; Yang, Xing-Dong
TI  - iWood: Makeable Vibration Sensor for Interactive Plywood
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545640
ER  - 

TY  - JOUR
AU  - Aghayi, Emad; LaToza, Thomas D.
TI  - A controlled experiment on the impact of microtasking on programming
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Empirical Software Engineering
VL  - 28
IS  - 1
PB  - 
DO  - 10.1007/s10664-022-10226-2
ER  - 

TY  - JOUR
AU  - He, Chang; Zhang, Min; Devahastin, Sakamon
TI  - Microwave-induced deformation behaviors of 4D printed starch-based food products as affected by edible salt and butter content
PY  - 2021
AB  - NA
SP  - 102699
EP  - NA
JF  - Innovative Food Science & Emerging Technologies
VL  - 70
IS  - NA
PB  - 
DO  - 10.1016/j.ifset.2021.102699
ER  - 

TY  - NA
AU  - Liu, Michael Xieyang; Kittur, Aniket; Myers, Brad A.
TI  - Crystalline: Lowering the Cost for Developers to Collect and Organize Information for Decision Making
PY  - 2022
AB  - Developers perform online sensemaking on a daily basis, such as researching and choosing libraries and APIs. Prior research has introduced tools that help developers capture information from various sources and organize it into structures useful for subsequent decision-making. However, it remains a laborious process for developers to manually identify and clip content, maintaining its provenance and synthesizing it with other content. In this work, we introduce a new system called Crystalline that attempts to automatically collect and organize information into tabular structures as the user searches and browses the web. It leverages natural language processing to automatically group similar criteria together to reduce clutter as well as passive behavioral signals such as mouse movement and dwell time to infer what information to collect and how to visualize and prioritize it. Our user study suggests that developers are able to create comparison tables about 20% faster with a 60% reduction in operational cost without sacrificing the quality of the tables.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501968
ER  - 

TY  - NA
AU  - Rodin, Ivan; Furnari, Antonino; Mavroedis, Dimitrios; Farinella, Giovanni Maria
TI  - Predicting the Future from First Person (Egocentric) Vision: A Survey
PY  - 2021
AB  - Egocentric videos can bring a lot of information about how humans perceive the world and interact with the environment, which can be beneficial for the analysis of human behaviour. The research in egocentric video analysis is developing rapidly thanks to the increasing availability of wearable devices and the opportunities offered by new large-scale egocentric datasets. As computer vision techniques continue to develop at an increasing pace, the tasks related to the prediction of future are starting to evolve from the need of understanding the present. Predicting future human activities, trajectories and interactions with objects is crucial in applications such as human-robot interaction, assistive wearable technologies for both industrial and daily living scenarios, entertainment and virtual or augmented reality. This survey summarises the evolution of studies in the context of future prediction from egocentric vision making an overview of applications, devices, existing problems, commonly used datasets, models and input modalities. Our analysis highlights that methods for future prediction from egocentric vision can have a significant impact in a range of applications and that further research efforts should be devoted to the standardisation of tasks and the proposal of datasets considering real-world scenarios such as the ones with an industrial vocation.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CHAP
AU  - Papanikolaou, Dimitris
TI  - Design, Control, Actuation and Modeling Approaches for Large-Scale Transformable Inflatables
PY  - 2022
AB  - NA
SP  - 305
EP  - 319
JF  - Towards Radical Regeneration
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-13249-0_26
ER  - 

TY  - JOUR
AU  - He, Chang; Zhang, Min; Devahastin, Sakamon
TI  - Investigation on Spontaneous Shape Change of 4D Printed Starch-Based Purees from Purple Sweet Potatoes As Induced by Microwave Dehydration
PY  - 2020
AB  - The time evolution of three-dimensional (3D) printed food structures as affected by their composition and postprinting stimulus is an area of research that has recently received increasing attention. In this study, the spontaneous shape change of 3D printed purple sweet potato purees of different formulations as triggered by microwave dehydration was investigated. The rheological properties, water distribution behavior, and dielectric properties of the purees were first studied. Addition of salt reduced the viscosity, storage modulus, loss modulus, and yield stress but increased the relaxation time of the purees. Addition of fructose syrup resulted in opposite results. Addition of both salt and syrup decreased the dielectric constant but increased the dielectric loss of the purees. Increased microwave power and salt content increased the rates of dehydration and deformation but decreased the maximum deformation degree of the printed samples. The syrup also decreased the maximum deformation degree. A desirable deformation pattern could also be achieved by manipulating the infill parameters. Transformation of two-dimensional planar flowers and butterflies into 3D configurations as a result of varying the aforementioned parameters is illustrated. The proposed technique to induce spontaneous shape change of a 3D printed starch-based product should lay a foundation for further application of four-dimensional food printing.
SP  - 37896
EP  - 37905
JF  - ACS applied materials & interfaces
VL  - 12
IS  - 34
PB  - 
DO  - 10.1021/acsami.0c10899
ER  - 

TY  - NA
AU  - Sun, Lingyun; Yang, Yue; Chen, Yu; Li, Jiaji; Luo, Danli; Liu, Haolin; Yao, Lining; Tao, Ye; Wang, Guanyun
TI  - CHI - ShrinCage: 4D Printing Accessories that Self-Adapt
PY  - 2021
AB  - 3D printing technology makes Do-It-Yourself and reforming everyday objects a reality. However, designing and fabricating attachments that can seamlessly adapt existing objects to extended functionality is a laborious process, which requires accurate measuring, modeling, manufacturing, and assembly. This paper presents ShrinCage, a 4D printing system that allows novices to easily create shrinkable adaptations to fit and fasten existing objects. Specifically, the design tool presented in this work aid in the design of attachment that adapts to irregular morphologies, which accommodates the variations in measurements and fabrication, subsequently simplifying the modeling and assembly processes. We further conduct mechanical tests and user studies to evaluate the availability and feasibility of this method. Numerous application examples created by ShrinCage prove that it can be adopted by aesthetic modification, assistive technology, repair, upcycling, and augmented 3D printing.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445220
ER  - 

TY  - NA
AU  - Htike, Hein Min; Margrain, Thomas Hengist; Lai, Yu-Kun; Eslambolchilar, Parisa
TI  - CHI - Augmented Reality Glasses as an Orientation and Mobility Aid for People with Low Vision: a Feasibility Study of Experiences and Requirements
PY  - 2021
AB  - People with low vision experience reduced mobility that affects their physical and mental wellbeing. With augmented reality (AR) glasses, there are new opportunities to provide visual and auditory information that can improve mobility for this vulnerable group. Current research into AR-based mobility aids has focused mainly on the technical aspects, and less emphasis has been placed on understanding the usability and suitability of these aids in people with various levels of visual impairment. In this paper, we present the results of qualitative interviews with 18 participants using HoloLens v1 and eight prototype augmentations to understand how these enhancements are perceived by people with low vision and how these aids should be adjusted to suit their needs. Our results suggested that participants with moderate vision loss could potentially perceive the most benefit from glasses and underlined the importance of extensive customizability to accommodate the needs of a highly varied low vision population.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445327
ER  - 

TY  - NA
AU  - Xu, Ruopeng; Sato, Toshiki; Miyafuji, Shio; Koike, Hideki
TI  - OmniLantern: Design and Implementations of a Portable and Coaxial Omnidirectional Projector-Camera System
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3531073.3531126
ER  - 

TY  - JOUR
AU  - Williams, Alex C.; Mark, Gloria; Milland, Kristy; Lank, Edward; Law, Edith
TI  - The Perpetual Work Life of Crowdworkers: How Tooling Practices Increase Fragmentation in Crowdwork
PY  - 2019
AB  - Crowdworkers regularly support their work with scripts, extensions, and software to enhance their productivity. Despite their evident significance, little is understood regarding how these tools affect crowdworkers' quality of life and work. In this study, we report findings from an interview study (N=21) aimed at exploring the tooling practices used by full-time crowdworkers on Amazon Mechanical Turk. Our interview data suggests that the tooling utilized by crowdworkers (1) strongly contributes to the fragmentation of microwork by enabling task switching and multitasking behavior; (2) promotes the fragmentation of crowdworkers' work-life boundaries by relying on tooling that encourages a 'work-anywhere' attitude; and (3) aids the fragmentation of social ties within worker communities through limited tooling access. Our findings have implications for building systems that unify crowdworkers' work practice in support of their productivity and well-being.
SP  - 24
EP  - 28
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 3
IS  - CSCW
PB  - 
DO  - 10.1145/3359126
ER  - 

TY  - NA
AU  - Kourtesis, Panagiotis; Argelaguet, Ferran; Vizcay, Sebastian; Marchal, Maud; Pacchierotti, Claudio
TI  - Electrotactile feedback for hand interactions: A systematic review, meta-analysis, and future directions
PY  - 2021
AB  - Haptic feedback is critical in a broad range of human-machine/computer-interaction applications. However, the high cost and low portability/wearability of haptic devices remains an unresolved issue, severely limiting the adoption of this otherwise promising technology. Electrotactile interfaces have the advantage of being more portable and wearable due to its reduced actuators’ size, as well as benefiting from lower power consumption and manufacturing cost. The usages of electrotactile feedback have been explored in human-computer interaction and human machine-interaction for facilitating hand-based interactions in applications such as prosthetics, virtual reality, robotic teleoperation, surface haptics, portable devices, and rehabilitation. This paper presents a systematic review and meta-analysis of electrotactile feedback systems for hand based interactions in the last decade. We categorize the different electrotactile systems according to their type of stimulation and implementation/application. We also present and discuss a quantitative congregation of the findings, so as to offer a high-level overview into the state-of-art and suggest future directions. Electrotactile feedback was successful in rendering and/or augmenting most tactile sensations, eliciting perceptual processes, and improving performance in many scenarios, especially in those where the wearability/portability of the system is important. However, knowledge gaps, technical drawbacks, and methodological limitations were detected, which should be addressed in future studies.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - 10.36227/techrxiv.14588250.v1
ER  - 

TY  - NA
AU  - Strandholt, Patrick L.; Dogaru, Oana A.; Nilsson, Niels Christian; Nordahl, Rolf; Serafin, Stefania
TI  - CHI - Knock on Wood: Combining Redirected Touching and Physical Props for Tool-Based Interaction in Virtual Reality
PY  - 2020
AB  - When physical props serve as proxies for virtual tools used to manipulate the virtual environment, it is challenging to provide appropriate haptic feedback. Redirected tool-mediated manipulation addresses this challenge by distorting the mapping between physical and virtual tools to provide a sensation of manipulating the virtual environment, when the physical tool comes into contact with another physical prop. For example, a virtual hammer's position can be offset to ensure that physical impacts accompany each strike of a virtual nail. We demonstrate the idea by showing that it can be used to create sensations of impact and resistance when driving a virtual nail into a surface, when tightening a virtual screw, and when sawing through a virtual plank. The results of a user study indicate that the proposed approach is perceived as more realistic than interaction with a single physical prop or controller and no notable detriments to precision were observed.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376303
ER  - 

TY  - JOUR
AU  - Pramod, Dhanya
TI  - Assistive Technology for Elderly People: State of the Art Review and Future Research Agenda
PY  - 2022
AB  - NA
SP  - 1
EP  - 34
JF  - Science & Technology Libraries
VL  - NA
IS  - NA
PB  - 
DO  - 10.1080/0194262x.2021.2024481
ER  - 

TY  - NA
AU  - Hubenschmid, Sebastian; Wieland, Jonathan; Fink, Daniel Immanuel; Batch, Andrea; Zagermann, Johannes; Elmqvist, Niklas; Reiterer, Harald
TI  - ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517550
ER  - 

TY  - CHAP
AU  - , 
TI  - Theories of Embodiment
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544573
ER  - 

TY  - NA
AU  - Alhafnawi, Merihan; Hunt, Edmund R.; Lemaignan, Severin; O'Dowd, Paul; Hauert, Sabine
TI  - MOSAIX: a Swarm of Robot Tiles for Social Human-Swarm Interaction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra46639.2022.9811723
ER  - 

TY  - NA
AU  - Jain, Harshika; Chen, Melinda; Collins, Alisha; Yao, Lining
TI  - Designing Morphing Artifacts for Creative STEM Explorations
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3531203
ER  - 

TY  - JOUR
AU  - Makiguchi, Motohiro; Takada, Hideaki; Kawakami, Tohru; Sasai, Mutsumi
TI  - 33-2: Improving Image Quality of 360-degree Tabletop 3D Screen System
PY  - 2020
AB  - NA
SP  - 470
EP  - 473
JF  - SID Symposium Digest of Technical Papers
VL  - 51
IS  - 1
PB  - 
DO  - 10.1002/sdtp.13907
ER  - 

TY  - NA
AU  - Ye, Huizhong; Lee, Chi-Jung; Wu, Te-Yen; Yang, Xing-Dong; Chen, Bing-Yu; Liang, Rong-Hao
TI  - Body-Centric NFC: Body-Centric Interaction with NFC Devices Through Near-Field Enabled Clothing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3534569
ER  - 

TY  - NA
AU  - Li, Yuan; Lee, Sang Won; Bowman, Doug A.; Hicks, David; Lages, Wallace santos; Sharma, Akshay
TI  - ARCritique: Supporting Remote Design Critique of Physical Artifacts through Collaborative Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567700
ER  - 

TY  - JOUR
AU  - Li, Franklin Mingzhe; Lu, Cheng; Lu, Zhicong; Carrington, Patrick; Truong, Khai N.
TI  - An Exploration of Captioning Practices and Challenges of Individual Content Creators on YouTube for People with Hearing Impairments
PY  - 2022
AB  - <jats:p>Deaf and Hard-of-Hearing (DHH) audiences have long complained about caption qualities for many online videos created by individual content creators on video-sharing platforms (e.g., YouTube). However, there lack explorations of practices, challenges, and perceptions of online video captions from the perspectives of both individual content creators and DHH audiences. In this work, we first explore DHH audiences' feedback on and reactions to YouTube video captions through interviews with 13 DHH individuals, and uncover DHH audiences' experiences, challenges, and perceptions on watching videos created by individual content creators (e.g., manually added caption tags could create additional confidence and trust in caption qualities for DHH audiences). We then discover individual content creators' practices, challenges, and perceptions on captioning their videos (e.g., back-captioning problems) by conducting a YouTube video analysis with 189 captioning-related YouTube videos, followed by a survey with 62 individual content creators. Overall, our findings provide an in-depth understanding of captions generated by individual content creators and bridge the knowledge gap mutually between content creators and DHH audiences on captions.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512922
ER  - 

TY  - CONF
AU  - Avellino, Ignacio; Nozari, Sheida; Canlorbe, Geoffroy; Jansen, Yvonne
TI  - Surgical Video Summarization: Multifarious Uses, Summarization Process and Ad-Hoc Coordination
PY  - 2021
AB  - While surgical videos are valuable support material for activities around surgery, their summarization demands great amounts of time from surgeons, resulting in the production of very few videos. We study the practices involving surgical video to motivate and inform the future design of tools for their summarization. Through interviews and observations in a field study, we find that (1) video summaries provide an important support for surgery, being used for self-improvement, education, discussing cases, scientific research, patient communication and as legal resources; (2) video summarization follows a process hindered by the loss of knowledge that originates during recording; and, (3) surgeons develop ad-hoc coordination strategies which involve using the video itself for articulation work, making it both the field of work and coordination artifact. We discuss ways in which tools can facilitate capturing knowledge during live action using these strategies.
SP  - 1
EP  - 23
JF  - NA
VL  - 5
IS  - 4
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Chen, Yan; Grossman, Tovi
TI  - UIST - Umitation: Retargeting UI Behavior Examples for Website Design
PY  - 2021
AB  - Interface designers often refer to UI behavior examples found in the wild (e.g., commercial websites) for reference or design inspiration. While past research has looked at retargeting interface and webpage design, limited work has explored the challenges in retargeting interactive visual behaviors. We introduce Umitation, a system that helps designers extract, edit, and adapt example front-end UI behaviors to target websites. Umitation can also help designers specify the desired behaviors and reconcile their intended interaction details with their existing UI. In a qualitative evaluation, we found evidence that Umitation helps participants extract and retarget dynamic front-end UI behavior examples quickly and expressively.
SP  - 922
EP  - 935
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474796
ER  - 

TY  - NA
AU  - Vatavu, Radu-Daniel; Bilius, Laura-Bianca
TI  - UIST - GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready Devices
PY  - 2021
AB  - Despite an exciting area with many promises for innovations in wearable interactive systems, research on interaction techniques for smart rings lacks structured knowledge and readily-available resources for designers to systematically attain such innovations. In this work, we conduct a systematic literature review of ring-based gesture input, from which we extract key results and a large set of gesture commands for ring, ring-like, and ring-ready devices. We use these findings to deliver GestuRING, our web-based tool to support design of ring-based gesture input. GestuRING features a searchable gesture-to-function dictionary of 579 records with downloadable numerical data files and an associated YouTube video library. These resources are meant to assist the community in attaining further innovations in ring-based gesture input for interactive systems.
SP  - 710
EP  - 723
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474780
ER  - 

TY  - JOUR
AU  - Chen, Xiaohuan; Zhang, Min; Teng, Xiuxiu; Mujumdar, Arun S.
TI  - Recent Progress in Modeling 3D/4D Printing of Foods
PY  - 2021
AB  - Based on the design of printing models, three-dimensional (3D) and four-dimensional (4D) printing technologies have shown extensive and promising application potential in the food industry. The majority of previous researches on printing models focus on single or multiple models to test the performance of printers and inks, assess the influence of printing parameters on product performance, and print new products. This review compares the differences between the recently proposed 3D/4D printing models and summarizes the key factors needing to be considered in model design. The solid models are mainly used to explore printing parameters, while the filling models are used to study the texture characteristics of food. Models with changing shapes or colors reflect the importance of model structural design. The reasons for distortion in the process of transition from digital models to food models are analyzed, and the corresponding solutions are proposed. In the future, it is necessary to expand model database and develop cloud platform services so as to facilitate the sharing of related resources and strengthen the personalized nutrition of different consumer groups.
SP  - 1
EP  - 14
JF  - Food Engineering Reviews
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Kong, Junhan; Sabha, Dena; Bigham, Jeffrey P.; Pavel, Amy; Guo, Anhong
TI  - SUI - TutorialLens: Authoring Interactive Augmented Reality Tutorials Through Narration and Demonstration
PY  - 2021
AB  - Exploring unfamiliar devices and interfaces through trial and error can be challenging and frustrating. Existing video tutorials require frequent context switching between the device showing the tutorial and the device being used. While augmented reality (AR) has been adopted to create user manuals, many are inflexible for diverse tasks, and usually require programming and AR development experience. We present TutorialLens, a system for authoring interactive AR tutorials through narration and demonstration. To use TutorialLens, authors demonstrate tasks step-by-step while verbally explaining what they are doing. TutorialLens automatically detects and records 3D finger positions and guides authors to capture important changes of the device. Using the created tutorials, TutorialLens then provides AR visual guidance and feedback for novice device users to complete the demonstrated tasks. TutorialLens is automated, friendly to users without AR development experience, and applicable to a variety of devices and tasks.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485279.3485289
ER  - 

TY  - NA
AU  - Hu, Donghan; Lee, Sang Won
TI  - Scrapbook: Screenshot-Based Bookmarks for Effective Digital Resource Curation across Applications
PY  - 2022
AB  - Modern knowledge workers typically need to use multiple resources, such as documents, web pages, and applications, at the same time. This complexity in their computing environments forces workers to restore various resources in the course of their work. However, conventional curation methods like bookmarks, recent document histories, and file systems place limitations on effective retrieval. Such features typically work only for resources of one type within one application, ignoring the interdependency between resources needed for a single task. In addition, text-based handles do not provide rich cues for users to recognize their associated resources. Hence, the need to locate and reopen relevant resources can significantly hinder knowledge workers' productivity. To address these issues, we designed and developed Scrapbook, a novel application for digital resource curation across applications that uses screenshot-based bookmarks. Scrapbook extracts and stores all the metadata (URL, file location, and application name) of windows visible in a captured screenshot to facilitate restoring them later. A week-long field study indicated that screenshot-based bookmarks helped participants curate digital resources. Additionally, participants reported that multimodal -- visual and textual -- data helped them recall past computer activities and reconstruct working contexts efficiently.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545678
ER  - 

TY  - NA
AU  - Chen, Yu-Wen; Lin, Wei-Ju; Chen, Yi; Cheng, Lung-Pan
TI  - UIST - PneuSeries: 3D Shape Forming with Modularized Serial-Connected Inflatables
PY  - 2021
AB  - We present PneuSeries, a series of modularized inflatables where their inflation and deflation are propagated in-between stage by stage to form various shapes. The key component of PneuSeries is the bidirectional check valve that passively regulates the air flowing in/out from/to adjacent inflatables, allowing each of the inflatables to be inflated/deflated one by one through serial propagation. The form of the inflatable series thus is programmed by the sequential operations of a pump that push/pull the air in/out. In this paper, we explored the design of PneuSeries and implemented working prototypes as a proof of concept. In particular, we built PneuSeries with (1) modularized cubical, cuboidal, tetrahedral, prismatic, and custom inflatables to examine their shape forming, (2) fast assembly connectors to allow quick reconfiguration of the series, and (3) folding mechanism to reduce irregularity of the shrunken inflatables. We also evaluated the inflating and deflating time and the flow rate of the valve for simulating the inflating and deflating process and display the steps and time required to transform in our software. Finally, we demonstrate example objects that show the capability of PneuSeries and its potential applications.
SP  - 431
EP  - 440
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474760
ER  - 

TY  - NA
AU  - Richards, Olivia K.; Marcu, Gabriela; Brewer, Robin
TI  - Conference on Designing Interactive Systems - Hugs, Bible Study, and Speakeasies: Designing for Older Adults’ Multimodal Connectedness
PY  - 2021
AB  - Older adults can experience significant changes to their social networks as they age, triggering changes in their social connection practices. In this paper, we extend research on older adults’ connectedness behaviors using a multimodal connectedness framing—that is, how they engage with others across platforms, devices, and modalities. Using the COVID-19 pandemic as a case study, we investigate how older adults navigate a major change or infrastructural breakdown in their social routines. We conducted a survey with 146 U.S.-based older adults (65+), and follow-up interviews with a subset of 23 survey respondents. Findings revealed the resilience and innovation with which older adults adapted their behaviors across multiple modalities to maintain social relationships and playfully connect with others in person and online. Using these findings, we propose that research on designing for aging extend beyond designing for connection in the smart home; we argue for a design agenda that prioritizes designing for smart relationships with the potential to persist across spaces via multimodal connectedness.
SP  - 815
EP  - 831
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462075
ER  - 

TY  - JOUR
AU  - Teodorovicz, Thomaz; Sadun, Raffaella; Kun, Andrew L.; Shaer, Orit
TI  - How does working from home during COVID-19 affect what managers do? Evidence from time-Use studies
PY  - 2021
AB  - NA
SP  - 532
EP  - 557
JF  - Human–Computer Interaction
VL  - 37
IS  - 6
PB  - 
DO  - 10.1080/07370024.2021.1987908
ER  - 

TY  - NA
AU  - Zhou, Yuqi; Popescu, Voicu
TI  - Tapping with a Handheld Stick in VR: Redirection Detection Thresholds for Passive Haptic Feedback
PY  - 2022
AB  - This paper investigates providing grounded passive haptic feedback to a user of a VR application through a handheld stick with which the user taps virtual objects. Such an investigation benefits VR applications beyond those where the stick interaction is actually an integral part of the narrative. Providing passive haptic feedback through a handheld stick as opposed to directly through the user&#x2019;s hand has the potential for more believable and more frequent feedback opportunities. The stick is likely to dull the user&#x2019;s haptics perception and proprioception, potentially avoiding a haptics perception uncanny valley and increasing the redirection detection thresholds. Two haptics redirection methods are proposed: the DriftingHand method, which alters the position of the user&#x2019;s virtual hand, and the Vari-Stick method, which alters the length of the virtual stick. Detection thresholds were measured in a user study (N = 60) by testing the two methods for a range of offsets between the virtual and the real object, for multiple stick lengths, and multiple distances from the user to the real object. Overall, the study reveals that VariStick and DriftingHand provide an undetectable range of offsets of [-20cm, +13cm] and [-11cm, +11cm], respectively.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00026
ER  - 

TY  - JOUR
AU  - Watanabe, Hiroki; Sumiya, Miwa; Terada, Tsutomu
TI  - Human-Machine Cooperative Echolocation Using Ultrasound
PY  - 2022
AB  - NA
SP  - 125264
EP  - 125278
JF  - IEEE Access
VL  - 10
IS  - NA
PB  - 
DO  - 10.1109/access.2022.3224468
ER  - 

TY  - CHAP
AU  - , 
TI  - Appendices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544579
ER  - 

TY  - NA
AU  - Xu, Xuhai; Gong, Jun; Brum, Carolina; Liang, Lilian; Suh, Bongsoo; Gupta, Shivam Kumar; Agarwal, Yash; Lindsey, Laurence; Kang, Runchang; Shahsavari, Behrooz; Nguyen, Tu; Nieto, Heriberto; Hudson, Scott E; Maalouf, Charlie; Mousavi, Jax Seyed; Laput, Gierad
TI  - Enabling Hand Gesture Customization on Wrist-Worn Devices
PY  - 2022
AB  - We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we first deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7% and a false-positive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3%, 83.1%, and 87.2% on using one, three, or five shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the effectiveness, learnability, and usability of our customization framework. Our approach paves the way for a future where users are no longer bound to pre-existing gestures, freeing them to creatively introduce new gestures tailored to their preferences and abilities.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501904
ER  - 

TY  - NA
AU  - Xanthidou, Ourania Koutzampasopoulou; Aburumman, Nadine; Ben-Abdallah, Hanene
TI  - Virtual Reality current trends and proposed research agenda
PY  - 2022
AB  - Among the hottest fields in Information Technology, together with AI/Machine Learning and Blockchain, just to name of couple more, Virtual Reality (as well as Augmented Reality and Extended Reality) are gaining momentum all the more every day. This is mainly because it is heavily applicable on a broad range of business and scientific domains reshaping the way human activities are done now and leaving promises for the near and far future. This short review study aims to provide some insights on the status of the main aspects of VR and some challenges to address as well as provide some possible research directions for anyone interested to delve into the field.
SP  - NA
EP  - NA
JF  - 2022 8th International Conference on Information Technology Trends (ITT)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/itt56123.2022.9863952
ER  - 

TY  - NA
AU  - Vanukuru, Rishi; Murugan, Amarnath; Pillai, Jayesh S.
TI  - VRST - Dual Phone AR: Exploring the use of Phones as Controllers for Mobile Augmented Reality
PY  - 2020
AB  - The possible interactions with Mobile Augmented Reality applications today are largely limited to on-screen gestures and spatial movement. There is an opportunity to design new interaction methods that address common issues and go beyond the screen. Through this project, we explore the idea of using a second phone as a controller for mobile AR experiences. We develop prototypes that demonstrate the use of a second phone controller for tasks such as pointing, selecting, and drawing in 3D space. We use these prototypes and insights from initial remote evaluations to discuss the benefits and drawbacks of such an interaction method. We conclude by outlining opportunities for future research on Dual Phone AR for multiple usage configurations, and in collaborative settings.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3422113
ER  - 

TY  - JOUR
AU  - Masuda, Nagisa; Furukawa, Koichi; Yairi, Ikuko Eguchi
TI  - Stetho Touch: Touch Action Recognition System by Deep Learning with Stethoscope Acoustic Sensing
PY  - 2022
AB  - NA
SP  - 718
EP  - 728
JF  - Journal of Information Processing
VL  - 30
IS  - 0
PB  - 
DO  - 10.2197/ipsjjip.30.718
ER  - 

TY  - CHAP
AU  - , 
TI  - Remarks
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544567
ER  - 

TY  - CHAP
AU  - , 
TI  - Bibliography
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544582
ER  - 

TY  - JOUR
AU  - Mendes, Daniel; Reis, Sofia; Guerreiro, Joao; Nicolau, Hugo
TI  - Collaborative Tabletops for Blind People: The Effect of Auditory Design on Workspace Awareness
PY  - 2020
AB  - Interactive tabletops offer unique collaborative features, particularly their size, geometry, orientation and, more importantly, the ability to support multi-user interaction. Although previous efforts were made to make interactive tabletops accessible to blind people, the potential to use them in collaborative activities remains unexplored. In this paper, we present the design and implementation of a multi-user auditory display for interactive tabletops, supporting three feedback modes that vary on how much information about the partners' actions is conveyed. We conducted a user study with ten blind people to assess the effect of feedback modes on workspace awareness and task performance. Furthermore, we analyze the type of awareness information exchanged and the emergent collaboration strategies. Finally, we provide implications for the design of future tabletop collaborative tools for blind users.
SP  - 1
EP  - 19
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427325
ER  - 

TY  - NA
AU  - Das Swain, Vedant; Williams, Shane; Fourney, Adam; Iqbal, Shamsi T.
TI  - Two Birds with One Phone: The Role of Mobile Use in the Daily Practices of Remote Information Work
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Human-Computer Interaction for Work
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3533406.3533416
ER  - 

TY  - JOUR
AU  - Zhang, Nan; Huang, Tianqi; Meng, Yang; Zhang, Xinran; Liao, Hongen
TI  - 35.1: Tabletop three‐dimensional floating autostereoscopic display system with 360‐degree continuous visualization
PY  - 2021
AB  - NA
SP  - 462
EP  - 467
JF  - SID Symposium Digest of Technical Papers
VL  - 52
IS  - S2
PB  - 
DO  - 10.1002/sdtp.15161
ER  - 

TY  - NA
AU  - Wang, Tzu-Hua; Lee, Tzu-Shan; Pan, Jr-Pin; Kuo, Tzu-Yang; Yong, Hui-Yang; Han, Ping-Hsuan
TI  - GroundFlow: Multiple Flows Feedback for Enhancing Immersive Experience on the Floor in the Wet Scenes
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2021 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3476122.3484837
ER  - 

TY  - NA
AU  - Gonzalez-Franco, Mar; Cohn, Brian A.; Ofek, Eyal; Burin, Dalila; Maselli, Antonella
TI  - VR - The Self-Avatar Follower Effect in Virtual Reality
PY  - 2020
AB  - When embodying a virtual avatar in immersive VR applications where body tracking is enabled, users typically are and feel in control the avatar movements. However, there are situations in which the technology could be tweaked to flip this relationship so that an embodied avatar could affect the user’s motor behavior without users noticing it. This has been shown in action retargeting applications and motor contagion experiments. Here we discuss a different way in which an embodied avatar could implicitly drive users movements: the self-avatar follower effect. We review previous evidences and present new experimental results showing how, whenever the virtual body does not overlay with their physical body, users tend to unconsciously follow their avatar, filling the gap if the system allows for it. We discuss this effect in the context of the relevant neuroscientific literature, and propose a theoretical account of the follower effect at the intersection of motor control and inference theories.
SP  - 18
EP  - 25
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1580500165557
ER  - 

TY  - NA
AU  - Lee, Sooyeon; Yu, Rui; Xie, Jingyi; Billah, Syed Masum; Carroll, John M.
TI  - Opportunities for Human-AI Collaboration in Remote Sighted Assistance
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 27th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490099.3511113
ER  - 

TY  - NA
AU  - Clarence, Aldrich; Knibbe, Jarrod; Cordeil, Maxime; Wybrow, Michael
TI  - Investigating The Effect of Direction on The Limits of Haptic Retargeting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00078
ER  - 

TY  - NA
AU  - Du, Pei; Bulusu, Nirupama
TI  - Indoor navigation for visually impaired people with vertex colored graphs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3498361.3538797
ER  - 

TY  - NA
AU  - Pei, Siyou; Chen, Alexander; Lee, Jaewook; Zhang, Yang
TI  - Hand Interfaces: Using Hands to Imitate Objects in AR/VR for Expressive Interactions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501898
ER  - 

TY  - NA
AU  - Okopnyi, Pavel; Juhlin, Oskar; Guribye, Frode
TI  - Designing for Collaborative Video Editing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Nordic Human-Computer Interaction Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3546155.3546664
ER  - 

TY  - NA
AU  - Aigner, Roland; Haberfellner, Mira Alida; Haller, Michael
TI  - spaceR: Knitting Ready-Made, Tactile, and Highly Responsive Spacer-Fabric Force Sensors for Continuous Input
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545694
ER  - 

TY  - NA
AU  - Nabil, Sara; Jones, Lee; Girouard, Audrey
TI  - TEI - Soft Speakers: Digital Embroidering of DIY Customizable Fabric Actuators
PY  - 2021
AB  - We introduce Soft Speakers, a systematic approach for designing custom fabric actuators that can be used as audio speakers and vibro-haptic actuators. Digitally-embroidered with e-textiles, we implement Soft Speakers as tactile, malleable and aesthetic designs to be part of wearables, soft furnishing and fabric objects. We present a rapid technique for the DIY fabrication of audio feedback into soft interfaces. We also discuss and evaluate 7 factors for their parametric design in additive and constructive methods. To demonstrate the feasibility of our approach and the breadth of new designs that it enables, we developed 5 prototypes: 3 wearables, a piece of furniture and a soft toy. Studying Soft Speakers with maker-users expanded the design space, empowering users and supporting inclusive design. Our study includes insights on user experience of real-world interactive applications for remote communication, e-learning, entertainment, navigation and gaming, enabled by Soft Speakers’ customizable and scalable form factor.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440630
ER  - 

TY  - NA
AU  - Parizi, Farshid Salemi; Kienzle, Wolf; Whitmire, Eric; Gupta, Aakar; Benko, Hrvoje
TI  - RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband
PY  - 2021
AB  - We introduce RotoWrist, an infrared (IR) light based solution for continuously and reliably tracking 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489886
ER  - 

TY  - NA
AU  - Chen, Yanjun; Liang, Xuewei; Chen, Si; Chen, Yuwen; Lin, Hongnan; Zhang, Hechuan; Jiang, Chutian; Tian, Feng; Zhang, Yu; Yao, Shanshan; Han, Teng
TI  - HapTag: A Compact Actuator for Rendering Push-Button Tactility on Soft Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545644
ER  - 

TY  - NA
AU  - Feick, Martin; Bateman, Scott; Tang, Anthony; Miede, André; Marquardt, Nicolai
TI  - TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation in Virtual Reality
PY  - 2020
AB  - Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit, and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives, and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators, and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi's proxies outperform traditional controllers, and were generally favored by participants.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Luo, Tianren; He, Zhenxuan; Cai, Chenyang; Han, Teng; Pan, Zhigeng; Tian, Feng
TI  - Exploring Sensory Conflict Effect Due to Upright Redirection While Using VR in Reclining & Lying Positions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545692
ER  - 

TY  - CHAP
AU  - , 
TI  - Index
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544584
ER  - 

TY  - NA
AU  - Vanukuru, Rishi; Murugan, Amarnath; Pillai, Jayesh S.
TI  - UIST (Adjunct Volume) - Dual Phone AR: Using a Second Phone as a Controller for Mobile Augmented Reality
PY  - 2020
AB  - Mobile Augmented Reality applications have become increasingly popular, however the possible interactions with AR content are largely limited to on-screen gestures and spatial movement. There has been a renewed interest in designing interaction methods for mobile AR that go beyond the screen. Mobile phones present a rich range of input, output, and tracking capabilities, and have been used as controllers for Virtual Reality and head-mounted Augmented Reality applications. In this project, we explore the use of a second phone as a controller for Mobile AR. We developed ARTWO, an application that showcases Handheld Dual Phone AR through a series of small demos in which a second phone can be used to perform basic tasks such as pointing, selecting, and drawing, in the context of real use cases. We believe that the Dual Phone AR approach can help address many of the issues faced when using conventional mobile AR interactions, and also serves as a stepping stone to the general use of phones with head-mounted AR systems in the near future.
SP  - 117
EP  - 119
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416139
ER  - 

TY  - JOUR
AU  - Amiri, Zahra; Sekhavat, Yoones A.; Goljaryan, Sakineh
TI  - StepAR: A personalized exergame for people with multiple sclerosis based on video-mapping
PY  - 2022
AB  - NA
SP  - 100487
EP  - 100487
JF  - Entertainment Computing
VL  - 42
IS  - NA
PB  - 
DO  - 10.1016/j.entcom.2022.100487
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Myers, Brad A.
TI  - A Need-finding Study for Understanding Text Entry in SmartphoneApp Usage.
PY  - 2021
AB  - Text entry makes up about one-fourth of the smartphone interaction events, and is known to be challenging and difficult. However, there has been little study about the characteristics of text entry in the context of smartphone app usage. In this paper, we present a mixed-method in-situ study conducted in 2016 with 17 active smartphone users to better understand text entry in smartphone app usage. Our results show 80% of text was entered into communication apps, with different apps exhibiting distinct usage patterns. We found that structured data such as URLs and email addresses are rarely typed but instead are auto-completed or replaced with search, copy-and-paste is rarely used, and sessions of smartphone usage with text entry involve more apps and last longer. We conclude with a discussion about the implications on the development of systems to better support mobile interaction.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Teodorovicz, Thomaz; Sadun, Raffaella; Kun, Andrew L.; Shaer, Orit
TI  - How does working from home during COVID-19 affect what managers do? Evidence from time-Use studies
PY  - 2021
AB  - The advent of the COVID-19 pandemic has forced millions of workers to suddenly shift their activity out of their offices and into their homes: 5–15% of Americans worked from home before the pandemi...
SP  - 1
EP  - 26
JF  - Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Li, Toby Jia-Jun; Radensky, Marissa; Jia, Justin; Singarajah, Kirielle; Mitchell, Tom M.; Myers, Brad A.
TI  - Interactive Task and Concept Learning from Natural Language Instructions and GUI Demonstrations
PY  - 2019
AB  - Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Fang, Fengyi; Zhang, Hongwei; Zhan, Lishuang; Guo, Shihui; Zhang, Minying; Lin, Juncong; Qin, Yipeng; Fu, Hongbo
TI  - Handwriting Velcro
PY  - 2022
AB  - <jats:p>Text input is a desired feature for AR glasses. While there already exist various input modalities (e.g., voice, mid-air gesture), the diverse demands required by different input scenarios can hardly be met by the small number of fixed input postures offered by existing solutions. In this paper, we present Handwriting Velcro, a novel text input solution for AR glasses based on flexible touch sensors. The distinct advantage of our system is that it can easily stick to different body parts, thus endowing AR glasses with posture-adaptive handwriting input. We explored the design space of on-body device positions and identified the best interaction positions for various user postures. To flatten users' learning curves, we adapt our device to the established writing habits of different users by training a 36-character (i.e., A-Z, 0-9) recognition neural network in a human-in-the-loop manner. Such a personalization attempt ultimately achieves a low error rate of 0.005 on average for users with different writing styles. Subjective feedback shows that our solution has a good performance in system practicability and social acceptance. Empirically, we conducted a heuristic study to explore and identify the best interaction Position-Posture Correlation. Experimental results show that our Handwriting Velcro excels similar work [6] and commercial product in both practicality (12.3 WPM) and user-friendliness in different contexts.</jats:p>
SP  - 1
EP  - 31
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569461
ER  - 

TY  - JOUR
AU  - Lee, Kyungjun; Sato, Daisuke; Asakawa, Saki; Asakawa, Chieko; Kacorri, Hernisa
TI  - Accessing Passersby Proxemic Signals through a Head-Worn Camera: Opportunities and Limitations for the Blind
PY  - 2021
AB  - The spatial behavior of passersby can be critical to blind individuals to initiate interactions, preserve personal space, or practice social distancing during a pandemic. Among other use cases, wearable cameras employing computer vision can be used to extract proxemic signals of others and thus increase access to the spatial behavior of passersby for blind people. Analyzing data collected in a study with blind (N=10) and sighted (N=40) participants, we explore: (i) visual information on approaching passersby captured by a head-worn camera; (ii) pedestrian detection algorithms for extracting proxemic signals such as passerby presence, relative position, distance, and head pose; and (iii) opportunities and limitations of using wearable cameras for helping blind people access proxemics related to nearby people. Our observations and findings provide insights into dyadic behaviors for assistive pedestrian detection and lead to implications for the design of future head-worn cameras and interactions.
SP  - NA
EP  - NA
JF  - ASSETS. Annual ACM Conference on Assistive Technologies
VL  - 21
IS  - NA
PB  - 
DO  - 10.1145/3441852.3471232
ER  - 

TY  - JOUR
AU  - Alessandrini, Andrea
TI  - A Study of Students Engaged in Electronic Circuit Wiring in an Undergraduate Course
PY  - 2022
AB  - NA
SP  - 78
EP  - 95
JF  - Journal of Science Education and Technology
VL  - 32
IS  - 1
PB  - 
DO  - 10.1007/s10956-022-09994-9
ER  - 

TY  - NA
AU  - Storer, Kevin M.; Sampath, Harini; Merrick, M. Alice Alice
TI  - CHI - ”It’s Just Everything Outside of the IDE that’s the Problem”:: Information Seeking by Software Developers with Visual Impairments
PY  - 2021
AB  - Many efforts to increase accessibility in coding for developers with visual impairments (DWVI) focus on supporting interactions with development tools. But, to understand how to appropriately modify and write source code, developers must seek information from a variety of disparate and highly technical sources. DWVI might benefit from technological support in this process. But, it is unclear what accessibility issues arise in technical information sources, whether accessibility impacts strategies for seeking technical information, or how best to support DWVI in information seeking. We conducted observations and interviews with twelve DWVI, to explore their information behaviors. We found that DWVI seek information in many of the same sources as their sighted peers, and accessibility issues in technical information sources were similar to those in nontechnical sources. But, despite these similarities, examining development as an information seeking process highlighted the role of contextual and social factors in determining accessibility for DWVI.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445090
ER  - 

TY  - NA
AU  - Lee, Ahreum; Jo, Wonse; Kannan, Shyam Sundar; Min, Byung-Cheol
TI  - Investigating the Effect of Deictic Movements of a Multi-robot
PY  - 2020
AB  - Multi-robot systems are made up of a team of multiple robots, which provides the advantage of performing complex tasks with high efficiency, flexibility, and robustness. Although research on human-robot interaction is ongoing as robots become more readily available and easier to use, the study of interactions between a human and multiple robots represents a relatively new field of research. In particular, how multi-robots could be used for everyday users has not been extensively explored. Additionally, the impact of the characteristics of multiple robots on human perception and cognition in human multi-robot interaction should be further explored. In this paper, we specifically focus on the benefits of physical affordances generated by the movements of multi-robots, and investigate the effects of deictic movements of multi-robots on information retrieval by conducting a delayed free recall task.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cheymol, Antonin; Hirao, Yutaro; Yoshida, Shigeo; Kuzuoka, Hideaki
TI  - Increasing the Perceived Speed of Dynamic Handheld Shape Displays through Visuo-Haptic Illusions
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519410
ER  - 

TY  - NA
AU  - Kim, Daehwa; Harrison, Chris
TI  - EtherPose: Continuous Hand Pose Tracking with Wrist-Worn Antenna Impedance Characteristic Sensing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545665
ER  - 

TY  - CHAP
AU  - , 
TI  - Evaluating TEI
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544576
ER  - 

TY  - CONF
AU  - Li, Toby Jia-Jun; Popowski, Lindsay; Mitchell, Tom M.; Myers, Brad A.
TI  - CHI - Screen2Vec: Semantic Embedding of GUI Screens and GUI Components
PY  - 2021
AB  - Representing the semantics of GUI screens and components is crucial to data-driven computational methods for modeling user-GUI interactions and mining GUI designs. Existing GUI semantic representations are limited to encoding either the textual content, the visual design and layout patterns, or the app contexts. Many representation techniques also require significant manual data annotation efforts. This paper presents Screen2Vec, a new self-supervised technique for generating representations in embedding vectors of GUI screens and components that encode all of the above GUI features without requiring manual annotation using the context of user interaction traces. Screen2Vec is inspired by the word embedding method Word2Vec, but uses a new two-layer pipeline informed by the structure of GUIs and interaction traces and incorporates screen- and app-specific metadata. Through several sample downstream tasks, we demonstrate Screen2Vec’s key useful properties: representing between-screen similarity through nearest neighbors, composability, and capability to represent user tasks.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Wei, Haowen; Li, Ziheng; Galvan, Alexander D.; Su, Zhuoran; Zhang, Xiao; Pahlavan, Kaveh; Solovey, Erin T.
TI  - IndexPen
PY  - 2022
AB  - <jats:p>In this paper, we introduce IndexPen, a novel interaction technique for text input through two-finger in-air micro-gestures, enabling touch-free, effortless, tracking-based interaction, designed to mirror real-world writing. Our system is based on millimeter-wave radar sensing, and does not require instrumentation on the user. IndexPen can successfully identify 30 distinct gestures, representing the letters A-Z, as well as Space, Backspace, Enter, and a special Activation gesture to prevent unintentional input. Additionally, we include a noise class to differentiate gesture and non-gesture noise. We present our system design, including the radio frequency (RF) processing pipeline, classification model, and real-time detection algorithms. We further demonstrate our proof-of-concept system with data collected over ten days with five participants yielding 95.89% cross-validation accuracy on 31 classes (including noise). Moreover, we explore the learnability and adaptability of our system for real-world text input with 16 participants who are first-time users to IndexPen over five sessions. After each session, the pre-trained model from the previous five-user study is calibrated on the data collected so far for a new user through transfer learning. The F-1 score showed an average increase of 9.14% per session with the calibration, reaching an average of 88.3% on the last session across the 16 users. Meanwhile, we show that the users can type sentences with IndexPen at 86.2% accuracy, measured by string similarity. This work builds a foundation and vision for future interaction interfaces that could be enabled with this paradigm.</jats:p>
SP  - 1
EP  - 39
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534601
ER  - 

TY  - CONF
AU  - Wang, Chenglong; Feng, Yu; Bodik, Rastislav; Dillig, Isil; Cheung, Alvin; Ko, Amy J.
TI  - CHI - Falx: Synthesis-Powered Visualization Authoring
PY  - 2021
AB  - Modern visualization tools aim to allow data analysts to easily create exploratory visualizations. When the input data layout conforms to the visualization design, users can easily specify visualizations by mapping data columns to visual channels of the design. However, when there is a mismatch between data layout and the design, users need to spend significant effort on data transformation. We propose Falx, a synthesis-powered visualization tool that allows users to specify visualizations in a similarly simple way but without needing to worry about data layout. In Falx, users specify visualizations using examples of how concrete values in the input are mapped to visual channels, and Falx automatically infers the visualization specification and transforms the data to match the design. In a study with 33 data analysts on four visualization tasks involving data transformation, we found that users can effectively adopt Falx to create visualizations they otherwise cannot implement.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Rudolph, Julius Cosmo Romeo; Holman, David; De Araujo, Bruno; Jota, Ricardo; Wigdor, Daniel; Savage, Valkyrie
TI  - Sensing Hand Interactions with Everyday Objects by Profiling Wrist Topography
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501320
ER  - 

TY  - CHAP
AU  - , 
TI  - Introduction
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544568
ER  - 

TY  - CHAP
AU  - , 
TI  - Foreword
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Weaving Fire into Form
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544564.3544566
ER  - 

TY  - JOUR
AU  - White, Ryen W.; Nouri, Elnaz; Woffinden-Luey, James; EncarnacióN, Mark; Jauhar, Sujay Kumar
TI  - Microtask Detection
PY  - 2021
AB  - <jats:p> Information systems, such as task management applications and digital assistants, can help people keep track of tasks of different types and different time durations, ranging from a few minutes to days or weeks. Helping people better manage their tasks and their time are core capabilities of assistive technologies, situated within a broader context of supporting more effective information access and use. Throughout the course of a day, there are typically many short time periods of downtime (e.g., five minutes or less) available to individuals. Microtasks are simple tasks that can be tackled in such short amounts of time. Identifying microtasks in task lists could help people utilize these periods of low activity to make progress on their task backlog. We define <jats:italic>actionable</jats:italic> tasks as self-contained tasks that need to be completed or acted on. However, not all to-do tasks are actionable. Many task lists are collections of miscellaneous items that can be completed at any time (e.g., books to read, movies to watch), notes (e.g., names, addresses), or the individual items are constituents in a list that is itself a task (e.g., a grocery list). In this article, we introduce the novel challenge of microtask detection, and we present machine-learned models for automatically determining which tasks are actionable and which of these actionable tasks are microtasks. Experiments show that our models can accurately identify actionable tasks, accurately detect actionable microtasks, and that we can combine these models to generate a solution that scales microtask detection to all tasks. We discuss our findings in detail, along with their limitations. These findings have implications for the design of systems to help people make the most of their time. </jats:p>
SP  - 1
EP  - 29
JF  - ACM Transactions on Information Systems
VL  - 39
IS  - 2
PB  - 
DO  - 10.1145/3432290
ER  - 

TY  - CHAP
AU  - Wang, Liwen; Sandor, Christian
TI  - Can You Perceive the Size Change? Discrimination Thresholds for Size Changes in Augmented Reality
PY  - 2021
AB  - NA
SP  - 25
EP  - 36
JF  - Virtual Reality and Mixed Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-90739-6_2
ER  - 

TY  - NA
AU  - Danilova, Anastasia; Naiakshina, Alena; Horstmann, Stefan; Smith, Matthew
TI  - Do you really code? Designing and Evaluating Screening Questions for Online Surveys with Programmers
PY  - 2021
AB  - Recruiting professional programmers in sufficient numbers for research studies can be challenging because they often cannot spare the time, or due to their geographical distribution and potentially the cost involved. Online platforms such as Clickworker or Qualtrics do provide options to recruit participants with programming skill; however, misunderstandings and fraud can be an issue. This can result in participants without programming skill taking part in studies and surveys. If these participants are not detected, they can cause detrimental noise in the survey data. In this paper, we develop screener questions that are easy and quick to answer for people with programming skill but difficult to answer correctly for those without. In order to evaluate our questionnaire for efficacy and efficiency, we recruited several batches of participants with and without programming skill and tested the questions. In our batch 42% of Clickworkers stating that they have programming skill did not meet our criteria and we would recommend filtering these from studies. We also evaluated the questions in an adversarial setting. We conclude with a set of recommended questions which researchers can use to recruit participants with programming skill from online platforms.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Qiu, Jianing; Chen, Lipeng; Gu, Xiao; Lo, Frank P.-W.; Tsai, Ya-Yen; Sun, Jiankai; Liu, Jiaqi; Lo, Benny
TI  - Egocentric Human Trajectory Forecasting With a Wearable Camera and Multi-Modal Fusion
PY  - 2022
AB  - NA
SP  - 8799
EP  - 8806
JF  - IEEE Robotics and Automation Letters
VL  - 7
IS  - 4
PB  - 
DO  - 10.1109/lra.2022.3188101
ER  - 

TY  - NA
AU  - Jain, Harshika; Lu, Kexin; Yao, Lining
TI  - Conference on Designing Interactive Systems - Hydrogel-based DIY Underwater Morphing Artifacts: A morphing and fabrication technique to democratize the creation of controllable morphing 3D underwater structures with low-cost, easily available hydrogel beads adhered to a substrate.
PY  - 2021
AB  - Hydrogels are versatile morphing materials that have recently been adopted for creating shape-changing interfaces. However, most shape-changing interfaces require advanced material synthesis, specialized lab settings for fabrication, and technical knowledge is needed to simulate their morphing behavior. To replicate such structures, these factors become a barrier for makers. Therefore, to democratize the creation of hydrogel-based morphing artifacts and to extend their design space in HCI, we propose a water-triggered morphing mechanism that utilizes the distance between adjacent hydrogel beads adhered on a thin substrate to control their bending angle. This paper describes the bending angle quantification experiments for creating a simulator, the process of developing a computational tool along with its user-friendly workflow and demonstrates kirigami and branch-based artifacts built with the tool. Using our method, anyone can easily design and fabricate custom morphing structures.
SP  - 1242
EP  - 1252
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462136
ER  - 

TY  - JOUR
AU  - Mantri, Prateek; Subramonyam, Hariharan; Michal, Audrey L; Xiong, Cindy
TI  - How Do Viewers Synthesize Conflicting Information from Data Visualizations?
PY  - 2022
AB  - Scientific knowledge develops through cumulative discoveries that build on, contradict, contextualize, or correct prior findings. Scientists and journalists often communicate these incremental findings to lay people through visualizations and text (e.g., the positive and negative effects of caffeine intake). Consequently, readers need to integrate diverse and contrasting evidence from multiple sources to form opinions or make decisions. However, the underlying mechanism for synthesizing information from multiple visualizations remains under-explored. To address this knowledge gap, we conducted a series of four experiments (N = 1166) in which participants synthesized empirical evidence from a pair of line charts presented sequentially. In Experiment 1, we administered a baseline condition with charts depicting no specific context where participants held no strong belief. To test for the generalizability, we introduced real-world scenarios to our visualizations in Experiment 2 and added accompanying text descriptions similar to online news articles or blog posts in Experiment 3. In all three experiments, we varied the relative direction and magnitude of line slopes within the chart pairs. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope). Participants tended to weigh the less steep slope more when the two charts depicted relationships in the same direction (e.g., both positive). Through these experiments, we characterize participants' synthesis behaviors depending on the relationship between the information they viewed, contribute to theories describing underlying cognitive mechanisms in information synthesis, and describe design implications for data storytelling.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209467
ER  - 

TY  - NA
AU  - Li, Zhi; Zhao, Maozheng; Das, Dibyendu; ZHAO, HANG; Ma, Yan; Liu, Wanyu; Beaudouin-Lafon, Michel; Wang, Fusheng; Ramakrishnan, IV; Bi, Xiaojun
TI  - Select or Suggest? Reinforcement Learning-based Method for High-Accuracy Target Selection on Touchscreens
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517472
ER  - 

TY  - BOOK
AU  - Yu, Difeng; Jiang, Weiwei; Wang, Chaofan; Dingler, Tilman; Velloso, Eduardo; Goncalves, Jorge
TI  - ISS Companion - ShadowDancXR: Body Gesture Digitization for Low-cost Extended Reality (XR) Headsets
PY  - 2020
AB  - Low-cost, smartphone-based extended reality (XR) headsets, such as Google Cardboard, are more accessible but lack advanced features like body-tracking, which limits the expressiveness of interaction. We introduce ShadowDancXR, a technique that relies on the front-facing camera of the smartphone to reconstruct users' body gestures from projected shadows, which allows for wider range of interaction possibilities with low-cost XR. Our approach requires no embedded accessories and serves as an inexpensive, portable solution for body gesture digitization. We provide two interactive experiences to demonstrate the feasibility of our concept.
SP  - 79
EP  - 80
JF  - Companion Proceedings of the 2020 Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3380867.3426222
ER  - 

TY  - NA
AU  - Cho, Youngjun
TI  - Rethinking Eye-blink: Assessing Task Difficulty through Physiological Representation of Spontaneous Blinking
PY  - 2021
AB  - Continuous assessment of task difficulty and mental workload is essential in improving the usability and accessibility of interactive systems. Eye tracking data has often been investigated to achieve this ability, with reports on the limited role of standard blink metrics. Here, we propose a new approach to the analysis of eye-blink responses for automated estimation of task difficulty. The core module is a time-frequency representation of eye-blink, which aims to capture the richness of information reflected on blinking. In our first study, we show that this method significantly improves the sensitivity to task difficulty. We then demonstrate how to form a framework where the represented patterns are analyzed with multi-dimensional Long Short-Term Memory recurrent neural networks for their non-linear mapping onto difficulty-related parameters. This framework outperformed other methods that used hand-engineered features. This approach works with any built-in camera, without requiring specialized devices. We conclude by discussing how Rethinking Eye-blink can benefit real-world applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445577
ER  - 

TY  - JOUR
AU  - Mishra, Abhijeet; Kumar, Piyush; Shukla, Jainendra; Parnami, Aman
TI  - HaptiDrag
PY  - 2022
AB  - <jats:p>We presently rely on mechanical approaches to leverage drag (friction) effects for digital interaction as haptic feedback over real surfaces. Unfortunately, due to their mechanical nature, such methods are inconvenient, difficult to scale, and include object deployment issues. Accordingly, we present HaptiDrag, a thin (1 mm) and lightweight (2 gram) device that can reliably produce various intensities of on-surface drag effects through electroadhesion phenomenon. We first performed design evaluation to determine minimal size (5 cm x 5 cm) of HaptiDrag to enable drag effect. Further, with reference to eight distinct surfaces, we present technical performance of 2 sizes of HaptiDrag in real environment conditions. Later, we conducted two user studies; the first to discover absolute detection threshold friction spots of varying intensities common to all surfaces under test and the second to validate the absolute detection threshold points for noticeability with all sizes of HaptiDrag. Finally, we demonstrate device's utility in different scenarios.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550310
ER  - 

TY  - NA
AU  - Mackey, Angella; Wakkary, Ron; Wensveen, Stephan; Hupfeld, Annika; Tomico, Oscar
TI  - Conference on Designing Interactive Systems - Alternative Presents for Dynamic Fabric
PY  - 2020
AB  - In this paper we investigate how a combination of "speculative" design methods can be used to generate theoretical understandings for dynamic, colour-changing fabrics for garments. Specifically, we combine a first-person, autobiographical, research through design (RtD) approach that draws strategies from speculative design. We call this approach alternative presents, inspired by the work of James Auger, and explore it as a way to generate theoretical propositions for dynamic fabric that emphasize the lived experience over technological innovation. The contributions of this framing are twofold. Firstly, we offer a theoretical contribution to the literature on dynamic fabric. Secondly, we make a methodological contribution for how autobiographical design and RtD can be oriented speculatively to generate intermediate knowledge, with particular emphasis on social-technical aspects.
SP  - 351
EP  - 364
JF  - Proceedings of the 2020 ACM Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3357236.3395447
ER  - 

TY  - JOUR
AU  - Lee, Hyo Ryun; Park, Jihun; Suh, Young-Joo
TI  - Improving Classification Accuracy of Hand Gesture Recognition Based on 60 GHz FMCW Radar with Deep Learning Domain Adaptation
PY  - 2020
AB  - With the recent development of small radars with high resolution, various human–computer interaction (HCI) applications using them have been developed. In particular, a method of applying a user’s hand gesture recognition using a short-range radar to an electronic device is being actively studied. In general, the time delay and Doppler shift characteristics that occur when a transmitted signal that is reflected off an object returns are classified through deep learning to recognize the motion. However, the main obstacle in the commercialization of radar-based hand gesture recognition is that even for the same type of hand gesture, recognition accuracy is degraded due to a slight difference in movement for each individual user. To solve this problem, in this paper, the domain adaptation is applied to hand gesture recognition to minimize the differences among users’ gesture information in the learning and the use stage. To verify the effectiveness of domain adaptation, a domain discriminator that cheats the classifier was applied to a deep learning network with a convolutional neural network (CNN) structure. Seven different hand gesture data were collected for 10 participants and used for learning, and the hand gestures of 10 users that were not included in the training data were input to confirm the recognition accuracy of an average of 98.8%.
SP  - 2140
EP  - NA
JF  - Electronics
VL  - 9
IS  - 12
PB  - 
DO  - 10.3390/electronics9122140
ER  - 

TY  - NA
AU  - Meena, Yogesh Kumar; Yang, Xing-Dong; Löchtefeld, Markus; Carnie, Matthew J.; Henze, Niels; Hodges, Steve; Jones, Matt; Arora, Nivedita; Abowd, Gregory D.
TI  - CHI Extended Abstracts - SelfSustainableCHI: Self-Powered Sustainable Interfaces and Interactions
PY  - 2020
AB  - The continued proliferation of computing devices comes with an ever-increasing energy requirement, both during production and use. As awareness of the global climate emergency increases, self-powered and sustainable (SelfSustainable) interactive devices are likely to play a valuable role. In this workshop we bring together researchers and practitioners from design, computer science, materials science, engineering and manufacturing industries working on this new area of endeavour. The workshop will provide a platform for participants to review and discuss challenges and opportunities associated with self-powered and sustainable interfaces and interactions, develop a design space and identify opportunities for future research.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375167
ER  - 

TY  - JOUR
AU  - David-John, Brendan; Hosfelt, Diane; Butler, Kevin R. B.; Jain, Eakta
TI  - A privacy-preserving approach to streaming eye-tracking data
PY  - 2021
AB  - Eye-tracking technology is being increasingly integrated into mixed reality devices. Although critical applications are being enabled, there are significant possibilities for violating user privacy expectations. We show that there is an appreciable risk of unique user identification even under natural viewing conditions in virtual reality. This identification would allow an app to connect a user's personal ID with their work ID without needing their consent, for example. To mitigate such risks we propose a framework that incorporates gatekeeping via the design of the application programming interface and via software-implemented privacy mechanisms. Our results indicate that these mechanisms can reduce the rate of identification from as much as 85% to as low as 30%. The impact of introducing these mechanisms is less than 1.5° error in gaze position for gaze prediction. Gaze data streams can thus be made private while still allowing for gaze prediction, for example, during foveated rendering. Our approach is the first to support privacy-by-design in the flow of eye-tracking data within mixed reality use cases.
SP  - 2555
EP  - 2565
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2021.3067787
ER  - 

TY  - NA
AU  - Yang, Shimin
TI  - Intelligent Improvement Measures for the Broadcasting and Hosting Major based on Multimedia and Virtual Reality
PY  - 2021
AB  - Intelligent improvement measures and solutions for the broadcasting and hosting major based on multimedia and virtual reality is proposed in this paper. With the rapid development of digital video technology and network technology, the application of the multiple media information tends to be integrated and interactive. This paper has the novelties as follows. First, the interference items will seriously affect the negative information data detection results of the multimedia network, and will lead to high false positive rates and also missed detection rates which will enhance the overall performance. Second, the compression coding and decoding technology of the video image data is one of the most important technologies in multimedia information processing, we applied it to the application. In the experiment, we compare the proposed model with the state-of-the-art approaches, and obtained the nice results.
SP  - NA
EP  - NA
JF  - 2021 Third International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icicv50876.2021.9388518
ER  - 

TY  - NA
AU  - Surale, Hemant Bhaskar; Tham, Yu Jiang; Smith, Brian A.; Vaish, Rajan
TI  - ARcall: Real-Time AR Communication using Smartphones and Smartglasses
PY  - 2022
AB  - Augmented Reality (AR) smartglasses are increasingly regarded as the next generation personal computing platform. However, there is a lack of understanding about how to design communication systems using them. We present ARcall, a novel Augmented Reality-based real-time communication system that enables an immersive, delightful, and privacy-preserving experience between a smartphone user and a smartglasses wearer. ARcall allows a remote friend (Friend) to send and project AR content to a smartglasses wearer (Wearer). The ARcall system was designed with the practical limits of existing AR glasses in mind, including shorter battery life and a reduced field of view. We conduct a qualitative evaluation of the three main components of ARcall: Drop-In, ARaction, and Micro-Chat. Our results provide novel insights for building future AR-based communication methods, including, the importance of context priming, user control over AR content placement, and the feeling of co-presence while conversing.
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519398
ER  - 

TY  - NA
AU  - Liu, Ruiyang; Nikolic, Predrag K.
TI  - Mutltimodal AI Companion for Interactive Fairytale Co-creation
PY  - 2021
AB  - AI fairy tale companions play an important role in early childhood education as an augmentation for parents' efforts to close the participation gap and boost kids' mental and language development. Existing systems are generally designed to provide vivid materials as unidirectional entertaining reading environments, e.g, visualizing inputting texts. However, due to the limited vocabulary of kids, these systems failed to afford effective interaction to motivate kids to write their own fairy tales. In this work, we propose AI.R Taletorium, an illustrative, immersive, and inclusive multimodal AI companion, for interactive fairy tale co-creation that actively involves kids to create fairy tales with both the AI agent and their normal peers. AI.R Taletorium consists a neural story generator and a doodler-based fairy tale visualizer. We design a character-centric bidirectional connection mechanism between the story generator and visualizer equipped with Contrastive Language Image Pretraining (CLIP), thus enabling kids to participant in the story generation process by simple sketching. Extensive experiments and user studies show that our system was able to generate and visualize meaningful and vivid fairy tales with limited training data and complete the full interaction cycle under various inputs (text, doodler) through the bidirectional connection.
SP  - NA
EP  - NA
JF  - arXiv: Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Mu, Mu; Dohan, Murtada; Goodyear, Alison; Hill, Gary; Johns, Cleyon; Mauthe, Andreas
TI  - User attention and behaviour in virtual reality art encounter
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators are experimenting with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform the creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience art encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between the behavioural data and the audience’s background. The work also introduced new integrated methods to visualise user attention for content creators.</jats:p>
SP  - NA
EP  - NA
JF  - Multimedia Tools and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s11042-022-13365-2
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Allen, Harrison Mitchell; DesPortes, Kayla; Mueller, Stefanie
TI  - Creativity &amp; Cognition - FabO: Integrating Fabrication with a Player’s Gameplay in Existing Digital Games
PY  - 2021
AB  - Fabricating objects from a player’s gameplay, for example, collectibles of valuable game items, or custom game controllers shaped from game objects, expands ways to engage with digital games. Researchers currently create such integrated fabrication games from scratch, which is time-consuming and misses the potential of integrating fabrication with the myriad existing games. Integrating fabrication with the real-time gameplay of existing games, however, is challenging without access to the source files. To address this challenge, we present a framework that uses on-screen visual content to integrate fabrication with existing digital games. To implement this framework, we built the FabO toolkit, in which (1) designers use the FabO designer interface to choose the gameplay moments for fabrication and tag the associated on-screen visual cues; (2) players then use the FabO player interface which monitors their gameplay, identifies these cues and auto-generates the fabrication files for the game objects. Results from our two user studies show that FabO supported in integrating fabrication with diverse games while augmenting players’ experience. We discuss insights from our studies on choosing suitable on-screen visual content and gameplay moments for seamless integration of fabrication.
SP  - 3465239
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450741.3465239
ER  - 

TY  - JOUR
AU  - Liu, Hailong; Dong, Weiling; Wang, Hao; Lu, Li; Ruan, Qifeng; Tan, You Sin; Simpson, Robert E.; Yang, Joel K. W.
TI  - Rewritable color nanoprints in antimony trisulfide films
PY  - 2020
AB  - Materials that exhibit large and rapid switching of their optical properties in the visible spectrum hold the key to color-changing devices. Antimony trisulfide (Sb2S3) is a chalcogenide material that exhibits large refractive index changes of ~1 between crystalline and amorphous states. However, little is known about its ability to endure multiple switching cycles, its capacity for recording high-resolution patterns, nor the optical properties of the crystallized state. Unexpectedly, we show that crystalline Sb2S3 films that are just 20 nm thick can produce substantial birefringent phase retardation. We also report a high-speed rewritable patterning approach at subdiffraction resolutions (>40,000 dpi) using 780-nm femtosecond laser pulses. Partial reamorphization is demonstrated and then used to write and erase multiple microscale color images with a wide range of colors over a ~120-nm band in the visible spectrum. These solid-state, rapid-switching, and ultrahigh-resolution color-changing devices could find applications in nonvolatile ultrathin displays.
SP  - NA
EP  - NA
JF  - Science advances
VL  - 6
IS  - 51
PB  - 
DO  - 10.1126/sciadv.abb7171
ER  - 

TY  - NA
AU  - Tian, Rundong
TI  - Lucid Fabrication
PY  - 2019
AB  - Advances in digital fabrication have created new capabilities and simultaneously reinforced outdated workflows. In my thesis work, I primarily explore alternative workflows for digital fabrication that introduce new capabilities and interactions. Methodologically, I build fabrication systems spanning mechanical design, electronics, and software in order to examine these ideas in specific detail. In this paper, I introduce related work and frame it within the historical context of digital fabrication, and discuss my previous and ongoing work.
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332167.3356881
ER  - 

TY  - JOUR
AU  - Elu, Aitzol; Azkune, Gorka; de Lacalle, Oier Lopez; Arganda-Carreras, Ignacio; Soroa, Aitor; Agirre, Eneko
TI  - Inferring spatial relations from textual descriptions of images
PY  - 2021
AB  - This project was partially supported by the project DeepReading (RTI2018-096846-BC21) supported by the Spanish Government, the Basque Government excellence research group (IT1343-19) and Etorkizuna Eraikiz 2019
SP  - 107847
EP  - NA
JF  - Pattern Recognition
VL  - 113
IS  - NA
PB  - 
DO  - 10.1016/j.patcog.2021.107847
ER  - 

TY  - JOUR
AU  - Dong-Soo, Choi; Jeong, SeongWon; Lee, Seok-Han; Ryu, SiHo; Kim, Sang-Youn
TI  - Soft bidirectional haptic I/O module based on bi-convex patterned PVC gel
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>In this paper, we propose a bidirectional soft haptic I/O module that not only senses the haptic force but also generates a mechanical vibrotactile sensation. Under external pressure, the distance between the moving plate and lower electrode layer decreases, and the bi-convex patterned poly vinyl chloride (bpPVC) gel gets compressed. These two motions make the capacitance of the proposed module change. Moreover, the application of external electric field (EF) creates an electrostatic force between the upper and lower electrode layers and generates the electric-field-induced deformation of the bpPVC gel simultaneously. As soon as the external EF disappears, the proposed module regains its original shape through the elastic restoring forces of the bpPVC gel and planar springs. Therefore, the applied AC voltage makes the proposed module vibrate. The dielectric and mechanical properties of the bpPVC gel are measured to investigate the optimal weight ratio of the PVC and plasticizers. Experiments are conducted to measure the haptic sensing and actuating performance of the proposed method. The capacitance of the proposed haptic I/O module increases from 17.4 pF to 54.8 pF when the external pressure varied from 0 kPa to 100 kPa. On the other hand, the haptic output of the proposed I/O module is observed as 0.81<jats:italic>g</jats:italic> (<jats:italic>g</jats:italic> = 9.8 m s<jats:sup>−2</jats:sup>) at 100 Hz. The results clearly indicate that the proposed haptic I/O module not only senses the static and dynamic pressure but also controls the amplitude of vibrotactile sensation. Owing to its mechanically soft structure, we expect that the proposed haptic I/O module has the potential to be applied or attached to various flexible/soft devices or the human body.</jats:p>
SP  - 045007
EP  - NA
JF  - Smart Materials and Structures
VL  - 30
IS  - 4
PB  - 
DO  - 10.1088/1361-665x/abe3aa
ER  - 

TY  - NA
AU  - Choi, Frederick; Mayer, Sven; Harrison, Chris
TI  - MobileHCI - 3D Hand Pose Estimation on Conventional Capacitive Touchscreens
PY  - 2021
AB  - Contemporary mobile devices with touchscreens capture the X/Y position of finger tips on the screen and pass these coordinates to applications as though the input were points in space. Of course, human hands are much more sophisticated, able to form rich 3D poses capable of far more complex interactions than poking at a screen. In this paper, we describe how conventional capacitive touchscreens can be used to estimate 3D hand pose, enabling richer interaction opportunities. Importantly, our software-only approach requires no special or new sensors, either internal or external. As a proof of concept, we use an off-the-shelf Samsung Tablet flashed with a custom kernel. After describing our software pipeline, we report findings from our user study, we conclude with several example applications we built to illustrate the potential of our approach.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472045
ER  - 

TY  - NA
AU  - Teng, Shan-Yuan; Wu, K. D.; Chen, Jacqueline; Lopes, Pedro
TI  - Prolonging VR Haptic Experiences by Harvesting Kinetic Energy from the User
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545635
ER  - 

TY  - NA
AU  - Yu, Difeng; Desai, Ruta; Zhang, Ting; Benko, Hrvoje; Jonker, Tanya R.; Gupta, Aakar
TI  - Optimizing the Timing of Intelligent Suggestion in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545632
ER  - 

TY  - NA
AU  - Aoki, Hiroaki; Ohnishi, Ayumi; Isoyama, Naoya; Terada, Tsutomu; Tsukamoto, Masahiko
TI  - AHs - FaceRecGlasses: A Wearable System for Recognizing Self Facial Expressions Using Compact Wearable Cameras
PY  - 2021
AB  - Facial expression images are useful for life-logging because they represent one’s emotions and mental state. However, using a wearable system to capture facial expressions from the front of the user obstructs the field of view. In this paper, we propose FaceRecGlasses, a compact wearable system that constantly records the user’s face and surroundings as images and indexes them with emotions without obstructing the user’s daily life. Our eyeglass-shaped device comprises two compact cameras and three mirrors, which enable the user’s facial expressions and surroundings to be captured constantly. The proposed system outputs a real-time pseudo-face-image by combining the captured facial components with a stored base-face-image. We investigated suitable facial areas to be captured by the proposed system and evaluated the recognition accuracy of the facial expressions captured. The results confirmed accuracies of 87.5%, 66.7%, and 71.4% for neutral emotion, happiness, and surprise, respectively, indicating that this method reproduces these three facial expressions to a high degree and can be applied to indexing of facial images.
SP  - 55
EP  - 65
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458983
ER  - 

TY  - NA
AU  - Sendhilnathan, Naveen; Zhang, Ting; Lafreniere, Ben; Grossman, Tovi; Jonker, Tanya R.
TI  - Detecting Input Recognition Errors and User Errors using Gaze Dynamics in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545628
ER  - 

TY  - JOUR
AU  - Zhang, Dingtian; Park, Jung Wook; Zhang, Yang; Zhao, Yuhui; Wang, Yiyang; Li, Yunzhi; Bhagwat, Tanvi; Chou, Wen-Fang; Jia, Xiaojia; Kippelen, Bernard; Fuentes-Hernandez, Canek; Starner, Thad; Abowd, Gregory D.
TI  - OptoSense: Towards Ubiquitous Self-Powered Ambient Light Sensing Surfaces
PY  - 2020
AB  - Ubiquitous computing requires robust and sustainable sensing techniques to detect users for explicit and implicit inputs. Existing solutions with cameras can be privacy-invasive. Battery-powered sensors require user maintenance, preventing practical ubiquitous sensor deployment. We present OptoSense, a general-purpose self-powered sensing system which senses ambient light at the surface level of everyday objects as a high-fidelity signal to infer user activities and interactions. To situate the novelty of OptoSense among prior work and highlight the generalizability of the approach, we propose a design framework of ambient light sensing surfaces, enabling implicit activity sensing and explicit interactions in a wide range of use cases with varying sensing dimensions (0D, 1D, 2D), fields of view (wide, narrow), and perspectives (egocentric, allocentric). OptoSense supports this framework through example applications ranging from object use and indoor traffic detection, to liquid sensing and multitouch input. Additionally, the system can achieve high detection accuracy while being self-powered by ambient light. On-going improvements that replace Optosense's silicon-based sensors with organic semiconductors (OSCs) enable devices that are ultra-thin, flexible, and cost effective to scale.
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1145/3411826
ER  - 

TY  - NA
AU  - Chowdhury, Tahiya; Aldeer, Murtadha; Yu, Justin; Florentine, Joseph; Haynes, Amber; Ortiz, Jorge
TI  - Is General Purpose Sensing a Pipe Dream?: A Case Study in Ambient Multi-sensing for Human Activity Recognition
PY  - 2021
AB  - Smart spaces equipped with sensors and combined with learning are becoming increasingly popular for many ubiquitous computing applications. However, application performance depends on many factors, including the number of sensors deployed, the phenomena they measure, their placement throughout the space, and spatio-temporal human-activity distribution characteristics. This work examines the sensitivity of activity-recognition models on the placement and distribution of multi-sensing measurement points in space. We present the results of an empirical study, whereby multi-sensing measurement units are placed throughout a room and train models to identify the type of human activity they observe. Our results show that while our models achieve 95% accuracy, performance degrades with distance and can be highly dependent on unanticipated sensor types. Certain phenomena trigger sensors that are difficult to anticipate a priori, supporting the notion that multi-sensing is useful for general activity recognition and improves robustness. We also show that the pattern of triggered sensors may not be consistent for the same event. Our results suggest designing more adaptive classification models and can guide the design of future cyber-physical systems to capture human activities.
SP  - 21
EP  - 26
JF  - Proceedings of the First International Workshop on Cyber-Physical-Human System Design and Implementation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458648.3460010
ER  - 

TY  - NA
AU  - Ziyue, Zhang; Huang, Jin; Tian, Feng
TI  - CHI Extended Abstracts - Modeling the Uncertainty in Pointing of Moving Targets with Arbitrary Shapes
PY  - 2020
AB  - When we try to acquire moving targets such as shooting enemies in computer games, the shapes of these targets are often varied. Considering the effects of target shape in moving target selection is essential for predicting user performances such as error rate in user interfaces involving dynamic content. In this paper, we propose a model to be descriptive of the endpoint uncertainty in pointing of moving targets with arbitrary shapes. The model combines the Gaussian mixture model (GMM) with a Ternary-Gaussian model to describe the impacts of target shape and target motion on selection endpoints of moving targets. Compared to the-state-of-the-art, our model achieved higher performance in the fitting of endpoint distribution and predicting selection error rate.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382875
ER  - 

TY  - NA
AU  - Hanton, Ollie; Wessely, Michael; Mueller, Stefanie; Fraser, Mike; Roudaut, Anne
TI  - CHI Extended Abstracts - ProtoSpray: Combining 3D Printing and Spraying to Create Interactive Displays with Arbitrary Shapes
PY  - 2020
AB  - ProtoSpray is a fabrication method that combines 3D printing and spray coating, to create interactive displays of arbitrary shapes. Our approach makes novel use of 3D printed conductive channels to create base electrodes on 3D shapes. This is then combined with spraying active materials to produce illumination. We demonstrate the feasibility and benefits of this combined approach in 6 evaluations exploring different shaped topologies. We analyze factors such as spray orientations, surface topologies and printer resolutions, to discuss how spray nozzles can be integrated into traditional 3D printers. We present a series of ProtoSprayed objects demonstrating how our technique goes beyond existing fabrication techniques by allowing creation of displays on objects with curvatures as complex as a Mobius strip. Our work provides a platform to empower makers to use displays as a fabrication material.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376543
ER  - 

TY  - NA
AU  - DeLine, Robert
TI  - CHI - Glinda: Supporting Data Science with Live Programming, GUIs and a Domain-specific Language
PY  - 2021
AB  - Researchers have explored several avenues to mitigate data scientists’ frustrations with computational notebooks, including: (1) live programming, to keep notebook results consistent and up to date; (2) supplementing scripting with graphical user interfaces (GUIs), to improve ease of use; and (3) providing domain-specific languages (DSLs), to raise a script’s level of abstraction. This paper introduces Glinda, which combines these three approaches by providing a live programming experience, with interactive results, for a domain-specific language for data science. The language’s compiler uses an open-ended set of “recipes” to execute steps in the user’s data science workflow. Each recipe is intended to combine the expressiveness of a written notation with the ease-of-use of a GUI. Live programming provides immediate feedback to a user’s input, whether in the form of program edits or GUI gestures. In a qualitative evaluation with 12 professional data scientists, participants highly rated the live programming and interactive results. They found the language productive and sufficiently expressive and suggested opportunities to extend it.
SP  - 1
EP  - 11
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445267
ER  - 

TY  - NA
AU  - Signer, Madlaina; Ion, Alexandra; Sorkine-Hornung, Olga
TI  - CHI - Developable Metamaterials: Mass-fabricable Metamaterials by Laser-Cutting Elastic Structures
PY  - 2021
AB  - We propose a novel design of engineered, structured materials that leverages fast fabrication technologies, pushing them towards mass-fabrication. Specifically, our metamaterial is designed to be laser cut, to approximate the volumetric shape and allow for locally varying compliance. Traditional mechanical metamaterials consist of intricate cells arranged on a 3-dimensional grid, limiting them to 3D printing—which is slow. Our metamaterial is designed for laser cutting, which is drastically faster. Our structures are best described as ruffled strips of thin sheet material, such as paper, plastics, metals, etc. Users can interactively define the ruffles’ anisotropic stiffness directions and local density. Our computational design tool assists users by automatically optimizing the ruffle to fill the shape’s volume, and exporting the flat ruffle design ready for cutting. We demonstrate how such ruffled metamaterials can be utilized for, e.g., custom toys with locally varying compliance, custom packaging material, or lightweight formwork for architectural shells.
SP  - 674
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445666
ER  - 

TY  - NA
AU  - Kimura, Naoki; Gemicioglu, Tan; Womack, Jonathan; Li, Richard; Zhao, Yuhui; Bedri, Abdelkareem; Su, Zixiong; Olwal, Alex; Rekimoto, Jun; Starner, Thad
TI  - SilentSpeller: Towards mobile, hands-free, silent speech text entry using electropalatography
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502015
ER  - 

TY  - JOUR
AU  - Gonzalez-Franco, Mar; Ofek, Eyal; Pan, Ye; Antley, Angus; Steed, Anthony; Spanlang, Bernhard; Maselli, Antonella; Banakou, Domna; Pelechano, Nuria; Orts-Escolano, Sergio; Orvalho, Verónica; Trutoiu, Laura; Wojcik, Markus; Sanchez-Vives, Maria V.; Bailenson, Jeremy N.; Slater, Mel; Lanier, Jaron
TI  - The Rocketbox library and the utility of freely available rigged avatars
PY  - 2020
AB  - As part of the open sourcing of the Microsoft Rocketbox avatar library for research and academic purposes, here we discuss the importance of rigged avatars for the Virtual and Augmented Reality (VR, AR) research community. Avatars, virtual representations of humans, are widely used in VR applications. Furthermore many research areas ranging from crowd simulation to neuroscience, psychology or sociology have used avatars to investigate new theories or to demonstrate how they influence human performance and interactions. We divide this paper in two main parts: the first one gives an overview of the different methods available to create and animate avatars. We cover the current main alternatives for face and body animation as well introduce upcoming capture methods. The second part presents the scientific evidence of the utility of using rigged avatars for embodiment but also for applications such as crowd simulation and entertainment. All in all this paper attempts to convey why rigged avatars will be key to the future of VR and its wide adoption.
SP  - 561558
EP  - NA
JF  - Frontiers in Virtual Reality
VL  - 1
IS  - NA
PB  - 
DO  - 10.3389/frvir.2020.561558
ER  - 

TY  - NA
AU  - Romat, Hugo; Fender, Andreas; Meier, Manuel; Holz, Christian
TI  - VR - Flashpen: A High-Fidelity and High-Precision Multi-Surface Pen for Virtual Reality
PY  - 2021
AB  - Digital pen interaction has become a first-class input modality for precision tasks such as writing, annotating, drawing, and 2D manipulation. The key enablers of digital inking are the capacitive or resistive sensors that are integrated in contemporary tablet devices. In Virtual Reality (VR), however, users typically provide input across large regions, hence limiting the suitability of using additional tablet devices for accurate pen input. In this paper, we present Flashpen, a digital pen for VR whose sensing principle affords accurately digitizing hand writing and intricate drawing, including small and quick turns. Flashpen re-purposes an inexpensive gaming mouse sensor that digitizes extremely fine grained motions in the micrometer range at over 8 kHz when moving on a surface. We combine Flashpen's high-fidelity relative input with the absolute tracking cues from a VR headset to enable pen interaction across a variety of VR applications. In our two-block evaluation, which consists of a tracing task and a writing task, we compare Flashpen to a professional drawing tablet (Wacom). With this, we demonstrate that Flashpen's fidelity matches the performance of state-of-the-art digitizers and approaches the fidelity of analog pens, while adding the flexibility of supporting a wide range of flat surfaces.
SP  - 306
EP  - 315
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00053
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Kato, Kunihiro; Kawakami, Rei
TI  - UIST (Adjunct Volume) - ShiftTouch: Sheet-type Interface Extending Capacitive Touch Inputs with Minimal Screen Occlusion
PY  - 2021
AB  - We present ShiftTouch, a sheet-type passive interface that provides multiple inputs for capacitive touch surfaces with minimal screen occlusion. It consists of a conductive layer and a masking layer that partially insulates the conductive one. ShiftTouch uses multiple linear electrodes to control the fine touch position. The touch input is triggered under the electrodes when several adjacent electrodes are grounded simultaneously. Although each input area shares some electrodes with neighboring input areas, the touch surface can identify the inputs from each input area by detecting the slight displacement of the touch position. Our interface is simple yet effective in implementing multiple input areas while reducing screen occlusion compared to existing approaches using finger-sized electrodes.
SP  - 86
EP  - 88
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480174
ER  - 

TY  - CHAP
AU  - Huang, Forrest; Schoop, Eldon; Ha, David; Nichols, Jeffrey; Canny, John
TI  - Sketch-based Creativity Support Tools using Deep Learning
PY  - 2021
AB  - Sketching is a natural and effective visual communication medium commonly used in creative processes. Recent developments in deep-learning models drastically improved machines’ ability in understanding and generating visual content. An exciting area of development explores deep-learning approaches used to model human sketches, opening opportunities for creative applications. This chapter describes three fundamental steps in developing deep-learning-driven creativity support tools that consume and generate sketches: (1) a data collection effort that generated a new paired dataset between sketches and mobile user interfaces; (2) a sketch-based user interface retrieval system adapted from state-of-the-art computer vision techniques; and, (3) a conversational sketching system that supports the novel interaction of a natural-language-based sketch/critique authoring process. In this chapter, we survey relevant prior work in both the deep-learning and human-computer interaction communities, document the data collection process and the systems’ architectures in detail, present qualitative and quantitative results, and paint the landscape of several future research directions in this exciting area.
SP  - 379
EP  - 415
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_12
ER  - 

TY  - NA
AU  - Jo, Jeyeon; Park, Huiju
TI  - ISWC - RFInsole: Batteryless Gait-Monitoring Smart Insole Based on Passive RFID Tags
PY  - 2021
AB  - There are growing demands for daily gait-monitoring smart insoles which are light, soft, and comfortable both in the general population interested in sports and people with disabilities, such as children with cerebral palsy or Autistic Spectrum Disorder. Currently available technologies are basically battery-powered systems, which are bulky, heavy, and not ideal for in-home and everyday use. This study introduces RFInsole, a battery-less smart insole based on RFID (Radio Frequency Identification), tracking the sequence of foot pressures within a stance. RFInsole utilizes passive RFID tags and push button switches, so that the foot pressure can activate the tags at each location. Soft, thin, affordable, and simple structure of the device open broad possibilities to implement the same system into lighter and soft applications such as socks or pressure-sensing garments.
SP  - 141
EP  - 143
JF  - 2021 International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460421.3478810
ER  - 

TY  - JOUR
AU  - Kawabe, Takahiro
TI  - Mid-Air Action Contributes to Pseudo-Haptic Stiffness Effects
PY  - 2019
AB  - Pseudo-haptic feedback takes advantage of a cross-modal integration between vision and haptics. Previous studies have shown that object stiffness can be rendered with pseudo-haptic feedback with external haptic inputs. This article explored whether the pseudo-haptic feedback was feasible with a mid-air action wherein no external haptic input was given. On each trial of the experiments, participants conduced a mid-air action to laterally move their hands as if they horizontally stretched an object in the display. In synchronized with the hands’ motion, the object horizontally deformed. The magnitude of the object deformation varied with the horizontal distance between participants’ hands (i.e., a hand distance). The ratio of deformation magnitudes to the hand distance (i.e., a deformation-to-distance ratio) was controlled; With a larger ratio, a smaller hand distance produced the maximum level of object deformation. The Poisson's ratio was also controlled; a higher Poisson's ratio produced a larger magnitude of vertical deformation. The participants were asked to report the stiffness of the objects with a five-point rating scale. Consequently, the stiffness rating decreased with the deformation-distance ratio and with the Poisson's ratio. The results indicate that pseudo-haptic stiffness can be rendered with mid-air action by manipulating the deformation-distance ratio and Poisson's ratio.
SP  - 18
EP  - 24
JF  - IEEE transactions on haptics
VL  - 13
IS  - 1
PB  - 
DO  - 10.1109/toh.2019.2961883
ER  - 

TY  - NA
AU  - Yu, Difeng; Lu, Xueshi; Shi, Rongkai; Liang, Hai-Ning; Dingler, Tilman; Velloso, Eduardo; Goncalves, Jorge
TI  - CHI - Gaze-Supported 3D Object Manipulation in Virtual Reality
PY  - 2021
AB  - This paper investigates integration, coordination, and transition strategies of gaze and hand input for 3D object manipulation in VR. Specifically, this work aims to understand whether incorporating gaze input can benefit VR object manipulation tasks, and how it should be combined with hand input for improved usability and efficiency. We designed four gaze-supported techniques that leverage different combination strategies for object manipulation and evaluated them in two user studies. Overall, we show that gaze did not offer significant performance benefits for transforming objects in the primary working space, where all objects were located in front of the user and within the arm-reach distance, but can be useful for a larger environment with distant targets. We further offer insights regarding combination strategies of gaze and hand input, and derive implications that can help guide the design of future VR systems that incorporate gaze input for 3D object manipulation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445343
ER  - 

TY  - CHAP
AU  - Takayama, Yuta; Ichikawa, Yuu; Kitagawa, Takumi; Shengmei, Song; Shizuki, Buntarou; Takahashi, Shin
TI  - HCI (2) - Touch Position Detection on the Front of Face Using Passive High-Functional RFID Tag with Magnetic Sensor
PY  - 2020
AB  - We used passive, high-functional radiofrequency identification (RFID) tags with magnetic sensors to detect front of face touch positions without the requirement for a battery. We implemented a prototype system consisting of a goggle-type device equipped with passive high-functional RFID tags with magnetic sensor, a ring with permanent magnets, and touch detection software for machine-learning. We evaluated the classification accuracy of the six front of face touch positions and a ‘no-touch’ case. The discrimination rate when using the learning data was 83% but the real-time discrimination was only 65%. In future, we will aim to improve the accuracy, and define more touch points and gesture inputs.
SP  - 523
EP  - 531
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_35
ER  - 

TY  - NA
AU  - Schröder, Michael; Cito, Jürgen
TI  - An Empirical Investigation of Command-Line Customization.
PY  - 2020
AB  - The interactive command line, also known as the shell, is a prominent mechanism used extensively by a wide range of software professionals (engineers, system administrators, data scientists, etc.). Shell customizations can therefore provide insight into the tasks they repeatedly perform, how well the standard environment supports those tasks, and ways in which the environment could be productively extended or modified. To characterize the patterns and complexities of command-line customization, we mined the collective knowledge of command-line users by analyzing more than 2.2 million shell alias definitions found on GitHub. Shell aliases allow command-line users to customize their environment by defining arbitrarily complex command substitutions. Using inductive coding methods, we found three types of aliases that each enable a number of customization practices: Shortcuts (for nicknaming commands, abbreviating subcommands, and bookmarking locations), Modifications (for substituting commands, overriding defaults, colorizing output, and elevating privilege), and Scripts (for transforming data and chaining subcommands). We conjecture that identifying common customization practices can point to particular usability issues within command-line programs, and that a deeper understanding of these practices can support researchers and tool developers in designing better user experiences. In addition to our analysis, we provide an extensive reproducibility package in the form of a curated dataset together with well-documented computational notebooks enabling further knowledge discovery and a basis for learning approaches to improve command-line workflows.
SP  - NA
EP  - NA
JF  - arXiv: Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Ikematsu, Kaori; Yamanaka, Shota
TI  - ScraTouch: Extending Interaction Technique Using Fingernail on Unmodified Capacitive Touch Surfaces
PY  - 2020
AB  - We present ScraTouch, an interaction technique using fingernails, as a new input modality by leveraging capacitive touch sensing. Differentiating between fingertip and fingernail touches requires only tens of milliseconds worth of shunt current data from unmodified capacitive touch surfaces, thus requires no hardware modification. ScraTouch is simple but practical technique for command invocation and mode switching. An evaluation using a point-and-select task on a touchpad showed that although the switching between the finger and nail in ScraTouch required a little more time compared with the baseline (finger touching without mode switching), in overall the operations, ScraTouch was just as fast as the baseline, and on average, 29 % faster than a long press with 500-ms threshold. We also confirmed that setting a simple threshold on the measured shunt current for recognition works robustly across users (97 % accuracy).
SP  - 81
EP  - 19
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1145/3411831
ER  - 

TY  - JOUR
AU  - Alkayyali, Amani; Iravantchi, Yasha; Herskovitz, Jaylin; Sample, Alanson P.
TI  - UbiChromics: Enabling Ubiquitously Deployable Interactive Displays with Photochromic Paint
PY  - 2022
AB  - <jats:p>Pervasive and interactive displays promise to present our digital content seamlessly throughout our environment. However, traditional display technologies do not scale to room-wide applications due to high per-unit-area costs and the need for constant wired power and data infrastructure. This research proposes the use of photochromic paint as a display medium. Applying the paint to any surface or object creates ultra-low-cost displays, which can change color when exposed to specific wavelengths of light. We develop new paint formulations that enable wide area application of photochromic material. Along with a specially modified wide-area laser projector and depth camera that can draw custom images and create on-demand, room-wide user interfaces on photochromic enabled surfaces. System parameters such as light intensity, material activation time, and user readability are examined to optimize the display. Results show that images and user interfaces can last up to 16 minutes and can be updated indefinitely. Finally, usage scenarios such as displaying static and dynamic images, ephemeral notifications, and the creation of on-demand interfaces, such as light switches and music controllers, are demonstrated and explored. Ultimately, the UbiChromics system demonstrates the possibility of extending digital content to all painted surfaces.</jats:p>
SP  - 118
EP  - 142
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567714
ER  - 

TY  - NA
AU  - Arakawa, Riku; Maekawa, Azumi; Kashino, Zendai; Inami, Masahiko
TI  - SUI - Hand with Sensing Sphere: Body-Centered Spatial Interactions with a Hand-Worn Spherical Camera
PY  - 2020
AB  - We propose a novel body-centered interaction system making use of a spherical camera attached to a hand. Its broad and unique field of view enables an all-in-one approach to sensing multiple pieces of contextual information in hand-based spatial interactions: (i) hand location on the body surface, (ii) hand posture, (iii) hand keypoints in certain postures, and (iv) the near-hand environment. The proposed system makes use of a deep-learning approach to perform hand location and posture recognition. The proposed system is capable of achieving high hand location and posture recognition accuracy, 85.0 % and 88.9 % respectively, after collecting sufficient data and training. Our result and example demonstrations show the potential of utilizing 360° cameras for vision-based sensing in context-aware body-centered spatial interactions.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418450
ER  - 

TY  - JOUR
AU  - Bhattacharjee, Sukanya; Chaudhuri, Parag
TI  - A Survey on Sketch Based Content Creation: from the Desktop to Virtual and Augmented Reality
PY  - 2020
AB  - NA
SP  - 757
EP  - 780
JF  - Computer Graphics Forum
VL  - 39
IS  - 2
PB  - 
DO  - 10.1111/cgf.14024
ER  - 

TY  - JOUR
AU  - Yamanaka, Shota
TI  - Utility of Crowdsourced User Experiments for Measuring the Central Tendency of User Performance: A Case of Error-Rate Model Evaluation in a Pointing Task.
PY  - 2022
AB  - The usage of crowdsourcing to recruit numerous participants has been recognized as beneficial in the human-computer interaction (HCI) field, such as for designing user interfaces and validating user performance models. In this work, we investigate its effectiveness for evaluating an error-rate prediction model in target pointing tasks. In contrast to models for operational times, a clicking error (i.e., missing a target) occurs by chance at a certain probability, e.g., 5%. Therefore, in traditional laboratory-based experiments, a lot of repetitions are needed to measure the central tendency of error rates. We hypothesize that recruiting many workers would enable us to keep the number of repetitions per worker much smaller. We collected data from 384 workers and found that existing models on operational time and error rate showed good fits (both <i>R</i> <sup>2</sup> > 0.95). A simulation where we changed the number of participants <i>N</i> <sub><i>P</i></sub> and the number of repetitions <i>N</i> <sub>repeat</sub> showed that the time prediction model was robust against small <i>N</i> <sub><i>P</i></sub> and <i>N</i> <sub>repeat</sub>, although the error-rate model fitness was considerably degraded. These findings empirically demonstrate a new utility of crowdsourced user experiments for collecting numerous participants, which should be of great use to HCI researchers for their evaluation studies.
SP  - 798892
EP  - NA
JF  - Frontiers in artificial intelligence
VL  - 5
IS  - NA
PB  - 
DO  - 10.3389/frai.2022.798892
ER  - 

TY  - NA
AU  - Shimobayashi, Hideki; Sasaki, Tomoya; Horie, Arata; Arakawa, Riku; Kashino, Zendai; Inami, Masahiko
TI  - AHs - Independent Control of Supernumerary Appendages Exploiting Upper Limb Redundancy
PY  - 2021
AB  - In the field of physical augmentation, researchers have attempted to extend human capabilities by expanding the number of human appendages. To fully realize the potential of having an additional appendage, supernumerary appendages should be independently controllable without interfering with the functionality of existing appendages. Herein, we propose a novel approach for controlling supernumerary appendages by exploiting upper limb redundancy. We present a headphone-style visual sensing device and a recognition system to estimate shoulder movement. Through a set of user experiments, we evaluate the feasibility of our system and reveal the potential of independent control using upper limb redundancy. Our results indicate that participants are able to intentionally give commands through their shoulder motions. Finally, we demonstrate the wide range of supernumerary appendage control applications that our novel approach enables and discuss future prospects for our work.
SP  - 19
EP  - 30
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458980
ER  - 

TY  - JOUR
AU  - Chen, Minchan; Lau, Manfred
TI  - A Motion-guided Interface for Modeling 3D Multi-functional Furniture
PY  - 2021
AB  - NA
SP  - 229
EP  - 240
JF  - Computer Graphics Forum
VL  - 40
IS  - 7
PB  - 
DO  - 10.1111/cgf.14416
ER  - 

TY  - NA
AU  - Dang, Hai; Benharrak, Karim; Lehmann, Florian; Buschek, Daniel
TI  - Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries
PY  - 2022
AB  - We propose a text editor to help users plan, structure and reflect on their writing process. It provides continuously updated paragraph-wise summaries as margin annotations, using automatic text summarization. Summary levels range from full text, to selected (central) sentences, down to a collection of keywords. To understand how users interact with this system during writing, we conducted two user studies (N=4 and N=8) in which people wrote analytic essays about a given topic and article. As a key finding, the summaries gave users an external perspective on their writing and helped them to revise the content and scope of their drafted paragraphs. People further used the tool to quickly gain an overview of the text and developed strategies to integrate insights from the automated summaries. More broadly, this work explores and highlights the value of designing AI tools for writers, with Natural Language Processing (NLP) capabilities that go beyond direct text generation and correction.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545672
ER  - 

TY  - NA
AU  - Costes, Antoine; Lécuyer, Anatole
TI  - The "Kinesthetic HMD": Enhancing Self-Motion Sensations in VR with Head-Based Force Feedback
PY  - 2021
AB  - The sensation of self-motion is essential in many virtual reality applications, from entertainment to training, such as flying and driving simulators. If the common approach used in amusement parks is to actuate the seats with cumbersome systems, multisensory integration can also be leveraged to get rich effects from lightweight solutions. In this short paper, we introduce a novel approach called the "Kinesthetic HMD": actuating a head-mounted display with force feedback in order to provide sensations of self-motion. We discuss its design considerations and demonstrate an augmented flight simulator use case with a proof-of-concept prototype. We conducted a user study assessing our approach's ability to enhance self-motion sensations. Taken together, our results show that our Kinesthetic HMD provides significantly stronger and more egocentric sensations than a visual-only self-motion experience. Thus, by providing congruent vestibular and proprioceptive cues related to balance and self-motion, the Kinesthetic HMD represents a promising approach for a variety of virtual reality applications in which motion sensations are prominent.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Xiao, Robert; Mayer, Sven; Harrison, Chris
TI  - MobileHCI - VibroComm: Using Commodity Gyroscopes for Vibroacoustic Data Reception
PY  - 2020
AB  - Inertial Measurement Units (IMUs) with gyroscopic sensors are standard in today's mobile devices. We show that these sensors can be co-opted for vibroacoustic data reception. Our approach, called VibroComm, requires direct physical contact to a transmitting (i.e., vibrating) surface. This makes interactions targeted and explicit in nature, making it well suited for contexts with many targets or requiring and intent. It also offers an orthogonal dimension of physical security to wireless technologies like Blue-tooth and NFC. Using our implementation, we achieve a transfer rate over 2000 bits/sec with less than 5% packet loss – an order of magnitude faster than prior IMU-based approaches at a quarter of the loss rate, opening new, powerful and practical use cases that could be enabled on mobile devices with a simple software update.
SP  - NA
EP  - NA
JF  - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379503.3403540
ER  - 

TY  - NA
AU  - Asahina, Ray; Nomoto, Takashi; Yoshida, Takatoshi; Watanabe, Yoshihiro
TI  - VR - Realistic 3D Swept-Volume Display with Hidden-Surface Removal Using Physical Materials
PY  - 2021
AB  - Conventional swept-volume displays can provide accurate physical cues for depth perception. However, the corresponding texture reproduction does not have high quality because such displays employ high-speed projectors with low bit-depth and low resolution. In this study, to address the limitation of swept-volume displays while retaining their advantages, a novel swept-volume three-dimensional (3D) display is proposed by incorporating physical materials as screens. Physical materials such as wool, felt, and so on are directly used for reproducing textures on a displayed 3D surface. Furthermore, we introduce the adaptive pattern generation based on real-time viewpoint tracking to perform the hidden-surface removal. Our algorithm leverages the ray-tracing concept and can run at high speed on GPU.
SP  - 113
EP  - 121
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00032
ER  - 

TY  - NA
AU  - Khurana, Rushil; Hodges, Steve
TI  - CHI - Beyond the Prototype: Understanding the Challenge of Scaling Hardware Device Production
PY  - 2020
AB  - The hardware research and development communities have invested heavily in tools and materials that facilitate the design and prototyping of electronic devices. Numerous easy-to-access and easy-to-use tools have streamlined the prototyping of interactive and embedded devices for experts and led to a remarkable growth in non-expert builders. However, there has been little exploration of challenges associated with moving beyond a prototype and creating hundreds or thousands of exact replicas - a process that is still challenging for many. We interviewed 25 individuals with experience taking prototype hardware devices into low volume production. We systematically investigated the common issues faced and mitigation strategies adopted. We present our findings in four main categories: (1) gaps in technical knowledge; (2) gaps in non-technical knowledge; (3) minimum viable rigor in manufacturing preparation; and (4) building relationships and a professional network. Our study unearthed several opportunities for new tools and processes to support the transition beyond a working prototype to cost effective low-volume manufacturing. These would complement the aforementioned tools and materials that support design and prototyping.
SP  - 1
EP  - 11
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376761
ER  - 

TY  - CHAP
AU  - Frutos-Pascual, Maite; Gale, Clara; Harrison, Jake Michael; Creed, Chris; Williams, Ian
TI  - INTERACT (1) - Character Input in Augmented Reality: An Evaluation of Keyboard Position and Interaction Visualisation for Head-Mounted Displays
PY  - 2021
AB  - Character input in immersive environments is a non trivial task that has attracted much attention in recent years. This paper presents an evaluation of keyboard position, orientation and interaction together with the influence of visual interaction feedback in a controlled character input task with 27 participants in Augmented Reality (AR). It presents 5 different keyboard locations (3 bounded to the headset and 2 bounded to the non-dominant hand of the user) and 3 visual interaction feedback methods (finger raycast, fingertip glow and both combined). Objective (completion time, accuracy, Key per Minute (KPM)) and subjective (After Scenario Questionnaire (ASQ)) metrics are presented. Results showed that keyboard placement had an effect on accuracy, KPM metrics and subjective preference, with keyboard visualisation parallel and bounded to the headset position and orientation outperforming other keyboard locations.
SP  - 480
EP  - 501
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85623-6_29
ER  - 

TY  - NA
AU  - Wessely, Michael; Jin, Yuhua; Nuengsigkapian, Cattalyya; Kashapov, Aleksei; Qamar, Isabel P. S.; Tsetserukou, Dzmitry; Mueller, Stefanie
TI  - CHI - ChromoUpdate: Fast Design Iteration of Photochromic Color Textures Using Grayscale Previews and Local Color Updates
PY  - 2021
AB  - ChromoUpdate is a texture transfer system for fast design iteration. For the early stages of design, ChromoUpdate provides a fast grayscale preview that enables a texture to be transferred in under one minute. Once designers are satisfied with the grayscale texture, ChromoUpdate supports designers in coloring the texture by transitioning individual pixels directly to a desired target color. Finally, if designers need to make a change to the color texture already transferred, ChromoUpdate can quickly transition individual pixels from one color to a new target color. ChromoUpdate accomplishes this by (1) using a UV projector rather than a UV LED, which enables pixels to be saturated individually rather than resetting the entire texture to black, and (2) providing two new texture transfer algorithms that allow for fast grayscale previews and color-to-color transitions. Our evaluation shows a significant increase in texture transfer speed for both the grayscale preview (89%) and color-to-color updates (11%).
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445391
ER  - 

TY  - BOOK
AU  - Liang, Rong-Hao; Chiumento, Alessandro; Zuniga, Marco; Pawelczak, Przemyslaw; Funk, Mathias; Chuang, Yaliang; Frens, Joep
TI  - EICS - CHIIoT 2: 2nd Workshop on Computer Human Interaction in IoT Applications
PY  - 2021
AB  - The CHIIoT workshop series brings together researchers and practitioners from human-computer interaction (HCI) design, computer science, and electrical engineering working on new challenges in industry and academia. In EICS 2021, This workshop will provide a platform for participants to review and discuss challenges and opportunities in the intersection of computer-human interaction and the internet of things, focusing on human-centered applications using emerging connectivity and sensing technologies. We aim to jointly develop a design space and identify opportunities for future research.
SP  - 77
EP  - 80
JF  - Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3459926.3464759
ER  - 

TY  - NA
AU  - Rodrigues, André; Nicolau, Hugo; Santos, André; Branco, Diogo; Rainey, Jay; Verweij, David; Smeddinck, Jan David; Montague, Kyle; Guerreiro, Tiago
TI  - Investigating the Tradeoffs of Everyday Text-Entry Collection Methods
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501908
ER  - 

TY  - CHAP
AU  - Yamamoto, Keiko; Kawaguchi, Hiroki; Tsujino, Yoshihiro
TI  - HCI (38) - Effect of Dialogs’ Arrangement on Accuracy and Workload for Confirming Input Data
PY  - 2020
AB  - Recently, a variety of web services are available through the Internet. On these web services, the users have to input data correctly such as personal information and ordering data on digital devices. If they make mistakes in their input data on this kind of services, it often makes serious troubles. To avoid these troubles, many services have functions like “confirmation dialog” for the users to confirm their input data. However, the traditional confirmation dialog can be skipped by simply clicking OK button on the dialog without confirming input data. To solve the problem, we proposed “Phantom Dialog” that shows simultaneously two parallel arranged dialogs with user’s input data and system-generated dummy data. It was found that the Phantom Dialog could make difficult to skip the process of the confirmation. But, it took longer confirmation time than the traditional confirmation dialog. In this paper, in order to reduce the time, we propose to increase the probability that user’s input data is shown on the left dialog, which is expected to be read firstly by the user on the Phantom Dialog. As the result of the evaluation with the three different probabilities on the Phantom Dialog, it is found that the participants can confirm their input data faster with left-biased probability without deterioration in the confirmation accuracy. Furthermore, most of them did not notice that the probabilities were different.
SP  - 587
EP  - 593
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-50726-8_77
ER  - 

TY  - NA
AU  - Jun, Eunice
TI  - Empowering domain experts to author valid statistical analyses
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558530
ER  - 

TY  - NA
AU  - Zhang, Tengxiang; Zeng, Xin; Zhang, Yinshuai; Jiang, Xin; Xu, Xuhai; Dey, Anind K; Chen, Yiqiang
TI  - BoldMove: Enabling IoT Device Control on Ubiquitous Touch Interfaces by Semantic Mapping and Sequential Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519805
ER  - 

TY  - JOUR
AU  - Gonzalez-Franco, Mar; Steed, Anthony; Hoogendyk, Steve; Ofek, Eyal
TI  - Using Facial Animation to Increase the Enfacement Illusion and Avatar Self-Identification
PY  - 2020
AB  - Through avatar embodiment in Virtual Reality (VR) we can achieve the illusion that an avatar is substituting our body: the avatar moves as we move and we see it from a first person perspective. However, self-identification, the process of identifying a representation as being oneself, poses new challenges because a key determinant is that we see and have agency in our own face. Providing control over the face is hard with current HMD technologies because face tracking is either cumbersome or error prone. However, limited animation is easily achieved based on speaking. We investigate the level of avatar enfacement, that is believing that a picture of a face is one's own face, with three levels of facial animation: (i) one in which the facial expressions of the avatars are static, (ii) one in which we implement lip-sync motion and (iii) one in which the avatar presents lip-sync plus additional facial animations, with blinks, designed by a professional animator. We measure self-identification using a face morphing tool that morphs from the face of the participant to the face of a gender matched avatar. We find that self-identification on avatars can be increased through pre-baked animations even when these are not photorealistic nor look like the participant.
SP  - 2023
EP  - 2029
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2020.2973075
ER  - 

TY  - CHAP
AU  - Ushiyama, Keigo; Takahashi, Akifumi; Kajimoto, Hiroyuki
TI  - Increasing Perceived Weight and Resistance by Applying Vibration to Tendons During Active Arm Movements
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>We proposed to use kinesthetic illusion to achieve wearable/portable haptic devices for kinesthetic feedback in VR experiences. The kinesthetic illusion is the illusion of limb movement typically induced by vibratory stimulation. We investigated how the kinesthetic illusion affected the perceived weight and resistance of the handheld object. We designed vibration patterns that simulate constant gravity and velocity-related resistance. Two experiments were conducted to measure changes in perceiving weight and resistance when wielding cylindrical weights and hand fans. The results of the experiments indicated that the designed kinesthetic illusions enhanced these sensations; the real weight was perceived heavier, and the real resistance was perceived larger. However, we could not find the explicit difference between the two stimulation patterns, and the resistance sensation induced by the illusion differed from the actual sensation of using the hand fans.</jats:p>
SP  - 93
EP  - 100
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06249-0_11
ER  - 

TY  - NA
AU  - Kim, Jin Hee; Huang, Kunpeng; White, Simone L; Conroy, Melissa; Kao, Cindy Hsin-Liu
TI  - Conference on Designing Interactive Systems - KnitDermis: Fabricating Tactile On-Body Interfaces Through Machine Knitting
PY  - 2021
AB  - We present KnitDermis, on-body interfaces that deliver expressive non-vibrating mechanotactile feedback on the wearer’s body. Fabricated through machine knitting, they embed shape-memory alloy micro-springs in knitted channels, which deliver tactile sensations on the skin when activated. KnitDermis interfaces take advantage of machine knitting’s shaping properties which allow it to generate slim, stretchable, and versatile forms that can conform to underexplored body locations, such as protruded joints and convex body locations. We introduce a fabrication approach and a series of case studies to design a wide range of form factors, textures, and tactile patterns, including compression, pinching, brushing, and twisting. We conduct a user study to elicit KnitDermis’ effectiveness and wearability on diverse body locations and engage users to unpack envisioned use cases and perceptions towards the interfaces. We draw insights from our extensive research-through-design investigations on the potential of knitting as a soft approach for close-body and expressive tactile interfaces.
SP  - 1183
EP  - 1200
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462007
ER  - 

TY  - NA
AU  - Weerasinghe, Maheshya
TI  - MobileHCI (Extended Abstracts) - Instructional Guidance in Extended Reality for Learning
PY  - 2021
AB  - Experiential learning is the process of learning through experience or more specifically “learning through reflection on doing”. These experiences can be simulated in Extended Reality (XR) systems– interactive environments generated by computer technology combining real and virtual worlds. Such systems have shown to be excellent tools for providing guidance to teach different skills and activities. In this research, we introduce a comprehensive design space grounded in (i) XR technologies, and (ii) theoretical underpinnings of experiential learning and guidance. We use this design space to describe key design decisions in prior work and elucidate existing gaps. Employing the design space, we intend to build an adaptive, personalised instructional guidance system in XR, which offers various levels of guidance that adapt to users’ learning progress. We utilise the Kolb’s experiential learning cycle considering all four stages and transitions within it. With XR technologies, we offer the environment that can provide guidance to support learners to move spontaneously from one stage to another in the experiential learning cycle and further progress on the learning spiral. It is the latter that we explore in this research by adopting potential learning scenarios such as language learning and spatial navigation. We attempt to investigate how different factors of instructional guidance, such as the amount (minimal guidance, full guidance, and adaptive guidance) and the type (from direct instructions to associative models based examples), would affect the experiential learning process and learning outcomes of different learning scenarios in XR environments.
SP  - NA
EP  - NA
JF  - Adjunct Publication of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447527.3474866
ER  - 

TY  - JOUR
AU  - Satō, Rintarō; Cohen, Michael
TI  - Particle System Parametrization with Bimanual Hand Gestures
PY  - 2022
AB  - <jats:p>General approaches in computer graphics to compose visual effects (VFX) usually involve editing textual modules and parameters or constructing procedural node networks. These techniques are used in many game engine editors and digital contents creation (DCC) tools. However, contemporary interfaces arc not intuitive, especially for inexperienced users. Therefore, the main aim of this research is to develop an intuitive interface for visual effects design which is easy to use for beginners of design and creating DCC but still useful for advanced and expert VFX artists. In this project, we developed a particle system parametrization (to define a particle system) and instantiation interface with a hand gesture recognition system in a VR (virtual reality) environment. This project uses an HMD (head-mounted display) device that supports 6 degrees-of-frcedom (DoFs) and hand gesture recognition via its front cameras. In an immersive environment, the user composes a visual effect by answering questions shown on GUI with hand gestures. As a result of this project, we succeeded in developing an application, deployablc to Android-based HMDs, to compose visual effects in VR scenes. The application features an archiving system, so that an exported visual efTect can be imported into Unity Editor to use in other projects or game compositions. Therefore, our application can be easily integrated into a DCC production workflow (“pipeline”).</jats:p>
SP  - 3023
EP  - NA
JF  - SHS Web of Conferences
VL  - 139
IS  - NA
PB  - 
DO  - 10.1051/shsconf/202213903023
ER  - 

TY  - NA
AU  - Dell, Nicola; Estrin, Deborah; Haraldsson, Harald; Ju, Wendy
TI  - Designing Extended Reality Guidance for Physical Caregiving Tasks
PY  - 2022
AB  - In this short paper we explore the opportunities and challenges of designing XR technologies to support the collaborative work between family caregivers and clinicians as they attend to the physical care needs of patients in the home setting.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00091
ER  - 

TY  - NA
AU  - Johnson, Janet G.; Gasques, Danilo; Sharkey, Tommy; Schmitz, Evan; Weibel, Nadir
TI  - CHI - Do You Really Need to Know Where “That” Is? Enhancing Support for Referencing in Collaborative Mixed Reality Environments
PY  - 2021
AB  - Mixed Reality has been shown to enhance remote guidance and is especially well-suited for physical tasks. Conversations during these tasks are heavily anchored around task objects and their spatial relationships in the real world, making referencing - the ability to refer to an object in a way that is understood by others - a crucial process that warrants explicit support in collaborative Mixed Reality systems. This paper presents a 2x2 mixed factorial experiment that explores the effects of providing spatial information and system-generated guidance to task objects. It also investigates the effects of such guidance on the remote collaborator’s need for spatial information. Our results show that guidance increases performance and communication efficiency while reducing the need for spatial information, especially in unfamiliar environments. Our results also demonstrate a reduced need for remote experts to be in immersive environments, making guidance more scalable, and expertise more accessible.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445246
ER  - 

TY  - JOUR
AU  - KasaharaShunichi, ; TakadaKazuma, 
TI  - Stealth Updates of Visual Information by Leveraging Change Blindness and Computational Visual Morphing
PY  - 2021
AB  - We present an approach for covert visual updates by leveraging change blindness with computationally generated morphed images. To clarify the design parameters for intentionally suppressing change ...
SP  - 1
EP  - 17
JF  - ACM Transactions on Applied Perception
VL  - 18
IS  - 4
PB  - 
DO  - 10.1145/3486581
ER  - 

TY  - NA
AU  - Kameoka, Takayuki; Kajimoto, Hiroyuki
TI  - WHC - Tactile Transfer of Finger Information through Suction Tactile Sensation in HMDs
PY  - 2021
AB  - Most head-mounted display (HMD)-based virtual reality (VR) experiences are visual and auditory. These experiences can be enriched by adding tactile modality. Haptic devices have been attached to hands or installed on desks, which require additional setup time. We developed a suction-type tactile display device embedded in an HMD that realizes a hands-free tactile experience by transferring the finger sensation in VR to the face. In this study, we optimized the suction device to provide pressure sensation rather than suction and experimented to verify that the device can present simple tactile cues along with the physical properties of softness and hardness for objects in virtual contact.
SP  - 949
EP  - 954
JF  - 2021 IEEE World Haptics Conference (WHC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/whc49131.2021.9517176
ER  - 

TY  - NA
AU  - Liu, Shi; Toreini, Peyman; Maedche, Alexander
TI  - Designing Gaze-Aware Attention Feedback for Learning in Mixed Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3547565
ER  - 

TY  - NA
AU  - Echterhoff, Jessica Maria; Wang, Edward
TI  - PAR: Personal Activity Radius Camera View for Contextual Sensing.
PY  - 2020
AB  - Contextual sensing using wearable cameras has seen a variety of different camera angles proposed to capture a wide gamut of different visual scenes. In this paper, we propose a new camera view that aims to capture the same visual information as many of the camera positions and orientations combined from a single camera view point. The camera, mounted on the corner of a glasses frame is pointing downwards towards the floor, a field-of-view we named Personal Activity Radius (PAR). The PAR field-of-view captures the visual information around a wearer's personal bubble, including items they interact with, their body motion, their surrounding environment, etc. In our evaluation, we tested the PAR view's interpretability by human labelers in two different activity tracking scenarios: food related behaviors and exercise tracking. Human labelers achieved an overall high level of precision in identifying body motions in exercise tracking of 91% precision and eating/drinking motions at 96% precision. Item interaction identification reached a precision of 86% precision for labeling grocery categories. We show a high level on the device setup and contextual views we were able to capture with the device. We see that the camera wide angle captures different activities such as driving, shopping, gym exercises, walking and eating and can observe the specific interaction item of the user as well as the immediate contextual surrounding.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gasques, Danilo; Johnson, Janet G.; Sharkey, Tommy; Feng, Yuanyuan; Wang, Ru; Xu, Zhuoqun Robin; Zavala, Enrique; Zhang, Yifei; Xie, Wanze; Zhang, Xinming; Davis, Konrad; Yip, Michael C.; Weibel, Nadir
TI  - CHI - ARTEMIS: A Collaborative Mixed-Reality System for Immersive Surgical Telementoring
PY  - 2021
AB  - Traumatic injuries require timely intervention, but medical expertise is not always available at the patient’s location. Despite recent advances in telecommunications, surgeons still have limited tools to remotely help inexperienced surgeons. Mixed Reality hints at a future where remote collaborators work side-by-side as if co-located; however, we still do not know how current technology can improve remote surgical collaboration. Through role-playing and iterative-prototyping, we identify collaboration practices used by expert surgeons to aid novice surgeons as well as technical requirements to facilitate these practices. We then introduce ARTEMIS, an AR-VR collaboration system that supports these key practices. Through an observational study with two expert surgeons and five novice surgeons operating on cadavers, we find that ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips. Most participants found that ARTEMIS facilitates collaboration despite existing technology limitations explored in this paper.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445576
ER  - 

TY  - NA
AU  - Elkin, Lisa A.; Kay, Matthew; Higgins, James J.; Wobbrock, Jacob O.
TI  - UIST - An Aligned Rank Transform Procedure for Multifactor Contrast Tests
PY  - 2021
AB  - Data from multifactor HCI experiments often violates the assumptions of parametric tests (i.e., nonconforming data). The Aligned Rank Transform (ART) has become a popular nonparametric analysis in HCI that can find main and interaction effects in nonconforming data, but leads to incorrect results when used to conduct post hoc contrast tests. We created a new algorithm called ART-C for conducting contrast tests within the ART paradigm and validated it on 72,000 synthetic data sets. Our results indicate that ART-C does not inflate Type I error rates, unlike contrasts based on ART, and that ART-C has more statistical power than a t-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also extended an open-source tool called ARTool with our ART-C algorithm for both Windows and R. Our validation had some limitations (e.g., only six distribution types, no mixed factorial designs, no random slopes), and data drawn from Cauchy distributions should not be analyzed with ART-C.
SP  - 754
EP  - 768
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474784
ER  - 

TY  - JOUR
AU  - Kortum, Philip
TI  - Where's my jetpack?
PY  - 2022
AB  - NA
SP  - 68
EP  - 71
JF  - Interactions
VL  - 29
IS  - 5
PB  - 
DO  - 10.1145/3551900
ER  - 

TY  - NA
AU  - Yamanaka, Shota; Usuba, Hiroki; Takahashi, Haruki; Miyashita, Homei
TI  - UIST - Servo-Gaussian Model to Predict Success Rates in Manual Tracking: Path Steering and Pursuit of 1D Moving Target
PY  - 2020
AB  - We propose a Servo-Gaussian model to predict success rates in continuous manual tracking tasks. Two tasks were conducted to validate this model: path steering and pursuit of a 1D moving target. We hypothesized that (1) hand movements follow the servo-mechanism model, (2) submovement endpoints form a bivariate Gaussian distribution, thus enabling us to predict the success rate at which a submovement endpoint falls inside the tolerance, and (3) the success rate for a whole trial can be predicted if the number of submovements is known. The cross-validation showed R^2>0.92 and MAE0.95 and MAE
SP  - 844
EP  - 857
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415896
ER  - 

TY  - JOUR
AU  - Yamanaka, Shota; Usuba, Hiroki
TI  - Rethinking the Dual Gaussian Distribution Model for Predicting Touch Accuracy in On-screen-start Pointing Tasks
PY  - 2020
AB  - The dual Gaussian distribution hypothesis has been used to predict the success rate of target pointing on touchscreens. Bi and Zhai evaluated their success-rate prediction model in off-screen-start pointing tasks. However, we found that their prediction model could also be used for on-screen-start pointing tasks. We discuss the reasons why and empirically validate our hypothesis in a series of four experiments with various target sizes and distances. The prediction accuracy of Bi and Zhai's model was high in all of the experiments, with a 10-point absolute (or 14.9% relative) prediction error at worst. Also, we show that there is no clear benefit to integrating the target distance when predicting the endpoint variability and success rate.
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427333
ER  - 

TY  - NA
AU  - Twigg-Smith, Hannah; O'Leary, Jasper Tran; Peek, Nadya
TI  - CHI - Tools, Tricks, and Hacks: Exploring Novel Digital Fabrication Workflows on #PlotterTwitter
PY  - 2021
AB  - As digital fabrication machines become widespread, online communities have provided space for diverse practitioners to share their work, troubleshoot, and socialize. These communities pioneer increasingly novel fabrication workflows, and it is critical that we understand and conceptualize these workflows beyond traditional manufacturing models. To this end, we conduct a qualitative study of #PlotterTwitter, an online community developing custom hardware and software tools to create artwork with computer-controlled drawing machines known as plotters. We documented and analyzed emergent themes where the traditional interpretation of digital fabrication workflows fails to capture important nuances and nascent directions. We find that #PlotterTwitter makers champion creative exploration of interwoven digital and physical materials over a predictable series of steps. We discuss how this challenges long-running views of digital fabrication and propose design implications for future frameworks and toolkits to account for this breadth of practice.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445653
ER  - 

TY  - NA
AU  - Endow, Shreyosi; Torres, César I.
TI  - Conference on Designing Interactive Systems - “I’m Better Off on my Own”: Understanding How a Tutorial’s Medium Affects Physical Skill Development
PY  - 2021
AB  - The shift towards distance learning brought forth by the pandemic has highlighted the shortcomings of teaching physical skills at a distance. With the emergence of new augmented and connected mediums, new opportunities arise for transferring physical skills that have resisted traditional documentation methods. However, there lacks a framework that allows tutorial authors to capitalize on a new medium’s unique affordances rather than remediating existing tutorial conventions. Our work analyzes a body of tutorials rendered in various mediums for centering clay on a pottery wheel — a foundational skill that exemplifies the difficulties of physical skill transfer. Through the lens of McLuhan’s “The Medium is the Message” we synthesize a taxonomy of medium conventions and themes derived from analyzing a body of centering tutorials and observation of how a tutorial’s medium affects how learners develop physical skills. We leverage our findings to motivate design recommendations to inform how new mediums can support material practices.
SP  - 1313
EP  - 1323
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462066
ER  - 

TY  - JOUR
AU  - He, Han; Chen, Xiaochen; Mehmood, Adnan; Raivio, Leevi; Huttunen, Heikki; Raumonen, Pasi; Virkki, Johanna
TI  - ClothFace: A Batteryless RFID-Based Textile Platform for Handwriting Recognition.
PY  - 2020
AB  - This paper introduces a prototype of ClothFace technology, a battery-free textile-based handwriting recognition platform that includes an e-textile antenna and a 10 × 10 array of radio frequency identification (RFID) integrated circuits (ICs), each with a unique ID. Touching the textile platform surface creates an electrical connection from specific ICs to the antenna, which enables the connected ICs to be read with an external UHF (ultra-haigh frequency) RFID reader. In this paper, the platform is demonstrated to recognize handwritten numbers 0-9. The raw data collected by the platform are a sequence of IDs from the touched ICs. The system converts the data into bitmaps and their details are increased by interpolating between neighboring samples using the sequential information of IDs. These images of digits written on the platform can be classified, with enough accuracy for practical use, by deep learning. The recognition system was trained and tested with samples from six volunteers using the platform. The real-time number recognition ability of the ClothFace technology is demonstrated to work successfully with a very low error rate. The overall recognition accuracy of the platform is 94.6% and the accuracy for each digit is between 91.1% and 98.3%. As the solution is fully passive and gets all the needed energy from the external RFID reader, it enables a maintenance-free and cost-effective user interface that can be integrated into clothing and into textiles around us.
SP  - 4878
EP  - NA
JF  - Sensors (Basel, Switzerland)
VL  - 20
IS  - 17
PB  - 
DO  - 10.3390/s20174878
ER  - 

TY  - NA
AU  - Yang, Jackie; Holz, Christian; Ofek, Eyal; Wilson, Andrew D.
TI  - UIST - DreamWalker: Substituting Real-World Walking Experiences with a Virtual Reality
PY  - 2019
AB  - We explore a future in which people spend considerably more time in virtual reality, even during moments when they transition between locations in the real world. In this paper, we present DreamWalker, a VR system that enables such real-world walking while users explore and stay fully immersed inside large virtual environments in a headset. Provided with a real-world destination, DreamWalker finds a similar path in a pre-authored VR environment and guides the user while real-walking the virtual world. To keep the user from colliding with objects and people in the real-world, DreamWalker's tracking system fuses GPS locations, inside-out tracking, and RGBD frames to 1) continuously and accurately position the user in the real world, 2) sense walkable paths and obstacles in real time, and 3) represent paths through a dynamically changing scene in VR to redirect the user towards the chosen destination. We demonstrate DreamWalker's versatility by enabling users to walk three paths across the large Microsoft campus while enjoying pre-authored VR worlds, supplemented with a variety of obstacle avoidance and redirection techniques. In our evaluation, 8 participants walked across campus along a 15-minute route, experiencing a lively virtual Manhattan that was full of animated cars, people, and other objects.
SP  - 1093
EP  - 1107
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347875
ER  - 

TY  - NA
AU  - Noma, Yuta; Narumi, Koya; Okuya, Fuminori; Kawahara, Yoshihiro
TI  - UIST - Pop-up Print: Rapidly 3D Printing Mechanically Reversible Objects in the Folded State
PY  - 2020
AB  - Despite recent advancements in 3D printing technology, which allows users to rapidly produce 3D objects, printing tall and/or large objects still consumes more time and large amount of support material. In order to address these problems, we propose Pop-up Print, a method to 3D print an object in a compact "folded" state and then unfold it after printing to achieve the final artifact. Using this method, we can reduce the object's print height and volume, which directly affects the printing time and support material consumption. In addition, thanks to the reversibility of folding/unfolding, we can reversibly minimize the printed object's volume when unused for storage or transportation, and expand it only in use. To achieve Pop-up Print, we first conducted an experiment using selected printed sample objects with several parameters, in order to determine suitable crease patterns that make both the unfolded and folded state mechanically stable. Based on this result, we developed an interactive design tool to convert 3D models - such as a Stanford Bunny or a Huffman's cone - to the folded shape. Our design tool allows users to decide non-intuitive parameters that may affect the form's mechanical stability, while maintaining both functional crease patterns and the object's original form factor. Finally, we demonstrate the feasibility of our method through several examples of folded objects.
SP  - 58
EP  - 70
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415853
ER  - 

TY  - NA
AU  - Wilberz, Alexander; Leschtschow, Dominik; Trepkowski, Christina; Maiero, Jens; Kruijff, Ernst; Riecke, Bernhard E.
TI  - CHI - FaceHaptics: Robot Arm based Versatile Facial Haptics for Immersive Environments
PY  - 2020
AB  - This paper introduces FaceHaptics, a novel haptic display based on a robot arm attached to a head-mounted virtual reality display. It provides localized, multi-directional and movable haptic cues in the form of wind, warmth, moving and single-point touch events and water spray to dedicated parts of the face not covered by the head-mounted display.The easily extensible system, however, can principally mount any type of compact haptic actuator or object. User study 1 showed that users appreciate the directional resolution of cues, and can judge wind direction well, especially when they move their head and wind direction is adjusted dynamically to compensate for head rotations. Study 2 showed that adding FaceHaptics cues to a VR walkthrough can significantly improve user experience, presence, and emotional responses.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376481
ER  - 

TY  - NA
AU  - Roumen, Thijs; Apel, Ingo; Shigeyama, Jotaro; Muhammad, Abdullah; Baudisch, Patrick
TI  - UIST - Kerf-Canceling Mechanisms: Making Laser-Cut Mechanisms Operate across Different Laser Cutters
PY  - 2020
AB  - Getting laser-cut mechanisms, such as those in micro-scopes, robots, vehicles, etc., to work, requires all their components to be dimensioned precisely. This precision, however, tends to be lost when fabricating on a differ-ent laser cutter, as it is likely to remove more or less mate-rial (aka 'kerf'). We address this with what we call kerf-canceling mechanisms. Kerf-canceling mechanisms replace laser-cut bearings, sliders, gear pairs, etc. Unlike their tradi-tional counterparts, however, they keep working when manufactured on a different laser cutter and/or with different kerf. Kerf-canceling mechanisms achieve this by adding an additional wedge element per mechanism. We have created a software tool KerfCanceler that locates traditional mecha-nisms in cutting plans and replaces them with their kerf-canceling counterparts. We evaluated our tool by converting 17 models found online to kerf-invariant models; we evaluated kerf-canceling bearings by testing with kerf values ranging from 0mm and 0.5mm and find that they perform reliably independent of this kerf.
SP  - 293
EP  - 303
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415895
ER  - 

TY  - NA
AU  - Tian, Rundong; Paulos, Eric
TI  - UIST - Adroid: Augmenting Hands-on Making with a Collaborative Robot
PY  - 2021
AB  - Adroid1 enables users to borrow precision and accuracy from a robotic arm when using hand-held tools. When a tool is mounted to the robot, the user can hold and move the tool directly—Adroid measures the user’s applied forces and commands the robot to move in response. Depending on the tool and scenario, Adroid can selectively restrict certain motions. In the resulting interaction, the robot acts like a virtual “jig” which constrains the tool’s motion, augmenting the user’s accuracy, technique, and strength, while not diminishing their agency during open-ended fabrication tasks. We complement these hands-on interactions with projected augmented reality for visual feedback about the state of the system. We show how tools augmented by Adroid can support hands-on making and discuss how it can be configured to support other tasks within and beyond fabrication.
SP  - 270
EP  - 281
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474749
ER  - 

TY  - JOUR
AU  - Li, Zhaoyang; Ma, Yuan; Zhang, Kaijun; Wan, Jun; Zhao, Dazhe; Pi, Yucong; Chen, Gangjin; Zhang, Jianfeng; Tang, Wei; Lin, Liwei; Zhong, Junwen
TI  - Air Permeable Vibrotactile Actuators for Wearable Wireless Haptics
PY  - 2022
AB  - NA
SP  - 2211146
EP  - NA
JF  - Advanced Functional Materials
VL  - NA
IS  - NA
PB  - 
DO  - 10.1002/adfm.202211146
ER  - 

TY  - NA
AU  - Zhang, Sunny; Jones, Brennan; Rintel, Sean; Neustaedter, Carman
TI  - CSCW Companion - XRmas: Extended Reality Multi-Agency Spaces for a Magical Remote Christmas
PY  - 2021
AB  - The COVID-19 pandemic has raised attention toward remote and hybrid communications. Currently, one highly-studied solution lets a remote user use virtual reality (VR) to enter an immersive view of a local space, and local users use augmented reality (AR) to see the remote user's representation and digital contents. Such systems give the remote user a sense of ‘being there’, but we identify two more challenges to address. First, current systems provide remote users with limited agency to control objects and influence the local space. It is necessary to further explore the relationship between users, virtual objects, and physical objects, and how they can play a role in providing richer agency. Second, current systems often try to replicate in-person experiences, but hardly surpass them. We propose XRmas: an AR/VR telepresence system that (1) provides a multi-agency space that allows a remote user to manipulate both virtual and physical objects in a local space, and (2) introduces three family activities in a Christmas context that adopt holographic animation effects to create a ‘magical’ experience that takes users beyond merely the feeling of ‘being there’. We report on preliminary insights from the use of such a system in a remote family communication context.
SP  - 203
EP  - 207
JF  - Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462204.3481782
ER  - 

TY  - JOUR
AU  - Schröder, Michael; Cito, Jürgen
TI  - An empirical investigation of command-line customization
PY  - 2021
AB  - <jats:title>Abstract</jats:title><jats:p>The interactive command line, also known as the shell, is a prominent mechanism used extensively by a wide range of software professionals (engineers, system administrators, data scientists, etc.). Shell customizations can therefore provide insight into the tasks they repeatedly perform, how well the standard environment supports those tasks, and ways in which the environment could be productively extended or modified. To characterize the patterns and complexities of command-line customization, we mined the collective knowledge of command-line users by analyzing more than 2.2 million shell alias definitions found on GitHub. Shell aliases allow command-line users to customize their environment by defining arbitrarily complex command substitutions. Using inductive coding methods, we found three types of aliases that each enable a number of customization practices:<jats:sc>Shortcuts</jats:sc>(for<jats:italic>nicknaming commands</jats:italic>,<jats:italic>abbreviating subcommands</jats:italic>, and<jats:italic>bookmarking locations</jats:italic>),<jats:sc>Modifications</jats:sc>(for<jats:italic>substituting commands</jats:italic>,<jats:italic>overriding defaults</jats:italic>,<jats:italic>colorizing output</jats:italic>, and<jats:italic>elevating privilege</jats:italic>), and<jats:sc>Scripts</jats:sc>(for<jats:italic>transforming data</jats:italic>and<jats:italic>chaining subcommands</jats:italic>). We conjecture that identifying common customization practices can point to particular usability issues within command-line programs, and that a deeper understanding of these practices can support researchers and tool developers in designing better user experiences. In addition to our analysis, we provide an extensive reproducibility package in the form of a curated dataset together with well-documented computational notebooks enabling further knowledge discovery and a basis for learning approaches to improve command-line workflows.</jats:p>
SP  - NA
EP  - NA
JF  - Empirical Software Engineering
VL  - 27
IS  - 2
PB  - 
DO  - 10.1007/s10664-021-10036-y
ER  - 

TY  - NA
AU  - Yoshida, Takatoshi; Ogawa, Junichi; Choi, Kyung Yun; Bushnaq, Sanad; Nakagaki, Ken; Ishii, Hiroshi
TI  - TEI - inDepth: Force-based Interaction with Objects beyond A Physical Barrier
PY  - 2021
AB  - We propose inDepth, a novel system that enables force-based interaction with objects beyond a physical barrier by using scalable force sensor modules. inDepth transforms a physical barrier (eg. glass showcase or 3D display) to a tangible input interface that enables users to interact with objects out of reach, by applying finger pressure on the barrier’s surface. To achieve this interaction, our system tracks the applied force as a directional vector by using three force sensors installed underneath the barrier. Meanwhile, our force-to-depth conversion algorithm translates force intensity into a spatial position along its direction beyond the barrier. Finally, the system executes various operations on objects in that position based on the type of application. In this paper, we introduce inDepth concept and its design space. We also demonstrate example applications, including selecting items in showcases and manipulating 3D rendered models.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3442447
ER  - 

TY  - JOUR
AU  - Guo, Shihui; Shi, Yubin; Xiao, Pintong; Fu, Yinan; Lin, Juncong; Zeng, Wei; Lee, Tong-Yee
TI  - Creative and Progressive Interior Color Design with Eye-tracked User Preference
PY  - 2022
AB  - <jats:p>Interior scene colorization is vastly demanded in areas such as personalized architecture design. Existing works either require manual efforts to colorize individual objects, or conform to fixed color patterns automatically learned from prior knowledge, whilst neglecting user preference. Quantitatively identifying user preference is challenging, particularly at the early stage of the design process. The 3D setup also presents new challenges as the inhabitant can observe from any possible viewpoints. We propose a representative view selection method based on visual attention, and a progressive preference inference model. We particularly focus on the progressive integration of eye-tracked user preference, which enables the assistance in creativity support and allows the possibility of convergent thinking. A series of user studies have been conducted to validate the effectiveness of the proposed view selection method, preference inference model and the creativity support.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3542922
ER  - 

TY  - NA
AU  - Ma, Jiaju; Wei, Li-Yi; Kazi, Rubaiat Habib
TI  - A Layered Authoring Tool for Stylized 3D animations
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501894
ER  - 

TY  - NA
AU  - Mir, Muhammad Sarmad; Guzman, Borja Genoves; Varshney, Ambuj; Giustiniano, Domenico
TI  - PassiveLiFi: rethinking LiFi for low-power and long range RF backscatter
PY  - 2021
AB  - Light bulbs have been recently explored to design Light Fidelity (LiFi) communication to battery-free tags, thus complementing Radiofrequency (RF) backscatter in the uplink. In this paper, we show that LiFi and RF backscatter are complementary and have unexplored interactions. We introduce PassiveLiFi, a battery-free system that uses LiFi to transmit RF backscatter at a meagre power budget. We address several challenges on the system design in the LiFi transmitter, the tag and the RF receiver. We design the first LiFi transmitter that implements a chirp spread spectrum (CSS) using the visible light spectrum. We use a small bank of solar cells for communication and harvesting and reconfigure them based on the amount of harvested energy and desired data rate. We further alleviate the low responsiveness of solar cells with a new low-power receiver design in the tag. Experimental results with an RF carrier of 17 dBm show that we can generate RF backscatter with a range of 80.3 meters/μW consumed in the tag, which is almost double with respect to prior work.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447993.3483262
ER  - 

TY  - JOUR
AU  - García-Pereira, Inma; Casanova-Salas, Pablo; Gimeno, Jesús; Morillo, Pedro; Reiners, Dirk
TI  - Cross-Device Augmented Reality Annotations Method for Asynchronous Collaboration in Unprepared Environments
PY  - 2021
AB  - <jats:p>Augmented Reality (AR) annotations are a powerful way of communication when collaborators cannot be present at the same time in a given environment. However, this situation presents several challenges, for example: how to record the AR annotations for later consumption, how to align virtual and real world in unprepared environments or how to offer the annotations to users with different AR devices. In this paper we present a cross-device AR annotation method that allows users to create and display annotations asynchronously in environments without the need for prior preparation (AR markers, point cloud capture, etc.). This is achieved through an easy user-assisted calibration process and a data model that allows any type of annotation to be stored on any device. The experimental study carried out with 40 participants has verified our two hypotheses: we are able to visualize AR annotations in indoor environments without prior preparation regardless of the device used and the overall usability of the system is satisfactory.</jats:p>
SP  - 519
EP  - 519
JF  - Information
VL  - 12
IS  - 12
PB  - 
DO  - 10.3390/info12120519
ER  - 

TY  - JOUR
AU  - Yu, Kevin; Eck, Ulrich; Pankratz, Frieder; Lazarovici, Marc; Wilhelm, Dirk; Navab, Nassir
TI  - Duplicated Reality for Co-located Augmented Reality Collaboration.
PY  - 2022
AB  - When two or more users attempt to collaborate in the same space with Augmented Reality, they often encounter conflicting intentions regarding the occupation of the same working area and self-positioning around such without mutual interference. Augmented Reality is a powerful tool for communicating ideas and intentions during a co-assisting task that requires multi-disciplinary expertise. To relax the constraint of physical co-location, we propose the concept of Duplicated Reality, where a digital copy of a 3D region of interest of the users' environment is reconstructed in real-time and visualized in-situ through an Augmented Reality user interface. This enables users to remotely annotate the region of interest while being co-located with others in Augmented Reality. We perform a user study to gain an in-depth understanding of the proposed method compared to an in-situ augmentation, including collaboration, effort, awareness, usability, and the quality of the task. The result indicates almost identical objective and subjective results, except a decrease in the consulting user's awareness of co-located users when using our method. The added benefit from duplicating the working area into a designated consulting area opens up new interaction paradigms to be further investigated for future co-located Augmented Reality collaboration systems.
SP  - 2190
EP  - 2200
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2022.3150520
ER  - 

TY  - NA
AU  - Huang, Forrest; Schoop, Eldon; Ha, David; Canny, John
TI  - IUI - Scones: towards conversational authoring of sketches
PY  - 2020
AB  - Iteratively refining and critiquing sketches are crucial steps to developing effective designs. We introduce Scones, a mixed-initiative, machine-learning-driven system that enables users to iteratively author sketches from text instructions. Scones is a novel deep-learning-based system that iteratively generates scenes of sketched objects composed with semantic specifications from natural language. Scones exceeds state-of-the-art performance on a text-based scene modification task, and introduces a mask-conditioned sketching model that can generate sketches with poses specified by high-level scene information. In an exploratory user evaluation of Scones, participants reported enjoying an iterative drawing task with Scones, and suggested additional features for further applications. We believe Scones is an early step towards automated, intelligent systems that support human-in-the-loop applications for communicating ideas through sketching in art and design.
SP  - 313
EP  - 323
JF  - Proceedings of the 25th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3377325.3377485
ER  - 

TY  - CHAP
AU  - Bender, Stuart; Broderick, Mick
TI  - On the Excitement of Measuring the Virtual Reality Audience
PY  - 2021
AB  - This chapter engages with the current enthusiasm, both industrial and academic, around measuring audience response to immersive VR experiences. These exciting approaches to VR audience studies include such user experience (UX) approaches as survey methodologies, as well as more robust psychophysiological methodologies such as skin conductance level (SCL), facial electromyography (fEMG) and, increasingly, other forms of biometric response. Such approaches are still nascent in screen studies, and so this chapter will function in part as a primer for screen academics interested in how quantifying the audience can benefit a research-interpretative programme. However, these approaches are also being utilised with tremendous enthusiasm by other disciplines, but also in many applied areas of VR development. Therefore, the chapter indicates benefits of these approaches, while simultaneously problematising the universalising attitude of such methodologies. Thus, the chapter also foregrounds the benefits of a phenomenological methodology for designing an empirical audience study but also for interpreting the outcomes of other research into VR and audiences. Drawing upon Sinnerbrink’s (Sinnerbrink’s 2016) connection of phenomenology and cognitivist approaches to traditional screen texts, the chapter demonstrates that, while it is currently an exciting period to use quantifiable methods of audience analysis for VR, it is also clear that in many of these cases the methodologies are being applied without a critical understanding of the stimulus material, namely the VR content and the user’s embodied experience of it. The chapter then provides a meta-analysis of three recent studies on the VR zombie experience, providing a clear demonstration of the contribution that quantitative methodologies can bring to screen studies but at the same time demonstrating the value VR phenomenology can provide to quantitative studies.
SP  - 53
EP  - 75
JF  - Virtual Realities
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82547-8_3
ER  - 

TY  - JOUR
AU  - Miura, Teppei; Sako, Shinji
TI  - Simple yet effective 3D ego-pose lift-up based on vector and distance for a mounted omnidirectional camera
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Following the advances in convolutional neural networks and synthetic data generation, 3D egocentric body pose estimations from a mounted fisheye camera have been developed. Previous works estimated 3D joint positions from raw image pixels and intermediate supervision during the process. The mounted fisheye camera captures notably different images that are affected by the optical properties of the lens, angle of views, and setup positions. Therefore, 3D ego-pose estimation from a mounted fisheye camera must be trained for each set of camera optics and setup. We propose a 3D ego-pose estimation from a single mounted omnidirectional camera that captures the entire circumference by back-to-back dual fisheye cameras. The omnidirectional camera can capture the user’s body in the 360<jats:sup>∘</jats:sup> field of view under a wide variety of motions. We also propose a simple feed-forward network model to estimate 3D joint positions from 2D joint locations. The lift-up model can be used in real time yet obtains accuracy comparable to those of previous works on our new dataset. Moreover, our model is trainable with the ground truth 3D joint positions and the unit vectors toward the 3D joint positions, which are easily generated from existing publicly available 3D mocap datasets. This advantage alleviates the data collection and training burden due to changes in the camera optics and setups, although it is limited to the effect after the 2D joint location estimation.</jats:p>
SP  - 2616
EP  - 2628
JF  - Applied Intelligence
VL  - 53
IS  - 3
PB  - 
DO  - 10.1007/s10489-022-03417-3
ER  - 

TY  - NA
AU  - Gonzalez-Franco, Mar; Egan, Zelia; Peachey, Matthew; Antley, Angus; Randhavane, Tanmay; Panda, Payod; Zhang, Yaying; Wang, Cheng Yao; Reilly, Derek; Peck, Tabitha C.; Won, Andrea Stevenson; Steed, Anthony; Ofek, Eyal
TI  - AIVR - MoveBox: Democratizing MoCap for the Microsoft Rocketbox Avatar Library
PY  - 2020
AB  - This paper presents MoveBox an open sourced toolbox for animating motion captured (MoCap) movements onto the Microsoft Rocketbox library of avatars. Motion capture is performed using a single depth sensor, such as Azure Kinect or Windows Kinect V2. Motion capture is performed in real-time using a single depth sensor, such as Azure Kinect or Windows Kinect V2, or extracted from existing RGB videos offline leveraging deep-learning computer vision techniques. Our toolbox enables real-time animation of the user’s avatar by converting the transformations between systems that have different joints and hierarchies. Additional features of the toolbox include recording, playback and looping animations, as well as basic audio lip sync, blinking and resizing of avatars as well as finger and hand animations. Our main contribution is both in the creation of this open source tool as well as the validation on different devices and discussion of MoveBox’s capabilities by end users.
SP  - 91
EP  - 98
JF  - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/aivr50618.2020.00026
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Yamanaka, Shota
TI  - CHI Extended Abstracts - ScraTouch: Extending Touch Interaction Technique Using Fingernail on Capacitive Touch Surfaces
PY  - 2020
AB  - We present ScraTouch, an interaction technique using fingernails, as a new input modality for capacitive touch surfaces. We revealed the differences between the fingertip and the nail from the viewpoint of capacitance touch sensing. Differentiating between fingertip and fingernail touches requires only tens of milliseconds worth of shunt current data from the capacitive touch sensing mechanism, thus external sensors or other hardware are unnecessary. Owing to the small amount of friction generated when a nail touches the surface, not only tapping but also touch gestures by sliding a nail can be easily performed. In our initial investigation, we confirmed that setting a simple threshold on the measured shunt current for recognition works robustly.
SP  - 3382858
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382858
ER  - 

TY  - NA
AU  - Wang, Xizi
TI  - PPVR: Teaching and Learning to Play the Piano in Virtual Reality
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Arora, Nivedita; Mirzazadeh, Ali; Moon, Injoo; Ramey, Charles; Zhao, Yuhui; Rodriguez, Daniela C.; Abowd, Gregory D.; Starner, Thad
TI  - UIST - MARS: Nano-Power Battery-free Wireless Interfaces for Touch, Swipe and Speech Input
PY  - 2021
AB  - Augmenting everyday surfaces with interaction sensing capability that is maintenance-free, low-cost (∼ $1), and in an appropriate form factor is a challenge with current technologies. MARS (Multi-channel Ambiently-powered Realtime Sensing) enables battery-free sensing and wireless communication of touch, swipe, and speech interactions by combining a nanowatt programmable oscillator with frequency-shifted analog backscatter communication. A zero-threshold voltage field-effect transistor (FET) is used to create an oscillator with a low startup voltage (∼ 500 mV) and current (
SP  - 1305
EP  - 1325
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474823
ER  - 

TY  - NA
AU  - Xiao, Chang; Bayer, Karl; Zheng, Changxi; Nayar, Shree K.
TI  - CHI - BackTrack: 2D Back-of-device Interaction Through Front Touchscreen
PY  - 2021
AB  - We present BackTrack, a trackpad placed on the back of a smartphone to track fine-grained finger motions. Our system has a small form factor, with all the circuits encapsulated in a thin layer attached to a phone case. It can be used with any off-the-shelf smartphone, requiring no power supply or modification of the operating systems. BackTrack simply extends the finger tracking area of the front screen, without interrupting the use of the front screen. It also provides a switch to prevent unintentional touch on the trackpad. All these features are enabled by a battery-free capacitive circuit, part of which is a transparent, thin-film conductor coated on a thin glass and attached to the front screen. To ensure accurate and robust tracking, the capacitive circuits are carefully designed. Our design is based on a circuit model of capacitive touchscreens, justified through both physics-based finite-element simulation and controlled laboratory experiments. We conduct user studies to evaluate the performance of using BackTrack. We also demonstrate its use in a number of smartphone applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445374
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Tsao, Chih-An; Liao, Fang-Ying; Yong, Seraphina; Yeh, Tom; Chen, Bing-Yu
TI  - UIST - Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture
PY  - 2021
AB  - Design tools and research regarding laser-cut architectures have been widely explored in the past decade. However, such discussion has mostly revolved around technical and structural design questions instead of another essential element of laser-cut models — assembly — a process that relies heavily on components’ visual affordance, therefore less accessible to blind or low vision (BLV) people. To narrow the gap in this area, we co-designed with 7 BLV people to examine their assembly experience with different laser-cut architectures. From their feedback, we proposed several design heuristics and guidelines for Daedalus, a generative design tool that can produce tactile aids for laser-cut assembly given a few high-level manual inputs. We validate the proposed aids in a user study with 8 new BLV participants. Our results revealed that BLV users can manage laser-cut assembly more efficiently with Daedalus. Going forth from this design iteration, we discuss implications for future research on accessible laser-cut assembly.
SP  - 344
EP  - 358
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474754
ER  - 

TY  - JOUR
AU  - de Winkel, Jasper; Kortbeek, Vito; Hester, Josiah; Pawelczak, Przemyslaw
TI  - Battery-Free Game Boy: Sustainable Interactive Devices
PY  - 2021
AB  - Any future mobile electronic device with which a user interacts (smartphone, hand-held game console) should not pollute our planet. Consequently, designers need to rethink how to build mobile devices with fewer components that negatively impact the environment (by replacing batteries with energy harvesting sources) while not compromising the user experience quality. This article addresses the challenges of battery-free mobile interaction and presents the first battery-free, personal mobile gaming device powered by energy harvested from gamer actions and sunlight. Our design implements a power failure resilient Nintendo Game Boy emulator that can run off-the-shelf classic Game Boy games like Tetris or Super Mario Land. Beyond a fun toy, our design represents the first battery-free system design for continuous user attention despite frequent power failures caused by intermittent energy harvesting.
SP  - 22
EP  - 26
JF  - GetMobile: Mobile Computing and Communications
VL  - 25
IS  - 2
PB  - 
DO  - 10.1145/3486880.3486888
ER  - 

TY  - NA
AU  - Wang, Ju; Li, Jianyan; Mazaheri, Mohammad Hossein; Katsuragawa, Keiko; Vogel, Daniel; Abari, Omid
TI  - SenSys - Sensing finger input using an RFID transmission line
PY  - 2020
AB  - We introduce a passive Radio Frequency IDentification (RFID) based system to detect finger gesture input for Human-Computer Interaction applications. The device is simple, inexpensive and does not require calibration to accommodate changes in the device location or the Radio Frequency (RF) environment. This is achieved by connecting the chips of two RFID tags together using a strip transmission line. The key observation is that touching different positions along the transmission line changes the impedance matching between each chip and its antenna, changing Received Signal Strength (RSS) values for each tag. When a finger slides in different directions between key positions along the transmission line, there are relative RSS patterns and trends that are robust to changes in the device location and the RF environment. We implemented and evaluated an detection algorithm and system using a commercial RFID reader and two commercial RFID chips. Results show that precision and recall are greater than 95% and 94% when detecting 10 finger gesture inputs across 48 different device locations.
SP  - 531
EP  - 543
JF  - Proceedings of the 18th Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384419.3430712
ER  - 

TY  - JOUR
AU  - Fages, Arthur; Fleury, Cédric; Tsandilas, Theophanis
TI  - Understanding Multi-View Collaboration between Augmented Reality and Remote Desktop Users
PY  - 2022
AB  - <jats:p>Establishing an effective collaboration between augmented-reality (AR) and remote desktop users is a challenge because collaborators do not share a common physical space and equipment. Yet, such asymmetrical collaboration configurations are common today for many design tasks, due to the geographical distance of people or unusual circumstances such as a lockdown. We conducted a first study to investigate trade-offs of three remote representations of an AR workspace: a fully virtual representation, a first-person view, and an external view. Building on our findings, we designed ARgus, a multi-view video-mediated communication system that combines these representations through interactive tools for navigation, previewing, pointing, and annotation. We report on a second user study that observed how 12 participants used ARgus to provide remote instructions for an AR furniture arrangement task. Participants extensively used its view transition tools, while the system reduced their reliance on verbal instructions.</jats:p>
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555607
ER  - 

TY  - NA
AU  - Fossdal, Frikk H.; Heldal, Rogardt; Peek, Nadya
TI  - Interactive Digital Fabrication Machine Control Directly Within a CAD Environment
PY  - 2021
AB  - Interactive fabrication aims to close the gap between design and fabrication, allowing for rich interactions with materials and reflection in action. Drawing from craft practice, we contribute software that enables real-time control of digital fabrication machines from a Computer-Aided Design (CAD) environment. Our software not only allows interactive control of toolpath geometry, but also enables the control of machine parameters such as speed, acceleration, or jerk. This creates new opportunities for toolpath and material exploration. We evaluate our software with a professional glass artist on a custom digital fabrication machine that can accommodate multiple tools such as brushes, engraving bits, or microscopes. Finally, we reflect on implications for machine control.
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485114.3485120
ER  - 

TY  - NA
AU  - Sun, Wei
TI  - RFID - RFitness: Enabling Smart Yoga Mat for Fitness Posture Detection with Commodity Passive RFIDs
PY  - 2021
AB  - Yoga is popular in our daily lives for body fitness practice. We can practice yoga on a specific mat for body fitness. It’s important to have the smart yoga mat, which can adjust the surrounding environment based on user’s physical activities on the yoga mat. For example, we can adjust the ambient light, music and temperature based on the yoga practitioner’s pose on the yoga mat.In this paper, we present RFitness, a system that can detect the fitness posture on the yoga mat. To do so, we can attach multiple commodity passive RFID tags on the yoga mat, such that the different fitness postures can affect different RFID tags. Based on the signal strength readings from all the tags, we can estimate the yoga fitness posture using the model-driven deep neural networks. We implement the prototype of RFitness using commodity passive RFID tags and USRP reader. Our extensive experiments show that RFitness can achieve the median accuracy of 0.96.
SP  - 1
EP  - 8
JF  - 2021 IEEE International Conference on RFID (RFID)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/rfid52461.2021.9444325
ER  - 

TY  - JOUR
AU  - Hetzel, Lorenz; Dudley, John; Feit, Anna Maria; Kristensson, Per Ola
TI  - Complex Interaction as Emergent Behaviour: Simulating Mid-Air Virtual Keyboard Typing using Reinforcement Learning
PY  - 2021
AB  - Accurately modelling user behaviour has the potential to significantly improve the quality of human-computer interaction. Traditionally, these models are carefully hand-crafted to approximate specific aspects of well-documented user behaviour. This limits their availability in virtual and augmented reality where user behaviour is often not yet well understood. Recent efforts have demonstrated that reinforcement learning can approximate human behaviour during simple goal-oriented reaching tasks. We build on these efforts and demonstrate that reinforcement learning can also approximate user behaviour in a complex mid-air interaction task: typing on a virtual keyboard. We present the first reinforcement learning-based user model for mid-air and surface-aligned typing on a virtual keyboard. Our model is shown to replicate high-level human typing behaviour. We demonstrate that this approach may be used to augment or replace human testing during the validation and development of virtual keyboards.
SP  - 4140
EP  - 4149
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2021.3106494
ER  - 

TY  - JOUR
AU  - Fife, Dustin A.; Longo, Gabrielle; Correll, Michael; Tremoulet, Patrice D
TI  - A graph for every analysis: Mapping visuals onto common analyses using flexplot
PY  - 2021
AB  - For decades, statisticians and methodologists have insisted researchers utilize graphical analysis much more heavily. Despite cogent and passionate recommendations, there has been no graphical revolution. Instead, researchers rely heavily on misleading graphics that violate visual processing heuristics. Perhaps the main reason for the persistence of deceptive graphics is software; most software familiar to psychological researchers suffer from poor defaults and limited capabilities. Also, visualization is ancillary to statistical analysis, providing an incentive to not produce graphics at all. In this paper, we argue that every statistical analysis must have an accompanying graphic, and we introduce the point-and-click software Flexplot, available both in JASP and Jamovi. We then present the theoretical framework that guides Flexplot, as well as show how to perform the most common statistical analyses in psychological literature.
SP  - 1
EP  - 19
JF  - Behavior research methods
VL  - 53
IS  - 5
PB  - 
DO  - 10.3758/s13428-020-01520-2
ER  - 

TY  - NA
AU  - Ogura, Ayumu; Ito, Kodai; Itoh, Yuichi
TI  - Transtiff: A Stick Interface with Various Stiffness by Artificial Muscle Mechanism
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558777
ER  - 

TY  - NA
AU  - Liu, Yang; Althoff, Tim; Heer, Jeffrey
TI  - CHI - Paths Explored, Paths Omitted, Paths Obscured: Decision Points & Selective Reporting in End-to-End Data Analysis
PY  - 2020
AB  - Drawing reliable inferences from data involves many, sometimes arbitrary, decisions across phases of data collection, wrangling, and modeling. As different choices can lead to diverging conclusions, understanding how researchers make analytic decisions is important for supporting robust and replicable analysis. In this study, we pore over nine published research studies and conduct semi-structured interviews with their authors. We observe that researchers often base their decisions on methodological or theoretical concerns, but subject to constraints arising from the data, expertise, or perceived interpretability. We confirm that researchers may experiment with choices in search of desirable results, but also identify other reasons why researchers explore alternatives yet omit findings. In concert with our interviews, we also contribute visualizations for communicating decision processes throughout an analysis. Based on our results, we identify design opportunities for strengthening end-to-end analysis, for instance via tracking and meta-analysis of multiple decision paths.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376533
ER  - 

TY  - CHAP
AU  - Wang, Ke; Li, Yi-Hsuan; Hsu, Chun-Chen; Jiang, Jiabei; Liu, Yan; Zhao, Zirui; Yue, Wei; Yao, Lu
TI  - HCI (9) - A Research on Sensing Localization and Orientation of Objects in VR with Facial Vibrotactile Display
PY  - 2021
AB  - Tactile display technology has been widely proved to be effective to human-computer interaction. In multiple quantitative research methods to evaluate VR user experience (such as presence, immersion, and usability), multi-sensory factors are significant proportion. Therefore, the integration of VR-HMD and tactile display is a possible application and innovation trend of VR. The BIP (Break in Presence) phenomenon affects the user's spatial awareness when entering or leaving VR environments. We extracted orientation and localization tasks to discuss the influence of facial vibrotactile display on these tasks. Correlational researches are mainly focused on the parts of human body such as torso, limb, and head regions. We chose face region and to carry out the experiment, a VR-based wearable prototype “VibroMask” was built to implement facial vibrotactile perception. Firstly, the behavioral data of subjects' discrimination of vibrotactile information were tested to obtain the appropriate display paradigm. It was found that the discrimination accuracy of low-frequency vibration was higher with loose wearing status, and the delay offset of one-point vibration could better adapt to the orientation task. Secondly, the effect of facial vibrotactile display on objects’ localization and orientation discriminating task in VR was discussed. Finally, subjects’ feedback was collected by using open-ended questionnaire, it is found that users have a higher subjective evaluation of VR experience with facial vibrotactile display.
SP  - 223
EP  - 241
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77599-5_17
ER  - 

TY  - JOUR
AU  - Tone, Daiki; Iwai, Daisuke; Hiura, Shinsaku; Sato, Kosuke
TI  - FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping
PY  - 2020
AB  - This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range.
SP  - 2030
EP  - 2040
JF  - IEEE transactions on visualization and computer graphics
VL  - 26
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2020.2973444
ER  - 

TY  - BOOK
AU  - Auda, Jonas; Gruenefeld, Uwe; Mayer, Sven
TI  - XR@ISS - It Takes Two To Tango: Conflicts Between Users on the Reality-Virtuality Continuum and Their Bystanders.
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Abdullah, Muhammad; Sommerfeld, Romeo; Sievers, Bjarne; Geier, Leonard; Noack, Jonas; Ding, Marcus; Thieme, Christoph; Seidel, Laurenz; Fritzsche, Lukas; Langenhan, Erik; Adameck, Oliver; Dzingel, Moritz; Kern, Thomas; Taraz, Martin; Lempert, Conrad; Katakura, Shohei; Elhassany, Hany Mohsen; Roumen, Thijs; Baudisch, Patrick
TI  - HingeCore: Laser-Cut Foamcore for Fast Assembly
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545618
ER  - 

TY  - JOUR
AU  - Hirsch, Linda; Schneegass, Christina; Welsch, Robin; Butz, Andreas
TI  - To See or Not to See: Exploring Inattentional Blindness for the Design of Unobtrusive Interfaces in Shared Public Places
PY  - 2021
AB  - People visit public places with different intentions and motivations. While some explore it carefully, others may just want to pass or are otherwise engaged. We investigate how to exploit the inattentional blindness (IB) of indirect users in the design of public interfaces to apply to such diverse needs. Beginning with a structured literature study in the ACM Digital Library on IB, we analyzed 135 publications to derive design strategies that benefit from IB or avoid IB. Using these findings, we selected three existing interfaces for information presentation on a large public square and created two additional interfaces ourselves. We then compared users' perceptions through a self-reported photography study (N = 40). Participants followed one of four scripted profiles to imitate different user intentions, two for direct and two for indirect users. We hypothesized that direct users would recognize the interfaces, while indirect users would experience IB and ignore them. Our results show that direct users reported up to 68% of our interfaces, whereas indirect users noticed only 16%. Thus, IB can be exploited to hide interfaces from indirect users while keeping them noticeable to direct users.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 1
PB  - 
DO  - 10.1145/3448123
ER  - 

TY  - NA
AU  - Wang, Po-Yao; Xu, Cong-He; Wang, Ping-Yi; Huang, Hsin-Yu; Chang, Yu-Wei; Cheng, Jen-Hao; Lin, Yu-Hsin; Cheng, Lung-Pan
TI  - UIST - Game Illusionization: A Workflow for Applying Optical Illusions to Video Games
PY  - 2021
AB  - Optical illusions have been brought into recent video games to enhance gaming experiences. However, a large corpus of optical illusions remains unused, while few games incorporate illusions seamlessly. To mitigate the gap, we propose a workflow to guide game designers in applying optical illusions to their video games, i.e., in making more illusion games. In particular, our workflow consists of 5 stages: (1) choosing a game object, (2) searching for a matching illusion, (3) selecting an illusion mechanic, (4) integrating the selected illusion into the game, and (5) optionally revealing the illusion. To facilitate our workflow, we provide a tag database with 163 illusions that are labeled by their in-game visual elements and desired effects. We also provide example editing interfaces of 6 illusions for game designers. We walk through our workflow and showcase 6 resulting illusion games. We implemented these 6 games (with and without illusion) and conducted a 12-participant study to gain a preliminary understanding of how illusions enhance gaming experiences. To evaluate our workflow, we invited 6 game designers and 6 experienced players to follow our workflow and design their own illusion games, where 3 experienced game designers completed 2-week in-depth developments. We report their games, qualitative feedback and discuss reflection on our workflow, database and editing interfaces.
SP  - 1326
EP  - 1344
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474824
ER  - 

TY  - NA
AU  - Zenner, André; Kriegler, Hannah Maria; Krüger, Antonio
TI  - CHI Extended Abstracts - HaRT - The Virtual Reality Hand Redirection Toolkit
PY  - 2021
AB  - Past research has proposed various hand redirection techniques for virtual reality (VR). Such techniques modify a user’s hand movements and have been successfully used to enhance haptics and 3D user interfaces. Up to now, however, no unified framework exists that implements previously proposed techniques such as body warping, world warping, and hybrid methods. In this work, we present the Virtual Reality Hand Redirection Toolkit (HaRT), an open-source framework developed for the Unity engine. The toolkit aims to support both novice and expert VR researchers and practitioners in implementing and evaluating hand redirection techniques. It provides implementations of popular redirection algorithms and exposes a modular class hierarchy for easy integration of new approaches. Moreover, simulation, logging, and visualization features allow users of the toolkit to analyze hand redirection setups with minimal technical effort. We present the architecture of the toolkit along with the results of a qualitative expert study.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451814
ER  - 

TY  - NA
AU  - Muthukumarana, Sachith; Messerschmidt, Moritz Alexander; Matthies, Denys J.C.; Steimle, Jürgen; Scholl, Philipp M.; Nanayakkara, Suranga
TI  - CHI - ClothTiles: A Prototyping Platform to Fabricate Customized Actuators on Clothing using 3D Printing and Shape-Memory Alloys
PY  - 2021
AB  - Emerging research has demonstrated the viability of on-textile actuation mechanisms, however, an easily customizable and versatile on-cloth actuation mechanism is yet to be explored. In this paper, we present ClothTiles along with its rapid fabrication technique that enables actuation of clothes. ClothTiles leverage flexible 3D-printing and Shape-Memory Alloys (SMAs) alongside new parametric actuation designs. We validate the concept of fabric actuation using a base element, and then systematically explore methods of aggregating, scaling, and orienting prospects for extended actuation in garments. A user study demonstrated that our technique enables multiple actuation types applied across a variety of clothes. Users identified both aesthetic and functional applications of ClothTiles. We conclude with a number of insights for the Do-It-Yourself community on how to employ 3D-printing with SMAs to enable actuation on clothes.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445613
ER  - 

TY  - NA
AU  - Katanbaf, Mohamad; Saffari, Ali; Smith, Joshua R.
TI  - SenSys - MultiScatter: Multistatic Backscatter Networking for Battery-Free Sensors
PY  - 2021
AB  - Realizing the vision of ubiquitous battery-free sensing has proven to be challenging, mainly due to the practical energy and range limitations of current wireless communication systems. To address this, we design the first wide-area and scalable backscatter network with multiple receivers (RX) and transmitters (TX) base units to communicate with battery-free sensor nodes. Our system circumvents the inherent limitations of backscatter systems -including the limited coverage area, frequency-dependent operability, and sensor node limitations in handling network tasks- by introducing several coordination techniques between the base units starting from a single RX-TX pair to networks with many RX and TX units. We build low-cost RX and TX base units and battery-free sensor nodes with multiple sensing modalities and evaluate the performance of the MultiScatter system in various deployments. Our evaluation shows that we can successfully communicate with battery-free sensor nodes across 23400 ft2 of a two-floor educational complex using 5 RX and 20 TX units, costing $569. Also, we show that the aggregated throughput of the backscatter network increases linearly as the number of RX units and the network coverage grows.
SP  - 69
EP  - 83
JF  - Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3485730.3485939
ER  - 

TY  - NA
AU  - Oshim, Farhan Tasnim; Killingback, Julian; Follette, Dave; Peng, Huaishu; Rahman, Tauhidur
TI  - UIST - MechanoBeat: Monitoring Interactions with Everyday Objects using 3D Printed Harmonic Oscillators and Ultra-Wideband Radar
PY  - 2020
AB  - In this paper we present MechanoBeat, a 3D printed mechanical tag that oscillates at a unique frequency upon user interaction. With the help of an ultra-wideband (UWB) radar array, MechanoBeat can unobtrusively monitor interactions with both stationary and mobile objects. MechanoBeat consists of small, scalable, and easy-to-install tags that do not require any batteries, silicon chips, or electronic components. Tags can be produced using commodity desktop 3D printers with cheap materials. We develop an efficient signal processing and deep learning method to locate and identify tags using only the signals reflected from the tag vibrations. MechanoBeat is capable of detecting simultaneous interactions with high accuracy, even in noisy environments. We leverage UWB radar signals' high penetration property to sense interactions behind walls in a non-line-of-sight (NLOS) scenario. A number of applications using MechanoBeat have been explored and the results have been presented in the paper.
SP  - 430
EP  - 444
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415902
ER  - 

TY  - NA
AU  - Ahuja, Karan; Mayer, Sven; Goel, Mayank; Harrison, Chris
TI  - CHI - Pose-on-the-Go: Approximating User Pose with Smartphone Sensor Fusion and Inverse Kinematics
PY  - 2021
AB  - We present Pose-on-the-Go, a full-body pose estimation system that uses sensors already found in today’s smartphones. This stands in contrast to prior systems, which require worn or external sensors. We achieve this result via extensive sensor fusion, leveraging a phone’s front and rear cameras, the user-facing depth camera, touchscreen, and IMU. Even still, we are missing data about a user’s body (e.g., angle of the elbow joint), and so we use inverse kinematics to estimate and animate probable body poses. We provide a detailed evaluation of our system, benchmarking it against a professional-grade Vicon tracking system. We conclude with a series of demonstration applications that underscore the unique potential of our approach, which could be enabled on many modern smartphones with a simple software update.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445582
ER  - 

TY  - JOUR
AU  - Yu, Kevin; Zacharis, Kostantinos; Eck, Ulrich; Navab, Nassir
TI  - Projective Bisector Mirror (PBM): Concept and Rationale.
PY  - 2022
AB  - Our world is full of cameras, whether they are installed in the environment or integrated into mobile devices such as mobile phones or head-mounted displays. Displaying external camera views in our egocentric view with a picture-in-picture approach allows us to understand their view; however, it would not allow us to correlate their viewpoint with our perceived reality. We introduce Projective Bisector Mirrors for visualizing a camera view comprehensibly in the egocentric view of an observer with the metaphor of a virtual mirror. Our concept projects the image of a capturing camera onto the bisecting plane between the capture and the observer camera. We present extensive mathematical descriptions of this novel paradigm for multi-view visualization, discuss the effects of tracking errors and provide concrete implementation for multiple exemplary use-cases.
SP  - 3694
EP  - 3704
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203108
ER  - 

TY  - JOUR
AU  - Dube, Tafadzwa Joseph; Ren, Yuan; Limerick, Hannah; MacKenzie, I. Scott; Arif, Ahmed Sabbir
TI  - Push, Tap, Dwell, and Pinch: Evaluation of Four Mid-air Selection Methods Augmented with Ultrasonic Haptic Feedback
PY  - 2022
AB  - <jats:p>This work compares four mid-air target selection methods (Push, Tap, Dwell, Pinch) with two types of ultrasonic haptic feedback (Select, HoverSelect) in a Fitts’ law experiment. Results revealed that Tap is the fastest, the most accurate, and one of the least physically and cognitively demanding selection methods. Pinch is relatively fast but error prone and physically and cognitively demanding. Dwell is slowest by design, yet the most accurate and the least physically and cognitively demanding. Both haptic feedback methods improve selection performance by increasing users’ spatial awareness. Particularly, Push augmented with Hover &amp; Select feedback is comparable to Tap. Besides, participants perceive the selection methods as faster, more accurate, and more physically and cognitively comfortable with the haptic feedback methods.</jats:p>
SP  - 207
EP  - 225
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567718
ER  - 

TY  - JOUR
AU  - Ko, Yu-Jung; Zhao, Hang; Kim, Yoonsang; Ramakrishnan, I. V.; Zhai, Shumin; Bi, Xiaojun
TI  - UIST - Modeling Two Dimensional Touch Pointing
PY  - 2020
AB  - Modeling touch pointing is essential to touchscreen interface development and research, as pointing is one of the most basic and common touch actions users perform on touchscreen devices. Finger-Fitts Law [4] revised the conventional Fitts? law into a 1D (one-dimensional) pointing model for finger touch by explicitly accounting for the fat finger ambiguity (absolute error) problem which was unaccounted for in the original Fitts? law. We generalize Finger-Fitts law to 2D touch pointing by solving two critical problems. First, we extend two of the most successful 2D Fitts law forms to accommodate finger ambiguity. Second, we discovered that using nominal target width and height is a conceptually simple yet effective approach for defining amplitude and directional constraints for 2D touch pointing across different movement directions. The evaluation shows our derived 2D Finger-Fitts law models can be both principled and powerful. Specifically, they outperformed the existing 2D Fitts? laws, as measured by the regression coefficient and model selection information criteria (e.g., Akaike Information Criterion) considering the number of parameters. Finally, 2D Finger-Fitts laws also advance our understanding of touch pointing and thereby serve as the basis for touch interface designs.
SP  - 858
EP  - 868
JF  - Proceedings of the ACM Symposium on User Interface Software and Technology. ACM Symposium on User Interface Software and Technology
VL  - 2020
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415871
ER  - 

TY  - NA
AU  - Merino, Leonel; Schwarzl, Magdalena; Kraus, Matthias; Sedlmair, Michael; Schmalstieg, Dieter; Weiskopf, Daniel
TI  - ISMAR - Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)
PY  - 2020
AB  - We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.
SP  - 438
EP  - 451
JF  - 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar50242.2020.00069
ER  - 

TY  - NA
AU  - Zhang, Fanxing; Liu, Zhihao; Cheng, Zhanglin; Deussen, Oliver; Chen, Baoquan; Wang, Yunhai
TI  - VR - Mid-Air Finger Sketching for Tree Modeling
PY  - 2021
AB  - 2D sketch-based tree modeling cannot guarantee to generate plausible depth values and full 3D tree shapes. With the advent of virtual reality (VR) technologies, 3D sketching enables a new form for 3D tree modeling. However, it is labor-intensive and difficult to create realistically-looking 3D trees with complicated geometry and lots of detailed twigs with a reasonable amount of effort. In this paper, we explore the use of mid-air finger 3D sketching in VR for tree modeling. We present a hybrid approach that integrates freehand 3D sketches with an automatic population of branch geometries. The user only needs to draw a few 3D strokes in mid-air to define the envelope of the foliage (denoted as lobes) and main branches. Our algorithm then automatically generates a full 3D tree model based on these stroke inputs. Additionally, the shape of the 3D tree model can be modified by freely dragging, squeezing, or moving lobes in mid-air. We demonstrate the ease-of-use, efficiency, and flexibility in tree modeling and overall shape control. We perform user studies and show a variety of realistic tree models generated instantaneously from 3D finger sketching.
SP  - 826
EP  - 834
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00110
ER  - 

TY  - NA
AU  - Li, Yang; Kumar, Ranjitha; Lasecki, Walter S.; Hilliges, Otmar
TI  - CHI Extended Abstracts - Artificial Intelligence for HCI: A Modern Approach
PY  - 2020
AB  - Artificial intelligence (AI) and Human Computer Interaction (HCI) share common roots and early work on conversational agents has laid the foundation for both fields. However, in subsequent decades the initial tight connection between the fields has become less pronounced. The recent rise of deep learning has revolutionized AI and has led to a raft of practical methods and tools that significantly impact areas outside of core-AI. In particular, modern AI techniques now power new ways for machines and humans to interact. Thus it is timely to investigate how modern AI can propel HCI research in new ways and how HCI research can help direct AI developments. This workshop offers a forum for researchers to discuss new opportunities that lie in bringing modern AI methods into HCI research, identifying important problems to investigate, showcasing computational and scientific methods that can be applied, and sharing datasets and tools that are already available or proposing those that should be further developed. The topics we are interested in including deep learning methods for understanding and modeling human behaviors and enabling new interaction modalities, hybrid intelligence that combine human and machine intelligence to solve difficult tasks, and tools and methods for interaction data curation and large-scale data-driven design. At the core of these topics, we want to start the conversation on how data-driven and data-centric approaches of modern AI can impact HCI.
SP  - 3375147
EP  - NA
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375147
ER  - 

TY  - JOUR
AU  - Pfeil, Kevin; Chatlani, Neeraj; LaViola, Joseph J.; Wisniewski, Pamela
TI  - Bridging the Socio-Technical Gaps in Body-worn Interpersonal Live-Streaming Telepresence through a Critical Review of the Literature
PY  - 2021
AB  - It is important to learn from the past as we endeavor into this uncharted territory of mobile, human-to-human, one-to-one telepresence for interpersonal use. With the ever-increasing access to live-streaming cameras, we are now at the cusp of being able to create novel, immersive, and interpersonal telepresence activities that have the potential to change how humans interact with one another on a daily basis. Due to its novelty, there are likely socio-technical gaps between the needs of users and the technical specifications of the prototypes that are currently being designed to support the complex social interactions of human-to-human telepresence. Therefore, in this paper, we use a socio-technical lens to conduct a systematic literature review of 52 peer-reviewed articles of early work in this space. Overall, we found that while progress has been made to address the social needs of those involved in one-to-one telepresence scenarios, there are discontinuities within the existing literature that need to be addressed, particularly with the way we attempt to measure and quantify human-centered outcomes with unvalidated instruments. We also found that the social needs of on-site users have been neglected, as in many articles the user was merely treated as a surrogate, or reported feeling socially awkward or unsafe, due to the conspicuous nature of the body-worn technology in public environments. These findings are prevalent, even as researchers consider adding to this body-worn burden in an attempt to improve the receiving users' sense of immersion and presence. To preserve the beneficial nature of telepresence interaction while ensuring that all users' needs are met, researchers should endeavor to further understand the dynamics of the relationship between all parties in the remote environment. Our paper creates a future research agenda that emphasizes the importance of ensuring that all parties involved feel comfortable in their role during interpersonal telepresence interactions.
SP  - 1
EP  - 39
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 5
IS  - CSCW1
PB  - 
DO  - 10.1145/3449194
ER  - 

TY  - NA
AU  - Chen, Xiaochen; He, Han; Mehmood, Adnan; Raumonen, Pasi; Leino, Mirka; Merilampi, Sari; Virkki, Johanna
TI  - ClothFace: Battery-Free On-body Interface Platform for Future Human-Machine Interaction
PY  - 2022
AB  - Smooth communication between people and machines plays an important role in the intelligent environments of the future, where the best sides of both people and machines are to be exploited in the name of efficiency and flexibility. This requires a functional human-machine interface that allows the necessary control actions but does not require any use of robot-specific devices. In this paper, we further introduce a passive ultra-high frequency (UHF) radio frequency identification (RFID)-based platform, which is an attractive solution for on-body interfaces, as it is passive and maintenance-free, does not require a line-of-sight to function, and has reading distances of several meters. The unique aspect of this study is the established fully textile-based battery-free touchpad for writing digits, which can be integrated into everyday clothing and worn on body. Further, the paper presents various use cases for the developed technology.
SP  - NA
EP  - NA
JF  - 2022 16th European Conference on Antennas and Propagation (EuCAP)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/eucap53622.2022.9769251
ER  - 

TY  - JOUR
AU  - Kwon, Yong Ho; Fernandes, Jayer; Kim, Jae-Jun; Chen, Jiangang; Jiang, Hongrui
TI  - Micro-Actuated Tunable Hierarchical Silver Nanostructures to Measure Tensile Force for Biomedical Wearable Sensing Applications.
PY  - 2021
AB  - Commercially available biomedical wearable sensors to measure tensile force/strain still struggle with miniaturization in terms of weight, size, and conformability. Flexible and epidermal electronic devices have been utilized in these applications to overcome these issues. However, current sensors still require a power supply and some form of powered data transfer, which present challenges to miniaturization and to applications. Here, we report on the development of flexible, passive (thus zero power consumption), and biocompatible nanostructured photonic devices that can measure tensile strain in real time by providing an optical readout instead of an electronic readout. Hierarchical silver (Ag) nanostructures in various thicknesses of 20–60 nm were fabricated and embedded on a stretchable substrate using e-beam lithography and a low-temperature dewetting process. The hierarchical Ag nanostructures offer more design flexibility through a two-level design approach. A tensional force applied in one lateral (x- or y-) direction of the stretchable substrate causes a Poisson contraction in the other, and as a result, a shift in the reflected light of the nanostructures. A clear blue shift of more than 100 nm in peak reflectance in the visible spectrum was observed in the reflected color, making the devices applicable in a variety of biomedical photonic sensing applications.
SP  - 476
EP  - NA
JF  - Micromachines
VL  - 12
IS  - 5
PB  - 
DO  - 10.3390/mi12050476
ER  - 

TY  - NA
AU  - Wang, Zeyu; Nguyen, Cuong; Asente, Paul; Dorsey, Julie
TI  - CHI - DistanciAR: Authoring Site-Specific Augmented Reality Experiences for Remote Environments
PY  - 2021
AB  - Most augmented reality (AR) authoring tools only support the author’s current environment, but designers often need to create site-specific experiences for a different environment. We propose DistanciAR, a novel tablet-based workflow for remote AR authoring. Our baseline solution involves three steps. A remote environment is captured by a camera with LiDAR; then, the author creates an AR experience from a different location using AR interactions; finally, a remote viewer consumes the AR content on site. A formative study revealed understanding and navigating the remote space as key challenges with this solution. We improved the authoring interface by adding two novel modes: Dollhouse, which renders a bird’s-eye view, and Peek, which creates photorealistic composite images using captured images. A second study compared this improved system with the baseline, and participants reported that the new modes made it easier to understand and navigate the remote scene.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445552
ER  - 

TY  - NA
AU  - Ahuja, Karan; Goel, Mayank; Harrison, Chris
TI  - SUI - BodySLAM: Opportunistic User Digitization in Multi-User AR/VR Experiences
PY  - 2020
AB  - Today’s augmented and virtual reality (AR/VR) systems do not provide body, hand or mouth tracking without special worn sensors or external infrastructure. Simultaneously, AR/VR systems are increasingly being used in co-located, multi-user experiences, opening the possibility for opportunistic capture of other users. This is the core idea behind BodySLAM, which uses disparate camera views from users to digitize the body, hands and mouth of other people, and then relay that information back to the respective users. If a user is seen by two or more people, 3D pose can be estimated via stereo reconstruction. Our system also maps the arrangement of users in real world coordinates. Our approach requires no additional hardware or sensors beyond what is already found in commercial AR/VR devices, such as Microsoft HoloLens or Oculus Quest.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418452
ER  - 

TY  - CHAP
AU  - Bruni, Luis Emilio; Dini, Hossein; Simonetti, Aline
TI  - HCI (9) - Narrative Cognition in Mixed Reality Systems: Towards an Empirical Framework
PY  - 2021
AB  - In this paper, we propose an interdisciplinary theoretical and empirical framework to investigate the particular faculties related to human “narrative cognition”, in general, and in relation to MRT in particular. In order to contextualize our approach, we shortly review the cognitive turn in narratology, as well as state of the art in different domains that have undertaken psychophysiological studies that either characterize aspects that are relevant to narrative cognition, or which investigate mixed reality experiences. The idea is to bring together knowledge and insights from narratology, different branches of semiotics and cognitive sciences, with empirical strategies that bridge the gap between first-person phenomenological approaches and psychophysiological and behavioural methods. We propose a rationale in order to combine tools and techniques from MRT/VR/AR, interactive digital narratives and storytelling, with a suite of integrated psychophysiological methods (such as EEG, HR, GSR and eye tracking) and phenomenological-subjective approaches.
SP  - 3
EP  - 17
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77599-5_1
ER  - 

TY  - NA
AU  - Kumar, Chandan; Hedeshy, Ramin; MacKenzie, I. Scott; Staab, Steffen
TI  - CHI - TAGSwipe: Touch Assisted Gaze Swipe for Text Entry
PY  - 2020
AB  - The conventional dwell-based methods for text entry by gaze are typically slow and uncomfortable. A swipe-based method that maps gaze path into words offers an alternative. However, it requires the user to explicitly indicate the beginning and ending of a word, which is typically achieved by tedious gaze-only selection. This paper introduces TAGSwipe, a bi-modal method that combines the simplicity of touch with the speed of gaze for swiping through a word. The result is an efficient and comfortable dwell-free text entry method. In the lab study TAGSwipe achieved an average text entry rate of 15.46 wpm and significantly outperformed conventional swipe-based and dwell-based methods in efficacy and user satisfaction.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376317
ER  - 

TY  - NA
AU  - Roumen, Thijs
TI  - UIST (Adjunct Volume) - Portable Laser Cutting
PY  - 2020
AB  - Laser-cut 3D models found in repositories tend to be basic and trivial-models build over long periods of time and by multiple designers are few/non existent. I argue that this is caused by a lack of an exchange format that would allow continuing the work. At first glance, it may seem like such a format already exist, as laser cut models are already widely shared in the form of 2D cutting plans. However, such files are susceptible to variations in cutter properties (aka kerf) and do not allow modifying the model in any meaningful way (no adjustment of material thickness, no parametric changes, etc.). My first take on the challenge is to see how far we can get by still building on the de-facto standard, i.e., 2D cutting plans. springFit [7] and kerf-canceling mechanisms [6] tackle the challenge by rewriting 2D cutting plans, replacing non-portable elements with portable ones (shown in Figure 1). However, this comes at a cost of extra incisions, reducing the structural integrity of models and impacting aesthetic qualities and rare mechanisms or joints may go undetected. I thus take a more radical approach, which is to move on to a 3D exchange format (kyub [1]). This eliminates these challenges as it guarantees portability by gener-ating a new machine-specific 2D file for the local machine. Instead, it raises the question of compatibility: Files already exist in 2D-how to get them into 3D? I demonstrate a software tool that automatically reconstructs the 3D geometry of the model encoded in a 2D cutting plan, allows modifying it using a 3D editor, and re-encodes it to a 2D cutting plan. I demonstrate how this approach allows me to make a much wider range of modifications, including scaling, changing material thickness, and even remixing mod-els. The transition from sharing machine-oriented 2D cutting files, to 3D files, enables users worldwide to collaborate, share, and reuse. And thus, to move on from users starting thousands of trivial models from scratch to collaborating on big complex projects.
SP  - 157
EP  - 161
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3415802
ER  - 

TY  - JOUR
AU  - Xiao, Chufeng; Yu, Deng; Han, Xiaoguang; Zheng, Youyi; Fu, Hongbo
TI  - SketchHairSalon
PY  - 2021
AB  - <jats:p> Recent deep generative models allow real-time generation of hair images from sketch inputs. Existing solutions often require a user-provided binary mask to specify a target hair shape. This not only costs users extra labor but also fails to capture complicated hair boundaries. Those solutions usually encode hair structures via orientation maps, which, however, are not very effective to encode complex structures. We observe that colored hair sketches already implicitly define target hair shapes as well as hair appearance and are more flexible to depict hair structures than orientation maps. Based on these observations, we present <jats:italic>SketchHairSalon</jats:italic> , a two-stage framework for generating realistic hair images directly from freehand sketches depicting desired hair structure and appearance. At the first stage, we train a network to predict a hair matte from an input hair sketch, with an optional set of non-hair strokes. At the second stage, another network is trained to synthesize the structure and appearance of hair images from the input sketch and the generated matte. To make the networks in the two stages aware of long-term dependency of strokes, we apply self-attention modules to them. To train these networks, we present a new dataset containing thousands of annotated hair sketch-image pairs and corresponding hair mattes. Two efficient methods for sketch completion are proposed to automatically complete repetitive braided parts and hair strokes, respectively, thus reducing the workload of users. Based on the trained networks and the two sketch completion strategies, we build an intuitive interface to allow even novice users to design visually pleasing hair images exhibiting various hair structures and appearance via freehand sketches. The qualitative and quantitative evaluations show the advantages of the proposed system over the existing or alternative solutions. </jats:p>
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 6
PB  - 
DO  - 10.1145/3478513.3480502
ER  - 

TY  - NA
AU  - Gupta, Agrim; Girerd, Cedric; Dunna, Manideep; Zhang, Qiming; Subbaraman, Raghav; Morimoto, Tania K.; Bharadia, Dinesh
TI  - WiForce: Wireless Sensing and Localization of Contact Forces on a Space Continuum
PY  - 2020
AB  - Contact force is a natural way for humans to interact with the physical world around us. However, most of our interactions with the digital world are largely based on a simple binary sense of touch (contact or no contact). Similarly, when interacting with robots to perform complex tasks, such as surgery, richer force information that includes both magnitude and contact location is important for task performance. To address these challenges, we present the design and fabrication of WiForce which is a 'wireless' sensor, sentient to contact force magnitude and location. WiForce achieves this by transducing force magnitude and location, to phase changes of an incident RF signal of a backscattering tag. The phase changes are thus modulated into the backscattered RF signal, which enables measurement of force magnitude and contact location by inferring the phases of the reflected RF signal. WiForce's sensor is designed to support wide-band frequencies all the way up to 3 GHz. We evaluate the force sensing wirelessly in different environments, including through phantom tissue, and achieve force accuracy of 0.3 N and contact location accuracy of 0.6 mm.
SP  - NA
EP  - NA
JF  - arXiv: Signal Processing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Zenner, André; Heqitz, Kora Persephone; Krüger, Antonio
TI  - VR - Blink-Suppressed Hand Redirection
PY  - 2021
AB  - Many interaction techniques in virtual reality break with the 1-to-1 mapping from real to virtual space. Instead, specialized techniques for 3D interaction and haptic retargeting leverage hand redirection, offsetting the virtual hand rendering from the real hand position. To achieve unnoticeable hand redirection, however, the utilization of change blindness phenomena has not been systematically explored. Inspired by recent advances in the domain of redirected walking, we present the first hand redirection technique that makes use of blink-induced visual suppression and corresponding change blindness. We introduce Blink-Suppressed Hand Redirection (BSHR) to study the feasibility and detectability of hand redirection based on blink suppression. Our technique is based on Cheng et al.'s (2017) [9] body warping algorithm and instantaneously shifts the virtual hand when the user's vision is suppressed during a blink. Additionally, it can be configured to continuously increment hand offsets when the user's eyes are opened, limited to an extent below detection thresholds. In a psychophysical experiment, we verify that unnoticeable blink-suppressed hand redirection is possible even in worst -case scenarios, and derive the corresponding conservative detection thresholds (CDTs). Moreover, our results show that the range of unnoticeable redirection can be increased by combining continuous warping and blink-suppressed instantaneous shifts. As an additional contribution, we derive the CDTs for Cheng et al.'s (2017) [9] redirection technique that does not leverage blinks.
SP  - 75
EP  - 84
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00028
ER  - 

TY  - NA
AU  - Huang, Kevin; Li, Jiannan; Sousa, Mauricio; Grossman, Tovi
TI  - immersivePOV: Filming How-To Videos with a Head-Mounted 360° Action Camera
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517468
ER  - 

TY  - NA
AU  - Savage, Valkyrie; Tejada, Carlos; Zhong, Mengyu; Ramakers, Raf; Ashbrook, Daniel; Kim, Hyunyoung
TI  - AirLogic: Embedding Pneumatic Computation and I/O in 3D Models to Fabricate Electronics-Free Interactive Objects
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545642
ER  - 

TY  - JOUR
AU  - Zhang, Tengxiang; Lan, Zitong; Xu, Chenren; Li, Yanrong; Chen, Yiqiang
TI  - BLEselect
PY  - 2022
AB  - <jats:p>Spontaneous selection of IoT devices from the head-mounted device is key for user-centered pervasive interaction. BLEselect enables users to select an unmodified Bluetooth 5.1 compatible IoT device by nodding at, pointing at, or drawing a circle in the air around it. We designed a compact antenna array that fits on a pair of smart glasses to estimate the Angle of Arrival (AoA) of IoT and wrist-worn devices' advertising signals. We then developed a sensing pipeline that supports all three selection gestures with lightweight machine learning models, which are trained in real-time for both hand gestures. Extensive characterizations and evaluations show that our system is accurate, natural, low-power, and privacy-preserving. Despite the small effective size of the antenna array, our system achieves a higher than 90% selection accuracy within a 3 meters distance in front of the user. In a user study that mimics real-life usage cases, the overall selection accuracy is 96.7% for a diverse set of 22 participants in terms of age, technology savviness, and body structures.</jats:p>
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569482
ER  - 

TY  - NA
AU  - Bell, Fiona; Alistar, Mirela
TI  - Designing with Alganyl: A Hands-on Exploration of Biodegradable Plastics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3503669
ER  - 

TY  - NA
AU  - Shim, Youngbo; Kim, Taejun; Lee, Sangyoon; Kim, Sunbum; Lee, Geehyuk
TI  - QuadStretch: A Forearm-wearable Skin Stretch Display for Immersive VR Experience
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2022 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3550471.3564761
ER  - 

TY  - NA
AU  - Cools, Robbe; Esteves, Augusto; Simeone, Adalberto L.
TI  - Blending Spaces: Cross-Reality Interaction Techniques for Object Transitions Between Distinct Virtual and Augmented Realities
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00069
ER  - 

TY  - NA
AU  - Aso, Kohei; Hwang, Dong-Hyun; Koike, Hideki
TI  - AHs - Portable 3D Human Pose Estimation for Human-Human Interaction using a Chest-Mounted Fisheye Camera
PY  - 2021
AB  - We propose a system that estimates the 3D body pose of other parties using a single RGB chest-mounted ultra-wide fisheye camera. Although the fisheye camera can capture a wide field of view, it is difficult to apply image processing for perspective images because of its strong distortion. In our method, the input fisheye image is converted to an equirectangular image to detect another person and their 2D keypoints, and then convert them to a 3D pose. In order to adapt to the distortion of equirectangular images, we generate a synthetic dataset and fine-tune the model. We also estimate the location of the other person so that we can reconstruct the absolute camera-centered global pose. We evaluate the accuracy on real-world data and show that the fine-tuned model performs best.
SP  - 116
EP  - 120
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458986
ER  - 

TY  - NA
AU  - Peng, Mengqi; Wei, Li-Yi; Kazi, Rubaiat Habib; Kim, Vladimir G.
TI  - UIST - Autocomplete Animated Sculpting
PY  - 2020
AB  - Keyframe-based sculpting provides unprecedented freedom to author animated organic models, which can be difficult to create with other methods such as simulation, scripting, and rigging. However, sculpting animated objects can require significant artistic skill and manual labor, even more so than sculpting static 3D shapes or drawing 2D animations, which are already quite challenging. We present a keyframe-based animated sculpting system with the capability to autocomplete user editing under a simple and intuitive brushing interface. Similar to current desktop sculpting and VR brushing tools, users can brush surface details and volume structures. Meanwhile, our system analyzes their workflows and predicts what they might do in the future, both spatially and temporally. Users can accept or ignore these suggestions and thus maintain full control. We propose the first interactive suggestive keyframe sculpting system, specifically for spatio-temporal repetitive tasks, including low-level spatial details and high-level brushing structures across multiple frames. Our key ideas include a deformation-based optimization framework to analyze recorded workflows and synthesize predictions, and a semi-causal global similarity measurement to support flexible brushing stroke sequences and complex shape changes. Our system supports a variety of shape and motion styles, including those difficult to achieve via existing animation systems, such as topological changes that cannot be accomplished via simple rig-based deformations and stylized physically-implausible motions that cannot be simulated. We evaluate our system via a pilot user study that demonstrates the effectiveness of our system.
SP  - 760
EP  - 777
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415884
ER  - 

TY  - NA
AU  - Zhang, Lei; Oney, Steve
TI  - UIST - FlowMatic: An Immersive Authoring Tool for Creating Interactive Scenes in Virtual Reality
PY  - 2020
AB  - Immersive authoring is a paradigm that makes Virtual Reality (VR) application development easier by allowing programmers to create VR content while immersed in the virtual environment. In this paradigm, programmers manipulate programming primitives through direct manipulation and get immediate feedback on their program's state and output. However, existing immersive authoring tools have a low ceiling; their programming primitives are intuitive but can only express a limited set of static relationships between elements in a scene. In this paper, we introduce FlowMatic, an immersive authoring tool that raises the ceiling of expressiveness by allowing programmers to specify reactive behaviors---behaviors that react to discrete events such as user actions, system timers, or collisions. FlowMatic also introduces primitives for programmatically creating and destroying new objects, for abstracting and re-using functionality, and for importing 3D models. Importantly, FlowMatic uses novel visual representations to allow these primitives to be represented directly in VR. We also describe the results of a user study that illustrates the usability advantages of FlowMatic relative to a 2D authoring tool and we demonstrate its expressiveness through several example applications that would be impossible to implement with existing immersive authoring tools. By combining a visual program representation with expressive programming primitives and a natural User Interface (UI) for authoring programs, FlowMatic shows how programmers can build fully interactive virtual experiences with immersive authoring.
SP  - 342
EP  - 353
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415824
ER  - 

TY  - NA
AU  - Yang, Xiaoying; Zhang, Yang
TI  - CHI Extended Abstracts - CubeSense: Wireless, Battery-Free Interactivity through Low-Cost Corner Reflector Mechanisms
PY  - 2021
AB  - Ubiquitous computing systems rely on ubiquitous methods to sense user interactions, which have manifested in our daily environments as physical buttons, switches, sliders, and beyond. These conventional controllers are either wired, which eliminates flexible deployments, or powered by batteries that require user maintenance. Additionally, built-in wireless communications such as Wi-Fi, Bluetooth, and RFID add up to the total cost. All aforementioned constraints prevent truly ubiquitous interactions from intelligent environments such as smart homes, industry 4.0, precision farming, and a wider range of Internet-of-Things (IoT) applications. We present CubeSense, a wireless and battery-free interactive sensing system which encodes user interactions into radar cross section (RCS) of corner reflectors. Through careful designs of corner reflector mechanisms, CubeSense achieves robust accuracies with controllers made of ultra-low-cost plastics and metal films, resulting in a total cost of around only 20 cents per unit.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451599
ER  - 

TY  - JOUR
AU  - Kato, Kunihiro; Ikematsu, Kaori; Kawahara, Yoshihiro
TI  - CAPath: 3D-Printed Interfaces with Conductive Points in Grid Layout to Extend Capacitive Touch Inputs
PY  - 2020
AB  - We propose a 3D-printed interface, CAPath, in which conductive contact points are in a grid layout. This structure allows not only specific inputs (e.g., scrolling or pinching) but also general 2D inputs and gestures that fully leverage the "touch surface." We provide the requirements to fabricate the interface and implement a designing system to generate 3D objects in the conductive grid structure. The CAPath interface can be utilized in the uniquely shaped interfaces and opens up further application fields that cannot currently be accessed with existing passive touch extensions. Our contributions also include an evaluation for the recognition accuracy of the touch operations with the implemented interfaces. The results show that our technique is promising to fabricate customizable touch-sensitive interactive objects.
SP  - 193
EP  - 17
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427321
ER  - 

TY  - NA
AU  - Aktar, Farzana; Maurer, Frank
TI  - Elicitation of Interaction Techniques with 3D Data Visualizations in Immersive Environment using HMDs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00053
ER  - 

TY  - NA
AU  - Yan, Yukang; Yu, Chun; Zheng, Wengrui; Tang, Ruining; Xu, Xuhai; Shi, Yuanchun
TI  - CHI - FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions
PY  - 2020
AB  - In the conversations with smart speakers, misunderstandings of users' requests lead to erroneous responses. We propose FrownOnError, a novel interaction technique that enables users to interrupt the responses by intentional but natural facial expressions. This method leverages the human nature that the facial expression changes when we receive unexpected responses. We conducted a first user study (N=12) to understand users' intuitive reactions to the correct and incorrect responses. Our results reveal the significant difference in the frequency of occurrence and intensity of users' facial expressions between two conditions, and frowning and raising eyebrows are intuitive to perform and easy to control. Our second user study (N=16) evaluated the user experience and interruption efficiency of FrownOnError and the third user study (N=12) explored suitable conversation recovery strategies after the interruptions. Our results show that FrownOnError can be accurately detected (precision: 97.4%, recall: 97.6%), provides the most timely interruption compared to the baseline methods of wake-up word and button press, and is rated as most intuitive and easiest to be performed by users.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376810
ER  - 

TY  - NA
AU  - Twigg-Smith, Hannah; Peek, Nadya
TI  - Demonstrating Dynamic Toolchains for Machine Control
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3565598
ER  - 

TY  - NA
AU  - Do, Seungwon; Chang, Minsuk; Lee, Byungjoo
TI  - CHI - A Simulation Model of Intermittently Controlled Point-and-Click Behaviour
PY  - 2021
AB  - We present a novel simulation model of point-and-click behaviour that is applicable both when a target is stationary or moving. To enable more realistic simulation than existing models, the model proposed in this study takes into account key features of the user and the external environment, such as intermittent motor control, click decision-making, visual perception, upper limb kinematics and the effect of input device. The simulated user’s point-and-click behaviour is formulated as a Markov decision process (MDP), and the user’s policy of action is optimised through deep reinforcement learning. As a result, our model successfully and accurately reproduced the trial completion time, distribution of click endpoints, and cursor trajectories of real users. Through an ablation study, we showed how the simulation results change when the model’s sub-modules are individually removed. The implemented model and dataset are publicly available.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445514
ER  - 

TY  - NA
AU  - Deja, Jordan Aiko
TI  - UMAP - Adaptive Visualisations Using Spatiotemporal and Heuristic Models to Support Piano Learning
PY  - 2021
AB  - Learning the piano is hard and many approaches including piano-roll visualisations have been explored in order to support novices and seasoned learners in this process. However, existing piano roll prototypes have not considered the spatiotemporal component (user’s ability to press on a moving target) when generating these visualisations and user modelling. In this PhD, we are going to look into two different approaches: (i) exploring whether existing techniques in single-target spatiotemporal modelling can be adapted to a multi-target scenario such as when learners use several fingers to press multiple moving targets when playing the piano, and (ii) exploring heuristics defined by experts marking various difficult parts of songs, and deciding on specific interventions needed for these marked parts. Using models and input from the experts we will design and build an adaptive piano roll training system. We will evaluate and compare these models in various user studies involving users trying to play piano pieces and develop their improvisation skills. We intend to uncover whether these adaptive visualisations will be helpful in the overall training of piano learners. Additionally, these models and adaptive visualisations will allow us to discover affordances that can potentially improve piano learning in general.
SP  - 286
EP  - 290
JF  - Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3450613.3459656
ER  - 

TY  - JOUR
AU  - Alharbi, Ohoud; Stuerzlinger, Wolfgang; Putze, Felix
TI  - The Effects of Predictive Features of Mobile Keyboards on Text Entry Speed and Errors
PY  - 2020
AB  - Mobile users rely on typing assistant mechanisms such as prediction and autocorrect. Previous studies on mobile keyboards showed decreased performance for heavy use of word prediction, which identifies a need for more research to better understand the effectiveness of predictive features for different users. Our work aims at such a better understanding of user interaction with autocorrections and the prediction panel while entering text, in particular when these approaches fail. We present a crowd-sourced mobile text entry study with 170 participants. Our mobile web application simulates autocorrection and word prediction to capture user behaviours around these features. We found that using word prediction saves an average of 3.43 characters per phrase but also adds an average of two seconds compared to actually typing the word, resulting in a negative effect on text entry speed. We also identified that the time to fix wrong autocorrections is on average 5.5 seconds but that autocorrection does not have a significant effect on typing speed.
SP  - 1
EP  - 16
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427311
ER  - 

TY  - NA
AU  - Lehmann, Florian; Markert, Niklas; Dang, Hai; Buschek, Daniel
TI  - Suggestion Lists vs. Continuous Generation: Interaction Design for Writing with Generative Models on Mobile Devices Affect Text Length, Wording and Perceived Authorship
PY  - 2022
AB  - Neural language models have the potential to support human writing. However, questions remain on their integration and influence on writing and output. To address this, we designed and compared two user interfaces for writing with AI on mobile devices, which manipulate levels of initiative and control: 1) Writing with continuously generated text, the AI adds text word-by-word and user steers. 2) Writing with suggestions, the AI suggests phrases and user selects from a list. In a supervised online study (N=18), participants used these prototypes and a baseline without AI. We collected touch interactions, ratings on inspiration and authorship, and interview data. With AI suggestions, people wrote less actively, yet felt they were the author. Continuously generated text reduced this perceived authorship, yet increased editing behavior. In both designs, AI increased text length and was perceived to influence wording. Our findings add new empirical evidence on the impact of UI design decisions on user experience and output with co-creative systems.
SP  - NA
EP  - NA
JF  - Mensch und Computer 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3543758.3543947
ER  - 

TY  - JOUR
AU  - Wang, Cheng Yao; Zhou, Qian; Fitzmaurice, George; Anderson, Fraser
TI  - VideoPoseVR: Authoring Virtual Reality Character Animations with Online Videos
PY  - 2022
AB  - <jats:p>We present VideoPoseVR, a video-based animation authoring workflow using online videos to author character animations in VR. It leverages the state-of-the-art deep learning approach to reconstruct 3D motions from online videos, caption the motions, and store them in a motion dataset. Creators can import the videos, search in the dataset, modify the motion timeline, and combine multiple motions from videos to author character animations in VR. We implemented a proof-of-concept prototype and conducted a user study to evaluate the feasibility of the video-based authoring approach as well as gather initial feedback of the prototype. The study results suggest that VideoPoseVR was easy to learn for novice users to author animations and enable rapid exploration of prototyping for applications such as entertainment, skills training, and crowd simulations.</jats:p>
SP  - 448
EP  - 467
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567728
ER  - 

TY  - NA
AU  - Lambrichts, Mannu; Tijerina, Jose Maria; Ramakers, Raf
TI  - Tangible and Embedded Interaction - SoftMod: A Soft Modular Plug-and-Play Kit for Prototyping Electronic Systems
PY  - 2020
AB  - We present SoftMod, a novel modular electronics kit consisting of soft and flexible modules that snap together. Unlike existing modular kits, SoftMod tracks the topology of interconnected modules and supports basic plug-and-play behavior as well as advanced user-specified behavior. As such, the shape of a SoftMod assembly does not depend on the desired behavior and various 2D and 3D electronic systems can be realized. While the plug-and-play nature of our modules stimulates play, the advanced features for specifying behavior and for making a variety of soft and flexible shapes, offer a high-ceiling when experimenting with novel types of interfaces, such as wearables, and interactive skin and textiles.
SP  - 287
EP  - 298
JF  - Proceedings of the Fourteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3374920.3374950
ER  - 

TY  - NA
AU  - Nazarova, Elena; Sautenkov, Oleg; Cabrera, Miguel Altamirano; Tirado, Jonathan; Serpiva, Valerii; Rakhmatulin, Viktor; Tsetserukou, Dzmitry
TI  - CobotAR: Interaction with Robots using Omnidirectionally Projected Image and DNN-based Gesture Recognition.
PY  - 2021
AB  - Several technological solutions supported the creation of interfaces for Augmented Reality (AR) multi-user collaboration in the last years. However, these technologies require the use of wearable devices. We present CobotAR - a new AR technology to achieve the Human-Robot Interaction (HRI) by gesture recognition based on Deep Neural Network (DNN) - without an extra wearable device for the user. The system allows users to have a more intuitive experience with robotic applications using just their hands. The CobotAR system assumes the AR spatial display created by a mobile projector mounted on a 6 DoF robot. The proposed technology suggests a novel way of interaction with machines to achieve safe, intuitive, and immersive control mediated by a robotic projection system and DNN-based algorithm. We conducted the experiment with several parameters assessment during this research, which allows the users to define the positives and negatives of the new approach. The mental demand of CobotAR system is twice less than Wireless Gamepad and by 16\% less than Teach Pendant.
SP  - NA
EP  - NA
JF  - arXiv: Robotics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Mahmood, Riyadh; Joshi, Arth; Lele, Adwait; Pennington, Jay
TI  - HAI-GEN+user2agent@IUI - Dynamic Natural Language User Interfaces Using Microservices.
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Saito, Tsubasa; Ijiri, Takashi
TI  - UIST (Adjunct Volume) - Animating Various Characters using Arm Gestures in Virtual Reality Environment
PY  - 2021
AB  - In this study, we propose a method for efficiently animating various characters. The main concept is to build an animation-authoring system in a virtual reality (VR) environment and allow users to move anchors of a character model with their arms. With our system, users select two anchors, which are associated with two VR controllers. The users then directly specify the three-dimensional (3D) motions of the anchors by moving the controllers. To animate various characters with multiple anchors, users can repeat this specification process multiple times. To demonstrate the feasibility of our method, we show animations designed with our system, such as a walking fox, a walking spider, and a flapping hawk.
SP  - 29
EP  - 31
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480220
ER  - 

TY  - CONF
AU  - Buschek, Daniel; Zürn, Martin; Eiband, Malin
TI  - CHI - The Impact of Multiple Parallel Phrase Suggestions on Email Input and Composition Behaviour of Native and Non-Native English Writers
PY  - 2021
AB  - We present an in-depth analysis of the impact of multi-word suggestion choices from a neural language model on user behaviour regarding input and text composition in email writing. Our study for the first time compares different numbers of parallel suggestions, and use by native and non-native English writers, to explore a trade-off of “efficiency vs ideation”, emerging from recent literature. We built a text editor prototype with a neural language model (GPT-2), refined in a prestudy with 30 people. In an online study (N=156), people composed emails in four conditions (0/1/3/6 parallel suggestions). Our results reveal (1) benefits for ideation, and costs for efficiency, when suggesting multiple phrases; (2) that non-native speakers benefit more from more suggestions; and (3) further insights into behaviour patterns. We discuss implications for research, the design of interactive suggestion systems, and the vision of supporting writers with AI instead of replacing them.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Nozaki, Hinako; Minawa, Kaori; Takakura, Rei; Shizuki, Buntarou
TI  - AsianCHI@CHI - Investigating Reaction Accuracy of Extended Touchscreen with Conductive Ink for Mobile Virtual Piano
PY  - 2021
AB  - Various methods have been researched to extend the interaction spaces around mobile devices. The method of using conductive ink extends touch interaction space using a sheet with a pattern of conductive ink, which is attached to a capacitive touchscreen. However, the extended area of the interaction space with this method depends on the size of the sheet on which the conductive ink can be printed. We propose a method of connecting sheets using conductive aluminum foil tape. As a test system and an application of this method, we fabricated a mobile virtual piano system with a two-octave keyboard and evaluated the reaction accuracy.
SP  - 111
EP  - 113
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3468191
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Kato, Kunihiro; Kawahara, Yoshihiro
TI  - CHI - LightTouch Gadgets: Extending Interactions on Capacitive Touchscreens by Converting Light Emission to Touch Inputs
PY  - 2021
AB  - We present LightTouch, a 3D-printed passive gadget to enhance touch interactions on unmodified capacitive touchscreens. The LightTouch gadgets simulate finger operations such as tapping, swiping, and multi-touch gestures by means of conductive materials and light-dependent resistors (LDR) embedded in the object. The touchscreen emits visible light and the LDR senses the level of this light, which changes its resistance value. By controlling the screen brightness, it intentionally connects or disconnects the path between the GND and the touchscreen, thus allowing the touch inputs to be controlled. In contrast to conventional physical extensions for touchscreens, our technique requires neither continuous finger contact on the conductive part nor the use of batteries. As such, it opens up new possibilities for touchscreen interactions beyond the simple automation of touch inputs, such as establishing a communication channel between devices, enhancing the trackability of tangibles, and inter-application operations.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445581
ER  - 

TY  - NA
AU  - Park, Eunji; Lee, Byungjoo
TI  - An Intermittent Click Planning Model
PY  - 2020
AB  - Pointing is the task of tracking a target with a pointer and confirming the target selection through a click action when the pointer is positioned within the target. Little is known about the mechanism by which users plan and execute the click action in the middle of the target tracking process. The Intermittent Click Planning model proposed in this study describes the process by which users plan and execute optimal click actions, from which the model predicts the pointing error rates. In two studies in which users pointed to a stationary target and a moving target, the model proved to accurately predict the pointing error rates (R2 = 0.992 and 0.985, respectively). The model has also successfully identified differences in cognitive characteristics among first-person shooter game players.
SP  - 3376725
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376725
ER  - 

TY  - NA
AU  - Sharma, Rahul; Dongare, Adwait; Miller, John H.; Wilkerson, Nicholas; Cohen, Daniel; Sekar, Vyas; Dutta, Prabal; Rowe, Anthony
TI  - IPSN - All that GLITTERs: Low-Power Spoof-Resilient Optical Markers for Augmented Reality
PY  - 2020
AB  - One of the major challenges faced by Augmented Reality (AR) systems is linking virtual content accurately on physical objects and locations. This problem is amplified for applications like mobile payment, device control or secure pairing that requires authentication. In this paper, we present an active LED tag system called GLITTER that uses a combination of Bluetooth Low-Energy (BLE) and modulated LEDs to anchor AR content with no a priori training or labeling of an environment. Unlike traditional optical markers that encode data spatially, each active optical marker encodes a tag’s identifier by blinking over time, improving both the tag density and range compared to AR tags and QR codes.We show that with a low-power BLE-enabled micro-controller and a single 5 mm LED, we are able to accurately link AR content from potentially hundreds of tags simultaneously on a standard mobile phone from as far as 30 meters. Expanding upon this, using active optical markers as a primitive, we show how a constellation of active optical markers can be used for full 3D pose estimation, which is required for many AR applications, using either a single LED on a planar surface or two or more arbitrarily positioned LEDs. Our design supports 108 unique codes in a single field of view with a detection latency of less than 400 ms even when held by hand.
SP  - 289
EP  - 300
JF  - 2020 19th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ipsn48710.2020.00-27
ER  - 

TY  - NA
AU  - Huang, Jin; Peng, Xiaolan; Chen, Rui; Duan, Shengcai; Tian, Feng; Wang, Hongan
TI  - CHI Extended Abstracts - Negative Emotion, Positive Performance? A Glimpse into Emotional Influences on Moving Target Selection
PY  - 2020
AB  - Moving target selection is one of the most fundamental interaction tasks in user interfaces involving dynamic content. In such kind of systems, many stimuli will cause individual positive or negative emotions. Users need to make the selection with the influences of these emotions, such as waving a stick to hit a fast-moving spider. In this study, we explored the effects of induced emotion on user performances in a time-constrained moving target selection task. We found that participants tended to select the targets faster but make more mistakes in positive emotion condition, while they select the targets slower with fewer mistakes in negative emotion condition. We discussed future research directions on this topic and how it could potentially help the design in user interfaces with dynamic content.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382874
ER  - 

TY  - NA
AU  - Maeda, Tatsuya; Kuwayama, Keita; Ito, Kodai; Fujita, Kazuyuki; Itoh, Yuichi
TI  - FullPull : A Stretchable UI to Input Pulling Strength on Touch Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558775
ER  - 

TY  - NA
AU  - Todi, Kashyap; Bailly, Gilles; Leiva, Luis A.; Oulasvirta, Antti
TI  - Adapting User Interfaces with Model-based Reinforcement Learning
PY  - 2021
AB  - Adapting an interface requires taking into account both the positive and negative effects that changes may have on the user. A carelessly picked adaptation may impose high costs to the user -- for example, due to surprise or relearning effort -- or "trap" the process to a suboptimal design immaturely. However, effects on users are hard to predict as they depend on factors that are latent and evolve over the course of interaction. We propose a novel approach for adaptive user interfaces that yields a conservative adaptation policy: It finds beneficial changes when there are such and avoids changes when there are none. Our model-based reinforcement learning method plans sequences of adaptations and consults predictive HCI models to estimate their effects. We present empirical and simulation results from the case of adaptive menus, showing that the method outperforms both a non-adaptive and a frequency-based policy.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445497
ER  - 

TY  - JOUR
AU  - Paredes, Luis; Ipsita, Ananya; Mesa, Juan C.; Martinez Garrido, Ramses V.; Ramani, Karthik
TI  - StretchAR
PY  - 2022
AB  - <jats:p>Over the past decade, augmented reality (AR) developers have explored a variety of approaches to allow users to interact with the information displayed on smart glasses and head-mounted displays (HMDs). Current interaction modalities such as mid-air gestures, voice commands, or hand-held controllers provide a limited range of interactions with the virtual content. Additionally, these modalities can also be exhausting, uncomfortable, obtrusive, and socially awkward. There is a need to introduce comfortable interaction techniques for smart glasses and HMDS without the need for visual attention. This paper presents StretchAR, wearable straps that exploit touch and stretch as input modalities to interact with the virtual content displayed on smart glasses. StretchAR straps are thin, lightweight, and can be attached to existing garments to enhance users' interactions in AR. StretchAR straps can withstand strains up to 190% while remaining sensitive to touch inputs. The strap allows the effective combination of these inputs as a mode of interaction with the content displayed through AR widgets, maps, menus, social media, and Internet of Things (IoT) devices. Furthermore, we conducted a user study with 15 participants to determine the potential implications of the use of StretchAR as input modalities when placed on four different body locations (head, chest, forearm, and wrist). This study reveals that StretchAR can be used as an efficient and convenient input modality for smart glasses with a 96% accuracy. Additionally, we provide a collection of 28 interactions enabled by the simultaneous touch-stretch capabilities of StretchAR. Finally, we facilitate recommendation guidelines for the design, fabrication, placement, and possible applications of StretchAR as an interaction modality for AR content displayed on smart glasses.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550305
ER  - 

TY  - NA
AU  - Olszewski, Kyle; Ceylan, Duygu; Xing, Jun; Echevarria, Jose; Chen, Zhili; Chen, Weikai; Li, Hao
TI  - Intuitive, Interactive Beard and Hair Synthesis with Generative Models
PY  - 2020
AB  - We present an interactive approach to synthesizing realistic variations in facial hair in images, ranging from subtle edits to existing hair to the addition of complex and challenging hair in images of clean-shaven subjects. To circumvent the tedious and computationally expensive tasks of modeling, rendering and compositing the 3D geometry of the target hairstyle using the traditional graphics pipeline, we employ a neural network pipeline that synthesizes realistic and detailed images of facial hair directly in the target image in under one second. The synthesis is controlled by simple and sparse guide strokes from the user defining the general structural and color properties of the target hairstyle. We qualitatively and quantitatively evaluate our chosen method compared to several alternative approaches. We show compelling interactive editing results with a prototype user interface that allows novice users to progressively refine the generated image to match their desired hairstyle, and demonstrate that our approach also allows for flexible and high-fidelity scalp hair synthesis.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Matthews, Shawn; Uribe-Quevedo, Alvaro; Theodorou, Alexander
TI  - SVR - Rendering Optimizations for Virtual Reality Using Eye-Tracking
PY  - 2020
AB  - Optimizing rendering in virtual reality is an open problem in computer science. The nature of modern VR display technology (high refresh rate and increasing pixel density), coupled with the relatively slow growth modern compute capability, is leading to a bottleneck in VR performance. As we further research methodologies for improving rendering performance and accuracy for VR, it is important to understand the historical approaches and where they succeeded or failed in their approaches. Some implementations will double computing because of the need of stereoscopy, and thus have higher overhead for rendering. This can be improved with Multi-View Rendering where the GPU hardware can assist in duplicating rasterization for multiple views with differing projections. More recently, perception-based rendering has gained traction, which can be further accelerated using Variable Shading Rate or Multi-Rate Shading technology found on more recent GPUs. There has also been some success in using deep neural networks to assist with transmitting foveated content over a network. The advances in the field leave many open research questions, including sparse pixel rendering, driving user attention, and techniques and methodologies for combining variable shading rate images. This review focuses research associated with rendering optimizations for virtual reality using eye tracking, since it is becoming a feature present in consumer-level head-mounted displays. From our review, affordable off-the-shelf virtual reality and eye tracking are both leading to freeing up rendering resources towards improved performance and visual fidelity, as well as providing new and exciting opportunities for human-computer interaction.
SP  - 398
EP  - 405
JF  - 2020 22nd Symposium on Virtual and Augmented Reality (SVR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/svr51698.2020.00066
ER  - 

TY  - JOUR
AU  - Park, Kyudong; Kim, Do-Hyeon; Han, Sung H.
TI  - Usability of the size, spacing, and operation method of virtual buttons with virtual hand on head-mounted displays
PY  - 2020
AB  - NA
SP  - 102939
EP  - NA
JF  - International Journal of Industrial Ergonomics
VL  - 76
IS  - NA
PB  - 
DO  - 10.1016/j.ergon.2020.102939
ER  - 

TY  - JOUR
AU  - Abisuga, Oluwayemisi Adebola; Doran, Kallie; de Beer, Deon
TI  - Study of Investment Casting Process For 3D Printed Jewellery Design
PY  - 2022
AB  - <jats:p>Manufacturing processes are increasingly complex with the growing demands of advanced technology in the production processes, especially in the handicraft industry. The complex jewellery designs are complicated to be produced by hand, considering the international demand and dynamics in the jewellery industry. However, advanced production processes and 3D printers are changing the way jewellery designers and manufacturers work and making it easier to produce quality products with fewer production and labour input hours. This study examines the investment casting process of 3D printed design as an option for jewellery manufacturing. The research aims to access jewellery manufacturing processes and its technology application by using trending 3D printing as a rapid prototype. It used the design and production process of 'OneCent Africa' as a case study to describe the process in the investment casting of jewellery products. The investment casting was conducted by prototyping, and the lost wax jewellery casting stages using the vacuum casting machines and burnout oven, with the casting process monitored in parts. This research results led to a better understanding of the experimental casting outcomes and described the potential for the future technological development of jewellery businesses.</jats:p>
SP  - 4002
EP  - NA
JF  - MATEC Web of Conferences
VL  - 370
IS  - NA
PB  - 
DO  - 10.1051/matecconf/202237004002
ER  - 

TY  - NA
AU  - Xiao, Chang; Zheng, Changxi
TI  - UIST - MoiréBoard: A Stable, Accurate and Low-cost Camera Tracking Method
PY  - 2021
AB  - Camera tracking is an essential building block in a myriad of HCI applications. For example, commercial VR devices are equipped with dedicated hardware, such as laser-emitting beacon stations, to enable accurate tracking of VR headsets. However, this hardware remains costly. On the other hand, low-cost solutions such as IMU sensors and visual markers exist, but they suffer from large tracking errors. In this work, we bring high accuracy and low cost together to present MoireBoard, a new 3-DOF camera position tracking method that leverages a seemingly irrelevant visual phenomenon, the moire effect. Based on a systematic analysis of the moire effect under camera projection, MoireBoard requires no power nor camera calibration. It can be easily made at a low cost (e.g., through 3D printing), ready to use with any stock mobile devices with a camera. Its tracking algorithm is computationally efficient, able to run at a high frame rate. Although it is simple to implement, it tracks devices at high accuracy, comparable to the state-of-the-art commercial VR tracking systems.
SP  - 881
EP  - 893
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474793
ER  - 

TY  - NA
AU  - Gebhardt, Christoph; Hilliges, Otmar
TI  - Optimal Control to Support High-Level User Goals in Human-Computer Interaction
PY  - 2021
AB  - With emerging technologies like robots, mixed-reality systems or mobile devices, machine-provided capabilities are increasing, so is the complexity of their control and display mechanisms. To address this dichotomy, we propose optimal control as a framework to support users in achieving their high-level goals in human-computer tasks. We reason that it will improve user support over usual approaches for adaptive interfaces as its formalism implicitly captures the iterative nature of human-computer interaction. We conduct two case studies to test this hypothesis. First, we propose a model-predictive-control-based optimization scheme that supports end-users to plan and execute robotic aerial videos. Second, we introduce a reinforcement-learning-based method to adapt mixed-reality augmentations based on users’ preferences or tasks learned from their gaze interactions with a UI. Our results show that optimal control can better support users’ high-level goals in human-computer tasks than common approaches. Optimal control models human-computer interaction as a sequential decision problem which represents its nature and, hence, results in better predictability of user behavior than for other methods. In addition, our work highlights that optimization- and learning-based optimal control have complementary strengths with respect to interface adaptation.
SP  - 33
EP  - 72
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_2
ER  - 

TY  - NA
AU  - Fang, Chiao; Chan, Vivian Hsinyueh; Cheng, Lung-Pan
TI  - Flaticulation: Laser Cutting Joints with Articulated Angles
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545695
ER  - 

TY  - CONF
AU  - Todi, Kashyap; Bailly, Gilles; Leiva, Luis A.; Oulasvirta, Antti
TI  - CHI - Adapting User Interfaces with Model-based Reinforcement Learning
PY  - 2021
AB  - Adapting an interface requires taking into account both the positive and negative effects that changes may have on the user. A carelessly picked adaptation may impose high costs to the user – for example, due to surprise or relearning effort – or “trap” the process to a suboptimal design immaturely. However, effects on users are hard to predict as they depend on factors that are latent and evolve over the course of interaction. We propose a novel approach for adaptive user interfaces that yields a conservative adaptation policy: It finds beneficial changes when there are such and avoids changes when there are none. Our model-based reinforcement learning method plans sequences of adaptations and consults predictive HCI models to estimate their effects. We present empirical and simulation results from the case of adaptive menus, showing that the method outperforms both a non-adaptive and a frequency-based policy.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - CONF
AU  - Cho, Youngjun
TI  - CHI - Rethinking Eye-blink: Assessing Task Difficulty through Physiological Representation of Spontaneous Blinking
PY  - 2021
AB  - Continuous assessment of task difficulty and mental workload is essential in improving the usability and accessibility of interactive systems. Eye tracking data has often been investigated to achieve this ability, with reports on the limited role of standard blink metrics. Here, we propose a new approach to the analysis of eye-blink responses for automated estimation of task difficulty. The core module is a time-frequency representation of eye-blink, which aims to capture the richness of information reflected on blinking. In our first study, we show that this method significantly improves the sensitivity to task difficulty. We then demonstrate how to form a framework where the represented patterns are analyzed with multi-dimensional Long Short-Term Memory recurrent neural networks for their non-linear mapping onto difficulty-related parameters. This framework outperformed other methods that used hand-engineered features. This approach works with any built-in camera, without requiring specialized devices. We conclude by discussing how Rethinking Eye-blink can benefit real-world applications.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Wessely, Michael; Sethapakdi, Ticha; Castillo, Carlos; Snowden, Jackson C.; Hanton, Ollie; Qamar, Isabel P. S.; Fraser, Mike; Roudaut, Anne; Mueller, Stefanie
TI  - CHI - Sprayable User Interfaces: Prototyping Large-Scale Interactive Surfaces with Sensors and Displays
PY  - 2020
AB  - We present Sprayable User Interfaces: room-sized interactive surfaces that contain sensor and display elements created by airbrushing functional inks. Since airbrushing is inherently mobile, designers can create large-scale user interfaces on complex 3D geometries where existing stationary fabrication methods fail. To enable Sprayable User Interfaces, we developed a novel design and fabrication pipeline that takes a desired user interface layout as input and automatically generates stencils for airbrushing the layout onto a physical surface. After fabricating stencils from cardboard or projecting stencils digitally, designers spray each layer with an airbrush, attach a microcontroller to the user interface, and the interface is ready to be used. Our technical evaluation shows that Sprayable User Interfaces work on various geometries and surface materials, such as porous stone and rough wood. We demonstrate our system with several application examples including interactive smart home applications on a wall and a soft leather sofa, an interactive smart city application, and interactive architecture in public office spaces.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376249
ER  - 

TY  - NA
AU  - Wang, Ju; Chang, Liqiong; Aggarwal, Shourya; Abari, Omid; Keshav, Srinivasan
TI  - MobiSys - Soil moisture sensing with commodity RFID systems
PY  - 2020
AB  - Intelligent irrigation based on measurements of soil moisture levels in every pot in a greenhouse can not only improve plant productivity and quality but also save water. However, existing soil moisture sensors are too expensive to deploy in every pot. We therefore introduce GreenTag, a low-cost RFID-based soil moisture sensing system whose accuracy is comparable to that of an expensive soil moisture sensor. Our key idea is to attach two RFID tags to a plant's container so that changes in soil moisture content are reflected in their Differential Minimum Response Threshold (DMRT) metric at the reader. We show that a low-pass filtered DMRT metric is robust to changes both in the RF environment (e.g., from human movement) and in pot locations. In a realistic setting, GreenTag achieves a 90-percentile moisture estimation errors of 5%, which is comparable to the 4% errors using expensive soil moisture sensors. Moreover, this accuracy is maintained despite changes in the RF environment and container locations. We also show the effectiveness of GreenTag in a real greenhouse.
SP  - 273
EP  - 285
JF  - Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3386901.3388940
ER  - 

TY  - NA
AU  - De Bauw, Tim; Cools, Robbe; Simeone, Adalberto L.
TI  - AdapTables: Using Conformal Mapping for Collaboration on Tables in Asymmetric Mixed Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567691
ER  - 

TY  - NA
AU  - Umetsu, Yuto; Punpongsanon, Parinya; Hiraki, Takefumi
TI  - InfiniteShader: Color Changeable 3D Printed Objects using Bi-Stable Thermochromic Materials
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2022 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3550082.3564209
ER  - 

TY  - NA
AU  - Singh, Vaibhav; Prabhakara, Akarsh; Zhang, Diana; Yagan, Osman; Kumar, Swarun
TI  - MobiCom - A community-driven approach to democratize access to satellite ground stations
PY  - 2021
AB  - Should you decide to launch a nano-satellite today in Low-Earth Orbit (LEO), the cost of renting ground station communication infrastructure is likely to significantly exceed your launch costs. While space launch costs have lowered significantly with innovative launch vehicles, private players, and smaller payloads, access to ground infrastructure remains a luxury. This is especially true for smaller LEO satellites that are only visible at any location for a few tens of minutes a day and whose signals are extremely weak, necessitating bulky and expensive ground station infrastructure. In this paper, we present a community-driven distributed reception paradigm for LEO satellite signals where signals received on many tiny handheld receivers (not necessarily deployed on rooftops but also indoors) are coherently combined to recover the desired signal. This is made possible by employing new synchronization and receiver orientation techniques that study satellite trajectories and leverage the presence of other ambient signals. We compare our results with a large commercial receiver deployed on a rooftop and show a 8 dB SNR increase both indoors and outdoors using 8 receivers, costing $38 per RF frontend.
SP  - 1
EP  - 14
JF  - Proceedings of the 27th Annual International Conference on Mobile Computing and Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447993.3448630
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Dachselt, Raimund
TI  - UIST (Adjunct Volume) - Bendable Color ePaper Displays for Novel Wearable Applications and Mobile Visualization
PY  - 2021
AB  - This paper presents a toolkit that allows to easily prototype with bendable color ePaper displays for designing and studying novel body-worn interfaces in mobile scenarios. We introduce a software and hardware platform that enables researchers for the first time to implement fully-functional wearable and UbiComp applications with interactive, curved color pixel displays. Further, we provide a set of visual and sensory-rich materials for customization and mounting options. To technically validate our approach and demonstrate its promising potential, we implemented eight real-world applications ranging from personal information and mobile data visualizations over active notifications to media controls. Finally, we report on first usage experiences and conclude with a research roadmap that outlines future applications and directions.
SP  - 6
EP  - 10
JF  - The Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3474349.3480213
ER  - 

TY  - JOUR
AU  - Hong, Sanghwa; Heo, Seongkook; Lee, Byungjoo
TI  - MaterialSense: Estimating and utilizing material properties of contact objects in multi-touch interaction
PY  - NA
AB  - NA
SP  - 102985
EP  - NA
JF  - International Journal of Human-Computer Studies
VL  - 172
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2022.102985
ER  - 

TY  - NA
AU  - Roumen, Thijs; Lempert, Conrad; Apel, Ingo; Brendel, Erik; Brand, Markus; Seidel, Laurenz; Rambold, Lukas; Baudisch, Patrick
TI  - UIST - autoAssembler: Automatic Reconstruction of Laser-Cut 3D Models
PY  - 2021
AB  - Recent research showed how to import laser cut 3D models encoded in the form of 2D cutting plans into a 3D editor (assembler3 [28]), which allows users to perform parametric manipulations on such models. In contrast to assembler3 , which requires users to perform this process manually, we present autoAssembler, which performs this process automatically. AutoAssembler uses a beam search algorithm to search possible ways of assembling plates. It uses joints on these plates to combine them into assembly candidates. It thereby preferably pursues candidates (1) that have no intersecting plates, (2) that fit into a small bounding box, (3) that use plates whose joints fit together well, (4) that do not add many unpaired joints, (5) that make use of constraints posed by other plates, and (6) that conform to symmetry axes of the plates. This works for models that have at least one edge joint (finger or t-joint). In our technical evaluation, we imported 66 models using autoAssembler. AutoAssembler assembled 79% of those models fully automatically; another 18% of models required on average 2.7 clicks of post-processing, for an overall success rate of 97%.
SP  - 652
EP  - 662
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474776
ER  - 

TY  - NA
AU  - Strohmeier, Paul; Güngör, Seref; Herres, Luis; Gudea, Dennis; Fruchard, Bruno; Steimle, Jürgen
TI  - UIST - bARefoot: Generating Virtual Materials using Motion Coupled Vibration in Shoes
PY  - 2020
AB  - Many features of materials can be experienced through tactile cues, even using one's feet. For example, one can easily distinguish between moss and stone without looking at the ground. However, this type of material experience is largely not supported in AR and VR applications. We present bARefoot, a prototype shoe providing tactile impulses tightly coupled to motor actions. This enables generating virtual material experiences such as compliance, elasticity, or friction. To explore the parameter space of such sensorimotor coupled vibrations, we present a design tool enabling rapid design of virtual materials. We report initial explorations to increase understanding of how parameters can be optimized for generating compliance, and to examine the effect of dynamic parameters on material experiences. Finally, we present a series of use cases that demonstrate the potential of bARefoot for VR and AR.
SP  - 579
EP  - 593
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415828
ER  - 

TY  - JOUR
AU  - Alderighi, T.; Malomo, L.; Auzinger, T.; Bickel, B.; Cignoni, P.; Pietroni, N.
TI  - State of the Art in Computational Mould Design
PY  - 2022
AB  - Moulding refers to a set of manufacturing techniques in which a mould, usually a cavity or a solid frame, is used to shape a liquid or pliable material into an object of the desired shape. The popularity of moulding comes from its effectiveness, scalability and versatility in terms of employed materials. Its relevance as a fabrication process is demonstrated by the extensive literature covering different aspects related to mould design, from material flow simulation to the automation of mould geometry design. In this state-of-the-art report, we provide an extensive review of the automatic methods for the design of moulds, focusing on contributions from a geometric perspective. We classify existing mould design methods based on their computational approach and the nature of their target moulding process. We summarize the relationships between computational approaches and moulding techniques, highlighting their strengths and limitations. Finally, we discuss potential future research directions.
SP  - 435
EP  - 452
JF  - Computer Graphics Forum
VL  - 41
IS  - 6
PB  - 
DO  - 10.1111/cgf.14581
ER  - 

TY  - BOOK
AU  - Blokker, Roy; Xu, Talia; Zuniga, Marco
TI  - CHIIoT@EWSN/EICS - Communication with Ambient Light using Digital Micromirror Devices.
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Krauß, Veronika; Nebeling, Michael; Jasche, Florian; Boden, Alexander
TI  - Elements of XR Prototyping: Characterizing the Role and Use of Prototypes in Augmented and Virtual Reality Design
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517714
ER  - 

TY  - NA
AU  - Yamamura, Hiroo; Baldauf, Holger; Kunze, Kai
TI  - AHs - HemodynamicVR - Adapting the User’s Field Of View during Virtual Reality Locomotion Tasks to Reduce Cybersickness using Wearable Functional Near-Infrared Spectroscopy
PY  - 2021
AB  - We present HemodynamicVR, a virtual reality headset combined with functional near-infrared spectroscopy (fNIRS). We believe that sensing brain activity will enable novel interactions in virtual reality. In this paper, we assess a user’s cybersickness based on the change of their total hemoglobin concentration measured via an fNIRS device, and we try to mitigate that by changing the field of view (FOV) in real-time. In this experiment, participants experienced VR locomotion with an added variable FOV controlled by velocity changes and by fNIRS. The results suggest that fNIRS can detect cybersickness as registered by the qualitative SSQ test.
SP  - 223
EP  - 227
JF  - Augmented Humans Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3458709.3458994
ER  - 

TY  - JOUR
AU  - Xu, Peng; Hospedales, Timothy M.; Yin, Qiyue; Song, Yi-Zhe; Xiang, Tao; Wang, Liang
TI  - Deep Learning for Free-Hand Sketch: A Survey
PY  - 2023
AB  - Free-hand sketches are highly illustrative, and have been widely used by humans to depict objects or stories from ancient times to the present. The recent prevalence of touchscreen devices has made sketch creation a much easier task than ever and consequently made sketch-oriented applications increasingly popular. The progress of deep learning has immensely benefited free-hand sketch research and applications. This paper presents a comprehensive survey of the deep learning techniques oriented at free-hand sketch data, and the applications that they enable. The main contents of this survey include: (i) A discussion of the intrinsic traits and unique challenges of free-hand sketch, to highlight the essential differences between sketch data and other data modalities, e.g., natural photos. (ii) A review of the developments of free-hand sketch research in the deep learning era, by surveying existing datasets, research topics, and the state-of-the-art methods through a detailed taxonomy and experimental evaluation. (iii) Promotion of future work via a discussion of bottlenecks, open problems, and potential research directions for the community.
SP  - 285
EP  - 312
JF  - IEEE Transactions on Pattern Analysis and Machine Intelligence
VL  - 45
IS  - 1
PB  - 
DO  - 10.1109/tpami.2022.3148853
ER  - 

TY  - JOUR
AU  - Isomoto, Toshiya; Yamanaka, Shota; Shizuki, Buntarou
TI  - Dwell Selection with ML-based Intent Prediction Using Only Gaze Data
PY  - 2022
AB  - <jats:p>We developed a dwell selection system with ML-based prediction of a user's intent to select. Because a user perceives visual information through the eyes, precise prediction of a user's intent will be essential to the establishment of gaze-based interaction. Our system first detects a dwell to roughly screen the user's intent to select and then predicts the intent by using an ML-based prediction model. We created the intent prediction model from the results of an experiment with five different gaze-only tasks representing everyday situations. The intent prediction model resulted in an overall area under the curve (AUC) of the receiver operator characteristic curve of 0.903. Moreover, it could perform independently of the user (AUC=0.898) and the eye-tracker (AUC=0.880). In a performance evaluation experiment with real interactive situations, our dwell selection method had both higher qualitative and quantitative performance than previously proposed dwell selection methods.</jats:p>
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550301
ER  - 

TY  - JOUR
AU  - Liu, Yang; Kale, Alex; Althoff, Tim; Heer, Jeffrey
TI  - Boba: Authoring and Visualizing Multiverse Analyses
PY  - 2021
AB  - Multiverse analysis is an approach to data analysis in which all “reasonable” analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Baba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.
SP  - 1753
EP  - 1763
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 2
PB  - 
DO  - 10.1109/tvcg.2020.3028985
ER  - 

TY  - NA
AU  - Roumen, Thijs; Kommana, Yannis; Apel, Ingo; Lempert, Conrad; Brand, Markus; Brendel, Erik; Seidel, Laurenz; Rambold, Lukas; Goedecken, Carl; Crenzin, Pascal; Hurdelhey, Ben; Abdullah, Muhammad; Baudisch, Patrick
TI  - CHI - Assembler3: 3D Reconstruction of Laser-Cut Models
PY  - 2021
AB  - We present Assembler3 a software tool that allows users to perform 3D parametric manipulations on 2D laser cutting plans. Assembler3 achieves this by semi-automatically converting 2D laser cutting plans to 3D, where users modify their models using available 3D tools (kyub), before converting them back to 2D. In our user study, this workflow allowed users to modify models 10x faster than using the traditional approach of editing 2D cutting plans directly. Assembler3 converts models to 3D in 5 steps: (1) plate detection, (2) joint detection, (3) material thickness detection, (4) joint matching based on hashed joint "signatures", and (5) interactive reconstruction. In our technical evaluation, Assembler3 was able to reconstruct 100 of 105 models. Once 3D-reconstructed, we expect users to store and share their models in 3D, which can simplify collaboration and thereby empower the laser cutting community to create models of higher complexity.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445453
ER  - 

TY  - JOUR
AU  - Liu, Fang; Deng, Xiaoming; Song, Jiancheng; Lai, Yu-Kun; Liu, Yong-Jin; Wang, Hao; Ma, Cuixia; Qin, Shengfeng; Wang, Hongan
TI  - SketchMaker: Sketch Extraction and Reuse for Interactive Scene Sketch Composition
PY  - 2022
AB  - <jats:p> Sketching is an intuitive and simple way to depict sciences with various object form and appearance characteristics. In the past few years, widely available touchscreen devices have increasingly made sketch-based human-AI co-creation applications popular. One key issue of sketch-oriented interaction is to prepare input sketches efficiently by non-professionals because it is usually difficult and time-consuming to draw an ideal sketch with appropriate outlines and rich details, especially for novice users with no sketching skills. Thus, sketching brings great obstacles for sketch applications in daily life. On the other hand, hand-drawn sketches are scarce and hard to collect. Given the fact that there are several large-scale sketch datasets providing sketch data resources, but they usually have a limited number of objects and categories in sketch, and do not support users to collect new sketch materials according to their personal preferences. In addition, few sketch-related applications support the reuse of existing sketch elements. Thus, knowing how to extract sketches from existing drawings and effectively re-use them in interactive scene sketch composition will provide an elegant way for <jats:bold>sketch-based image retrieval (SBIR)</jats:bold> applications, which are widely used in various touch screen devices. In this study, we first conduct a study on current SBIR to better understand the main requirements and challenges in sketch-oriented applications. Then we develop the SketchMaker as an interactive sketch extraction and composition system to help users generate scene sketches via reusing object sketches in existing scene sketches with minimal manual intervention. Moreover, we demonstrate how SBIR improves from composited scene sketches to verify the performance of our interactive sketch processing system. We also include a sketch-based video localization task as an alternative application of our sketch composition scheme. Our pilot study shows that our system is effective and efficient, and provides a way to promote practical applications of sketches. </jats:p>
SP  - 1
EP  - 26
JF  - ACM Transactions on Interactive Intelligent Systems
VL  - 12
IS  - 3
PB  - 
DO  - 10.1145/3543956
ER  - 

TY  - JOUR
AU  - Vellingiri, Shanthi; McMahan, Ryan P.; Johnson, Vinu; Prabhakaran, Balakrishnan
TI  - An augmented virtuality system facilitating learning through nature walk
PY  - 2022
AB  - NA
SP  - 1553
EP  - 1564
JF  - Multimedia Tools and Applications
VL  - 82
IS  - 1
PB  - 
DO  - 10.1007/s11042-022-13379-w
ER  - 

TY  - JOUR
AU  - Maddali, Hanuma Teja; Irlitti, Andrew; Lazar, Amanda
TI  - Probing the Potential of Extended Reality to Connect Experts and Novices in the Garden
PY  - 2022
AB  - <jats:p>As extended reality (XR) systems become increasingly available, XR-based remote instruction is being adopted for diverse purposes in professional settings such as surgery and field servicing. Hobbyists have been well-studied in HCI and may similarly benefit from remote skill-sharing. However, little is known about how XR technologies might support expert-novice collaboration for skilled hobby activities. This paper examines the potential and limitations of XR to connect experts and novices for one such activity: gardening. Through two studies involving 27 expert and novice gardeners, we designed prototypes to understand 1) practitioner perceptions of XR and remote skill-sharing in the garden and 2) what kinds of interactions can be supported in XR for expert-novice groups. We discuss design opportunities and challenges for XR systems in supporting informal connecting interactions and meaningful sensory interactions with a remote environment during skill-sharing.</jats:p>
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW2
PB  - 
DO  - 10.1145/3555211
ER  - 

TY  - NA
AU  - Jones, Brennan; Zhang, Yaying; Wong, Priscilla N. Y.; Rintel, Sean
TI  - CHI Extended Abstracts - VROOM: Virtual Robot Overlay for Online Meetings
PY  - 2020
AB  - Telepresence robots allow users to freely explore a remote space and provide a physical embodiment in that space. However, they lack a compelling representation of the remote user in the local space. We present VROOM (Virtual Robot Overlay for Online Meetings), a two-way system for exploring how to improve the social experience of robotic telepresence. For the local user, an augmented-reality (AR) interface shows a life-size avatar of the remote user overlaid on a telepresence robot. For the remote user, a head-mounted virtual-reality (VR) interface presents an immersive 360° view of the local space with mobile autonomy. The VR system tracks the remote user's head pose and hand movements, which are applied to an avatar. This provides the remote user with an identifiable self-embodiment and allows the local user to see the remote user's head direction and arm gestures.
SP  - 1
EP  - 10
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382820
ER  - 

TY  - NA
AU  - Abdullah, Muhammad; Sommerfeld, Romeo; Seidel, Laurenz; Noack, Jonas; Zhang, Ran; Roumen, Thijs; Baudisch, Patrick
TI  - UIST - Roadkill: Nesting Laser-Cut Objects for Fast Assembly
PY  - 2021
AB  - We present Roadkill, a software tool that converts 3D models to 2D cutting plans for laser cutting—such that the resulting layouts allow for fast assembly. Roadkill achieves this by putting all relevant information into the cutting plan: (1) Thumbnails indicate which area of the model a set of parts belongs to. (2) Parts with exposed finger joints are easy to access, thereby suggesting to start assembly here. (3) Openings in the sheet act as jigs, affording assembly within the sheet. (4) Users continue assembly by inserting what has already been assembled into parts that are immediately adjacent or are pointed to by arrows. Roadkill maximizes the number of joints rendered in immediate adjacency by breaking down models into “subassemblies.” Within a subassembly, Roadkill holds the parts together using break-away tabs. (5) Users complete subassemblies according to their labels 1, 2, 3…, following 1 -> 1 links to insert subassemblies into other subassemblies, until all parts come together. In our user study, Roadkill allowed participants to assemble layouts 2.4 times faster than layouts generated by a traditional pair-wise labeling of plates.
SP  - 972
EP  - 984
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474799
ER  - 

TY  - NA
AU  - Chang, Ruei-Che; Wang, Wen-Ping; Chiang, Chi-huan; Wu, Te-Yen; Xu, Zheer; Luo, Justin; Chen, Bing-Yu; Yang, Xing-Dong
TI  - CHI - AccessibleCircuits: Adaptive Add-On Circuit Components for People with Blindness or Low Vision
PY  - 2021
AB  - In this paper, we propose the designs for low cost and 3D-printable add-on components to adapt existing breadboards, circuit components and electronics tools for blind or low vision (BLV) users. Through an initial user study, we identified several barriers to entry for beginners with BLV in electronics and circuit prototyping. These barriers guided the design and development of our add-on components. We focused on developing adaptations that provide additional information about the specific component pins and breadboard holes, modify tools to make them easier to use for users with BLV, and expand non-visual feedback (e.g., audio, tactile) for tasks that require vision. Through a second user study, we demonstrated that our adaptations can effectively overcome the accessibility barriers in breadboard circuit prototyping for users with BLV.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445690
ER  - 

TY  - NA
AU  - Kim, Hyung-il; Kim, Taehei; Song, Eunhwa; Oh, Seo Young; Kim, Dooyoung; Woo, Woontack
TI  - ISMAR Adjunct - Multi-scale Mixed Reality Collaboration for Digital Twin
PY  - 2021
AB  - In this poster, we present a digital twin-based mixed reality system for remote collaboration with the size-scaling of the user and the space. The proposed system supports collaboration between an AR host user and a VR remote user by sharing a 3D digital twin of the AR host user. To enhance the coarse authoring of a shared digital twin environment, we provide a size scaling of the digital twin environment with the world-in-miniature view. Also, we enable scaling the size of the VR user’s avatar to enhance both coarse (size-up) and fine-grained (size-down) authoring of the digital twin environment. We describe the system setup, input methods, and interaction methods for scaling space and user.
SP  - 435
EP  - 436
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct54149.2021.00098
ER  - 

TY  - CHAP
AU  - Chen, Yuan; Sun, Junwei; Xu, Qiang; Lank, Edward; Irani, Pourang; Li, Wei
TI  - INTERACT (4) - Empirical Evaluation of Moving Target Selection in Virtual Reality Using Egocentric Metaphors
PY  - 2021
AB  - Virtual hand or pointer metaphors are among the key approaches for target selection in immersive environments. However, targeting moving objects is complicated by factors including target speed, direction, and depth, such that a basic implementation of these techniques might fail to optimize user performance. We present results of two empirical studies comparing characteristics of virtual hand and pointer metaphors for moving target acquisition. Through a first study, we examine the impact of depth on users’ performance when targets move beyond and within arms’ reach. We find that movement in depth has a great impact on both metaphors. In a follow-up study, we design a reach-bounded Go-Go (rbGo-Go) technique to address challenges of virtual hand and compare it to Ray-Casting. We find that target width and speed are significant determinants of user performance and we highlight the pros and cons for each of the techniques in the given context. Our results inform the UI design for immersive selection of moving targets.
SP  - 29
EP  - 50
JF  - Human-Computer Interaction – INTERACT 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-85610-6_3
ER  - 

TY  - NA
AU  - Yu, Kevin; Winkler, Alexander; Pankratz, Frieder; Lazarovici, Marc; Wilhelm, Dirk; Eck, Ulrich; Roth, Daniel; Navab, Nassir
TI  - VR - Magnoramas: Magnifying Dioramas for Precise Annotations in Asymmetric 3D Teleconsultation
PY  - 2021
AB  - When users create hand-drawn annotations in Virtual Reality they often reach their physical limits in terms of precision, especially if the region to be annotated is small. One intuitive solution employs magnification beyond natural scale. However, scaling the whole environment results in wrong assumptions about the coherence between physical and virtual space. In this paper, we introduce Mag-noramas, a novel interaction method for selecting and extracting a region of interest that the user can subsequently scale and transform inside the virtual space. Our technique enhances the user's capabilities to perform supernaturally precise virtual annotations on virtual objects. We explored our technique in a user study within asimplified clinical scenario of a teleconsultation-supported craniectomy procedure that requires accurate annotations on a human head. Teleconsultation was performed asymmetrically between a remote expert in Virtual Reality that collaborated with a local user through Augmented Reality. The remote expert operates inside a reconstructed environment, captured from RGB-D sensors at the local site, and is embodied by an avatar to establish co-presence. The results show that Magnoramas significantly improve the precision of annotations while preserving usability and perceived presence measures compared to the baseline method. By hiding the 3D reconstruction while keeping the Magnorama, users can intentionally choose to lower their perceived social presence and focus on their tasks.
SP  - 392
EP  - 401
JF  - 2021 IEEE Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr50410.2021.00062
ER  - 

TY  - NA
AU  - Ahuja, Karan; Shen, Vivian; Fang, Cathy Mengying; Riopelle, Nathan; Kong, Andy; Harrison, Chris
TI  - ControllerPose: Inside-Out Body Capture with VR Controller Cameras
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502105
ER  - 

TY  - CHAP
AU  - Zhang, Mingrui Ray; Wen, He; Cui, Wenzhe; Zhu, Suwen; Schwartz, H. Andrew; Bi, Xiaojun; Wobbrock, Jacob O.
TI  - AI-Driven Intelligent Text Correction Techniques for Mobile Text Entry
PY  - 2021
AB  - Current text correction processes on mobile touch devices are laborious: users either extensively use backspace, or navigate the cursor to the error position, make a correction, and navigate back, usually by employing multiple taps or drags over small targets. In this chapter, we present two techniques, Type, Then Correct and JustCorrect, that utilize the power of artificial intelligence to improve the text correction experience on mobile devices. All of the techniques skip error-deletion and cursor-positioning procedures, and instead allow the user to type the correction first, and then apply that correction to a previously committed error. We evaluated these techniques in and the results show that correction with the new techniques was faster than de facto cursor and backspace-based correction.
SP  - 131
EP  - 168
JF  - Human–Computer Interaction Series
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-82681-9_5
ER  - 

TY  - NA
AU  - Whitlock, Matt; Smart, Stephen; Szafir, Danielle Albers
TI  - VR - Graphical Perception for Immersive Analytics
PY  - 2020
AB  - Immersive Analytics (IA) uses immersive virtual and augmented reality displays for data visualization and visual analytics. Designers rely on studies of how accurately people interpret data in different visualizations to make effective visualization choices. However, these studies focus on data analysis in traditional desktop environments. We lack empirical grounding for how to best visualize data in immersive environments. This study explores how people interpret data visualizations across different display types by measuring how quickly and accurately people conduct three analysis tasks over five visual channels: color, size, height, orientation, and depth. We identify key quantitative differences in performance and user behavior, indicating that stereo viewing resolves some of the challenges of visualizations in 3D space. We also find that while AR displays encourage increased navigation, they decrease performance with color-based visualizations. Our results provide guidelines on how to tailor visualizations to different displays in order to better leverage the affordances of IA modalities.
SP  - 616
EP  - 625
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1582298687237
ER  - 

TY  - NA
AU  - Fan, Jiayue; Xu, Chenning; Yu, Chun; Shi, Yuanchun
TI  - UIST - Just Speak It: Minimize Cognitive Load for Eyes-Free Text Editing with a Smart Voice Assistant
PY  - 2021
AB  - Entering text precisely by voice, users might encounter colloquial inserts, inappropriate wording, and recognition errors, which brings difficulties to voice editing. Users need to locate the errors and then correct them. In eyes-free scenarios, this select-modify mode brings a cognitive burden and a risk of error. This paper introduces neural networks and pre-trained models to understand users’ revision intention based on semantics, reducing the need for the information from users’ statements. We present two strategies. One is to remove the colloquial inserts automatically. The other is to allow users to edit by just speaking out the target words without having to say the context and the incorrect text. Accordingly, our approach can predict whether to insert or replace, the incorrect text to replace, and the position to insert. We implement these strategies in SmartEdit, an eyes-free voice input agent controlled with earphone buttons. The evaluation shows that our techniques reduce the cognitive load and decrease the average failure rate by 54.1% compared to descriptive command or re-speaking.
SP  - 910
EP  - 921
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474795
ER  - 

TY  - NA
AU  - Chen, QianQian; Li, ZhaoWen; Lin, ShuJin; Zhou, Fan
TI  - Automatic Sketch Generation Method Based on Text
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 9th International Conference on Digital Home (ICDH)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icdh57206.2022.00038
ER  - 

TY  - JOUR
AU  - Ge, Ce; Sun, Haifeng; Song, Yi-Zhe; Ma, Zhanyu; Liao, Jianxin
TI  - Exploring Local Detail Perception for Scene Sketch Semantic Segmentation.
PY  - 2022
AB  - In this paper, we aim to explore the fine-grained perception ability of deep models for the newly proposed scene sketch semantic segmentation task. Scene sketches are abstract drawings containing multiple related objects. It plays a vital role in daily communication and human-computer interaction. The study has only recently started due to a main obstacle of the absence of large-scale datasets. The currently available dataset SketchyScene is composed of clip art-style edge maps, which lacks abstractness and diversity. To drive further research, we contribute two new large-scale datasets based on real hand-drawn object sketches. A general automatic scene sketch synthesis process is developed to assist with new dataset composition. Furthermore, we propose to enhancing local detail perception in deep models to realize accurate stroke-oriented scene sketch segmentation. Due to the inherent differences between hand-drawn sketches and natural images, extreme low-level local features of strokes are incorporated to improve detail discrimination. Stroke masks are also integrated into model training to guide the learning attention. Extensive experiments are conducted on three large-scale scene sketch datasets. Our method achieves state-of-the-art performance under four evaluation metrics and yields meaningful interpretability via visual analytics.
SP  - 1447
EP  - 1461
JF  - IEEE transactions on image processing : a publication of the IEEE Signal Processing Society
VL  - 31
IS  - NA
PB  - 
DO  - 10.1109/tip.2022.3142511
ER  - 

TY  - NA
AU  - Glowacka, Dorota; Howes, Andrew; Jokinen, Jussi P. P.; Oulasvirta, Antti; Şimşek, Özgür
TI  - CHI Extended Abstracts - RL4HCI: Reinforcement Learning for Humans, Computers, and Interaction
PY  - 2021
AB  - Reinforcement learning (RL) is emerging as an approach to understand intelligence in both humans and machines. However, if RL is to have a meaningful impact in human–computer interaction, it is critical that these two threads are integrated. This is required for genuinely interactive RL-based systems which take into account user capacities and preferences. This workshop will build a community and form a research agenda for investigating RL in HCI.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3441323
ER  - 

TY  - NA
AU  - Ngoon, Tricia J.; Kim, Joy O.; Klemmer, Scott R.
TI  - Conference on Designing Interactive Systems - Shöwn: Adaptive Conceptual Guidance Aids Example Use in Creative Tasks
PY  - 2021
AB  - Examples are powerful tools for creativity and can provide inspiration and structure. However, novices often don’t know when to seek examples or how to apply them. Showing real-time examples targeted to the current task (adaptive conceptual guidance) may help novices consider inspiration more often and better implement the ideas that examples illustrate. We explore this in a Wizard-of-Oz system, Shown, that presents examples based on the user’s current activity while drawing a comic strip. Shown’s design is informed by interviews with novice and expert comic artists (n=18). A between-subjects experiment (n=24) found that adaptive conceptual guidance improved the clarity and uniqueness of drawings and stories compared to non-adaptive examples. Users also found the examples more useful and inspirational than those without adaptive guidance. Our results present an initial exploration of the challenges and benefits of contextually showing examples in interactive creativity tools.
SP  - 1834
EP  - 1845
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462072
ER  - 

TY  - JOUR
AU  - de Winkel, Jasper; Kortbeek, Vito; Hester, Josiah; Pawelczak, Przemyslaw
TI  - Battery-Free Game Boy
PY  - 2020
AB  - We present ENGAGE, the first battery-free, personal mobile gaming device powered by energy harvested from the gamer actions and sunlight. Our design implements a power failure resilient Nintendo Game Boy emulator that can run off-the-shelf classic Game Boy games like Tetris or Super Mario Land. This emulator is capable of intermittent operation by tracking memory usage, avoiding the need for always checkpointing all volatile memory, and decouples the game loop from user interface mechanics allowing for restoration after power failure. We build custom hardware that harvests energy from gamer button presses and sunlight, and leverages a mixed volatility memory architecture for efficient intermittent emulation of game binaries. Beyond a fun toy, our design represents the first battery-free system design for continuous user attention despite frequent power failures caused by intermittent energy harvesting. We tackle key challenges in intermittent computing for interaction including seamless displays and dynamic incentive-based gameplay for energy harvesting. This work provides a reference implementation and framework for a future of battery-free mobile gaming in a more sustainable Internet of Things.
SP  - 111
EP  - 34
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 3
PB  - 
DO  - 10.1145/3411839
ER  - 

TY  - NA
AU  - Olszewski, Kyle; Ceylan, Duygu; Xing, Jun; Echevarria, Jose; Chen, Zhili; Chen, Weikai; Li, Hao
TI  - CVPR - Intuitive, Interactive Beard and Hair Synthesis With Generative Models
PY  - 2020
AB  - We present an interactive approach to synthesizing realistic variations in facial hair in images, ranging from subtle edits to existing hair to the addition of complex and challenging hair in images of clean-shaven subjects. To circumvent the tedious and computationally expensive tasks of modeling, rendering and compositing the 3D geometry of the target hairstyle using the traditional graphics pipeline, we employ a neural network pipeline that synthesizes realistic and detailed images of facial hair directly in the target image in under one second. The synthesis is controlled by simple and sparse guide strokes from the user defining the general structural and color properties of the target hairstyle. We qualitatively and quantitatively evaluate our chosen method compared to several alternative approaches. We show compelling interactive editing results with a prototype user interface that allows novice users to progressively refine the generated image to match their desired hairstyle, and demonstrate that our approach also allows for flexible and high-fidelity scalp hair synthesis.
SP  - 7446
EP  - 7456
JF  - 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/cvpr42600.2020.00747
ER  - 

TY  - JOUR
AU  - Lehmann, Florian; Buschek, Daniel
TI  - Examining Autocompletion as a Basic Concept for Interaction with Generative AI
PY  - 2020
AB  - <jats:title>Abstract</jats:title> <jats:p>Autocompletion is an approach that extends and continues partial user input. We propose to interpret autocompletion as a basic interaction concept in human-AI interaction. We first describe the concept of autocompletion and dissect its user interface and interaction elements, using the well-established textual autocompletion in search engines as an example. We then highlight how these elements reoccur in other application domains, such as code completion, GUI sketching, and layouting. This comparison and transfer highlights an inherent role of such intelligent systems to extend and complete user input, in particular useful for designing interactions with and for generative AI. We reflect on and discuss our conceptual analysis of autocompletion to provide inspiration and a conceptual lens on current challenges in designing for human-AI interaction.</jats:p>
SP  - 251
EP  - 264
JF  - i-com
VL  - 19
IS  - 3
PB  - 
DO  - 10.1515/icom-2020-0025
ER  - 

TY  - JOUR
AU  - Katanbaf, Mohamad; Jain, Vivek; Smith, Joshua R.
TI  - Relacks: Reliable Backscatter Communication in Indoor Environments
PY  - 2020
AB  - The increasing number of embedded, plugged-in radios around homes and offices in everyday objects such as appliances, TVs, and smart speakers provides an excellent opportunity to bring ultra-low-power backscatter connectivity to billions of devices. However, backscatter links suffer from high loss due to two consecutive propagations and have to operate on narrow link-budget margins, which makes them more susceptible to multipath losses in indoor environments. To address this, we propose a closed-loop backscatter system that exploits diversity sources such as communication frequency and transceivers antennas based on the channel metrics to deliver reliable coverage over an entire area. We prototype a backscatter system with Bluetooth Low Energy (BLE) transceivers and BLE compatible tags and deploy it in several multipath rich indoor environments. Our evaluations show that we can successfully communicate with a backscatter tag in a 50m2 indoor area. The proposed algorithm for selecting communication parameters achieves an average 2.7 x success rate compared to the random selection while satisfying FCC output power requirements for frequency hopping transceivers.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 2
PB  - 
DO  - 10.1145/3397314
ER  - 

TY  - NA
AU  - zheng, yawen; Huang, Jin; Liu, Juan; Yang, Chenglei; Tian, Feng
TI  - MobileHCI - A Scenario Adaptive Model for Predicting Error Rates in Moving Target Selection on Smartphones
PY  - 2021
AB  - Modeling error rates in moving target selection is critical in guiding the design and improving user performance in the user interface with dynamic contents. Despite the high accuracy of existing models in predicting error rates for various inputs, they need to fit a large number of samples for a specific interaction scenario. This paper presents a scenario adaptive model that can quickly learn the characterizes of endpoints from small data in a specific scenario, and then accurately predict the error rates in this scenario. We report on two studies evaluating the adaptability of the model in specific users (children and the older user) and pointing postures (walking and one-handed) with a smartphone. Significant improvements in error rates prediction of our model were observed compared to the previous non-adaptive one. We conclude with the findings from our results for insights into future interface design.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472049
ER  - 

TY  - NA
AU  - Fang, Likun; Zhu, Ting; Pescara, Erik; Huang, Yiran; Zhou, Yexu; Beigl, Michael
TI  - DragTapVib: An On-Skin Electromagnetic Drag, Tap, and Vibration Actuator for Wearable Computing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519395
ER  - 

TY  - JOUR
AU  - Mohammed, Noor; Wang, Rui; Jackson, R.W.; Noh, Yeonsik; Gummeson, Jeremy; Lee, Sunghoon Ivan
TI  - ShaZam: Charge-Free Wearable Devices via Intra-Body Power Transfer from Everyday Objects
PY  - 2021
AB  - In this work, we investigate a wireless power transfer technology that can unobtrusively charge wearable devices while users interact with everyday objects, such as an office desk, laptop, or car. We design and develop our solution, ShaZam, that exploits the human body as a medium to transfer Radio Frequency (RF) energy-carrier signals from minimally-instrumented daily objects to wearable devices. We focus on establishing the technical groundwork of the proposed technology by incorporating the capacitive coupling mechanism, in which the forward signal path is established through the human body, and the return path is established via capacitive coupling to the surrounding environment. To showcase the feasibility of our technology, we investigate three different use scenarios---i.e., interacting with a keyboard on a desk, a laptop, and the steering wheel of a car---to transfer power to a wrist-worn device. Using data obtained from ten healthy individuals within a setting where uncontrolled electromagnetic interference was relatively low, we demonstrate that we can transfer approximately 0.5 mW - 1 mW of DC power to the wrist-worn device. We also investigate several critical environmental and design parameters that could affect the power transfer and offer design guidelines that optimize performance. Our initial results suggest the potential for a new design paradigm towards completely charge-free wearable devices.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 2
PB  - 
DO  - 10.1145/3463505
ER  - 

TY  - JOUR
AU  - Röddiger, Tobias; Clarke, Christopher; Breitling, Paula; Schneegans, Tim; Zhao, Haibin; Gellersen, Hans; Beigl, Michael
TI  - Sensing with Earables
PY  - 2022
AB  - <jats:p>Earables have emerged as a unique platform for ubiquitous computing by augmenting ear-worn devices with state-of-the-art sensing. This new platform has spurred a wealth of new research exploring what can be detected on a wearable, small form factor. As a sensing platform, the ears are less susceptible to motion artifacts and are located in close proximity to a number of important anatomical structures including the brain, blood vessels, and facial muscles which reveal a wealth of information. They can be easily reached by the hands and the ear canal itself is affected by mouth, face, and head movements. We have conducted a systematic literature review of 271 earable publications from the ACM and IEEE libraries. These were synthesized into an open-ended taxonomy of 47 different phenomena that can be sensed in, on, or around the ear. Through analysis, we identify 13 fundamental phenomena from which all other phenomena can be derived, and discuss the different sensors and sensing principles used to detect them. We comprehensively review the phenomena in four main areas of (i) physiological monitoring and health, (ii) movement and activity, (iii) interaction, and (iv) authentication and identification. This breadth highlights the potential that earables have to offer as a ubiquitous, general-purpose platform.</jats:p>
SP  - 1
EP  - 57
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550314
ER  - 

TY  - NA
AU  - Stemasov, Evgeny; Wagner, Tobias; Gugenheimer, Jan; Rukzio, Enrico
TI  - CHI - Mix&Match: Towards Omitting Modelling Through In-situ Remixing of Model Repository Artifacts in Mixed Reality
PY  - 2020
AB  - The accessibility of tools to model artifacts is one of the core driving factors for the adoption of Personal Fabrication. Subsequently, model repositories like Thingiverse became important tools in (novice) makers' processes. They allow them to shorten or even omit the design process, offloading a majority of the effort to other parties. However, steps like measurement of surrounding constraints (e.g., clearance) which exist only inside the users' environment, can not be similarly outsourced. We propose Mix&Match a mixed-reality-based system which allows users to browse model repositories, preview the models in-situ, and adapt them to their environment in a simple and immediate fashion. Mix&Match aims to provide users with CSG operations which can be based on both virtual and real geometry. We present interaction patterns and scenarios for Mix&Match, arguing for the combination of mixed reality and model repositories. This enables almost modelling-free personal fabrication for both novices and expert makers.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376839
ER  - 

TY  - JOUR
AU  - Feger, Sebastian S.; Semmler, Lars; Schmidt, Albrecht; Kosch, Thomas
TI  - ElectronicsAR: Design and Evaluation of a Mobile and Tangible High-Fidelity Augmented Electronics Toolkit
PY  - 2022
AB  - <jats:p>Exploring and interacting with electronics is challenging as the internal processes of components are not visible. Further barriers to engagement with electronics include fear of injury and hardware damage. In response, Augmented Reality (AR) applications address those challenges to make internal processes and the functionality of circuits visible. However, current apps are either limited to abstract low-fidelity applications or entirely virtual environments. We present ElectronicsAR, a tangible high-fidelity AR electronics kit with scaled hardware components representing the shape of real electronics. Our evaluation with 24 participants showed that users were more efficient and more effective at naming components, as well as building and debugging circuits. We discuss our findings in the context of ElectronicsAR's unique characteristics that we contrast with related work. Based on this, we discuss opportunities for future research to design functional mobile AR applications that meet the needs of beginners and experts.</jats:p>
SP  - 700
EP  - 721
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567740
ER  - 

TY  - NA
AU  - Kuo, Tzu-Sheng; Rawn, Eric
TI  - UIST (Adjunct Volume) - Let It Rip! Using Velcro for Acoustic Labeling
PY  - 2020
AB  - We present an early stage prototype of an acoustic labeling system using Velcro, a two-sided household adhesive product. We create labels by varying the shape of Velcro pieces to produce distinct sounds when the two sides are separated, and we use an automatic audio classification pipeline to detect and classify small sets of labels. We evaluate our classifier on four sets of three simple Velcro labels, present a demo highlighting potential use cases of these labels, and discuss future applications.
SP  - 28
EP  - 30
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416175
ER  - 

TY  - JOUR
AU  - Badogu, Ketan; Kumar, Raman; Kumar, Ranvijay
TI  - 3D Printing of Glass Fiber-Reinforced Polymeric Composites: A Review
PY  - 2022
AB  - NA
SP  - 1285
EP  - 1301
JF  - Journal of The Institution of Engineers (India): Series C
VL  - 103
IS  - 5
PB  - 
DO  - 10.1007/s40032-022-00873-1
ER  - 

TY  - JOUR
AU  - Yu, Kevin; Gorbachev, Gleb; Eck, Ulrich; Pankratz, Frieder; Navab, Nassir; Roth, Daniel
TI  - Avatars for Teleconsultation: Effects of Avatar Embodiment Techniques on User Perception in 3D Asymmetric Telepresence
PY  - 2021
AB  - A 3D Telepresence system allows users to interact with each other in a virtual, mixed, or augmented reality (VR, MR, AR) environment, creating a shared space for collaboration and communication. There are two main methods for representing users within these 3D environments. Users can be represented either as point cloud reconstruction-based avatars that resemble a physical user or as virtual character-based avatars controlled by tracking the users' body motion. This work compares both techniques to identify the differences between user representations and their fit in the reconstructed environments regarding the perceived presence, uncanny valley factors, and behavior impression. Our study uses an asymmetric VR/AR teleconsultation system that allows a remote user to join a local scene using VR. The local user observes the remote user with an AR head-mounted display, leading to facial occlusions in the 3D reconstruction. Participants perform a warm-up interaction task followed by a goal-directed collaborative puzzle task, pursuing a common goal. The local user was represented either as a point cloud reconstruction or as a virtual character-based avatar, in which case the point cloud reconstruction of the local user was masked. Our results show that the point cloud reconstruction-based avatar was superior to the virtual character avatar regarding perceived co-presence, social presence, behavioral impression, and humanness. Further, we found that the task type partly affected the perception. The point cloud reconstruction-based approach led to higher usability ratings, while objective performance measures showed no significant difference. We conclude that despite partly missing facial information, the point cloud-based reconstruction resulted in better conveyance of the user behavior and a more coherent fit into the simulation context.
SP  - 4129
EP  - 4139
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2021.3106480
ER  - 

TY  - JOUR
AU  - Zhao, Maozheng; Cui, Wenzhe; Ramakrishnan, I. V.; Zhai, Shumin; Bi, Xiaojun
TI  - UIST - Voice and Touch Based Error-tolerant Multimodal Text Editing and Correction for Smartphones
PY  - 2021
AB  - Editing operations such as cut, copy, paste, and correcting errors in typed text are often tedious and challenging to perform on smartphones. In this paper, we present VT, a voice and touch-based multi-modal text editing and correction method for smartphones. To edit text with VT, the user glides over a text fragment with a finger and dictates a command, such as ”bold” to change the format of the fragment, or the user can tap inside a text area and speak a command such as ”highlight this paragraph” to edit the text. For text correcting, the user taps approximately at the area of erroneous text fragment and dictates the new content for substitution or insertion. VT combines touch and voice inputs with language context such as language model and phrase similarity to infer a user’s editing intention, which can handle ambiguities and noisy input signals. It is a great advantage over the existing error correction methods (e.g., iOS’s Voice Control) which require precise cursor control or text selection. Our evaluation shows that VT significantly improves the efficiency of text editing and text correcting on smartphones over the touch-only method and the iOS’s Voice Control method. Our user studies showed that VT reduced the text editing time by 30.80%, and text correcting time by 29.97% over the touch-only method. VT reduced the text editing time by 30.81%, and text correcting time by 47.96% over the iOS’s Voice Control method.
SP  - 162
EP  - 178
JF  - Proceedings of the ACM Symposium on User Interface Software and Technology. ACM Symposium on User Interface Software and Technology
VL  - 2021
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474742
ER  - 

TY  - NA
AU  - Buschek, Daniel; Zürn, Martin; Eiband, Malin
TI  - The Impact of Multiple Parallel Phrase Suggestions on Email Input and Composition Behaviour of Native and Non-Native English Writers
PY  - 2021
AB  - We present an in-depth analysis of the impact of multi-word suggestion choices from a neural language model on user behaviour regarding input and text composition in email writing. Our study for the first time compares different numbers of parallel suggestions, and use by native and non-native English writers, to explore a trade-off of "efficiency vs ideation", emerging from recent literature. We built a text editor prototype with a neural language model (GPT-2), refined in a prestudy with 30 people. In an online study (N=156), people composed emails in four conditions (0/1/3/6 parallel suggestions). Our results reveal (1) benefits for ideation, and costs for efficiency, when suggesting multiple phrases; (2) that non-native speakers benefit more from more suggestions; and (3) further insights into behaviour patterns. We discuss implications for research, the design of interactive suggestion systems, and the vision of supporting writers with AI instead of replacing them.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445372
ER  - 

TY  - NA
AU  - Mu, Mu; Dohan, Murtada; Goodyear, Alison; Hill, Gary; Johns, Cleyon; Mauthe, Andreas
TI  - User Attention and Behaviour in Virtual Reality Art Encounter.
PY  - 2020
AB  - With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators have started to experiment with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between behavioural data and audience background. New integrated methods to visualise user attention as part of the artwork are also developed as a feedback loop to the content creator.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Toro, Usman Saleh; ElHalawany, Basem M.; Wong, Aslan B.; Wang, Lu; Wu, Kaishun
TI  - Backscatter communication-based wireless sensing (BBWS): Performance enhancement and future applications
PY  - 2022
AB  - Wireless Sensing (WS) provides a low-cost means of monitoring humans and objects for the next generation of IoT. However, the deployment of WS for various applications is limited by; the heterogeneous nature of underlying technology in IoT devices, limited battery, and computational power of sensing devices. Backscatter Communication (BackCom) enables wireless sensing and communication by reflecting wireless signals (such as radio frequency (RF), acoustic and visible light (VL)) from a source to a receiver using a backscatter tag at an ultra-low (microwatt) power level. Hence, backscatter communication-based wireless sensing (BBWS) could enable ubiquitously and battery-free applications (such as human and plant physiology, orientation sensing, and localization). Despite existing surveys and literature on BackCom, a study on leveraging BackCom for WS is still lacking. This paper takes the first approach in discussing applications of BackCom in WS and techniques for enhancing performance (such as power management, channel model, range and coverage, throughput, security, quality of backscatter signal, modulation, and coding) in BBWS applications. It also discusses future sensing applications where BBWS is applicable and research issues related to such applications.
SP  - 103518
EP  - 103518
JF  - Journal of Network and Computer Applications
VL  - 208
IS  - NA
PB  - 
DO  - 10.1016/j.jnca.2022.103518
ER  - 

TY  - NA
AU  - Lin, Ying-Ju; Punpongsanon, Parinya; Wen, Xin; Iwai, Daisuke; Sato, Kosuke; Obrist, Marianna; Mueller, Stefanie
TI  - CHI - FoodFab: Creating Food Perception Illusions using Food 3D Printing
PY  - 2020
AB  - Personalization of eating such that everyone consumes only what they need allows improving our management of food waste. In this paper, we explore the use of food 3D printing to create perceptual illusions for controlling the level of perceived satiety given a defined amount of calories. We present FoodFab, a system that allows users to control their food intake through modifying a food's internal structure via two 3D printing parameters: infill pattern and infill density. In two experiments with a total of 30 participants, we studied the effect of these parameters on users' chewing time that is known to affect people's feeling of satiety. Our results show that we can indeed modify the chewing time by varying infill pattern and density, and thus control perceived satiety. Based on the results, we propose two computational models and integrate them into a user interface that simplifies the creation of personalized food structures.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376421
ER  - 

TY  - CONF
AU  - Li, Jingjie; Chowdhury, Amrita Roy; Fawaz, Kassem; Kim, Younghyun
TI  - USENIX Security Symposium - Kalεido: Real-Time Privacy Control for Eye-Tracking Systems.
PY  - 2021
AB  - NA
SP  - 1793
EP  - 1810
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Vatavu, Radu-Daniel; Ungurean, Ovidiu-Ciprian
TI  - Understanding Gesture Input Articulation with Upper-Body Wearables for Users with Upper-Body Motor Impairments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501964
ER  - 

TY  - NA
AU  - Pu, Kevin; Fu, Rainey; Dong, Rui; Wang, Xinyu; Chen, Yan; Grossman, Tovi
TI  - SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545691
ER  - 

TY  - JOUR
AU  - Thanyadit, Santawat; Punpongsanon, Parinya; Piumsomboon, Thammathip; Pong, Ting-Chuen
TI  - XR-LIVE: Enhancing Asynchronous Shared-Space Demonstrations with Spatial-temporal Assistive Toolsets for Effective Learning in Immersive Virtual Laboratories
PY  - 2022
AB  - <jats:p>An immersive virtual laboratory (VL) could offer flexibility of time and space, as well as safety, for remote students to conduct laboratory activities through online experiential learning. Recording an instructor's demonstration inside a VL is an approach that allows students to learn directly from a demonstration. However, students have to learn from a recording while controlling the playback, which requires attention spent on additional spatial and temporal cues. This additional cognitive load could lead to errors during the laboratory procedure. To address these challenges, we have identified four design requirements to reduce attention load in VLs; namely, organized learning steps, improved student sense of co-presence, reduction of task-instructor split-attention, and learning independent of interpersonal distance. Based on these requirements, we have designed and implemented spatial-temporal assistive toolsets for laboratories in a virtual environment, namely XR-LIVE, to reduce cognitive load and enhance learning in an asynchronous shared-space demonstration, implemented based on the setup of a standard civil engineering laboratory. We also analyzed students' behavior in the VL demonstration to design guidelines applicable to generic VLs.</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512983
ER  - 

TY  - NA
AU  - Thoravi Kumaravel, Balasaravanan; Wilson, Andrew D
TI  - DreamStream: Immersive and Interactive Spectating in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517508
ER  - 

TY  - NA
AU  - Scheirer, Cassandra; Harrison, Chris
TI  - DynaTags: Low-Cost Fiducial Marker Mechanisms
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3536221.3556591
ER  - 

TY  - JOUR
AU  - Li, Tianshi; Quinn, Philip; Zhai, Shumin
TI  - C-PAK: Correcting and Completing Variable-length Prefix-based Abbreviated Keystrokes
PY  - 2022
AB  - <jats:p> Improving keystroke savings is a long-term goal of text input research. We present a study into the design space of an abbreviated style of text input called <jats:italic>C-PAK</jats:italic> (Correcting and completing variable-length Prefix-based Abbreviated Keystrokes) for text entry on mobile devices. Given a variable length and potentially inaccurate input string (e.g. “li g t m”), C-PAK aims to expand it into a complete phrase (e.g. “looks good to me”). We develop a C-PAK prototype keyboard, <jats:italic>PhraseWriter</jats:italic> , based on a current state-of-the-art mobile keyboard consisting of 1.3 million n-grams and 164,000 words. Using computational simulations on a large dataset of realistic input text, we found that, in comparison to conventional single-word suggestions, PhraseWriter improves the maximum keystroke savings rate by 6.7% (from <jats:inline-formula content-type="math/tex"> <jats:tex-math notation="TeX" version="MathJaX">\(46.3\% \)</jats:tex-math> </jats:inline-formula> to <jats:inline-formula content-type="math/tex"> <jats:tex-math notation="TeX" version="MathJaX">\(49.4\% \)</jats:tex-math> </jats:inline-formula> ), reduces the word error rate by 14.7%, and is particularly advantageous for common phrases. We conducted a lab study of novice user behavior and performance which found that users could quickly utilize the C-PAK style abbreviations implemented in PhraseWriter, achieving a higher keystroke savings rate than forward suggestions (25% vs. 16%). Furthermore, they intuitively and successfully abbreviated more with common phrases. However, users had a lower overall text entry rate due to their limited experience with the system (28.5 words per minute vs. 37.7). We outline future technical directions to improve C-PAK over the PhraseWriter baseline, and further opportunities to study the perceptual, cognitive, and physical action trade-offs that underlie the learning curve of C-PAK systems. </jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3544101
ER  - 

TY  - NA
AU  - Rekimoto, Jun
TI  - DualVoice: Speech Interaction that Discriminates between Normal and Whispered Voice Input
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545685
ER  - 

TY  - JOUR
AU  - Wang, Peng; Bai, Xiaoliang; Billinghurst, Mark; Zhang, Shusheng; Wei, Sili; Xu, Guangyao; He, Weiping; Zhang, Xiangyu; Zhang, Jie
TI  - 3DGAM: using 3D gesture and CAD models for training on mixed reality remote collaboration
PY  - 2020
AB  - As Virtual Reality(VR), Augmented Reality(AR), Mixed Reality(MR) technology becomes more accessible, it is important to explore VR/AR/MR technologies that can be used for remote collaboration on physical tasks. Previous research has shown that gesture-based interaction is intuitive and expressive for remote collaboration, and using 3D CAD models can provide clear instructions for assembly tasks. In this paper, therefore, we describe a new MR remote collaboration system which combines the use of gesture and CAD models in a complementary manner. The prototype system enables a remote expert in VR to provide instructions based on 3D gesture and CAD models (3DGAM) for a local worker who uses AR to see these instructions. Using this interface, we conducted a formal user study to explore the effect of sharing 3D gesture and CAD models in an assembly training task. We found that the combination of 3D gesture and CAD models can improve remote collaboration on an assembly task with respect to the performance time and user experience. Finally, we provide some conclusions and directions for future research.
SP  - 31059
EP  - 31084
JF  - Multimedia Tools and Applications
VL  - 80
IS  - 20
PB  - 
DO  - 10.1007/s11042-020-09731-7
ER  - 

TY  - NA
AU  - Hanton, Ollie; Shen, Zichao; Fraser, Mike; Roudaut, Anne
TI  - FabricatINK: Personal Fabrication of Bespoke Displays Using Electronic Ink from Upcycled E Readers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501844
ER  - 

TY  - NA
AU  - Qin, Yue; Yu, Chun; Li, Zhaoheng; Zhong, Mingyuan; Yan, Yukang; Shi, Yuanchun
TI  - CHI - ProxiMic: Convenient Voice Activation via Close-to-Mic Speech Detected by a Single Microphone
PY  - 2021
AB  - Wake-up-free techniques (e.g., Raise-to-Speak) are important for improving the voice input experience. We present ProxiMic, a close-to-mic (within 5 cm) speech sensing technique using only one microphone. With ProxiMic, a user keeps a microphone-embedded device close to the mouth and speaks directly to the device without wake-up phrases or button presses. To detect close-to-mic speech, we use the feature from pop noise observed when a user speaks and blows air onto the microphone. Sound input is first passed through a low-pass adaptive threshold filter, then analyzed by a CNN which detects subtle close-to-mic features (mainly pop noise). Our two-stage algorithm can achieve 94.1% activation recall, 12.3 False Accepts per Week per User (FAWU) with 68 KB memory size, which can run at 352 fps on the smartphone. The user study shows that ProxiMic is efficient, user-friendly, and practical.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445687
ER  - 

TY  - NA
AU  - Jiang, Yuming; Huang, Ziqi; Pan, Xingang; Loy, Chen Change; Liu, Ziwei
TI  - Talk-to-Edit: Fine-Grained Facial Editing via Dialog
PY  - 2021
AB  - Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual "semantic field" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field. We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants. Our project page is this https URL.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Hu, Jinghui; Dudley, John J.; Kristensson, Per Ola
TI  - An Evaluation of Caret Navigation Methods for Text Editing in Augmented Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar-adjunct57072.2022.00132
ER  - 

TY  - NA
AU  - Batra, Ritik; Lee, Kaitlyn Yi Ran
TI  - Redycler: Daily Outfit Texture Fabrication Appliance Using Re-Programmable Dyes
PY  - 2022
AB  - We present a speculative design for a novel appliance for future fabrication in the home to revitalize textiles using re-programmable multi-color textures. Utilizing colored photochromic dyes activated by ultraviolet (UV) light, we can selectively deactivate hues using complementary colors in visible light to result in the final desired dye pattern. Our proposed appliance would automate this process within a box placed in the bedroom. We envision a future where people are able to transform old apparel into unique and fashionable pieces of clothing. We discuss how the user would interact with the appliance and how this device elongates the life-cycle of clothing through modification. We also outline the central issues to integrate such a concept into the home. Finally, we analyze how this device fits into personal modification trends in HCI to show how this device could change existing conceptions around sustainable fashion and personal style.
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3502424
ER  - 

TY  - CHAP
AU  - Tanaka, Satoshi; Ushiyama, Keigo; Takahashi, Akifumi; Kajimoto, Hiroyuki
TI  - EuroHaptics - Movement-Free Virtual Reality Interface Using Kinesthetic Illusion Induced by Tendon Vibration
PY  - 2020
AB  - In current virtual reality (VR) systems, the physical movement of the body is required, which creates problems of safety, cost, and accessibility. To solve those problems, we propose a system that fixes a user’s body, detects force when a user tries to move, and generates the sensation of movement using kinesthetic illusion caused by tendon vibration. We implemented a system limited to simple motion, and conducted an experiment to evaluate operability, body ownership, and agency. Although we could not statistically verify the effect of kinesthetic illusion, the results suggested that it may be possible that kinesthetic illusion could increase ownership and decrease agency.
SP  - 316
EP  - 324
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58147-3_35
ER  - 

TY  - NA
AU  - Liu, Jingyuan; Fu, Hongbo; Tai, Chiew-Lan
TI  - UIST - PoseTween: Pose-driven Tween Animation
PY  - 2020
AB  - Augmenting human action videos with visual effects often requires professional tools and skills. To make this more accessible by novice users, existing attempts have focused on automatically adding visual effects to faces and hands, or let virtual objects strictly track certain body parts, resulting in rigid-looking effects. We present PoseTween, an interactive system that allows novice users to easily add vivid virtual objects with their movement interacting with a moving subject in an input video. Our key idea is to leverage the motion of the subject to create pose-driven tween animations of virtual objects. With our tool, a user only needs to edit the properties of a virtual object with respect to the subject's movement at keyframes, and the object is associated with certain body parts automatically. The properties of the object at intermediate frames are then determined by both the body movement and the interpolated object keyframe properties, producing natural object movements and interactions with the subject. We design a user interface to facilitate editing of keyframes and previewing animation results. Our user study shows that PoseTween significantly requires less editing time and fewer keyframes than using the traditional tween animation in making pose-driven tween animations for novice users.
SP  - 791
EP  - 804
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415822
ER  - 

TY  - NA
AU  - Tian, Yingtao; Ha, David
TI  - Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts
PY  - 2021
AB  - Evolutionary algorithms have been used in the digital art scene since the 1970s. A popular application of genetic algorithms is to optimize the procedural placement of vector graphic primitives to resemble a given painting. In recent years, deep learning-based approaches have also been proposed to generate procedural drawings, which can be optimized using gradient descent. In this work, we revisit the use of evolutionary algorithms for computational creativity. We find that modern evolution strategies (ES) algorithms, when tasked with the placement of shapes, offer large improvements in both quality and efficiency compared to traditional genetic algorithms, and even comparable to gradient-based methods. We demonstrate that ES is also well suited at optimizing the placement of shapes to fit the CLIP model, and can produce diverse, distinct geometric abstractions that are aligned with human interpretation of language. Videos and demo: this https URL
SP  - NA
EP  - NA
JF  - arXiv: Neural and Evolutionary Computing
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Sun, Wei; Srinivasan, Kannan
TI  - On the Feasibility of Securing Vehicle-Pavement Interaction
PY  - 2022
AB  - <jats:p>Road surface information (e.g., smooth road or bumpy road with potholes or bumps) is important for safe driving (i.e., it's necessary to be aware of the road surface conditions during driving). However, the high-cost sensor (e.g., LiDAR and camera) based road surface sensing approaches cannot work properly in inclement weather conditions (e.g., fogging and snowing) due to the line-of-sight requirement. The low-cost and ubiquitous smartphone-based road surface sensing approach is not reliable and safe to use, since it relies on the vibration of the vehicle body to sense the road surface (i.e., the vehicle's tires need to touch the bumps on the road surface). Can we automate the contact-free road surface sensing with low-cost sensors for safe driving without requiring the vehicle's tires to touch the bumps on the road surface?</jats:p> <jats:p>In this paper, we propose Tago, a system that can achieve contact-free road surface sensing with commodity passive RFID tags. Instead of deploying RFID tags or readers along the road or lamp post (i.e., infrastructure-based deployment), we deploy the reader inside of the vehicle and attach the tag and the reader's antenna at the front end of the vehicle like the vehicle's headlights (i.e., infrastructure-free deployment). However, there is a great challenge to obtain the clean reflection from the road surface, since the reflection may be drown in the backscattered signals due to multipath effect. Moreover, it is not reliable to use the composite signals received at the reader to sense the road surface conditions. Therefore, we first comprehensively analyse the variation of composite signals received at the reader. Then, we propose a signal cancellation approach to extract the clean reflections from the road surface, such that we can accurately sense the road surface conditions for safe driving. Our experiments with different vehicles (e.g., Honda Civic Frankenfish, Folsom, Flutter and CR-V Warner) driven on different roadways (e.g., urban and residential area) show that Tago can effectively sense the road surface information.</jats:p>
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 1
PB  - 
DO  - 10.1145/3517230
ER  - 

TY  - NA
AU  - Chowdhury, Tahiya
TI  - IPSN - Towards Reducing Labeling Efforts in IoT-based Machine Learning Systems: PhD Forum Abstract
PY  - 2021
AB  - The number of Internet-of-Things (IoT) and edge devices has exploded in recent years. Coupled with recent advances in learning methodologies, these can make the vision of smart building a reality and transform how people interact with their environment. Deep learning-driven systems have been already successful in many IoT application areas of pervasive sensing and ubiquitous computing including human activity monitoring and occupancy detection. However, the robustness of such systems at scale is dependent on the availability of a large amount of data labeled by a human expert. Acquiring human annotation involves high effort, high cost, and is often error-prone. Therefore, learning to execute tasks in a machine learning system without manual inspection is of prime interest as it can open up opportunities for continual learning in the long-term deployment of IoT systems. In this work, we propose deep neural network-based approaches for segmentation and classification tasks in sensor streams with no or limited labels. We focus on reducing human labeling efforts in machine learning systems by 1) leveraging temporal features learned through deep neural networks for downstream tasks and 2) learning robust features in dynamic environments by utilizing multiple sensing sources and semi-supervised learning (e.g. small amount of labeled data).
SP  - 416
EP  - 417
JF  - Proceedings of the 20th International Conference on Information Processing in Sensor Networks (co-located with CPS-IoT Week 2021)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3412382.3459211
ER  - 

TY  - NA
AU  - Tseng, Wen-Jie; Lee, Yi-Chen; Peiris, Roshan Lalintha; Chan, Liwei
TI  - CHI - A Skin-Stroke Display on the Eye-Ring Through Head-Mounted Displays
PY  - 2020
AB  - We present the Skin-Stroke Display, a system mounted on the lens inside the head-mounted display, which exerts subtle yet recognizable tactile feedback on the eye-ring using a motorized air jet. To inform our design of noticeable air-jet haptic feedback, we conducted a user study to identify absolute detection thresholds. Our results show that the tactile sensation had different sensitivity around the eyes, and we determined a standard intensity (8 mbar) to prevent turbulent airflow blowing into the eyes. In the second study, we asked participants to adjust the intensity around the eye for equal sensation based on standard intensity. Next, we investigated the recognition of point and stroke stimuli with or without inducing cognitive load on eight directions on the eye-ring. Our longStroke stimulus can achieve an accuracy of 82.6% without cognitive load and 80.6% with cognitive load simulated by the Stroop test. Finally, we demonstrate example applications using the skin-stroke display as the off-screen indicator, tactile I/O progress display, and tactile display.
SP  - 3376700
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376700
ER  - 

TY  - NA
AU  - Twigg-Smith, Hannah; Peek, Nadya
TI  - Demonstrating Dynamic Toolchains for Machine Control
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558662
ER  - 

TY  - NA
AU  - Li, Nianlong; Han, Teng; Tian, Feng; Huang, Jin; Sun, Minghui; Irani, Pourang; Alexander, Jason
TI  - CHI - Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen
PY  - 2020
AB  - The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion---we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376698
ER  - 

TY  - NA
AU  - Rodrigues, André; Santos, André R.B.; Montague, Kyle; Nicolau, Hugo; Guerreiro, Tiago
TI  - WildKey: A Privacy-Aware Keyboard Toolkit for Data Collection In-The-Wild
PY  - 2021
AB  - Touch data, and in particular text-entry data, has been mostly collected in the laboratory, under controlled conditions. While touch and text-entry data have consistently shown its potential for monitoring and detecting a variety of conditions and impairments, its deployment in-the-wild remains a challenge. In this paper, we present WildKey, an Android keyboard toolkit that allows for the usable deployment of in-the-wild user studies. WildKey is able to analyze text-entry behaviors through implicit and explicit text-entry data collection while ensuring user privacy. We detail each of the WildKey's components and features, all of the metrics collected, and discuss the steps taken to ensure user privacy and promote compliance.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Cui, Xiang; Zhang, Ji; Zhou, Hao; Deng, Chang
TI  - BigCom - PowerPool: Multi-source Ambient Energy harvesting
PY  - 2020
AB  - Energy has become the bottleneck issue restricting the development of IoT. Ambient energy harvesting is a promising mechanism to resolve the contradiction between continuous perception and energy costs of IoT devices. Multi-source Ambient energy harvesting could further increase the availability and stability of energy supply. However, when multiple energy sources work together, it is a challenge to address issues as current backflow and low efficiency due to the difference in energy level.We establish the energy pool model to achieve multi-source energy harvesting fusion. We build an unified access interface for different energy sources, where MOSTEF is used to replace the diode for achieving unidirectional conduction and preventing current backflow. We implement the prototype and evaluate it with different energy sources such as wind, light, radio frequency and piezoelectric energy. The experiment results suggest that energy transmission loss of the interface is less than one percent of total energy.
SP  - 86
EP  - 90
JF  - 2020 6th International Conference on Big Data Computing and Communications (BIGCOM)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/bigcom51056.2020.00019
ER  - 

TY  - JOUR
AU  - Messerschmidt, Moritz Alexander; Muthukumarana, Sachith; Hamdan, Nur Al-Huda; Wagner, Adrian; Zhang, Haimo; Borchers, Jan; Nanayakkara, Suranga Chandima
TI  - ANISMA: A Prototyping Toolkit to Explore Haptic Skin Deformation Applications Using Shape-Memory Alloys
PY  - 2022
AB  - <jats:p>We present ANISMA, a software and hardware toolkit to prototype on-skin haptic devices that generate skin deformation stimuli like pressure, stretch, and motion using shape-memory alloys (SMAs). Our toolkit embeds expert knowledge that makes SMA spring actuators more accessible to human–computer interaction (HCI) researchers. Using our software tool, users can design different actuator layouts, program their spatio-temporal actuation and preview the resulting deformation behavior to verify a design at an early stage. Our toolkit allows exporting the actuator layout and 3D printing it directly on skin adhesive. To test different actuation sequences on the skin, a user can connect the SMA actuators to our customized driver board and reprogram them using our visual programming interface. We report a technical analysis, verify the perceptibility of essential ANISMA skin deformation devices with 8 participants, and evaluate ANISMA regarding its usability and supported creativity with 12 HCI researchers in a creative design task.</jats:p>
SP  - 1
EP  - 34
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 3
PB  - 
DO  - 10.1145/3490497
ER  - 

TY  - NA
AU  - Jiang, Yuming; Huang, Ziqi; Pan, Xingang; Loy, Chen Change; Liu, Ziwei
TI  - Talk-to-Edit: Fine-Grained Facial Editing via Dialog
PY  - 2021
AB  - Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual "semantic field" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field. We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.
SP  - NA
EP  - NA
JF  - 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iccv48922.2021.01354
ER  - 

TY  - NA
AU  - Park, Eunji; Lee, Byungjoo
TI  - Predicting Error Rates in Pointing Regardless of Target Motion
PY  - 2018
AB  - In a pointing task with time constraints, it was only possible to predict the user's error rate when pointing to a stationary target. This study presents a novel model for predicting pointing error rates regardless of the target motion. The model assumes that in the last submovement of the pointing trajectory just before the click, the timing to activate the button is anticipated by the user's internal clock decoding the temporal cues present in the relative movement between the cursor and the target. Then, based on the recent theory of temporal pointing, the model can predict the user's pointing error rate with a high R2 for both stationary (0.993) and moving targets (0.986) by analyzing the kinematic characteristics of the last submovement. In addition, empirical parameters obtained from the model fit succeeded in revealing differences in the cognitive characteristics of experts and novices in first-person shooter games.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Park, Keunwoo; Lempert, Conrad; Abdullah, Muhammad; Katakura, Shohei; Shigeyama, Jotaro; Roumen, Thijs; Baudisch, Patrick
TI  - FoolProofJoint: Reducing Assembly Errors of Laser Cut 3D Models by Means of Custom Joint Patterns
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501919
ER  - 

TY  - NA
AU  - Di Laura Chen, NA; Balakrishnan, Ravin; Grossman, Tovi
TI  - VR - Disambiguation Techniques for Freehand Object Manipulations in Virtual Reality
PY  - 2020
AB  - Manipulating virtual objects using bare hands has been an attractive interaction paradigm in virtual and augmented reality due to its intuitive nature. However, one limitation of freehand input lies in the ambiguous resulting effect of the interaction. The same gesture performed on a virtual object could invoke different operations on the object depending on the context, object properties, and user intention. We present an experimental analysis of a set of disambiguation techniques in a virtual reality environment, comparing three input modalities (head gaze, speech, and foot tap) paired with three different timings in which options appear to resolve ambiguity (before, during, and after an interaction). The results indicate that using head gaze for disambiguation during an interaction with the object achieved the best performance.
SP  - 285
EP  - 292
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr46266.2020.1581293094740
ER  - 

TY  - JOUR
AU  - Akpınar, Elgi̇n; Yeşi̇lada, Yeli̇z; Karagöz, Pınar
TI  - Effect of Context on Smartphone Users' Typing Performance in the Wild
PY  - 2022
AB  - <jats:p>Smartphones play a crucial role in daily activities, however, situationally-induced impairments and disabilities (SIIDs) can easily be experienced depending on the context. Previous studies explored the effect of context but mainly done in controlled environments with limited research done in the wild. In this article, we present an in-situ remote user study with 48 participants’ keyboard interaction on smartphones including the performance and context details. We first propose an automated approach for error detection by combining approaches introduced in the literature and with a follow-up study, show that the accuracy of error detection is improved. We then investigate the effect of context on the typing performance based on five dimensions: environment, mobility, social, multitasking and distraction, and reveal that the context affects participants’ error rate significantly but with individual differences. Our main contribution is providing empirical evidence with an in-situ study showing the effect of context on error rate.</jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3577013
ER  - 

TY  - NA
AU  - Arora, Rahul
TI  - CHI Extended Abstracts - Creative Expression with Immersive 3D Interactions
PY  - 2020
AB  - Virtual and augmented realities (VR/AR) allow artists to create 3D content in a three-dimensional space  both display and inputs are 3D. Getting rid of 2D proxies such as screens and graphic tablets removes a significant barrier from 3D creation and allows artists to create more intuitively, and potentially more efficiently. However, creating in VR/AR introduces new control, precision, and ergonomic challenges. Designing interactive tools for 3D creation is therefore non-trivial. A deep understanding of human factors, user preferences, as well as biases stemming from users' experience with 2D tools is essential to develop effective creative tools for VR/AR. My research combines exploratory user studies and technical advancements to build novel tools for creating 3D content in immersive spaces. I present two computer graphics applications which utilize 3D interactions to improve existing creative workflows and devise novel ones for visual creative expression in three-dimensions. The first studies concept sketching, while the second explores animation of dynamic physical phenomena. I then describe my ongoing work and planned future work on other creative applications.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3375028
ER  - 

TY  - NA
AU  - González, Rafael Morales; Marzo, Asier; Freeman, Euan; Frier, William; Georgiou, Orestis
TI  - TEI - UltraPower: Powering Tangible & Wearable Devices with Focused Ultrasound
PY  - 2021
AB  - Wireless power transfer creates new opportunities for interaction with tangible and wearable devices, by freeing designers from the constraints of an integrated power source. We explore the use of focused ultrasound as a means of transferring power to a distal device, transforming passive props into dynamic active objects. We analyse the ability to transfer power from an ultrasound array commonly used for mid-air haptic feedback and investigate the practical challenges of ultrasonic power transfer (e.g., receiving and rectifying energy from sound waves). We also explore the ability to power electronic components and multimodal actuators such as lights, speakers and motors. Finally, we describe exemplar wearable and tangible device prototypes that are activated by UltraPower, illustrating the potential applications of this novel technology.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440620
ER  - 

TY  - NA
AU  - Elkin, Lisa A.; Kay, Matthew; Higgins, James J.; Wobbrock, Jacob O.
TI  - An Aligned Rank Transform Procedure for Multifactor Contrast Tests
PY  - 2021
AB  - Data from multifactor HCI experiments often violates the normality assumption of parametric tests (i.e., nonconforming data). The Aligned Rank Transform (ART) is a popular nonparametric analysis technique that can find main and interaction effects in nonconforming data, but leads to incorrect results when used to conduct contrast tests. We created a new algorithm called ART-C for conducting contrasts within the ART paradigm and validated it on 72,000 data sets. Our results indicate that ART-C does not inflate Type I error rates, unlike contrasts based on ART, and that ART-C has more statistical power than a t-test, Mann-Whitney U test, Wilcoxon signed-rank test, and ART. We also extended a tool called ARTool with our ART-C algorithm for both Windows and R. Our validation had some limitations (e.g., only six distribution types, no mixed factorial designs, no random slopes), and data drawn from Cauchy distributions should not be analyzed with ART-C.
SP  - NA
EP  - NA
JF  - arXiv: Methodology
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Balbão, Arthur E.; Walter, Marcelo
TI  - A Biologically Inspired Hair Aging Model
PY  - 2022
AB  - <jats:p>Hair rendering has been a focal point of attention in computer graphics for the last couple of decades. However, there have been few contributions to the modeling and rendering of the natural hair aging phenomenon. We present a new technique that simulates the process of hair graying and hair thinning on digital models due to aging. Given a 3D human head model with hair, we first compute a segmentation of the head using K-means since hair aging occurs at different rates in distinct head parts. Hair graying is simulated according to recent biological knowledge on aging factors for hairs, and hair thinning decreases hair diameters linearly with time. Our system is biologically inspired, supports facial hair, both genders and many ethnicities, and is compatible with different lengths of hair strands. Our real-time results resemble real-life hair aging, accomplished by simulating the stochastic nature of the process and the gradual decrease of melanin.</jats:p>
SP  - 1
EP  - 9
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 6
PB  - 
DO  - 10.1145/3550454.3555444
ER  - 

TY  - NA
AU  - Ko, Hyung-Kwon; An, Subin; Park, Gwanmo; Kim, Seung Kwon; Kim, Daesik; Kim, Bohyoung; Jo, Jaemin; Seo, Jinwook
TI  - We-toon: A Communication Support System between Writers and Artists in Collaborative Webtoon Sketch Revision
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545612
ER  - 

TY  - BOOK
AU  - Somanath, Sowmya; Oehlberg, Lora; Sharlin, Ehud
TI  - FabLearn - Making despite Material Constraints with Augmented Reality-Mediated Prototyping
PY  - 2020
AB  - We present a discussion on designing an Augmented Reality (AR)-based prototyping approach to help makers continue building low-fidelity physical computing projects despite material constraints and demonstrate an example, Polymorphic Cube (PMC). Lack of immediate or easy access to electronics is a roadblock to building physical computing projects. We present AR-mediated prototyping as an approach where mobile AR can be used to simulate missing I/O components in-situ during electronics prototyping. Using our suggested approach makers can build a circuit with available real-world materials, substitute the missing components using any augmented physical proxy, and continue implementation tinkering and interaction with the hybrid circuit. Evaluation of PMC demonstrated that users can leverage computing to overcome the lack of electronic components and build low-fidelity prototypes to support design thinking. Our study revealed the benefits and limitations of our current prototype system and encourages future explorations into an AR-mediated prototyping approach to making.
SP  - 18
EP  - 25
JF  - Proceedings of the FabLearn 2020 - 9th Annual Conference on Maker Education
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3386201.3386206
ER  - 

TY  - NA
AU  - Liang, Rong-Hao; Guo, Zengrong
TI  - CHI - NFCSense: Data-Defined Rich-ID Motion Sensing for Fluent Tangible Interaction Using a Commodity NFC Reader
PY  - 2021
AB  - This paper presents NFCSense, a data-defined rich-ID motion sensing technique for fluent tangible interaction design by using commodity near-field communication (NFC) tags and a single NFC tag reader. An NFC reader can reliably recognize the presence of an NFC tag at a high read rate (∼ 300 reads/s) with low latency, but such high-speed reading has rarely been exploited because the reader may not effectively resolve collisions of multiple tags. Therefore, its human–computer interface applications have been typically limited to a discrete, hands-on interaction style using one tag at a time. In this work, we realized fluent, hands-off, and multi-tag tangible interactions by leveraging gravity and anti-collision physical constraints, which support effortless user input and maximize throughput. Furthermore, our system provides hot-swappable interactivity that enables smooth transitions throughout extended use. Based on the design parameters explored through a series of studies, we present a design space with proof-of-concept implementations in various applications.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445214
ER  - 

TY  - NA
AU  - Degraen, Donald; Fruchard, Bruno; Smolders, Frederik; Potetsianakis, Emmanouil; Güngör, Seref; Krüger, Antonio; Steimle, Jürgen
TI  - UIST - Weirding Haptics: In-Situ Prototyping of Vibrotactile Feedback in Virtual Reality through Vocalization
PY  - 2021
AB  - Effective haptic feedback in virtual reality (VR) is an essential element for creating convincing immersive experiences. To design such feedback, state-of-the-art VR setups provide APIs for programmatically generating controller vibration patterns. While tools for designing vibrotactile feedback keep evolving, they often require expert knowledge and rarely support direct manipulation methods for mapping feedback to user interactions within the VR environment. To address these challenges, we contribute a novel concept called Weirding Haptics, that supports fast-prototyping by leveraging the user’s voice to design such feedback while manipulating virtual objects in-situ. Through a pilot study (N = 9) focusing on how tactile experiences are vocalized during object manipulation, we identify spatio-temporal mappings and supporting features needed to produce intended vocalizations. To study our concept, we built a VR design tool informed by the results of the pilot study. This tool enables users to design tactile experiences using their voice while manipulating objects, provides a set of modifiers for fine-tuning the created experiences in VR, and allows to rapidly compare various experiences by feeling them. Results from a validation study (N = 8) show that novice hapticians can vocalize experiences and refine their designs with the fine-tuning modifiers to match their intentions. We conclude our work by discussing uncovered design implications for direct manipulation and vocalization of vibrotactile feedback in immersive virtual environments.
SP  - 936
EP  - 953
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474797
ER  - 

TY  - JOUR
AU  - Kent, Lee; Snider, Chris; Gopsill, James; Hicks, Ben
TI  - Mixed Reality in design prototyping: A systematic review
PY  - 2021
AB  - NA
SP  - 101046
EP  - NA
JF  - Design Studies
VL  - 77
IS  - NA
PB  - 
DO  - 10.1016/j.destud.2021.101046
ER  - 

TY  - NA
AU  - Qamar, Isabel P. S.; Chen, Sabina W; Tskhovrebadze, Dimitri; Boni, Paolo; Faruqi, Faraz; Wessely, Michael; Mueller, Stefanie
TI  - ChromoPrint: A Multi-Color 3D Printer Based on a Reprogrammable Photochromic Resin
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519784
ER  - 

TY  - NA
AU  - Dogan, Mustafa Doga; Colon, Steven Vidal Acevedo; Sinha, Varnika; Akşit, Kaan; Mueller, Stefanie
TI  - UIST - SensiCut: Material-Aware Laser Cutting Using Speckle Sensing and Deep Learning
PY  - 2021
AB  - Laser cutter users face difficulties distinguishing between visually similar materials. This can lead to problems, such as using the wrong power/speed settings or accidentally cutting hazardous materials. To support users, we present SensiCut, an integrated material sensing platform for laser cutters. SensiCut enables material awareness beyond what users are able to see and reliably differentiates among similar-looking types. It achieves this by detecting materials’ surface structures using speckle sensing and deep learning. SensiCut consists of a compact hardware add-on for laser cutters and a user interface that integrates material sensing into the laser cutting workflow. In addition to improving the traditional workflow and its safety1, SensiCut enables new applications, such as automatically partitioning designs when engraving on multi-material objects or adjusting their geometry based on the kerf of the identified material. We evaluate SensiCut’s accuracy for different types of materials under different sheet orientations and illumination conditions.
SP  - 24
EP  - 38
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474733
ER  - 

TY  - NA
AU  - Zhang, Mingrui Ray; Zhai, Shumin
TI  - CHI - PhraseFlow: Designs and Empirical Studies of Phrase-Level Input
PY  - 2021
AB  - Decoding on phrase-level may afford more correction accuracy than on word-level according to previous research. However, how phrase-level input affects the user typing behavior, and how to design the interaction to make it practical remain under explored. We present PhraseFlow, a phrase-level input keyboard that is able to correct previous text based on the subsequently input sequences. Computational studies show that phrase-level input reduces the error rate of autocorrection by over 16%. We found that phrase-level input introduced extra cognitive load to the user that hindered their performance. Through an iterative design-implement-research process, we optimized the design of PhraseFlow that alleviated the cognitive load. An in-lab study shows that users could adopt PhraseFlow quickly, resulting in 19% fewer error without losing speed. In real-life settings, we conducted a six-day deployment study with 42 participants, showing that 78.6% of the users would like to have the phrase-level input feature in future keyboards.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445166
ER  - 

TY  - NA
AU  - Nazarova, Elena; Sautenkov, Oleg; Altamirano Cabrera, Miguel; Tirado, Jonathan; Serpiva, Valerii; Rakhmatulin, Viktor; Tsetserukou, Dzmitry
TI  - CobotAR: Interaction with Robots using Omnidirectionally Projected Image and DNN-based Gesture Recognition
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/smc52423.2021.9658827
ER  - 

TY  - NA
AU  - Shin, Joongi; Kim, Doheon; So, Chaehan; Saakes, Daniel
TI  - CHI - Body Follows Eye: Unobtrusive Posture Manipulation Through a Dynamic Content Position in Virtual Reality
PY  - 2020
AB  - While virtual objects are likely to be a part of future interfaces, we lack knowledge of how the dynamic position of virtual objects influences users' posture. In this study, we investigated users' posture change following the unobtrusive and swift motions of a content window in virtual reality (VR). In two perception studies, we estimated the perception threshold on undetectable slow motions and displacement during an eye blink. In a formative study, we compared users' performance, posture change as well as subjective responses on unobtrusive, swift, and no motions. Based on the result, we designed concept applications and explored potential design space of moving virtual content for unobtrusive posture change. With our study, we discuss the interfaces that control users and the initial design guidelines of unobtrusive posture manipulation.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376794
ER  - 

TY  - JOUR
AU  - Sakashita, Mose; Ricci, E. Andy; Arora, Jatin; Guimbretière, François
TI  - RemoteCoDe: Robotic Embodiment for Enhancing Peripheral Awareness in Remote Collaboration Tasks
PY  - 2022
AB  - <jats:p>Collaborative design activities are often centered around physical artifacts. Depending on the design activity, this can be the model of a building, paper crafts, carving artwork, or a new circuit to be debugged and evaluated. In a typical setting, collaborators are seated around a table and divide their attention between the design artifact under review, at least one laptop supporting measurements and information foraging, and of course their collaborators. Although these activities involve complex sets of tools and configurations, people can easily work together when they are present in the same space. This is because the physical presence of a partner affords peripheral awareness to inform where the partner's attention is and what they are doing. This peripheral awareness allows collaborators to coordinate actions and manage coupling to achieve a shared task. For example, it is quite easy to know when your partner switches their focus from a breadboard to you as a request to start a face to face discussion.</jats:p>
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - CSCW1
PB  - 
DO  - 10.1145/3512910
ER  - 

TY  - NA
AU  - Shen, Vivian; Shultz, Craig; Harrison, Chris
TI  - Mouth Haptics in VR using a Headset Ultrasound Phased Array
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501960
ER  - 

TY  - NA
AU  - He, Zhenyi; Lutteroth, Christof; Perlin, Ken
TI  - TapGazer: Text Entry with Finger Tapping and Gaze-directed Word Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501838
ER  - 

TY  - JOUR
AU  - Buga, C S; Viana, J C
TI  - The role of printed electronics and related technologies in the development of smart connected products
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>The emergence of novel materials with flexible and stretchable characteristics, and the use of new processing technologies, have allowed for the development of new connected devices and applications. Using printed electronics, traditional electronic elements are being combined with flexible components and allowing for the development of new smart connected products. As a result, devices that are capable of sensing, actuating, and communicating remotely while being low-cost, lightweight, conformable, and easily customizable are already being developed. Combined with the expansion of the Internet of Things, artificial intelligence, and encryption algorithms, the overall attractiveness of these technologies has prompted new applications to appear in almost every sector. The exponential technological development is currently allowing for the ‘smartification’ of cities, manufacturing, healthcare, agriculture, logistics, among others. In this review article, the steps towards this transition are approached, starting from the conceptualization of smart connected products and their main markets. The manufacturing technologies are then presented, with focus on printing-based ones, compatible with organic materials. Finally, each one of the printable components is presented and some applications are discussed.</jats:p>
SP  - 43001
EP  - 043001
JF  - Flexible and Printed Electronics
VL  - 7
IS  - 4
PB  - 
DO  - 10.1088/2058-8585/ac91de
ER  - 

TY  - NA
AU  - Chidambaram, Subramanian; Reddy, Sai Swarup; Rumple, Matthew; Ipsita, Ananya; Villanueva, Ana; Redick, Thomas; Stuerzlinger, Wolfgang; Ramani, Karthik
TI  - EditAR: A Digital Twin Authoring Environment for Creation of AR/VR and Video Instructions from a Single Demonstration
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00048
ER  - 

TY  - NA
AU  - Getschmann, Christopher; Mthunzi, Everett Mondliwethu; Echtler, Florian
TI  - MirrorForge: Rapid Prototyping of Complex Mirrors for Camera and Projector Systems
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501329
ER  - 

TY  - JOUR
AU  - Ma, Yan; Zhai, Shumin; Ramakrishnan, I. V.; Bi, Xiaojun
TI  - UIST - Modeling Touch Point Distribution with Rotational Dual Gaussian Model
PY  - 2021
AB  - Touch point distribution models are important tools for designing touchscreen interfaces. In this paper, we investigate how the finger movement direction affects the touch point distribution, and how to account for it in modeling. We propose the Rotational Dual Gaussian model, a refinement and generalization of the Dual Gaussian model, to account for the finger movement direction in predicting touch point distribution. In this model, the major axis of the prediction ellipse of the touch point distribution is along the finger movement direction, and the minor axis is perpendicular to the finger movement direction. We also propose using projected target width and height, in lieu of nominal target width and height to model touch point distribution. Evaluation on three empirical datasets shows that the new model reflects the observation that the touch point distribution is elongated along the finger movement direction, and outperforms the original Dual Gaussian Model in all prediction tests. Compared with the original Dual Gaussian model, the Rotational Dual Gaussian model reduces the RMSE of touch error rate prediction from 8.49% to 4.95%, and more accurately predicts the touch point distribution in target acquisition. Using the Rotational Dual Gaussian model can also improve the soft keyboard decoding accuracy on smartwatches.
SP  - 1197
EP  - 1209
JF  - Proceedings of the ACM Symposium on User Interface Software and Technology. ACM Symposium on User Interface Software and Technology
VL  - 2021
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474816
ER  - 

TY  - NA
AU  - Sapkota, Shardul; Ram, Ashwin; Zhao, Shengdong
TI  - MobileHCI - Ubiquitous Interactions for Heads-Up Computing: Understanding Users’ Preferences for Subtle Interaction Techniques in Everyday Settings
PY  - 2021
AB  - In order to satisfy users’ information needs while incurring minimum interference to their ongoing activities, previous studies have proposed using Optical Head-mounted Displays (OHMDs) with different input techniques. However, it is unclear how these techniques compare against one another in terms of being comfortable and non-intrusive to a user’s everyday tasks. Through a wizard-of-oz study, we thus compared four subtle interaction techniques (feet, arms, thumb-index-fingers, and teeth) in three daily hands-busy tasks under different settings (giving a presentation–sitting, carrying bags–walking, and folding clothes–standing). We found that while each interaction technique has its niche, thumb-index-finger interaction has the best overall balance and is most preferred as a cross-scenario subtle interaction technique for smart glasses. We provide further evaluation of thumb-index-finger interaction with an in-the-wild study with 8 users. Our results contribute to an enhanced understanding of user preferences for subtle interaction techniques with smart glasses for everyday use.
SP  - NA
EP  - NA
JF  - Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3447526.3472035
ER  - 

TY  - JOUR
AU  - Usuba, Hiroki; Yamanaka, Shota; Sato, Junichi; Miyashita, Homei
TI  - Predicting Touch Accuracy for Rectangular Targets by Using One-Dimensional Task Results
PY  - 2022
AB  - <jats:p>We propose a method that predicts the success rate in pointing to 2D rectangular targets by using 1D vertical-bar and horizontal-bar task results. The method can predict the success rates for more practical situations under fewer experimental conditions. This shortens the duration of experiments, thus saving costs for researchers and practitioners. We verified the method through two experiments: laboratory-based and crowdsourced ones. In the laboratory-based experiment, we found that using 1D task results to predict the success rate for 2D targets slightly decreases the prediction accuracy. In the crowdsourced experiment, this method scored better than using 2D task results. Thus, we recommend that researchers use the method properly depending on the situation.</jats:p>
SP  - 525
EP  - 537
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567732
ER  - 

TY  - NA
AU  - Lee, Chi-Jung; Tsai, Hsin-Ruey; Chen, Bing-Yu
TI  - CHI - HairTouch: Providing Stiffness, Roughness and Surface Height Differences Using Reconfigurable Brush Hairs on a VR Controller
PY  - 2021
AB  - Tactile feedback is widely used to enhance realism in virtual reality (VR). When touching virtual objects, stiffness and roughness are common and obvious factors perceived by the users. Furthermore, when touching a surface with complicated surface structure, differences from not only stiffness and roughness but also surface height are crucial. To integrate these factors, we propose a pin-based handheld device, HairTouch, to provide stiffness differences, roughness differences, surface height differences and their combinations. HairTouch consists of two pins for the two finger segments close to the index fingertip, respectively. By controlling brush hairs’ length and bending direction to change the hairs’ elasticity and hair tip direction, each pin renders various stiffness and roughness, respectively. By further independently controlling the hairs’ configuration and pins’ height, versatile stiffness, roughness and surface height differences are achieved. We conducted a perception study to realize users’ distinguishability of stiffness and roughness on each of the segments. Based on the results, we performed a VR experience study to verify that the tactile feedback from HairTouch enhances VR realism.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445285
ER  - 

TY  - NA
AU  - Yoshida, Takatoshi; Okazaki, Narin; Takaki, Ken; Hirose, Masaharu; Kitagawa, Shingo; Inami, Masahiko
TI  - Flexel: A Modular Floor Interface for Room-Scale Tactile Sensing
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545699
ER  - 

TY  - JOUR
AU  - Arora, Rahul; Singh, Karan
TI  - Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality
PY  - 2021
AB  - Complex 3D curves can be created by directly drawing mid-air in immersive environments (Augmented and Virtual Realities). Drawing mid-air strokes precisely on the surface of a 3D virtual object, however, is difficult, necessitating a projection of the mid-air stroke onto the user “intended” surface curve. We present the first detailed investigation of the fundamental problem of 3D stroke projection in VR. An assessment of the design requirements of real-time drawing of curves on 3D objects in VR is followed by the definition and classification of multiple techniques for 3D stroke projection. We analyze the advantages and shortcomings of these approaches both theoretically and via practical pilot testing. We then formally evaluate the two most promising techniques spraycan and mimicry with 20 users in VR. The study shows a strong qualitative and quantitative user preference for our novel stroke mimicry projection algorithm. We further illustrate the effectiveness andutility of stroke mimicry to draw complex 3D curves on surfaces for various artistic and functional design applications.
SP  - 1
EP  - 17
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 3
PB  - 
DO  - 10.1145/3459090
ER  - 

TY  - NA
AU  - Rodrigues, André; Santos, André R.B.; Montague, Kyle; Nicolau, Hugo; Guerreiro, Tiago
TI  - UbiComp/ISWC Adjunct - WildKey: A Privacy-Aware Keyboard Toolkit for Data Collection In-The-Wild
PY  - 2021
AB  - Touch data, and in particular text-entry data, has been predominantly collected in laboratory settings, under controlled conditions. While touch and text-entry data has consistently shown its potential for monitoring and detecting a variety of conditions and impairments, its deployment in-the-wild remains a challenge. In this paper, we present WildKey, an Android keyboard toolkit that allows for the usable deployment of in-the-wild user studies. WildKey is able to analyse text-entry behaviours through implicit and explicit text-entry data collection while ensuring user privacy. We detail each of the WildKey’s components and features, metrics collected, and discuss the steps taken to ensure user privacy thus promoting compliance.
SP  - 542
EP  - 545
JF  - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460418.3482872
ER  - 

TY  - JOUR
AU  - Arora, Nivedita; Starner, Thad; Abowd, Gregory D.
TI  - SATURN: an introduction to the internet of materials
PY  - 2020
AB  - We envision a new generation of computation devices, computational materials, which are self-sustainable, cheaply manufactured at scale and exhibit form factors that are easily incorporated into everyday environments. These materials can enable ordinary objects such as walls, carpet, furniture, jewelry, and cups to do computational things without looking like today's computational devices. Self-powered Audio Triboelectric Ultra-thin Rollable Nanogenerator (SATURN) is an early example of a computational material that can sense vibration, such as sound. SATURN can be manufactured from inexpensive components, is flexible so that it can be integrated into many different surfaces, and powers itself through the sound or vibration it is sensing. Using radio backscatter, we demonstrate that SATURN's sensed data is passively transmitted to remote computers, alleviating the need for batteries or any wired power for the material itself. The proliferation of these types of computational materials ushers an era of Internet of Materials, further blurring the distinction between the physical and digital worlds.
SP  - 92
EP  - 99
JF  - Communications of the ACM
VL  - 63
IS  - 12
PB  - 
DO  - 10.1145/3429948
ER  - 

TY  - NA
AU  - Yoon, Leonard; Yang, Dongseok; Chung, Choongho; Lee, Sung-Hee
TI  - A Full Body Avatar-Based Telepresence System for Dissimilar Spaces.
PY  - 2021
AB  - We present a novel mixed reality (MR) telepresence system enabling a local user to interact with a remote user through full-body avatars in their own rooms. If the remote rooms have different sizes and furniture arrangements, directly applying a user's motion to an avatar leads to a mismatch of placement and deictic gesture. To overcome this problem, we retarget the placement, arm gesture, and head movement of a local user to an avatar in a remote room to preserve a local user's environment and interaction context. This allows avatars to utilize real furniture and interact with a local user and shared objects as if they were in the same room. This paper describes our system's design and implementation in detail and a set of example scenarios in the living room and office room. A qualitative user study delves into a user experience, challenges, and possible extensions of the proposed system.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Zhang, Qian; Wang, Dong; Zhao, Run; Yu, Yinggang
TI  - SoundLip: Enabling Word and Sentence-level Lip Interaction for Smart Devices
PY  - 2021
AB  - As a natural and convenient interaction modality, voice input has now become indispensable to smart devices (e.g. mobile phones and smart appliances). However, voice input is strongly constrained by surroundings and may raise privacy leakage in public areas. In this paper, we present SoundLip, an end-to-end interaction system enabling users to interact with smart devices via silent voice input. The key insight is to use inaudible acoustic signals to capture the lip movements of users when they issue commands. Previous works have considered lip reading as a naive classification task and thus can only recognize individual words. In contrast, our proposed system enables lip reading at both word and sentence levels, which are more suitable for daily-life use. We exploit the built-in speakers and microphones of smart devices to emit acoustic signals and listen to their reflections, respectively. In order to better abstract representations from multi-frequency and multi-modality acoustic signals, we elaborate a hierarchical convolutional neural network (HCNN) to serve as the front-end as well as recognize individual word commands. Then, for the sentence-level recognition, we exploit a multi-task encoder-decoder network to get around temporal segmentation and output sentences in an end-to-end way. We evaluate SoundLip on 20 individual words and 70 sentences from 12 participants. Our system achieves an accuracy of 91.2% at word-level and a word error rate of 7.1% at sentence-level in both user-independent and environment-independent settings. Given its innovative solution and promising performance, we believe that SoundLip has made a significant contribution to the advancement of silent voice input technology.
SP  - 1
EP  - 28
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 1
PB  - 
DO  - 10.1145/3448087
ER  - 

TY  - JOUR
AU  - Shen, Yuefan; Zhang, Changgeng; Fu, Hongbo; Zhou, Kun; Zheng, Youyi
TI  - DeepSketchHair: Deep Sketch-Based 3D Hair Modeling
PY  - 2021
AB  - We present DeepSketchHair , a deep learning based tool for modeling of 3D hair from 2D sketches. Given a 3D bust model as reference, our sketching system takes as input a user-drawn sketch (consisting of hair contour and a few strokes indicating the hair growing direction within a hair region), and automatically generates a 3D hair model, matching the input sketch. The key enablers of our system are three carefully designed neural networks, namely, S2ONet , which converts an input sketch to a dense 2D hair orientation field; O2VNet , which maps the 2D orientation field to a 3D vector field; and V2VNet , which updates the 3D vector field with respect to the new sketches, enabling hair editing with additional sketches in new views. All the three networks are trained with synthetic data generated from a 3D hairstyle database. We demonstrate the effectiveness and expressiveness of our tool using a variety of hairstyles and also compare our method with prior art.
SP  - 3250
EP  - 3263
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 7
PB  - 
DO  - 10.1109/tvcg.2020.2968433
ER  - 

TY  - NA
AU  - Xiao, Chufeng; Yu, Deng; Han, Xiaoguang; Zheng, Youyi; Fu, Hongbo
TI  - SketchHairSalon: Deep Sketch-based Hair Image Synthesis
PY  - 2021
AB  - Recent deep generative models allow real-time generation of hair images from sketch inputs. Existing solutions often require a user-provided binary mask to specify a target hair shape. This not only costs users extra labor but also fails to capture complicated hair boundaries. Those solutions usually encode hair structures via orientation maps, which, however, are not very effective to encode complex structures. We observe that colored hair sketches already implicitly define target hair shapes as well as hair appearance and are more flexible to depict hair structures than orientation maps. Based on these observations, we present SketchHairSalon, a two-stage framework for generating realistic hair images directly from freehand sketches depicting desired hair structure and appearance. At the first stage, we train a network to predict a hair matte from an input hair sketch, with an optional set of non-hair strokes. At the second stage, another network is trained to synthesize the structure and appearance of hair images from the input sketch and the generated matte. To make the networks in the two stages aware of long-term dependency of strokes, we apply self-attention modules to them. To train these networks, we present a new dataset containing thousands of annotated hair sketch-image pairs and corresponding hair mattes. Two efficient methods for sketch completion are proposed to automatically complete repetitive braided parts and hair strokes, respectively, thus reducing the workload of users. Based on the trained networks and the two sketch completion strategies, we build an intuitive interface to allow even novice users to design visually pleasing hair images exhibiting various hair structures and appearance via freehand sketches. The qualitative and quantitative evaluations show the advantages of the proposed system over the existing or alternative solutions.
SP  - NA
EP  - NA
JF  - arXiv: Computer Vision and Pattern Recognition
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Seo, Hyunggoog; Kim, Jaedong; Seo, Kwanggyoon; Kim, Bumki; Noh, Junyong
TI  - Overthere: A Simple and Intuitive Object Registration Method for an Absolute Mid-air Pointing Interface
PY  - 2021
AB  - An absolute mid-air pointing technique requires a preprocess called registration that makes the system remember the 3D positions and types of objects in advance. Previous studies have simply assumed that the information is already available because it requires a cumbersome process performed by an expert in a carefully calibrated environment. We introduce Overthere, which allows the user to intuitively register the objects in a smart environment by pointing to each target object a few times. To ensure accurate and coherent pointing gestures made by the user regardless of individual differences between them, we performed a user study and identified a desirable gesture motion for this purpose. In addition, we provide the user with various feedback to help them understand the current registration progress and adhere to required conditions, which will lead to accurate registration results. The user studies show that Overthere is sufficiently intuitive to be used by ordinary people.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478128
ER  - 

TY  - NA
AU  - He, Weijia; Zhao, Valerie; Morkved, Olivia; Siddiqui, Sabeeka; Fernandes, Earlence; Hester, Josiah; Ur, Blase
TI  - EuroS&amp;P - SoK: Context Sensing for Access Control in the Adversarial Home IoT
PY  - 2021
AB  - In smart homes, access-control policies increasingly depend on contexts, such as who is taking an action, whether there is an emergency, or whether an adult is nearby. The vast literature on context sensing could potentially be leveraged to support contextual access control, yet this literature mostly ignores attacks, adversaries, and privacy. In this paper, we reevaluate the literature on home context sensing through a security and privacy mindset. We first describe a novel threat model in smart homes focusing on the capabilities of non-technical adversaries. Replay, imitation, and shoulder-surfing attacks are much more likely in this model. We summarize contexts relevant to access control in homes, mapping them to existing sensors. We then systematize the sensing literature to construct a decision framework for home context sensing that considers security, privacy, and usability. Applying our framework, we find that current sensors do not fully mitigate likely threats in homes. Some sensors are susceptible to simple threats like physical denial-of-service attacks, making it easy to bypass policies relying on the absence of a characteristic. Many sensors collect more data than needed and are not effective for all groups of users or under all situations.
SP  - 37
EP  - 53
JF  - 2021 IEEE European Symposium on Security and Privacy (EuroS&P)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/eurosp51992.2021.00014
ER  - 

TY  - NA
AU  - Yang, Jackie (Junrui); Chen, Tuochao; Qin, Fang; Lam, Monica S.; Landay, James A.
TI  - HybridTrak: Adding Full-Body Tracking to VR Using an Off-the-Shelf Webcam
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502045
ER  - 

TY  - NA
AU  - Lukes, Dylan; Sarracino, John; Coleman, Cora; Peleg, Hila; Lerner, Sorin; Polikarpova, Nadia
TI  - ESEC/SIGSOFT FSE - Synthesis of web layouts from examples
PY  - 2021
AB  - We present a new technique for synthesizing dynamic, constraint-based visual layouts from examples. Our technique tackles two major challenges of layout synthesis. First, realistic layouts, especially on the web, often contain hundreds of elements, so the synthesizer needs to scale to layouts of this complexity. Second, in common usage scenarios, examples contain noise, so the synthesizer needs to be tolerant to imprecise inputs. To address these challenges we propose a two-phase approach to synthesis, where a local inference phase rapidly generates a set of likely candidate constraints that satisfy the given examples, and then a global inference phase selects a subset of the candidates that generalizes to unseen inputs. This separation of concerns helps our technique tackle the two challenges: the local phase employs Bayesian inference to handle noisy inputs, while the global phase leverages the hierarchical nature of complex layouts to decompose the global inference problem into inference of independent sub-layouts. We implemented this technique in a tool called Mockdown and evaluated it on nine real-world web layouts, as well as a series of widespread layout components and an existing dataset of 644 Android applications. Our experiments show that Mockdown is able to synthesize a highly accurate layout for the majority of benchmarks from just three examples (two for Android layouts), and that it scales to layouts with over 600 elements, about 30x more than has been reported in prior work on layout synthesis.
SP  - 651
EP  - 663
JF  - Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3468264.3468533
ER  - 

TY  - JOUR
AU  - Biener, Verena; Gesslein, Travis; Schneider, Daniel; Kawala, Felix; Otte, Alexander; Kristensson, Per Ola; Pahud, Michel; Ofek, Eyal; Campos, Cuauhtli; Kljun, Matjaz; Pucihar, Klen Copic; Grubert, Jens
TI  - PoVRPoint: Authoring Presentations in Mobile Virtual Reality.
PY  - 2022
AB  - Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint-a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded shapes or objects. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.
SP  - 2069
EP  - 2079
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2022.3150474
ER  - 

TY  - NA
AU  - Penzkofer, Anna; Müller, Philipp; Bühler, Felix; Mayer, Sven; Bulling, Andreas
TI  - ICMI - ConAn: A Usable Tool for Multimodal Conversation Analysis
PY  - 2021
AB  - Multimodal analysis of group behavior is a key task in human-computer interaction, and in the social and behavioral sciences, but is often limited to more easily controllable laboratory settings or requires elaborate multi-sensor setups and time-consuming manual data annotation. We present ConAn – a usable tool to explore and automatically analyze non-verbal behavior of multiple persons during natural group conversations. In contrast to traditional multi-sensor setups, our tool only requires a single 360° camera and uses state-of-the-art computer vision methods to automatically extract behavioral indicators, such as gaze direction, facial expressions, and speaking activity. As such, our tool allows for easy and fast deployment and supports researchers in understanding individual behavior, group interaction dynamics, and in quantifying user-object interactions. We illustrate the benefits of ConAn on three sample use cases: conversation analysis, assessment of collaboration quality, and impact of technology on audience behavior. Taken together, ConAn represents an important step towards democratizing automatic conversation analysis in HCI and beyond.
SP  - 341
EP  - 351
JF  - Proceedings of the 2021 International Conference on Multimodal Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3462244.3479886
ER  - 

TY  - BOOK
AU  - Günther, Sebastian; Müller, Florian; Hübner, Felix; Mühlhäuser, Max; Matviienko, Andrii
TI  - EICS - ActuBoard: An Open Rapid Prototyping Platform to integrate Hardware Actuators in Remote Applications
PY  - 2021
AB  - Prototyping is an essential step in developing tangible experiences and novel devices, ranging from haptic feedback to wearables. However, prototyping of actuated devices nowadays often requires repetitive and time-consuming steps, such as wiring, soldering, and programming basic communication, before HCI researchers and designers can focus on their primary interest: designing interaction. In this paper, we present ActuBoard, a prototyping platform to support 1) quick assembly, 2) less preparation work, and 3) the inclusion of non-tech-savvy users. With ActuBoard, users are not required to create complex circuitry, write a single line of firmware, or implementing communication protocols. Acknowledging existing systems, our platform combines the flexibility of low-level microcontrollers and ease-of-use of abstracted tinker platforms to control actuators from separate applications. As further contribution, we highlight the technical specifications and published the ActuBoard platform as Open Source.
SP  - 70
EP  - 76
JF  - Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3459926.3464757
ER  - 

TY  - NA
AU  - Shim, Youngbo Aram; Kim, Taejun; Lee, Geehyuk
TI  - QuadStretch: A Forearm-wearable Multi-dimensional Skin Stretch Display for Immersive VR Haptic Feedback
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3519908
ER  - 

TY  - NA
AU  - Hwang, Dong-Hyun; Aso, Kohei; Yuan, Ye; Kitani, Kris M.; Koike, Hideki
TI  - UIST - MonoEye: Multimodal Human Motion Capture System Using A Single Ultra-Wide Fisheye Camera
PY  - 2020
AB  - We present MonoEye, a multimodal human motion capture system using a single RGB camera with an ultra-wide fisheye lens, mounted on the user's chest. Existing optical motion capture systems use multiple cameras, which are synchronized and require camera calibration. These systems also have usability constraints that limit the user's movement and operating space. Since the MonoEye system is based on a wearable single RGB camera, the wearer's 3D body pose can be captured without space and environment limitations. The body pose, captured with our system, is aware of the camera orientation and therefore it is possible to recognize various motions that existing egocentric motion capture systems cannot recognize. Furthermore, the proposed system captures not only the wearer's body motion but also their viewport using the head pose estimation and an ultra-wide image. To implement robust multimodal motion capture, we design three deep neural networks: BodyPoseNet, HeadPoseNet, and CameraPoseNet, that estimate 3D body pose, head pose, and camera pose in real-time, respectively. We train these networks with our new extensive synthetic dataset providing 680K frames of renderings of people with a wide range of body shapes, clothing, actions, backgrounds, and lighting conditions. To demonstrate the interactive potential of the MonoEye system, we present several application examples from common body gestural to context-aware interactions.
SP  - 98
EP  - 111
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415856
ER  - 

TY  - JOUR
AU  - Li, Guangchuan; Rempel, David; Liu, Yue; Song, Weitao; Adamson, Carisa Harris
TI  - Design of 3D Microgestures for Commands in Virtual Reality or Augmented Reality
PY  - 2021
AB  - Virtual and augmented reality (VR, AR) systems present 3D images that users can interact with using controllers or gestures. The design of the user input process is crucial and determines the interactive efficiency, comfort, and adoption. Gesture-based input provides a device-free interaction that may improve safety and creativity compared to using a hand controller while allowing the hands to perform other tasks. Microgestures with small finger and hand motions may have an advantage over the larger forearm and upper arm gestures by reducing distraction, reducing fatigue, and increasing privacy during the interaction. The design of microgestures should consider user experience, ergonomic principles, and interface design to optimize productivity and comfort while minimizing errors. Forty VR/AR or smart device users evaluated a set of 33 microgestures, designed by ergonomists, and linked them to 20 common AR/VR commands based on usability, comfort, and preference. Based primarily on preference, a set of microgestures linked to specific commands is proposed for VR or AR systems. The proposed microgesture set will likely minimize fatigue and optimize usability. Furthermore, the methodology presented for selecting microgestures and assigning them to commands can be applied to the design of other gesture sets.
SP  - 6375
EP  - NA
JF  - Applied Sciences
VL  - 11
IS  - 14
PB  - 
DO  - 10.3390/app11146375
ER  - 

TY  - NA
AU  - Takano, Masamune; Morimoto, Yuki
TI  - Modeling 3D Hair by Outlining Hair Cards
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - ACM SIGGRAPH 2022 Posters
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532719.3543214
ER  - 

TY  - NA
AU  - Nebeling, Michael; Rajaram, Shwetha; Wu, Liwei; Cheng, Yifei; Herskovitz, Jaylin
TI  - CHI - XRStudio: A Virtual Production and Live Streaming System for Immersive Instructional Experiences
PY  - 2021
AB  - There is increased interest in using virtual reality in education, but it often remains an isolated experience that is difficult to integrate into current instructional experiences. In this work, we adapt virtual production techniques from filmmaking to enable mixed reality capture of instructors so that they appear to be standing directly in the virtual scene. We also capitalize on the growing popularity of live streaming software for video conferencing and live production. With XRStudio, we develop a pipeline for giving lectures in VR, enabling live compositing using a variety of presets and real-time output to traditional video and more immersive formats. We present interviews with media designers experienced in film and MOOC production that informed our design. Through walkthrough demonstrations of XRStudio with instructors experienced with VR, we learn how it could be used in a variety of domains. In end-to-end evaluations with students, we analyze and compare differences of traditional video vs. more immersive lectures with XRStudio.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445323
ER  - 

TY  - JOUR
AU  - Hou, Pengpeng; Zhang, Heng; Wu, Yanjun; Yu, Jiageng; Miao, Yuxia; Tai, Yang
TI  - FindCmd: A personalised command retrieval tool
PY  - 2021
AB  - NA
SP  - 161
EP  - 173
JF  - IET Software
VL  - 15
IS  - 2
PB  - 
DO  - 10.1049/sfw2.12015
ER  - 

TY  - NA
AU  - Merino, Leonel; Schwarzl, Magdalena; Kraus, Matthias; Sedlmair, Michael; Schmalstieg, Dieter; Weiskopf, Daniel
TI  - Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)
PY  - 2020
AB  - We present a systematic review of 458 papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-the-wild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - BOOK
AU  - Tsukamoto, Kenji; Nakajima, Tatsuo; Gushima, Kota
TI  - LifeTech - Investigating a Method to Reduce Japanese People's Embarrassment in Using Voice Inputs
PY  - 2021
AB  - In this paper, we investigate the reasons for the embarrassment that Japanese people feel when using voice inputs in public and propose a way to reduce it. Recently, multiple devices have added a voice input function. However, few people use voice input even in situations where using voice input would be better. We first investigate the reasons why Japanese people do not use voice inputs even when they can, and it becomes clear that many people feel embarrassment at speaking out. After the investigation, we conduct an experiment to compare the embarrassment caused by using voice input when the target of voice input is an inorganic machine compared to when it is a biological character. In the experiment, we conclude that a user's emotion can be altered by using an animal-like character as the target of voice commands.
SP  - 166
EP  - 170
JF  - 2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/lifetech52111.2021.9391937
ER  - 

TY  - JOUR
AU  - Hoppe, Adrian Heinrich; van de Camp, Florian; Stiefelhagen, Rainer
TI  - ShiSha: Enabling Shared Perspective With Face-to-Face Collaboration Using Redirected Avatars in Virtual Reality
PY  - 2021
AB  - The importance of remote collaboration grows in an interconnected world as the reasons to avoid travel increase. The spatial rendering and collaboration capabilities of virtual and augmented reality systems are well suited for tasks such as support or training. Users can take a shared perspective to build a common understanding. Also, users may engage in face-to-face cooperation to support interpersonal communication. However, a shared perspective and face-to-face collaboration are both desirable but naturally exclude each other. We place all users at the same location to provide a shared perspective. To avoid overlapping body parts, the avatars of the other connected users are shifted to the side. A redirected body pose modification corrects the resulting inconsistencies. The implemented system is compared to a baseline of two users standing in the same location and working with overlapping avatars. The results of a user study show that the proposed modifications provide an easy to use, efficient collaboration and yield higher co-presence and the feeling of teamwork. Applying redirection techniques to other users opens up novel ways to increase social presence for local or remote collaboration.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - CSCW3
PB  - 
DO  - 10.1145/3432950
ER  - 

TY  - NA
AU  - Kumaravel, Balasaravanan Thoravi; Nguyen, Cuong; DiVerdi, Stephen; Hartmann, Bjoern
TI  - UIST - TransceiVR: Bridging Asymmetrical Communication Between VR Users and External Collaborators
PY  - 2020
AB  - Virtual Reality (VR) users often need to work with other users, who observe them outside of VR using an external display. Communication between them is difficult; the VR user cannot see the external user's gestures, and the external user cannot see VR scene elements outside of the VR user's view. We carried out formative interviews with experts to understand these asymmetrical interactions and identify their goals and challenges. From this, we identify high-level system design goals to facilitate asymmetrical interactions and a corresponding space of implementation approaches based on the level of programmatic access to a VR application. We present TransceiVR, a system that utilizes VR platform APIs to enable asymmetric communication interfaces for third-party applications without requiring source code access. TransceiVR allows external users to explore the VR scene spatially or temporally, to annotate elements in the VR scene at correct depths, and to discuss via a shared static virtual display. An initial co-located user evaluation with 10 pairs shows that our system makes asymmetric collaborations in VR more effective and successful in terms of task time, error rate, and task load index. An informal evaluation with a remote expert gives additional insight on utility of features for real world tasks.
SP  - 182
EP  - 195
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415827
ER  - 

TY  - NA
AU  - Wolf, Julian; Lohmeyer, Quentin; Holz, Christian; Meboldt, Mirko
TI  - ISMAR - Gaze Comes in Handy: Predicting and Preventing Erroneous Hand Actions in AR-Supported Manual Tasks
PY  - 2021
AB  - Emerging Augmented Reality headsets incorporate gaze and hand tracking and can, thus, observe the user’s behavior without interfering with ongoing activities. In this paper, we analyze hand-eye coordination in real-time to predict hand actions during target selection and warn users of potential errors before they occur. In our first user study, we recorded 10 participants playing a memory card game, which involves frequent hand-eye coordination with little task-relevant information. We found that participants’ gaze locked onto target cards 350ms before the hands touched them in 73.3% of all cases, which coincided with the peak velocity of the hand moving to the target. Based on our findings, we then introduce a closed-loop support system that monitors the user’s fingertip position to detect the first card turn and analyzes gaze, hand velocity and trajectory to predict the second card before it is turned by the user. In a second study with 12 participants, our support system correctly displayed color-coded visual alerts in a timely manner with an accuracy of 85.9%. The results indicate the high value of eye and hand tracking features for behavior prediction and provide a first step towards predictive real-time user support.
SP  - 166
EP  - 175
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00031
ER  - 

TY  - JOUR
AU  - Weerasinghe, Maheshya; Quigley, Aaron; Pucihar, Klen Copic; Toniolo, Alice; Miguel, Angela; Kljun, Matjaz
TI  - Arigatō: Effects of Adaptive Guidance on Engagement and Performance in Augmented Reality Learning Environments
PY  - 2022
AB  - Experiential learning (ExL) is the process of learning through experience or more specifically “learning through reflection on doing”. In this paper, we propose a simulation of these experiences, in Augmented Reality (AR), addressing the problem of language learning. Such systems provide an excellent setting to support “adaptive guidance”, in a digital form, within a real environment. Adaptive guidance allows the instructions and learning content to be customised for the individual learner, thus creating a unique learning experience. We developed an adaptive guidance AR system for language learning, we call Arigatō (Augmented Reality Instructional Guidance & Tailored Omniverse), which offers immediate assistance, resources specific to the learner's needs, manipulation of these resources, and relevant feedback. Considering guidance, we employ this prototype to investigate the effect of the amount of guidance (fixed vs. adaptive-amount) and the type of guidance (fixed vs. adaptive-associations) on the engagement and consequently the learning outcomes of language learning in an AR environment. The results for the amount of guidance show that compared to the adaptive-amount, the fixed-amount of guidance group scored better in the immediate and delayed (after 7 days) recall tests. However, this group also invested a significantly higher mental effort to complete the task. The results for the type of guidance show that the adaptive-associations group outperforms the fixed-associations group in the immediate, delayed (after 7 days) recall tests, and learning efficiency. The adaptive-associations group also showed significantly lower mental effort and spent less time to complete the task.
SP  - 3737
EP  - 3747
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203088
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Koelle, Marion; Fruchard, Bruno; Mavali, Sahar; Klamka, Konstantin; Groeger, Daniel; Strohmeier, Paul; Steimle, Jürgen
TI  - CHI - BodyStylus: Freehand On-Body Design and Fabrication of Epidermal Interfaces
PY  - 2021
AB  - In traditional body-art, designs are adjusted to the body as they are applied, enabling creative improvisation and exploration. Conventional design and fabrication methods of epidermal interfaces, however, separate these steps. With BodyStylus we present the first computer-assisted approach for on-body design and fabrication of epidermal interfaces. Inspired by traditional techniques, we propose a hand-held tool that augments freehand inking with digital support: projected in-situ guidance assists creating valid on-body circuits and aesthetic ornaments that align with the human bodyscape, while pro-active switching between inking and non-inking creates error preventing constraints. We contribute BodyStylus’s design rationale and interaction concept along with an interactive prototype that uses self-sintering conductive ink. Results of two focus group explorations showed that guidance was more appreciated by artists, while constraints appeared more useful to engineers, and that working on the body inspired critical reflection on the relationship between bodyscape, interaction, and designs.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445475
ER  - 

TY  - JOUR
AU  - Peng, Xirui; Yue, Liang; Liang, Sizhuang; Montgomery, Stuart; Lu, Chunliang; Cheng, Chieh‐Min; Beyah, Raheem; Zhao, Ruike Renee; Qi, H. Jerry
TI  - Multi‐Color 3D Printing via Single‐Vat Grayscale Digital Light Processing
PY  - 2022
AB  - NA
SP  - 2112329
EP  - 2112329
JF  - Advanced Functional Materials
VL  - 32
IS  - 28
PB  - 
DO  - 10.1002/adfm.202112329
ER  - 

TY  - BOOK
AU  - David-John, Brendan; Peacock, Candace E.; Zhang, Ting; Murdison, T. Scott; Benko, Hrvoje; Jonker, Tanya R.
TI  - ETRA Short Papers - Towards gaze-based prediction of the intent to interact in virtual reality
PY  - 2021
AB  - With the increasing frequency of eye tracking in consumer products, including head-mounted augmented and virtual reality displays, gaze-based models have the potential to predict user intent and unlock intuitive new interaction schemes. In the present work, we explored whether gaze dynamics can predict when a user intends to interact with the real or digital world, which could be used to develop predictive interfaces for low-effort input. Eye-tracking data were collected from 15 participants performing an item-selection task in virtual reality. Using logistic regression, we demonstrated successful prediction of the onset of item selection. The most prevalent predictive features in the model were gaze velocity, ambient/focal attention, and saccade dynamics, demonstrating that gaze features typically used to characterize visual attention can be applied to model interaction intent. In the future, these types of models can be used to infer user’s near-term interaction goals and drive ultra-low-friction predictive interfaces.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448018.3458008
ER  - 

TY  - NA
AU  - Pfeil, Kevin; Masnadi, Sina; Belga, Jacob; Sera-Josef, Jose-Valentin T.; LaViola, Joseph J.
TI  - CHI - Distance Perception with a Video See-Through Head-Mounted Display
PY  - 2021
AB  - In recent years, pass-through cameras have resurfaced as inclusions for virtual reality (VR) hardware. With modern cameras that now have increased resolution and frame rate, Video See-Through (VST) Head-Mounted Displays (HMD) can be used to provide an Augmented Reality (AR) experience. However, because users see their surroundings through video capture and HMD lenses, there is question surrounding how people perceive their environment with these devices. We conducted a user study with 26 participants to help understand if distance perception is altered when viewing surroundings with a VST HMD. Although previous work shows that distance estimation in VR with an HTC Vive is comparable to that in the real world, our results show that the inclusion of a ZED Mini pass-through camera causes a significant difference between normal, unrestricted viewing and that through a VST HMD.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445223
ER  - 

TY  - JOUR
AU  - Yoon, Leonard; Yang, Dongseok; Kim, Jaehyun; Chung, Choongho; Lee, Sung-Hee
TI  - Placement Retargeting of Virtual Avatars to Dissimilar Indoor Environments.
PY  - 2020
AB  - Rapidly developing technologies are realizing a 3D telepresence, in which geographically separated users can interact with each other through their virtual avatars. In this paper, we present novel methods to determine the avatar's position in an indoor space to preserve the semantics of the user's position in a dissimilar indoor space with different space configurations and furniture layouts. To this end, we first perform a user survey on the preferred avatar placements for various indoor configurations and user placements, and identify a set of related attributes, including interpersonal relation, visual attention, pose, and spatial characteristics, and quantify these attributes with a set of features. By using the obtained dataset and identified features, we train a neural network that predicts the similarity between two placements. Next, we develop avatar placement method that preserves the semantics of the placement of the remote user in a different space as much as possible. We show the effectiveness of our methods by implementing a prototype AR-based telepresence system and user evaluations.
SP  - 1
EP  - 1
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - NA
IS  - 01
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Lee, Sunjae; Kim, Hoyoung; Kim, Sijung; Lee, Sangwook; Kim, Hyosu; Song, Jean Young; Ko, Steven Y.; Oh, Sangeun; Shin, Insik
TI  - A-mash
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 28th Annual International Conference on Mobile Computing And Networking
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3495243.3560522
ER  - 

TY  - NA
AU  - Jayagopal, Dhanya; Lubin, Justin; Chasins, Sarah E.
TI  - Exploring the Learnability of Program Synthesizers by Novice Programmers
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545659
ER  - 

TY  - JOUR
AU  - Yoon, Leonard; Yang, Dongseok; Kim, Jaehyun; Chung, Choongho; Lee, Sung-Hee
TI  - Placement Retargeting of Virtual Avatars to Dissimilar Indoor Environments.
PY  - 2022
AB  - Rapidly developing technologies are realizing a 3D telepresence, in which geographically separated users can interact with each other through their virtual avatars. In this article, we present novel methods to determine the avatar's position in an indoor space to preserve the semantics of the user's position in a dissimilar indoor space with different space configurations and furniture layouts. To this end, we first perform a user survey on the preferred avatar placements for various indoor configurations and user placements, and identify a set of related attributes, including interpersonal relation, visual attention, pose, and spatial characteristics, and quantify these attributes with a set of features. By using the obtained dataset and identified features, we train a neural network that predicts the similarity between two placements. Next, we develop an avatar placement method that preserves the semantics of the placement of the remote user in a different space as much as possible. We show the effectiveness of our methods by implementing a prototype AR-based telepresence system and user evaluations.
SP  - 1619
EP  - 1633
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 3
PB  - 
DO  - 10.1109/tvcg.2020.3018458
ER  - 

TY  - NA
AU  - Ikematsu, Kaori; Kato, Kunihiro; Kawahara, Yoshihiro
TI  - UIST (Adjunct Volume) - LightTouch: Passive Gadgets for Extending Interactions on Capacitive Touchscreens by Automating Touch Inputs
PY  - 2020
AB  - We present LightTouch, a passive gadget to enhance touch interactions on unmodified capacitive touchscreens. It simulates finger operations such as tapping, swiping, or multi-touch gestures using conductive materials and photoresistors embedded inside the objects. The touchscreen emits visible light and the photoresistor senses the level of this light, which changes its resistance value. By controlling the screen brightness, it connects or disconnects the path between the GND and the touchscreen, thus allowing the touch inputs to be controlled. In contrast to conventional physical extensions for touchscreens, our technique does not require continuous finger contact on the conductive part nor the use of batteries. Our technique opens up new possibilities for touch interaction such as for enhancing the trackability of tangibles beyond the simple automation of touch inputs.
SP  - 10
EP  - 12
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416169
ER  - 

TY  - CHAP
AU  - Kerdvibulvech, Chutisant
TI  - HCI (2) - Recent Multimodal Communication Methodologies in Phonology, Vision, and Touch
PY  - 2020
AB  - Due to the innovation and technology’s capacity in recent years, multimodal communication can help humans to communicate and interact in more basis senses simultaneously using technological innovations. However, there are still many difficulties and challenges for achieving the multimodal communication systems in phonology, vision, and touch. In this paper, we present recent multimodal communication methodologies in different aspects. To begin with, we review the multimodal communication systems in phonology, phonetics and light. Then, we investigate the multimodal communication works in vision using spatial augmented reality (SAR), computer vision, and interactive digital multimedia. Next, we introduce the multimodal communication researches in touch using haptic sensors and related-technological innovations. After that, we propose the qualitative research of the extended augmented reality and interactive experience-based system using use in-depth interviews. Our experimental results have revealed that the presented system works well and increases users’ satisfaction levels. After examining the approaches for integrated multimodal communication in phonology, vision, and touch presented in recent years, we can understand the interactions of human with recent innovations.
SP  - 392
EP  - 400
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_27
ER  - 

TY  - JOUR
AU  - Tsuchida, Taichi; Fujita, Kazuyuki; Ikematsu, Kaori; Sarcar, Sayan; Takashima, Kazuki; Kitamura, Yoshifumi
TI  - TetraForce: A Magnetic-Based Interface Enabling Pressure Force and Shear Force Input Applied to Front and Back of a Smartphone
PY  - 2022
AB  - <jats:p>We propose a novel phone-case-shaped interface named TetraForce, which enables four types of force input consisting of two force directions (i.e., pressure force and shear force) and two force-applied surfaces (i.e., touch surface and back surface) in a single device. Force detection is achieved using the smartphone's built-in magnetometer (and supplementary accelerometer and gyro sensor) by estimating the displacement of a magnet attached to a 3-DoF passively movable panel at the back. We conducted a user study (N=12) to investigate the fundamental user performance by our interface and demonstrated that the input was detected as intended with a success rate of 97.4% on average for all four input types. We further conducted an ideation workshop with people who were involved in human-computer interaction (N=12) to explore possible applications of this interface, and we obtained 137 ideas for applications using individual input types and 51 possible scenarios using them in combination. Organizing these ideas reveals the advantages of each input type and suggests that our interface is useful for applications that require complex operations and that it can improve intuitiveness through elastic feedback.</jats:p>
SP  - 185
EP  - 206
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567717
ER  - 

TY  - JOUR
AU  - Tapia, Miguel Chávez; Xu, Talia; Wu, Zehang; Zamalloa, Marco Zúñiga
TI  - SunBox
PY  - 2022
AB  - <jats:p>A recent development in wireless communication is the use of optical shutters and smartphone cameras to create optical links solely from ambient light. At the transmitter, a liquid crystal display (LCD) modulates ambient light by changing its level of transparency. At the receiver, a smartphone camera decodes the optical pattern. This LCD-to-camera link requires low-power levels at the transmitter, and it is easy to deploy because it does not require modifying the existing lighting infrastructure. The system, however, provides a low data rate, of just a few tens of bps. This occurs because the LCDs used in the state-of-the-art are slow single-pixel transmitters. To overcome this limitation, we introduce a novel multi-pixel display. Our display is similar to a simple screen, but instead of using embedded LEDs to radiate information, it uses only the surrounding ambient light. We build a prototype, called SunBox, and evaluate it indoors and outdoors with both, artificial and natural ambient light. Our results show that SunBox can achieve a throughput between 2 kbps and 10 kbps using a low-end smartphone camera with just 30 FPS. To the best of our knowledge, this is the first screen-to-camera system that works solely with ambient light.</jats:p>
SP  - 1
EP  - 26
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534602
ER  - 

TY  - BOOK
AU  - Strak, Robin; Yu, Kevin; Pankratz, Frieder; Lazarovici, Marc; Sandmeyer, Benedikt; Reichling, Julia; Weidert, Simon; Kraetsch, Clemens; Roegele, Barbara; Navab, Nassir; Eck, Ulrich; Roth, Daniel
TI  - Mensch und Computer - Comparison Between Video-mediated and Asymmetric 3D Teleconsultation During a Preclinical Scenario
PY  - 2021
AB  - Current teleconsultation solutions for preclinical emergencies can transmit knowledge from a remote expert to a local paramedic using audio and 2D video channels. Such technology lacks precision and efficiency for medical diagnostic tasks, and visual feedback is often missing between participants. We investigate a mixed reality 3D teleconsultation solution for preclinical use, which provides a 3D reconstruction of the local scene to a remote expert, displayed in Virtual Reality. A remote expert can join the local scene virtually as an Augmented Reality avatar. The remote expert can annotate the local scene and guide the local paramedics through the procedure. We explored our system in a user study within a preclinical scenario on a collaborative task of attaching chest lead electrodes of a 12 channel electrocardiogram on a mannequin. We compared the 3D teleconsultation system to a 2D video-mediated teleconsultation via a top-mounted camera and report results from the consultee side in AR. Based on our empirical user study with 10 paramedics with an average of 17 years experience, we observe an improvement in the electrode placement quality using the 3D teleconsultation system. Results indicate no significant difference in the cognitive task-load between conditions. Participants perceived the video-mediated consultation as more usable due to their unfamiliarity with the 3D teleconsultation system. However, participants acknowledge the potential of 3D teleconsultation and believe such a system can significantly improve the preclinical treatment.
SP  - 227
EP  - 235
JF  - Mensch und Computer 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3473856.3473883
ER  - 

TY  - CONF
AU  - Yamanaka, Shota
TI  - Utility of Crowdsourced User Experiments for Measuring the Central Tendency of User Performance to Evaluate Error-Rate Models on GUIs
PY  - 2021
AB  - The usage of crowdsourcing to recruit numerous participants has been recognized as beneficial in the human-computer interaction (HCI) field, such as for designing user interfaces and validating user performance models. In this work, we investigate its effectiveness for evaluating an error-rate prediction model in target pointing tasks. In contrast to models for operational times, a clicking error (i.e., missing a target) occurs by chance at a certain probability, e.g., 5%. Therefore, in traditional laboratory-based experiments, a lot of repetitions are needed to measure the central tendency of error rates. We hypothesize that recruiting many workers would enable us to keep the number of repetitions per worker much smaller. We collected data from 384 workers and found that existing models on operational time and error rate showed good fits (both R^2 > 0.95). A simulation where we changed the number of participants N_P and the number of repetitions N_repeat showed that the time prediction model was robust against small N_P and N_repeat, although the error-rate model fitness was considerably degraded. These findings empirically demonstrate a new utility of crowdsourced user experiments for collecting numerous participants, which should be of great use to HCI researchers for their evaluation studies.
SP  - 155
EP  - 165
JF  - NA
VL  - 9
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Rajaram, Shwetha; Nebeling, Michael
TI  - Paper Trail: An Immersive Authoring System for Augmented Reality Instructional Experiences
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517486
ER  - 

TY  - NA
AU  - Zhao, Maozheng; Huang, Henry; Li, Zhi; Liu, Rui; Cui, Wenzhe; Toshniwal, Kajal; Goel, Ananya; Wang, Andrew; Zhao, Xia; Rashidian, Sina; Baig, Furqan; Phi, Khiem; Zhai, Shumin; Ramakrishnan, IV; Wang, Fusheng; Bi, Xiaojun
TI  - EyeSayCorrect: Eye Gaze and Voice Based Hands-free Text Correction for Mobile Devices
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 27th International Conference on Intelligent User Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490099.3511103
ER  - 

TY  - CHAP
AU  - Ganapathi, Priya; Sorathia, Keyur
TI  - Analysis of Body-Gestures Elucidated Through Elicitation Study for Natural Locomotion in Virtual Reality
PY  - 2022
AB  - NA
SP  - 1313
EP  - 1326
JF  - Ergonomics for Design and Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-94277-9_112
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Blikstein, Paulo; Holbert, Nathan R; Worsley, Marcelo; Jacobs, Jennifer; Anderson, Fraser; Gong, Jun; DesPortes, Kayla; Mueller, Stefanie
TI  - Reimagining Systems for Learning Hands-on Creative and Maker Skills
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems Extended Abstracts
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491101.3503732
ER  - 

TY  - NA
AU  - Orlosky, Jason; Sra, Misha; Bektaş, Kenan; Peng, Huaishu; Kim, Jeeeun; Kos'myna, Nataliya; Höllerer, Tobias; Steed, Anthony; Kiyokawa, Kiyoshi; Akşit, Kaan
TI  - Telelife: The Future of Remote Living
PY  - 2021
AB  - In recent years, everyday activities such as work and socialization have steadily shifted to more remote and virtual settings. With the COVID-19 pandemic, the switch from physical to virtual has been accelerated, which has substantially affected various aspects of our lives, including business, education, commerce, healthcare, and personal life. This rapid and large-scale switch from in-person to remote interactions has revealed that our current technologies lack functionality and are limited in their ability to recreate interpersonal interactions. To help address these limitations in the future, we introduce "Telelife," a vision for the near future that depicts the potential means to improve remote living better aligned with how we interact, live and work in the physical world. Telelife encompasses novel synergies of technologies and concepts such as digital twins, virtual prototyping, and attention and context-aware user interfaces with innovative hardware that can support ultrarealistic graphics, user state detection, and more. These ideas will guide the transformation of our daily lives and routines soon, targeting the year 2035. In addition, we identify opportunities across high-impact applications in domains related to this vision of Telelife. Along with a recent survey of relevant fields such as human-computer interaction, pervasive computing, and virtual reality, the directions outlined in this paper will guide future research on remote living.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Gil, Hyunjae; Oakley, Ian
TI  - ThumbAir
PY  - 2022
AB  - <jats:p>Typing while wearing a standalone Head Mounted Display (HMD)---systems without external input devices or sensors to support text entry---is hard. To address this issue, prior work has used external trackers to monitor finger movements to support in-air typing on virtual keyboards. While performance has been promising, current systems are practically infeasible: finger movements may be visually occluded from inside-out HMD based tracking systems or, otherwise, awkward and uncomfortable to perform. To address these issues, this paper explores an alternative approach. Taking inspiration from the prevalence of thumb-typing on mobile phones, we describe four studies exploring, defining and validating the performance of ThumbAir, an in-air thumb-typing system implemented on a commercial HMD. The first study explores viable target locations, ultimately recommending eight targets sites. The second study collects performance data for taps on pairs of these targets to both inform the design of a target selection procedure and also support a computational design process to select a keyboard layout. The final two studies validate the selected keyboard layout in word repetition and phrase entry tasks, ultimately achieving final WPMs of 27.1 and 13.73. Qualitative data captured in the final study indicate that the discreet movements required to operate ThumbAir, in comparison to the larger scale finger and hand motions used in a baseline design from prior work, lead to reduced levels of perceived exertion and physical demand and are rated as acceptable for use in a wider range of social situations.</jats:p>
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569474
ER  - 

TY  - JOUR
AU  - Xia, Haijun; Glueck, Michael; Annett, Michelle; Wang, Michael; Wigdor, Daniel
TI  - Iteratively Designing Gesture Vocabularies: A Survey and Analysis of Best Practices in the HCI Literature
PY  - 2022
AB  - <jats:p>Gestural interaction has evolved from a set of novel interaction techniques developed in research labs, to a dominant interaction modality used by millions of users everyday. Despite its widespread adoption, the design of appropriate gesture vocabularies remains a challenging task for developers and designers. Existing research has largely used Expert-Led, User-Led, or Computationally-Based methodologies to design gesture vocabularies. These methodologies leverage the expertise, experience, and capabilities of experts, users, and systems to fulfill different requirements. In practice, however, none of these methodologies provide designers with a complete, multi-faceted perspective of the many factors that influence the design of gesture vocabularies, largely because a singular set of factors has yet to be established. Additionally, these methodologies do not identify or emphasize the subset of factors that are crucial to consider when designing for a given use case. Therefore, this work reports on the findings from an exhaustive literature review that identified 13 factors crucial to gesture vocabulary design and examines the evaluation methods and interaction techniques commonly associated with each factor. The identified factors also enable a holistic examination of existing gesture design methodologies from a factor-oriented viewpoint and highlighting the strengths and weaknesses of each methodology. This work closes with proposals of future research directions of developing an iterative user-centered and factor-centric gesture design approach as well as establishing an evolving ecosystem of factors that are crucial to gesture design.</jats:p>
SP  - 1
EP  - 54
JF  - ACM Transactions on Computer-Human Interaction
VL  - 29
IS  - 4
PB  - 
DO  - 10.1145/3503537
ER  - 

TY  - NA
AU  - Turakhia, Dishita G; Mueller, Stefanie; DesPortes, Kayla
TI  - Identifying Game Mechanics for Integrating Fabrication Activities within Existing Digital Games
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517721
ER  - 

TY  - JOUR
AU  - Hossain, Tahera; Shen, Wanggang; Antar, Anindya Das; Prabhudesai, Snehal; Inoue, Sozo; Huan, Xun; Banovic, Nikola
TI  - A Bayesian Approach for Quantifying Data Scarcity when Modeling Human Behavior via Inverse Reinforcement Learning
PY  - 2022
AB  - <jats:p> Computational models that formalize complex human behaviors enable study and understanding of such behaviors. However, collecting behavior data required to estimate the parameters of such models is often tedious and resource intensive. Thus, estimating dataset size as part of data collection planning (also known as Sample Size Determination) is important to reduce the time and effort of behavior data collection while maintaining an accurate estimate of model parameters. In this paper, we present a sample size determination method based on Uncertainty Quantification (UQ) for a specific Inverse Reinforcement Learning (IRL) model of human behavior, in two cases: 1) <jats:italic>pre-hoc</jats:italic> experiment design—conducted in the planning stage before any data is collected, to guide the estimation of how many samples to collect; and 2) <jats:italic>post-hoc</jats:italic> dataset analysis—performed after data is collected, to decide if the existing dataset has sufficient samples and whether more data is needed. We validate our approach in experiments with a realistic model of behaviors of people with Multiple Sclerosis (MS) and illustrate how to pick a reasonable sample size target. Our work enables model designers to perform a deeper, principled investigation of effects of dataset size on IRL model parameters. </jats:p>
SP  - NA
EP  - NA
JF  - ACM Transactions on Computer-Human Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3551388
ER  - 

TY  - JOUR
AU  - Cai, Haofan; Wang, Ge; Shi, Xiaofeng; Xie, Junjie; Wang, Minmei; Qian, Chen; Chen, Shigang
TI  - When Tags 'Read' Each Other: Enabling Low-Cost and Convenient Tag Mutual Identification
PY  - 2022
AB  - <jats:p>Though widely used in industrial and logistic applications, current passive Radio Frequency Identification (RFID) technology still has a fundamental limitation: Individual users who do not carry any reader find it difficult to interact with tagged items, such as retrieving their digital profiles and requesting certain associations with them. Recent proposals to improve the user–item interaction experience rely on special hardware, such as a smartphone-based RFID scanner. This work presents a promising approach to allowing each user to interact with a tagged item using only one passive tag, which is named the Tag Mutual Identification Interface (TagMii). TagMii requires a user to put one’s user tag in physical proximity with an item tag to express certain interactions between the user and item. The key idea behind TagMii is to utilize two experimental observations: (1) inductive coupling for detecting interaction events, and (2) channel similarity for determining the actual interacting tags. We implement TagMii using commodity off-the-shelf RFID devices and conduct experiments in complex environments with rich multipath, mobility, wireless signals, electrical devices, and magnetic fields. The results show that TagMii provides accurate mutual identification. TagMii is a completely new approach for user–item interactions in pervasive environments and enables many user-friendly Internet of Things applications with low cost and convenience.</jats:p>
SP  - 1
EP  - 22
JF  - ACM Transactions on Sensor Networks
VL  - 18
IS  - 2
PB  - 
DO  - 10.1145/3494541
ER  - 

TY  - NA
AU  - Landwehr Sydow, Sophie; Jonsson, Martin; Tholander, Jakob
TI  - Modding the Pliable Machine: Unpacking the Creative and Social Practice of Upkeep at the Makerspace
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Creativity and Cognition
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3527927.3532804
ER  - 

TY  - CHAP
AU  - Fischer, Roland; Mühlenbrock, Andre; Kulapichitr, Farin; Uslar, Verena Nicole; Weyhe, Dirk; Zachmann, Gabriel
TI  - Evaluation of Point Cloud Streaming and Rendering for VR-Based Telepresence in the OR
PY  - 2022
AB  - AbstractImmersive and high-quality VR-based telepresence systems could be of great benefit in the medical field and the operating room (OR) specifically, as they allow distant experts to interact with each other and to assist local doctors as if they were physically present. Despite recent advances in VR technology, and more telepresence systems making use of it, most of the current solutions in use in health care (if any), are just video-based and don’t provide the feeling of presence or spatial awareness, which are highly important for tasks such as remote consultation, -supervision, and -teaching. Reasons still holding back VR telepresence systems are high demands regarding bandwidth and computational power, subpar visualization quality, and complicated setups. We propose an easy-to-set-up telepresence system that enables remote experts to meet in a multi-user virtual operating room, view live-streamed and 3D-visualized operations, interact with each other, and collaboratively explore medical data. Our system is based on Azure Kinect RGB-D cameras, a point cloud streaming pipeline, and fast point cloud rendering methods integrated into a state-of-the-art 3D game engine. Remote experts are visualized via personalized real-time 3D point cloud avatars. For this, we have developed a high-speed/low-latency multi-camera point cloud streaming pipeline including efficient filtering and compression. Furthermore, we have developed splatting-based and mesh-based point cloud rendering solutions and integrated them into the Unreal Engine 4. We conducted two user studies with doctors and medical students to evaluate our proposed system, compare the rendering solutions, and highlight our system’s capabilities.KeywordsTelepresenceVirtual RealityCollaborative VRPoint cloudAvatarPoint cloud renderingUnreal EngineAzure KinectMesh reconstruction
SP  - 89
EP  - 110
JF  - Virtual Reality and Mixed Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-16234-3_6
ER  - 

TY  - NA
AU  - Skreinig, Lucchas Ribeiro; Stanescu, Ana; Mori, Shohei; Heyen, Frank; Mohr, Peter; Sedlmair, Michael; Schmalstieg, Dieter; Kalkofen, Denis
TI  - AR Hero: Generating Interactive Augmented Reality Guitar Tutorials
PY  - 2022
AB  - We introduce a system capable of generating interactive Aug-mented Reality guitar tutorials by parsing common digital guitar tablature and by capturing the performance of an expert using a multi-camera array. Instructions are presented to the user in an Augmented Reality application using either an abstract visualization, a 3D virtual hand, or a 3D video. To support individual users at different skill levels the system provides full control of the play-back of a tutorial, including its speed and looping behavior, while delivering live feedback on the user&#x0027;s performance.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw55335.2022.00086
ER  - 

TY  - NA
AU  - Murseli, Ella; Mandic, Lidija; Pibernik, Jesenka; Srdic, Ida
TI  - AR application for education
PY  - 2022
AB  - This paper presents the application of augmented reality, AR, for education. The application is intended for high school students with the aim of raising the user experience that enhances the adoption of materials using smartphones. The AR application is based on the visualization of a eukaryotic cell in the form of a virtual 3D model. The research was conducted in the Classical Gymnasium in Zagreb in the first grade.
SP  - NA
EP  - NA
JF  - 2022 International Symposium ELMAR
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/elmar55880.2022.9899791
ER  - 

TY  - NA
AU  - Roumen, Thijs; Apel, Ingo; Kern, Thomas; Taraz, Martin; Sharma, Ritesh; Schlueter, Ole; Johnson, Jeffrey; Meier, Dominik; Lempert, Conrad; Baudisch, Patrick
TI  - Structure-Preserving Editing of Plates and Volumes for Laser Cutting
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3561996
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Ikematsu, Kaori; Siio, Itiro
TI  - Designing Electrolysis Ion Display on Everyday Open Wet Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Designing Interactive Systems Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3532106.3533574
ER  - 

TY  - NA
AU  - Jun, Eunice; Seo, Audrey; Heer, Jeffrey; Just, René
TI  - Tisane: Authoring Statistical Models via Formal Reasoning from Conceptual and Data Relationships
PY  - 2022
AB  - Proper statistical modeling incorporates domain theory about how concepts relate and details of how data were measured. However, data analysts currently lack tool support for recording and reasoning about domain assumptions, data collection, and modeling choices in an integrated manner, leading to mistakes that can compromise scientific validity. For instance, generalized linear mixed-effects models (GLMMs) help answer complex research questions, but omitting random effects impairs the generalizability of results. To address this need, we present Tisane, a mixed-initiative system for authoring generalized linear models with and without mixed-effects. Tisane introduces a study design specification language for expressing and asking questions about relationships between variables. Tisane contributes an interactive compilation process that represents relationships in a graph, infers candidate statistical models, and asks follow-up questions to disambiguate user queries to construct a valid model. In case studies with three researchers, we find that Tisane helps them focus on their goals and assumptions while avoiding past mistakes.
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501888
ER  - 

TY  - JOUR
AU  - Wadekar, Paras; Amanatides, Chelsea; Kapllani, Levi; Dion, Genevieve; Kamien, Randall D.; Breen, David E.
TI  - Geometric modeling of complex knitting stitches using a bicontinuous surface and its offsets
PY  - 2021
AB  - NA
SP  - 102024
EP  - NA
JF  - Computer Aided Geometric Design
VL  - 89
IS  - NA
PB  - 
DO  - 10.1016/j.cagd.2021.102024
ER  - 

TY  - JOUR
AU  - Bhatia, Arpit; Ahuja, Yajur; Agarwal, Suyash; Parnami, Aman
TI  - Exploring the Design Space of Badge Based Input
PY  - 2020
AB  - In this paper, we explore input with wearables that can be attached and detached at will from any of our regular clothes. These wearables do not cause any permanent effect on our clothing and are suitable to be worn anywhere, thus making them very similar to badges we wear. To explore this idea of non-permanent badge input, we studied various methods to fasten objects to our clothing and organise them in the form of a design space. We leverage this synthesis, along with literature and existing products to present possible interaction gestures these badge-based wearables can enable.
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427313
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Tanaka, Namiki; Siio, Itiro
TI  - SIGGRAPH ASIA Emerging Technologies - Bubble Mirror: An Interactive Face Image Display Using Electrolysis Bubbles
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - SIGGRAPH Asia 2020 Emerging Technologies
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3415255.3422890
ER  - 

TY  - JOUR
AU  - Scheidt, Fabian; Ou, Jifei; Ishii, Hiroshi; Meisen, Tobias
TI  - deepKnit: Learning-based Generation of Machine Knitting Code
PY  - 2020
AB  - Abstract Modern knitting machines allow the manufacturing of various textile products with complex surface structures and patterns. However, programming these machines requires expert knowledge due to constraints of the process and the programming language. We present a long short-term memory (LSTM) based deep learning model that generates low-level code of novel knitting patterns based on high-level style specifications. To be processable by our model, we describe knitting instructions as one-dimensional sequences of tokens, which diverts from image-based approaches reported in previous research. We integrate our model into a design tool, that allows to assemble the atomic patterns to bigger swatches or garments. To evaluate our approach quantitatively, we formalize the requirements for patterns to be syntactically correct and valid to manufacture. Although our generated patterns look more random and seem to resemble less to human patterns, our evaluation shows that their knittability is orders of magnitudes better than randomly generated patterns.
SP  - 485
EP  - 492
JF  - Procedia Manufacturing
VL  - 51
IS  - NA
PB  - 
DO  - 10.1016/j.promfg.2020.10.068
ER  - 

TY  - JOUR
AU  - Kapllani, Levi; Amanatides, Chelsea; Dion, Genevieve; Shapiro, Vadim; Breen, David E.
TI  - TopoKnit: A Process-Oriented Representation for Modeling the Topology of Yarns in Weft-Knitted Textiles
PY  - 2021
AB  - Abstract Machine knitted textiles are complex multi-scale material structures increasingly important in many industries, including consumer products, architecture, composites, medical, and military. Computational modeling, simulation, and design of industrial fabrics require efficient representations of the spatial, material, and physical properties of such structures. We propose a process-oriented representation, TopoKnit, that defines a foundational data structure for representing the topology of weft-knitted textiles at the yarn scale. Process space serves as an intermediary between the machine and fabric spaces, and supports a concise, computationally efficient evaluation approach based on on-demand, near constant-time queries. In this paper, we define the properties of the process space, and design a data structure to represent it and algorithms to evaluate it. We demonstrate the effectiveness of the representation scheme by providing results of evaluations of the data structure in support of common topological operations in the fabric space.
SP  - 101114
EP  - NA
JF  - Graphical Models
VL  - 118
IS  - NA
PB  - 
DO  - 10.1016/j.gmod.2021.101114
ER  - 

TY  - JOUR
AU  - Murata, Atsuo; Doi, Toshihisa; Kageyama, Kazushi; Karwowski, Waldemar
TI  - Development of an Eye-Gaze Input System With High Speed and Accuracy through Target Prediction Based on Homing Eye Movements
PY  - 2021
AB  - In this study, a method to predict a target on the basis of the trajectory of eye movements and to increase the pointing speed while maintaining high predictive accuracy is proposed. First, a predictive method based on ballistic (fast) eye movements (Approach 1) was evaluated in terms of pointing speed and predictive accuracy. In Approach 1, the so-called Midas touch problem (pointing to an unintended target) occurred, particularly when a small number of samples was used to predict a target. Therefore, to overcome the poor predictive accuracy of Approach 1, we developed a new predictive method (Approach 2) using homing (slow) eye movements rather than ballistic (fast) eye movements. Approach 2 overcame the disadvantage (inaccurate prediction) of Approach 1 by shortening the pointing time while maintaining high predictive accuracy.
SP  - 22688
EP  - 22697
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3055514
ER  - 

TY  - BOOK
AU  - Sidenmark, Ludwig; Mardanbegi, Diako; Gomez, Argenis Ramirez; Clarke, Christopher; Gellersen, Hans
TI  - ETRA - BimodalGaze: Seamlessly Refined Pointing with Gaze and Filtered Gestural Head Movement
PY  - 2020
AB  - Eye gaze is a fast and ergonomic modality for pointing but limited in precision and accuracy. In this work, we introduce BimodalGaze, a novel technique for seamless head-based refinement of a gaze cursor. The technique leverages eye-head coordination insights to separate natural from gestural head movement. This allows users to quickly shift their gaze to targets over larger fields of view with naturally combined eye-head movement, and to refine the cursor position with gestural head movement. In contrast to an existing baseline, head refinement is invoked automatically, and only if a target is not already acquired by the initial gaze shift. Study results show that users reliably achieve fine-grained target selection, but we observed a higher rate of initial selection errors affecting overall performance. An in-depth analysis of user performance provides insight into the classification of natural versus gestural head movement, for improvement of BimodalGaze and other potential applications.
SP  - 1
EP  - 9
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379155.3391312
ER  - 

TY  - NA
AU  - Chung, John Joon Young; He, Shiqing; Adar, Eytan
TI  - Conference on Designing Interactive Systems - The Intersection of Users, Roles, Interactions, and Technologies in Creativity Support Tools
PY  - 2021
AB  - Creativity Support Tools (CSTs) have become an integral part of artistic creation. The range of CST technologies is broad—from fabricators to generative algorithms to robots. The interaction approaches for CSTs are accordingly broad. CSTs combine specific technologies and interaction types to serve a spectrum of roles and users. In this work, we tackle a comprehensive understanding of how the intersections of users, roles, interactions, and technologies form a design space for CSTs. We accomplish this by reviewing 111 art-creation CSTs from HCI and computing research and analyzing how diverse aspects of CSTs relate to each other. Our findings identify patterns for designing CSTs, which can give guidance to future CST designers. We also highlight under-explored types of CSTs within the HCI community, providing future directions that CST researchers can pursue given the current trajectory of technological advancement. This work contributes an integrating perspective to understand the landscape of art-creation CSTs.
SP  - 1817
EP  - 1833
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462050
ER  - 

TY  - NA
AU  - Makin, Lawrence; Barnaby, Gareth; Roudaut, Anne
TI  - IHM - Tactile and kinesthetic feedbacks improve distance perception in virtual reality
PY  - 2019
AB  - Research spanning psychology, neuroscience and HCI found that depth perception distortion is a common problem in virtual reality. This distortion results in depth compression, where users perceive objects closer than their intended distance. Studies suggested that cues, such as audio and haptic, help to solve this issue. We focus on haptic feedback and investigate how force feedback compares to tactile feedback within peripersonal space in reducing depth perception distortion. Our study (N=12) compares the use of haptic force feedback, vibration haptic feedback, a combination of both or no feedback. Our results show that both vibration and force feedback improve depth perception distortion over no feedback (8.3 times better distance estimation than with no haptic feedback vs. 1.4 to 1.5 times better with either vibration or force feedback on their own). Participants also subjectively preferred using force feedback, or a combination of force and vibration feedback, over no feedback.
SP  - NA
EP  - NA
JF  - Proceedings of the 31st Conference on l'Interaction Homme-Machine
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3366550.3372248
ER  - 

TY  - JOUR
AU  - Nader, Georges; Quek, Yu Han; Chia, Pei Zhi; Weeger, Oliver; Yeung, Sai-Kit
TI  - KnitKit: a flexible system for machine knitting of customizable textiles
PY  - 2021
AB  - In this work, we introduce KnitKit, a flexible and customizable system for the computational design and production of functional, multi-material, and three-dimensional knitted textiles. Our system greatly simplifies the knitting of 3D objects with complex, varying patterns that use multiple yarns and stitch patterns by separating the high-level design specification in terms of geometry, stitch patterns, materials or colors from the low-level, machine-specific knitting instruction generation. Starting from a triangular 3D mesh and a 2D texture that specifies knitting patterns on top of the geometry, our system generates the required machine instructions in three major steps. First, the input is processed and the KnitNet data structure is generated. This graph structure serves as an abstract interface between the high-level geometric and knitting configuration and the low-level, machine-specific knitting instructions. Second, a graph rewriting procedure is applied on the KnitNet that produces a sequence of abstract machine actions. Finally, the low-level machine instructions are generated by adapting those abstract actions to a specific machine context. We showcase the potential of this computational approach by designing and fabricating a variety of objects with complex geometries, multiple yarns, and multiple stitch patterns.
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3450626.3459790
ER  - 

TY  - CHAP
AU  - Hayashida, Nozomi; Matsuyama, Hitoshi; Aoki, Shunsuke; Yonezawa, Takuro; Kawaguchi, Nobuo
TI  - HCI (21) - A Gaze-Based Unobstructive Information Selection by Context-Aware Moving UI in Mixed Reality
PY  - 2021
AB  - Mixed reality (MR) technology has been attracting attention in the automobile industry and logistics industry for work training and remote work support for newcomers. However, work support using MR technology has the problem that displaying too much information obstructs the user’s field of vision and rather interferes with the work. Therefore, it is necessary to detect the user’s intention and provide only the information that the user wants. In addition, the system should be able to detect naturally from the user’s behavior without interrupting the work, which the user explicitly selects. To solve these problems, in this paper, we use the user’s gaze to determine whether to display content by estimating whether or not they are looking at information. Another problem with gaze-based research in MR work support is that it is difficult to determine which space you are looking at in a space where virtual space and real space intersect. In order to overcome this problem, in this research, we grasp the user’s behavior from the movement of the user’s gaze, and we suggest a way to determine make the user interface (UI) the movement of the gaze that is unlikely to occur during that behavior to see if the user is looking at the UI. As a result of the experiment, we were able to promote the movement of gaze that is different from the characteristics of the movement of gaze during work with the proposed UI movement.
SP  - 301
EP  - 315
JF  - Distributed, Ambient and Pervasive Interactions
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-77015-0_22
ER  - 

TY  - JOUR
AU  - Park, Kyeong-Beom; Choi, Sung Ho; Lee, Jae Yeol; Ghasemi, Yalda; Mohammed, Mustafa; Jeong, Heejin
TI  - Hands-Free Human–Robot Interaction Using Multimodal Gestures and Deep Learning in Wearable Mixed Reality
PY  - 2021
AB  - This study proposes a novel hands-free interaction method using multimodal gestures such as eye gazing and head gestures and deep learning for human-robot interaction (HRI) in mixed reality (MR) environments. Since human operators hold some objects for conducting tasks, there are many constrained situations where they cannot use their hands for HRI interactions. To provide more effective and intuitive task assistance, the proposed hands-free method supports coarse-to-fine interactions. Eye gazing-based interaction is used for coarse interactions such as searching and previewing of target objects, and head gesture interactions are used for fine interactions such as selection and 3D manipulation. In addition, deep learning-based object detection is applied to estimate the initial positioning of physical objects to be manipulated by the robot. The result of object detection is then combined with 3D spatial mapping in the MR environment for supporting accurate initial object positioning. Furthermore, virtual object-based indirect manipulation is proposed to support more intuitive and efficient control of the robot, compared with traditional direct manipulation (e.g., joint-based and end effector-based manipulations). In particular, a digital twin, the synchronized virtual robot of the real robot, is used to provide a preview and simulation of the real robot to manipulate it more effectively and accurately. Two case studies were conducted to confirm the originality and advantages of the proposed hands-free HRI: (1) performance evaluation of initial object positioning and (2) comparative analysis with traditional direct robot manipulations. The deep learning-based initial positioning reduces much effort for robot manipulation using eye gazing and head gestures. The object-based indirect manipulation also supports more effective HRI than previous direct interaction methods.
SP  - 55448
EP  - 55464
JF  - IEEE Access
VL  - 9
IS  - NA
PB  - 
DO  - 10.1109/access.2021.3071364
ER  - 

TY  - JOUR
AU  - Kwolek, Bogdan; Baczynski, Wojciech; Sako, Shinji
TI  - Recognition of JSL fingerspelling using Deep Convolutional Neural Networks
PY  - 2021
AB  - NA
SP  - 586
EP  - 598
JF  - Neurocomputing
VL  - 456
IS  - NA
PB  - 
DO  - 10.1016/j.neucom.2021.03.133
ER  - 

TY  - NA
AU  - Oh, Seo Young; Yoon, Boram; Kim, Hyung-il; Woo, Woontack
TI  - CHI Extended Abstracts - Finger Contact in Gesture Interaction Improves Time-domain Input Accuracy in HMD-based Augmented Reality
PY  - 2020
AB  - This paper reports that the time-domain accuracy of bare-hand interactions in HMD-based Augmented Reality can be improved by using finger contact: touching a finger with another or tapping one's own hand. The activation of input can be precisely defined by the moment of finger contact, allowing the user to perform the input precisely at the desired moment. Finger contact is better suited to the user's mental model, and natural tactile feedback from the fingertip also benefits the user with the self-perception of the input. The experimental results revealed that using finger contact is the preferred method of input that increases the time-domain accuracy and enables the user to be aware of the moment the input is activated.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383098
ER  - 

TY  - JOUR
AU  - Cornelio, Patricia; Velasco, Carlos; Obrist, Marianna
TI  - Multisensory Integration as per Technological Advances: A Review.
PY  - 2021
AB  - Multisensory integration research has allowed us to better understand how humans integrate sensory information to produce a unitary experience of the external world. However, this field is often challenged by the limited ability to deliver and control sensory stimuli, especially when going beyond audio-visual events and outside laboratory settings. In this review, we examine the scope and challenges of new technology in the study of multisensory integration in a world that is increasingly characterized as a fusion of physical and digital/virtual events. We discuss multisensory integration research through the lens of novel multisensory technologies and, thus, bring research in human-computer interaction, experimental psychology, and neuroscience closer together. Today, for instance, displays have become volumetric so that visual content is no longer limited to 2D screens, new haptic devices enable tactile stimulation without physical contact, olfactory interfaces provide users with smells precisely synchronized with events in virtual environments, and novel gustatory interfaces enable taste perception through levitating stimuli. These technological advances offer new ways to control and deliver sensory stimulation for multisensory integration research beyond traditional laboratory settings and open up new experimentations in naturally occurring events in everyday life experiences. Our review then summarizes these multisensory technologies and discusses initial insights to introduce a bridge between the disciplines in order to advance the study of multisensory integration.
SP  - 652611
EP  - 652611
JF  - Frontiers in neuroscience
VL  - 15
IS  - NA
PB  - 
DO  - 10.3389/fnins.2021.652611
ER  - 

TY  - JOUR
AU  - Loos, Cornelia; Napoli, Donna Jo
TI  - Expanding Echo: Coordinated Head Articulations as Nonmanual Enhancements in Sign Language Phonology.
PY  - 2021
AB  - Echo phonology was originally proposed to account for obligatory coordination of manual and mouth articulations observed in several sign languages. However, previous research into the phenomenon lacks clear criteria for which components of movement can or must be copied when the articulators are so different. Nor is there discussion of which nonmanual articulators can echo manual movement. Given the prosodic properties of echoes (coordination of onset/offset and of dynamics such as speed) as well as general motoric coordination of various articulators in the human body, we expect that the mouth is not the only nonmanual articulator involved in echo phonology. In this study, we look at a fixed set of lexical items across 36 sign languages and establish that the head can echo manual movement with respect to timing and to the axis/axes of manual movement. We propose that what matters in echo phonology is the visual percept of temporally coordinated movement that repeats a salient movement property in such a way as to give the visual impression of a copy. Our findings suggest that echoes are not obligatory motor couplings of two or more articulators but may enhance phonological distinctions that are otherwise difficult to see.
SP  - e12958
EP  - NA
JF  - Cognitive science
VL  - 45
IS  - 5
PB  - 
DO  - 10.1111/cogs.12958
ER  - 

TY  - JOUR
AU  - Azmandian, Mahdi; Yahata, Rhys; Grechkin, Timofey; Thomas, Jerald; Rosenberg, Evan Suma
TI  - Validating Simulation-Based Evaluation of Redirected Walking Systems.
PY  - 2022
AB  - Developing effective strategies for redirected walking requires extensive evaluations across a variety of factors that influence performance. Because these large-scale experiments are often not practical with user studies, researchers have instead utilized simulations to systematically test different algorithm parameters, physical space configurations, and virtual walking paths. Although simulation offers an efficient way to evaluate redirected walking algorithms, it remains an open question whether this evaluation methodology is ecologically valid. In this paper, we investigate the interaction between locomotion behavior and redirection gains at a micro-level (across small path segments) and macro-level (across an entire experience). This examination involves analyzing data from real users and comparing algorithm performance metrics with a simulated user model. The results identify specific properties of user locomotion behavior that influence the application of redirected walking gains and resets. Overall, we found that the simulation provided a conservative estimate of the average performance with real users and observed that performance trends when comparing two redirected walking algorithms were preserved. In general, these results indicate that simulation is an empirically valid evaluation methodology for redirected walking algorithms.
SP  - 2288
EP  - 2298
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 5
PB  - 
DO  - 10.1109/tvcg.2022.3150466
ER  - 

TY  - NA
AU  - Grubert, Jens
TI  - Mixed Reality Interaction Techniques.
PY  - 2021
AB  - This chapter gives an overview of interaction techniques for mixed reality including augmented and virtual reality (AR/VR). Various modalities for input and output are discussed. Specifically, techniques for tangible and surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and mouse-based, as well as haptic interaction are discussed. Furthermore, the combination of multiple modalities in multisensory and multimodal interaction, as well as interaction using multiple physical or virtual displays, are presented. Finally, interaction with intelligent virtual agents is considered.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Lystbæk, Mathias N.; Rosenberg, Peter; Pfeuffer, Ken; Grønbæk, Jens Emil; Gellersen, Hans
TI  - Gaze-Hand Alignment
PY  - 2022
AB  - <jats:p>Gaze and freehand gestures suit Augmented Reality as users can interact with objects at a distance without need for a separate input device. We propose Gaze-Hand Alignment as a novel multimodal selection principle, defined by concurrent use of both gaze and hand for pointing and alignment of their input on an object as selection trigger. Gaze naturally precedes manual action and is leveraged for pre-selection, and manual crossing of a pre-selected target completes the selection. We demonstrate the principle in two novel techniques, Gaze&amp;Finger for input by direct alignment of hand and finger raised into the line of sight, and Gaze&amp;Hand for input by indirect alignment of a cursor with relative hand movement. In a menu selection experiment, we evaluate the techniques in comparison with Gaze&amp;Pinch and a hands-only baseline. The study showed the gaze-assisted techniques to outperform hands-only input, and gives insight into trade-offs in combining gaze with direct or indirect, and spatial or semantic freehand gestures.</jats:p>
SP  - 1
EP  - 18
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ETRA
PB  - 
DO  - 10.1145/3530886
ER  - 

TY  - JOUR
AU  - Sidenmark, Ludwig; Gellersen, Hans
TI  - Eye, Head and Torso Coordination During Gaze Shifts in Virtual Reality
PY  - 2019
AB  - Humans perform gaze shifts naturally through a combination of eye, head and body movements. Although gaze has been long studied as input modality for interaction, this has previously ignored the coordination of the eyes, head and body. This article reports a study of gaze shifts in virtual reality aimed to address the gap and inform design. We identify general eye, head and torso coordination patterns and provide an analysis of the relative movements’ contribution and temporal alignment. We quantify effects of target distance, direction and user posture, describe preferred eye-in-head motion ranges and identify a high variability in head movement tendency. Study insights lead us to propose gaze zones that reflect different levels of contribution from eye, head and body. We discuss design implications for HCI and VR, and in conclusion argue to treat gaze as multimodal input, and eye, head and body movement as synergetic in interaction design.
SP  - 1
EP  - 40
JF  - ACM Transactions on Computer-Human Interaction
VL  - 27
IS  - 1
PB  - 
DO  - 10.1145/3361218
ER  - 

TY  - JOUR
AU  - Pandey, Laxmi; Arif, Ahmed Sabbir
TI  - Design and Evaluation of a Silent Speech-Based Selection Method for Eye-Gaze Pointing
PY  - 2022
AB  - <jats:p>We investigate silent speech as a hands-free selection method in eye-gaze pointing. We first propose a stripped-down image-based model that can recognize a small number of silent commands almost as fast as state-of-the-art speech recognition models. We then compare it with other hands-free selection methods (dwell, speech) in a Fitts' law study. Results revealed that speech and silent speech are comparable in throughput and selection time, but the latter is significantly more accurate than the other methods. A follow-up study revealed that target selection around the center of a display is significantly faster and more accurate, while around the top corners and the bottom are slower and error prone. We then present a method for selecting menu items with eye-gaze and silent speech. A study revealed that it significantly reduces task completion time and error rate.</jats:p>
SP  - 328
EP  - 353
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 6
IS  - ISS
PB  - 
DO  - 10.1145/3567723
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Fukushima, Manaka; Matoba, Yasushi; Siio, Itiro
TI  - CHI Extended Abstracts - UTAKATA: Floating Bubble Display
PY  - 2020
AB  - We propose UTAKATA, an ephemeral display device that presents information by using floating clusters of bubbles. In our previous study, we implemented an electrolysis bubble display using drinkable beverages. Although clear clusters of bubbles could be created in short times, they did not disappear for a long time; consequently, the refresh time period of the display was considerably long. To overcome this deficiency, we present UTAKATA, a ticker-like bubble display using a running-water channel. To establish this display, a linear array of seven electrodes is fabricated on the bottom of a water channel. By activating the appropriate electrodes among the seven ones, circular bubble clusters are generated above the electrodes and then imposed to float downstream using a water flow. This allows representing an N × 7 dot-matrix display to have a shorter refresh time compared with our previous method, and expanding the range of expressions of ephemeral user interfaces using bubbles.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382801
ER  - 

TY  - JOUR
AU  - Xu, Xuhai; Yu, Chun; Wang, Yuntao; Shi, Yuanchun
TI  - Recognizing Unintentional Touch on Interactive Tabletop
PY  - 2020
AB  - A multi-touch interactive tabletop is designed to embody the benefits of a digital computer within the familiar surface of a physical tabletop. However, the nature of current multi-touch tabletops to detect and react to all forms of touch, including unintentional touches, impedes users from acting naturally on them. In our research, we leverage gaze direction, head orientation and screen contact data to identify and filter out unintentional touches, so that users can take full advantage of the physical properties of an interactive tabletop, e.g., resting hands or leaning on the tabletop during the interaction. To achieve this, we first conducted a user study to identify behavioral pattern differences (gaze, head and touch) between completing usual tasks on digital versus physical tabletops. We then compiled our findings into five types of spatiotemporal features, and train a machine learning model to recognize unintentional touches with an F1 score of 91.3%, outperforming the state-of-the-art model by 4.3%. Finally we evaluated our algorithm in a real-time filtering system. A user study shows that our algorithm is stable and the improved tabletop effectively screens out unintentional touches, and provide more relaxing and natural user experience. By linking their gaze and head behavior to their touch behavior, our work sheds light on the possibility of future tabletop technology to improve the understanding of users' input intention.
SP  - 1
EP  - 24
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3381011
ER  - 

TY  - NA
AU  - Pfeuffer, Ken; Mecke, Lukas; Rodriguez, Sarah Delgado; Hassib, Mariam; Maier, Hannah; Alt, Florian
TI  - VRST - Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality
PY  - 2020
AB  - Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments.
SP  - NA
EP  - NA
JF  - 26th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385956.3418962
ER  - 

TY  - NA
AU  - Rezaei, Mohammad; Farahanipad, Farnaz; Dillhoff, Alex; Elmasri, Ramez; Athitsos, Vassilis
TI  - PETRA - Weakly-supervised hand part segmentation from depth images
PY  - 2021
AB  - Existing learning-based methods require a large number of labeled data to produce accurate part segmentation labels. However, acquiring ground truth labels is costly, giving rise to a need for methods that either require fewer labels or can utilize other currently available labels as a form of weak supervision for training. In this paper, in order to mitigate the burden of labeled-data acquisition, we propose a data-driven method for hand part segmentation on depth maps without any need for extra effort to obtain segmentation labels. The proposed method uses the labels already provided by public datasets in terms of major 3D hand joint locations to learn to estimate the hand shape and pose given a depth map. Given the pose and shape of a hand, the corresponding 3D hand mesh is generated using a deformable hand model and then rendered to a color image using a texture based on Linear Blend Skinning (LBS) weights of the hand model. The segmentation labels are then computed from the rendered color image. Since segmentation labels are not provided with current public datasets, we manually annotate a subset of the NYU dataset to perform quantitative evaluation of our method and show that a mIoU of 42% can be achieved with a model trained without using segmentation-based labels. Both qualitative and quantitative results confirm the effectiveness of our method. The code is publicly available for research purposes at: https://git.io/JmCBS.
SP  - 218
EP  - 225
JF  - The 14th PErvasive Technologies Related to Assistive Environments Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3453892.3453902
ER  - 

TY  - NA
AU  - Kaiser, Jonah-Noël; Marianski, Thu; Muras, Marco; Chamunorwa, Michael
TI  - Popup Observation Kit for Remote Usability Testing
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3497871
ER  - 

TY  - JOUR
AU  - Morales, Rafael; Ezcurdia, Iñigo; Irisarri, Josu; Andrade, Marco A. B.; Marzo, Asier
TI  - Generating Airborne Ultrasonic Amplitude Patterns Using an Open Hardware Phased Array
PY  - 2021
AB  - Holographic methods from optics can be adapted to acoustics for enabling novel applications in particle manipulation or patterning by generating dynamic custom-tailored acoustic fields. Here, we present three contributions towards making the field of acoustic holography more widespread. Firstly, we introduce an iterative algorithm that accurately calculates the amplitudes and phases of an array of ultrasound emitters in order to create a target amplitude field in mid-air. Secondly, we use the algorithm to analyse the impact of spatial, amplitude and phase emission resolution on the resulting acoustic field, thus providing engineering insights towards array design. For example, we show an onset of diminishing returns for smaller than a quarter-wavelength sized emitters and a phase and amplitude resolution of eight and four divisions per period, respectively. Lastly, we present a hardware platform for the generation of acoustic holograms. The array is integrated in a single board composed of 256 emitters operating at 40 kHz. We hope that the results and procedures described within this paper enable researchers to build their own ultrasonic arrays and explore novel applications of ultrasonic holograms.
SP  - 2981
EP  - NA
JF  - Applied Sciences
VL  - 11
IS  - 7
PB  - 
DO  - 10.3390/app11072981
ER  - 

TY  - NA
AU  - Beiter, Benjamin; Herron, Connor; Leonessa, Alexander
TI  - Whole Body Control for Haptic Interaction with VR
PY  - 2022
AB  - In this paper, we present a Quadratic Programming (QP) based Whole-Body Control (WBC) approach to generating Haptic Interaction with Virtual Reality. The primary challenge of controlling haptic feedback devices is the ability to create realistic contact forces for static or dynamic contact scenarios at any time, while otherwise not impeding the motion of a user. The combination of four control tasks in a WBC solves this problem by providing a general solution for generating haptic feedback for any interaction with a virtual object. We present a method to decouple the real and virtual dynamics of the system through the introduction of a Haptic Contact Object (HCO), which simplifies the representation of virtual interactions for feedback to the high-level controller. We implement our controller on a 10-DOF upper-body powered exoskeleton and demonstrate its effectiveness at generating sufficient haptic feedback for static, dynamic, and no contact scenarios.
SP  - NA
EP  - NA
JF  - 2022 American Control Conference (ACC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.23919/acc53348.2022.9867505
ER  - 

TY  - JOUR
AU  - Saha, Swapnil Sayan; Sandha, Sandeep Singh; Srivastava, Mani
TI  - Machine Learning for Microcontroller-Class Hardware: A Review.
PY  - 2022
AB  - The advancements in machine learning opened a new opportunity to bring intelligence to the low-end Internet-of-Things nodes such as microcontrollers. Conventional machine learning deployment has high memory and compute footprint hindering their direct deployment on ultra resource-constrained microcontrollers. This paper highlights the unique requirements of enabling onboard machine learning for microcontroller class devices. Researchers use a specialized model development workflow for resource-limited applications to ensure the compute and latency budget is within the device limits while still maintaining the desired performance. We characterize a closed-loop widely applicable workflow of machine learning model development for microcontroller class devices and show that several classes of applications adopt a specific instance of it. We present both qualitative and numerical insights into different stages of model development by showcasing several use cases. Finally, we identify the open research challenges and unsolved questions demanding careful considerations moving forward.
SP  - 21362
EP  - 21390
JF  - IEEE sensors journal
VL  - 22
IS  - 22
PB  - 
DO  - 10.1109/jsen.2022.3210773
ER  - 

TY  - JOUR
AU  - Khan, Arshad; Rahman, Khalid; Ali, Shawkat; Khan, Saleem; Wang, Bo; Bermak, Amine
TI  - Fabrication of circuits by multi-nozzle electrohydrodynamic inkjet printing for soft wearable electronics
PY  - 2021
AB  - Wearable electronic devices are evolving from current rigid configurations to flexible and ultimately stretchable structures. These emerging systems require soft circuits for connecting the various working units of the overall system. This paper presents fabrication of soft circuits by electrohydrodynamic (EHD) inkjet-printing technique. Multi-nozzle EHD printing head is employed for rapid fabrication of electric circuits on a wide set of materials, including glass substrate (rigid), flexible polyethylene terephthalate (PET) films, and stretchable thermoplastic polyurethane (TPU) films. To avoid the effects of substrate materials on the jettability, the proposed multi-nozzle head is equipped with integrated individual counter electrodes (electrodes are placed above the printing substrate). High-resolution circuits (50 ± 5 µm) with high electrical conductivity (0.6 Ω □−1) on soft substrate materials validate our well-controlled multi-nozzle EHD printing approach. The produced circuits showed excellent flexibility (bending radius ≈ 5 mm radius), high stretchability (strain ≈ 100%), and long-term mechanical stability (500 cycles at 30% strain). The concept is further demonstrated with a soft strain sensor based on a multi-nozzle EHD-printed circuit, employed for monitoring the human motion (finger bending), indicating the potential applications of these circuits in soft wearable electronic devices.
SP  - 1
EP  - 11
JF  - Journal of Materials Research
VL  - 36
IS  - 18
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - González, Rafael Morales; Freeman, Euan; Georgiou, Orestis
TI  - CHI Extended Abstracts - Levi-loop: A Mid-Air Gesture Controlled Levitating Particle Game
PY  - 2020
AB  - Acoustic levitation offers a novel alternative to traditional volumetric displays. With state-of-the-art hand-tracking technology, direct interaction and manipulation of levitating objects in 3D is now possible. Further, adding game-elements like completing simple tasks can encourage participant exploration of new technologies. We have therefore developed a gesture controlled levitating particle game, akin to the classic wire-loop game, that combines all these elements (levitation, hand-tracking, and gameplay) together with physical obstacles. Further, we have designed a gesture input set that constrains false triggering gestures and dropping of the levitating particle.
SP  - 1
EP  - 4
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383152
ER  - 

TY  - JOUR
AU  - Tang, Xiao; Li, Ruihui; Fu, Chi-Wing
TI  - CAFI-AR
PY  - 2022
AB  - <jats:p>Freehand interaction enhances user experience, allowing one to use bare hands to manipulate virtual objects in AR. Yet, it remains challenging to accurately and efficiently detect contacts between real hand and virtual object, due to the imprecise captured/estimated hand geometry. This paper presents CAFI-AR, a new approach for Contact-Aware Freehand Interaction with virtual AR objects, enabling us to automatically detect hand-object contacts in real-time with low latency. Specifically, we formulate a compact deep architecture to efficiently learn to predict hand action and contact moment from sequences of captured RGB images relative to the 3D virtual object. To train the architecture for detecting contacts on AR objects, we build a new dataset with 4,008 frame sequences, each with annotated hand-object interaction information. Further, we integrate CAFI-AR into our prototyping AR system and develop various interactive scenarios, demonstrating fine-grained contact-aware interactions on a rich variety of virtual AR objects, which cannot be achieved by existing AR interaction approaches. Lastly, we also evaluate CAFI-AR, quantitatively and qualitatively, through two user studies to demonstrate its effectiveness in terms of accurately detecting the hand-object contacts and promoting fluid freehand interactions</jats:p>
SP  - 1
EP  - 23
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 4
PB  - 
DO  - 10.1145/3569499
ER  - 

TY  - NA
AU  - Lee, En-Shiun Annie; Kuber, Karthik; Rohian, Hashmat; Woodhead, Sean
TI  - SIGCSE - Pillars of Program Design and Delivery: A Case Study using Self-Directed, Problem-Based, and Supportive Learning
PY  - 2021
AB  - As machine learning (ML) becomes prevalent in industries and businesses, the need to use these algorithms to solve real-world problems grows rapidly. However, there is a serious deficit of qualified talent in this field and thus a corresponding shortage of educational programs. To address the shortage of ML specialists in the field, universities are offering continuing education programs that fast-track the development of technical and transverse skills needed for success in the field. The award-winning machine learning program described in this paper is carefully designed with industry and community partners while focusing on practical skills and participation in the local industry network. This program can be summarized in three learning principles: 1) learners are encouraged to build their knowledge and skills in a self-directed manner; 2) group projects in both simulated and workplace settings are incorporated to support problem-based learning; and 3) supportive learning environment is established to encourage open and safe learning. This paper reports on the instructors' experiences in teaching the four courses based on these principles, which has resulted in high satisfaction from students, successfully placing students in industry, and winning a national award. We offer this experiential report in the hope that it may serve as a point of reference for other instructors and programs for mature technical learners in machine learning.
SP  - 205
EP  - 211
JF  - Proceedings of the 52nd ACM Technical Symposium on Computer Science Education
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3408877.3432458
ER  - 

TY  - NA
AU  - Mathis, Florian; Fawaz, Hassan Ismail; Khamis, Mohamed
TI  - CHI Extended Abstracts - Knowledge-driven Biometric Authentication in Virtual Reality
PY  - 2020
AB  - With the increasing adoption of virtual reality (VR) in public spaces, protecting users from observation attacks is becoming essential to prevent attackers from accessing context-sensitive data or performing malicious payment transactions in VR. In this work, we propose RubikBiom, a knowledge-driven behavioural biometric authentication scheme for authentication in VR. We show that hand movement patterns performed during interactions with a knowledge-based authentication scheme (e.g., when entering a PIN) can be leveraged to establish an additional security layer. Based on a dataset gathered in a lab study with 23 participants, we show that knowledge-driven behavioural biometric authentication increases security in an unobtrusive way. We achieve an accuracy of up to 98.91% by applying a Fully Convolutional Network (FCN) on 32 authentications per subject. Our results pave the way for further investigations towards knowledge-driven behavioural biometric authentication in VR.
SP  - 1
EP  - 10
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382799
ER  - 

TY  - CHAP
AU  - Hoshi, Takayuki
TI  - Introduction to Ultrasonic Mid-Air Haptic Effects
PY  - 2022
AB  - AbstractIn this chapter, we discuss the basic physical principles of ultrasonic mid-air haptics. Our aim is to provide a holistic introduction to the technology, facilitate a better understanding of these principles, and help newcomers to join this exciting field of research in following the rest of this book. To that end, we have assumed a simplified and idealized situation and divide our discussion into four sub-topics: acoustic radiation pressure, phased array focusing, vibrotactile stimulation, and by-product audible sounds.
SP  - 1
EP  - 20
JF  - Ultrasound Mid-Air Haptics for Touchless Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-04043-6_1
ER  - 

TY  - NA
AU  - Junnarkar, Suyash; Yang, Xiangyi; Drawdy, Morgan; Gupta, Inika; Yeo, Woon-Hong; Posner, Noah; Leigh, Sang-won
TI  - ISWC - Exploiting the Slowness of Electrochromic Displays
PY  - 2021
AB  - This paper explores the slow behaviors of electrochromic displays. Electrochromic displays have several qualities that make them an attractive material for wearable design; they are run at a low power, require power only at transition, demonstrate slow and natural chromic change, and are entirely passive. They are increasingly explored for their applications in wearables, however, mostly focused on the binary switching function between visible and invisible states. We envision that their slow and natural chromic properties can be exploited further in slow wearable displays. To that end, this paper investigates strategies for designing with the slowness of electrochromic displays, and interconnect design techniques to achieve granular control over the display behaviors. Our exploration with electrochromic materials discusses how they could expand the design space of wearable displays as well as further areas of research in electrochromic displays.
SP  - 97
EP  - 101
JF  - 2021 International Symposium on Wearable Computers
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3460421.3480422
ER  - 

TY  - NA
AU  - Khan, Arshad; Ali, Shawkat; Khan, Saleem; Bermak, Amine
TI  - ISCAS - Rapid Fabrication of Soft Strain Sensors by Multi-Nozzle Electrohydrodynamic Inkjet Printing for Wearable Electronics
PY  - 2021
AB  - Soft wearable strain sensors having high sensitivity along with high deformability have been actively investigated. However, their facile and mass-production at a lower cost remains a challenge. This paper presents the rapid fabrication of soft resistive strain sensors by multi-nozzle electrohydrodynamic (EHD) inkjet printing technique. printing head comprises of five needles with discrete counter/ground electrodes is used for simultaneous printing of silver nanoparticle ink on prestrained thermoplastic polyurethane (TPU) substrate. By means of this approach, soft circuits with diverse working strain ranges are realized and tested. As an application, a soft strain sensor based on the multi-nozzle EHD printed circuit is employed for monitoring the human motion (finger bending), demonstrating the potential applications of these circuits in soft wearable electronic devices and soft human-machine interfaces.
SP  - 1
EP  - 4
JF  - 2021 IEEE International Symposium on Circuits and Systems (ISCAS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iscas51556.2021.9401105
ER  - 

TY  - NA
AU  - Marquardt, Nicolai; Riche, Nathalie Henry; Holz, Christian; Romat, Hugo; Pahud, Michel; Brudy, Frederik; Ledo, David; Park, Chunjong; Nicholas, Molly Jane; Seyed, Teddy; Ofek, Eyal; Lee, Bongshin; Buxton, William A. S.; Hinckley, Ken
TI  - UIST - AirConstellations: In-Air Device Formations for Cross-Device Interaction via Multiple Spatially-Aware Armatures
PY  - 2021
AB  - AirConstellations supports a unique semi-fixed style of cross-device interactions via multiple self-spatially-aware armatures to which users can easily attach (or detach) tablets and other devices. In particular, AirConstellations affords highly flexible and dynamic device formations where the users can bring multiple devices together in-air — with 2–5 armatures poseable in 7DoF within the same workspace — to suit the demands of their current task, social situation, app scenario, or mobility needs. This affords an interaction metaphor where relative orientation, proximity, attaching (or detaching) devices, and continuous movement into and out of ad-hoc ensembles can drive context-sensitive interactions. Yet all devices remain self-stable in useful configurations even when released in mid-air. We explore flexible physical arrangement, feedforward of transition options, and layering of devices in-air across a variety of multi-device app scenarios. These include video conferencing with flexible arrangement of the person-space of multiple remote participants around a shared task-space, layered and tiled device formations with overview+detail and shared-to-personal transitions, and flexible composition of UI panels and tool palettes across devices for productivity applications. A preliminary interview study highlights user reactions to AirConstellations, such as for minimally disruptive device formations, easier physical transitions, and balancing ”seeing and being seen” in remote work.
SP  - 1252
EP  - 1268
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474820
ER  - 

TY  - JOUR
AU  - Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers
PY  - 2020
AB  - Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.
SP  - 3490
EP  - 3502
JF  - IEEE Transactions on Visualization and Computer Graphics
VL  - 26
IS  - 12
PB  - 
DO  - 10.1109/tvcg.2020.3023567
ER  - 

TY  - JOUR
AU  - Bairaktaris, Georgios; Khan, Fasihullah; Jayawardena, K. D. J. Imalka; Frohlich, David M.; Sporea, Radu A.
TI  - Printable and flexible photodetectors via scalable fabrication for reading applications
PY  - 2022
AB  - <jats:title>Abstract</jats:title><jats:p>Printing techniques have been widely adopted in the fabrication of flexible electronic components. However, its application is still limited in complex control and communication circuitry due to the low performance and low fabrication uniformity amongst printed devices, compared to conventional electronics. Thus, the electronic systems in real-world applications are hybrid integrations of printed and conventional electronics. Here we demonstrate a low-cost, low-complexity, fully-printable flexible photodetector that can withstand over 100 1 mm-radius bending cycles using a simple and scalable two-step fabrication process. The prototypes are implemented in an augmented book system to automatically detect the ambient light through optical apertures on paper of a printed book, and then transmit the information to an adjunct device. This technique demonstrates the utility of low-cost materials and processes for robust large area sensing applications and could act as a gateway to pertinent multimedia information.</jats:p>
SP  - NA
EP  - NA
JF  - Communications Engineering
VL  - 1
IS  - 1
PB  - 
DO  - 10.1038/s44172-022-00041-4
ER  - 

TY  - NA
AU  - Liu, Chang; Orlosky, Jason; Plopski, Alexander
TI  - SUI - Eye Gaze-based Object Rotation for Head-mounted Displays
PY  - 2020
AB  - Hands-free manipulation of 3D objects has long been a challenge for augmented and virtual reality (AR/VR). While many methods use eye gaze to assist with hand-based manipulations, interfaces cannot yet provide completely gaze-based 6 degree-of-freedom (DoF) manipulations in an efficient manner. To address this problem, we implemented three methods to handle rotations of virtual objects using gaze, including RotBar: a method that maps line-of-sight eye gaze onto per-axis rotations, RotPlane: a method that makes use of orthogonal planes to achieve per-axis angular rotations, and RotBall: a method that combines a traditional arcball with an external ring to handle user-perspective roll manipulations. We validated the efficiency of each method by conducting a user study involving a series of orientation tasks along different axes with each method. Experimental results showed that users could accomplish single-axis orientation tasks with RotBar and RotPlane significantly faster and more accurate than RotBall. On the other hand for multi-axis orientation tasks, RotBall significantly outperformed RotBar and RotPlane in terms of speed and accuracy.
SP  - NA
EP  - NA
JF  - Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3385959.3418444
ER  - 

TY  - NA
AU  - Zhu, Junyi; Snowden, Jackson C.; Verdejo, Joshua; Chen, Emily; Zhang, Paul; Ghaednia, Hamid; Schwab, Joseph H.; Mueller, Stefanie
TI  - UIST (Adjunct Volume) - EIT-kit Demo: An Electrical Impedance Tomography Toolkit for Health and Motion Sensing
PY  - 2021
AB  - In this paper, we propose EIT-kit, an electrical impedance tomography toolkit for designing and fabricating health and motion sensing devices. EIT-kit contains (1) an extension to a 3D editor for personalizing the form factor of electrode arrays and electrode distribution, (2) a customized EIT sensing motherboard for performing the measurements, (3) a microcontroller library that automates signal calibration and facilitates data collection, and (4) an image reconstruction library for mobile devices for interpolating and visualizing the measured data. Together, these EIT-kit components allow for applications that require 2- or 4-terminal setups, up to 64 electrodes, and single or multiple (up to four) electrode arrays simultaneously. We motivate the design of each component of EIT-kit with a formative study, and conduct a technical evaluation of the data fidelity of our EIT measurements. We demonstrate the design space that EIT-kit enables by showing various applications in health as well as motion sensing and control.
SP  - 100
EP  - 102
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474758
ER  - 

TY  - JOUR
AU  - Cano, Cesar Flores; Roudaut, Anne
TI  - MorphBenches: Using mixed reality experimentation platforms to study dynamic affordances in shape-changing devices
PY  - 2019
AB  - NA
SP  - 1
EP  - 11
JF  - International Journal of Human-Computer Studies
VL  - 132
IS  - NA
PB  - 
DO  - 10.1016/j.ijhcs.2019.07.006
ER  - 

TY  - JOUR
AU  - Hou, Wen-jun; Chen, Xiao-lin
TI  - Comparison of Eye-Based and Controller-Based Selection in Virtual Reality
PY  - 2020
AB  - Eye tracking, or pointing, in head-mounted displays enables new input modalities for point-select tasks. The goal of this paper is to explore the Fitts’ modeling of the eye-based selection in a vir...
SP  - 484
EP  - 495
JF  - International Journal of Human–Computer Interaction
VL  - 37
IS  - 5
PB  - 
DO  - 10.1080/10447318.2020.1826190
ER  - 

TY  - JOUR
AU  - Huaying, Wu; Jingjun, Zhu; Xuan, Wang; Li, Yuqiang
TI  - Design of ultrasonic standing wave levitation support for three-dimensional printed filaments.
PY  - 2021
AB  - Fused deposition modeling (FDM) three-dimensional (3D) printing is the process of forming a part by depositing molten thermoplastic materials layer by layer. Support structures need to be added below the overhangs or bridges in 3D printing. This paper proposes an idea for support-free FDM printing by studying the method of filament levitation. In this paper, an ultrasonic phased array device is designed, and different slender objects with length much longer than the sound wavelength are levitated in the air by multiple standing wave points. Experiments show that slender objects can be stably held at the sound pressure nodes in the standing wave field. After adding the ultrasonic field in FDM printing, the maximum deformation of single filament due to gravity on the bridge structure decreases from 5 to 2 mm. This proves that it is feasible for an ultrasonic phased array system to play an important role in the field of support-free FDM printing.
SP  - 2848
EP  - 2853
JF  - The Journal of the Acoustical Society of America
VL  - 149
IS  - 4
PB  - 
DO  - 10.1121/10.0003922
ER  - 

TY  - JOUR
AU  - Sarani, Babak; Shiroff, Adam; Pieracci, Fredric M.; Gasparri, Mario; White, Thomas W.; Whitbeck, SarahAnn; Gross, Ronald I.
TI  - Use of the Internet to Facilitate an Annual Scientific Meeting: A Report of the First Virtual Chest Wall Injury Society Summit
PY  - 2020
AB  - INTRODUCTION: The COVID-19 pandemic has resulted in cancellation of medical peer meetings. The Chest Wall Injury Society Annual Summit was scheduled for April 2020. Due to safety concerns, the Society altered the meeting to an online format. The purpose of this paper is to describe how this was accomplished and also to highlight its outcomes. METHODS: An online survey of participants was carried out to assess their views on the educational yield and technical difficulties encountered as compared to in-person meetings. RESULTS: Sixty two of 275 (23%) registered participants filled out the survey. Eighty four percent felt that the educational quality was excellent/good. Seventy five percent and 95% felt in-person meetings are better for education and for networking, respectively. Eighty seven percent preferred in-person meetings in the future but would attend a virtual meeting again. Thirteen percent had technical difficulties accessing the meeting. CONCLUSION: Online meetings are feasible but in-person meetings have more educational and networking value.
SP  - 889
EP  - 895
JF  - Journal of surgical education
VL  - 78
IS  - 3
PB  - 
DO  - 10.1016/j.jsurg.2020.09.004
ER  - 

TY  - JOUR
AU  - Lin, Fu-Sung; Yang, Po-Wei; Hsieh, Ching-Chuan; Su, Hsin-Yi; Chen, Li-Xiang; Li, Chih-Ying; Huang, Chih Hsien
TI  - Investigation of a novel acoustic levitation technique using the transition period between acoustic pulse trains and electrical driving signals.
PY  - 2022
AB  - Acoustic levitation is considered one of the most effective non-contact particle manipulation methods, along with aerodynamic, ferromagnetic, and optical levitation techniques. It is not restricted by the material properties of the target. However, existing acoustic levitation techniques have some drawbacks that limit their potential applications. Therefore, in this paper, an innovative approach is proposed to manipulate objects more intuitively and freely. By taking advantage of the transition periods between the acoustic pulse trains and electrical driving signals, acoustic traps can be created by switching the acoustic focal spots rapidly. Since the high-energy-density points are not formed simultaneously, the computation of the acoustic field distribution with complicated mutual interference can be eliminated. Therefore, comparing to the existing approaches that created acoustic traps by solving pressure distributions using iterative methods, the proposed method simplifies the computation of time delay and makes it possible to be solved even with a microcontroller. In this work, three experiments have been demonstrated successfully to prove the capability of the proposed method including lifting a Styrofoam sphere, transportation of a single target, and suspending two objects. Besides, simulations of the distributions of acoustic pressure, radiation force, and Gor'kov potential were conducted to confirm the presence of acoustic traps in the scenarios of lifting one and two objects. The proposed tactic should be considered effective since the results of the practical experiments and simulations support each other.
SP  - 1
EP  - 1
JF  - IEEE transactions on ultrasonics, ferroelectrics, and frequency control
VL  - 69
IS  - 2
PB  - 
DO  - 10.1109/tuffc.2021.3124278
ER  - 

TY  - JOUR
AU  - Suzuki, Shun; Inoue, Seki; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - AUTD3: Scalable Airborne Ultrasound Tactile Display.
PY  - 2021
AB  - Through nonlinear effects, airborne ultrasound phased arrays enable mid-air tactile presentations, as well as auditory presentation and acoustic levitation. To create workplaces flexibly, we have developed a scalable phased array system in which multiple modules can be connected via Ethernet cables and controlled from a PC or other host device. Each module has 249 transducers and the software used can individually specify the phase and amplitude of each of the connected transducers. Using EtherCAT for communication, the system achieves high accuracy synchronization among the connected modules. In this paper, we describe the details of the hardware and software architecture of the developed system and evaluate it. We experimentally confirmed the synchronization of 20 modules within an accuracy of 0.1 's and the phase and amplitude can be specified at 8 bits resolution. In addition, using nine modules, we confirmed that we could make a focal point of the size consistent with the theory at 500 mm above the array surface.
SP  - 1
EP  - 1
JF  - IEEE transactions on haptics
VL  - 14
IS  - 4
PB  - 
DO  - 10.1109/toh.2021.3069976
ER  - 

TY  - NA
AU  - Nam, Hye Yeon; Hernandez, Iyleah; Harmon, Brendan
TI  - Unmasked
PY  - 2020
AB  - NA
SP  - NA
EP  - NA
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416137
ER  - 

TY  - NA
AU  - Kamihori, Mai; Ogura, Ayumu; Ito, Kodai; Itoh, Yuichi
TI  - DAWBalloon: An Intuitive Musical Interface Using Floating Balloons
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3561354
ER  - 

TY  - NA
AU  - Leake, Mackenzie; Lai, Frances; Grossman, Tovi; Wigdor, Daniel; Lafreniere, Ben
TI  - CHI - PatchProv: Supporting Improvisational Design Practices for Modern Quilting
PY  - 2021
AB  - The craft of improvisational quilting involves working without the use of a predefined pattern. Design decisions are made “in the fabric,” with design experimentation tightly interleaved with the creation of the final artifact. To investigate how this type of design process can be supported, and to address challenges faced by practitioners, this paper presents PatchProv, a system for supporting improvisational quilt design. Based on a review of popular books on improvisational quilting, a set of design principles and key challenges to improvisational quilt design were identified, and PatchProv was developed to support the unique aspects of this process. An evaluation with a small group of quilters showed enthusiasm for the approach and revealed further possibilities for how computational tools can support improvisational quilting and improvisational design practices more broadly.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445601
ER  - 

TY  - NA
AU  - Pater, Jessica; Coupe, Amanda; Pfafman, Rachel; Phelan, Chanda; Toscos, Tammy; Jacobs, Maia
TI  - CHI - Standardizing Reporting of Participant Compensation in HCI: A Systematic Literature Review and Recommendations for the Field
PY  - 2021
AB  - The user study is a fundamental method used in HCI. In designing user studies, we often use compensation strategies to incentivize recruitment. However, compensation can also lead to ethical issues, such as coercion. The CHI community has yet to establish best practices for participant compensation. Through a systematic review of manuscripts at CHI and other associated publication venues, we found high levels of variation in the compensation strategies used within the community and how we report on this aspect of the study methods. A qualitative analysis of justifications offered for compensation sheds light into how some researchers are currently contextualizing this practice. This paper provides a description of current compensation strategies and information that can inform the design of compensation strategies in future studies. The findings may be helpful to generate productive discourse in the HCI community towards the development of best practices for participant compensation in user studies.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445734
ER  - 

TY  - NA
AU  - Lin, Jenny; McCann, James
TI  - ICRA - An Artin Braid Group Representation of Knitting Machine State with Applications to Validation and Optimization of Fabrication Plans
PY  - 2021
AB  - Industrial knitting machines create fabric by manipulating loops held on hundreds of needles. A core problem in pattern making for these machines is transfer planning – coming up with a sequence of low-level operations that move loops to the appropriate needles so that knitting through those loops produces the correct final structure. Since each loop is connected to the larger piece in progress, transfer plans must account for not only loop position, but the way strands of yarn tangle around each other.We present the first complete, discrete representation of the machine’s loop-tangling process. Our representation combines a braid from the Artin Braid Group with an array of explicit loop positions to fully capture loop crossings. By storing braids in the Symmetric Normal Form, states can be quickly compared and updated incrementally with machine operations. This representation can be used to verify the equivalence of transfer operations, providing an important tool in optimizing knit manufacturing.We improve on prior work in transfer planning algorithms, which can only solve certain subclasses of problems and are frequently suboptimal in terms of fabrication time, by introducing a novel A* search heuristic and state-collapsing mechanism, which we show finds optimal transfer plans for a large benchmark set of small transfer planning problems.
SP  - 1147
EP  - 1153
JF  - 2021 IEEE International Conference on Robotics and Automation (ICRA)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icra48506.2021.9562113
ER  - 

TY  - JOUR
AU  - Jiang, Chen; Wang, Kan; Liu, Yi; Zhang, Chuck; Wang, Ben
TI  - Application of textile technology in tissue engineering: A review.
PY  - 2021
AB  - NA
SP  - 60
EP  - 76
JF  - Acta biomaterialia
VL  - 128
IS  - NA
PB  - 
DO  - 10.1016/j.actbio.2021.04.047
ER  - 

TY  - NA
AU  - Reynal, Maxime; Freeman, Euan; Brewster, Stephen
TI  - CHI Extended Abstracts - Towards Adding Pseudo-Haptic Effects to Acoustic Levitation Displays
PY  - 2021
AB  - Acoustic levitation displays are a novel technology that use ultrasound to levitate small ‘particles’ in mid-air, to create physical 3D content. Interaction with levitated content is currently limited to simple operations, e.g., moving a particle. In this paper, we investigate the addition of pseudo-haptic effects to levitation displays, to enrich the interaction with new sensory experiences and provide simulated touch feedback. We describe a range of pseudo-haptic effects and possible implementations, based on the relationship between user control and particle response. A user-study is proposed to evaluate the potential contribution of pseudo-haptic effects when used with acoustic levitation displays.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451661
ER  - 

TY  - JOUR
AU  - Brument, Hugo; Bruder, Gerd; Marchal, Maud; Olivier, Anne-Hélène; Argelaguet, Ferran
TI  - Understanding, Modeling and Simulating Unintended Positional Drift during Repetitive Steering Navigation Tasks in Virtual Reality
PY  - 2021
AB  - Virtual steering techniques enable users to navigate in larger Virtual Environments (VEs) than the physical workspace available. Even though these techniques do not require physical movement of the users (e.g. using a joystick and the head orientation to steer towards a virtual direction), recent work observed that users might unintentionally move in the physical workspace while navigating, resulting in Unintended Positional Drift (UPD). This phenomenon can be a safety issue since users may unintentionally reach the physical boundaries of the workspace while using a steering technique. In this context, as a necessary first step to improve the design of navigation techniques minimizing the UPD, this paper aims at analyzing and modeling the UPD during a virtual navigation task. In particular, we characterize and analyze the UPD for a dataset containing the positions and orientations of eighteen users performing a virtual slalom task using virtual steering techniques. Participants wore a head-mounted display and had to follow three different sinusoidal-like trajectories (with low, medium and high curvature) using a torso-steering navigation technique. We analyzed the performed motions and proposed two UPD models: the first based on a linear regression analysis and the second based on a Gaussian Mixture Model (GMM) analysis. Then, we assessed both models through a simulation-based evaluation where we reproduced the same navigation task using virtual agents. Our results indicate the feasibility of using simulation-based evaluations to study UPD. The paper concludes with a discussion of potential applications of the results in order to gain a better understanding of UPD during steering and therefore improve the design of navigation techniques by compensating for UPD.
SP  - 4300
EP  - 4310
JF  - IEEE transactions on visualization and computer graphics
VL  - 27
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2021.3106504
ER  - 

TY  - NA
AU  - Andersson, Carl; Ahrens, Jens
TI  - Creation of Large Quiet Zones in the Presence of Acoustical Levitation Traps
PY  - 2021
AB  - We propose a method to generate an acoustical levitation trap at the same time as suppressing the sound in a multi-wavelength region of space. The method uses a spherical basis expansion of the sound field in the quiet zone, calculated by translating individual source expansions of elements in a transducer array. We show that it is possible to control the size of the quiet zone with the truncation order of the expansion, and explain the trade-off between field suppression in the quiet zone and stiffness loss of the levitation trap. Measurements of a generated sound field show the existence of a region of lower sound pressure. Simulations demonstrate a contrast up to 50 dB and sizes up to 60 mm for a 256 element array.
SP  - NA
EP  - NA
JF  - 2021 IEEE International Ultrasonics Symposium (IUS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ius52206.2021.9593601
ER  - 

TY  - NA
AU  - Li, Zhen; Chan, Joannes; Walton, Joshua; Benko, Hrvoje; Wigdor, Daniel; Glueck, Michael
TI  - CHI - Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality
PY  - 2021
AB  - In virtual reality (VR) environments, asymmetric bimanual interaction techniques can increase users’ input bandwidth by complementing their perceptual and motor systems (e.g., using the dominant hand to select 3D UI controls anchored around the non-dominant arm). However, it is unclear how to optimize the layout of such 3D UI controls for near-body and mid-air interactions. We evaluate the performance and limitations of non-dominant arm-anchored 3D UIs in VR environments through a bimanual pointing study. Results demonstrated that targets appearing closer to the skin, located around the wrist, or placed on the medial side of the forearm could be selected more quickly than targets farther away from the skin, located around the elbow, or on the lateral side of the forearm. Based on these results, we developed Armstrong guidelines, demonstrated through a Unity plugin to enable designers to create performance-optimized arm-anchored 3D UI layouts.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445064
ER  - 

TY  - NA
AU  - Yi, Xin; Qiu, Leping; Tang, Wenjing; Fan, Yehan; Li, Hewu; Shi, Yuanchun
TI  - DEEP: 3D Gaze Pointing in Virtual Reality Leveraging Eyelid Movement
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545673
ER  - 

TY  - CHAP
AU  - Suzuki, Yutaro; Sekimori, Kodai; Yamato, Yuki; Yamasaki, Yusuke; Shizuki, Buntarou; Takahashi, Shin
TI  - HCI (2) - A Mouth Gesture Interface Featuring a Mutual-Capacitance Sensor Embedded in a Surgical Mask
PY  - 2020
AB  - We developed a mouth gesture interface featuring a mutual-capacitance sensor embedded in a surgical mask. This wearable hands-free interface recognizes non-verbal mouth gestures; others cannot eavesdrop on anything the user does with the user’s device. The mouth is hidden by the mask; others do not know what the user is doing. We confirm the feasibility of our approach and demonstrate the accuracy of mouth shape recognition. We present two applications. Mouth shape can be used to zoom in or out, or to select an application from a menu.
SP  - 154
EP  - 165
JF  - Lecture Notes in Computer Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-49062-1_10
ER  - 

TY  - JOUR
AU  - Sidenmark, Ludwig; Parent, Mark; Wu, Chi-Hao; Chan, Joannes; Glueck, Michael; Wigdor, Daniel; Grossman, Tovi; Giordano, Marcello
TI  - Weighted Pointer: Error-aware Gaze-based Interaction through Fallback Modalities.
PY  - 2022
AB  - Gaze-based interaction is a fast and ergonomic type of hands-free interaction that is often used with augmented and virtual reality when pointing at targets. Such interaction, however, can be cumbersome whenever user, tracking, or environmental factors cause eye tracking errors. Recent research has suggested that fallback modalities could be leveraged to ensure stable interaction irrespective of the current level of eye tracking error. This work thus presents Weighted Pointer interaction, a collection of error-aware pointing techniques that determine whether pointing should be performed by gaze, a fallback modality, or a combination of the two, depending on the level of eye tracking error that is present. These techniques enable users to accurately point at targets when eye tracking is accurate and inaccurate. A virtual reality target selection study demonstrated that Weighted Pointer techniques were more performant and preferred over techniques that required the use of manual modality switching.
SP  - 3585
EP  - 3595
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 11
PB  - 
DO  - 10.1109/tvcg.2022.3203096
ER  - 

TY  - NA
AU  - Raptis, George E.; Katsini, Christina; Cen, Andrew Jian-lan; Arachchilage, Nalin Asanka Gamagedara; Nacke, Lennart E.
TI  - CHI - Better, Funner, Stronger: A Gameful Approach to Nudge People into Making Less Predictable Graphical Password Choices
PY  - 2021
AB  - Graphical user authentication (GUA) is a common alternative to text-based user authentication, where people are required to draw graphical passwords on background images. Such schemes are theoretically considered remarkably secure because they offer a large password space. However, people tend to create their passwords on salient image areas introducing high password predictability. Aiming to help people use the password space more effectively, we propose a gameful password creation process. In this paper, we present GamePass, a gamified mechanism that integrates the GUA password creation process. We provide the first evidence that it is possible to nudge people towards better password choices by gamifying the process. GamePass randomly guides participants’ attention to areas other than the salient areas of authentication images, makes the password creation process more fun, and people are more engaged. Gamifying the password creation process enables users to interact better and make less predictable graphical password choices instead of being forced to use a strict password policy.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445658
ER  - 

TY  - CONF
AU  - Suresh, Harini; Gomez, Steven R.; Nam, Kevin K.; Satyanarayan, Arvind
TI  - CHI - Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs
PY  - 2021
AB  - To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders’ knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.
SP  - NA
EP  - NA
JF  - NA
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - JOUR
AU  - Kumar, Aayan; Seshadri, Vivek; Sharma, Rahul
TI  - Shiftry: RNN inference in 2KB of RAM
PY  - 2020
AB  - Traditionally, IoT devices send collected sensor data to an intelligent cloud where machine learning (ML) inference happens. However, this course is rapidly changing and there is a recent trend to run ML on the edge IoT devices themselves. An intelligent edge is attractive because it saves network round trip (efficiency) and keeps user data at the source (privacy). However, the IoT devices are much more resource constrained than the cloud, which makes running ML on them challenging. Specifically, consider Arduino Uno, a commonly used board, that has 2KB of RAM and 32KB of read-only Flash memory. Although recent breakthroughs in ML have created novel recurrent neural network (RNN) models that provide good accuracy with KB-sized models, deploying them on tiny devices with such hard memory requirements has remained elusive. We provide, Shiftry, an automatic compiler from high-level floating-point ML models to fixed-point C-programs with 8-bit and 16-bit integers, which have significantly lower memory requirements. For this conversion, Shiftry uses a data-driven float-to-fixed procedure and a RAM management mechanism. These techniques enable us to provide first empirical evaluation of RNNs running on tiny edge devices. On simpler ML models that prior work could handle, Shiftry-generated code has lower latency and higher accuracy.
SP  - 1
EP  - 30
JF  - Proceedings of the ACM on Programming Languages
VL  - 4
IS  - OOPSLA
PB  - 
DO  - 10.1145/3428250
ER  - 

TY  - JOUR
AU  - Wu, Kui; Tarini, Marco; Yuksel, Cem; McCann, James; Gao, Xifeng
TI  - Wearable 3D Machine Knitting: Automatic Generation of Shaped Knit Sheets to Cover Real-World Objects.
PY  - 2022
AB  - Knitting can efficiently fabricate stretchable and durable soft surfaces. These surfaces are often designed to be worn on solid objects as covers, garments, and accessories. Given a 3D model, we consider a knit for it wearable if the knit not only reproduces the shape of the 3D model but also can be put on and taken off from the model without deforming the model. This '`wearability'' places additional constraints on surface design and fabrication, which existing machine knitting approaches do not take into account. We introduce the first practical automatic pipeline to generate knit designs that are both wearable and machine knittable. Our pipeline handles knittability and wearability with two separate modules that run in parallel. Specifically, given a 3D object and its corresponding 3D garment surface, our approach first converts the garment surface into a topological disc by introducing a set of cuts. The resulting cut surface is then fed into a physically-based unclothing simulation module to ensure the garment's wearability over the object. The unclothing simulation determines which of the previously introduced cuts could be sewn permanently without impacting wearability. Concurrently, the cut surface is converted into an anisotropic stitch mesh. Then, our novel, stochastic, any-time flat-knitting scheduler generates fabrication instructions for an industrial knitting machine. Finally, we fabricate the garment and manually assemble it into one complete covering worn by the target object. We demonstrate our method's robustness and knitting efficiency by fabricating models with various topological and geometric complexities.
SP  - 1
EP  - 1
JF  - IEEE transactions on visualization and computer graphics
VL  - 28
IS  - 9
PB  - 
DO  - 10.1109/tvcg.2021.3056101
ER  - 

TY  - NA
AU  - Doolani, Jayesh; Wright, Matthew; Setty, Rajesh; Haque, S. M. Taiabul
TI  - CHI - LociMotion: Towards Learning a Strong Authentication Secret in a Single Session
PY  - 2021
AB  - In this work, we design and evaluate LociMotion, a training interface to learn a strong authentication secret in a single session. LociMotion automatically takes a random password with twelve lowercase letters (56-bit entropy) to generate the training interface. It first leverages users’ spatial and visual (declarative) memory by showing them a video clip based on the method of loci, and then consolidates the learning process by having them play a computer game that leverages their motor (procedural) memory. The results of a memorability study with 300 participants showed that LociMotion had a significantly higher recall success rate than a control condition. A second study with 200 participants demonstrated the effectiveness of LociMotion over a period of time (99%, 96%, and 81% recall success rates after 1, 4, and 18 days, respectively). LociMotion offers an alternative to the spaced repetition technique, as it does not require dozens of training sessions.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445105
ER  - 

TY  - JOUR
AU  - Hirayama, Ryuji; Subramanian, Sriram
TI  - Magical multi-modal displays using acoustophoresis
PY  - 2022
AB  - <jats:p>Using sound to levitate objects for creating displays that can deliver visual, auditory, tactile, and gustatory experiences.</jats:p>
SP  - 54
EP  - 58
JF  - XRDS: Crossroads, The ACM Magazine for Students
VL  - 29
IS  - 1
PB  - 
DO  - 10.1145/3558195
ER  - 

TY  - NA
AU  - Sharma, Adwait; Hedderich, Michael A.; Bhardwaj, Divyanshu; Fruchard, Bruno; McIntosh, Jess; Nittala, Aditya Shekhar; Klakow, Dietrich; Ashbrook, Daniel; Steimle, Jürgen
TI  - CHI - SoloFinger: Robust Microgestures while Grasping Everyday Objects
PY  - 2021
AB  - Using microgestures, prior work has successfully enabled gestural interactions while holding objects. Yet, these existing methods are prone to false activations caused by natural finger movements while holding or manipulating the object. We address this issue with SoloFinger, a novel concept that allows design of microgestures that are robust against movements that naturally occur during primary activities. Using a data-driven approach, we establish that single-finger movements are rare in everyday hand-object actions and infer a single-finger input technique resilient to false activation. We demonstrate this concept’s robustness using a white-box classifier on a pre-existing dataset comprising 36 everyday hand-object actions. Our findings validate that simple SoloFinger gestures can relieve the need for complex finger configurations or delimiting gestures and that SoloFinger is applicable to diverse hand-object actions. Finally, we demonstrate SoloFinger’s high performance on commodity hardware using random forest classifiers.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445197
ER  - 

TY  - JOUR
AU  - Li, Zhi; Zhao, Maozheng; Wang, Yifan; Rashidian, Sina; Baig, Furqan; Liu, Rui; Liu, Wanyu; Beaudouin-Lafon, Michel; Ellison, Brooke; Wang, Fusheng; Bi, Xiaojun
TI  - BayesGaze: A Bayesian Approach to Eye-Gaze Based Target Selection
PY  - 2021
AB  - Selecting targets accurately and quickly with eye-gaze input remains an open research question. In this paper, we introduce BayesGaze, a Bayesian approach of determining the selected target given an eye-gaze trajectory. This approach views each sampling point in an eye-gaze trajectory as a signal for selecting a target. It then uses the Bayes' theorem to calculate the posterior probability of selecting a target given a sampling point, and accumulates the posterior probabilities weighted by sampling interval to determine the selected target. The selection results are fed back to update the prior distribution of targets, which is modeled by a categorical distribution. Our investigation shows that BayesGaze improves target selection accuracy and speed over a dwell-based selection method, and the Center of Gravity Mapping (CM) method. Our research shows that both accumulating posterior and incorporating the prior are effective in improving the performance of eye-gaze based target selection.
SP  - 231
EP  - 240
JF  - Proceedings. Graphics Interface (Conference)
VL  - 2021
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Reynal, Maxime; Freeman, Euan; Brewster, Stephen
TI  - CHI Extended Abstracts - Avoiding Collisions when Interacting with Levitating Particle Displays
PY  - 2020
AB  - Levitating particle displays are an emerging technology where content is composed of physical pixels. Unlike digital displays, manipulating the content is not straightforward because physical constraints affect the placement and movement of each particle: dragging a particle may cause it to collide with others along its movement path. We describe initial work into four new interaction techniques that allow users to avoid collisions when directly manipulating display content. Techniques such as these are required for interactive levitating displays to be practical when scaled up to large sizes.
SP  - 1
EP  - 7
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382965
ER  - 

TY  - NA
AU  - Hofmann, Megan; Mankoff, Jennifer; Hudson, Scott E.
TI  - UIST - KnitGIST: A Programming Synthesis Toolkit for Generating Functional Machine-Knitting Textures
PY  - 2020
AB  - Automatic knitting machines are robust, digital fabrication devices that enable rapid and reliable production of attractive, functional objects by combining stitches to produce unique physical properties. However, no existing design tools support optimization for desirable physical and aesthetic knitted properties. We present KnitGIST (Generative Instantiation Synthesis Toolkit for knitting), a program synthesis pipeline and library for generating hand- and machine-knitting patterns by intuitively mapping objectives to tactics for texture design. KnitGIST generates a machine-knittable program in a domain-specific programming language.
SP  - 1234
EP  - 1247
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415590
ER  - 

TY  - NA
AU  - Aoyama, Shutaro; Asano, Kei
TI  - Shadow Play using Ultrasound Levitated Props
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3561349
ER  - 

TY  - NA
AU  - Hou, Min; Xu, Chang; Liu, Yang; Liu, Weiqing; Bian, Jiang; Wu, Le; Li, Zhi; Chen, Enhong; Liu, Tie-Yan
TI  - CIKM - Stock Trend Prediction with Multi-granularity Data: A Contrastive Learning Approach with Adaptive Fusion
PY  - 2021
AB  - Stock trend prediction plays a crucial role in quantitative investing. Given the prediction task on a certain granularity (e.g., daily trend), a large portion of existing studies merely leverage market data of the same granularity (e.g., daily market data). In financial investment scenarios, however, there exist amounts of finer-grained information (e.g., high-frequency data) that contain more detailed investment signals beyond the original granularity data. This motivates us to investigate how to leverage multi-granularity market data to enhance the accuracy of stock trend prediction. Some straightforward methods, such as concatenating finer-grained data as features or fusing with a model based on finer-grained features, may not lead to more precise stock trend prediction due to some unique challenges. First, the inconsistency of granularity between the target trend and finer-grained data could substantially increase optimization difficulty, such as the relative sparsity of the target trend compared with higher dimensions of finer-grained features. Moreover, the continuously changing financial market state could result in varying efficacy of heterogeneous multi-granularity information, which consequently requires a dynamic approach for proper fusion among them. In this paper, we propose the Contrastive Multi-Granularity Learning Framework (CMLF) to address these challenges. Particularly, we first design two novel contrastive learning objectives at the pre-training stage to address the inconsistency issue by constructing additional self-supervised signals relying on the inherent character of stock data. We also design a gate mechanism based on market-aware technical indicators to fuse the multi-granularity features at each time step adaptively. Extensive experiments on three real-world datasets show significant improvements of our approach over the state-of-the-art baselines on stock trend prediction and profitability in real investing scenarios.
SP  - 700
EP  - 709
JF  - Proceedings of the 30th ACM International Conference on Information & Knowledge Management
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3459637.3482483
ER  - 

TY  - CHAP
AU  - Nakashima, Ryota; Kanmuri, Yuta; Hong, Hua; Tomiyama, Tatsuro; Sasaki, Atsuko; Tomisugi, Masahiro; Tobita, Hiroaki
TI  - BlueSkype: A Shared Virtual 3D World for Off-Site Meetings in Nature
PY  - 2022
AB  - NA
SP  - 98
EP  - 105
JF  - Communications in Computer and Information Science
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-06394-7_15
ER  - 

TY  - JOUR
AU  - Fuvattanasilp, Varunyu; Fujimoto, Yuichiro; Plopski, Alexander; Taketomi, Takafumi; Sandor, Christian; Kanbara, Masayuki; Kato, Hirokazu
TI  - SlidAR+: Gravity-aware 3D object manipulation for handheld augmented reality
PY  - 2021
AB  - Abstract Accurately placing virtual objects in a scene is a challenging tasks in handheld augmented reality (HAR). To add and arrange virtual objects in HAR, users must manipulate 6 degrees of freedom (DoFs) of the virtual object, namely: position (3) and orientation (3). However, it is difficult to manipulate all DoFs with the two-dimensional display of the handheld device. We present SlidAR+, a method for controlling the position and orientation of objects in HAR. SlidAR+ is an extension of SlidAR [1], a technique that allows users to control the position of a virtual object by manipulating only 1 DoF. We use the direction of gravity as a constraint to improve the user’s control and reduce the time it takes to adjust the orientation. Upon comparing it with a state-of-the-art object manipulation method, using SlidAR+, user were able to complete the tasks faster under our expected conditions and were also preferred by most participants.
SP  - 23
EP  - 35
JF  - Computers & Graphics
VL  - 95
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.01.005
ER  - 

TY  - NA
AU  - Furumoto, Takuro; Kasai, Takumi; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - UIST - Midair Balloon Interface: A Soft and Lightweight Midair Object for Proximate Interactions
PY  - 2021
AB  - This paper introduces a midair balloon interface, a fast and soft interactive object in mid-air. Our approach tackles the trade-off between safety and speed by controlling a soft helium-filled balloon with external actuators and sensors. We developed a prototype system that uses airborne ultrasound phased arrays to propel a balloon and high-speed stereo cameras to track its motion. This configuration realizes both a high thrust/weight ratio and such a soft body that is safe-to-collide. We describe a sight-based interaction and a touch-based interaction that leverage the safety and speed of a midair balloon interface. A sight-based interaction allows the user to keep the object inside her/his view within reach by controlling a balloon to follow the direction of the user’s face. A touch-based interaction allows the user to manipulate the object directly with his/her hand, issue a command by moving his/her finger on the surface, and receive vibrotactile feedback produced by vibrating the balloon with amplitude-modulated ultrasound. We describe the implementation and evaluation of the prototype and explore the application scenarios.
SP  - 783
EP  - 795
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474786
ER  - 

TY  - NA
AU  - Koelle, Marion; Nicolae, Madalina; Nittala, Aditya Shekhar; Teyssier, Marc; Steimle, Jürgen
TI  - Prototyping Soft Devices with Interactive Bioplastics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545623
ER  - 

TY  - JOUR
AU  - Sathya, Anup; Li, Jiasheng; Rahman, Tauhidur; Gao, Ge; Peng, Huaishu
TI  - Calico
PY  - 2022
AB  - <jats:p>We explore Calico, a miniature relocatable wearable system with fast and precise locomotion for on-body interaction, actuation and sensing. Calico consists of a two-wheel robot and an on-cloth track mechanism or "railway," on which the robot travels. The robot is self-contained, small in size, and has additional sensor expansion options. The track system allows the robot to move along the user's body and reach any predetermined location. It also includes rotational switches to enable complex routing options when diverging tracks are presented. We report the design and implementation of Calico with a series of technical evaluations for system performance. We then present a few application scenarios, and user studies to understand the potential of Calico as a dance trainer and also explore the qualitative perception of our scenarios to inform future research in this space.</jats:p>
SP  - 1
EP  - 32
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 3
PB  - 
DO  - 10.1145/3550323
ER  - 

TY  - JOUR
AU  - Takahashi, Ryo; Yukita, Wakako; Sasatani, Takuya; Yokota, Tomoyuki; Someya, Takao; Kawahara, Yoshihiro
TI  - Twin Meander Coil
PY  - 2021
AB  - <jats:p>Energy-efficient and unconstrained wearable sensing platforms are essential for ubiquitous healthcare and activity monitoring applications. This paper presents Twin Meander Coil for wirelessly connecting battery-free on-body sensors to a textile-based reader knitted into clothing. This connection is based on passive inductive telemetry (PIT), wherein an external reader coil collects data from passive sensor coils via the magnetic field. In contrast to standard active sensing techniques, PIT does not require the reader to power up the sensors. Thus, the reader can be fabricated using a lossy conductive thread and industrial knitting machines. Furthermore, the sensors can superimpose information such as ID, touch, rotation, and pressure on its frequency response. However, conventional PIT technology needs a strong coupling between the reader and the sensor, requiring the reader to be small to the same extent as the sensors' size. Thus, applying this technology to body-scale sensing systems is challenging. To enable body-scale readout, Twin Meander Coil enhances the sensitivity of PIT technology by dividing the body-scale meander-shaped reader coils into two parts and integrating them so that they support the readout of each other. To demonstrate its feasibility, we built a prototype with a knitting machine, evaluated its sensing ability, and demonstrated several applications.</jats:p>
SP  - 1
EP  - 21
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 4
PB  - 
DO  - 10.1145/3494996
ER  - 

TY  - JOUR
AU  - Barnaby, Gareth; Roudaut, Anne
TI  - Autogrip: Enabling Force Feedback Devices to Self-Attach to End-Users' Fingers
PY  - 2020
AB  - Autogrip is a new thimble that enables force feedback devices to autonomously attach themselves to a finger. Although self-attachment is a simple concept, it has never been explored in the space of force feedback devices where current thimble solutions require complex attachment procedures and often swapping between interchangeable parts. Self-attachment is advantageous in many applications such as: immersive spaces, multi-user, walk up and use contexts, and especially multi-point force feedback systems as it can allow a lone user to quickly attach multiple devices to fingers on both hands - a difficult task with current thimbles. We present the design of our open-source contraption, Autogrip, a one-size-fits-all thimble that retro-fits to existing force feedback devices, enabling them to automatically attach themselves to a fingertip. We demonstrate Autogrip by retrofitting it to a Phantom 1.5 and a 4-finger Mantis system. We report preliminary user-testing results that indicated Autogrip was three times faster to attach than a typical method. We also present further refinements based on user feedback.
SP  - 1
EP  - 14
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427312
ER  - 

TY  - BOOK
AU  - Guo, Runbo; Lin, Jenny; Narayanan, Vidya; McCann, James
TI  - SCF - Representing Crochet with Stitch Meshes
PY  - 2020
AB  - Crochet is a fabrication technique in which a 3D surface is created from yarn by interlacing loops formed with a special hook. Crochet patterns are typically represented using a standardized set of abstract pictorial symbols. Unfortunately, while this notation is enough for someone well-versed in the individual stitches, it does not directly show the yarn layout of stitches. This lack of specification makes it difficult for both novice users and computer programs to parse, visualize, and design crochet patterns. We demonstrate how to represent crochet patterns within the “stitch mesh” paradigm. That is, the pattern is represented using a library of tiles, where each tile contains yarn geometry, and tiles connect along their edges. In order to adapt stitch meshes to crochet, we introduce a special edge type which captures the idea of the current loop – the loop of yarn held on the crochet hook during fabrication. We also create a library of mesh face types which model commonly-used crochet stitches. We illustrate the richness of the crochet stitch faces by showing a number of examples including patterns generated from 3D models.
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3424630.3425409
ER  - 

TY  - JOUR
AU  - Yan, Yukang; Shi, Yingtian; Yu, Chun; Shi, Yuanchun
TI  - HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays
PY  - 2020
AB  - We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.
SP  - 1
EP  - 22
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3380983
ER  - 

TY  - NA
AU  - Jensen, Walther; Löchtefeld, Markus
TI  - ECPlotter: A Toolkit for Rapid Prototyping of Electrochromic Displays
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3568444.3568466
ER  - 

TY  - NA
AU  - Yu, Difeng; Zhou, Qiushi; Dingler, Tilman; Velloso, Eduardo; Goncalves, Jorge
TI  - Blending On-Body and Mid-Air Interaction in Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00081
ER  - 

TY  - CHAP
AU  - Zhang, Xinyong
TI  - HCI (7) - An Evaluation of Eye-Foot Input for Target Acquisitions
PY  - 2021
AB  - The multimodal interaction technique that combines eye and foot input not only provides a great opportunity for the user with busy hands or hand disabilities to interaction with computer, but can also overcome the drawbacks when using dwell time to solve the “Midas Problem” of gaze input. However, the user’s capability of eye-foot coordination was still unclear. At the same time, the human performance in the basic task of target acquisitions by eye-foot input was also uncertain, and especially a proper performance model was lacking. Motivated by this situation, an eye pointing and foot tapping task experiment had been carried out to fill these gaps. A low-cost eye tracker and a USB foot pedal switcher were used as the input devices from different modalities. The experimental results indicated that the user was soon able to coordinate her/his foot with the eyes for target acquisitions, and that the user could respond fast to tap the foot pedal to finish a task trial in the level of 600 ms. The main performance measures of eye movement time (EMT) and eye pointing time (EPT) under the eye-foot multimodal input condition were significantly increased with the increase of saccadic amplitude A and/or the decrease of target width (size) W, and vice versa. Regression analysis shown that the \(ID_{eye}\) model was more suitable than the standard Fitts’ law to model the human performance in this multimodal interaction context.
SP  - 499
EP  - 517
JF  - Universal Access in Human-Computer Interaction. Design Methods and User Experience
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-78092-0_34
ER  - 

TY  - NA
AU  - Rodriguez, Sarah Delgado; Prange, Sarah; Mecke, Lukas; Alt, Florian
TI  - CHI Extended Abstracts - ActPad– A Smart Desk Platform to Enable User Interaction with IoT Devices
PY  - 2021
AB  - ActPad is a desk pad, capable of sensing capacitive touch input in desk setups. Our prototype can sense touches on both, its electrodes and on connected objects. ActPad’s interaction-space is customizable, allowing easy integration and extension of existing desk environments. In smart environments, users may interact with more than one device at the same time. This generates the need for new interaction mechanisms that bundle the control of multiple ubiquitous devices. We support this need through a platform that extends interaction with IoT devices. ActPad accounts for different ways of controlling IoT devices by enabling various modes of interaction – in particular simultaneous, sequential, implicit and explicit – and, hence, a rich input space. As a proof of concept, we illustrate several use cases, including, but not limited to, controlling the browser on a PC, turning lights on/off, switching songs, or preparing coffee.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451825
ER  - 

TY  - NA
AU  - Bovo, Riccardo; Giunchi, Daniele; Sidenmark, Ludwig; Gellersen, Hans; Costanza, Enrico; Heinis, Thomas
TI  - Real-time head-based deep-learning model for gaze probability regions in collaborative VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529642
ER  - 

TY  - NA
AU  - Bergström, Joanna; Dalsgaard, Tor-Salve; Alexander, Jason; Hornbæk, Kasper
TI  - CHI - How to Evaluate Object Selection and Manipulation in VR? Guidelines from 20 Years of Studies
PY  - 2021
AB  - The VR community has introduced many object selection and manipulation techniques during the past two decades. Typically, they are empirically studied to establish their benefits over the state-of-the-art. However, the literature contains few guidelines on how to conduct such studies; standards developed for evaluating 2D interaction often do not apply. This lack of guidelines makes it hard to compare techniques across studies, to report evaluations consistently, and therefore to accumulate or replicate findings. To build such guidelines, we review 20 years of studies on VR object selection and manipulation. Based on the review, we propose recommendations for designing studies and a checklist for reporting them. We also identify research directions for improving evaluation methods and offer ideas for how to make studies more ecologically valid and rigorous.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445193
ER  - 

TY  - JOUR
AU  - Marques, Bernardo; Alves, João; Neves, Miguel Santos; Justo, Inês; Santos, André; Rainho, Raquel; Maio, Rafael; Costa, Dany; Ferreira, Carlos; Dias, Paulo; Santos, Beatriz Sousa
TI  - Interaction with Virtual Content using Augmented Reality: A User Study in Assembly Procedures
PY  - 2020
AB  - Assembly procedures are a common task in several domains of application. Augmented Reality (AR) has been considered as having great potential in assisting users while performing such tasks. However, poor interaction design and lack of studies, often results in complex and hard to use AR systems. This paper considers three different interaction methods for assembly procedures (Touch gestures in a mobile device; Mobile Device movements; 3D Controllers and See-through HMD). It also describes a controlled experiment aimed at comparing acceptance and usability between these methods in an assembly task using Lego blocks. The main conclusions are that participants were faster using the 3D controllers and Video see-through HMD. Participants also preferred the HMD condition, even though some reported light symptoms of nausea, sickness and/or disorientation, probably due to limited resolution of the HMD cameras used in the video see-through setting and some latency issues. In addition, although some research claims that manipulation of virtual objects with movements of the mobile device can be considered as natural, this condition was the least preferred by the participants.
SP  - 1
EP  - 17
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427324
ER  - 

TY  - CHAP
AU  - Jane, L E; Landay, James A.
TI  - Designing Photography Guidance for Rapid In-Camera Iteration
PY  - 2021
AB  - Designers have long known the benefits of iteration and rapid prototyping. Many experienced photographers follow a similar process, in particular of iterating in camera: trying out different angles and compositions, varying lighting, adjusting a subject’s pose, etc. However, amateurs often do not realize the benefits of capturing variations of a single shot. Inspired by the parallels between the design thinking process and the photographic process, we design new interfaces that provide contextual in-camera feedback to aid users in learning visual elements of photography. We interactively visualize results of image processing algorithms as additional information for the user at capture time. In this chapter, we explore ways to encourage stages of the design thinking process, specifically through the design of guided photography interfaces that aid in iterating through the exploration of three different photographic concepts: lighting, composition, and decluttering.
SP  - 151
EP  - 165
JF  - Understanding Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-76324-4_8
ER  - 

TY  - JOUR
AU  - Dai, Shaozhang; Smiley, Jim; Dwyer, Tim; Ens, Barrett; Besancon, Lonni
TI  - RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics.
PY  - 2022
AB  - Immersive environments offer new possibilities for exploring three-dimensional volumetric or abstract data. However, typical mid-air interaction offers little guidance to the user in interacting with the resulting visuals. Previous work has explored the use of haptic controls to give users tangible affordances for interacting with the data, but these controls have either: been limited in their range and resolution; were spatially fixed; or required users to manually align them with the data space. We explore the use of a robot arm with hand tracking to align tangible controls under the user's fingers as they reach out to interact with data affordances. We begin with a study evaluating the effectiveness of a robot-extended slider control compared to a large fixed physical slider and a purely virtual mid-air slider. We find that the robot slider has similar accuracy to the physical slider but is significantly more accurate than mid-air interaction. Further, the robot slider can be arbitrarily reoriented, opening up many new possibilities for tangible haptic interaction with immersive visualisations. We demonstrate these possibilities through three use-cases: selection in a time-series chart; interactive slicing of CT scans; and finally exploration of a scatter plot depicting time-varying socio-economic data.
SP  - 1
EP  - 11
JF  - IEEE transactions on visualization and computer graphics
VL  - PP
IS  - NA
PB  - 
DO  - 10.1109/tvcg.2022.3209433
ER  - 

TY  - NA
AU  - Dewez, Diane; Hoyet, Ludovic; Lécuyer, Anatole; Sanz, Ferran Argelaguet
TI  - CHI - Towards “Avatar-Friendly” 3D Manipulation Techniques: Bridging the Gap Between Sense of Embodiment and Interaction in Virtual Reality
PY  - 2021
AB  - Avatars, the users’ virtual representations, are becoming ubiquitous in virtual reality applications. In this context, the avatar becomes the medium which enables users to manipulate objects in the virtual environment. It also becomes the users’ main spatial reference, which can not only alter their interaction with the virtual environment, but also the perception of themselves. In this paper, we review and analyse the current state-of-the-art for 3D object manipulation and the sense of embodiment. Our analysis is twofold. First, we discuss the impact that the avatar can have on object manipulation. Second, we discuss how the different components of a manipulation technique (i.e. input, control and feedback) can influence the user’s sense of embodiment. Throughout the analysis, we crystallise our discussion with practical guidelines for VR application designers and we propose several research topics towards “avatar-friendly’’ manipulation techniques.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445379
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Koelle, Marion; Mjaku, Fjolla; Strohmeier, Paul; Steimle, Jürgen
TI  - Print-A-Sketch: A Handheld Printer for Physical Sketching of Circuits and Sensors on Everyday Surfaces
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3502074
ER  - 

TY  - JOUR
AU  - Mohanty, Ronak R.; Krishnamurthy, Vinayak R.
TI  - Kinesthetic Metaphors for Precise Spatial Manipulation: A Study of Object Rotation
PY  - 2020
AB  - <jats:title>Abstract</jats:title><jats:p>In this article, we report on our investigation of kinesthetic feedback as a means to provide precision, accuracy, and mitigation of arm fatigue in spatial manipulation tasks. Most works on spatial manipulation discuss the use of haptics (kinesthetic/force and tactile) primarily as a means to offer physical realism in spatial user interfaces (SUIs). Our work offers a new perspective in terms of how force-feedback can promote precise manipulations in spatial interactions to aid manual labor, controllability, and precision. To demonstrate this, we develop, implement, and evaluate three new haptics-enabled interaction techniques (kinesthetic metaphors) for precise rotation of 3D objects. The quantitative and qualitative analyses of experiments reveal that the addition of force-feedback improves precision for each of the rotation techniques. Self-reported user feedback further exposes a novel aspect of kinesthetic manipulation in its ability to mitigate arm fatigue for close-range spatial manipulation tasks.</jats:p>
SP  - NA
EP  - NA
JF  - Journal of Computing and Information Science in Engineering
VL  - 21
IS  - 2
PB  - 
DO  - 10.1115/1.4048618
ER  - 

TY  - CHAP
AU  - Khan, Arshad; Ali, Shawkat; Khan, Saleem; Ahmed, Moaaz; Wang, Bo; Bermak, Amine
TI  - Vacuum-Free Fabrication of Transparent Electrodes for Soft Electronics
PY  - 2021
AB  - <jats:p>Optoelectronic devices are advancing from existing rigid configurations to deformable configurations. These developing devices need transparent electrodes (TEs) having high mechanical deformability while preserving the high electrical conductivity and optical transparency. In agreement with these requirements, vacuum-fabricated conventional TEs based on transparent conducting oxides (TCOs) are receiving difficulties due to its low abundance, film brittleness, and low optical transmittance. Novel solution-processed TE materials including regular metal meshes, metal nanowire (NW) grids, carbon materials, and conducting polymers have been studied and confirmed their capabilities to address the limitations of the TCO-based TEs. This chapter presents a comprehensive review of the latest advances of these vacuum-free TEs, comprising the electrode material classes, the optical, electrical, mechanical and surface feature properties of the soft TEs, and the vacuum-free practices for their fabrication.</jats:p>
SP  - NA
EP  - NA
JF  - Nanofibers - Synthesis, Properties and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.5772/intechopen.96311
ER  - 

TY  - NA
AU  - Chen, Yuxin; Li, Huiying; Teng, Shan-Yuan; Nagels, Steven; Li, Zhijing; Lopes, Pedro; Zhao, Ben Y.; Zheng, Haitao
TI  - CHI - Wearable Microphone Jamming
PY  - 2020
AB  - We engineered a wearable microphone jammer that is capable of disabling microphones in its user's surroundings, including hidden microphones. Our device is based on a recent exploit that leverages the fact that when exposed to ultrasonic noise, commodity microphones will leak the noise into the audible range. Unfortunately, ultrasonic jammers are built from multiple transducers and therefore exhibit blind spots, i.e., locations in which transducers destructively interfere and where a microphone cannot be jammed. To solve this, our device exploits a synergy between ultrasonic jamming and the naturally occur- ring movements that users induce on their wearable devices (e.g., bracelets) as they gesture or walk. We demonstrate that these movements can blur jamming blind spots and increase jamming coverage. Moreover, current jammers are also directional, requiring users to point the jammer to a microphone; instead, our wearable bracelet is built in a ring-layout that al- lows it to jam in multiple directions. This is beneficial in that it allows our jammer to protect against microphones hidden out of sight. We evaluated our jammer in a series of experiments and found that: (1) it jams in all directions, e.g., our device jams over 87% of the words uttered around it in any direction, while existing devices jam only 30% when not pointed directly at the microphone; (2) it exhibits significantly less blind spots; and, (3) our device induced a feeling of privacy to participants of our user study. We believe our wearable provides stronger privacy in a world in which most devices are constantly eavesdropping on our conversations.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376304
ER  - 

TY  - JOUR
AU  - Jones, Benjamin; Mei, Yuxuan; Zhao, Haisen; Gotfrid, Taylor; Mankoff, Jennifer; Schulz, Adriana
TI  - Computational Design of Knit Templates
PY  - 2021
AB  - <jats:p>We present an interactive design system for knitting that allows users to create template patterns that can be fabricated using an industrial knitting machine. Our interactive design tool is novel in that it allows direct control of key knitting design axes we have identified in our formative study and does so consistently across the variations of an input parametric template geometry. This is achieved with two key technical advances. First, we present an interactive meshing tool that lets users build a coarse quadrilateral mesh that adheres to their knit design guidelines. This solution ensures consistency across the parameter space for further customization over shape variations and avoids helices, promoting knittability. Second, we lift and formalize low-level machine knitting constraints to the level of this coarse quad mesh. This enables us to not only guarantee hand- and machine-knittability, but also provides automatic design assistance through auto-completion and suggestions. We show the capabilities through a set of fabricated examples that illustrate the effectiveness of our approach in creating a wide variety of objects and interactively exploring the space of design variations.</jats:p>
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 2
PB  - 
DO  - 10.1145/3488006
ER  - 

TY  - NA
AU  - Zhu, Junyi; Zhu, Yunyi; Cui, Jiaming; Cheng, Leon; Snowden, Jackson C.; Chounlakone, Mark; Wessely, Michael; Mueller, Stefanie
TI  - UIST - MorphSensor: A 3D Electronic Design Tool for Reforming Sensor Modules
PY  - 2020
AB  - MorphSensor is a 3D electronic design tool that enables designers to morph existing sensor modules of pre-defined two-dimensional shape into free-form electronic component arrangements that better integrate with the three-dimensional shape of a physical prototype. MorphSensor builds onto existing sensor module schematics that already define the electronic components and the wiring required to build the sensor. Since MorphSensor maintains the wire connections throughout the editing process, the sensor remains fully functional even when designers change the electronic component layout on the prototype geometry. We detail the MorphSensor editor that supports designers in re-arranging the electronic components, and discuss a fabrication pipeline based on customized PCB footprints for making the resulting freeform sensor. We then demonstrate the capabilities of our system by morphing a range of sensor modules of different complexity and provide a technical evaluation of the quality of the resulting free-form sensors.
SP  - 541
EP  - 553
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415898
ER  - 

TY  - NA
AU  - Paneva, Viktorija; Bachynskyi, Myroslav; Müller, Jörg
TI  - Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in Virtual Reality
PY  - 2020
AB  - We present the Levitation Simulator, a system that enables researchers and designers to iteratively develop and prototype levitation interface ideas in Virtual Reality. This includes user tests and formal experiments. We derive a model of the movement of a levitating particle in such an interface. Based on this, we develop an interactive simulation of the levitation interface in VR, which exhibits the dynamical properties of the real interface. The results of a Fitts' Law pointing study show that the Levitation Simulator enables performance, comparable to the real prototype. We developed the first two interactive games, dedicated for levitation interfaces: LeviShooter and BeadBounce, in the Levitation Simulator, and then implemented them on the real interface. Our results indicate that participants experienced similar levels of user engagement when playing the games, in the two environments. We share our Levitation Simulator as Open Source, thereby democratizing levitation research, without the need for a levitation apparatus.
SP  - 1
EP  - 12
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376409
ER  - 

TY  - JOUR
AU  - Hirayama, Ryuji; Christopoulos, Giorgos; Martinez Plasencia, Diego; Subramanian, Sriram
TI  - High-speed acoustic holography with arbitrary scattering objects.
PY  - 2022
AB  - Recent advances in high-speed acoustic holography have enabled levitation-based volumetric displays with tactile and audio sensations. However, current approaches do not compute sound scattering of objects' surfaces; thus, any physical object inside can distort the sound field. Here, we present a fast computational technique that allows high-speed multipoint levitation even with arbitrary sound-scattering surfaces and demonstrate a volumetric display that works in the presence of any physical object. Our technique has a two-step scattering model and a simplified levitation solver, which together can achieve more than 10,000 updates per second to create volumetric images above and below static sound-scattering objects. The model estimates transducer contributions in real time by reformulating the boundary element method for acoustic holography, and the solver creates multiple levitation traps. We explain how our technique achieves its speed with minimum loss in the trap quality and illustrate how it brings digital and physical content together by demonstrating mixed-reality interactive applications.
SP  - eabn7614
EP  - NA
JF  - Science advances
VL  - 8
IS  - 24
PB  - 
DO  - 10.1126/sciadv.abn7614
ER  - 

TY  - NA
AU  - Reed, Courtney N.; Skach, Sophie; Strohmeier, Paul; McPherson, Andrew P.
TI  - Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519412
ER  - 

TY  - NA
AU  - Sidenmark, Ludwig; Clarke, Christopher; Zhang, Xuesong; Phu, Jenny; Gellersen, Hans
TI  - CHI - Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality
PY  - 2020
AB  - In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.
SP  - 1
EP  - 13
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376438
ER  - 

TY  - JOUR
AU  - Fushimi, Tatsuki; Yamamoto, Kenta; Ochiai, Yoichi
TI  - Acoustic Hologram Optimisation Using Automatic Differentiation
PY  - 2021
AB  - Acoustic holograms are the keystone of modern acoustics. It encodes three-dimensional acoustic fields in two dimensions, and its quality determine the performance of acoustic systems. Optimisation methods that control only the phase of an acoustic wave are considered inferior to methods that control both the amplitude and phase of the wave. In this paper, we present Diff-PAT, an acoustic hologram optimisation algorithm with automatic differentiation. We demonstrate that our method achieves superior accuracy than conventional methods. The performance of Diff-PAT was evaluated by randomly generating 1000 sets of up to 32 control points for single-sided arrays and single-axis arrays. The improved acoustic hologram can be used in wide range of applications of PATs without introducing any changes to existing systems that control the PATs. In addition, we applied Diff-PAT to acoustic metamaterial and achieved an >8 dB increase in the peak noise-to-signal ratio of acoustic hologram.
SP  - 12678
EP  - 12678
JF  - Scientific reports
VL  - 11
IS  - 1
PB  - 
DO  - 10.1038/s41598-021-91880-2
ER  - 

TY  - NA
AU  - Nittala, Aditya Shekhar; Khan, Arshad; Kruttwig, Klaus; Kraus, Tobias; Steimle, Jürgen
TI  - CHI - PhysioSkin: Rapid Fabrication of Skin-Conformal Physiological Interfaces
PY  - 2020
AB  - Advances in rapid prototyping platforms have made physiological sensing accessible to a wide audience. However, off-the-shelf electrodes commonly used for capturing biosignals are typically thick, non-conformal and do not support customization. We present PhysioSkin, a rapid, do-it-yourself prototyping method for fabricating custom multi-modal physiological sensors, using commercial materials and a commodity desktop inkjet printer. It realizes ultrathin skin-conformal patches (~1μm) and interactive textiles that capture sEMG, EDA and ECG signals. It further supports fabricating devices with custom levels of thickness and stretchability. We present detailed fabrication explorations on multiple substrate materials, functional inks and skin adhesive materials. Informed from the literature, we also provide design recommendations for each of the modalities. Evaluation results show that the sensor patches achieve a high signal-to-noise ratio. Example applications demonstrate the functionality and versatility of our approach for prototyping a next generation of physiological devices that intimately couple with the human body.
SP  - 1
EP  - 10
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376366
ER  - 

TY  - NA
AU  - Sidenmark, Ludwig; Potts, Dominic; Bapisch, Bill; Gellersen, Hans
TI  - CHI - Radi-Eye: Hands-Free Radial Interfaces for 3D Interaction using Gaze-Activated Head-Crossing
PY  - 2021
AB  - Eye gaze and head movement are attractive for hands-free 3D interaction in head-mounted displays, but existing interfaces afford only limited control. Radi-Eye is a novel pop-up radial interface designed to maximise expressiveness with input from only the eyes and head. Radi-Eye provides widgets for discrete and continuous input and scales to support larger feature sets. Widgets can be selected with Look & Cross, using gaze for pre-selection followed by head-crossing as trigger and for manipulation. The technique leverages natural eye-head coordination where eye and head move at an offset unless explicitly brought into alignment, enabling interaction without risk of unintended input. We explore Radi-Eye in three augmented and virtual reality applications, and evaluate the effect of radial interface scale and orientation on performance with Look & Cross. The results show that Radi-Eye provides users with fast and accurate input while opening up a new design space for hands-free fluid interaction.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445697
ER  - 

TY  - JOUR
AU  - Nittala, Aditya Shekhar; Karrenbauer, Andreas; Khan, Arshad; Kraus, Tobias; Steimle, Jürgen
TI  - Computational design and optimization of electro-physiological sensors.
PY  - 2021
AB  - Electro-physiological sensing devices are becoming increasingly common in diverse applications. However, designing such sensors in compact form factors and for high-quality signal acquisition is a challenging task even for experts, is typically done using heuristics, and requires extensive training. Our work proposes a computational approach for designing multi-modal electro-physiological sensors. By employing an optimization-based approach alongside an integrated predictive model for multiple modalities, compact sensors can be created which offer an optimal trade-off between high signal quality and small device size. The task is assisted by a graphical tool that allows to easily specify design preferences and to visually analyze the generated designs in real-time, enabling designer-in-the-loop optimization. Experimental results show high quantitative agreement between the prediction of the optimizer and experimentally collected physiological data. They demonstrate that generated designs can achieve an optimal balance between the size of the sensor and its signal acquisition capability, outperforming expert generated solutions. Though skin-conformable electro-physiological sensors are attractive for epidermal electronics, their optimal design remains a challenge. Here, the authors report a computational design approach for realizing multi-modal electro-physiological sensors that optimizes electrode layout design.
SP  - 1
EP  - 14
JF  - Nature communications
VL  - 12
IS  - 1
PB  - 
DO  - 10.1038/s41467-021-26442-1
ER  - 

TY  - NA
AU  - Auda, Jonas; Verheyen, Nils; Mayer, Sven; Schneegass, Stefan
TI  - Flyables: Haptic Input Devices for Virtual Realityusing Quadcopters
PY  - 2021
AB  - Virtual Reality (VR) has made its way into everyday life. While VR delivers an ever-increasing level of immersion, controls and their haptics are still limited. Current VR headsets come with dedicated controllers that are used to control every virtual interface element. However, the controller input mostly differs from the virtual interface. This reduces immersion. To provide a more realistic input, we present Flyables, a toolkit that provides matching haptics for virtual user interface elements using quadcopters. We took five common virtual UI elements and built their physical counterparts. We attached them to quadcopters to deliver on-demand haptic feedback. In a user study, we compared Flyables to controller-based VR input. While controllers still outperform Flyables in terms of precision and task completion time, we found that Flyables present a more natural and playful way to interact with VR environments. Based on the results from the study, we outline research challenges that could improve interaction with Flyables in the future.
SP  - NA
EP  - NA
JF  - Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3489849.3489855
ER  - 

TY  - NA
AU  - Zhao, Xuan; Fan, Mingming; Han, Teng
TI  - "I Don't Want People to Look At Me Differently"
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517552
ER  - 

TY  - JOUR
AU  - Kim, Hubert; Asbeck, Alan T.
TI  - An elbow exoskeleton for haptic feedback made with a direct drive hobby motor
PY  - 2020
AB  - Abstract A direct drive motor is one of the simplest mechanisms that can be used to move a mechanical joint. In particular, a brushless direct current (BLDC) motor with no gearing produces a low parasitic torque due to its backdrivability and low inertia, which is ideal for some applications such as wearable systems. While capable of operating with a higher power density than brushed motors, BLDC motors require accurate position feedback to be controlled via vector control at slow speeds. The MotorWare ™ library from Texas Instruments (TI), which is designed to run with a C2000 microcontroller, is written to run BLDCs. However, the code was written to run the motor continuously with an incremental encoder and requires further engineering to be used at low speeds such as in an exoskeleton. In this paper, we present the design of an elbow exoskeleton that can be used for haptic feedback. We provide instructions to build the exoskeleton hardware, custom code to modify software provided by TI so that a motor can provide a controlled torque at low speeds, code to enable the microcontroller to communicate with a computer for high-level commands and data storage, and also provide an overview of how alternate motors could be used with this software setup.
SP  - e00153
EP  - NA
JF  - HardwareX
VL  - 8
IS  - NA
PB  - 
DO  - 10.1016/j.ohx.2020.e00153
ER  - 

TY  - JOUR
AU  - Meiklejohn, Elizabeth; Hagan, Brooks; Ko, Joy
TI  - Rapid Sketching of Woven Textile Behavior: The Experimental Use of Parametric Modeling and Interactive Simulation in the Weaving Process
PY  - 2022
AB  - NA
SP  - 103263
EP  - 103263
JF  - Computer-Aided Design
VL  - 149
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2022.103263
ER  - 

TY  - NA
AU  - Cheung, Victor; Girouard, Audrey
TI  - CHI Extended Abstracts - Exploring Acceptability and Utility of Deformable Interactive Garment Buttons
PY  - 2020
AB  - Wearable devices have received tremendous interest in fitness and personal assistance sectors. Yet most are still worn as auxiliary hardware; falling short in ubiquity and convenience. We examine the potential of a novel deformable wearable device that embeds interactive technologies into garment buttons, and seek to enhance the form factor of buttons to incorporate deformation and motion as inputs. We surveyed garment buttons in everyday clothing to inform an exploratory study, where we investigated social acceptance and elicited interaction gestures using mockups. Our results indicate people mostly prefer smaller sizes, and regard sleeves as the most comfortable area to operate and look at when seen by others. Based on our findings, we discuss potential context of use, possible applications, and future work.
SP  - 1
EP  - 8
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3382941
ER  - 

TY  - NA
AU  - Ku, Pin-Sung; Gong, Jun; Wu, Te-Yen; Wei, Yixin; Tang, Yiwen; Ens, Barrett; Yang, Xing-Dong
TI  - CHI - Zippro: The Design and Implementation of An Interactive Zipper
PY  - 2020
AB  - Zippers are common in a wide variety of objects that we use daily. This work investigates how we can take advantage of such common daily activities to support seamless interaction with technology. We look beyond simple zipper-sliding interactions explored previously to determine how to weave foreground and background interactions into a vocabulary of natural usage patterns. We begin by conducting two user studies to understand how people typically interact with zippers. The findings identify several opportunities for zipper input and sensing, which inform the design of Zippro, a self-contained prototype zipper slider, which we evaluate with a standard jacket zipper. We conclude by demonstrating several applications that make use of the identified foreground and background input methods.
SP  - 627
EP  - NA
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376756
ER  - 

TY  - JOUR
AU  - Haji‐Georgi, Maria; Xu, Xinyun; Rosca, Oxana
TI  - Academic conferencing in 2020: A virtual conference model
PY  - 2020
AB  - The COVID-19 pandemic caused many institutions to adapt or cancel their events in the Spring of 2020 One organization at the State University of New York, University at Albany took this opportunity to innovatively transform a conference-style poster session to a virtual format Overall, the virtual conference was well accepted by participants and guests This article is a case study of the event and elaborates on the feedback received We present a model for adapting conferences and poster sessions to virtual formats with recommendations for adaptation © 2020 Wiley Periodicals LLC
SP  - 176
EP  - 184
JF  - Human Behavior and Emerging Technologies
VL  - 3
IS  - 1
PB  - 
DO  - 10.1002/hbe2.235
ER  - 

TY  - BOOK
AU  - Masnadi, Sina; Gonzalez, Andres N. Vargas; Williamson, Brian M.; LaViola, Joseph J.
TI  - VR Workshops - AffordIt!: A Tool for Authoring Object Component Behavior in VR
PY  - 2020
AB  - This paper presents AffordIt!, a tool for adding affordances to the component parts of a virtual object. Following 3D scene reconstruction and segmentation procedures, domain experts find themselves with complete virtual objects, but no intrinsic behaviors have been assigned, forcing them to use unfamiliar Desktop-based 3D editing tools. Our solution allows a user to select a region of interest for a mesh cutter tool, assign an intrinsic behavior and view an animation preview of their work. To evaluate the usability and workload of AffordIt! we ran an exploratory study to gather feedback. Results show high usability and low workload ratings.
SP  - 740
EP  - 741
JF  - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vrw50115.2020.00221
ER  - 

TY  - NA
AU  - Barrera-Machuca, Mayra D.; Cassinelli, Alvaro; Sandor, Christian
TI  - UIST (Adjunct Volume) - Context-Based 3D Grids for Augmented Reality User Interfaces
PY  - 2020
AB  - Accurate 3D registration of real and virtual objects is a crucial step in AR, especially when manipulating those objects in space. Previous work simplifies mid-air 3D manipulations by removing one or more degrees of freedom by constraining motion using automatic algorithms. However, when designing objects, limiting the user's actions can affect their creativity. To solve this problem, we present a new system called Context-based 3D Grids that allows users to do precise mid-air 3D manipulations without constraining their actions. Our system creates 3D grids for each object in the scene that change depending on the object pose. Users can display additional reference frames inside the virtual environment using natural hand gestures that are commonly used when designing an object. Our goal is to help users visualize more clearly the spatial relation and the differences in pose and size of the objects.
SP  - 73
EP  - 76
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416190
ER  - 

TY  - CHAP
AU  - Naito, Daiki; Kajimoto, Hiroyuki
TI  - EuroHaptics - Haptic Display Using Fishing Rod
PY  - 2020
AB  - This paper proposes a grounded haptic display that can widely present a force sensation using a fishing rod. The proposed haptic display incorporates a fishing rod to present the force sensation at the user’s fingertip using a thread being reeled from the tip of the fishing rod. The multidirectional force sensation is presented by driving the base supporting the fishing rod using a pan-tilt mechanism, and wending the thread using the reel. The fishing rod is lightweight and moves at high speed. Additionally, it is possible to widely present the force by bending the fishing rod. This device requires conversion from the position of the user’s fingertip and magnitude of the force to the necessary winding force of the thread and the posture of the fishing rod. This study developed a control method and evaluated it by considering a case wherein the force to be presented is exerted in the vertical direction.
SP  - 325
EP  - 333
JF  - Haptics: Science, Technology, Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-030-58147-3_36
ER  - 

TY  - NA
AU  - Leake, Mackenzie; Bernstein, Gilbert; Agrawala, Maneesh
TI  - Sketch-Based Design of Foundation Paper Pieceable Quilts
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545643
ER  - 

TY  - NA
AU  - Gopinath, Sridhar; Ghanathe, Nikhil Pratap; Seshadri, Vivek; Sharma, Rahul
TI  - PLDI - Compiling KB-sized machine learning models to tiny IoT devices
PY  - 2019
AB  - Recent advances in machine learning (ML) have produced KiloByte-size models that can directly run on constrained IoT devices. This approach avoids expensive communication between IoT devices and the cloud, thereby enabling energy-efficient real-time analytics. However, ML models are expressed typically in floating-point, and IoT hardware typically does not support floating-point. Therefore, running these models on IoT devices requires simulating IEEE-754 floating-point using software, which is very inefficient. We present SeeDot, a domain-specific language to express ML inference algorithms and a compiler that compiles SeeDot programs to fixed-point code that can efficiently run on constrained IoT devices. We propose 1) a novel compilation strategy that reduces the search space for some key parameters used in the fixed-point code, and 2) new efficient implementations of expensive operations. SeeDot compiles state-of-the-art KB-sized models to various microcontrollers and low-end FPGAs. We show that SeeDot outperforms 1) software emulation of floating-point (Arduino), 2) high-bitwidth fixed-point (MATLAB), 3) post-training quantization (TensorFlow-Lite), and 4) floating- and fixed-point FPGA implementations generated using high-level synthesis tools.
SP  - 79
EP  - 95
JF  - Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3314221.3314597
ER  - 

TY  - NA
AU  - Abdelrahman, Yomna; Mathis, Florian; Knierim, Pascal; Kettler, Axel; Alt, Florian; Khamis, Mohamed
TI  - CueVR: Studying the Usability of Cue-based Authentication for Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 International Conference on Advanced Visual Interfaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3531073.3531092
ER  - 

TY  - BOOK
AU  - Hirai, Rio; Ikeda, Ryo; Shizuki, Buntarou
TI  - AsianCHI@CHI - Preliminary Investigation of Text Entry Method with Haptic Feedback from Real Object Surfaces Estimated Using Hand Tracking on HMD
PY  - 2021
AB  - In virtual reality systems, users enter text by selecting virtual keys with the fingers. A virtual keyboard is displayed in mid-air and thus does not provide haptic feedback. To address this problem, we present a text entry method that uses the surfaces of real objects around the user to provide haptic feedback. A real object surface touched a hand is recognized using the position and posture of the hand acquired by a hand-tracking sensor on a head-mounted display; a virtual keyboard is placed on that surface to provide haptic feedback. We performed a pilot study to compare text entry performance when the virtual keyboard was placed in mid-air, on a wall, on a desk, and on the user’s thighs. The result shows that each virtual keyboard placement and the presence/absence of haptic feedback did not affect input performance.
SP  - 129
EP  - 131
JF  - Asian CHI Symposium 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3429360.3468194
ER  - 

TY  - JOUR
AU  - Tsironi Lamari, Agapi; Panagiotakis, Spyros; Kamarianakis, Zacharias; Loukas, George; Malamos, Athanasios; Markakis, Evangelos
TI  - Construction of a Low-Cost Layered Interactive Dashboard with Capacitive Sensing
PY  - 2022
AB  - <jats:p>In the present work, a methodology for the low-cost crafting of an interactive layered dashboard is proposed. Our aim is that the tangible surface be constructed using domestic materials that are easily available in every household. Several tests were performed on different capacitive materials before the selection of the most suitable one for use as a capacitive touch sensor. Various calibration methods were evaluated so that the behavior of the constructed capacitive touch sensors is smooth and reliable. The layered approach is achieved by a menu of few touch buttons on the left side of the dashboard. Thus, various different layers of content can be projected over the same construction, offering extendibility and ease of use to the users. For demonstration purposes, we developed an entertaining plus an educational application of projection mapping for the pervasive and interactive projection of multimedia content to the users of the presented tangible interface. The whole design and implementation approach are thoroughly analyzed in the paper and are presented through the illustration and application of various multimedia layers over the dashboard. An evaluation of the final construction proves the feasibility of the proposed work.</jats:p>
SP  - 304
EP  - 304
JF  - Information
VL  - 13
IS  - 6
PB  - 
DO  - 10.3390/info13060304
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Fukushima, Manaka; Tanaka, Namiki; Matoba, Yasushi; Ikematsu, Kaori; Siio, Itiro
TI  - TEI - Electrolysis Bubble Display based Art Installations
PY  - 2021
AB  - Research was conducted on a digital information display using electrolysis bubbles. Although the research mainly focused on information displays in daily life, the ephemerality of bubbles is also a promising method for dynamic art installations. In this paper, we present two novel artworks using this electrolysis bubble display mechanism. First, we present “UTAKATA,” a ticker-like bubble display, using a running-water channel. Seven electrodes are placed linearly on the channel bed, and they generate text messages using bubble dots that drift toward the lower end of the channel. Second, we present the “Bubble Mirror,” which is a water pan with a camera that captures a visitor’s face and displays it using electrolysis bubbles as pixels. Facial images with six levels of grayscales are displayed on the water surface using 32 × 32 electrodes. We evaluated the output properties of these configurations and discuss the results obtained.
SP  - 13
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3440632
ER  - 

TY  - JOUR
AU  - Liu, Zishun; Han, Xingjian; Zhang, Yuchen; Chen, Xiangjia; Lai, Yu-Kun; Doubrovski, Eugeni L.; Whiting, Emily; Wang, Charlie C. L.
TI  - Knitting 4D garments with elasticity controlled for body motion
PY  - 2021
AB  - In this paper, we present a new computational pipeline for designing and fabricating 4D garments as knitwear that considers comfort during body movement. This is achieved by careful control of elasticity distribution to reduce uncomfortable pressure and unwanted sliding caused by body motion. We exploit the ability to knit patterns in different elastic levels by single-jersey jacquard (SJJ) with two yarns. We design the distribution of elasticity for a garment by physics-based computation, the optimized elasticity on the garment is then converted into instructions for a digital knitting machine by two algorithms proposed in this paper. Specifically, a graph-based algorithm is proposed to generate knittable stitch meshes that can accurately capture the 3D shape of a garment, and a tiling algorithm is employed to assign SJJ patterns on the stitch mesh to realize the designed distribution of elasticity. The effectiveness of our approach is verified on simulation results and on specimens physically fabricated by knitting machines.
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3450626.3459868
ER  - 

TY  - JOUR
AU  - Cheng, Tingyu; Narumi, Koya; Do, Youngwook; Zhang, Yang; Ta, Tung D.; Sasatani, Takuya; Markvicka, Eric; Kawahara, Yoshihiro; Yao, Lining; Abowd, Gregory D.; Oh, Hyunjoo
TI  - Silver Tape: Inkjet-Printed Circuits Peeled-and-Transferred on Versatile Substrates
PY  - 2020
AB  - We propose Silver Tape, a simple yet novel fabrication technique to transfer inkjet-printed silver traces from paper onto versatile substrates, without time-/space- consuming processes such as screen printing or heat sintering. This allows users to quickly implement silver traces with a variety of properties by exploiting a wide range of substrates. For instance, high flexibility can be achieved with Scotch tape, high transparency with polydimethylsiloxane (PDMS), heat durability with Kapton polyimide tape, water solubility with 3M water-soluble tape, and beyond. Many of these properties are not achievable with conventional substrates that are used for inkjet-printing conductive traces. Specifically, our technique leverages the commonly undesired low adhesion property of the inkjet printing films and repurposes these films as temporary transfer media. We describe our fabrication methods with a library of materials we can utilize, evaluate the mechanical and electrical properties of the transferred traces, and conclude with several demonstrative applications. We believe Silver Tape enriches novel interactions for the ubiquitous computing domain, by enabling digital fabrication of electronics on versatile materials, surfaces, and shapes.
SP  - 1
EP  - 17
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 4
IS  - 1
PB  - 
DO  - 10.1145/3381013
ER  - 

TY  - NA
AU  - Yi, Xin; Lu, Yiqin; Cai, Ziyin; Wu, Zihan; Wang, Yuntao; Shi, Yuanchun
TI  - GazeDock: Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu
PY  - 2022
AB  - Gaze-only input techniques in VR face the challenge of avoiding false triggering due to continuous eye tracking while maintaining interaction performance. In this paper, we proposed GazeDock, a technique for enabling fast and robust gaze-based menu selection in VR. GazeDock features a view-fixed peripheral menu layout that automatically triggers appearing and selection when the user&#x2019;s gaze approaches and leaves the menu zone, thus facilitating interaction speed and minimizing the false triggering rate. We built a dataset of 12 participants&#x2019; natural gaze movements in typical VR applications. By analyzing their gaze movement patterns, we designed the menu UI personalization and optimized selection detection algorithm of GazeDock. We also examined users&#x2019; gaze selection precision for targets on the peripheral menu and found that 4&#x2013;8 menu items yield the highest throughput when considering both speed and accuracy. Finally, we validated the usability of GazeDock in a VR navigation game that contains both scene exploration and menu selection. Results showed that GazeDock achieved an average selection time of 471ms and a false triggering rate of 3.6%. And it received higher user preference ratings compared with dwell-based and pursuit-based techniques.
SP  - NA
EP  - NA
JF  - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/vr51125.2022.00105
ER  - 

TY  - JOUR
AU  - Peixiao, Zheng; Jiang, Gaoming; Cong, Honglian
TI  - Design Method of Circular Weft-Knitted Jacquard Fabric Based on Jacquard Module
PY  - 2022
AB  - <jats:title>Abstract</jats:title> <jats:p>Recently, there is an increasing interest in design of circular weft jacquard because of the pursuit of fashion and comfort. Aiming at the complexity of the computer-aided design method of the existing circular weft-knitted jacquard fabrics, which is not conducive to the rapid design and intelligible for designers, a design method was proposed to transform pattern notation into knitting diagram efficiently, which was based on knitting rules and its creation as a set of jacquard modules. Knitting characteristics of jacquard fabrics were studied as a precondition. On this basis, the design procedures of jacquard modules were analyzed and illustrated by taking tricolor bird's eye backing jacquard as an example. Jacquard modules with various jacquard effects were designed and stored in a jacquard module database. To mathematically describe pattern notation, knitting diagram, and jacquard module, two-dimensional matrixes were established by the method of mathematical modeling, and a corresponding algorithm for the transformation of the pattern to knitting information according to the knitting rules of jacquard modules, which can be applied to ordinary jacquard fabrics was summarized. The project of tricolor circular weft-knitted jacquard with bird's eye in the reverse and four-color air-layer jacquard were taken for instance to verify the models and algorithm. The results obtained show that the approach can efficiently and conveniently realize the designation and machine-knitting of weft-knitted jacquard fabric, which provide a theoretical basis and notation of modeling for the computer-aided design of circular weft-knitted jacquard fabrics.</jats:p>
SP  - 217
EP  - 224
JF  - Autex Research Journal
VL  - 22
IS  - 2
PB  - 
DO  - 10.2478/aut-2020-0062
ER  - 

TY  - NA
AU  - Pei, Siyou; Chari, Pradyumna; Wang, Xue; Yang, Xiaoying; Kadambi, Achuta; Zhang, Yang
TI  - ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545622
ER  - 

TY  - NA
AU  - Kim, Taejun; Ham, Auejin; Ahn, Sunggeun; Lee, Geehyuk
TI  - Lattice Menu: A Low-Error Gaze-Based Marking Menu Utilizing Target-Assisted Gaze Gestures on a Lattice of Visual Anchors
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501977
ER  - 

TY  - NA
AU  - Jane, L E; Zhai, Kevin Y.; Echevarria, Jose; Fried, Ohad; Hanrahan, Pat; Landay, James A.
TI  - UIST - Dynamic Guidance for Decluttering Photographic Compositions
PY  - 2021
AB  - Unwanted clutter in a photo can be incredibly distracting. However in the moment, photographers have so many things to simultaneously consider, it can be hard to catch every detail. Designers have long known the benefits of abstraction for seeing a more holistic view of their design. We wondered if, similarly, some form of image abstraction might be helpful for photographers as an alternative perspective or “lens” with which to see their image. Specifically, we wondered if such abstraction might draw the photographer’s attention away from details in the subject to noticing objects in the background, such as unwanted clutter. We present our process for designing such a camera overlay, based on the idea of using abstraction to recognize clutter. Our final design uses object-based saliency and edge detection to highlight contrast along subject and image borders, outlining potential distractors in these regions. We describe the implementation and evaluation of a capture-time tool that interactively displays these overlays and find that the tool is helpful for making users more confident in their ability to take decluttered photos that clearly convey their intended story.
SP  - 359
EP  - 371
JF  - The 34th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3472749.3474755
ER  - 

TY  - JOUR
AU  - Kaspar, Alexandre; Wu, Kui; Luo, Yiyue; Makatura, Liane; Matusik, Wojciech
TI  - Knit sketching
PY  - 2021
AB  - NA
SP  - 1
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3476576.3476614
ER  - 

TY  - NA
AU  - Arita, Shoko; Shimoda, Yusuke; Toda, Kazunari; Yamagiwa, Yoshiki
TI  - Initial Study on Mission of Cubic Deployable Structure with Thin Membrane Circuit
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - AIAA Scitech 2021 Forum
VL  - NA
IS  - NA
PB  - 
DO  - 10.2514/6.2021-1705
ER  - 

TY  - NA
AU  - Cui, Zhitong; Gong, Hebo; Wang, Yanan; Shen, Chengyi; Zou, Wenyin; Luo, Shijian
TI  - AutomotiveUI - Enhancing Interactions for In-Car Voice User Interface with Gestural Input on the Steering Wheel
PY  - 2021
AB  - Voice user interfaces (VUI) are becoming indispensable in car for offering drivers the opportunity to make distraction-free inputs and conduct complex tasks. However, the usability and control efficiency of today’s VUI remain to be enhanced due to its sequential nature. In this work, we explored gestural input on the steering wheel to improve the interaction efficiency of VUI. Based on limitations in VUI, we designed novel gestural commands on the steering wheel to augment them. We also elicited corresponding user-defined gestures by exploring drivers’ touch behavior. Then, we implemented a prototype attached to the steering wheel for recognizing gestures. Finally, we evaluated our system’s usability regarding driving performance, interaction efficiency, cognitive workload and user feedback. Results revealed that our system improved the control efficiency of VUI and reduced workload without a significant reduction in driving distraction than just using VUI.
SP  - 59
EP  - 68
JF  - 13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3409118.3475126
ER  - 

TY  - NA
AU  - Venzke, Marcus; Klisch, Daniel; Kubik, Philipp; Ali, Asad; Missier, Jesper Dell; Turau, Volker
TI  - Artificial Neural Networks for Sensor Data Classification on Small Embedded Systems.
PY  - 2020
AB  - In this paper we investigate the usage of machine learning for interpreting measured sensor values in sensor modules. In particular we analyze the potential of artificial neural networks (ANNs) on low-cost micro-controllers with a few kilobytes of memory to semantically enrich data captured by sensors. The focus is on classifying temporal data series with a high level of reliability. Design and implementation of ANNs are analyzed considering Feed Forward Neural Networks (FFNNs) and Recurrent Neural Networks (RNNs). We validate the developed ANNs in a case study of optical hand gesture recognition on an 8-bit micro-controller. The best reliability was found for an FFNN with two layers and 1493 parameters requiring an execution time of 36 ms. We propose a workflow to develop ANNs for embedded devices.
SP  - NA
EP  - NA
JF  - arXiv: Learning
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Strohmeier, Paul; Honnet, Cedric; Perner-Wilson, Hannah; Teyssier, Marc; Fruchard, Bruno; Baptista, Ana Catarina; Steimle, Jürgen
TI  - CHI Extended Abstracts - Demo of PolySense: How to Make Electrically Functional Textiles
PY  - 2020
AB  - We demonstrate a simple and accessible method for enhancing textiles with custom piezo-resistive properties. Based on in-situ polymerization, our method offers seamless integration at the material level, preserving a textile's haptic and mechanical properties. We demonstrate how to enhance a wide set of fabrics and yarns using only readily available tools. During each demo session, conference attendees may bring textile samples which will be polymerized in a shared batch. Attendees may keep these samples. While the polymerization is happening, attendees can inspect pre-made samples and explore how these might be integrated in functional circuits. Examples objects created using polymerization include rapid manufacturing of on-body interfaces, tie-dyed motion-capture clothing, and zippers that act as potentiometers.
SP  - 1
EP  - 4
JF  - Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3334480.3383148
ER  - 

TY  - JOUR
AU  - Rezaie, Maryam; Malekmakan, Morteza; Shirehjini, Ali Asghar Nazari; Shirmohammadi, Shervin
TI  - The Effect of Room Complexity on Physical Object Selection Performance in 3-D Mobile User Interfaces
PY  - 2020
AB  - An important challenge in smart environments is how to manipulate the smart objects. Although mobile applications are typically used for controlling a smart environment, no previous study has evaluated the users performance in manipulating smart objects under different environmental complexities. This article presents an experimental comparison between three different selection techniques 3-D, 2-D, and physical user interfaces (UIs). We evaluate these techniques across two levels of environment complexity measuring 51 participants timing data and errors. Our results indicate that the 3-D UI is superior for task completion time and error, and the 2-D UI is not a better solution than the physical UI when the environment is not complex. The results also show the importance of considering the environment complexity in choosing the proper UI.
SP  - 349
EP  - 357
JF  - IEEE Transactions on Human-Machine Systems
VL  - 50
IS  - 4
PB  - 
DO  - 10.1109/thms.2020.2984750
ER  - 

TY  - JOUR
AU  - Papavlasopoulou, Sofia; Sharma, Kshitij; Melhart, David; Schellekens, Jasper; Lee-Cultura, Serena; Giannakos, Michail N.; Yiannakakis, Georgios N.
TI  - Investigating gaze interaction to support children’s gameplay
PY  - 2021
AB  - Abstract Gaze interaction has become an affordable option in the development of innovative interaction methods for user input. Gaze holds great promise as an input modality, offering increased immersion and opportunities for combined interactions (e.g., gaze and mouse, touch). However, the use of gaze as an input modality to support children’s gameplay has not been examined to unveil those opportunities. To investigate the potential of gaze interaction to support children’s gameplay, we designed and developed a game that enables children to utilize gaze interaction as an input modality. Then, we performed a between subjects research design study with 28 children using mouse as an input mechanism and 29 children using their gaze (8–14 years old). During the study, we collected children’s attitudes (via self-reported questionnaire) and actual usage behavior (using facial video, physiological data and computer logs). The results show no significant difference on children’s attitudes regarding the ease of use and enjoyment of the two conditions, as well as on the scores achieved and number of sessions played. Usage data from children’s facial video and physiological data show that sadness and stress are significantly higher in the mouse condition, while joy, surprise, physiological arousal and emotional arousal are significantly higher in the gaze condition. In addition, our findings highlight the benefits of using multimodal data to reveal children’s behavior while playing the game, by complementing self-reported measures. As well, we uncover a need for more studies to examine gaze as an input mechanism.
SP  - 100349
EP  - NA
JF  - International Journal of Child-Computer Interaction
VL  - 30
IS  - NA
PB  - 
DO  - 10.1016/j.ijcci.2021.100349
ER  - 

TY  - NA
AU  - Vechev, Velko; Hinchet, Ronan; Coros, Stelian; Thomaszewski, Bernhard; Hilliges, Otmar
TI  - Computational Design of Active Kinesthetic Garments
PY  - 2022
AB  - Garments with the ability to provide kinesthetic force-feedback on-demand can augment human capabilities in a non-obtrusive way, enabling numerous applications in VR haptics, motion assistance, and robotic control. However, designing such garments is a complex, and often manual task, particularly when the goal is to resist multiple motions with a single design. In this work, we propose a computational pipeline for designing connecting structures between active components - one of the central challenges in this context. We focus on electrostatic (ES) clutches that are compliant in their passive state while strongly resisting elongation when activated. Our method automatically computes optimized connecting structures that efficiently resist a range of pre-defined body motions on demand. We propose a novel dual-objective optimization approach to simultaneously maximize the resistance to motion when clutches are active, while minimizing resistance when inactive. We demonstrate our method on a set of problems involving different body sites and a range of motions. We further fabricate and evaluate a subset of our automatically created designs against manually created baselines using mechanical testing and in a VR pointing study.
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545674
ER  - 

TY  - BOOK
AU  - Ezcurdia, Iñigo
TI  - ISS Companion - Volumetric Reach-through Displays for Direct Manipulation of 3D Content
PY  - 2020
AB  - In my PhD, I aim at developing a reach-through volumetric display where points of light are emitted from each 3d position of the display volume, and yet it allows people to introduce theirs hands inside to directly interact with the rendered content. Here, I present TomoLit, an inverse tomographic display, where multiple emitters project rays of different intensities for each angle, rendering a target image in mid-air. We have analysed the effect on image quality of the number of emitters, their locations, the angular resolution and the levels of intensities. We have developed a simple emitter and we are in the process of putting together multiple of them. And what I plan to do next, e.g. moving from 2D to 3D and exploring interaction techniques. The feedback obtained in this symposium will clearly dissipate some of of my doubts and guide my research career.
SP  - 107
EP  - 110
JF  - Companion Proceedings of the 2020 Conference on Interactive Surfaces and Spaces
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3380867.3427410
ER  - 

TY  - NA
AU  - Deng, Jialin; Olivier, Patrick; Andres, Josh; Ellis, Kirsten; Wee, Ryan; Floyd Mueller, Florian
TI  - Logic Bonbon: Exploring Food as Computational Artifact
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3501926
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Kato, Kunihiro; Ikematsu, Kaori; Kawahara, Yoshihiro; Siio, Itiro
TI  - CircWood: Laser Printed Circuit Boards and Sensors for Affordable DIY Woodworking
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490149.3501317
ER  - 

TY  - NA
AU  - Ruston, Olivia; Watts, Leon; Fraser, Mike
TI  - Conference on Designing Interactive Systems - More than it Seams: Garment Stitching in Wearable e-Textiles
PY  - 2021
AB  - Abstract: We consider how seams in clothing can be designed for interactive sensing of body movement. Conductive yarn is activated at low voltages which fluctuate as body movement varies tension across the seams. Traditional garment construction describes how fabrics, blocks and seam types are configured according to mechanical performance and historical changes in fashion. Bodice blocks span the human torso, anchoring and reflecting core body movement. We explore the stability and calibration of seam voltage in a bodice block, while performing a yoga-inspired movement sequence. Our exploratory studies test signal orthogonality between seam stitch types and seam placements for supporting reliable sensing. We use our exploratory work to consider design alternatives which balance seam requirements against sensing opportunities, including back seams for torso garments. Our findings contrast optimal seam placement and design in wearable e-textiles with those in traditional clothing.
SP  - 1171
EP  - 1182
JF  - Designing Interactive Systems Conference 2021
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3461778.3462103
ER  - 

TY  - NA
AU  - Fender, Andreas; Plasencia, Diego Martinez; Subramanian, Sriram
TI  - CHI - ArticuLev: An Integrated Self-Assembly Pipeline for Articulated Multi-Bead Levitation Primitives
PY  - 2021
AB  - Acoustic levitation is gaining popularity as an approach to create physicalized mid-air content by levitating different types of levitation primitives. Such primitives can be independent particles or particles that are physically connected via threads or pieces of cloth to form shapes in mid-air. However, initialization (i.e., placement of such primitives in their mid-air target locations) currently relies on either manual placement or specialized ad-hoc implementations, which limits their practical usage. We present ArticuLev, an integrated pipeline that deals with the identification, assembly and mid-air placement of levitated shape primitives. We designed ArticuLev with the physical properties of commonly used levitation primitives in mind. It enables experiences that seamlessly combine different primitives into meaningful structures (including fully articulated animated shapes) and supports various levitation display approaches (e.g., particles moving at high speed). In this paper, we describe our pipeline and demonstrate it with heterogeneous combinations of levitation primitives.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445342
ER  - 

TY  - NA
AU  - Klamka, Konstantin; Dachselt, Raimund; Steimle, Jürgen
TI  - CHI - Rapid Iron-On User Interfaces: Hands-on Fabrication of Interactive Textile Prototypes
PY  - 2020
AB  - Rapid prototyping of interactive textiles is still challenging, since manual skills, several processing steps, and expert knowledge are involved. We present Rapid Iron-On User Interfaces, a novel fabrication approach for empowering designers and makers to enhance fabrics with interactive functionalities. It builds on heat-activated adhesive materials consisting of smart textiles and printed electronics, which can be flexibly ironed onto the fabric to create custom interface functionality. To support rapid fabrication in a sketching-like fashion, we developed a handheld dispenser tool for directly applying continuous functional tapes of desired length as well as discrete patches. We introduce versatile compositions techniques that allow for creating complex circuits, utilizing commodity textile accessories and sketching custom-shaped I/O modules. We further contribute a comprehensive library of components for input, output, wiring and computing. Three example applications, results from technical experiments and expert reviews demonstrate the functionality, versatility and potential of this approach.
SP  - 1
EP  - 14
JF  - Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3313831.3376220
ER  - 

TY  - NA
AU  - Strohmeier, Paul; Pourjafarian, Narjes; Koelle, Marion; Honnet, Cedric; Fruchard, Bruno; Steimle, Jürgen
TI  - AHs - Sketching On-Body Interactions using Piezo-Resistive Kinesiology Tape
PY  - 2020
AB  - Skin is personal and sensitive. As a result, design and placement of on-body physical interfaces need to be well thought out. One way of "getting the design right" is to quickly sketch a multitude of designs to be modified, adjusted and elaborated on. To date, on-body rapid prototyping methods do not afford these "quick-and-dirty" design processes. We propose using piezo-resistive kinesiology tape as a low-cost and versatile resource for sketching functional on-skin interfaces. Our method uses pretreated kinesiology tape, which is made piezo-resistive through polymerization, and serves as touch, pressure and stretch sensor. We illustrate ketching techniques with both pretreated and untreated tape for iterative design of on-skin interfaces. In addition, we contribute a set of sensor primitives that facilitate various input modalities for creating interactive sketches.
SP  - NA
EP  - NA
JF  - Proceedings of the Augmented Humans International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3384657.3384774
ER  - 

TY  - NA
AU  - Wang, Tongyan; Mankoff, Jennifer; Hofmann, Megan
TI  - Fabricating Accessible Designs with Knitting Machines
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Symposium on Computational Fabrication
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3559400.3565584
ER  - 

TY  - JOUR
AU  - Mai, Chunming; Xie, Dongliang; Zeng, Lina; Li, Zaijin; Li, Zhibo; Qiao, Zhongliang; Qu, Yi; Liu, Guojun; Li, Lin
TI  - Laser Sensing and Vision Sensing Smart Blind Cane: A Review
PY  - 2023
AB  - <jats:p>Laser sensing and vision sensing smart canes can improve the convenience of travel for the visually impaired, but for the present, most of the system functions of laser sensing and vision sensing smart canes are still defective. Guide equipment and smart blind canes are introduced and classified first, and the smart blind canes based on vision sensing, laser sensing and laser vision sensing are investigated, respectively, and the research status of laser vision sensing smart blind canes is sorted out. The advantages and disadvantages of various laser vision sensing smart blind canes are summarized, especially the research development of laser vision fusion as the core of new smart canes. The future development prospects of laser vision sensing smart blind cane are overviewed, to boost the development of laser vision sensing smart blind cane, to provide safe and efficient travel guarantee for the visually impaired.</jats:p>
SP  - 869
EP  - NA
JF  - Sensors
VL  - 23
IS  - 2
PB  - 
DO  - 10.3390/s23020869
ER  - 

TY  - JOUR
AU  - Hossain, Ishtia Zahir; Khan, Ashaduzzaman; Hossain, Gaffar
TI  - A Piezoelectric Smart Textile for Energy Harvesting and Wearable Self-Powered Sensors
PY  - 2022
AB  - <jats:p>Today’s wearable electronics have dramatically altered our daily lives and created an urgent demand for new and intelligent sensor technologies. As a new energy source, self-powering sensors are currently seen as critically important units for wearable and non-wearable textile–electronic systems. To this aim, this paper presents a smart textile-based piezoelectric energy-autonomous harvester and a self-powered sensor for wearable application, where the sandwich structure of the wearable sensor consists of top and bottom textile conductors, and in between the two textile electrodes there is a piezoelectric PVDF thin film. The generating voltage, current, charge, power, and capacitor charging–discharging behaviour of the device were confirmed using multimeter, oscilloscope, Keithley, etc., analyses. Finally, a piezoelectric-textile sensor was integrated into wearable clothes for breathing detection; a shoe insole for footstep recognition; and it can store energy by tapping, to power electronics, such as a calculator, timer, LED, etc., at a later time. The sensitivity of the sensor was enough for generating voltage from a tiny water droplet. Thus, we can assume raindrops to be utilized as a power-generating source on days when no sun is available to solar cells.</jats:p>
SP  - 5541
EP  - 5541
JF  - Energies
VL  - 15
IS  - 15
PB  - 
DO  - 10.3390/en15155541
ER  - 

TY  - NA
AU  - Mayer, Sven; Xu, Xiangyu; Harrison, Chris
TI  - CHI - Super-Resolution Capacitive Touchscreens
PY  - 2021
AB  - Capacitive touchscreens are near-ubiquitous in today’s touch-driven devices, such as smartphones and tablets. By using rows and columns of electrodes, specialized touch controllers are able to capture a 2D image of capacitance at the surface of a screen. For over a decade, capacitive “pixels” have been around 4 millimeters in size – a surprisingly low resolution that precludes a wide range of interesting applications. In this paper, we show how super-resolution techniques, long used in fields such as biology and astronomy, can be applied to capacitive touchscreen data. By integrating data from many frames, our software-only process is able to resolve geometric details finer than the original sensor resolution. This opens the door to passive tangibles with higher-density fiducials and also recognition of every-day metal objects, such as keys and coins. We built several applications to illustrate the potential of our approach and report the findings of a multipart evaluation.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445703
ER  - 

TY  - BOOK
AU  - Mutasim, Aunnoy K; Batmaz, Anil Ufuk; Stuerzlinger, Wolfgang
TI  - ETRA Short Papers - Pinch, Click, or Dwell: Comparing Different Selection Techniques for Eye-Gaze-Based Pointing in Virtual Reality
PY  - 2021
AB  - While a pinch action is gaining popularity for selection of virtual objects in eye-gaze-based systems, it is still unknown how well this method performs compared to other popular alternatives, e.g., a button click or a dwell action. To determine pinch’s performance in terms of execution time, error rate, and throughput, we implemented a Fitts’ law task in Virtual Reality (VR) where the subjects pointed with their (eye-)gaze and selected / activated the targets by pinch, clicking a button, or dwell. Results revealed that although pinch was slower, made more errors, and had less throughput compared to button clicks, none of these differences were significant. Dwell exhibited the least errors but was significantly slower and achieved less throughput compared to the other conditions. Based on these findings, we conclude that the pinch gesture is a reasonable alternative to button clicks for eye-gaze-based VR systems.
SP  - NA
EP  - NA
JF  - ACM Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3448018.3457998
ER  - 

TY  - JOUR
AU  - Shen, Yingle; Dong, Zhijia; Cong, Honglian
TI  - Structure modeling in three-dimensional simulation of weft-knitted seamless kneepads
PY  - 2021
AB  - This paper proposes a simulation method suitable for weft-knitted seamless kneepads. The purpose of this study is to realize the simulation of complete weft-knitted products, and it has the advanta...
SP  - 004051752110408
EP  - 617
JF  - Textile Research Journal
VL  - 92
IS  - 3-4
PB  - 
DO  - 10.1177/00405175211040872
ER  - 

TY  - JOUR
AU  - Torabi, Ali; Nazari, Ali A.; Conrad-Baldwin, Everly; Zareinia, Kourosh; Tavakoli, NA
TI  - Kinematic design of linkage-based haptic interfaces for medical applications: a review
PY  - 2021
AB  - <jats:title>Abstract</jats:title> <jats:p>A haptic interface recreates haptic feedback from virtual environments or haptic teleoperation systems that engages the user’s sense of touch. High-fidelity haptic feedback is critical to the safety and success of any interaction with human beings. Such interactions can be seen in haptic systems utilized in medical fields, such as for surgical training, robotic tele-surgery, and tele-rehabilitation, which require appropriate haptic interface design and control. In order to recreate high-fidelity soft and stiff contact experiences for the user in the intended application, different designs strike different trade-offs between the desirable characteristics of an interface, such as back-drivability, low apparent inertia and low friction for the best perception of small reflected forces, large intrinsic stiffness and force feedback capability for the best perception of large reflected forces, a large-enough workspace for exploring the remote or virtual environment, and the uniformity of haptic feedback and its adequate sensitivity over the workspace. Meeting all of the requirements simultaneously is impossible, and different application-driven compromises need to be made. This paper reviews how various kinematic designs have helped address these trade-offs in desired specifications. First, we investigate the required characteristics of linkage-based haptic interfaces and inevitable trade-offs between them. Then, we study the state of the art in the kinematic design of haptic interfaces and their advantages and limitations. In all sections, we consider the applications of the intended haptic interfaces in medical scenarios. Non-linkage-based haptic interfaces are also shortly discussed to show the broad range of haptic technologies in the area. The potentials of kinematic redundancy to address the design trade-offs are introduced. Current challenges and future directions of haptic interface designs for medical applications are shortly discussed, which is finally followed by the conclusion.</jats:p>
SP  - 022005
EP  - NA
JF  - Progress in Biomedical Engineering
VL  - 3
IS  - 2
PB  - 
DO  - 10.1088/2516-1091/abee66
ER  - 

TY  - JOUR
AU  - Plasencia, Diego Martinez; Hirayama, Ryuji; Montano-Murillo, Roberto A.; Subramanian, Sriram
TI  - GS-PAT: high-speed multi-point sound-fields for phased arrays of transducers
PY  - 2020
AB  - Phased Arrays of Transducers (PATs) allow accurate control of ultrasound fields, with applications in haptics, levitation (i.e. displays) and parametric audio. However, algorithms for multi-point levitation or tactile feedback are usually limited to computing solutions in the order of hundreds of sound-fields per second, preventing the use of multiple high-speed points, a feature that can broaden the scope of applications of PATs. We present GS-PAT, a GPU multi-point phase retrieval algorithm, capable of computing 17K solutions per second for up to 32 simultaneous points in a mid-end consumer grade GPU (NVidia GTX 1660). We describe the algorithm and compare it to state of the art multi-point algorithms used for ultrasound haptics and levitation, showing similar quality of the generated sound-fields, and much higher computation rates. We then illustrate how the shift in paradigm enabled by GS-PAT (i.e. real-time control of several high-speed points) opens new applications for PAT technologies, such as in volumetric fully coloured displays, multi-point spatio-temporal tactile feedback, parametric audio and simultaneous combinations of these modalities.
SP  - 138
EP  - NA
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 4
PB  - 
DO  - 10.1145/3386569.3392492
ER  - 

TY  - JOUR
AU  - Adhanom, Isayas Berhe; MacNeilage, Paul; Folmer, Eelke
TI  - Eye Tracking in Virtual Reality: a Broad Review of Applications and Challenges
PY  - 2023
AB  - <jats:title>Abstract</jats:title><jats:p>Eye tracking is becoming increasingly available in head-mounted virtual reality displays with various headsets with integrated eye trackers already commercially available. The applications of eye tracking in virtual reality are highly diversified and span multiple disciplines. As a result, the number of peer-reviewed publications that study eye tracking applications has surged in recent years. We performed a broad review to comprehensively search academic literature databases with the aim of assessing the extent of published research dealing with applications of eye tracking in virtual reality, and highlighting challenges, limitations and areas for future research.</jats:p>
SP  - NA
EP  - NA
JF  - Virtual Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10055-022-00738-z
ER  - 

TY  - NA
AU  - Kaspar, Alexandre; Makatura, Liane; Matusik, Wojciech
TI  - UIST - Knitting Skeletons: A Computer-Aided Design Tool for Shaping and Patterning of Knitted Garments
PY  - 2019
AB  - This work presents a novel interactive system for simple garment composition and surface patterning. Our approach makes it easier for casual users to customize machine-knitted garments, while enabling more advanced users to design their own composable templates. Our tool combines ideas from CAD software and image editing: it allows the composition of (1) parametric knitted primitives, and (2) stitch pattern layers with different resampling behaviours. By leveraging the regularity of our primitives, our tool enables interactive customization with automated layout and real-time patterning feedback. We show a variety of garments and patterns created with our tool, and highlight our ability to transfer shape and pattern customizations between users.
SP  - 53
EP  - 65
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347879
ER  - 

TY  - NA
AU  - Zhang, Zuoyi; Zheng, Huizhe; Rempel, Sawyer; Hong, Kenny; Han, Teng; Sakamoto, Yumiko; Irani, Pourang
TI  - AH - A smart utensil for detecting food pick-up gesture and amount while eating
PY  - 2020
AB  - Higher food intake rates (i.e., eating too fast) are linked to several health concerns such as an elevated risk of obesity or gastritis. Raising awareness of one's eating habits can regulate one's pace of food intake. In this paper, we propose a novel smart-eating utensil that can potentially increase the users' awareness of their eating rate by detecting their food pick-up gesture as well as the weight upon each bite. We design and implement a proof-of-concept prototype fork with multiple embedded sensors and processor to collect the eating data. After that, we propose a solution for food pick-up gesture detection and food amount estimation in each food pick-up. We assess the accuracy of our solution through ten successful data collection sessions with participants. We demonstrate that our method has strong potential to accurately detect the food pick-up gesture and estimate the amount of food on each pick-up.
SP  - NA
EP  - NA
JF  - Proceedings of the 11th Augmented Human International Conference
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3396339.3396361
ER  - 

TY  - JOUR
AU  - Sterman, Yoav; Almog, Eli
TI  - A Computational Design Tool for Gradual Transition of Knit Structures in Seamless Circular Knitting
PY  - 2022
AB  - NA
SP  - 103214
EP  - 103214
JF  - Computer-Aided Design
VL  - 146
IS  - NA
PB  - 
DO  - 10.1016/j.cad.2022.103214
ER  - 

TY  - NA
AU  - Khalip, Suhaila; Alkawaz, Mohammed Hazim; Puaad, Syamim Nadhira Ahmad
TI  - 3D Digital Animal Model: Complication and Future Impact
PY  - 2019
AB  - Life frameworks shape the reason for clinical assessment and are essential to the present helpful instructive modules. Notwithstanding, continuously obvious in the composing is a contribution from clinicians and masters about the clear nonappearance of anatomical data among late veterinarian graduated class. To appreciate the issues incorporating understudy learning and support of life structures, a mixed procedure setup was utilized to examine helpful understudies' anatomical data all through their Bachelor of Veterinary Science (BVSc) declaration. The results revealed that data of life structures declined after some time and this was basic in the last two clinical years. The drop in anatomical data was not uniform. The regions of life frameworks better held were connected with ordinary introduction and fortress in the clinical years. Time-constrained instructive modules, poor compromise in the clinical years and exceptional open entryways for reevaluating and testing life frameworks are the principal purposes behind the lessening of learning. These results in like manner highlight the prerequisite for determined knowledge at the period of teaching; the noteworthiness of vertical compromise in outfitting understudies with ceaseless learning openings in the clinical years; and, the estimation of formative testing understudies' data of life structures all through the clinical years.
SP  - NA
EP  - NA
JF  - 2019 IEEE 7th Conference on Systems, Process and Control (ICSPC)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/icspc47137.2019.9068060
ER  - 

TY  - NA
AU  - Jang, Jaehyun; Frier, William; Park, Jinah
TI  - Multimodal Volume Data Exploration through Mid-Air Haptics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar55827.2022.00039
ER  - 

TY  - JOUR
AU  - Paneva, Viktorija; Fleig, Arthur; Plasencia, Diego MartíNez; Faulwasser, Timm; Müller, Jörg
TI  - OptiTrap: Optimal Trap Trajectories for Acoustic Levitation Displays
PY  - 2022
AB  - <jats:p> Acoustic levitation has recently demonstrated the ability to create volumetric content by trapping and quickly moving particles along reference paths to reveal shapes in mid-air. However, the problem of specifying physically feasible trap trajectories to display desired shapes remains unsolved. Even if only the final shape is of interest to the content creator, the trap trajectories need to determine where and when the traps need to be, for the particle to reveal the intended shape. We propose <jats:italic>OptiTrap</jats:italic> , the first structured numerical approach to compute trap trajectories for acoustic levitation displays. Our approach generates trap trajectories that are physically feasible and nearly time-optimal, and reveal generic mid-air shapes, given only a reference path (i.e., a shape with no time information). We provide a multi-dimensional model of the acoustic forces around a trap to model the trap-particle system dynamics and compute optimal trap trajectories by formulating and solving a non-linear path following problem. We formulate our approach and evaluate it, demonstrating how <jats:italic>OptiTrap</jats:italic> consistently produces feasible and nearly optimal paths, with increases in size, frequency, and accuracy of the shapes rendered, allowing us to demonstrate larger and more complex shapes than ever shown to date. </jats:p>
SP  - 1
EP  - 14
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 5
PB  - 
DO  - 10.1145/3517746
ER  - 

TY  - NA
AU  - Pourjafarian, Narjes; Strohmeier, Paul
TI  - Polymerized Tape
PY  - 2021
AB  - We present polymerized sports tape as a general purpose prototyping and sensor-design resource. We use it in a somewhat similar manner to how conductive copper tape is often used for fast production of lo-fi electrical prototypes. However, copper tape has a number of drawbacks if one is prototyping with soft materials, textiles, or even directly on the human body. Because it is not elastic, it does not adhere well to such materials. Sports tape, however, has the desired elastic qualities, and additionally is designed to adhere not only to arbitrary objects, but also to human skin. We polymerize sports tape to make it conductive. The resulting conductive tape has a series of electrical properties which make it an exciting sensor-material. It is piezo-resistive. This means its resistance changes with applied forces. It can also be used to create a voltage gradient, which enables simple and precise position sensing. This unique combination of mechanical and electrical properties makes it an exciting and unique prototyping resource.
SP  - NA
EP  - NA
JF  - Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3430524.3444706
ER  - 

TY  - NA
AU  - Hertel, Julia; Karaosmanoglu, Sukran; Schmidt, Susanne; Bräker, Julia; Semmann, Martin; Steinicke, Frank
TI  - ISMAR - A Taxonomy of Interaction Techniques for Immersive Augmented Reality based on an Iterative Literature Review
PY  - 2021
AB  - Developers of interactive systems have a variety of interaction techniques to choose from, each with individual strengths and limitations in terms of the considered task, context, and users. While there are taxonomies for desktop, mobile, and virtual reality applications, augmented reality (AR) taxonomies have not been established yet. However, recent advances in immersive AR technology (i.e., head-worn or projection-based AR), such as the emergence of untethered headsets with integrated gesture and speech sensors, have enabled the inclusion of additional input modalities and, therefore, novel multimodal interaction methods have been introduced. To provide an overview of interaction techniques for current immersive AR systems, we conducted a literature review of publications between 2016 and 2021. Based on 44 relevant papers, we developed a comprehensive taxonomy focusing on two identified dimensions – task and modality. We further present an adaptation of an iterative taxonomy development method to the field of human-computer interaction. Finally, we discuss observed trends and implications for future work.
SP  - 431
EP  - 440
JF  - 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ismar52148.2021.00060
ER  - 

TY  - NA
AU  - Sun, Lingyun; Shao, Ziqian; Luo, Danli; Gu, Jianzhe; Tao, Ye; Yao, Lining; Wang, Guanyun
TI  - UIST (Adjunct Volume) - FabricFit: Transforming Form-Fitting Fabrics
PY  - 2020
AB  - Textiles encompass a wide variety and rich characteristics that ranges from soft to tough, foldable to rigid, cuttable to stiff, elastic to plastic. On the other hand, 3D printing thermoplastic (e.g., PLA) materials exhibit controlled deformation at a certain transition temperature. In this paper, we present a method of fabricating form-fitting composite textiles by printing PLA on different fabric substrates, designing different structures and transforming the fabric-PLA composite to form different textures.
SP  - 99
EP  - 101
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416198
ER  - 

TY  - JOUR
AU  - Reizabal, Ander; Castro, Nelson; Pereira, Nelson; Costa, Carlos M.; Pérez, Leyre; Vilas-Vilela, José Luis; Lanceros-Méndez, Senentxu
TI  - Silk Fibroin Nanocomposites with Indium Tin Oxide toward Sustainable Capacitive Touch Sensing Applications
PY  - 2022
AB  - NA
SP  - 1901
EP  - 1909
JF  - ACS Applied Electronic Materials
VL  - 4
IS  - 4
PB  - 
DO  - 10.1021/acsaelm.2c00100
ER  - 

TY  - NA
AU  - Gotfrid, Taylor; Mack, Kelly; Lum, Kathryn J; Yang, Evelyn; Hodgins, Jessica K.; Hudson, Scott E.; Mankoff, Jennifer
TI  - CHI - Stitching Together the Experiences of Disabled Knitters
PY  - 2021
AB  - Knitting is a popular craft that can be used to create customized fabric objects such as household items, clothing and toys. Additionally, many knitters find knitting to be a relaxing and calming exercise. Little is known about how disabled knitters use and benefit from knitting, and what accessibility solutions and challenges they create and encounter. We conducted interviews with 16 experienced, disabled knitters and analyzed 20 threads from six forums that discussed accessible knitting to identify how and why disabled knitters knit, and what accessibility concerns remain. We additionally conducted an iterative design case study developing knitting tools for a knitter who found existing solutions insufficient. Our innovations improved the range of stitches she could produce. We conclude by arguing for the importance of improving tools for both pattern generation and modification as well as adaptations or modifications to existing tools such as looms to make it easier to track progress
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445521
ER  - 

TY  - NA
AU  - Kasai, Takumi; Furumoto, Takuro; Shinoda, Hiroyuki
TI  - Rotation and Position Control of a Cubic Object Using Airborne Ultrasound
PY  - 2020
AB  - The long-term goal of our research is to create a mid-air robot that can be driven using airborne ultrasound phased array devices placed in the surrounding environment. In this study, we developed a method to control the position and rotation of a cube-shaped object using airborne ultrasound. Multiple phased array devices synthesize focal points on the surface of a balloon to exert acoustic radiation force upon it. We constructed a prototype system and performed dynamic PID control based on visual feedback. Through experiments, we confirmed that it was possible to control the position and the rotation around the yaw axis of a cube-shaped object simultaneously. Our method is useful for long-duration midair robot applications such as 24-hour surveillance and all-day exhibitions.
SP  - NA
EP  - NA
JF  - 2020 IEEE International Ultrasonics Symposium (IUS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/ius46767.2020.9251409
ER  - 

TY  - JOUR
AU  - Caluya, Nicko R.; Plopski, Alexander; Sandor, Christian; Fujimoto, Yuichiro; Kanbara, Masayuki; Kato, Hirokazu
TI  - Does overlay field of view in head-mounted displays affect spatial memorization?
PY  - 2022
AB  - Abstract One of the main targets of criticism of head-mounted displays (HMDs) is the field of view (FOV) size, whether in virtual or augmented reality. This limitation is prominent with optical see-through head-mounted displays (OST-HMD), as those with narrow overlay FOV (OFOV) sizes only provide a small window to view virtual objects. We investigated if restricting this OFOV negatively affects a user’s ability to memorize spatial locations in a simulation of a work environment, and consequently, long-term memory transfer to an equivalent scenario in the real world two days later. To find empirical evidence, we conducted a within-subjects experiment with 18 participants performing in three phases with an OST-HMD, simulated on an immersive HMD. For each phase, they viewed the training scenario with a different OFOV size of the augmentable area (30°, 70°, 110° diagonal). Results from recall tests showed that smaller OFOV size did not significantly affect user’s performance on both short-term and transfer tests, but HMD data revealed that users rotated their heads less with a 110° OFOV. We also found that proximity of objects to memorize had an interaction effect with smaller OFOV sizes. Our findings could have implications on the design and HMD choices of augmented training.
SP  - 554
EP  - 565
JF  - Computers & Graphics
VL  - 102
IS  - NA
PB  - 
DO  - 10.1016/j.cag.2021.09.004
ER  - 

TY  - JOUR
AU  - Lee, Joon Hyub; Kim, Hanbit; Bae, Seok-Hyung
TI  - Rapid design of articulated objects
PY  - 2022
AB  - <jats:p>Designing articulated objects is challenging because, unlike with static objects, it requires complex decisions to be made regarding the form, parts, rig, poses, and motion. We present a novel 3D sketching system for rapidly authoring concepts of articulated objects for the early stages of design, when designers make such decisions. Compared to existing CAD software, which focuses on slowly but elaborately producing models consisting of precise surfaces and volumes, our system focuses on quickly but roughly producing models consisting of key curves through a small set of coherent pen and multi-touch gestures. We found that professional designers could easily learn and use our system and author compelling concepts in a short time, showing that 3D sketching can be extended to designing articulated objects and is generally applicable in film, animation, game, and product design.</jats:p>
SP  - 1
EP  - 8
JF  - ACM Transactions on Graphics
VL  - 41
IS  - 4
PB  - 
DO  - 10.1145/3528223.3530092
ER  - 

TY  - NA
AU  - Choi, Youngkyung; Ryu, Neung; Kim, Myung Jin; Dementyev, Artem; Bianchi, Andrea
TI  - UIST - BodyPrinter: Fabricating Circuits Directly on the Skin at Arbitrary Locations Using a Wearable Compact Plotter
PY  - 2020
AB  - On-body electronics and sensors offer the opportunity to seamlessly augment the human with computing power. Accordingly, numerous previous work investigated methods that exploit conductive materials and flexible substrates to fabricate circuits in the form of wearable devices, stretchable patches, and stickers that can be attached to the skin. For all these methods, the fabrication process involves several manual steps, such as designing the circuit in software, constructing conductive patches, and manually placing these physical patches on the body. In contrast, in this work, we propose to fabricate electronics directly on the skin. We present BodyPrinter, a wearable conductive-ink deposition machine, that prints flexible electronics directly on the body using skin-safe conductive ink. The paper describes our system in detail and, through a series of examples and a technical evaluation, we show how direct on-body fabrication of electronic circuits and sensors can further enhance the human body.
SP  - 554
EP  - 564
JF  - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379337.3415840
ER  - 

TY  - JOUR
AU  - Kaspar, Alexandre; Wu, Kui; Luo, Yiyue; Makatura, Liane; Matusik, Wojciech
TI  - Knit sketching: from cut & sew patterns to machine-knit garments
PY  - 2021
AB  - We present a novel workflow to design and program knitted garments for industrial whole-garment knitting machines. Inspired by traditional garment making based on cutting and sewing, we propose a sketch representation with additional annotations necessary to model the knitting process. Our system bypasses complex editing operations in 3D space, which allows us to achieve interactive editing of both the garment shape and its underlying time process. We provide control of the local knitting direction, the location of important course interfaces, as well as the placement of stitch irregularities that form seams in the final garment. After solving for the constrained knitting time process, the garment sketches are automatically segmented into a minimal set of simple regions that can be knitted using simple knitting procedures. Finally, our system optimizes a stitch graph hierarchically while providing control over the tradeoff between accuracy and simplicity. We showcase different garments created with our web interface.
SP  - 1
EP  - 15
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3450626.3459752
ER  - 

TY  - JOUR
AU  - Chen, Yu-Chun; Liao, Chia-Ying; Hsu, Shuo-wen; Huang, Da-Yuan; Chen, Bing-Yu
TI  - Exploring User Defined Gestures for Ear-Based Interactions
PY  - 2020
AB  - The human ear is highly sensitive and accessible, making it especially suitable for being used as an interface for interacting with smart earpieces or augmented glasses. However, previous works on ear-based input mainly address gesture sensing technology and researcher-designed gestures. This paper aims to bring more understandings of gesture design. Thus, for a user elicitation study, we recruited 28 participants, each of whom designed gestures for 31 smart device-related tasks. This resulted in a total of 868 gestures generated. Upon the basis of these gestures, we compiled a taxonomy and concluded the considerations underlying the participants' designs that also offer insights into their design rationales and preferences. Thereafter, based on these study results, we propose a set of user-defined gestures and share interesting findings. We hope this work can shed some light on not only sensing technologies of ear-based input, but also the interface design of future wearable interfaces.
SP  - 1
EP  - 20
JF  - Proceedings of the ACM on Human-Computer Interaction
VL  - 4
IS  - ISS
PB  - 
DO  - 10.1145/3427314
ER  - 

TY  - NA
AU  - Hofmann, Megan; Albaugh, Lea; Sethapakadi, Ticha; Hodgins, Jessica K.; Hudson, Scott E.; McCann, James; Mankoff, Jennifer
TI  - UIST - KnitPicking Textures: Programming and Modifying Complex Knitted Textures for Machine and Hand Knitting
PY  - 2019
AB  - Knitting creates complex, soft fabrics with unique texture properties that can be used to create interactive objects.However, little work addresses the challenges of designing and using knitted textures computationally. We present KnitPick: a pipeline for interpreting hand-knitting texture patterns into KnitGraphs which can be output to machine and hand-knitting instructions. Using KnitPick, we contribute a measured and photographed data set of 472 knitted textures. Based on findings from this data set, we contribute two algorithms for manipulating KnitGraphs. KnitCarving shapes a graph while respecting a texture, and KnitPatching combines graphs with disparate textures while maintaining a consistent shape. KnitPick is the first system to bridge the gap between hand- and machine-knitting when creating complex knitted textures.
SP  - 5
EP  - 16
JF  - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3332165.3347886
ER  - 

TY  - JOUR
AU  - Wu, Rundong; Zhang, Joy Xiaoji; Leaf, Jonathan; Hua, Xinru; Qu, Ante; Harvey, Claire; Holtzman, Emily; Ko, Joy; Hagan, Brooks; James, Doug L.; Guimbretière, François; Marschner, Steve
TI  - Weavecraft: an interactive design and simulation tool for 3D weaving
PY  - 2020
AB  - 3D weaving is an emerging technology for manufacturing multilayer woven textiles. In this work, we present Weavecraft: an interactive, simulation-based design tool for 3D weaving. Unlike existing textile software that uses 2D representations for design patterns, we propose a novel weave block representation that helps the user to understand 3D woven structures and to create complex multi-layered patterns. With Weavecraft, users can create blocks either from scratch or by loading traditional weaves, compose the blocks into large structures, and edit the pattern at various scales. Furthermore, users can verify the design with a physically based simulator, which predicts and visualizes the geometric structure of the woven material and reveals potential defects at an interactive rate. We demonstrate a range of results created with our tool, from simple two-layer cloth and well known 3D structures to a more sophisticated design of a 3D woven shoe, and we evaluate the effectiveness of our system via a formative user study.
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 39
IS  - 6
PB  - 
DO  - 10.1145/3414685.3417865
ER  - 

TY  - JOUR
AU  - Hellum, Owen; Kersten-Oertel, Marta; Xiao, Yiming
TI  - Assessment of user-interaction strategies for neurosurgical data navigation and annotation in virtual reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Virtual Reality
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/s10055-022-00740-5
ER  - 

TY  - NA
AU  - Kusupati, Aditya; Ramanujan, Vivek; Somani, Raghav; Wortsman, Mitchell; Jain, Prateek; Kakade, Sham M.; Farhadi, Ali
TI  - Soft Threshold Weight Reparameterization for Learnable Sparsity
PY  - 2020
AB  - Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at this https URL.
SP  - NA
EP  - NA
JF  - arXiv: Learning
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Gao, Lei; Hardwick, James; Plasencia, Diego Martinez; Subramanian, Sriram; Hirayama, Ryuji
TI  - DATALEV: Acoustophoretic Data Physicalisation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558638
ER  - 

TY  - NA
AU  - Ahn, Sunggeun; Santosa, Stephanie; Parent, Mark; Wigdor, Daniel; Grossman, Tovi; Giordano, Marcello
TI  - CHI - StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR
PY  - 2021
AB  - This work explores the design of marking menus for gaze-based AR/VR menu selection by expert and novice users. It first identifies and explains the challenges inherent in ocular motor control and current eye tracking hardware, including overshooting, incorrect selections, and false activations. Through three empirical studies, we optimized and validated design parameters to mitigate these errors while reducing completion time, task load, and eye fatigue. Based on the findings from these studies, we derived a set of design guidelines to support gaze-based marking menus in AR/VR. To overcome the overshoot errors found with eye-based expert marking menu behaviour, we developed StickyPie, a marking menu technique that enables scale-independent marking input by estimating saccade landing positions. An evaluation of StickyPie revealed that StickyPie was easier to learn than the traditional technique (i.e., RegularPie) and was 10% more efficient after 3 sessions.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445297
ER  - 

TY  - NA
AU  - Suresh, Harini; Gomez, Steven R.; Nam, Kevin K.; Satyanarayan, Arvind
TI  - Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs
PY  - 2021
AB  - To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders' knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the interpretability literature, adds precision to the design and comparison of user studies, and facilitates a more reflexive approach to conducting this research.
SP  - NA
EP  - NA
JF  - Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411764.3445088
ER  - 

TY  - JOUR
AU  - Cheng, Tingyu; Li, Bu; Zhang, Yang; Li, Yunzhi; Ramey, Charles; Jung, Eui Min; Cui, Yepu; Swaminathan, Saiganesh; Do, Youngwook; Tentzeris, Manos M.; Abowd, Gregory D.; Oh, Hyunjoo
TI  - Duco: Autonomous Large-Scale Direct-Circuit-Writing (DCW) on Vertical Everyday Surfaces Using A Scalable Hanging Plotter
PY  - 2021
AB  - Human environments are filled with large open spaces that are separated by structures like walls, facades, glass windows, etc. Most often, these structures are largely passive offering little to no interactivity. In this paper, we present Duco, a large-scale electronics fabrication robot that enables room-scale & building-scale circuitry to add interactivity to vertical everyday surfaces. Duco negates the need for any human intervention by leveraging a hanging robotic system that automatically sketches multi-layered circuity to enable novel large-scale interfaces. The key idea behind Duco is that it achieves single-layer or multi-layer circuit fabrication on 2D surfaces as well as 2D cutouts that can be assembled into 3D objects by loading various functional inks (e.g., conductive, dielectric, or cleaning) to the wall-hanging drawing robot, as well as employing an optional laser cutting head as a cutting tool. Our technical evaluation shows that Duco's mechanical system works reliably on various surface materials with a wide range of roughness and surface morphologies. The system achieves superior mechanical tolerances (0.1mm XY axis resolution and 1mm smallest feature size). We demonstrate our system with five application examples, including an interactive piano, an IoT coffee maker controller, an FM energy-harvester printed on a large glass window, a human-scale touch sensor and a 3D interactive lamp.
SP  - 1
EP  - 25
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 5
IS  - 3
PB  - 
DO  - 10.1145/3478118
ER  - 

TY  - NA
AU  - Maekawa, Azumi; Saito, Hiroto; Uriu, Daisuke; Kasahara, Shunichi; Inami, Masahiko
TI  - Machine-Mediated Teaming: Mixture of Human and Machine in Physical Gaming Experience
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517555
ER  - 

TY  - JOUR
AU  - Costa, Pedro; Polícia, Rita; Perinka, Nikola; Alesanco, Yolanda; Viñuales, Ana; Carvalho, Estela O.; Pereira, Nelson; Fernandes, Margarida M.; Lanceros‐Mendez, Senentxu
TI  - Multifunctional Touch Sensing and Antibacterial Polymer‐Based Core‐Shell Metallic Nanowire Composites for High Traffic Surfaces
PY  - 2022
AB  - The transmission of bacterial infections through contaminated surfaces is nowadays an increasing source of concern, also related to the current pandemic situation. Functional materials that prevent the adhesion of microorganisms and/or induce their eradication thus avoiding fomite transmission are highly needed. In this work, a highly antimicrobial hybrid with sensorial capability is developed to be further applied as interactive high traffic surface coatings. The nanocomposite is composed of polyvinylidene fluoride (PVDF), a highly stable fluorinated polymer, incorporating copper core-shell nanowires (NWs). The NWs comprised of copper and shelled with silver is highly antimicrobial, inducing a full kill effect against Escherichia coli and Staphylococcus epidermidis strains but biocompatible towards mammalian cells at concentrations below 0.5 mg mL−1. Further NWs incorporation on PVDF matrix retains its antimicrobial activity reducing in 6.5 logs the E. coli and 4.5 logs the S. epidermidis. NW/PVDF composites demonstrate suitable mechanical and electrical characteristics for the development of capacitive sensing surfaces, allowing for the fabrication of an antimicrobial capacitive touch sensing matrix for interactive surfaces.
SP  - 2101575
EP  - 2101575
JF  - Advanced Materials Technologies
VL  - 7
IS  - 10
PB  - 
DO  - 10.1002/admt.202101575
ER  - 

TY  - NA
AU  - Ezcurdia, Iñigo; Morales, Rafael; Andrade, Marco A. B.; Marzo, Asier
TI  - LeviPrint: Contactless Fabrication using Full Acoustic Trapping of Elongated Parts.
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Special Interest Group on Computer Graphics and Interactive Techniques Conference Proceedings
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3528233.3530752
ER  - 

TY  - JOUR
AU  - Marina-Miranda, Javier; Traver, V. Javier
TI  - Head and Eye Egocentric Gesture Recognition for Human-Robot Interaction Using Eyewear Cameras
PY  - 2022
AB  - Non-verbal communication plays a particularly important role in a wide range of scenarios in Human-Robot Interaction (HRI). Accordingly, this work addresses the problem of human gesture recognition. In particular, we focus on head and eye gestures, and adopt an egocentric (first-person) perspective using eyewear cameras. We argue that this egocentric view may offer a number of conceptual and technical benefits over scene- or robot-centric perspectives. A motion-based recognition approach is proposed, which operates at two temporal granularities. Locally, frame-to-frame homographies are estimated with a convolutional neural network (CNN). The output of this CNN is input to a long short-term memory (LSTM) to capture longer-term temporal visual relationships, which are relevant to characterize gestures. Regarding the configuration of the network architecture, one particularly interesting finding is that using the output of an internal layer of the homography CNN increases the recognition rate with respect to using the homography matrix itself. While this work focuses on action recognition, and no robot or user study has been conducted yet, the system has been designed to meet real-time constraints. The encouraging results suggest that the proposed egocentric perspective is viable, and this proof-of-concept work provides novel and useful contributions to the exciting area of HRI.
SP  - 7067
EP  - 7074
JF  - IEEE Robotics and Automation Letters
VL  - 7
IS  - 3
PB  - 
DO  - 10.1109/lra.2022.3180442
ER  - 

TY  - NA
AU  - Isomoto, Toshiya; Yamanaka, Shota; Shizuki, Buntarou
TI  - Interaction Design of Dwell Selection Toward Gaze-based AR/VR Interaction
PY  - 2022
AB  - In this paper, we first position the current dwell selection among gaze-based interactions and its advantages against head-gaze selection, which is the mainstream interface for HMDs. Next, we show how dwell selection and head-gaze selection are used in an actual interaction situation. By comparing these two selection methods, we describe the potential of dwell selection as an essential AR/VR interaction.
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3531628
ER  - 

TY  - JOUR
AU  - Tadeja, Slawomir Konrad; Lu, Yupu; Rydlewicz, Maciej; Rydlewicz, Wojciech; Bubas, Tomasz; Kristensson, Per Ola
TI  - Exploring gestural input for engineering surveys of real-life structures in virtual reality using photogrammetric 3D models
PY  - 2021
AB  - Photogrammetry is a promising set of methods for generating photorealistic 3D models of physical objects and structures. Such methods may rely solely on camera-captured photographs or include additional sensor data. Digital twins are digital replicas of physical objects and structures. Photogrammetry is an opportune approach for generating 3D models for the purpose of preparing digital twins. At a sufficiently high level of quality, digital twins provide effective archival representations of physical objects and structures and become effective substitutes for engineering inspections and surveying. While photogrammetric techniques are well-established, insights about effective methods for interacting with such models in virtual reality remain underexplored. We report the results of a qualitative engineering case study in which we asked six domain experts to carry out engineering measurement tasks in an immersive environment using bimanual gestural input coupled with gaze-tracking. The qualitative case study revealed that gaze-supported bimanual interaction of photogrammetric 3D models is a promising modality for domain experts. It allows the experts to efficiently manipulate and measure elements of the 3D model. To better allow designers to support this modality, we report design implications distilled from the feedback from the domain experts.
SP  - 31039
EP  - 31058
JF  - Multimedia Tools and Applications
VL  - 80
IS  - 20
PB  - 
DO  - 10.1007/s11042-021-10520-z
ER  - 

TY  - JOUR
AU  - Nader, Georges; Quek, Yu Han; Chia, Pei Zhi; Weeger, Oliver; Yeung, Sai-Kit
TI  - KnitKit
PY  - 2021
AB  - NA
SP  - 1
EP  - 16
JF  - ACM Transactions on Graphics
VL  - 40
IS  - 4
PB  - 
DO  - 10.1145/3476576.3476615
ER  - 

TY  - JOUR
AU  - Bisen, Dhananjay; Shukla, Rishabh; Rajpoot, Narendra; Maurya, Praphull; Uttam, Atul Kr.; Arjaria, Siddhartha kr.
TI  - Responsive human-computer interaction model based on recognition of facial landmarks using machine learning algorithms
PY  - 2022
AB  - NA
SP  - 18011
EP  - 18031
JF  - Multimedia Tools and Applications
VL  - 81
IS  - 13
PB  - 
DO  - 10.1007/s11042-022-12775-6
ER  - 

TY  - NA
AU  - Kapllani, Levi; Amanatides, Chelsea; Dion, Genevieve; Shapiro, Vadim; Breen, David E.
TI  - TopoKnit : A Process-Oriented Representation for Modeling the Topology of Yarns in Weft-Knitted Textiles
PY  - 2021
AB  - Machine knitted textiles are complex multi-scale material structures increasingly important in many industries, including consumer products, architecture, composites, medical, and military. Computational modeling, simulation, and design of industrial fabrics require efficient representations of the spatial, material, and physical properties of such structures. We propose a process-oriented representation, TopoKnit, that defines a foundational data structure for representing the topology of weft-knitted textiles at the yarn scale. Process space serves as an intermediary between the machine and fabric spaces, and supports a concise, computationally efficient evaluation approach based on on-demand, near constant-time queries. In this paper, we define the properties of the process space, and design a data structure to represent it and algorithms to evaluate it. We demonstrate the effectiveness of the representation scheme by providing results of evaluations of the data structure in support of common topological operations in the fabric space.
SP  - NA
EP  - NA
JF  - arXiv: Graphics
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Khan, Arshad; Ali, Shawkat; Khan, Saleem; Bermak, Amine
TI  - Ultra-thin and Skin-conformable Strain Sensors Fabricated by Inkjet Printing for Soft Wearable Electronics
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 IEEE International Symposium on Circuits and Systems (ISCAS)
VL  - NA
IS  - NA
PB  - 
DO  - 10.1109/iscas48785.2022.9937335
ER  - 

TY  - NA
AU  - Kim, Jiwan; Gil, Hyunjae
TI  - Top-Levi: Multi-User Interactive System Using Acoustic Levitation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3561347
ER  - 

TY  - NA
AU  - Kumar, Abhishek; Lee, Lik-Hang; Chauhan, Jagmohan; Su, Xiang; Hoque, Mohammad A.; Pirttikangas, Susanna; Tarkoma, Sasu; Hui, Pan
TI  - PassWalk: Spatial Authentication Leveraging Lateral Shift and Gaze on Mobile Headsets
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 30th ACM International Conference on Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3503161.3548252
ER  - 

TY  - JOUR
AU  - Sanchez, Vanessa; Payne, Christopher J.; Preston, Daniel J.; Alvarez, Jonathan T.; Weaver, James C.; Atalay, Asli; Boyvat, Mustafa; Vogt, Daniel M.; Wood, Robert J.; Whitesides, George M.; Walsh, Conor J.
TI  - Smart Thermally Actuating Textiles
PY  - 2020
AB  - NA
SP  - 2000383
EP  - NA
JF  - Advanced Materials Technologies
VL  - 5
IS  - 8
PB  - 
DO  - 10.1002/admt.202000383
ER  - 

TY  - NA
AU  - Ohmori, Task; Abe, Yuki; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - CHI Extended Abstracts - Remote Friction Control on 3-dimensional Object Made of Polystyrene Foam Using Airborne Ultrasound Focus
PY  - 2021
AB  - In this study, we propose a method to remotely change the friction of a three-dimensional surface made of polystyrene foam. In a previous study, a system that projects an image onto a two-dimensional polystyrene foam surface and irradiates ultrasound according to a finger’s position in contact with the surface to change the friction has been proposed. In this study, we extended it to a three-dimensional shape and quantitatively evaluated the change in friction. It was confirmed that the frictional change could be obtained even when extended to three dimensions but that the frictional change was less likely to occur depending on how the polystyrene foam was fabricated. This technology can be applied for adding the tactile feel to a simple 3D mockup model made of polystyrene foam.
SP  - NA
EP  - NA
JF  - Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3411763.3451598
ER  - 

TY  - NA
AU  - Mutasim, Aunnoy; Batmaz, Anil Ufuk; Hudhud Mughrabi, Moaaz; Stuerzlinger, Wolfgang
TI  - Performance Analysis of Saccades for Primary and Confirmatory Target Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 28th ACM Symposium on Virtual Reality Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3562939.3565619
ER  - 

TY  - NA
AU  - Kawahara, Hiroki; Yamao, Kaito; Oda, Kentaro
TI  - Magic Drops:Food 3D Printing of Colored Liquid Balls by Ultrasound Levitation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3561348
ER  - 

TY  - NA
AU  - Ishii, Ayaka; Ikematsu, Kaori; Siio, Itiro
TI  - UIST (Adjunct Volume) - Electrolysis Ion Display on Wet Surfaces
PY  - 2020
AB  - We present a novel method to render color patterns using electrolysis applied onto open wet surfaces. By implementing electrodes within a wet object and electrifying them, electrolysis can occur and generate ions. Using color indicators reacting to such ions, we can create a color-forming display. By applying common techniques, such as a printed circuit board, arbitrary patterns can be displayed by computer control. By reversing the polarity of electrodes and varying the number of ions, it is possible to fade the existing pattern and display a contrasting color. The proposed method can be used to create a color-forming display using various objects more common in everyday life than those used in conventional substantial displays.
SP  - 19
EP  - 21
JF  - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3379350.3416172
ER  - 

TY  - NA
AU  - Lee, Sang-Hyun; Jin, Taegyu; Lee, Joon Hyub; Bae, Seok-Hyung
TI  - WireSketch: Bimanual Interactions for 3D Curve Networks in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3558726
ER  - 

TY  - NA
AU  - Shi, Rongkai; Zhang, Jialin; Stuerzlinger, Wolfgang; Liang, Hai-Ning
TI  - Group-based Object Alignment in Virtual Reality Environments
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM Symposium on Spatial User Interaction
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3565970.3567682
ER  - 

TY  - NA
AU  - Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Kristensson, Per Ola; Ofek, Eyal; Pahud, Michel; Grubert, Jens
TI  - Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers
PY  - 2020
AB  - Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Reiter, Katharina; Pfeuffer, Ken; Esteves, Augusto; Mittermeier, Tim; Alt, Florian
TI  - Look & Turn: One-handed and Expressive Menu Interaction by Gaze and Arm Turns in VR
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - 2022 Symposium on Eye Tracking Research and Applications
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3517031.3529233
ER  - 

TY  - NA
AU  - Seitz, Klara; Rein, Patrick; Lincke, Jens; Hirschfeld, Robert
TI  - Digital Crochet: Toward a Visual Language for Pattern Description
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Proceedings of the 2022 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3563835.3567657
ER  - 

TY  - JOUR
AU  - Khan, Arshad; Rahman, Khalid; Ali, Shawkat; Khan, Saleem; Wang, Bo; Bermak, Amine
TI  - Fabrication of circuits by multi-nozzle electrohydrodynamic inkjet printing for soft wearable electronics
PY  - 2021
AB  - <jats:sec> <jats:title>Abstract</jats:title> <jats:p>Wearable electronic devices are evolving from current rigid configurations to flexible and ultimately stretchable structures. These emerging systems require soft circuits for connecting the various working units of the overall system. This paper presents fabrication of soft circuits by electrohydrodynamic (EHD) inkjet-printing technique. Multi-nozzle EHD printing head is employed for rapid fabrication of electric circuits on a wide set of materials, including glass substrate (rigid), flexible polyethylene terephthalate (PET) films, and stretchable thermoplastic polyurethane (TPU) films. To avoid the effects of substrate materials on the jettability, the proposed multi-nozzle head is equipped with integrated individual counter electrodes (electrodes are placed above the printing substrate). High-resolution circuits (50 ± 5 µm) with high electrical conductivity (0.6 Ω □<jats:sup>−1</jats:sup>) on soft substrate materials validate our well-controlled multi-nozzle EHD printing approach. The produced circuits showed excellent flexibility (bending radius ≈ 5 mm radius), high stretchability (strain ≈ 100%), and long-term mechanical stability (500 cycles at 30% strain). The concept is further demonstrated with a soft strain sensor based on a multi-nozzle EHD-printed circuit, employed for monitoring the human motion (finger bending), indicating the potential applications of these circuits in soft wearable electronic devices.</jats:p> </jats:sec><jats:sec> <jats:title>Graphic Abstract</jats:title> </jats:sec>
SP  - 3568
EP  - 3578
JF  - Journal of Materials Research
VL  - 36
IS  - 18
PB  - 
DO  - 10.1557/s43578-021-00188-4
ER  - 

TY  - JOUR
AU  - Amesaka, Takashi; Watanabe, Hiroki; Sugimoto, Masanori; Shizuki, Buntarou
TI  - Gesture Recognition Method Using Acoustic Sensing on Usual Garment
PY  - 2022
AB  - <jats:p>In this study, we show a new gesture recognition method for clothing-based gesture input methods using active and passive acoustic sensing. Our system consists of a piezoelectric speaker and a microphone. The speaker transmits ultrasonic swept sine signals, and the microphone simultaneously records the ultrasonic signals that propagate through the garment and the rubbing sounds generated by the gestures on the garment. Our method recognizes a variety of gestures, such as pinch, twist, touch, and swipe, by incorporating active and passive acoustic sensing. An important feature of our method is that it does not require a dedicated garment or embroidery embedded since our system only requires a pair of piezoelectric elements to be attached to the usual garment with a magnet. We performed recognition experiments of 11 gestures on the forearm with four types of garments made from different materials and recognition experiments of five one-handed gestures on the button of a shirt and the pocket of pants. The results of a per-user classifier confirmed that the f-scores were 83.9% and 95.9% for 11 gestures with four different types of garments and 5 gestures that were selected assuming actual use, respectively. In addition, we confirmed that the system recognizes five gestures, which can be performed with one hand, with 89.2% and 92.6% accuracy in the button and pocket sites, respectively.</jats:p>
SP  - 1
EP  - 27
JF  - Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
VL  - 6
IS  - 2
PB  - 
DO  - 10.1145/3534579
ER  - 

TY  - NA
AU  - Onishi, Ryoya; Morisaki, Tao; Suzuki, Shun; Mizutani, Saya; Kamigaki, Takaaki; Fujiwara, Masahiro; Makino, Yasutoshi; Shinoda, Hiroyuki
TI  - GazeBreath: Input Method Using Gaze Pointing and Breath Selection
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Augmented Humans 2022
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3519391.3519405
ER  - 

TY  - NA
AU  - Naumann, Antonius; Methfessel, Paul
TI  - Improving 3D-Editing Workflows via Acoustic Levitation
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3561353
ER  - 

TY  - JOUR
AU  - Morales, Rafael; Pittera, Dario; Georgiou, Orestis; Kappus, Brian; Frier, William
TI  - UltraButton: A Minimalist Touchless Multimodal Haptic Button
PY  - 2022
AB  - NA
SP  - 729
EP  - 740
JF  - IEEE Transactions on Haptics
VL  - 15
IS  - 4
PB  - 
DO  - 10.1109/toh.2022.3214322
ER  - 

TY  - NA
AU  - Jankauskis, Eimontas; Elizondo, Sonia; Montano Murillo, Roberto; Marzo, Asier; Martinez Plasencia, Diego
TI  - TipTrap: A Co-located Direct Manipulation Technique for Acoustically Levitated Content.
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526113.3545675
ER  - 

TY  - NA
AU  - Hossain, Tamzid; Islam, Md. Fahimul; Delamare, William; Chowdhury, Farida; Hasan, Khalad
TI  - Exploring Social Acceptability and Users' Preferences of Head- and Eye-Based Interaction with Mobile Devices
PY  - 2021
AB  - NA
SP  - NA
EP  - NA
JF  - 20th International Conference on Mobile and Ubiquitous Multimedia
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3490632.3490636
ER  - 

TY  - NA
AU  - Narayanan, Archana; Hu, Erzhen; Heo, Seongkook
TI  - Enabling Remote Hand Guidance in Video Calls Using Directional Force Illusion
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - Companion Computer Supported Cooperative Work and Social Computing
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3500868.3559470
ER  - 

TY  - NA
AU  - Kaspar, Alexandre; Makatura, Liane; Matusik, Wojciech
TI  - Knitting Skeletons: A Computer-Aided Design Tool for Shaping and Patterning of Knitted Garments.
PY  - 2019
AB  - This work presents a novel interactive system for simple garment composition and surface patterning. Our approach makes it easier for casual users to customize machine-knitted garments, while enabling more advanced users to design their own composable templates. Our tool combines ideas from CAD software and image editing: it allows the composition of (1) parametric knitted primitives, and (2) stitch pattern layers with different resampling behaviours. By leveraging the regularity of our primitives, our tool enables interactive customization with automated layout and real-time patterning feedback. We show a variety of garments and patterns created with our tool, and highlight our ability to transfer shape and pattern customizations between users.
SP  - NA
EP  - NA
JF  - arXiv: Human-Computer Interaction
VL  - NA
IS  - NA
PB  - 
DO  - NA
ER  - 

TY  - NA
AU  - Choi, Myungguen; Sakamoto, Daisuke; Ono, Tetsuo
TI  - Kuiper Belt: Utilizing the "Out-of-natural Angle" Region in the Eye-gaze Interaction for Virtual Reality
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517725
ER  - 

TY  - NA
AU  - Tsai, Ching-Yi; Sun, Chen-Kuo; Cheng, Lung-Pan
TI  - Garnish into Thin Air
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3526114.3561351
ER  - 

TY  - CHAP
AU  - Bandeira, Ivana; Zucatelli, Fernando H. G.
TI  - Evaluation of Draw and Use Buttons Methods for Curve Plot Using a Facial Tracker
PY  - 2022
AB  - NA
SP  - 140
EP  - 151
JF  - Human-Computer Interaction. Technological Innovation
VL  - NA
IS  - NA
PB  - 
DO  - 10.1007/978-3-031-05409-9_11
ER  - 

TY  - NA
AU  - Engert, Severin; Klamka, Konstantin; Peetz, Andreas; Dachselt, Raimund
TI  - STRAIDE: A Research Platform for Shape-Changing Spatial Displays based on Actuated Strings
PY  - 2022
AB  - NA
SP  - NA
EP  - NA
JF  - CHI Conference on Human Factors in Computing Systems
VL  - NA
IS  - NA
PB  - 
DO  - 10.1145/3491102.3517462
ER  - 
